{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting earthquakes within a polygon.", "response": "def within_polygon(self, polygon, distance=None, **kwargs):\n        '''\n        Select earthquakes within polygon\n\n        :param polygon:\n            Centre point as instance of nhlib.geo.polygon.Polygon class\n\n        :param float distance:\n            Buffer distance (km) (can take negative values)\n\n        :returns:\n            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n            containing only selected events\n        '''\n\n        if distance:\n            # If a distance is specified then dilate the polyon by distance\n            zone_polygon = polygon.dilate(distance)\n        else:\n            zone_polygon = polygon\n\n        # Make valid all events inside depth range\n        upper_depth, lower_depth = _check_depth_limits(kwargs)\n        valid_depth = np.logical_and(\n            self.catalogue.data['depth'] >= upper_depth,\n            self.catalogue.data['depth'] < lower_depth)\n\n        # Events outside polygon returned to invalid assignment\n        catalogue_mesh = Mesh(self.catalogue.data['longitude'],\n                              self.catalogue.data['latitude'],\n                              self.catalogue.data['depth'])\n        valid_id = np.logical_and(valid_depth,\n                                  zone_polygon.intersects(catalogue_mesh))\n\n        return self.select_catalogue(valid_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nselects earthquakes within a distance from a Point object.", "response": "def circular_distance_from_point(self, point, distance, **kwargs):\n        '''\n        Select earthquakes within a distance from a Point\n\n        :param point:\n            Centre point as instance of nhlib.geo.point.Point class\n\n        :param float distance:\n            Distance (km)\n\n        :returns:\n            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n            containing only selected events\n        '''\n\n        if kwargs['distance_type'] is 'epicentral':\n            locations = Mesh(\n                self.catalogue.data['longitude'],\n                self.catalogue.data['latitude'],\n                np.zeros(len(self.catalogue.data['longitude']), dtype=float))\n            point = Point(point.longitude, point.latitude, 0.0)\n        else:\n            locations = self.catalogue.hypocentres_as_mesh()\n\n        is_close = point.closer_than(locations, distance)\n\n        return self.select_catalogue(is_close)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cartesian_square_centred_on_point(self, point, distance, **kwargs):\n        '''\n        Select earthquakes from within a square centered on a point\n\n        :param point:\n            Centre point as instance of nhlib.geo.point.Point class\n\n        :param distance:\n            Distance (km)\n\n        :returns:\n            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n            class containing only selected events\n        '''\n        point_surface = Point(point.longitude, point.latitude, 0.)\n        # As distance is\n        north_point = point_surface.point_at(distance, 0., 0.)\n        east_point = point_surface.point_at(distance, 0., 90.)\n        south_point = point_surface.point_at(distance, 0., 180.)\n        west_point = point_surface.point_at(distance, 0., 270.)\n        is_long = np.logical_and(\n            self.catalogue.data['longitude'] >= west_point.longitude,\n            self.catalogue.data['longitude'] < east_point.longitude)\n        is_surface = np.logical_and(\n            is_long,\n            self.catalogue.data['latitude'] >= south_point.latitude,\n            self.catalogue.data['latitude'] < north_point.latitude)\n\n        upper_depth, lower_depth = _check_depth_limits(kwargs)\n        is_valid = np.logical_and(\n            is_surface,\n            self.catalogue.data['depth'] >= upper_depth,\n            self.catalogue.data['depth'] < lower_depth)\n\n        return self.select_catalogue(is_valid)", "response": "Select earthquakes from within a square centered on a point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef within_joyner_boore_distance(self, surface, distance, **kwargs):\n        '''\n        Select events within a Joyner-Boore distance of a fault\n\n        :param surface:\n            Fault surface as instance of\n            nhlib.geo.surface.base.SimpleFaultSurface  or as instance of\n            nhlib.geo.surface.ComplexFaultSurface\n\n        :param float distance:\n            Rupture distance (km)\n\n        :returns:\n            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n            containing only selected events\n        '''\n\n        upper_depth, lower_depth = _check_depth_limits(kwargs)\n\n        rjb = surface.get_joyner_boore_distance(\n            self.catalogue.hypocentres_as_mesh())\n        is_valid = np.logical_and(\n            rjb <= distance,\n            np.logical_and(self.catalogue.data['depth'] >= upper_depth,\n                           self.catalogue.data['depth'] < lower_depth))\n        return self.select_catalogue(is_valid)", "response": "Select events within a Joyner - Boore distance of a fault."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef within_rupture_distance(self, surface, distance,  **kwargs):\n        '''\n        Select events within a rupture distance from a fault surface\n\n        :param surface:\n            Fault surface as instance of nhlib.geo.surface.base.BaseSurface\n\n        :param float distance:\n            Rupture distance (km)\n\n        :returns:\n            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n            containing only selected events\n        '''\n        # Check for upper and lower depths\n        upper_depth, lower_depth = _check_depth_limits(kwargs)\n\n        rrupt = surface.get_min_distance(self.catalogue.hypocentres_as_mesh())\n        is_valid = np.logical_and(\n            rrupt <= distance,\n            np.logical_and(self.catalogue.data['depth'] >= upper_depth,\n                           self.catalogue.data['depth'] < lower_depth))\n\n        return self.select_catalogue(is_valid)", "response": "Select events within a rupture distance from a fault surface."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef within_time_period(self, start_time=None, end_time=None):\n        '''\n        Select earthquakes occurring within a given time period\n\n        :param start_time:\n            Earliest time (as datetime.datetime object)\n\n        :param end_time:\n            Latest time (as datetime.datetime object)\n\n        :returns:\n            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n            containing only selected events\n        '''\n        time_value = self.catalogue.get_decimal_time()\n        if not start_time:\n            if not end_time:\n                # No times input, therefore skip everything and return catalog\n                return self.catalogue\n            else:\n                start_time = np.min(self.catalogue.data['year'])\n        else:\n            start_time = _get_decimal_from_datetime(start_time)\n\n        if not end_time:\n            end_time = _get_decimal_from_datetime(datetime.now())\n        else:\n            end_time = _get_decimal_from_datetime(end_time)\n\n        # Get decimal time values\n        time_value = self.catalogue.get_decimal_time()\n\n        is_valid = np.logical_and(time_value >= start_time,\n                                  time_value < end_time)\n\n        return self.select_catalogue(is_valid)", "response": "Select earthquakes occurring within a given time period."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef within_depth_range(self, lower_depth=None, upper_depth=None):\n        '''\n        Selects events within a specified depth range\n\n        :param float lower_depth:\n            Lower depth for consideration\n\n        :param float upper_depth:\n            Upper depth for consideration\n\n        :returns:\n            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n            containing only selected events\n        '''\n        if not lower_depth:\n            if not upper_depth:\n                # No limiting depths defined - so return entire catalogue!\n                return self.catalogue\n            else:\n                lower_depth = np.inf\n\n        if not upper_depth:\n            upper_depth = 0.0\n\n        is_valid = np.logical_and(self.catalogue.data['depth'] >= upper_depth,\n                                  self.catalogue.data['depth'] < lower_depth)\n        return self.select_catalogue(is_valid)", "response": "Selects events within a specified depth range."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef within_magnitude_range(self, lower_mag=None, upper_mag=None):\n        '''\n        :param float lower_mag:\n            Lower magnitude for consideration\n\n        :param float upper_mag:\n            Upper magnitude for consideration\n\n        :returns:\n            Instance of openquake.hmtk.seismicity.catalogue.Catalogue class containing\n            only selected events\n        '''\n        if not lower_mag:\n            if not upper_mag:\n                # No limiting magnitudes defined - return entire catalogue!\n                return self.catalogue\n            else:\n                lower_mag = -np.inf\n\n        if not upper_mag:\n            upper_mag = np.inf\n\n        is_valid = np.logical_and(\n            self.catalogue.data['magnitude'] >= lower_mag,\n            self.catalogue.data['magnitude'] < upper_mag)\n\n        return self.select_catalogue(is_valid)", "response": "Returns a new instance of openquake. hmtk. seismicity. catalogue. Catalogue class containing only events within the specified magnitude range."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_cluster_set(self, vcl):\n        num_clust = np.max(vcl)\n        cluster_set = []\n        for clid in range(0, num_clust + 1):\n            idx = np.where(vcl == clid)[0]\n            cluster_cat = deepcopy(self.catalogue)\n            cluster_cat.select_catalogue_events(idx)\n            cluster_set.append((clid, cluster_cat))\n        return dict(cluster_set)", "response": "This function splits the catalogue into a dictionary containing an individual catalogue\n            of events within each cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nselect the earthquakes within a bounding box.", "response": "def within_bounding_box(self, limits):\n        \"\"\"\n        Selects the earthquakes within a bounding box.\n\n        :parameter limits:\n            A list or a numpy array with four elements in the following order:\n                - min x (longitude)\n                - min y (latitude)\n                - max x (longitude)\n                - max y (latitude)\n        :returns:\n            Returns a :class:htmk.seismicity.catalogue.Catalogue` instance\n        \"\"\"\n        is_valid = np.logical_and(\n            self.catalogue.data['longitude'] >= limits[0],\n            np.logical_and(self.catalogue.data['longitude'] <= limits[2],\n                           np.logical_and(\n                               self.catalogue.data['latitude'] >= limits[1],\n                               self.catalogue.data['latitude'] <= limits[3])))\n        return self.select_catalogue(is_valid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the path of the openquake data directory where the openquake data is stored.", "response": "def get_datadir():\n    \"\"\"\n    Extracts the path of the directory where the openquake data are stored\n    from the environment ($OQ_DATADIR) or from the shared_dir in the\n    configuration file.\n    \"\"\"\n    datadir = os.environ.get('OQ_DATADIR')\n    if not datadir:\n        shared_dir = config.directory.shared_dir\n        if shared_dir:\n            datadir = os.path.join(shared_dir, getpass.getuser(), 'oqdata')\n        else:  # use the home of the user\n            datadir = os.path.join(os.path.expanduser('~'), 'oqdata')\n    return datadir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the available calculation IDs from the datadir.", "response": "def get_calc_ids(datadir=None):\n    \"\"\"\n    Extract the available calculation IDs from the datadir, in order.\n    \"\"\"\n    datadir = datadir or get_datadir()\n    if not os.path.exists(datadir):\n        return []\n    calc_ids = set()\n    for f in os.listdir(datadir):\n        mo = re.match(CALC_REGEX, f)\n        if mo:\n            calc_ids.add(int(mo.group(2)))\n    return sorted(calc_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_last_calc_id(datadir=None):\n    datadir = datadir or get_datadir()\n    calcs = get_calc_ids(datadir)\n    if not calcs:\n        return 0\n    return calcs[-1]", "response": "Extract the latest calculation ID from the given directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hdf5new(datadir=None):\n    datadir = datadir or get_datadir()\n    calc_id = get_last_calc_id(datadir) + 1\n    fname = os.path.join(datadir, 'calc_%d.hdf5' % calc_id)\n    new = hdf5.File(fname, 'w')\n    new.path = fname\n    return new", "response": "Returns a new hdf5. File by instance with name determined by the last\n    calculation in the datadir plus one."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_calc_id_datadir(filename, datadir=None):\n    datadir = datadir or get_datadir()\n    try:\n        calc_id = int(filename)\n    except ValueError:\n        filename = os.path.abspath(filename)\n        datadir = os.path.dirname(filename)\n        mo = re.match(CALC_REGEX, os.path.basename(filename))\n        if mo is None:\n            raise ValueError('Cannot extract calc_id from %s' % filename)\n        calc_id = int(mo.group(2))\n    return calc_id, datadir", "response": "Extract the calculation ID from the given filename or integer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(calc_id, mode='r', datadir=None):\n    datadir = datadir or get_datadir()\n    dstore = DataStore(calc_id, datadir, mode=mode)\n    try:\n        hc_id = dstore['oqparam'].hazard_calculation_id\n    except KeyError:  # no oqparam\n        hc_id = None\n    if hc_id:\n        dstore.parent = read(hc_id, datadir=os.path.dirname(dstore.filename))\n    return dstore", "response": "Read the datastore for the next available item in the order they are in."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen the underlying. hdf5 file and the parent.", "response": "def open(self, mode):\n        \"\"\"\n        Open the underlying .hdf5 file and the parent, if any\n        \"\"\"\n        if self.hdf5 == ():  # not already open\n            kw = dict(mode=mode, libver='latest')\n            if mode == 'r':\n                kw['swmr'] = True\n            try:\n                self.hdf5 = hdf5.File(self.filename, **kw)\n            except OSError as exc:\n                raise OSError('%s in %s' % (exc, self.filename))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_attr(self, key, name, default=None):\n        try:\n            obj = h5py.File.__getitem__(self.hdf5, key)\n        except KeyError:\n            if self.parent != ():\n                return self.parent.get_attr(key, name, default)\n            else:\n                raise\n        try:\n            return obj.attrs[name]\n        except KeyError:\n            if default is None:\n                raise\n            return default", "response": "Get the attribute of the object with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns dictionary of attributes for that dataset path", "response": "def get_attrs(self, key):\n        \"\"\"\n        :param key: dataset path\n        :returns: dictionary of attributes for that path\n        \"\"\"\n        try:\n            dset = h5py.File.__getitem__(self.hdf5, key)\n        except KeyError:\n            if self.parent != ():\n                dset = h5py.File.__getitem__(self.parent.hdf5, key)\n            else:\n                raise\n        return dict(dset.attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a one - dimensional HDF5 dataset.", "response": "def create_dset(self, key, dtype, shape=(None,), compression=None,\n                    fillvalue=0, attrs=None):\n        \"\"\"\n        Create a one-dimensional HDF5 dataset.\n\n        :param key: name of the dataset\n        :param dtype: dtype of the dataset (usually composite)\n        :param shape: shape of the dataset, possibly extendable\n        :param compression: the kind of HDF5 compression to use\n        :param attrs: dictionary of attributes of the dataset\n        :returns: a HDF5 dataset\n        \"\"\"\n        return hdf5.create(\n            self.hdf5, key, dtype, shape, compression, fillvalue, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extend(self, key, array, **attrs):\n        try:\n            dset = self.hdf5[key]\n        except KeyError:\n            dset = hdf5.create(self.hdf5, key, array.dtype,\n                               shape=(None,) + array.shape[1:])\n        hdf5.extend(dset, array)\n        for k, v in attrs.items():\n            dset.attrs[k] = v\n        return dset", "response": "Extend the dataset associated to the given key ; create it if needed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, key, kw):\n        if key not in self:\n            obj = hdf5.LiteralAttrs()\n        else:\n            obj = self[key]\n        vars(obj).update(kw)\n        self[key] = obj\n        self.flush()", "response": "Save the object associated to key with the kw dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the path of the exported file by adding the export_dir in the front and the calculation ID at the end.", "response": "def export_path(self, relname, export_dir=None):\n        \"\"\"\n        Return the path of the exported file by adding the export_dir in\n        front, the calculation ID at the end.\n\n        :param relname: relative file name\n        :param export_dir: export directory (if None use .export_dir)\n        \"\"\"\n        # removing inner slashed to avoid creating intermediate directories\n        name, ext = relname.replace('/', '-').rsplit('.', 1)\n        newname = '%s_%s.%s' % (name, self.calc_id, ext)\n        if export_dir is None:\n            export_dir = self.export_dir\n        return os.path.join(export_dir, newname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a file name from a realization object by using prefix and extension.", "response": "def build_fname(self, prefix, postfix, fmt, export_dir=None):\n        \"\"\"\n        Build a file name from a realization, by using prefix and extension.\n\n        :param prefix: the prefix to use\n        :param postfix: the postfix to use (can be a realization object)\n        :param fmt: the extension ('csv', 'xml', etc)\n        :param export_dir: export directory (if None use .export_dir)\n        :returns: relative pathname including the extension\n        \"\"\"\n        if hasattr(postfix, 'sm_lt_path'):  # is a realization\n            fname = '%s-rlz-%03d.%s' % (prefix, postfix.ordinal, fmt)\n        else:\n            fname = prefix + ('-%s' % postfix if postfix else '') + '.' + fmt\n        return self.export_path(fname, export_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nflushes the underlying hdf5 file", "response": "def flush(self):\n        \"\"\"Flush the underlying hdf5 file\"\"\"\n        if self.parent != ():\n            self.parent.flush()\n        if self.hdf5:  # is open\n            self.hdf5.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self):\n        if self.parent != ():\n            self.parent.flush()\n            self.parent.close()\n        if self.hdf5:  # is open\n            self.hdf5.flush()\n            self.hdf5.close()\n            self.hdf5 = ()", "response": "Close the underlying hdf5 file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getsize(self, key=None):\n        if key is None:\n            return os.path.getsize(self.filename)\n        return hdf5.ByteCounter.get_nbytes(\n            h5py.File.__getitem__(self.hdf5, key))", "response": "Return the size in byte of the output associated to the given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maybe_encode(value):\n    if isinstance(value, (list, tuple)) and isinstance(value[0], str):\n        return encode(value)\n    return value", "response": "If value is a sequence of strings encode it otherwise return value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(hdf5, name, dtype, shape=(None,), compression=None,\n           fillvalue=0, attrs=None):\n    \"\"\"\n    :param hdf5: a h5py.File object\n    :param name: an hdf5 key string\n    :param dtype: dtype of the dataset (usually composite)\n    :param shape: shape of the dataset (can be extendable)\n    :param compression: None or 'gzip' are recommended\n    :param attrs: dictionary of attributes of the dataset\n    :returns: a HDF5 dataset\n    \"\"\"\n    if shape[0] is None:  # extendable dataset\n        dset = hdf5.create_dataset(\n            name, (0,) + shape[1:], dtype, chunks=True, maxshape=shape,\n            compression=compression)\n    else:  # fixed-shape dataset\n        dset = hdf5.create_dataset(name, shape, dtype, fillvalue=fillvalue,\n                                   compression=compression)\n    if attrs:\n        for k, v in attrs.items():\n            dset.attrs[k] = maybe_encode(v)\n    return dset", "response": "Create a new dataset containing the n - term data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextending an extensible dataset with an array of compatible dtype.", "response": "def extend(dset, array, **attrs):\n    \"\"\"\n    Extend an extensible dataset with an array of a compatible dtype.\n\n    :param dset: an h5py dataset\n    :param array: an array of length L\n    :returns: the total length of the dataset (i.e. initial length + L)\n    \"\"\"\n    length = len(dset)\n    if len(array) == 0:\n        return length\n    newlength = length + len(array)\n    if array.dtype.name == 'object':  # vlen array\n        shape = (newlength,) + preshape(array[0])\n    else:\n        shape = (newlength,) + array.shape[1:]\n    dset.resize(shape)\n    dset[length:newlength] = array\n    for key, val in attrs.items():\n        dset.attrs[key] = val\n    return newlength"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextend an HDF5 file dataset with the given array", "response": "def extend3(filename, key, array, **attrs):\n    \"\"\"\n    Extend an HDF5 file dataset with the given array\n    \"\"\"\n    with h5py.File(filename) as h5:\n        try:\n            dset = h5[key]\n        except KeyError:\n            if array.dtype.name == 'object':  # vlen array\n                shape = (None,) + preshape(array[0])\n            else:\n                shape = (None,) + array.shape[1:]\n            dset = create(h5, key, array.dtype, shape)\n        length = extend(dset, array)\n        for key, val in attrs.items():\n            dset.attrs[key] = val\n        h5.flush()\n    return length"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dotname2cls(dotname):\n    modname, clsname = dotname.rsplit('.', 1)\n    return getattr(importlib.import_module(modname), clsname)", "response": "Returns the class associated to the given dotname."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_nbytes(dset):\n    if 'nbytes' in dset.attrs:\n        # look if the dataset has an attribute nbytes\n        return dset.attrs['nbytes']\n    elif hasattr(dset, 'dtype'):\n        # else extract nbytes from the underlying array\n        return dset.size * numpy.zeros(1, dset.dtype).nbytes", "response": "Returns the nbytes of the underlying array if it exists otherwise returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef array_of_vstr(lst):\n    ls = []\n    for el in lst:\n        try:\n            ls.append(el.encode('utf-8'))\n        except AttributeError:\n            ls.append(el)\n    return numpy.array(ls, vstr)", "response": "returns an array of variable length ASCII strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes the values which are bytestrings.", "response": "def decode_array(values):\n    \"\"\"\n    Decode the values which are bytestrings.\n    \"\"\"\n    out = []\n    for val in values:\n        try:\n            out.append(val.decode('utf8'))\n        except AttributeError:\n            out.append(val)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the n - grams from a D - dimensional dataset or array of n - grams from a D - dimensional dataset or array of n - grams from a D - dimensional dataset or array of n - grams from a D - dimensional dataset", "response": "def extract(dset, *d_slices):\n    \"\"\"\n    :param dset: a D-dimensional dataset or array\n    :param d_slices: D slice objects (or similar)\n    :returns: a reduced D-dimensional array\n\n    >>> a = numpy.array([[1, 2, 3], [4, 5, 6]])  # shape (2, 3)\n    >>> extract(a, slice(None), 1)\n    array([[2],\n           [5]])\n    >>> extract(a, [0, 1], slice(1, 3))\n    array([[2, 3],\n           [5, 6]])\n    \"\"\"\n    shp = list(dset.shape)\n    if len(shp) != len(d_slices):\n        raise ValueError('Array with %d dimensions but %d slices' %\n                         (len(shp), len(d_slices)))\n    sizes = []\n    slices = []\n    for i, slc in enumerate(d_slices):\n        if slc == slice(None):\n            size = shp[i]\n            slices.append([slice(None)])\n        elif hasattr(slc, 'start'):\n            size = slc.stop - slc.start\n            slices.append([slice(slc.start, slc.stop, 0)])\n        elif isinstance(slc, list):\n            size = len(slc)\n            slices.append([slice(s, s + 1, j) for j, s in enumerate(slc)])\n        elif isinstance(slc, Number):\n            size = 1\n            slices.append([slice(slc, slc + 1, 0)])\n        else:\n            size = shp[i]\n            slices.append([slc])\n        sizes.append(size)\n    array = numpy.zeros(sizes, dset.dtype)\n    for tup in itertools.product(*slices):\n        aidx = tuple(s if s.step is None\n                     else slice(s.step, s.step + s.stop - s.start)\n                     for s in tup)\n        sel = tuple(s if s.step is None else slice(s.start, s.stop)\n                    for s in tup)\n        array[aidx] = dset[sel]\n    return array"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a temporary hdf5 file open for writing.", "response": "def temporary(cls):\n        \"\"\"\n        Returns a temporary hdf5 file, open for writing.\n        The temporary name is stored in the .path attribute.\n        It is the user responsability to remove the file when closed.\n        \"\"\"\n        fh, path = tempfile.mkstemp(suffix='.hdf5')\n        os.close(fh)\n        self = cls(path, 'w')\n        self.path = path\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_vlen(self, key, data):\n        shape = (None,) + data[0].shape[:-1]\n        try:\n            dset = self[key]\n        except KeyError:\n            vdt = h5py.special_dtype(vlen=data[0].dtype)\n            dset = create(self, key, vdt, shape, fillvalue=None)\n        nbytes = dset.attrs.get('nbytes', 0)\n        totlen = dset.attrs.get('totlen', 0)\n        for i, val in enumerate(data):\n            nbytes += val.nbytes\n            totlen += len(val)\n        length = len(dset)\n        dset.resize((length + len(data),) + shape[1:])\n        for i, arr in enumerate(data):\n            dset[length + i] = arr\n        dset.attrs['nbytes'] = nbytes\n        dset.attrs['totlen'] = totlen", "response": "Save a sequence of variable - length arrays in the dataset with the given key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the nbytes attribute on the HDF5 object identified by key.", "response": "def set_nbytes(self, key, nbytes=None):\n        \"\"\"\n        Set the `nbytes` attribute on the HDF5 object identified by `key`.\n        \"\"\"\n        obj = super().__getitem__(key)\n        if nbytes is not None:  # size set from outside\n            obj.attrs['nbytes'] = nbytes\n        else:  # recursively determine the size of the datagroup\n            obj.attrs['nbytes'] = nbytes = ByteCounter.get_nbytes(obj)\n        return nbytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves a node dictionary in the. hdf5 file starting from the root HDF5 dataset.", "response": "def save(self, nodedict, root=''):\n        \"\"\"\n        Save a node dictionary in the .hdf5 file, starting from the root\n        dataset. A common application is to convert XML files into .hdf5\n        files, see the usage in :mod:`openquake.commands.to_hdf5`.\n\n        :param nodedict:\n            a dictionary with keys 'tag', 'attrib', 'text', 'nodes'\n        \"\"\"\n        setitem = super().__setitem__\n        getitem = super().__getitem__\n        tag = nodedict['tag']\n        text = nodedict.get('text', None)\n        if hasattr(text, 'strip'):\n            text = text.strip()\n        attrib = nodedict.get('attrib', {})\n        path = '/'.join([root, tag])\n        nodes = nodedict.get('nodes', [])\n        if text not in ('', None):  # text=0 is stored\n            try:\n                setitem(path, text)\n            except Exception as exc:\n                sys.stderr.write('%s: %s\\n' % (path, exc))\n                raise\n        elif attrib and not nodes:\n            setitem(path, numpy.nan)\n        for subdict in _resolve_duplicates(nodes):\n            self.save(subdict, path)\n        if attrib:\n            dset = getitem(path)\n            for k, v in attrib.items():\n                dset.attrs[k] = maybe_encode(v)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the object to an HDF5 file.", "response": "def save(self, path, **extra):\n        \"\"\"\n        :param path: an .hdf5 pathname\n        :param extra: extra attributes to be saved in the file\n        \"\"\"\n        with File(path, 'w') as f:\n            for key, val in vars(self).items():\n                assert val is not None, key  # sanity check\n                try:\n                    f[key] = maybe_encode(val)\n                except ValueError as err:\n                    if 'Object header message is too large' in str(err):\n                        logging.error(str(err))\n            for k, v in extra.items():\n                f.attrs[k] = maybe_encode(v)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_table(self):\n        shape = self.shape\n        # the tagnames are bytestrings so they must be decoded\n        tagnames = decode_array(self.tagnames)\n        if len(shape) == len(tagnames):\n            return [tagnames + ['value']] + self._to_table()\n        elif len(shape) == len(tagnames) + 1:  # there is an extra field\n            tbl = [tagnames + [self.extra[0], 'value']]\n            return tbl + self._to_table(self.extra[1:])\n        else:\n            raise TypeError(\n                'There are %d dimensions but only %d tagnames' %\n                (len(shape), len(tagnames)))", "response": "Convert an arrayWrapper with shape D1... DN and attributes T1... DN into a table of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See documentation for method `GroundShakingIntensityModel` in\n        :class:~`openquake.hazardlib.gsim.base.GSIM`\n        \"\"\"\n        mean, stds = self._get_mean_and_stddevs(sites, rup, dists, imt,\n                                                stddev_types)\n        stddevs = [np.ones(len(dists.repi))*get_sigma(imt)]\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_delta(self, stds, dists):\n        delta = np.maximum((0.1-0.001*dists.repi), np.zeros_like(dists.repi))\n        return delta", "response": "Compute the additional delta to be used for the computation of the\n        upp and low models"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # distances\n        distsl = copy.copy(dists)\n        distsl.rjb, distsl.rrup = \\\n            utils.get_equivalent_distances_east(rup.mag, dists.repi)\n        #\n        # Pezeshk et al. 2011 - Rrup\n        mean1, stds1 = super().get_mean_and_stddevs(sites, rup, distsl, imt,\n                                                    stddev_types)\n        mean1 = self.apply_correction_to_BC(mean1, imt, distsl)\n        #\n        # Atkinson 2008 - Rjb\n        gmpe = Atkinson2008prime()\n        mean2, stds2 = gmpe.get_mean_and_stddevs(sites, rup, distsl, imt,\n                                                 stddev_types)\n        #\n        # Silva et al. 2002 - Rjb\n        gmpe = SilvaEtAl2002SingleCornerSaturation()\n        mean4, stds4 = gmpe.get_mean_and_stddevs(sites, rup, distsl, imt,\n                                                 stddev_types)\n        mean4 = self.apply_correction_to_BC(mean4, imt, distsl)\n        #\n        # Silva et al. 2002 - Rjb\n        gmpe = SilvaEtAl2002DoubleCornerSaturation()\n        mean5, stds5 = gmpe.get_mean_and_stddevs(sites, rup, distsl, imt,\n                                                 stddev_types)\n        mean5 = self.apply_correction_to_BC(mean5, imt, distsl)\n        #\n        # distances\n        distsl.rjb, distsl.rrup = \\\n            utils.get_equivalent_distances_east(rup.mag, dists.repi, ab06=True)\n        #\n        # Atkinson and Boore 2006 - Rrup\n        gmpe = AtkinsonBoore2006Modified2011()\n        mean3, stds3 = gmpe.get_mean_and_stddevs(sites, rup, distsl, imt,\n                                                 stddev_types)\n        # Computing adjusted mean and stds\n        mean_adj = mean1*0.2 + mean2*0.2 + mean3*0.2 + mean4*0.2 + mean5*0.2\n\n        # Note that in this case we do not apply a triangular smoothing on\n        # distance as explained at page 996 of Atkinson and Adams (2013)\n        # for the calculation of the standard deviation\n        stds_adj = np.log(np.exp(stds1)*0.2 + np.exp(stds2)*0.2 +\n                          np.exp(stds3)*0.2 + np.exp(stds4)*0.2 +\n                          np.exp(stds5)*0.2)\n        #\n        return mean_adj, stds_adj", "response": "Returns only the mean values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the mean and standard deviation for the given sites and dists.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See documentation for method `GroundShakingIntensityModel` in\n        :class:~`openquake.hazardlib.gsim.base.GSIM`\n        \"\"\"\n        # This is just used for testing purposes\n        if len(stddev_types) == 0:\n            stddev_types = [StdDev.TOTAL]\n        mean, stds = self._get_mean_and_stddevs(sites, rup, dists, imt,\n                                                stddev_types)\n        stddevs = [np.ones(len(dists.repi))*get_sigma(imt)]\n        delta = self._get_delta(stds, dists)\n        mean = mean + stds + delta\n        mean = np.squeeze(mean)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_ruptures_xml(ekey, dstore):\n    fmt = ekey[-1]\n    oq = dstore['oqparam']\n    num_ses = oq.ses_per_logic_tree_path\n    mesh = get_mesh(dstore['sitecol'])\n    ruptures_by_grp = {}\n    for rgetter in gen_rupture_getters(dstore):\n        ebrs = [ebr.export(mesh, rgetter.rlzs_by_gsim, num_ses)\n                for ebr in rgetter.get_ruptures()]\n        if ebrs:\n            ruptures_by_grp[rgetter.grp_id] = ebrs\n    dest = dstore.export_path('ses.' + fmt)\n    writer = hazard_writers.SESXMLWriter(dest)\n    writer.serialize(ruptures_by_grp, oq.investigation_time)\n    return [dest]", "response": "Exports the ruptures of the relevant rupture classes in the datastore as an XML file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_ruptures_csv(ekey, dstore):\n    oq = dstore['oqparam']\n    if 'scenario' in oq.calculation_mode:\n        return []\n    dest = dstore.export_path('ruptures.csv')\n    header = ('rupid multiplicity mag centroid_lon centroid_lat '\n              'centroid_depth trt strike dip rake boundary').split()\n    rows = []\n    for rgetter in gen_rupture_getters(dstore):\n        rups = rgetter.get_ruptures()\n        rup_data = calc.RuptureData(rgetter.trt, rgetter.rlzs_by_gsim)\n        for r in rup_data.to_array(rups):\n            rows.append(\n                (r['rup_id'], r['multiplicity'], r['mag'],\n                 r['lon'], r['lat'], r['depth'],\n                 rgetter.trt, r['strike'], r['dip'], r['rake'],\n                 r['boundary']))\n    rows.sort()  # by rupture serial\n    comment = 'investigation_time=%s, ses_per_logic_tree_path=%s' % (\n        oq.investigation_time, oq.ses_per_logic_tree_path)\n    writers.write_csv(dest, rows, header=header, sep='\\t', comment=comment)\n    return [dest]", "response": "Export the rupture data as a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting the hazard maps of the given realization into CSV.", "response": "def export_hmaps_csv(key, dest, sitemesh, array, comment):\n    \"\"\"\n    Export the hazard maps of the given realization into CSV.\n\n    :param key: output_type and export_type\n    :param dest: name of the exported file\n    :param sitemesh: site collection\n    :param array: a composite array of dtype hmap_dt\n    :param comment: comment to use as header of the exported CSV file\n    \"\"\"\n    curves = util.compose_arrays(sitemesh, array)\n    writers.write_csv(dest, curves, comment=comment)\n    return [dest]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a single Hcurve object to the file fname.", "response": "def add_imt(fname, imt):\n    \"\"\"\n    >>> add_imt('/path/to/hcurve_23.csv', 'SA(0.1)')\n    '/path/to/hcurve-SA(0.1)_23.csv'\n    \"\"\"\n    name = os.path.basename(fname)\n    newname = re.sub(r'(_\\d+\\.)', '-%s\\\\1' % imt, name)\n    return os.path.join(os.path.dirname(fname), newname)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the curves of the given realization into CSV.", "response": "def export_hcurves_by_imt_csv(\n        key, kind, rlzs_assoc, fname, sitecol, array, oq, checksum):\n    \"\"\"\n    Export the curves of the given realization into CSV.\n\n    :param key: output_type and export_type\n    :param kind: a string with the kind of output (realization or statistics)\n    :param rlzs_assoc: a :class:`openquake.commonlib.source.RlzsAssoc` instance\n    :param fname: name of the exported file\n    :param sitecol: site collection\n    :param array: an array of shape (N, L) and dtype numpy.float32\n    :param oq: job.ini parameters\n    \"\"\"\n    nsites = len(sitecol)\n    fnames = []\n    for imt, imls in oq.imtls.items():\n        slc = oq.imtls(imt)\n        dest = add_imt(fname, imt)\n        lst = [('lon', F32), ('lat', F32), ('depth', F32)]\n        for iml in imls:\n            lst.append(('poe-%s' % iml, F32))\n        hcurves = numpy.zeros(nsites, lst)\n        for sid, lon, lat, dep in zip(\n                range(nsites), sitecol.lons, sitecol.lats, sitecol.depths):\n            hcurves[sid] = (lon, lat, dep) + tuple(array[sid, slc])\n        fnames.append(writers.write_csv(dest, hcurves, comment=_comment(\n            rlzs_assoc, kind, oq.investigation_time) + (\n                ', imt=\"%s\", checksum=%d' % (imt, checksum)\n            ), header=[name for (name, dt) in lst]))\n    return fnames"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the name of the hazard curve for the current export.", "response": "def hazard_curve_name(dstore, ekey, kind, rlzs_assoc):\n    \"\"\"\n    :param calc_id: the calculation ID\n    :param ekey: the export key\n    :param kind: the kind of key\n    :param rlzs_assoc: a RlzsAssoc instance\n    \"\"\"\n    key, fmt = ekey\n    prefix = {'hcurves': 'hazard_curve', 'hmaps': 'hazard_map',\n              'uhs': 'hazard_uhs'}[key]\n    if kind.startswith('quantile-'):  # strip the 7 characters 'hazard_'\n        fname = dstore.build_fname('quantile_' + prefix[7:], kind[9:], fmt)\n    else:\n        fname = dstore.build_fname(prefix, kind, fmt)\n    return fname"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_kkf(ekey):\n    key, fmt = ekey\n    if '/' in key:\n        key, kind = key.split('/', 1)\n    else:\n        kind = ''\n    return key, kind, fmt", "response": "get the key kind and fmt from an export key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport the hazard curves into several. csv files.", "response": "def export_hcurves_csv(ekey, dstore):\n    \"\"\"\n    Exports the hazard curves into several .csv files\n\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    oq = dstore['oqparam']\n    info = get_info(dstore)\n    rlzs_assoc = dstore['csm_info'].get_rlzs_assoc()\n    R = len(rlzs_assoc.realizations)\n    sitecol = dstore['sitecol']\n    sitemesh = get_mesh(sitecol)\n    key, kind, fmt = get_kkf(ekey)\n    fnames = []\n    checksum = dstore.get_attr('/', 'checksum32')\n    hmap_dt = oq.hmap_dt()\n    for kind in oq.get_kinds(kind, R):\n        fname = hazard_curve_name(dstore, (key, fmt), kind, rlzs_assoc)\n        comment = _comment(rlzs_assoc, kind, oq.investigation_time)\n        if (key in ('hmaps', 'uhs') and oq.uniform_hazard_spectra or\n                oq.hazard_maps):\n            hmap = extract(dstore, 'hmaps?kind=' + kind)[kind]\n        if key == 'uhs' and oq.poes and oq.uniform_hazard_spectra:\n            uhs_curves = calc.make_uhs(hmap, info)\n            writers.write_csv(\n                fname, util.compose_arrays(sitemesh, uhs_curves),\n                comment=comment + ', checksum=%d' % checksum)\n            fnames.append(fname)\n        elif key == 'hmaps' and oq.poes and oq.hazard_maps:\n            fnames.extend(\n                export_hmaps_csv(ekey, fname, sitemesh,\n                                 hmap.flatten().view(hmap_dt),\n                                 comment + ', checksum=%d' % checksum))\n        elif key == 'hcurves':\n            hcurves = extract(dstore, 'hcurves?kind=' + kind)[kind]\n            fnames.extend(\n                export_hcurves_by_imt_csv(\n                    ekey, kind, rlzs_assoc, fname, sitecol, hcurves, oq,\n                    checksum))\n    return sorted(fnames)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the metadata for a single object in the datastore.", "response": "def get_metadata(realizations, kind):\n    \"\"\"\n    :param list realizations:\n        realization objects\n    :param str kind:\n        kind of data, i.e. a key in the datastore\n    :returns:\n        a dictionary with smlt_path, gsimlt_path, statistics, quantile_value\n    \"\"\"\n    metadata = {}\n    if kind.startswith('rlz-'):\n        rlz = realizations[int(kind[4:])]\n        metadata['smlt_path'] = '_'.join(rlz.sm_lt_path)\n        metadata['gsimlt_path'] = rlz.gsim_rlz.uid\n    elif kind.startswith('quantile-'):\n        metadata['statistics'] = 'quantile'\n        metadata['quantile_value'] = float(kind[9:])\n    elif kind == 'mean':\n        metadata['statistics'] = 'mean'\n    elif kind == 'max':\n        metadata['statistics'] = 'max'\n    elif kind == 'std':\n        metadata['statistics'] = 'std'\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_gmf(ekey, dstore):\n    oq = dstore['oqparam']\n    if not oq.calculation_mode.startswith('scenario'):\n        return []\n    sitecol = dstore['sitecol']\n    investigation_time = (None if oq.calculation_mode == 'scenario'\n                          else oq.investigation_time)\n    fmt = ekey[-1]\n    gmf_data = dstore['gmf_data']\n    nbytes = gmf_data.attrs['nbytes']\n    logging.info('Internal size of the GMFs: %s', humansize(nbytes))\n    if nbytes > GMF_MAX_SIZE:\n        logging.warning(GMF_WARNING, dstore.filename)\n    data = gmf_data['data'].value\n    ses_idx = 1  # for scenario only\n    events = []\n    for eid, gmfa in group_array(data, 'eid').items():\n        rup = Event(eid, ses_idx, sorted(set(gmfa['sid'])), gmfa)\n        events.append(rup)\n    fname = dstore.build_fname('gmf', 'scenario', fmt)\n    writer = hazard_writers.EventBasedGMFXMLWriter(\n        fname, sm_lt_path='', gsim_lt_path='')\n    writer.serialize(\n        GmfCollection(sitecol, oq.imtls, events, investigation_time))\n    return [fname]", "response": "Export the GMFs as a list of event based GMF XML files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_disagg_to_csv(metadata, matrices):\n    skip_keys = ('Mag', 'Dist', 'Lon', 'Lat', 'Eps', 'TRT')\n    base_header = ','.join(\n        '%s=%s' % (key, value) for key, value in metadata.items()\n        if value is not None and key not in skip_keys)\n    for disag_tup, (poe, iml, matrix, fname) in matrices.items():\n        header = '%s,poe=%.7f,iml=%.7e\\n' % (base_header, poe, iml)\n\n        if disag_tup == ('Mag', 'Lon', 'Lat'):\n            matrix = numpy.swapaxes(matrix, 0, 1)\n            matrix = numpy.swapaxes(matrix, 1, 2)\n            disag_tup = ('Lon', 'Lat', 'Mag')\n\n        axis = [metadata[v] for v in disag_tup]\n        header += ','.join(v for v in disag_tup)\n        header += ',poe'\n\n        # compute axis mid points\n        axis = [(ax[: -1] + ax[1:]) / 2. if ax.dtype == float\n                else ax for ax in axis]\n\n        values = None\n        if len(axis) == 1:\n            values = numpy.array([axis[0], matrix.flatten()]).T\n        else:\n            grids = numpy.meshgrid(*axis, indexing='ij')\n            values = [g.flatten() for g in grids]\n            values.append(matrix.flatten())\n            values = numpy.array(values).T\n\n        writers.write_csv(fname, values, comment=header, fmt='%.5E')", "response": "Save disaggregation matrices to multiple. csv files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n        if isinstance(imt, PGA):\n            imt_per = 0.0\n        else:\n            imt_per = imt.period\n        # Fix site parameters for consistent dS2S application.\n        sites.vs30 = np.array([250])\n        sites.z1pt0 = np.array([330])\n        # intensity on a reference soil is used for both mean\n        # and stddev calculations.\n        ln_y_ref = self._get_ln_y_ref(rup, dists, C)\n        # exp1 and exp2 are parts of eq. 7\n        exp1 = np.exp(C['phi3'] * (sites.vs30.clip(-np.inf, 1130) - 360))\n        exp2 = np.exp(C['phi3'] * (1130 - 360))\n        # v1 is the period dependent site term. The Vs30 above which, the\n        # amplification is constant\n        v1 = self._get_v1(imt)\n        # Get log-mean from regular unadjusted model\n        b13a_mean = self._get_mean(sites, C, ln_y_ref, exp1, exp2, v1)\n        # Adjust mean and standard deviation\n        mean = b13a_mean + self._get_dL2L(imt_per) + self._get_dS2S(imt_per)\n        mean += convert_to_LHC(imt)\n        stddevs = self._get_adjusted_stddevs(sites, rup, C, stddev_types,\n                                             ln_y_ref, exp1, exp2, imt_per)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _interp_function(self, y_ip1, y_i, t_ip1, t_i, imt_per):\n        return y_i + (y_ip1 - y_i) / (t_ip1 - t_i) * (imt_per - t_i)", "response": "Compute the interpolation function used in equation 19 of 2013 report."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_SRF_tau(self, imt_per):\n        if imt_per < 1:\n            srf = 0.87\n        elif 1 <= imt_per < 5:\n            srf = self._interp_function(0.58, 0.87, 5, 1, imt_per)\n        elif 5 <= imt_per <= 10:\n            srf = 0.58\n        else:\n            srf = 1\n\n        return srf", "response": "Returns the SRF term for the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_SRF_phi(self, imt_per):\n        if imt_per < 0.6:\n            srf = 0.8\n        elif 0.6 <= imt_per < 1:\n            srf = self._interp_function(0.7, 0.8, 1, 0.6, imt_per)\n        elif 1 <= imt_per <= 10:\n            srf = self._interp_function(0.6, 0.7, 10, 1, imt_per)\n        else:\n            srf = 1\n\n        return srf", "response": "Returns the SRF term in equation 19 of 2013 report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the sigma term in equation 19 of 2013 report.", "response": "def _get_SRF_sigma(self, imt_per):\n        \"\"\"\n        Table 8 and equation 19 of 2013 report. NB change in notation,\n        2013 report calls this term 'sigma_t' but it is referred to\n        here as sigma. Note that Table 8 is identical to Table 7 in\n        the 2013 report.\n        \"\"\"\n        if imt_per < 0.6:\n            srf = 0.8\n        elif 0.6 <= imt_per < 1:\n            srf = self._interp_function(0.7, 0.8, 1, 0.6, imt_per)\n        elif 1 <= imt_per <= 10:\n            srf = self._interp_function(0.6, 0.7, 10, 1, imt_per)\n        else:\n            srf = 1\n\n        return srf"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_dL2L(self, imt_per):\n        if imt_per < 0.18:\n            dL2L = -0.06\n        elif 0.18 <= imt_per < 0.35:\n            dL2L = self._interp_function(0.12, -0.06, 0.35, 0.18, imt_per)\n        elif 0.35 <= imt_per <= 10:\n            dL2L = self._interp_function(0.65, 0.12, 10, 0.35, imt_per)\n        else:\n            dL2L = 0\n\n        return dL2L", "response": "Returns the dL2L term for the logarithmic term in equation 19 of 2013 report."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_dS2S(self, imt_per):\n        if imt_per == 0:\n            dS2S = 0.05\n        elif 0 < imt_per < 0.15:\n            dS2S = self._interp_function(-0.15, 0.05, 0.15, 0, imt_per)\n        elif 0.15 <= imt_per < 0.45:\n            dS2S = self._interp_function(0.4, -0.15, 0.45, 0.15, imt_per)\n        elif 0.45 <= imt_per < 3.2:\n            dS2S = 0.4\n        elif 3.2 <= imt_per < 5:\n            dS2S = self._interp_function(0.08, 0.4, 5, 3.2, imt_per)\n        elif 5 <= imt_per <= 10:\n            dS2S = 0.08\n        else:\n            dS2S = 0\n\n        return dS2S", "response": "Returns the DSS2S term corresponding to the standardized logarithmic standard of the terms of the logarithmic standard."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses to add the source_id to the error message. To be used as with context(src): operation_with(src) Typically the operation is filtering a source, that can fail for tricky geometries.", "response": "def context(src):\n    \"\"\"\n    Used to add the source_id to the error message. To be used as\n\n    with context(src):\n        operation_with(src)\n\n    Typically the operation is filtering a source, that can fail for\n    tricky geometries.\n    \"\"\"\n    try:\n        yield\n    except Exception:\n        etype, err, tb = sys.exc_info()\n        msg = 'An error occurred with source id=%s. Error: %s'\n        msg %= (src.source_id, err)\n        raise_(etype, msg, tb)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_sources(srcs):\n    from openquake.hazardlib.source import splittable\n    sources = []\n    split_time = {}  # src.id -> time\n    for src in srcs:\n        t0 = time.time()\n        mag_a, mag_b = src.get_min_max_mag()\n        min_mag = src.min_mag\n        if mag_b < min_mag:  # discard the source completely\n            continue\n        has_serial = hasattr(src, 'serial')\n        if has_serial:\n            src.serial = numpy.arange(\n                src.serial, src.serial + src.num_ruptures)\n        if not splittable(src):\n            sources.append(src)\n            split_time[src.id] = time.time() - t0\n            continue\n        if min_mag:\n            splits = []\n            for s in src:\n                s.min_mag = min_mag\n                mag_a, mag_b = s.get_min_max_mag()\n                if mag_b < min_mag:\n                    continue\n                s.num_ruptures = s.count_ruptures()\n                if s.num_ruptures:\n                    splits.append(s)\n        else:\n            splits = list(src)\n        split_time[src.id] = time.time() - t0\n        sources.extend(splits)\n        has_samples = hasattr(src, 'samples')\n        if len(splits) > 1:\n            start = 0\n            for i, split in enumerate(splits):\n                split.source_id = '%s:%s' % (src.source_id, i)\n                split.src_group_id = src.src_group_id\n                split.id = src.id\n                if has_serial:\n                    nr = split.num_ruptures\n                    split.serial = src.serial[start:start + nr]\n                    start += nr\n                if has_samples:\n                    split.samples = src.samples\n        elif splits:  # single source\n            splits[0].id = src.id\n            if has_serial:\n                splits[0].serial = src.serial\n            if has_samples:\n                splits[0].samples = src.samples\n    return sources, split_time", "response": "returns a list of sources and split_sources"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a bounding box around the given lon lat.", "response": "def get_bounding_box(self, lon, lat, trt=None, mag=None):\n        \"\"\"\n        Build a bounding box around the given lon, lat by computing the\n        maximum_distance at the given tectonic region type and magnitude.\n\n        :param lon: longitude\n        :param lat: latitude\n        :param trt: tectonic region type, possibly None\n        :param mag: magnitude, possibly None\n        :returns: min_lon, min_lat, max_lon, max_lat\n        \"\"\"\n        if trt is None:  # take the greatest integration distance\n            maxdist = max(self(trt, mag) for trt in self.dic)\n        else:  # get the integration distance for the given TRT\n            maxdist = self(trt, mag)\n        a1 = min(maxdist * KM_TO_DEGREES, 90)\n        a2 = min(angular_distance(maxdist, lat), 180)\n        return lon - a2, lat - a1, lon + a2, lat + a1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the enlarged bounding box of a source object.", "response": "def get_affected_box(self, src):\n        \"\"\"\n        Get the enlarged bounding box of a source.\n\n        :param src: a source object\n        :returns: a bounding box (min_lon, min_lat, max_lon, max_lat)\n        \"\"\"\n        mag = src.get_min_max_mag()[1]\n        maxdist = self(src.tectonic_region_type, mag)\n        bbox = get_bounding_box(src, maxdist)\n        return (fix_lon(bbox[0]), bbox[1], fix_lon(bbox[2]), bbox[3])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sitecol(self):\n        if 'sitecol' in vars(self):\n            return self.__dict__['sitecol']\n        if self.filename is None or not os.path.exists(self.filename):\n            # case of nofilter/None sitecol\n            return\n        with hdf5.File(self.filename, 'r') as h5:\n            self.__dict__['sitecol'] = sc = h5.get('sitecol')\n        return sc", "response": "Read the site collection from. filename and cache it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the rectangle of the object", "response": "def get_rectangle(self, src):\n        \"\"\"\n        :param src: a source object\n        :returns: ((min_lon, min_lat), width, height), useful for plotting\n        \"\"\"\n        min_lon, min_lat, max_lon, max_lat = (\n            self.integration_distance.get_affected_box(src))\n        return (min_lon, min_lat), (max_lon - min_lon) % 360, max_lat - min_lat"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bounding_boxes(self, trt=None, mag=None):\n        bbs = []\n        for site in self.sitecol:\n            bb = self.integration_distance.get_bounding_box(\n                site.location.longitude, site.location.latitude, trt, mag)\n            bbs.append(bb)\n        return bbs", "response": "returns a list of bounding boxes for all sites in the siteset"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all the sites in the record that are within the bounding box enlarged by the integration distance for the given TRT and magnitude", "response": "def close_sids(self, rec, trt, mag):\n        \"\"\"\n        :param rec:\n           a record with fields minlon, minlat, maxlon, maxlat\n        :param trt:\n           tectonic region type string\n        :param mag:\n           magnitude\n        :returns:\n           the site indices within the bounding box enlarged by the integration\n           distance for the given TRT and magnitude\n        \"\"\"\n        if self.sitecol is None:\n            return []\n        elif not self.integration_distance:  # do not filter\n            return self.sitecol.sids\n        if hasattr(rec, 'dtype'):\n            bbox = rec['minlon'], rec['minlat'], rec['maxlon'], rec['maxlat']\n        else:\n            bbox = rec  # assume it is a 4-tuple\n        maxdist = self.integration_distance(trt, mag)\n        a1 = min(maxdist * KM_TO_DEGREES, 90)\n        a2 = min(angular_distance(maxdist, bbox[1], bbox[3]), 180)\n        bb = bbox[0] - a2, bbox[1] - a1, bbox[2] + a2, bbox[3] + a1\n        if hasattr(self, 'index'):  # RtreeFilter\n            return within(bb, self.index)\n        return self.sitecol.within_bbox(bb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields a sequence of sources with indices set to those that are within the bounding box", "response": "def filter(self, sources):\n        \"\"\"\n        :param sources: a sequence of sources\n        :yields: sources with .indices\n        \"\"\"\n        for src in sources:\n            if hasattr(src, 'indices'):   # already filtered\n                yield src\n                continue\n            box = self.integration_distance.get_affected_box(src)\n            indices = self.sitecol.within_bbox(box)\n            if len(indices):\n                src.indices = indices\n                yield src"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter(self, sources):\n        if self.sitecol is None:  # do not filter\n            yield from sources\n            return\n        for src in sources:\n            box = self.integration_distance.get_affected_box(src)\n            indices = within(box, self.index)\n            if len(indices):\n                src.indices = indices\n                yield src", "response": "yields rtree - filtered sources"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a NRML source model file to ESRI Shapefile", "response": "def to_shapefile(output, input_nrml_file, validate):\n    \"\"\"\n    Convert a NRML source model file to ESRI Shapefile(s).\n\n    For each type of source geometry defined in the NRML file (point, area,\n    simple fault, complex fault, planar) a separate shapefile is created. Each\n    shapefile is differentiated by a specific ending('_point', '_area',\n    '_simple', '_complex', '_planar').\n\n    NB: nonparametric sources are not supported.\n    \"\"\"\n    input_parser = shapefileparser.SourceModelParser()\n    source_model = input_parser.read(input_nrml_file, validate)\n    if not output:\n        output = os.path.splitext(input_nrml_file)[0]\n    print('Extracting %s_ files' % output)\n    shapefileparser.ShapefileParser().write(output, source_model)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Node object that represents a simple fault geometry", "response": "def simple_fault_node(fault_trace, dip, upper_depth, lower_depth):\n    \"\"\"\n    :param fault_trace: an object with an attribute .points\n    :param dip: dip parameter\n    :param upper_depth: upper seismogenic depth\n    :param lower_depth: lower seismogenic depth\n    :returns: a Node of kind simpleFaultGeometry\n    \"\"\"\n    node = Node('simpleFaultGeometry')\n    line = []\n    for p in fault_trace.points:\n        line.append(p.longitude)\n        line.append(p.latitude)\n    node.append(Node('gml:LineString', nodes=[Node('gml:posList', {}, line)]))\n    node.append(Node('dip', {}, dip))\n    node.append(Node('upperSeismoDepth', {}, upper_depth))\n    node.append(Node('lowerSeismoDepth', {}, lower_depth))\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_fault_data(cls, fault_trace, upper_seismogenic_depth,\n                         lower_seismogenic_depth, dip, mesh_spacing):\n        \"\"\"\n        Verify the fault data and raise ``ValueError`` if anything is wrong.\n\n        This method doesn't have to be called by hands before creating the\n        surface object, because it is called from :meth:`from_fault_data`.\n        \"\"\"\n        if not len(fault_trace) >= 2:\n            raise ValueError(\"the fault trace must have at least two points\")\n        if not fault_trace.horizontal():\n            raise ValueError(\"the fault trace must be horizontal\")\n        tlats = [point.latitude for point in fault_trace.points]\n        tlons = [point.longitude for point in fault_trace.points]\n        if geo_utils.line_intersects_itself(tlons, tlats):\n            raise ValueError(\"fault trace intersects itself\")\n        if not 0.0 < dip <= 90.0:\n            raise ValueError(\"dip must be between 0.0 and 90.0\")\n        if not lower_seismogenic_depth > upper_seismogenic_depth:\n            raise ValueError(\"lower seismogenic depth must be greater than \"\n                             \"upper seismogenic depth\")\n        if not upper_seismogenic_depth >= fault_trace[0].depth:\n            raise ValueError(\"upper seismogenic depth must be greater than \"\n                             \"or equal to depth of fault trace\")\n        if not mesh_spacing > 0.0:\n            raise ValueError(\"mesh spacing must be positive\")", "response": "Verify the fault data and raise a helpful ValueError if not."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_fault_data(cls, fault_trace, upper_seismogenic_depth,\n                        lower_seismogenic_depth, dip, mesh_spacing):\n        \"\"\"\n        Create and return a fault surface using fault source data.\n\n        :param openquake.hazardlib.geo.line.Line fault_trace:\n            Geographical line representing the intersection between the fault\n            surface and the earth surface. The line must be horizontal (i.e.\n            all depth values must be equal). If the depths are not given, they\n            are assumed to be zero, meaning the trace intersects the surface at\n            sea level, e.g. fault_trace = Line([Point(1, 1), Point(1, 2)]).\n        :param upper_seismo_depth:\n            Minimum depth ruptures can reach, in km (i.e. depth\n            to fault's top edge).\n        :param lower_seismo_depth:\n            Maximum depth ruptures can reach, in km (i.e. depth\n            to fault's bottom edge).\n        :param dip:\n            Dip angle (i.e. angle between fault surface\n            and earth surface), in degrees.\n        :param mesh_spacing:\n            Distance between two subsequent points in a mesh, in km.\n        :returns:\n            An instance of :class:`SimpleFaultSurface` created using that data.\n\n        Uses :meth:`check_fault_data` for checking parameters.\n        \"\"\"\n        cls.check_fault_data(fault_trace, upper_seismogenic_depth,\n                             lower_seismogenic_depth, dip, mesh_spacing)\n        # Loops over points in the top edge, for each point\n        # on the top edge compute corresponding point on the bottom edge, then\n        # computes equally spaced points between top and bottom points.\n\n        vdist_top = upper_seismogenic_depth - fault_trace[0].depth\n        vdist_bottom = lower_seismogenic_depth - fault_trace[0].depth\n\n        hdist_top = vdist_top / math.tan(math.radians(dip))\n        hdist_bottom = vdist_bottom / math.tan(math.radians(dip))\n\n        strike = fault_trace[0].azimuth(fault_trace[-1])\n        azimuth = (strike + 90.0) % 360\n\n        mesh = []\n        for point in fault_trace.resample(mesh_spacing):\n            top = point.point_at(hdist_top, vdist_top, azimuth)\n            bottom = point.point_at(hdist_bottom, vdist_bottom, azimuth)\n            mesh.append(top.equally_spaced_points(bottom, mesh_spacing))\n\n        # number of rows corresponds to number of points along dip\n        # number of columns corresponds to number of points along strike\n        surface_points = numpy.array(mesh).transpose().tolist()\n        mesh = RectangularMesh.from_points_list(surface_points)\n        assert 1 not in mesh.shape, (\n            \"Mesh must have at least 2 nodes along both length and width.\"\n            \" Possible cause: Mesh spacing could be too large with respect to\"\n            \" the fault length and width.\"\n        )\n        self = cls(mesh)\n        self.surface_nodes = [simple_fault_node(\n            fault_trace, dip,\n            upper_seismogenic_depth, lower_seismogenic_depth)]\n        return self", "response": "Create and return a simple fault surface from fault source data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fault_patch_vertices(cls, rupture_top_edge,\n                                 upper_seismogenic_depth,\n                                 lower_seismogenic_depth, dip, index_patch=1):\n        \"\"\"\n        Get surface main vertices.\n        Parameters are the same as for :meth:`from_fault_data`, excluding\n        fault_trace, and mesh spacing.\n\n        :param rupture_top_edge:\n            A instances of :class:`openquake.hazardlib.geo.line.Line`\n            representing the rupture surface's top edge.\n        :param index_patch:\n            Indicate the patch of the fault in order to output the vertices.\n            The fault patch numbering follows the same logic of the right-hand\n            rule i.e. patch with index 1 is the first patch along the trace.\n        :returns:\n            Four :class:~openquake.hazardlib.geo.point.Point objects\n            representing the four vertices of the target patch.\n        \"\"\"\n        # Similar to :meth:`from_fault_data`, we just don't resample edges\n        dip_tan = math.tan(math.radians(dip))\n        hdist_bottom = (\n            lower_seismogenic_depth - upper_seismogenic_depth) / dip_tan\n        strike = rupture_top_edge[0].azimuth(rupture_top_edge[-1])\n        azimuth = (strike + 90.0) % 360\n        # Collect coordinates of vertices on the top and bottom edge\n        lons = []\n        lats = []\n        deps = []\n\n        t_lon = []\n        t_lat = []\n        t_dep = []\n        for point in rupture_top_edge.points:\n            top_edge_point = point\n            bottom_edge_point = point.point_at(hdist_bottom, 0, azimuth)\n            lons.append(top_edge_point.longitude)\n            lats.append(top_edge_point.latitude)\n            deps.append(upper_seismogenic_depth)\n            t_lon.append(bottom_edge_point.longitude)\n            t_lat.append(bottom_edge_point.latitude)\n            t_dep.append(lower_seismogenic_depth)\n\n            all_lons = numpy.array(lons + list(reversed(t_lon)), float)\n            all_lats = numpy.array(lats + list(reversed(t_lat)), float)\n            all_deps = numpy.array(deps + list(reversed(t_dep)), float)\n        index1 = int(index_patch - 1)\n        index2 = int(index_patch)\n        index3 = int(2 * len(rupture_top_edge) - (index_patch + 1))\n        index4 = int(2 * len(rupture_top_edge) - index_patch)\n        p0 = Point(all_lons[index1], all_lats[index1], all_deps[index1])\n        p1 = Point(all_lons[index2], all_lats[index2], all_deps[index2])\n        p2 = Point(all_lons[index3], all_lats[index3], all_deps[index3])\n        p3 = Point(all_lons[index4], all_lats[index4], all_deps[index4])\n        return p0, p1, p2, p3", "response": "Get the surface main vertices of fault patches."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hypocentre_patch_index(cls, hypocentre, rupture_top_edge,\n                               upper_seismogenic_depth,\n                               lower_seismogenic_depth, dip):\n        \"\"\"\n        This methods finds the index of the fault patch including\n        the hypocentre.\n\n        :param hypocentre:\n            :class:`~openquake.hazardlib.geo.point.Point` object\n            representing the location of hypocentre.\n        :param rupture_top_edge:\n            A instances of :class:`openquake.hazardlib.geo.line.Line`\n            representing the rupture surface's top edge.\n        :param upper_seismo_depth:\n            Minimum depth ruptures can reach, in km (i.e. depth\n            to fault's top edge).\n        :param lower_seismo_depth:\n            Maximum depth ruptures can reach, in km (i.e. depth\n            to fault's bottom edge).\n        :param dip:\n            Dip angle (i.e. angle between fault surface\n            and earth surface), in degrees.\n        :return:\n            An integer corresponding to the index of the fault patch which\n            contains the hypocentre.\n        \"\"\"\n        totaln_patch = len(rupture_top_edge)\n        indexlist = []\n        dist_list = []\n        for i, index in enumerate(range(1, totaln_patch)):\n            p0, p1, p2, p3 = cls.get_fault_patch_vertices(\n                rupture_top_edge, upper_seismogenic_depth,\n                lower_seismogenic_depth, dip, index_patch=index)\n\n            [normal, dist_to_plane] = get_plane_equation(p0, p1, p2,\n                                                         hypocentre)\n            indexlist.append(index)\n            dist_list.append(dist_to_plane)\n            if numpy.allclose(dist_to_plane, 0., atol=25., rtol=0.):\n                return index\n                break\n        index = indexlist[numpy.argmin(dist_list)]\n        return index", "response": "This method finds the index of the fault patch which contains the hypocentre."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_surface_vertexes(cls, fault_trace,\n                             upper_seismogenic_depth,\n                             lower_seismogenic_depth, dip):\n        \"\"\"\n        Get surface main vertexes.\n\n        Parameters are the same as for :meth:`from_fault_data`, excluding\n        mesh spacing.\n\n        :returns:\n            Instance of :class:`~openquake.hazardlib.geo.polygon.Polygon`\n            describing the surface projection of the simple fault with\n            specified parameters.\n        \"\"\"\n        # Similar to :meth:`from_fault_data`, we just don't resample edges\n        dip_tan = math.tan(math.radians(dip))\n        hdist_top = upper_seismogenic_depth / dip_tan\n        hdist_bottom = lower_seismogenic_depth / dip_tan\n\n        strike = fault_trace[0].azimuth(fault_trace[-1])\n        azimuth = (strike + 90.0) % 360\n\n        # Collect coordinates of vertices on the top and bottom edge\n        lons = []\n        lats = []\n        for point in fault_trace.points:\n            top_edge_point = point.point_at(hdist_top, 0, azimuth)\n            bottom_edge_point = point.point_at(hdist_bottom, 0, azimuth)\n            lons.append(top_edge_point.longitude)\n            lats.append(top_edge_point.latitude)\n            lons.append(bottom_edge_point.longitude)\n            lats.append(bottom_edge_point.latitude)\n\n        lons = numpy.array(lons, float)\n        lats = numpy.array(lats, float)\n        return lons, lats", "response": "Get the surface vertices of the simple fault with the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef surface_projection_from_fault_data(cls, fault_trace,\n                                           upper_seismogenic_depth,\n                                           lower_seismogenic_depth, dip):\n        \"\"\"\n        Get a surface projection of the simple fault surface.\n\n        Parameters are the same as for :meth:`from_fault_data`, excluding\n        mesh spacing.\n\n        :returns:\n            Instance of :class:`~openquake.hazardlib.geo.polygon.Polygon`\n            describing the surface projection of the simple fault with\n            specified parameters.\n        \"\"\"\n        lons, lats = cls.get_surface_vertexes(fault_trace,\n                                              upper_seismogenic_depth,\n                                              lower_seismogenic_depth, dip)\n        return Mesh(lons, lats, depths=None).get_convex_hull()", "response": "Returns a polygon describing the surface projection of the simple fault surface."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS_ASC[imt]\n\n        # mean value as given by equation 1, p. 901, without considering the\n        # interface and intraslab terms (that is SI, SS, SSL = 0) and the\n        # inter and intra event terms, plus the magnitude-squared term\n        # correction factor (equation 5 p. 909).\n        mean = self._compute_magnitude_term(C, rup.mag) +\\\n            self._compute_distance_term(C, rup.mag, dists.rrup) +\\\n            self._compute_focal_depth_term(C, rup.hypo_depth) +\\\n            self._compute_faulting_style_term(C, rup.rake) +\\\n            self._compute_site_class_term(C, sites.vs30) +\\\n            self._compute_magnitude_squared_term(P=0.0, M=6.3, Q=C['QC'],\n                                                 W=C['WC'], mag=rup.mag)\n\n        # convert from cm/s**2 to g\n        mean = np.log(np.exp(mean) * 1e-2 / g)\n\n        stddevs = self._get_stddevs(C['sigma'], C['tauC'], stddev_types,\n                                    num_sites=len(sites.vs30))\n\n        return mean, stddevs", "response": "This method calculates the mean value and standard deviation from the standard deviation of the resource table entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the distance term in equation 1 p. 901.", "response": "def _compute_distance_term(self, C, mag, rrup):\n        \"\"\"\n        Compute second and third terms in equation 1, p. 901.\n        \"\"\"\n        term1 = C['b'] * rrup\n        term2 = - np.log(rrup + C['c'] * np.exp(C['d'] * mag))\n\n        return term1 + term2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_focal_depth_term(self, C, hypo_depth):\n        # p. 901. \"(i.e, depth is capped at 125 km)\".\n        focal_depth = hypo_depth\n        if focal_depth > 125.0:\n            focal_depth = 125.0\n\n        # p. 902. \"We used the value of 15 km for the\n        # depth coefficient hc ...\".\n        hc = 15.0\n\n        # p. 901. \"When h is larger than hc, the depth terms takes\n        # effect ...\". The next sentence specifies h>=hc.\n        return float(focal_depth >= hc) * C['e'] * (focal_depth - hc)", "response": "Compute the fourth term in equation 1 p. 901."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_site_class_term(self, C, vs30):\n        # map vs30 value to site class, see table 2, p. 901.\n        site_term = np.zeros(len(vs30))\n\n        # hard rock\n        site_term[vs30 > 1100.0] = C['CH']\n\n        # rock\n        site_term[(vs30 > 600) & (vs30 <= 1100)] = C['C1']\n\n        # hard soil\n        site_term[(vs30 > 300) & (vs30 <= 600)] = C['C2']\n\n        # medium soil\n        site_term[(vs30 > 200) & (vs30 <= 300)] = C['C3']\n\n        # soft soil\n        site_term[vs30 <= 200] = C['C4']\n\n        return site_term", "response": "Compute nine - th term in equation 1 p. 901."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes magnitude squared term in equation 5 p. 909.", "response": "def _compute_magnitude_squared_term(self, P, M, Q, W, mag):\n        \"\"\"\n        Compute magnitude squared term, equation 5, p. 909.\n        \"\"\"\n        return P * (mag - M) + Q * (mag - M) ** 2 + W"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n\n        dists_mod = copy.deepcopy(dists)\n        dists_mod.rrup[dists.rrup <= 5.] = 5.\n\n        return super().get_mean_and_stddevs(\n            sites, rup, dists_mod, imt, stddev_types)", "response": "Returns the mean and standard deviation for the given sites and dists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nasking for confirmation given a prompt and return a boolean value.", "response": "def confirm(prompt):\n    \"\"\"\n    Ask for confirmation, given a ``prompt`` and return a boolean value.\n    \"\"\"\n    while True:\n        try:\n            answer = input(prompt)\n        except KeyboardInterrupt:\n            # the user presses ctrl+c, just say 'no'\n            return False\n        answer = answer.strip().lower()\n        if answer not in ('y', 'n'):\n            print('Please enter y or n')\n            continue\n        return answer == 'y'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_asset_array(assets_by_site, tagnames=(), time_event=None):\n    for assets in assets_by_site:\n        if len(assets):\n            first_asset = assets[0]\n            break\n    else:  # no break\n        raise ValueError('There are no assets!')\n    loss_types = []\n    occupancy_periods = []\n    for name in sorted(first_asset.values):\n        if name.startswith('occupants_'):\n            period = name.split('_', 1)[1]\n            if period != 'None':\n                # see scenario_risk test_case_2d\n                occupancy_periods.append(period)\n            loss_types.append(name)\n            # discard occupants for different time periods\n        else:\n            loss_types.append('value-' + name)\n    # loss_types can be ['value-business_interruption', 'value-contents',\n    # 'value-nonstructural', 'occupants_None', 'occupants_day',\n    # 'occupants_night', 'occupants_transit']\n    deductible_d = first_asset.deductibles or {}\n    limit_d = first_asset.insurance_limits or {}\n    if deductible_d or limit_d:\n        logging.warning('Exposures with insuranceLimit/deductible fields are '\n                        'deprecated and may be removed in the future')\n    retro = ['retrofitted'] if first_asset._retrofitted else []\n    float_fields = loss_types + retro\n    int_fields = [(str(name), U16) for name in tagnames]\n    tagi = {str(name): i for i, name in enumerate(tagnames)}\n    asset_dt = numpy.dtype(\n        [('ordinal', U32), ('lon', F32), ('lat', F32), ('site_id', U32),\n         ('number', F32), ('area', F32)] + [\n             (str(name), float) for name in float_fields] + int_fields)\n    num_assets = sum(len(assets) for assets in assets_by_site)\n    assetcol = numpy.zeros(num_assets, asset_dt)\n    asset_ordinal = 0\n    fields = set(asset_dt.fields)\n    for sid, assets_ in enumerate(assets_by_site):\n        for asset in assets_:\n            asset.ordinal = asset_ordinal\n            record = assetcol[asset_ordinal]\n            asset_ordinal += 1\n            for field in fields:\n                if field == 'ordinal':\n                    value = asset.ordinal\n                elif field == 'number':\n                    value = asset.number\n                elif field == 'area':\n                    value = asset.area\n                elif field == 'site_id':\n                    value = sid\n                elif field == 'lon':\n                    value = asset.location[0]\n                elif field == 'lat':\n                    value = asset.location[1]\n                elif field.startswith('occupants_'):\n                    value = asset.values[field]\n                elif field == 'retrofitted':\n                    value = asset.retrofitted()\n                elif field in tagnames:\n                    value = asset.tagidxs[tagi[field]]\n                else:\n                    name, lt = field.split('-')\n                    value = asset.value(lt, time_event)\n                record[field] = value\n    return assetcol, ' '.join(occupancy_periods)", "response": "Builds an array of asset col for a list of assets_by_site and tagnames."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_exposure(fname, stop=None):\n    [exposure] = nrml.read(fname, stop=stop)\n    if not exposure.tag.endswith('exposureModel'):\n        raise InvalidFile('%s: expected exposureModel, got %s' %\n                          (fname, exposure.tag))\n    description = exposure.description\n    try:\n        conversions = exposure.conversions\n    except AttributeError:\n        conversions = Node('conversions', nodes=[Node('costTypes', [])])\n    try:\n        inslimit = conversions.insuranceLimit\n    except AttributeError:\n        inslimit = Node('insuranceLimit', text=True)\n    try:\n        deductible = conversions.deductible\n    except AttributeError:\n        deductible = Node('deductible', text=True)\n    try:\n        area = conversions.area\n    except AttributeError:\n        # NB: the area type cannot be an empty string because when sending\n        # around the CostCalculator object we would run into this numpy bug\n        # about pickling dictionaries with empty strings:\n        # https://github.com/numpy/numpy/pull/5475\n        area = Node('area', dict(type='?'))\n    try:\n        occupancy_periods = exposure.occupancyPeriods.text or ''\n    except AttributeError:\n        occupancy_periods = ''\n    try:\n        tagNames = exposure.tagNames\n    except AttributeError:\n        tagNames = Node('tagNames', text='')\n    tagnames = ~tagNames or []\n    if set(tagnames) & {'taxonomy', 'exposure', 'country'}:\n        raise InvalidFile('taxonomy, exposure and country are reserved names '\n                          'you cannot use it in <tagNames>: %s' % fname)\n    tagnames.insert(0, 'taxonomy')\n\n    # read the cost types and make some check\n    cost_types = []\n    retrofitted = False\n    for ct in conversions.costTypes:\n        with context(fname, ct):\n            ctname = ct['name']\n            if ctname == 'structural' and 'retrofittedType' in ct.attrib:\n                if ct['retrofittedType'] != ct['type']:\n                    raise ValueError(\n                        'The retrofittedType %s is different from the type'\n                        '%s' % (ct['retrofittedType'], ct['type']))\n                if ct['retrofittedUnit'] != ct['unit']:\n                    raise ValueError(\n                        'The retrofittedUnit %s is different from the unit'\n                        '%s' % (ct['retrofittedUnit'], ct['unit']))\n                retrofitted = True\n            cost_types.append(\n                (ctname, valid.cost_type_type(ct['type']), ct['unit']))\n    if 'occupants' in cost_types:\n        cost_types.append(('occupants', 'per_area', 'people'))\n    cost_types.sort(key=operator.itemgetter(0))\n    cost_types = numpy.array(cost_types, cost_type_dt)\n    insurance_limit_is_absolute = il = inslimit.get('isAbsolute')\n    deductible_is_absolute = de = deductible.get('isAbsolute')\n    cc = CostCalculator(\n        {}, {}, {},\n        True if de is None else de,\n        True if il is None else il,\n        {name: i for i, name in enumerate(tagnames)},\n    )\n    for ct in cost_types:\n        name = ct['name']  # structural, nonstructural, ...\n        cc.cost_types[name] = ct['type']  # aggregated, per_asset, per_area\n        cc.area_types[name] = area['type']\n        cc.units[name] = ct['unit']\n    assets = []\n    asset_refs = []\n    exp = Exposure(\n        exposure['id'], exposure['category'],\n        description.text, cost_types, occupancy_periods,\n        insurance_limit_is_absolute, deductible_is_absolute, retrofitted,\n        area.attrib, assets, asset_refs, cc, TagCollection(tagnames))\n    assets_text = exposure.assets.text.strip()\n    if assets_text:\n        # the <assets> tag contains a list of file names\n        dirname = os.path.dirname(fname)\n        exp.datafiles = [os.path.join(dirname, f) for f in assets_text.split()]\n    else:\n        exp.datafiles = []\n    return exp, exposure.assets", "response": "Reads an exposure XML file and returns a tuple containing the exposure instance and a list of asset nodes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_units(self, loss_types):\n        lst = []\n        for lt in loss_types:\n            if lt.endswith('_ins'):\n                lt = lt[:-4]\n            if lt == 'occupants':\n                unit = 'people'\n            else:\n                unit = self.units[lt]\n            lst.append(encode(unit))\n        return numpy.array(lst)", "response": "returns a numpy array of units suitable for HDF5"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef value(self, loss_type, time_event=None):\n        if loss_type == 'occupants':\n            return self.values['occupants_' + str(time_event)]\n        try:  # extract from the cache\n            val = self._cost[loss_type]\n        except KeyError:  # compute\n            val = self.calc(loss_type, self.values, self.area, self.number)\n            self._cost[loss_type] = val\n        return val", "response": "returns the total asset value for loss_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deductible(self, loss_type, dummy=None):\n        val = self.calc(loss_type, self.deductibles, self.area, self.number)\n        if self.calc.deduct_abs:  # convert to relative value\n            return val / self.calc(loss_type, self.values,\n                                   self.area, self.number)\n        else:\n            return val", "response": "returns the deductible fraction of the asset cost for loss_type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insurance_limit(self, loss_type, dummy=None):\n        val = self.calc(loss_type, self.insurance_limits, self.area,\n                        self.number)\n        if self.calc.limit_abs:  # convert to relative value\n            return val / self.calc(loss_type, self.values,\n                                   self.area, self.number)\n        else:\n            return val", "response": "returns the limit fraction of the asset cost for loss_type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retrofitted(self):\n        return self.calc('structural', {'structural': self._retrofitted},\n                         self.area, self.number)", "response": "returns the asset retrofitted value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a boolean array with True where the assets has tags", "response": "def tagmask(self, tags):\n        \"\"\"\n        :returns: a boolean array with True where the assets has tags\n        \"\"\"\n        mask = numpy.zeros(len(tags), bool)\n        for t, tag in enumerate(tags):\n            tagname, tagvalue = tag.split('=')\n            mask[t] = self.tagvalue(tagname) == tagvalue\n        return mask"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, tagname, tagvalue):\n        dic = getattr(self, tagname + '_idx')\n        try:\n            return dic[tagvalue]\n        except KeyError:\n            dic[tagvalue] = idx = len(dic)\n            getattr(self, tagname).append(tagvalue)\n            if idx > TWO16:\n                raise InvalidFile('contains more then %d tags' % TWO16)\n            return idx", "response": "Adds a tag to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_tags(self, dic, prefix):\n        # fill missing tagvalues with \"?\", raise an error for unknown tagnames\n        idxs = []\n        for tagname in self.tagnames:\n            if tagname in ('exposure', 'country'):\n                idxs.append(self.add(tagname, prefix))\n                continue\n            try:\n                tagvalue = dic.pop(tagname)\n            except KeyError:\n                tagvalue = '?'\n            else:\n                if tagvalue in '?*':\n                    raise ValueError(\n                        'Invalid tagvalue=\"%s\"' % tagvalue)\n            idxs.append(self.add(tagname, tagvalue))\n        if dic:\n            raise ValueError(\n                'Unknown tagname %s or <tagNames> not '\n                'specified in the exposure' % ', '.join(dic))\n        return idxs", "response": "Adds tags to the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tag(self, tagname, tagidx):\n        return '%s=%s' % (tagname, decode(getattr(self, tagname)[tagidx]))", "response": "returns the tag associated to the given tagname and tag index"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tagvalues(self, tagnames, tagidxs):\n        values = tuple(getattr(self, tagname)[tagidx + 1]\n                       for tagidx, tagname in zip(tagidxs, tagnames))\n        return values", "response": "returns the tag values associated to the given tagnames and tag index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gen_tags(self, tagname):\n        for tagvalue in getattr(self, tagname):\n            yield '%s=%s' % (tagname, decode(tagvalue))", "response": "Yields the tags associated to the given tagname."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a shape shp + tuple depending on the tagnames", "response": "def agg_shape(self, shp, aggregate_by):\n        \"\"\"\n        :returns: a shape shp + (T, ...) depending on the tagnames\n        \"\"\"\n        return shp + tuple(\n            len(getattr(self, tagname)) - 1 for tagname in aggregate_by)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an array with the number of assets per each site", "response": "def num_taxonomies_by_site(self):\n        \"\"\"\n        :returns: an array with the number of assets per each site\n        \"\"\"\n        dic = general.group_array(self.array, 'site_id')\n        num_taxonomies = numpy.zeros(self.tot_sites, U32)\n        for sid, arr in dic.items():\n            num_taxonomies[sid] = len(numpy.unique(arr['taxonomy']))\n        return num_taxonomies"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_aids_by_tag(self):\n        aids_by_tag = general.AccumDict(accum=set())\n        for aid, ass in enumerate(self):\n            for tagname in self.tagnames:\n                tag = self.tagcol.get_tag(tagname, ass[tagname])\n                aids_by_tag[tag].add(aid)\n        return aids_by_tag", "response": "returns dict of asset ID - > asset ID"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assets_by_site(self):\n        assets_by_site = [[] for sid in range(self.tot_sites)]\n        for i, ass in enumerate(self.array):\n            assets_by_site[ass['site_id']].append(self[i])\n        return numpy.array(assets_by_site)", "response": "returns a numpy array with the assets by each site"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\naggregate the asset collection by the given tagnames.", "response": "def aggregate_by(self, tagnames, array):\n        \"\"\"\n        :param tagnames: a list of valid tag names\n        :param array: an array with the same length as the asset collection\n        :returns: an array of aggregate values with the proper shape\n        \"\"\"\n        missing = set(tagnames) - set(self.tagcol.tagnames)\n        if missing:\n            raise ValueError('Unknown tagname(s) %s' % missing)\n        A, *shp = array.shape\n        if A != len(self):\n            raise ValueError('The array must have length %d, got %d' %\n                             (len(self), A))\n        if not tagnames:\n            return array.sum(axis=0)\n        shape = [len(getattr(self.tagcol, tagname))-1 for tagname in tagnames]\n        acc = numpy.zeros(shape, (F32, shp) if shp else F32)\n        for asset, row in zip(self.array, array):\n            acc[tuple(idx - 1 for idx in asset[tagnames])] += row\n        return acc"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new array with the values of the exposure aggregated by the given tagnames.", "response": "def agg_value(self, *tagnames):\n        \"\"\"\n        :param tagnames:\n            tagnames of lengths T1, T2, ... respectively\n        :returns:\n            the values of the exposure aggregated by tagnames as an array\n            of shape (T1, T2, ..., L)\n        \"\"\"\n        aval = numpy.zeros((len(self), len(self.loss_types)), F32)  # (A, L)\n        for asset in self:\n            for lti, lt in enumerate(self.loss_types):\n                if lt == 'occupants':\n                    aval[asset['ordinal'], lti] = asset[lt + '_None']\n                else:\n                    aval[asset['ordinal'], lti] = asset['value-' + lt]\n        return self.aggregate_by(list(tagnames), aval)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a reduced AssetCollection on the given sitecol.", "response": "def reduce(self, sitecol):\n        \"\"\"\n        :returns: a reduced AssetCollection on the given sitecol\n        \"\"\"\n        ok_indices = numpy.sum(\n            [self.array['site_id'] == sid for sid in sitecol.sids],\n            axis=0, dtype=bool)\n        new = object.__new__(self.__class__)\n        vars(new).update(vars(self))\n        new.array = self.array[ok_indices]\n        new.array['ordinal'] = numpy.arange(len(new.array))\n        new.asset_refs = self.asset_refs[ok_indices]\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reduce_also(self, sitecol):\n        array = []\n        asset_refs = []\n        for idx, sid in enumerate(sitecol.sids):\n            mask = self.array['site_id'] == sid\n            arr = self.array[mask]\n            arr['site_id'] = idx\n            array.append(arr)\n            asset_refs.append(self.asset_refs[mask])\n        new = object.__new__(self.__class__)\n        vars(new).update(vars(self))\n        new.tot_sites = len(sitecol)\n        new.array = numpy.concatenate(array)\n        new.array['ordinal'] = numpy.arange(len(new.array))\n        new.asset_refs = numpy.concatenate(asset_refs)\n        sitecol.make_complete()\n        return new", "response": "Returns a reduced SiteCollection on the given sitecol."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(fnames, calculation_mode='', region_constraint='',\n             ignore_missing_costs=(), asset_nodes=False, check_dupl=True,\n             tagcol=None, by_country=False):\n        \"\"\"\n        Call `Exposure.read(fname)` to get an :class:`Exposure` instance\n        keeping all the assets in memory or\n        `Exposure.read(fname, asset_nodes=True)` to get an iterator over\n        Node objects (one Node for each asset).\n        \"\"\"\n        if by_country:  # E??_ -> countrycode\n            prefix2cc = countries.from_exposures(\n                os.path.basename(f) for f in fnames)\n        else:\n            prefix = ''\n        allargs = []\n        tagcol = _minimal_tagcol(fnames, by_country)\n        for i, fname in enumerate(fnames, 1):\n            if by_country and len(fnames) > 1:\n                prefix = prefix2cc['E%02d_' % i] + '_'\n            elif len(fnames) > 1:\n                prefix = 'E%02d_' % i\n            else:\n                prefix = ''\n            allargs.append((fname, calculation_mode, region_constraint,\n                            ignore_missing_costs, asset_nodes, check_dupl,\n                            prefix, tagcol))\n        exp = None\n        for exposure in parallel.Starmap(\n                Exposure.read_exp, allargs, distribute='no'):\n            if exp is None:  # first time\n                exp = exposure\n                exp.description = 'Composite exposure[%d]' % len(fnames)\n            else:\n                assert exposure.cost_types == exp.cost_types\n                assert exposure.occupancy_periods == exp.occupancy_periods\n                assert (exposure.insurance_limit_is_absolute ==\n                        exp.insurance_limit_is_absolute)\n                assert exposure.retrofitted == exp.retrofitted\n                assert exposure.area == exp.area\n                exp.assets.extend(exposure.assets)\n                exp.asset_refs.extend(exposure.asset_refs)\n                exp.tagcol.extend(exposure.tagcol)\n        exp.exposures = [os.path.splitext(os.path.basename(f))[0]\n                         for f in fnames]\n        return exp", "response": "Read a list of Exposure files and return a list of all the relevant information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the expected CSV header from the exposure metadata", "response": "def _csv_header(self):\n        \"\"\"\n        Extract the expected CSV header from the exposure metadata\n        \"\"\"\n        fields = ['id', 'number', 'taxonomy', 'lon', 'lat']\n        for name in self.cost_types['name']:\n            fields.append(name)\n        if 'per_area' in self.cost_types['type']:\n            fields.append('area')\n        if self.occupancy_periods:\n            fields.extend(self.occupancy_periods.split())\n        fields.extend(self.tagcol.tagnames)\n        return set(fields)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_csv(self):\n        expected_header = self._csv_header()\n        for fname in self.datafiles:\n            with open(fname, encoding='utf-8') as f:\n                fields = next(csv.reader(f))\n                header = set(fields)\n                if len(header) < len(fields):\n                    raise InvalidFile(\n                        '%s: The header %s contains a duplicated field' %\n                        (fname, header))\n                elif expected_header - header - {'exposure', 'country'}:\n                    raise InvalidFile(\n                        'Unexpected header in %s\\nExpected: %s\\nGot: %s' %\n                        (fname, sorted(expected_header), sorted(header)))\n        occupancy_periods = self.occupancy_periods.split()\n        for fname in self.datafiles:\n            with open(fname, encoding='utf-8') as f:\n                for i, dic in enumerate(csv.DictReader(f), 1):\n                    asset = Node('asset', lineno=i)\n                    with context(fname, asset):\n                        asset['id'] = dic['id']\n                        asset['number'] = valid.positivefloat(dic['number'])\n                        asset['taxonomy'] = dic['taxonomy']\n                        if 'area' in dic:  # optional attribute\n                            asset['area'] = dic['area']\n                        loc = Node('location',\n                                   dict(lon=valid.longitude(dic['lon']),\n                                        lat=valid.latitude(dic['lat'])))\n                        costs = Node('costs')\n                        for cost in self.cost_types['name']:\n                            a = dict(type=cost, value=dic[cost])\n                            if 'retrofitted' in dic:\n                                a['retrofitted'] = dic['retrofitted']\n                            costs.append(Node('cost', a))\n                        occupancies = Node('occupancies')\n                        for period in occupancy_periods:\n                            a = dict(occupants=float(dic[period]),\n                                     period=period)\n                            occupancies.append(Node('occupancy', a))\n                        tags = Node('tags')\n                        for tagname in self.tagcol.tagnames:\n                            if tagname not in (\n                                    'taxonomy', 'exposure', 'country'):\n                                tags.attrib[tagname] = dic[tagname]\n                        asset.nodes.extend([loc, costs, occupancies, tags])\n                    yield asset", "response": "Yields a list of nodes for each of the data files in the CSV file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mesh_assets_by_site(self):\n        assets_by_loc = general.groupby(self, key=lambda a: a.location)\n        mesh = geo.Mesh.from_coords(list(assets_by_loc))\n        assets_by_site = [\n            assets_by_loc[lonlat] for lonlat in zip(mesh.lons, mesh.lats)]\n        return mesh, assets_by_site", "response": "Returns a mesh instance and a list of assets_by_site."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of file type and risk type for the given file", "response": "def get_risk_files(inputs):\n    \"\"\"\n    :param inputs: a dictionary key -> path name\n    :returns: a pair (file_type, {risk_type: path})\n    \"\"\"\n    rfs = {}\n    job_ini = inputs['job_ini']\n    for key in inputs:\n        if key == 'fragility':\n            # backward compatibily for .ini files with key fragility_file\n            # instead of structural_fragility_file\n            rfs['fragility/structural'] = inputs[\n                'structural_fragility'] = inputs[key]\n            del inputs['fragility']\n        elif key.endswith(('_fragility', '_vulnerability', '_consequence')):\n            match = RISK_TYPE_REGEX.match(key)\n            if match and 'retrofitted' not in key and 'consequence' not in key:\n                rfs['%s/%s' % (match.group(2), match.group(1))] = inputs[key]\n            elif match is None:\n                raise ValueError('Invalid key in %s: %s_file' % (job_ini, key))\n    return rfs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_vf_node(vf):\n    nodes = [Node('imls', {'imt': vf.imt}, vf.imls),\n             Node('meanLRs', {}, vf.mean_loss_ratios),\n             Node('covLRs', {}, vf.covs)]\n    return Node(\n        'vulnerabilityFunction',\n        {'id': vf.id, 'dist': vf.distribution_name}, nodes=nodes)", "response": "Convert a VulnerabilityFunction object into a suitable Node object for XML conversion."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of risk models for a given class.", "response": "def get_risk_models(oqparam, kind='vulnerability vulnerability_retrofitted '\n                    'fragility consequence'):\n    \"\"\"\n    :param oqparam:\n        an OqParam instance\n    :param kind:\n        a space-separated string with the kinds of risk models to read\n    :returns:\n        a dictionary riskid -> loss_type, kind -> function\n    \"\"\"\n    kinds = kind.split()\n    rmodels = AccumDict()\n    for kind in kinds:\n        for key in sorted(oqparam.inputs):\n            mo = re.match('(occupants|%s)_%s$' % (COST_TYPE_REGEX, kind), key)\n            if mo:\n                loss_type = mo.group(1)  # the cost_type in the key\n                # can be occupants, structural, nonstructural, ...\n                rmodel = nrml.to_python(oqparam.inputs[key])\n                if len(rmodel) == 0:\n                    raise InvalidFile('%s is empty!' % oqparam.inputs[key])\n                rmodels[loss_type, kind] = rmodel\n                if rmodel.lossCategory is None:  # NRML 0.4\n                    continue\n                cost_type = str(rmodel.lossCategory)\n                rmodel_kind = rmodel.__class__.__name__\n                kind_ = kind.replace('_retrofitted', '')  # strip retrofitted\n                if not rmodel_kind.lower().startswith(kind_):\n                    raise ValueError(\n                        'Error in the file \"%s_file=%s\": is '\n                        'of kind %s, expected %s' % (\n                            key, oqparam.inputs[key], rmodel_kind,\n                            kind.capitalize() + 'Model'))\n                if cost_type != loss_type:\n                    raise ValueError(\n                        'Error in the file \"%s_file=%s\": lossCategory is of '\n                        'type \"%s\", expected \"%s\"' %\n                        (key, oqparam.inputs[key],\n                         rmodel.lossCategory, loss_type))\n    rdict = AccumDict(accum={})\n    rdict.limit_states = []\n    for (loss_type, kind), rm in sorted(rmodels.items()):\n        if kind == 'fragility':\n            # build a copy of the FragilityModel with different IM levels\n            newfm = rm.build(oqparam.continuous_fragility_discretization,\n                             oqparam.steps_per_interval)\n            for (imt, riskid), ffl in newfm.items():\n                if not rdict.limit_states:\n                    rdict.limit_states.extend(rm.limitStates)\n                # we are rejecting the case of loss types with different\n                # limit states; this may change in the future\n                assert rdict.limit_states == rm.limitStates, (\n                    rdict.limit_states, rm.limitStates)\n                rdict[riskid][loss_type, kind] = ffl\n                # TODO: see if it is possible to remove the attribute\n                # below, used in classical_damage\n                ffl.steps_per_interval = oqparam.steps_per_interval\n        elif kind == 'consequence':\n            for riskid, cf in rm.items():\n                rdict[riskid][loss_type, kind] = cf\n        else:  # vulnerability\n            cl_risk = oqparam.calculation_mode in (\n                'classical', 'classical_risk')\n            # only for classical_risk reduce the loss_ratios\n            # to make sure they are strictly increasing\n            for (imt, riskid), rf in rm.items():\n                rdict[riskid][loss_type, kind] = (\n                    rf.strictly_increasing() if cl_risk else rf)\n    return rdict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rescale(curves, values):\n    A, _, C = curves.shape\n    assert A == len(values), (A, len(values))\n    array = numpy.zeros((A, C), loss_poe_dt)\n    array['loss'] = [c * v for c, v in zip(curves[:, 0], values)]\n    array['poe'] = curves[:, 1]\n    return array", "response": "Rescales the losses in each curve by the corresponding value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_riskmodel(taxonomy, oqparam, **extra):\n    riskmodel_class = registry[oqparam.calculation_mode]\n    # arguments needed to instantiate the riskmodel class\n    argnames = inspect.getfullargspec(riskmodel_class.__init__).args[3:]\n\n    # arguments extracted from oqparam\n    known_args = set(name for name, value in\n                     inspect.getmembers(oqparam.__class__)\n                     if isinstance(value, valid.Param))\n    all_args = {}\n    for argname in argnames:\n        if argname in known_args:\n            all_args[argname] = getattr(oqparam, argname)\n\n    if 'hazard_imtls' in argnames:  # special case\n        all_args['hazard_imtls'] = oqparam.imtls\n    all_args.update(extra)\n    missing = set(argnames) - set(all_args)\n    if missing:\n        raise TypeError('Missing parameter: %s' % ', '.join(missing))\n\n    return riskmodel_class(taxonomy, **all_args)", "response": "Returns an instance of the correct riskmodel class depending on the calculation_mode of the object oqparam."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of loss types with risk functions of the given intensity measure type", "response": "def get_loss_types(self, imt):\n        \"\"\"\n        :param imt: Intensity Measure Type string\n        :returns: loss types with risk functions of the given imt\n        \"\"\"\n        return [lt for lt in self.loss_types\n                if self.risk_functions[lt].imt == imt]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a beach ball as a collection of matplotlib axes instances.", "response": "def Beach(fm, linewidth=2, facecolor='b', bgcolor='w', edgecolor='k',\n          alpha=1.0, xy=(0, 0), width=200, size=100, nofill=False,\n          zorder=100, axes=None):\n    \"\"\"\n    Return a beach ball as a collection which can be connected to an\n    current matplotlib axes instance (ax.add_collection).\n\n    S1, D1, and R1, the strike, dip and rake of one of the focal planes, can\n    be vectors of multiple focal mechanisms.\n\n    :param fm: Focal mechanism that is either number of mechanisms (NM) by 3\n        (strike, dip, and rake) or NM x 6 (M11, M22, M33, M12, M13, M23 - the\n        six independent components of the moment tensor, where the coordinate\n        system is 1,2,3 = Up,South,East which equals r,theta,phi). The strike\n        is of the first plane, clockwise relative to north.\n        The dip is of the first plane, defined clockwise and perpendicular to\n        strike, relative to horizontal such that 0 is horizontal and 90 is\n        vertical. The rake is of the first focal plane solution. 90 moves the\n        hanging wall up-dip (thrust), 0 moves it in the strike direction\n        (left-lateral), -90 moves it down-dip (normal), and 180 moves it\n        opposite to strike (right-lateral).\n    :param facecolor: Color to use for quadrants of tension; can be a string,\n        e.g. ``'r'``, ``'b'`` or three component color vector, [R G B].\n        Defaults to ``'b'`` (blue).\n    :param bgcolor: The background color. Defaults to ``'w'`` (white).\n    :param edgecolor: Color of the edges. Defaults to ``'k'`` (black).\n    :param alpha: The alpha level of the beach ball. Defaults to ``1.0``\n        (opaque).\n    :param xy: Origin position of the beach ball as tuple. Defaults to\n        ``(0, 0)``.\n    :type width: int or tuple\n    :param width: Symbol size of beach ball, or tuple for elliptically\n        shaped patches. Defaults to size ``200``.\n    :param size: Controls the number of interpolation points for the\n        curves. Minimum is automatically set to ``100``.\n    :param nofill: Do not fill the beach ball, but only plot the planes.\n    :param zorder: Set zorder. Artists with lower zorder values are drawn\n        first.\n    :type axes: :class:`matplotlib.axes.Axes`\n    :param axes: Used to make beach balls circular on non-scaled axes. Also\n        maintains the aspect ratio when resizing the figure. Will not add\n        the returned collection to the axes instance.\n    \"\"\"\n    # check if one or two widths are specified (Circle or Ellipse)\n    try:\n        assert(len(width) == 2)\n    except TypeError:\n        width = (width, width)\n    mt = None\n    np1 = None\n    if isinstance(fm, MomentTensor):\n        mt = fm\n        np1 = MT2Plane(mt)\n    elif isinstance(fm, NodalPlane):\n        np1 = fm\n    elif len(fm) == 6:\n        mt = MomentTensor(fm[0], fm[1], fm[2], fm[3], fm[4], fm[5], 0)\n        np1 = MT2Plane(mt)\n    elif len(fm) == 3:\n        np1 = NodalPlane(fm[0], fm[1], fm[2])\n    else:\n        raise TypeError(\"Wrong input value for 'fm'.\")\n\n    # Only at least size 100, i.e. 100 points in the matrix are allowed\n    if size < 100:\n        size = 100\n\n    # Return as collection\n    if mt:\n        (T, N, P) = MT2Axes(mt)\n        if np.fabs(N.val) < EPSILON and np.fabs(T.val + P.val) < EPSILON:\n            colors, p = plotDC(np1, size, xy=xy, width=width)\n        else:\n            colors, p = plotMT(T, N, P, size,\n                               plot_zerotrace=True, xy=xy, width=width)\n    else:\n        colors, p = plotDC(np1, size=size, xy=xy, width=width)\n\n    if nofill:\n        # XXX: not tested with plotMT\n        col = collections.PatchCollection([p[1]], match_original=False)\n        col.set_facecolor('none')\n    else:\n        col = collections.PatchCollection(p, match_original=False)\n        # Replace color dummies 'b' and 'w' by face and bgcolor\n        fc = [facecolor if c == 'b' else bgcolor for c in colors]\n        col.set_facecolors(fc)\n\n    # Use the given axes to maintain the aspect ratio of beachballs on figure\n    # resize.\n    if axes is not None:\n        # This is what holds the aspect ratio (but breaks the positioning)\n        col.set_transform(transforms.IdentityTransform())\n        # Next is a dirty hack to fix the positioning:\n        # 1. Need to bring the all patches to the origin (0, 0).\n        for p in col._paths:\n            p.vertices -= xy\n        # 2. Then use the offset property of the collection to position the\n        #    patches\n        col.set_offsets(xy)\n        col._transOffset = axes.transData\n\n    col.set_edgecolor(edgecolor)\n    col.set_alpha(alpha)\n    col.set_linewidth(linewidth)\n    col.set_zorder(zorder)\n    return col"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndrawing a beach ball diagram of a focal mechanism.", "response": "def Beachball(fm, linewidth=2, facecolor='b', bgcolor='w', edgecolor='k',\n              alpha=1.0, xy=(0, 0), width=200, size=100, nofill=False,\n              zorder=100, outfile=None, format=None, fig=None):\n    \"\"\"\n    Draws a beach ball diagram of an earthquake focal mechanism.\n\n    S1, D1, and R1, the strike, dip and rake of one of the focal planes, can\n    be vectors of multiple focal mechanisms.\n\n    :param fm: Focal mechanism that is either number of mechanisms (NM) by 3\n        (strike, dip, and rake) or NM x 6 (M11, M22, M33, M12, M13, M23 - the\n        six independent components of the moment tensor, where the coordinate\n        system is 1,2,3 = Up,South,East which equals r,theta,phi). The strike\n        is of the first plane, clockwise relative to north.\n        The dip is of the first plane, defined clockwise and perpendicular to\n        strike, relative to horizontal such that 0 is horizontal and 90 is\n        vertical. The rake is of the first focal plane solution. 90 moves the\n        hanging wall up-dip (thrust), 0 moves it in the strike direction\n        (left-lateral), -90 moves it down-dip (normal), and 180 moves it\n        opposite to strike (right-lateral).\n    :param facecolor: Color to use for quadrants of tension; can be a string,\n        e.g. ``'r'``, ``'b'`` or three component color vector, [R G B].\n        Defaults to ``'b'`` (blue).\n    :param bgcolor: The background color. Defaults to ``'w'`` (white).\n    :param edgecolor: Color of the edges. Defaults to ``'k'`` (black).\n    :param alpha: The alpha level of the beach ball. Defaults to ``1.0``\n        (opaque).\n    :param xy: Origin position of the beach ball as tuple. Defaults to\n        ``(0, 0)``.\n    :type width: int\n    :param width: Symbol size of beach ball. Defaults to ``200``.\n    :param size: Controls the number of interpolation points for the\n        curves. Minimum is automatically set to ``100``.\n    :param nofill: Do not fill the beach ball, but only plot the planes.\n    :param zorder: Set zorder. Artists with lower zorder values are drawn\n        first.\n    :param outfile: Output file string. Also used to automatically\n        determine the output format. Supported file formats depend on your\n        matplotlib backend. Most backends support png, pdf, ps, eps and\n        svg. Defaults to ``None``.\n    :param format: Format of the graph picture. If no format is given the\n        outfile parameter will be used to try to automatically determine\n        the output format. If no format is found it defaults to png output.\n        If no outfile is specified but a format is, than a binary\n        imagestring will be returned.\n        Defaults to ``None``.\n    :param fig: Give an existing figure instance to plot into. New Figure if\n        set to ``None``.\n    \"\"\"\n    plot_width = width * 0.95\n\n    # plot the figure\n    if not fig:\n        fig = plt.figure(figsize=(3, 3), dpi=100)\n        fig.subplots_adjust(left=0, bottom=0, right=1, top=1)\n        fig.set_figheight(width // 100)\n        fig.set_figwidth(width // 100)\n    ax = fig.add_subplot(111, aspect='equal')\n\n    # hide axes + ticks\n    ax.axison = False\n\n    # plot the collection\n    collection = Beach(fm, linewidth=linewidth, facecolor=facecolor,\n                       edgecolor=edgecolor, bgcolor=bgcolor,\n                       alpha=alpha, nofill=nofill, xy=xy,\n                       width=plot_width, size=size, zorder=zorder)\n    ax.add_collection(collection)\n\n    ax.autoscale_view(tight=False, scalex=True, scaley=True)\n    # export\n    if outfile:\n        if format:\n            fig.savefig(outfile, dpi=100, transparent=True, format=format)\n        else:\n            fig.savefig(outfile, dpi=100, transparent=True)\n    elif format and not outfile:\n        imgdata = compatibility.BytesIO()\n        fig.savefig(imgdata, format=format, dpi=100, transparent=True)\n        imgdata.seek(0)\n        return imgdata.read()\n    else:\n        plt.show()\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot a beach ball plot on a plot.", "response": "def plotMT(T, N, P, size=200, plot_zerotrace=True,\n           x0=0, y0=0, xy=(0, 0), width=200):\n    \"\"\"\n    Uses a principal axis T, N and P to draw a beach ball plot.\n\n    :param ax: axis object of a matplotlib figure\n    :param T: :class:`~PrincipalAxis`\n    :param N: :class:`~PrincipalAxis`\n    :param P: :class:`~PrincipalAxis`\n\n    Adapted from ps_tensor / utilmeca.c / `Generic Mapping Tools (GMT)`_.\n\n    .. _`Generic Mapping Tools (GMT)`: http://gmt.soest.hawaii.edu\n    \"\"\"\n    # check if one or two widths are specified (Circle or Ellipse)\n    try:\n        assert(len(width) == 2)\n    except TypeError:\n        width = (width, width)\n    collect = []\n    colors = []\n    res = [value / float(size) for value in width]\n    b = 1\n    big_iso = 0\n    j = 1\n    j2 = 0\n    j3 = 0\n    n = 0\n    azi = np.zeros((3, 2))\n    x = np.zeros(400)\n    y = np.zeros(400)\n    x2 = np.zeros(400)\n    y2 = np.zeros(400)\n    x3 = np.zeros(400)\n    y3 = np.zeros(400)\n    xp1 = np.zeros(800)\n    yp1 = np.zeros(800)\n    xp2 = np.zeros(400)\n    yp2 = np.zeros(400)\n\n    a = np.zeros(3)\n    p = np.zeros(3)\n    v = np.zeros(3)\n    a[0] = T.strike\n    a[1] = N.strike\n    a[2] = P.strike\n    p[0] = T.dip\n    p[1] = N.dip\n    p[2] = P.dip\n    v[0] = T.val\n    v[1] = N.val\n    v[2] = P.val\n\n    vi = (v[0] + v[1] + v[2]) / 3.\n    for i in range(0, 3):\n        v[i] = v[i] - vi\n\n    radius_size = size * 0.5\n\n    if np.fabs(v[0] * v[0] + v[1] * v[1] + v[2] * v[2]) < EPSILON:\n        # pure implosion-explosion\n        if vi > 0.:\n            cir = patches.Ellipse(xy, width=width[0], height=width[1])\n            collect.append(cir)\n            colors.append('b')\n        if vi < 0.:\n            cir = patches.Ellipse(xy, width=width[0], height=width[1])\n            collect.append(cir)\n            colors.append('w')\n        return colors, collect\n\n    if np.fabs(v[0]) >= np.fabs(v[2]):\n        d = 0\n        m = 2\n    else:\n        d = 2\n        m = 0\n\n    if (plot_zerotrace):\n        vi = 0.\n\n    f = -v[1] / float(v[d])\n    iso = vi / float(v[d])\n\n    # Cliff Frohlich, Seismological Research letters,\n    # Vol 7, Number 1, January-February, 1996\n    # Unless the isotropic parameter lies in the range\n    # between -1 and 1 - f there will be no nodes whatsoever\n\n    if iso < -1:\n        cir = patches.Ellipse(xy, width=width[0], height=width[1])\n        collect.append(cir)\n        colors.append('w')\n        return colors, collect\n    elif iso > 1 - f:\n        cir = patches.Ellipse(xy, width=width[0], height=width[1])\n        collect.append(cir)\n        colors.append('b')\n        return colors, collect\n\n    spd = np.sin(p[d] * D2R)\n    cpd = np.cos(p[d] * D2R)\n    spb = np.sin(p[b] * D2R)\n    cpb = np.cos(p[b] * D2R)\n    spm = np.sin(p[m] * D2R)\n    cpm = np.cos(p[m] * D2R)\n    sad = np.sin(a[d] * D2R)\n    cad = np.cos(a[d] * D2R)\n    sab = np.sin(a[b] * D2R)\n    cab = np.cos(a[b] * D2R)\n    sam = np.sin(a[m] * D2R)\n    cam = np.cos(a[m] * D2R)\n\n    for i in range(0, 360):\n        fir = i * D2R\n        s2alphan = (2. + 2. * iso) / \\\n            float(3. + (1. - 2. * f) * np.cos(2. * fir))\n        if s2alphan > 1.:\n            big_iso += 1\n        else:\n            alphan = np.arcsin(np.sqrt(s2alphan))\n            sfi = np.sin(fir)\n            cfi = np.cos(fir)\n            san = np.sin(alphan)\n            can = np.cos(alphan)\n\n            xz = can * spd + san * sfi * spb + san * cfi * spm\n            xn = can * cpd * cad + san * sfi * cpb * cab + \\\n                san * cfi * cpm * cam\n            xe = can * cpd * sad + san * sfi * cpb * sab + \\\n                san * cfi * cpm * sam\n\n            if np.fabs(xn) < EPSILON and np.fabs(xe) < EPSILON:\n                takeoff = 0.\n                az = 0.\n            else:\n                az = np.arctan2(xe, xn)\n                if az < 0.:\n                    az += np.pi * 2.\n                takeoff = np.arccos(xz / float(np.sqrt(xz * xz + xn * xn +\n                                                       xe * xe)))\n            if takeoff > np.pi / 2.:\n                takeoff = np.pi - takeoff\n                az += np.pi\n                if az > np.pi * 2.:\n                    az -= np.pi * 2.\n            r = np.sqrt(2) * np.sin(takeoff / 2.)\n            si = np.sin(az)\n            co = np.cos(az)\n            if i == 0:\n                azi[i][0] = az\n                x[i] = x0 + radius_size * r * si\n                y[i] = y0 + radius_size * r * co\n                azp = az\n            else:\n                if np.fabs(np.fabs(az - azp) - np.pi) < D2R * 10.:\n                    azi[n][1] = azp\n                    n += 1\n                    azi[n][0] = az\n                if np.fabs(np.fabs(az - azp) - np.pi * 2.) < D2R * 2.:\n                    if azp < az:\n                        azi[n][0] += np.pi * 2.\n                    else:\n                        azi[n][0] -= np.pi * 2.\n                if n == 0:\n                    x[j] = x0 + radius_size * r * si\n                    y[j] = y0 + radius_size * r * co\n                    j += 1\n                elif n == 1:\n                    x2[j2] = x0 + radius_size * r * si\n                    y2[j2] = y0 + radius_size * r * co\n                    j2 += 1\n                elif n == 2:\n                    x3[j3] = x0 + radius_size * r * si\n                    y3[j3] = y0 + radius_size * r * co\n                    j3 += 1\n                azp = az\n    azi[n][1] = az\n\n    if v[1] < 0.:\n        rgb1 = 'b'\n        rgb2 = 'w'\n    else:\n        rgb1 = 'w'\n        rgb2 = 'b'\n\n    cir = patches.Ellipse(xy, width=width[0], height=width[1])\n    collect.append(cir)\n    colors.append(rgb2)\n    if n == 0:\n        collect.append(xy2patch(x[0:360], y[0:360], res, xy))\n        colors.append(rgb1)\n        return colors, collect\n    elif n == 1:\n        for i in range(0, j):\n            xp1[i] = x[i]\n            yp1[i] = y[i]\n        if azi[0][0] - azi[0][1] > np.pi:\n            azi[0][0] -= np.pi * 2.\n        elif azi[0][1] - azi[0][0] > np.pi:\n            azi[0][0] += np.pi * 2.\n        if azi[0][0] < azi[0][1]:\n            az = azi[0][1] - D2R\n            while az > azi[0][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                yp1[i] = y0 + radius_size * co\n                i += 1\n                az -= D2R\n        else:\n            az = azi[0][1] + D2R\n            while az < azi[0][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                yp1[i] = y0 + radius_size * co\n                i += 1\n                az += D2R\n        collect.append(xy2patch(xp1[0:i], yp1[0:i], res, xy))\n        colors.append(rgb1)\n        for i in range(0, j2):\n            xp2[i] = x2[i]\n            yp2[i] = y2[i]\n        if azi[1][0] - azi[1][1] > np.pi:\n            azi[1][0] -= np.pi * 2.\n        elif azi[1][1] - azi[1][0] > np.pi:\n            azi[1][0] += np.pi * 2.\n        if azi[1][0] < azi[1][1]:\n            az = azi[1][1] - D2R\n            while az > azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az -= D2R\n        else:\n            az = azi[1][1] + D2R\n            while az < azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az += D2R\n        collect.append(xy2patch(xp2[0:i], yp2[0:i], res, xy))\n        colors.append(rgb1)\n        return colors, collect\n    elif n == 2:\n        for i in range(0, j3):\n            xp1[i] = x3[i]\n            yp1[i] = y3[i]\n        for ii in range(0, j):\n            xp1[i] = x[ii]\n            i += 1\n            yp1[i] = y[ii]\n        if big_iso:\n            ii = j2 - 1\n            while ii >= 0:\n                xp1[i] = x2[ii]\n                i += 1\n                yp1[i] = y2[ii]\n                ii -= 1\n            collect.append(xy2patch(xp1[0:i], yp1[0:i], res, xy))\n            colors.append(rgb1)\n            return colors, collect\n\n        if azi[2][0] - azi[0][1] > np.pi:\n            azi[2][0] -= np.pi * 2.\n        elif azi[0][1] - azi[2][0] > np.pi:\n            azi[2][0] += np.pi * 2.\n        if azi[2][0] < azi[0][1]:\n            az = azi[0][1] - D2R\n            while az > azi[2][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                i += 1\n                yp1[i] = y0 + radius_size * co\n                az -= D2R\n        else:\n            az = azi[0][1] + D2R\n            while az < azi[2][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                i += 1\n                yp1[i] = y0 + radius_size * co\n                az += D2R\n        collect.append(xy2patch(xp1[0:i], yp1[0:i], res, xy))\n        colors.append(rgb1)\n\n        for i in range(0, j2):\n            xp2[i] = x2[i]\n            yp2[i] = y2[i]\n        if azi[1][0] - azi[1][1] > np.pi:\n            azi[1][0] -= np.pi * 2.\n        elif azi[1][1] - azi[1][0] > np.pi:\n            azi[1][0] += np.pi * 2.\n        if azi[1][0] < azi[1][1]:\n            az = azi[1][1] - D2R\n            while az > azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az -= D2R\n        else:\n            az = azi[1][1] + D2R\n            while az < azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az += D2R\n        collect.append(xy2patch(xp2[0:i], yp2[0:i], res, xy))\n        colors.append(rgb1)\n        return colors, collect"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a single double couple of the same degree as the one in the figure.", "response": "def plotDC(np1, size=200, xy=(0, 0), width=200):\n    \"\"\"\n    Uses one nodal plane of a double couple to draw a beach ball plot.\n\n    :param ax: axis object of a matplotlib figure\n    :param np1: :class:`~NodalPlane`\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n    # check if one or two widths are specified (Circle or Ellipse)\n    try:\n        assert(len(width) == 2)\n    except TypeError:\n        width = (width, width)\n    S1 = np1.strike\n    D1 = np1.dip\n    R1 = np1.rake\n\n    M = 0\n    if R1 > 180:\n        R1 -= 180\n        M = 1\n    if R1 < 0:\n        R1 += 180\n        M = 1\n\n    # Get azimuth and dip of second plane\n    (S2, D2, _R2) = AuxPlane(S1, D1, R1)\n\n    D = size / 2\n\n    if D1 >= 90:\n        D1 = 89.9999\n    if D2 >= 90:\n        D2 = 89.9999\n\n    # arange checked for numerical stablility, np.pi is not multiple of 0.1\n    phi = np.arange(0, np.pi, .01)\n    l1 = np.sqrt(\n        np.power(90 - D1, 2) / (\n            np.power(np.sin(phi), 2) +\n            np.power(np.cos(phi), 2) * np.power(90 - D1, 2) / np.power(90, 2)))\n    l2 = np.sqrt(\n        np.power(90 - D2, 2) / (\n            np.power(np.sin(phi), 2) + np.power(np.cos(phi), 2) *\n            np.power(90 - D2, 2) / np.power(90, 2)))\n\n    inc = 1\n    (X1, Y1) = Pol2Cart(phi + S1 * D2R, l1)\n\n    if M == 1:\n        lo = S1 - 180\n        hi = S2\n        if lo > hi:\n            inc = -1\n        th1 = np.arange(S1 - 180, S2, inc)\n        (Xs1, Ys1) = Pol2Cart(th1 * D2R, 90 * np.ones((1, len(th1))))\n        (X2, Y2) = Pol2Cart(phi + S2 * D2R, l2)\n        th2 = np.arange(S2 + 180, S1, -inc)\n    else:\n        hi = S1 - 180\n        lo = S2 - 180\n        if lo > hi:\n            inc = -1\n        th1 = np.arange(hi, lo, -inc)\n        (Xs1, Ys1) = Pol2Cart(th1 * D2R, 90 * np.ones((1, len(th1))))\n        (X2, Y2) = Pol2Cart(phi + S2 * D2R, l2)\n        X2 = X2[::-1]\n        Y2 = Y2[::-1]\n        th2 = np.arange(S2, S1, inc)\n    (Xs2, Ys2) = Pol2Cart(th2 * D2R, 90 * np.ones((1, len(th2))))\n    X = np.concatenate((X1, Xs1[0], X2, Xs2[0]))\n    Y = np.concatenate((Y1, Ys1[0], Y2, Ys2[0]))\n\n    X = X * D / 90\n    Y = Y * D / 90\n\n    # calculate resolution\n    res = [value / float(size) for value in width]\n\n    # construct the patches\n    collect = [patches.Ellipse(xy, width=width[0], height=width[1])]\n    collect.append(xy2patch(Y, X, res, xy))\n    return ['b', 'w'], collect"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds strike and dip of plane given normal vector having components n e and u.", "response": "def StrikeDip(n, e, u):\n    \"\"\"\n    Finds strike and dip of plane given normal vector having components n, e,\n    and u.\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n    r2d = 180 / np.pi\n    if u < 0:\n        n = -n\n        e = -e\n        u = -u\n\n    strike = np.arctan2(e, n) * r2d\n    strike = strike - 90\n    while strike >= 360:\n        strike = strike - 360\n    while strike < 0:\n        strike = strike + 360\n    x = np.sqrt(np.power(n, 2) + np.power(e, 2))\n    dip = np.arctan2(x, u) * r2d\n    return (strike, dip)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AuxPlane(s1, d1, r1):\n    r2d = 180 / np.pi\n\n    z = (s1 + 90) / r2d\n    z2 = d1 / r2d\n    z3 = r1 / r2d\n    # slick vector in plane 1\n    sl1 = -np.cos(z3) * np.cos(z) - np.sin(z3) * np.sin(z) * np.cos(z2)\n    sl2 = np.cos(z3) * np.sin(z) - np.sin(z3) * np.cos(z) * np.cos(z2)\n    sl3 = np.sin(z3) * np.sin(z2)\n    (strike, dip) = StrikeDip(sl2, sl1, sl3)\n\n    n1 = np.sin(z) * np.sin(z2)  # normal vector to plane 1\n    n2 = np.cos(z) * np.sin(z2)\n    h1 = -sl2  # strike vector of plane 2\n    h2 = sl1\n    # note h3=0 always so we leave it out\n    # n3 = np.cos(z2)\n\n    z = h1 * n1 + h2 * n2\n    z = z / np.sqrt(h1 * h1 + h2 * h2)\n    z = np.arccos(z)\n    rake = 0\n    if sl3 > 0:\n        rake = z * r2d\n    if sl3 <= 0:\n        rake = -z * r2d\n    return (strike, dip, rake)", "response": "Get Strike and dip of second plane."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef MT2Plane(mt):\n    (d, v) = np.linalg.eig(mt.mt)\n    D = np.array([d[1], d[0], d[2]])\n    V = np.array([[v[1, 1], -v[1, 0], -v[1, 2]],\n                  [v[2, 1], -v[2, 0], -v[2, 2]],\n                  [-v[0, 1], v[0, 0], v[0, 2]]])\n    IMAX = D.argmax()\n    IMIN = D.argmin()\n    AE = (V[:, IMAX] + V[:, IMIN]) / np.sqrt(2.0)\n    AN = (V[:, IMAX] - V[:, IMIN]) / np.sqrt(2.0)\n    AER = np.sqrt(np.power(AE[0], 2) + np.power(AE[1], 2) + np.power(AE[2], 2))\n    ANR = np.sqrt(np.power(AN[0], 2) + np.power(AN[1], 2) + np.power(AN[2], 2))\n    AE = AE / AER\n    if not ANR:\n        AN = np.array([np.nan, np.nan, np.nan])\n    else:\n        AN = AN / ANR\n    if AN[2] <= 0.:\n        AN1 = AN\n        AE1 = AE\n    else:\n        AN1 = -AN\n        AE1 = -AE\n    (ft, fd, fl) = TDL(AN1, AE1)\n    return NodalPlane(360 - ft, fd, 180 - fl)", "response": "Calculates a nodal plane of a given moment tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef MT2Axes(mt):\n    (D, V) = np.linalg.eigh(mt.mt)\n    pl = np.arcsin(-V[0])\n    az = np.arctan2(V[2], -V[1])\n    for i in range(0, 3):\n        if pl[i] <= 0:\n            pl[i] = -pl[i]\n            az[i] += np.pi\n        if az[i] < 0:\n            az[i] += 2 * np.pi\n        if az[i] > 2 * np.pi:\n            az[i] -= 2 * np.pi\n    pl *= R2D\n    az *= R2D\n\n    T = PrincipalAxis(D[2], az[2], pl[2])\n    N = PrincipalAxis(D[1], az[1], pl[1])\n    P = PrincipalAxis(D[0], az[0], pl[0])\n    return (T, N, P)", "response": "Calculates the principal axes of a given moment tensor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the tapering function of the tapered Gutenberg & Richter model.", "response": "def calculate_taper_function(obs_threshold_moment, sel_threshold_moment,\n                             corner_moment, beta):\n    '''\n    Calculates the tapering function of the tapered Gutenberg & Richter model:\n    as described in Bird & Liu (2007)::\n\n     taper_function = (M_0(M_T) / M_0(M_T^{CMT}))^-beta x exp((M_0(m_T^CMT) -\n         M_0(m_T)) / M_0(m_c))\n\n    :param numpy.ndarray obs_threshold_moment:\n        Moment of the threshold magnitude of the observed earthquake catalogue\n    :param numpy.ndarray sel_threshold_moment:\n        Moment of the target magnitude\n    :param float corner_momnet:\n        Corner moment of the Tapered Gutenberg-Richter Function\n    :param float beta:\n        Beta value (b * ln(10.)) of the Tapered Gutenberg-Richter Function\n    :returns:\n        Relative moment rate\n    '''\n    argument = (obs_threshold_moment - sel_threshold_moment) /\\\n        corner_moment\n    if argument < -100.0:\n        g_function = 0.0\n    else:\n        g_function = ((sel_threshold_moment / obs_threshold_moment) **\n                      -beta) * exp(argument)\n    return g_function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntapering Gutenberg - Richter Cumulative Density Function", "response": "def tapered_gutenberg_richter_cdf(moment, moment_threshold, beta,\n                                  corner_moment):\n    '''\n    Tapered Gutenberg Richter Cumulative Density Function\n\n    :param float or numpy.ndarray moment:\n        Moment for calculation of rate\n\n    :param float or numpy.ndarray moment_threshold:\n        Threshold Moment of the distribution (moment rate essentially!)\n\n    :param float beta:\n        Beta value (b * ln(10.)) of the Tapered Gutenberg-Richter Function\n\n    :param float corner_momnet:\n        Corner moment of the Tapered Gutenberg-Richter Function\n\n    :returns:\n        Cumulative probability of moment release > moment\n\n\n    '''\n    cdf = np.exp((moment_threshold - moment) / corner_moment)\n    return ((moment / moment_threshold) ** (-beta)) * cdf"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tapered_gutenberg_richter_pdf(moment, moment_threshold, beta,\n                                  corner_moment):\n    '''\n    Tapered Gutenberg-Richter Probability Density Function\n\n    :param float or numpy.ndarray moment:\n        Moment for calculation of rate\n\n    :param float or numpy.ndarray moment_threshold:\n        Threshold Moment of the distribution (moment rate essentially!)\n\n    :param float beta:\n        Beta value (b * ln(10.)) of the Tapered Gutenberg-Richter Function\n\n    :param float corner_momnet:\n        Corner moment of the Tapered Gutenberg-Richter Function\n\n    :returns:\n        Absolute probability of moment release > moment\n    '''\n    return ((beta / moment + 1. / corner_moment) *\n            tapered_gutenberg_richter_cdf(moment, moment_threshold, beta,\n                                          corner_moment))", "response": "Tapered Gutenberg - Richter Probability Density Function"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the version of the item in the datastore is different from the current version", "response": "def check_version(dstore):\n    \"\"\"\n    :param dstore: a DataStore instance\n    :returns:\n        a message if the stored version is different from the current version\n    \"\"\"\n    ds_version = dstore.hdf5.attrs['engine_version']\n    if ds_version != __version__:\n        return (': the datastore is at version %s, but the exporter at '\n                'version %s' % (ds_version, __version__))\n    else:\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_from_db(output_key, calc_id, datadir, target):\n    makedirs(target)\n    export.from_db = True\n    ds_key, fmt = output_key\n    with datastore.read(calc_id, datadir=datadir) as dstore:\n        dstore.export_dir = target\n        try:\n            exported = export(output_key, dstore)\n        except Exception:\n            etype, err, tb = sys.exc_info()\n            tb_str = ''.join(traceback.format_tb(tb))\n            version = check_version(dstore)\n            raise DataStoreExportError(\n                'Could not export %s in %s%s\\n%s%s' %\n                (output_key + (version, tb_str, err)))\n        return exported", "response": "Export the entire hierarchy of the items in the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes all of the directories in the path using os. makedirs.", "response": "def makedirs(path):\n    \"\"\"\n    Make all of the directories in the ``path`` using `os.makedirs`.\n    \"\"\"\n    if os.path.exists(path):\n        if not os.path.isdir(path):\n            # If it's not a directory, we can't do anything.\n            # This is a problem\n            raise RuntimeError('%s already exists and is not a directory.'\n                               % path)\n    else:\n        os.makedirs(path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the first pair of data keys and type from the export list", "response": "def get_outkey(dskey, export_types):\n    \"\"\"\n    Extract the first pair (dskey, exptype) found in export\n    \"\"\"\n    for exptype in export_types:\n        if (dskey, exptype) in export:\n            return (dskey, exptype)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_output(dskey, calc_id, datadir, target_dir, export_types):\n    outkey = get_outkey(dskey, export_types.split(','))\n    if export_types and not outkey:\n        yield 'There is no exporter for %s, %s' % (dskey, export_types)\n        return\n    yield from export_from_db(outkey, calc_id, datadir, target_dir)", "response": "Export the files exported by the given calculation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_observed_mmax(catalogue, config):\n    '''Check see if observed mmax values are input, if not then take\n    from the catalogue'''\n    if config['input_mmax']:\n        obsmax = config['input_mmax']\n        if config['input_mmax_uncertainty']:\n            return config['input_mmax'], config['input_mmax_uncertainty']\n        else:\n            raise ValueError('Input mmax uncertainty must be specified!')\n\n    max_location = np.argmax(catalogue['magnitude'])\n    obsmax = catalogue['magnitude'][max_location]\n    cond = isinstance(catalogue['sigmaMagnitude'], np.ndarray) and \\\n        len(catalogue['sigmaMagnitude']) > 0 and not \\\n        np.all(np.isnan(catalogue['sigmaMagnitude']))\n\n    if cond:\n        if not np.isnan(catalogue['sigmaMagnitude'][max_location]):\n            return obsmax, catalogue['sigmaMagnitude'][max_location]\n        else:\n            print('Uncertainty not given on observed Mmax\\n'\n                  'Taking largest magnitude uncertainty found in catalogue')\n            return obsmax, np.nanmax(catalogue['sigmaMagnitude'])\n    elif config['input_mmax_uncertainty']:\n        return obsmax, config['input_mmax_uncertainty']\n    else:\n        raise ValueError('Input mmax uncertainty must be specified!')", "response": "Check if observed mmax values are input take the largest magnitude uncertainty from the catalogue"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the properties of the catalogue that are above the input minimum magnitude - returns corresponding properties", "response": "def _get_magnitude_vector_properties(catalogue, config):\n    '''If an input minimum magnitude is given then consider catalogue\n    only above the minimum magnitude - returns corresponding properties'''\n\n    mmin = config.get('input_mmin', np.min(catalogue['magnitude']))\n    neq = np.float(np.sum(catalogue['magnitude'] >= mmin - 1.E-7))\n    return neq, mmin"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing and return mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        R = (dists.rrup)\n        M = rup.mag\n        # get constants\n        Ssr = self.get_Ssr_term(sites.vs30)\n        Shr = self.get_Shr_term(sites.vs30)\n        rake = rup.rake\n        F = self.get_fault_term(rake)\n\n        # compute mean\n        mean = -3.512 + (0.904 * M) - (1.328 * np.log(np.sqrt(R**2\n               + (0.149 * np.exp(0.647 * M))**2))) \\\n               + (1.125 - 0.112 * np.log(R) - 0.0957 * M) * F \\\n               + (0.440 - 0.171 * np.log(R)) * Ssr \\\n               + (0.405 - 0.222 * np.log(R)) * Shr\n        stddevs = self.get_stddevs(mean, stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_fault_term(self, rake):\n        rake = rake + 360 if rake < 0 else rake\n\n        if (rake >= 45) & (rake <= 135):\n            f = 1.\n        elif (rake >= 225) & (rake <= 315):\n            f = 0.5\n        else:\n            f = 0.\n        return f", "response": "Returns faulting term for faulting style pg 156"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_stddevs(self, mean, stddev_types):\n        mean = np.exp(mean)\n        sigma = 0.39 + np.zeros(mean.shape)\n        sigma[mean < 0.068] = 0.55\n        idx = np.logical_and(mean >= 0.068, mean <= 0.21)\n        sigma[idx] = 0.173- 0.140 * np.log(mean[idx])\n        stddevs = []\n        for stddev in stddev_types:\n            if stddev == const.StdDev.TOTAL:\n                stddevs.append(sigma)\n        return stddevs", "response": "Return the standard deviations from a given mean."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nseeing :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients\n        C = self.COEFFS[imt]\n\n        mag = self._convert_magnitude(rup.mag)\n\n        # computing the magnitude term. Equation 19, page 2291\n        f1 = self._compute_magnitude_scaling_term(C, mag)\n\n        # computing the geometrical spreading term. Equation 20, page 2291\n        f2 = self._compute_geometrical_spreading_term(C, dists.rrup)\n\n        # computing the anelastic attenuation term. Equation 21, page 2291\n        f3 = self._compute_anelastic_attenuation_term(C, dists.rrup, mag)\n\n        # computing the mean ln(IMT) using equation 18 at page 2290\n        mean = f1 + f2 + f3\n\n        mean = self._clip_mean(imt, mean)\n\n        # computing the total standard deviation\n        stddevs = self._get_stddevs(C, stddev_types, num_sites=len(dists.rrup),\n                                    mag=mag)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stddevs(self, C, stddev_types, num_sites, mag):\n        stddevs = []\n        sigma = (C['c14'] + C['c15'] * mag) if mag < 7.2 else C['c16']\n        vals = sigma * np.ones((num_sites))\n        for _ in stddev_types:\n            stddevs.append(vals)\n        return stddevs", "response": "Returns the standard deviation as defined in equation 23 page 2291\n        and Pezeshk 2005."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_magnitude_scaling_term(self, C, mag):\n        assert mag <= 8.5\n        return C['c1'] + C['c2'] * mag + C['c3'] * (8.5 - mag) ** 2.5", "response": "Compute magnitude scaling term as defined in equation 19 page 2291"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_geometrical_spreading_term(self, C, rrup):\n        f2 = np.ones_like(rrup)\n        idx1 = np.nonzero(rrup <= 70.)\n        idx2 = np.nonzero((rrup > 70.) & (rrup <= 130.))\n        idx3 = np.nonzero(rrup > 130.)\n\n        f2[idx1] = (C['c9'] * np.log(rrup[idx1] + 4.5))\n        f2[idx2] = (C['c10'] * np.log(rrup[idx2]/70.) +\n                    C['c9'] * np.log(rrup[idx2] + 4.5))\n        f2[idx3] = (C['c11'] * np.log(rrup[idx3]/130.) +\n                    C['c10'] * np.log(rrup[idx3]/70.) +\n                    C['c9'] * np.log(rrup[idx3] + 4.5))\n        return f2", "response": "Compute the geometrical spreading term as defined in equation 19 page 2291"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_anelastic_attenuation_term(self, C, rrup, mag):\n        r = (rrup**2. + (C['c5'] * np.exp(C['c6'] * mag +\n                                          C['c7'] * (8.5 - mag)**2.5))**2.)**.5\n        f3 = ((C['c4'] + C['c13'] * mag) * np.log(r) +\n              (C['c8'] + C['c12'] * mag) * r)\n        return f3", "response": "Compute anelastic attenuation term as defined in equation 21 page 2291."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a node of kind faultTopEdge intermediateEdge or faultBottomEdge", "response": "def edge_node(name, points):\n    \"\"\"\n    :param name: 'faultTopEdge', 'intermediateEdge' or 'faultBottomEdge'\n    :param points: a list of Point objects\n    :returns: a Node of kind faultTopEdge, intermediateEdge or faultBottomEdge\n    \"\"\"\n    line = []\n    for point in points:\n        line.append(point.longitude)\n        line.append(point.latitude)\n        line.append(point.depth)\n    pos = Node('gml:posList', {}, line)\n    node = Node(name, nodes=[Node('gml:LineString', nodes=[pos])])\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef complex_fault_node(edges):\n    node = Node('complexFaultGeometry')\n    node.append(edge_node('faultTopEdge', edges[0]))\n    for edge in edges[1:-1]:\n        node.append(edge_node('intermediateEdge', edge))\n    node.append(edge_node('faultBottomEdge', edges[-1]))\n    return node", "response": "returns a node that represents a complex fault geometry"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the fault dip as the average dip over the mesh.", "response": "def get_dip(self):\n        \"\"\"\n        Return the fault dip as the average dip over the mesh.\n\n        The average dip is defined as the weighted mean inclination\n        of all the mesh cells. See\n        :meth:`openquake.hazardlib.geo.mesh.RectangularMesh.get_mean_inclination_and_azimuth`\n\n        :returns:\n            The average dip, in decimal degrees.\n        \"\"\"\n        # uses the same approach as in simple fault surface\n        if self.dip is None:\n            mesh = self.mesh\n            self.dip, self.strike = mesh.get_mean_inclination_and_azimuth()\n        return self.dip"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_aki_richards_convention(cls, edges):\n        # 1) extract 4 corner points of surface mesh\n        # 2) compute cross products between left and right edges and top edge\n        # (these define vectors normal to the surface)\n        # 3) compute dot products between cross product results and\n        # position vectors associated with upper left and right corners (if\n        # both angles are less then 90 degrees then the surface is correctly\n        # defined)\n        ul = edges[0].points[0]\n        ur = edges[0].points[-1]\n        bl = edges[-1].points[0]\n        br = edges[-1].points[-1]\n        ul, ur, bl, br = spherical_to_cartesian(\n            [ul.longitude, ur.longitude, bl.longitude, br.longitude],\n            [ul.latitude, ur.latitude, bl.latitude, br.latitude],\n            [ul.depth, ur.depth, bl.depth, br.depth],\n        )\n\n        top_edge = ur - ul\n        left_edge = bl - ul\n        right_edge = br - ur\n        left_cross_top = numpy.cross(left_edge, top_edge)\n        right_cross_top = numpy.cross(right_edge, top_edge)\n\n        left_cross_top /= numpy.sqrt(numpy.dot(left_cross_top, left_cross_top))\n        right_cross_top /= numpy.sqrt(\n            numpy.dot(right_cross_top, right_cross_top)\n        )\n        ul /= numpy.sqrt(numpy.dot(ul, ul))\n        ur /= numpy.sqrt(numpy.dot(ur, ur))\n\n        # rounding to 1st digit, to avoid ValueError raised for floating point\n        # imprecision\n        angle_ul = round(\n            numpy.degrees(numpy.arccos(numpy.dot(ul, left_cross_top))), 1\n        )\n        angle_ur = round(\n            numpy.degrees(numpy.arccos(numpy.dot(ur, right_cross_top))), 1\n        )\n\n        if (angle_ul > 90) or (angle_ur > 90):\n            raise ValueError(\n                \"Surface does not conform with Aki & Richards convention\"\n            )", "response": "Verify that the surface of the Aki and Aki Richard surface is correctly defined."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_surface_validity(cls, edges):\n        # extract coordinates of surface boundary (as defined from edges)\n        full_boundary = []\n        left_boundary = []\n        right_boundary = []\n\n        for i in range(1, len(edges) - 1):\n            left_boundary.append(edges[i].points[0])\n            right_boundary.append(edges[i].points[-1])\n\n        full_boundary.extend(edges[0].points)\n        full_boundary.extend(right_boundary)\n        full_boundary.extend(edges[-1].points[::-1])\n        full_boundary.extend(left_boundary[::-1])\n\n        lons = [p.longitude for p in full_boundary]\n        lats = [p.latitude for p in full_boundary]\n        depths = [p.depth for p in full_boundary]\n\n        # define reference plane. Corner points are separated by an arbitrary\n        # distance of 10 km. The mesh spacing is set to 2 km. Both corner\n        # distance and mesh spacing values do not affect the algorithm results.\n        ul = edges[0].points[0]\n        strike = ul.azimuth(edges[0].points[-1])\n        dist = 10.\n\n        ur = ul.point_at(dist, 0, strike)\n        bl = Point(ul.longitude, ul.latitude, ul.depth + dist)\n        br = bl.point_at(dist, 0, strike)\n\n        # project surface boundary to reference plane and check for\n        # validity.\n        ref_plane = PlanarSurface.from_corner_points(ul, ur, br, bl)\n        _, xx, yy = ref_plane._project(\n            spherical_to_cartesian(lons, lats, depths))\n        coords = [(x, y) for x, y in zip(xx, yy)]\n        p = shapely.geometry.Polygon(coords)\n        if not p.is_valid:\n            raise ValueError('Edges points are not in the right order')", "response": "Check validity of the surface."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying the fault data and raise a helpful ValueError if any of the data is wrong.", "response": "def check_fault_data(cls, edges, mesh_spacing):\n        \"\"\"\n        Verify the fault data and raise ``ValueError`` if anything is wrong.\n\n        This method doesn't have to be called by hands before creating the\n        surface object, because it is called from :meth:`from_fault_data`.\n        \"\"\"\n        if not len(edges) >= 2:\n            raise ValueError(\"at least two edges are required\")\n        if not all(len(edge) >= 2 for edge in edges):\n            raise ValueError(\"at least two points must be defined \"\n                             \"in each edge\")\n        if not mesh_spacing > 0.0:\n            raise ValueError(\"mesh spacing must be positive\")\n\n        cls.check_surface_validity(edges)\n        cls.check_aki_richards_convention(edges)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_fault_data(cls, edges, mesh_spacing):\n        cls.check_fault_data(edges, mesh_spacing)\n        surface_nodes = [complex_fault_node(edges)]\n        mean_length = numpy.mean([edge.get_length() for edge in edges])\n        num_hor_points = int(round(mean_length / mesh_spacing)) + 1\n        if num_hor_points <= 1:\n            raise ValueError(\n                'mesh spacing %.1f km is too big for mean length %.1f km' %\n                (mesh_spacing, mean_length)\n            )\n        edges = [edge.resample_to_num_points(num_hor_points).points\n                 for i, edge in enumerate(edges)]\n\n        vert_edges = [Line(v_edge) for v_edge in zip(*edges)]\n        mean_width = numpy.mean([v_edge.get_length() for v_edge in vert_edges])\n        num_vert_points = int(round(mean_width / mesh_spacing)) + 1\n        if num_vert_points <= 1:\n            raise ValueError(\n                'mesh spacing %.1f km is too big for mean width %.1f km' %\n                (mesh_spacing, mean_width)\n            )\n\n        points = zip(*[v_edge.resample_to_num_points(num_vert_points).points\n                       for v_edge in vert_edges])\n        mesh = RectangularMesh.from_points_list(list(points))\n        assert 1 not in mesh.shape\n        self = cls(mesh)\n        self.surface_nodes = surface_nodes\n        return self", "response": "Create and return a complex fault surface using fault source data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef surface_projection_from_fault_data(cls, edges):\n        # collect lons and lats of all the vertices of all the edges\n        lons = []\n        lats = []\n        for edge in edges:\n            for point in edge:\n                lons.append(point.longitude)\n                lats.append(point.latitude)\n        lons = numpy.array(lons, dtype=float)\n        lats = numpy.array(lats, dtype=float)\n\n        return Mesh(lons, lats, depths=None).get_convex_hull()", "response": "Returns a surface projection of the complex fault surface."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfixes the unphysical probabilities of the non - zero probability of exceedence.", "response": "def fix_ones(pmap):\n    \"\"\"\n    Physically, an extremely small intensity measure level can have an\n    extremely large probability of exceedence, however that probability\n    cannot be exactly 1 unless the level is exactly 0. Numerically, the\n    PoE can be 1 and this give issues when calculating the damage (there\n    is a log(0) in\n    :class:`openquake.risklib.scientific.annual_frequency_of_exceedence`).\n    Here we solve the issue by replacing the unphysical probabilities 1\n    with .9999999999999999 (the float64 closest to 1).\n    \"\"\"\n    for sid in pmap:\n        array = pmap[sid].array\n        array[array == 1.] = .9999999999999999\n    return pmap"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_weights(realizations, imt_dt):\n    arr = numpy.zeros((len(realizations), len(imt_dt.names)))\n    for m, imt in enumerate(imt_dt.names):\n        arr[:, m] = [rlz.weight[imt] for rlz in realizations]\n    return arr", "response": "Builds the array of realization weights for the given realizations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the values of longarray to numpy. nan values.", "response": "def set_array(longarray, shortarray):\n    \"\"\"\n    :param longarray: a numpy array of floats of length L >= l\n    :param shortarray: a numpy array of floats of length l\n\n    Fill `longarray` with the values of `shortarray`, starting from the left.\n    If `shortarry` is shorter than `longarray`, then the remaining elements on\n    the right are filled with `numpy.nan` values.\n    \"\"\"\n    longarray[:len(shortarray)] = shortarray\n    longarray[len(shortarray):] = numpy.nan"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the time_event parameter in the datastore is in the exposure.", "response": "def check_time_event(oqparam, occupancy_periods):\n    \"\"\"\n    Check the `time_event` parameter in the datastore, by comparing\n    with the periods found in the exposure.\n    \"\"\"\n    time_event = oqparam.time_event\n    if time_event and time_event not in occupancy_periods:\n        raise ValueError(\n            'time_event is %s in %s, but the exposure contains %s' %\n            (time_event, oqparam.inputs['job_ini'],\n             ', '.join(occupancy_periods)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_hmaps(hcurves_by_kind, slice_, imtls, poes, monitor):\n    dic = {}\n    for kind, hcurves in hcurves_by_kind.items():\n        dic[kind] = calc.make_hmap_array(hcurves, imtls, poes, len(hcurves))\n    return dic, slice_", "response": "Builds hazard maps from a slice of hazard curves."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an array of shape N E M into a numpy array of type gmv_data_dt", "response": "def get_gmv_data(sids, gmfs, events):\n    \"\"\"\n    Convert an array of shape (N, E, M) into an array of type gmv_data_dt\n    \"\"\"\n    N, E, M = gmfs.shape\n    gmv_data_dt = numpy.dtype(\n        [('rlzi', U16), ('sid', U32), ('eid', U64), ('gmv', (F32, (M,)))])\n    # NB: ordering of the loops: first site, then event\n    lst = [(event['rlz'], sids[s], ei, gmfs[s, ei])\n           for s in numpy.arange(N, dtype=U32)\n           for ei, event in enumerate(events)]\n    return numpy.array(lst, gmv_data_dt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the GMFs of the current object to the datastore.", "response": "def save_gmfs(calculator):\n    \"\"\"\n    :param calculator: a scenario_risk/damage or event_based_risk calculator\n    :returns: a pair (eids, R) where R is the number of realizations\n    \"\"\"\n    dstore = calculator.datastore\n    oq = calculator.oqparam\n    logging.info('Reading gmfs from file')\n    if oq.inputs['gmfs'].endswith('.csv'):\n        # TODO: check if import_gmfs can be removed\n        eids = import_gmfs(\n            dstore, oq.inputs['gmfs'], calculator.sitecol.complete.sids)\n    else:  # XML\n        eids, gmfs = readinput.eids, readinput.gmfs\n    E = len(eids)\n    events = numpy.zeros(E, rupture.events_dt)\n    events['eid'] = eids\n    calculator.eids = eids\n    if hasattr(oq, 'number_of_ground_motion_fields'):\n        if oq.number_of_ground_motion_fields != E:\n            raise RuntimeError(\n                'Expected %d ground motion fields, found %d' %\n                (oq.number_of_ground_motion_fields, E))\n    else:  # set the number of GMFs from the file\n        oq.number_of_ground_motion_fields = E\n    # NB: save_gmfs redefine oq.sites in case of GMFs from XML or CSV\n    if oq.inputs['gmfs'].endswith('.xml'):\n        haz_sitecol = readinput.get_site_collection(oq)\n        N, E, M = gmfs.shape\n        save_gmf_data(dstore, haz_sitecol, gmfs[haz_sitecol.sids],\n                      oq.imtls, events)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the data for the current GMFs and the IMTs in the GMFs.", "response": "def save_gmf_data(dstore, sitecol, gmfs, imts, events=()):\n    \"\"\"\n    :param dstore: a :class:`openquake.baselib.datastore.DataStore` instance\n    :param sitecol: a :class:`openquake.hazardlib.site.SiteCollection` instance\n    :param gmfs: an array of shape (N, E, M)\n    :param imts: a list of IMT strings\n    :param events: E event IDs or the empty tuple\n    \"\"\"\n    if len(events) == 0:\n        E = gmfs.shape[1]\n        events = numpy.zeros(E, rupture.events_dt)\n        events['eid'] = numpy.arange(E, dtype=U64)\n    dstore['events'] = events\n    offset = 0\n    gmfa = get_gmv_data(sitecol.sids, gmfs, events)\n    dstore['gmf_data/data'] = gmfa\n    dic = general.group_array(gmfa, 'sid')\n    lst = []\n    all_sids = sitecol.complete.sids\n    for sid in all_sids:\n        rows = dic.get(sid, ())\n        n = len(rows)\n        lst.append((offset, offset + n))\n        offset += n\n    dstore['gmf_data/imts'] = ' '.join(imts)\n    dstore['gmf_data/indices'] = numpy.array(lst, U32)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_idxs(data, eid2idx):\n    uniq, inv = numpy.unique(data['eid'], return_inverse=True)\n    idxs = numpy.array([eid2idx[eid] for eid in uniq])[inv]\n    return idxs", "response": "Convert from event IDs to event indices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport in the datastore a ground motion field CSV file.", "response": "def import_gmfs(dstore, fname, sids):\n    \"\"\"\n    Import in the datastore a ground motion field CSV file.\n\n    :param dstore: the datastore\n    :param fname: the CSV file\n    :param sids: the site IDs (complete)\n    :returns: event_ids, num_rlzs\n    \"\"\"\n    array = writers.read_composite_array(fname).array\n    # has header rlzi, sid, eid, gmv_PGA, ...\n    imts = [name[4:] for name in array.dtype.names[3:]]\n    n_imts = len(imts)\n    gmf_data_dt = numpy.dtype(\n        [('rlzi', U16), ('sid', U32), ('eid', U64), ('gmv', (F32, (n_imts,)))])\n    # store the events\n    eids = numpy.unique(array['eid'])\n    eids.sort()\n    E = len(eids)\n    eid2idx = dict(zip(eids, range(E)))\n    events = numpy.zeros(E, rupture.events_dt)\n    events['eid'] = eids\n    dstore['events'] = events\n    # store the GMFs\n    dic = general.group_array(array.view(gmf_data_dt), 'sid')\n    lst = []\n    offset = 0\n    for sid in sids:\n        n = len(dic.get(sid, []))\n        lst.append((offset, offset + n))\n        if n:\n            offset += n\n            gmvs = dic[sid]\n            gmvs['eid'] = get_idxs(gmvs, eid2idx)\n            gmvs['rlzi'] = 0   # effectively there is only 1 realization\n            dstore.extend('gmf_data/data', gmvs)\n    dstore['gmf_data/indices'] = numpy.array(lst, U32)\n    dstore['gmf_data/imts'] = ' '.join(imts)\n    sig_eps_dt = [('eid', U64), ('sig', (F32, n_imts)), ('eps', (F32, n_imts))]\n    dstore['gmf_data/sigma_epsilon'] = numpy.zeros(0, sig_eps_dt)\n    dstore['weights'] = numpy.ones((1, n_imts))\n    return eids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef monitor(self, operation='', **kw):\n        mon = self._monitor(operation, hdf5=self.datastore.hdf5)\n        self._monitor.calc_id = mon.calc_id = self.datastore.calc_id\n        vars(mon).update(kw)\n        return mon", "response": "Returns a new Monitor instance with the given operation and update the current calc_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the current calculation parameters and save the engine_version", "response": "def save_params(self, **kw):\n        \"\"\"\n        Update the current calculation parameters and save engine_version\n        \"\"\"\n        if ('hazard_calculation_id' in kw and\n                kw['hazard_calculation_id'] is None):\n            del kw['hazard_calculation_id']\n        vars(self.oqparam).update(**kw)\n        self.datastore['oqparam'] = self.oqparam  # save the updated oqparam\n        attrs = self.datastore['/'].attrs\n        attrs['engine_version'] = engine_version\n        attrs['date'] = datetime.now().isoformat()[:19]\n        if 'checksum32' not in attrs:\n            attrs['checksum32'] = readinput.get_checksum32(self.oqparam)\n        self.datastore.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_precalc(self, precalc_mode):\n        calc_mode = self.oqparam.calculation_mode\n        ok_mode = self.accept_precalc\n        if calc_mode != precalc_mode and precalc_mode not in ok_mode:\n            raise InvalidCalculationID(\n                'In order to run a calculation of kind %r, '\n                'you need to provide a calculation of kind %r, '\n                'but you provided a %r instead' %\n                (calc_mode, ok_mode, precalc_mode))", "response": "Check that the user has provided a pre - calculation ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, pre_execute=True, concurrent_tasks=None, close=True, **kw):\n        with self._monitor:\n            self._monitor.username = kw.get('username', '')\n            self._monitor.hdf5 = self.datastore.hdf5\n            if concurrent_tasks is None:  # use the job.ini parameter\n                ct = self.oqparam.concurrent_tasks\n            else:  # used the parameter passed in the command-line\n                ct = concurrent_tasks\n            if ct == 0:  # disable distribution temporarily\n                oq_distribute = os.environ.get('OQ_DISTRIBUTE')\n                os.environ['OQ_DISTRIBUTE'] = 'no'\n            if ct != self.oqparam.concurrent_tasks:\n                # save the used concurrent_tasks\n                self.oqparam.concurrent_tasks = ct\n            self.save_params(**kw)\n            try:\n                if pre_execute:\n                    self.pre_execute()\n                self.result = self.execute()\n                if self.result is not None:\n                    self.post_execute(self.result)\n                self.before_export()\n                self.export(kw.get('exports', ''))\n            except Exception:\n                if kw.get('pdb'):  # post-mortem debug\n                    tb = sys.exc_info()[2]\n                    traceback.print_tb(tb)\n                    pdb.post_mortem(tb)\n                else:\n                    logging.critical('', exc_info=True)\n                    raise\n            finally:\n                # cleanup globals\n                if ct == 0:  # restore OQ_DISTRIBUTE\n                    if oq_distribute is None:  # was not set\n                        del os.environ['OQ_DISTRIBUTE']\n                    else:\n                        os.environ['OQ_DISTRIBUTE'] = oq_distribute\n                readinput.pmap = None\n                readinput.exposure = None\n                readinput.gmfs = None\n                readinput.eids = None\n                self._monitor.flush()\n\n                if close:  # in the engine we close later\n                    self.result = None\n                    try:\n                        self.datastore.close()\n                    except (RuntimeError, ValueError):\n                        # sometimes produces errors but they are difficult to\n                        # reproduce\n                        logging.warning('', exc_info=True)\n\n        return getattr(self, 'exported', {})", "response": "Runs the calculation and returns the exported outputs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, exports=None):\n        self.exported = getattr(self.precalc, 'exported', {})\n        if isinstance(exports, tuple):\n            fmts = exports\n        elif exports:  # is a string\n            fmts = exports.split(',')\n        elif isinstance(self.oqparam.exports, tuple):\n            fmts = self.oqparam.exports\n        else:  # is a string\n            fmts = self.oqparam.exports.split(',')\n        keys = set(self.datastore)\n        has_hcurves = ('hcurves-stats' in self.datastore or\n                       'hcurves-rlzs' in self.datastore)\n        if has_hcurves:\n            keys.add('hcurves')\n        for fmt in fmts:\n            if not fmt:\n                continue\n            for key in sorted(keys):  # top level keys\n                if 'rlzs' in key and self.R > 1:\n                    continue  # skip individual curves\n                self._export((key, fmt))\n            if has_hcurves and self.oqparam.hazard_maps:\n                self._export(('hmaps', fmt))\n            if has_hcurves and self.oqparam.uniform_hazard_spectra:\n                self._export(('uhs', fmt))", "response": "Export all the outputs in the datastore in the given export formats."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef before_export(self):\n        # sanity check that eff_ruptures have been set, i.e. are not -1\n        try:\n            csm_info = self.datastore['csm_info']\n        except KeyError:\n            csm_info = self.datastore['csm_info'] = self.csm.info\n        for sm in csm_info.source_models:\n            for sg in sm.src_groups:\n                assert sg.eff_ruptures != -1, sg\n\n        for key in self.datastore:\n            self.datastore.set_nbytes(key)\n        self.datastore.flush()", "response": "Set the attributes nbytes\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef src_filter(self):\n        oq = self.oqparam\n        self.hdf5cache = self.datastore.hdf5cache()\n        sitecol = self.sitecol.complete if self.sitecol else None\n        if 'ucerf' in oq.calculation_mode:\n            return UcerfFilter(sitecol, oq.maximum_distance, self.hdf5cache)\n        return SourceFilter(sitecol, oq.maximum_distance, self.hdf5cache)", "response": "returns a SourceFilter or UcerfFilter depending on oq. calculation_mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an instance of the RtreeFilter class", "response": "def rtree_filter(self):\n        \"\"\"\n        :returns: an RtreeFilter\n        \"\"\"\n        return RtreeFilter(self.src_filter.sitecol,\n                           self.oqparam.maximum_distance,\n                           self.src_filter.filename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the total number of sites in the current object.", "response": "def N(self):\n        \"\"\"\n        :returns: the total number of sites\n        \"\"\"\n        if hasattr(self, 'sitecol'):\n            return len(self.sitecol.complete) if self.sitecol else None\n        return len(self.datastore['sitecol/array'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_inputs(self):\n        oq = self.oqparam\n        self._read_risk_data()\n        self.check_overflow()  # check if self.sitecol is too large\n        if ('source_model_logic_tree' in oq.inputs and\n                oq.hazard_calculation_id is None):\n            self.csm = readinput.get_composite_source_model(\n                oq, self.monitor(), srcfilter=self.src_filter)\n        self.init()", "response": "Read risk data and sources if any\n        is set to True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the hazard curves and the hazard variables from the nrml file.", "response": "def pre_execute(self):\n        \"\"\"\n        Check if there is a previous calculation ID.\n        If yes, read the inputs by retrieving the previous calculation;\n        if not, read the inputs directly.\n        \"\"\"\n        oq = self.oqparam\n        if 'gmfs' in oq.inputs or 'multi_peril' in oq.inputs:\n            # read hazard from files\n            assert not oq.hazard_calculation_id, (\n                'You cannot use --hc together with gmfs_file')\n            self.read_inputs()\n            if 'gmfs' in oq.inputs:\n                save_gmfs(self)\n            else:\n                self.save_multi_peril()\n        elif 'hazard_curves' in oq.inputs:  # read hazard from file\n            assert not oq.hazard_calculation_id, (\n                'You cannot use --hc together with hazard_curves')\n            haz_sitecol = readinput.get_site_collection(oq)\n            # NB: horrible: get_site_collection calls get_pmap_from_nrml\n            # that sets oq.investigation_time, so it must be called first\n            self.load_riskmodel()  # must be after get_site_collection\n            self.read_exposure(haz_sitecol)  # define .assets_by_site\n            self.datastore['poes/grp-00'] = fix_ones(readinput.pmap)\n            self.datastore['sitecol'] = self.sitecol\n            self.datastore['assetcol'] = self.assetcol\n            self.datastore['csm_info'] = fake = source.CompositionInfo.fake()\n            self.rlzs_assoc = fake.get_rlzs_assoc()\n        elif oq.hazard_calculation_id:\n            parent = util.read(oq.hazard_calculation_id)\n            self.check_precalc(parent['oqparam'].calculation_mode)\n            self.datastore.parent = parent\n            # copy missing parameters from the parent\n            params = {name: value for name, value in\n                      vars(parent['oqparam']).items()\n                      if name not in vars(self.oqparam)}\n            self.save_params(**params)\n            self.read_inputs()\n            oqp = parent['oqparam']\n            if oqp.investigation_time != oq.investigation_time:\n                raise ValueError(\n                    'The parent calculation was using investigation_time=%s'\n                    ' != %s' % (oqp.investigation_time, oq.investigation_time))\n            if oqp.minimum_intensity != oq.minimum_intensity:\n                raise ValueError(\n                    'The parent calculation was using minimum_intensity=%s'\n                    ' != %s' % (oqp.minimum_intensity, oq.minimum_intensity))\n            missing_imts = set(oq.risk_imtls) - set(oqp.imtls)\n            if missing_imts:\n                raise ValueError(\n                    'The parent calculation is missing the IMT(s) %s' %\n                    ', '.join(missing_imts))\n        elif self.__class__.precalc:\n            calc = calculators[self.__class__.precalc](\n                self.oqparam, self.datastore.calc_id)\n            calc.run()\n            self.param = calc.param\n            self.sitecol = calc.sitecol\n            self.assetcol = calc.assetcol\n            self.riskmodel = calc.riskmodel\n            if hasattr(calc, 'rlzs_assoc'):\n                self.rlzs_assoc = calc.rlzs_assoc\n            else:\n                # this happens for instance for a scenario_damage without\n                # rupture, gmfs, multi_peril\n                raise InvalidFile(\n                    '%(job_ini)s: missing gmfs_csv, multi_peril_csv' %\n                    oq.inputs)\n            if hasattr(calc, 'csm'):  # no scenario\n                self.csm = calc.csm\n        else:\n            self.read_inputs()\n        if self.riskmodel:\n            self.save_riskmodel()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the object with the correct attributes.", "response": "def init(self):\n        \"\"\"\n        To be overridden to initialize the datasets needed by the calculation\n        \"\"\"\n        oq = self.oqparam\n        if not oq.risk_imtls:\n            if self.datastore.parent:\n                oq.risk_imtls = (\n                    self.datastore.parent['oqparam'].risk_imtls)\n        if 'precalc' in vars(self):\n            self.rlzs_assoc = self.precalc.rlzs_assoc\n        elif 'csm_info' in self.datastore:\n            csm_info = self.datastore['csm_info']\n            if oq.hazard_calculation_id and 'gsim_logic_tree' in oq.inputs:\n                # redefine the realizations by reading the weights from the\n                # gsim_logic_tree_file that could be different from the parent\n                csm_info.gsim_lt = logictree.GsimLogicTree(\n                    oq.inputs['gsim_logic_tree'], set(csm_info.trts))\n            self.rlzs_assoc = csm_info.get_rlzs_assoc()\n        elif hasattr(self, 'csm'):\n            self.check_floating_spinning()\n            self.rlzs_assoc = self.csm.info.get_rlzs_assoc()\n        else:  # build a fake; used by risk-from-file calculators\n            self.datastore['csm_info'] = fake = source.CompositionInfo.fake()\n            self.rlzs_assoc = fake.get_rlzs_assoc()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef R(self):\n        try:\n            return self.csm.info.get_num_rlzs()\n        except AttributeError:  # no self.csm\n            return self.datastore['csm_info'].get_num_rlzs()", "response": "returns the number of realizations in the CSM"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the exposure and the riskmodel and update the attributes of the object with the relevant attributes.", "response": "def read_exposure(self, haz_sitecol=None):  # after load_risk_model\n        \"\"\"\n        Read the exposure, the riskmodel and update the attributes\n        .sitecol, .assetcol\n        \"\"\"\n        with self.monitor('reading exposure', autoflush=True):\n            self.sitecol, self.assetcol, discarded = (\n                readinput.get_sitecol_assetcol(\n                    self.oqparam, haz_sitecol, self.riskmodel.loss_types))\n            if len(discarded):\n                self.datastore['discarded'] = discarded\n                if hasattr(self, 'rup'):\n                    # this is normal for the case of scenario from rupture\n                    logging.info('%d assets were discarded because too far '\n                                 'from the rupture; use `oq show discarded` '\n                                 'to show them and `oq plot_assets` to plot '\n                                 'them' % len(discarded))\n                elif not self.oqparam.discard_assets:  # raise an error\n                    self.datastore['sitecol'] = self.sitecol\n                    self.datastore['assetcol'] = self.assetcol\n                    raise RuntimeError(\n                        '%d assets were discarded; use `oq show discarded` to'\n                        ' show them and `oq plot_assets` to plot them' %\n                        len(discarded))\n\n        # reduce the riskmodel to the relevant taxonomies\n        taxonomies = set(taxo for taxo in self.assetcol.tagcol.taxonomy\n                         if taxo != '?')\n        if len(self.riskmodel.taxonomies) > len(taxonomies):\n            logging.info('Reducing risk model from %d to %d taxonomies',\n                         len(self.riskmodel.taxonomies), len(taxonomies))\n            self.riskmodel = self.riskmodel.reduce(taxonomies)\n        return readinput.exposure"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_riskmodel(self):\n        # to be called before read_exposure\n        # NB: this is called even if there is no risk model\n        \"\"\"\n        Read the risk model and set the attribute .riskmodel.\n        The riskmodel can be empty for hazard calculations.\n        Save the loss ratios (if any) in the datastore.\n        \"\"\"\n        logging.info('Reading the risk model if present')\n        self.riskmodel = readinput.get_risk_model(self.oqparam)\n        if not self.riskmodel:\n            parent = self.datastore.parent\n            if 'risk_model' in parent:\n                self.riskmodel = riskinput.CompositeRiskModel.read(parent)\n            return\n        if self.oqparam.ground_motion_fields and not self.oqparam.imtls:\n            raise InvalidFile('No intensity_measure_types specified in %s' %\n                              self.oqparam.inputs['job_ini'])\n        self.save_params()", "response": "Load the risk model and set the attribute. riskmodel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the risk models in the datastore", "response": "def save_riskmodel(self):\n        \"\"\"\n        Save the risk models in the datastore\n        \"\"\"\n        self.datastore['risk_model'] = rm = self.riskmodel\n        self.datastore['taxonomy_mapping'] = self.riskmodel.tmap\n        attrs = self.datastore.getitem('risk_model').attrs\n        attrs['min_iml'] = hdf5.array_of_vstr(sorted(rm.min_iml.items()))\n        self.datastore.set_nbytes('risk_model')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving info about the composite source model inside the csm_info dataset", "response": "def store_rlz_info(self, eff_ruptures=None):\n        \"\"\"\n        Save info about the composite source model inside the csm_info dataset\n        \"\"\"\n        if hasattr(self, 'csm'):  # no scenario\n            self.csm.info.update_eff_ruptures(eff_ruptures)\n            self.rlzs_assoc = self.csm.info.get_rlzs_assoc(\n                self.oqparam.sm_lt_path)\n            if not self.rlzs_assoc:\n                raise RuntimeError('Empty logic tree: too much filtering?')\n            self.datastore['csm_info'] = self.csm.info\n        R = len(self.rlzs_assoc.realizations)\n        logging.info('There are %d realization(s)', R)\n        if self.oqparam.imtls:\n            self.datastore['weights'] = arr = build_weights(\n                self.rlzs_assoc.realizations, self.oqparam.imt_dt())\n            self.datastore.set_attrs('weights', nbytes=arr.nbytes)\n            if hasattr(self, 'hdf5cache'):  # no scenario\n                with hdf5.File(self.hdf5cache, 'r+') as cache:\n                    cache['weights'] = arr\n        if 'event_based' in self.oqparam.calculation_mode and R >= TWO16:\n            # rlzi is 16 bit integer in the GMFs, so there is hard limit or R\n            raise ValueError(\n                'The logic tree has %d realizations, the maximum '\n                'is %d' % (R, TWO16))\n        elif R > 10000:\n            logging.warning(\n                'The logic tree has %d realizations(!), please consider '\n                'sampling it', R)\n        self.datastore.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_source_info(self, calc_times):\n        if calc_times:\n            source_info = self.datastore['source_info']\n            arr = numpy.zeros((len(source_info), 3), F32)\n            ids, vals = zip(*sorted(calc_times.items()))\n            arr[numpy.array(ids)] = vals\n            source_info['weight'] += arr[:, 0]\n            source_info['num_sites'] += arr[:, 1]\n            source_info['calc_time'] += arr[:, 2]", "response": "Save the weight num_sites calc_time in the source_info dataset"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_shakemap(self, haz_sitecol, assetcol):\n        oq = self.oqparam\n        E = oq.number_of_ground_motion_fields\n        oq.risk_imtls = oq.imtls or self.datastore.parent['oqparam'].imtls\n        extra = self.riskmodel.get_extra_imts(oq.risk_imtls)\n        if extra:\n            logging.warning('There are risk functions for not available IMTs '\n                            'which will be ignored: %s' % extra)\n\n        logging.info('Getting/reducing shakemap')\n        with self.monitor('getting/reducing shakemap'):\n            smap = oq.shakemap_id if oq.shakemap_id else numpy.load(\n                oq.inputs['shakemap'])\n            sitecol, shakemap, discarded = get_sitecol_shakemap(\n                smap, oq.imtls, haz_sitecol,\n                oq.asset_hazard_distance['default'],\n                oq.discard_assets)\n            if len(discarded):\n                self.datastore['discarded'] = discarded\n            assetcol = assetcol.reduce_also(sitecol)\n\n        logging.info('Building GMFs')\n        with self.monitor('building/saving GMFs'):\n            imts, gmfs = to_gmfs(\n                shakemap, oq.spatial_correlation, oq.cross_correlation,\n                oq.site_effects, oq.truncation_level, E, oq.random_seed,\n                oq.imtls)\n            save_gmf_data(self.datastore, sitecol, gmfs, imts)\n        return sitecol, assetcol", "response": "Read the USGS shakemap files and return a set of GMFs and a set of discarded sites."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a list of RiskInputs objects for the given type of realization.", "response": "def build_riskinputs(self, kind):\n        \"\"\"\n        :param kind:\n            kind of hazard getter, can be 'poe' or 'gmf'\n        :returns:\n            a list of RiskInputs objects, sorted by IMT.\n        \"\"\"\n        logging.info('Building risk inputs from %d realization(s)', self.R)\n        imtls = self.oqparam.imtls\n        if not set(self.oqparam.risk_imtls) & set(imtls):\n            rsk = ', '.join(self.oqparam.risk_imtls)\n            haz = ', '.join(imtls)\n            raise ValueError('The IMTs in the risk models (%s) are disjoint '\n                             \"from the IMTs in the hazard (%s)\" % (rsk, haz))\n        self.riskmodel.taxonomy = self.assetcol.tagcol.taxonomy\n        with self.monitor('building riskinputs', autoflush=True):\n            riskinputs = list(self._gen_riskinputs(kind))\n        assert riskinputs\n        logging.info('Built %d risk inputs', len(riskinputs))\n        return riskinputs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a PmapGetter or GmfDataGetter object for the given site ID.", "response": "def get_getter(self, kind, sid):\n        \"\"\"\n        :param kind: 'poe' or 'gmf'\n        :param sid: a site ID\n        :returns: a PmapGetter or GmfDataGetter\n        \"\"\"\n        hdf5cache = getattr(self, 'hdf5cache', None)\n        if hdf5cache:\n            dstore = hdf5cache\n        elif (self.oqparam.hazard_calculation_id and\n              'gmf_data' not in self.datastore):\n            # 'gmf_data' in self.datastore happens for ShakeMap calculations\n            self.datastore.parent.close()  # make sure it is closed\n            dstore = self.datastore.parent\n        else:\n            dstore = self.datastore\n        if kind == 'poe':  # hcurves, shape (R, N)\n            getter = getters.PmapGetter(dstore, self.rlzs_assoc, [sid])\n        else:  # gmf\n            getter = getters.GmfDataGetter(dstore, [sid], self.R)\n        if dstore is self.datastore:\n            getter.init()\n        return getter"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self):\n        if not hasattr(self, 'riskinputs'):  # in the reportwriter\n            return\n        res = Starmap.apply(\n            self.core_task.__func__,\n            (self.riskinputs, self.riskmodel, self.param, self.monitor()),\n            concurrent_tasks=self.oqparam.concurrent_tasks or 1,\n            weight=get_weight\n        ).reduce(self.combine)\n        return res", "response": "Execute the core task."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind(end_point, socket_type):\n    sock = context.socket(socket_type)\n    try:\n        sock.bind(end_point)\n    except zmq.error.ZMQError as exc:\n        sock.close()\n        raise exc.__class__('%s: %s' % (exc, end_point))\n    return sock", "response": "Bind to a zmq socket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend an object to the remote server ; block and return the reply", "response": "def send(self, obj):\n        \"\"\"\n        Send an object to the remote server; block and return the reply\n        if the socket type is REQ.\n\n        :param obj:\n            the Python object to send\n        \"\"\"\n        self.zsocket.send_pyobj(obj)\n        self.num_sent += 1\n        if self.socket_type == zmq.REQ:\n            return self.zsocket.recv_pyobj()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the angular distance of two points at the given latitude.", "response": "def angular_distance(km, lat, lat2=None):\n    \"\"\"\n    Return the angular distance of two points at the given latitude.\n\n    >>> '%.3f' % angular_distance(100, lat=40)\n    '1.174'\n    >>> '%.3f' % angular_distance(100, lat=80)\n    '5.179'\n    \"\"\"\n    if lat2 is not None:\n        # use the largest latitude to compute the angular distance\n        lat = max(abs(lat), abs(lat2))\n    return km * KM_TO_DEGREES / math.cos(lat * DEGREES_TO_RAD)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assoc(objects, sitecol, assoc_dist, mode, asset_refs=()):\n    if isinstance(objects, numpy.ndarray) or hasattr(objects, 'lons'):\n        # objects is a geo array with lon, lat fields or a mesh-like instance\n        return _GeographicObjects(objects).assoc(sitecol, assoc_dist, mode)\n    else:  # objects is the list assets_by_site\n        return _GeographicObjects(sitecol).assoc2(\n            objects, assoc_dist, mode, asset_refs)", "response": "Associate geographic objects to a site collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_points(points):\n    if not points:\n        return points\n\n    result = [points[0]]\n    for point in points:\n        if point != result[-1]:\n            result.append(point)\n    return result", "response": "Given a list of points return a new list with adjacent duplicate points removed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if line of points intersecting itself.", "response": "def line_intersects_itself(lons, lats, closed_shape=False):\n    \"\"\"\n    Return ``True`` if line of points intersects itself.\n    Line with the last point repeating the first one considered\n    intersecting itself.\n\n    The line is defined by lists (or numpy arrays) of points'\n    longitudes and latitudes (depth is not taken into account).\n\n    :param closed_shape:\n        If ``True`` the line will be checked twice: first time with\n        its original shape and second time with the points sequence\n        being shifted by one point (the last point becomes first,\n        the first turns second and so on). This is useful for\n        checking that the sequence of points defines a valid\n        :class:`~openquake.hazardlib.geo.polygon.Polygon`.\n    \"\"\"\n    assert len(lons) == len(lats)\n\n    if len(lons) <= 3:\n        # line can not intersect itself unless there are\n        # at least four points\n        return False\n\n    west, east, north, south = get_spherical_bounding_box(lons, lats)\n    proj = OrthographicProjection(west, east, north, south)\n\n    xx, yy = proj(lons, lats)\n    if not shapely.geometry.LineString(list(zip(xx, yy))).is_simple:\n        return True\n\n    if closed_shape:\n        xx, yy = proj(numpy.roll(lons, 1), numpy.roll(lats, 1))\n        if not shapely.geometry.LineString(list(zip(xx, yy))).is_simple:\n            return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_bounding_box(obj, maxdist):\n    if hasattr(obj, 'get_bounding_box'):\n        return obj.get_bounding_box(maxdist)\n    elif hasattr(obj, 'polygon'):\n        bbox = obj.polygon.get_bbox()\n    else:\n        if isinstance(obj, list):  # a list of locations\n            lons = numpy.array([loc.longitude for loc in obj])\n            lats = numpy.array([loc.latitude for loc in obj])\n        else:  # assume an array with fields lon, lat\n            lons, lats = obj['lon'], obj['lat']\n        min_lon, max_lon = lons.min(), lons.max()\n        if cross_idl(min_lon, max_lon):\n            lons %= 360\n        bbox = lons.min(), lats.min(), lons.max(), lats.max()\n    a1 = min(maxdist * KM_TO_DEGREES, 90)\n    a2 = min(angular_distance(maxdist, bbox[1], bbox[3]), 180)\n    return bbox[0] - a2, bbox[1] - a1, bbox[2] + a2, bbox[3] + a1", "response": "Returns the dilated bounding box of a geometric object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a list of lon lat values find and return the bounding box of a single hemisphere containing the points.", "response": "def get_spherical_bounding_box(lons, lats):\n    \"\"\"\n    Given a collection of points find and return the bounding box,\n    as a pair of longitudes and a pair of latitudes.\n\n    Parameters define longitudes and latitudes of a point collection\n    respectively in a form of lists or numpy arrays.\n\n    :return:\n        A tuple of four items. These items represent western, eastern,\n        northern and southern borders of the bounding box respectively.\n        Values are floats in decimal degrees.\n    :raises ValueError:\n        If points collection has the longitudinal extent of more than\n        180 degrees (it is impossible to define a single hemisphere\n        bound to poles that would contain the whole collection).\n    \"\"\"\n    north, south = numpy.max(lats), numpy.min(lats)\n    west, east = numpy.min(lons), numpy.max(lons)\n    assert (-180 <= west <= 180) and (-180 <= east <= 180), (west, east)\n    if get_longitudinal_extent(west, east) < 0:\n        # points are lying on both sides of the international date line\n        # (meridian 180). the actual west longitude is the lowest positive\n        # longitude and east one is the highest negative.\n        if hasattr(lons, 'flatten'):\n            # fixes test_surface_crossing_international_date_line\n            lons = lons.flatten()\n        west = min(lon for lon in lons if lon > 0)\n        east = max(lon for lon in lons if lon < 0)\n        if not all((get_longitudinal_extent(west, lon) >= 0\n                    and get_longitudinal_extent(lon, east) >= 0)\n                   for lon in lons):\n            raise ValueError('points collection has longitudinal extent '\n                             'wider than 180 deg')\n    return SphericalBB(west, east, north, south)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_middle_point(lon1, lat1, lon2, lat2):\n    if lon1 == lon2 and lat1 == lat2:\n        return lon1, lat1\n    dist = geodetic.geodetic_distance(lon1, lat1, lon2, lat2)\n    azimuth = geodetic.azimuth(lon1, lat1, lon2, lat2)\n    return geodetic.point_at(lon1, lat1, azimuth, dist / 2.0)", "response": "Returns the point in the middle of the two points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the spherical coordinates for coordinates in Cartesian space.", "response": "def cartesian_to_spherical(vectors):\n    \"\"\"\n    Return the spherical coordinates for coordinates in Cartesian space.\n\n    This function does an opposite to :func:`spherical_to_cartesian`.\n\n    :param vectors:\n        Array of 3d vectors in Cartesian space of shape (..., 3)\n    :returns:\n        Tuple of three arrays of the same shape as ``vectors`` representing\n        longitude (decimal degrees), latitude (decimal degrees) and depth (km)\n        in specified order.\n    \"\"\"\n    rr = numpy.sqrt(numpy.sum(vectors * vectors, axis=-1))\n    xx, yy, zz = vectors.T\n    lats = numpy.degrees(numpy.arcsin((zz / rr).clip(-1., 1.)))\n    lons = numpy.degrees(numpy.arctan2(yy, xx))\n    depths = EARTH_RADIUS - rr\n    return lons.T, lats.T, depths"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the area of a triangle formed by three vectors.", "response": "def triangle_area(e1, e2, e3):\n    \"\"\"\n    Get the area of triangle formed by three vectors.\n\n    Parameters are three three-dimensional numpy arrays representing\n    vectors of triangle's edges in Cartesian space.\n\n    :returns:\n        Float number, the area of the triangle in squared units of coordinates,\n        or numpy array of shape of edges with one dimension less.\n\n    Uses Heron formula, see http://mathworld.wolfram.com/HeronsFormula.html.\n    \"\"\"\n    # calculating edges length\n    e1_length = numpy.sqrt(numpy.sum(e1 * e1, axis=-1))\n    e2_length = numpy.sqrt(numpy.sum(e2 * e2, axis=-1))\n    e3_length = numpy.sqrt(numpy.sum(e3 * e3, axis=-1))\n    # calculating half perimeter\n    s = (e1_length + e2_length + e3_length) / 2.0\n    # applying Heron's formula\n    return numpy.sqrt(s * (s - e1_length) * (s - e2_length) * (s - e3_length))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting unit vector for a given one.", "response": "def normalized(vector):\n    \"\"\"\n    Get unit vector for a given one.\n\n    :param vector:\n        Numpy vector as coordinates in Cartesian space, or an array of such.\n    :returns:\n        Numpy array of the same shape and structure where all vectors are\n        normalized. That is, each coordinate component is divided by its\n        vector's length.\n    \"\"\"\n    length = numpy.sum(vector * vector, axis=-1)\n    length = numpy.sqrt(length.reshape(length.shape + (1, )))\n    return vector / length"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the distance to polygon for each point in the collection.", "response": "def point_to_polygon_distance(polygon, pxx, pyy):\n    \"\"\"\n    Calculate the distance to polygon for each point of the collection\n    on the 2d Cartesian plane.\n\n    :param polygon:\n        Shapely \"Polygon\" geometry object.\n    :param pxx:\n        List or numpy array of abscissae values of points to calculate\n        the distance from.\n    :param pyy:\n        Same structure as ``pxx``, but with ordinate values.\n    :returns:\n        Numpy array of distances in units of coordinate system. Points\n        that lie inside the polygon have zero distance.\n    \"\"\"\n    pxx = numpy.array(pxx)\n    pyy = numpy.array(pyy)\n    assert pxx.shape == pyy.shape\n    if pxx.ndim == 0:\n        pxx = pxx.reshape((1, ))\n        pyy = pyy.reshape((1, ))\n    result = numpy.array([\n        polygon.distance(shapely.geometry.Point(pxx.item(i), pyy.item(i)))\n        for i in range(pxx.size)\n    ])\n    return result.reshape(pxx.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cross_idl(lon1, lon2, *lons):\n    lons = (lon1, lon2) + lons\n    l1, l2 = min(lons), max(lons)\n    # a line crosses the international date line if the end positions\n    # have different sign and they are more than 180 degrees longitude apart\n    return l1 * l2 < 0 and abs(l1 - l2) > 180", "response": "Return True if two longitude values define line crossing international date line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_lons(l1, l2):\n    if l1 > l2:  # exchange lons\n        l1, l2 = l2, l1\n    delta = l2 - l1\n    if l1 < 0 and l2 > 0 and delta > 180:\n        return [(-180, l1), (l2, 180)]\n    elif l1 > 0 and l2 > 180 and delta < 180:\n        return [(l1, 180), (-180, l2 - 360)]\n    elif l1 < -180 and l2 < 0 and delta < 180:\n        return [(l1 + 360, 180), (l2, -180)]\n    return [(l1, l2)]", "response": "Returns a list of lists of longitudes that are in the same range as l1 and l2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef within(bbox, lonlat_index):\n    lon1, lat1, lon2, lat2 = bbox\n    set_ = set()\n    for l1, l2 in normalize_lons(lon1, lon2):\n        box = (l1, lat1, l2, lat2)\n        set_ |= set(lonlat_index.intersection(box))\n    return numpy.array(sorted(set_), numpy.uint32)", "response": "returns a list of indices within a bounding box"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_closest(self, lon, lat, depth=0):\n        xyz = spherical_to_cartesian(lon, lat, depth)\n        min_dist, idx = self.kdtree.query(xyz)\n        return self.objects[idx], min_dist", "response": "Get the closest object to the given longitude and latitude."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assoc(self, sitecol, assoc_dist, mode):\n        assert mode in 'strict warn filter', mode\n        dic = {}\n        discarded = []\n        for sid, lon, lat in zip(sitecol.sids, sitecol.lons, sitecol.lats):\n            obj, distance = self.get_closest(lon, lat)\n            if assoc_dist is None:\n                dic[sid] = obj  # associate all\n            elif distance <= assoc_dist:\n                dic[sid] = obj  # associate within\n            elif mode == 'warn':\n                dic[sid] = obj  # associate outside\n                logging.warning(\n                    'The closest vs30 site (%.1f %.1f) is distant more than %d'\n                    ' km from site #%d (%.1f %.1f)', obj['lon'], obj['lat'],\n                    int(distance), sid, lon, lat)\n            elif mode == 'filter':\n                discarded.append(obj)\n            elif mode == 'strict':\n                raise SiteAssociationError(\n                    'There is nothing closer than %s km '\n                    'to site (%s %s)' % (assoc_dist, lon, lat))\n        if not dic:\n            raise SiteAssociationError(\n                'No sites could be associated within %s km' % assoc_dist)\n        return (sitecol.filtered(dic),\n                numpy.array([dic[sid] for sid in sorted(dic)]),\n                discarded)", "response": "Returns a filtered site collection with the closest vs30 site."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assoc2(self, assets_by_site, assoc_dist, mode, asset_refs):\n        assert mode in 'strict filter', mode\n        self.objects.filtered  # self.objects must be a SiteCollection\n        asset_dt = numpy.dtype(\n            [('asset_ref', vstr), ('lon', F32), ('lat', F32)])\n        assets_by_sid = collections.defaultdict(list)\n        discarded = []\n        for assets in assets_by_site:\n            lon, lat = assets[0].location\n            obj, distance = self.get_closest(lon, lat)\n            if distance <= assoc_dist:\n                # keep the assets, otherwise discard them\n                assets_by_sid[obj['sids']].extend(assets)\n            elif mode == 'strict':\n                raise SiteAssociationError(\n                    'There is nothing closer than %s km '\n                    'to site (%s %s)' % (assoc_dist, lon, lat))\n            else:\n                discarded.extend(assets)\n        sids = sorted(assets_by_sid)\n        if not sids:\n            raise SiteAssociationError(\n                'Could not associate any site to any assets within the '\n                'asset_hazard_distance of %s km' % assoc_dist)\n        assets_by_site = [\n            sorted(assets_by_sid[sid], key=operator.attrgetter('ordinal'))\n            for sid in sids]\n        data = [(asset_refs[asset.ordinal],) + asset.location\n                for asset in discarded]\n        discarded = numpy.array(data, asset_dt)\n        return self.objects.filtered(sids), assets_by_site, discarded", "response": "This function is used to associate a list of assets by site to the site collection used to instantiate GeographicObjects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_vulnerability_functions_05(node, fname):\n    # NB: the IMTs can be duplicated and with different levels, each\n    # vulnerability function in a set will get its own levels\n    vf_ids = set()\n    vmodel = scientific.VulnerabilityModel(**node.attrib)\n    # imt, vf_id -> vulnerability function\n    for vfun in node.getnodes('vulnerabilityFunction'):\n        with context(fname, vfun):\n            imt = vfun.imls['imt']\n            imls = numpy.array(~vfun.imls)\n            vf_id = vfun['id']\n        if vf_id in vf_ids:\n            raise InvalidFile(\n                'Duplicated vulnerabilityFunctionID: %s: %s, line %d' %\n                (vf_id, fname, vfun.lineno))\n        vf_ids.add(vf_id)\n        num_probs = None\n        if vfun['dist'] == 'PM':\n            loss_ratios, probs = [], []\n            for probabilities in vfun[1:]:\n                loss_ratios.append(probabilities['lr'])\n                probs.append(valid.probabilities(~probabilities))\n                if num_probs is None:\n                    num_probs = len(probs[-1])\n                elif len(probs[-1]) != num_probs:\n                    raise ValueError(\n                        'Wrong number of probabilities (expected %d, '\n                        'got %d) in %s, line %d' %\n                        (num_probs, len(probs[-1]), fname,\n                         probabilities.lineno))\n            all_probs = numpy.array(probs)\n            assert all_probs.shape == (len(loss_ratios), len(imls)), (\n                len(loss_ratios), len(imls))\n            vmodel[imt, vf_id] = (\n                scientific.VulnerabilityFunctionWithPMF(\n                    vf_id, imt, imls, numpy.array(loss_ratios),\n                    all_probs))\n            # the seed will be set by readinput.get_risk_model\n        else:\n            with context(fname, vfun):\n                loss_ratios = ~vfun.meanLRs\n                coefficients = ~vfun.covLRs\n            if len(loss_ratios) != len(imls):\n                raise InvalidFile(\n                    'There are %d loss ratios, but %d imls: %s, line %d' %\n                    (len(loss_ratios), len(imls), fname,\n                     vfun.meanLRs.lineno))\n            if len(coefficients) != len(imls):\n                raise InvalidFile(\n                    'There are %d coefficients, but %d imls: %s, '\n                    'line %d' % (len(coefficients), len(imls), fname,\n                                 vfun.covLRs.lineno))\n            with context(fname, vfun):\n                vmodel[imt, vf_id] = scientific.VulnerabilityFunction(\n                    vf_id, imt, imls, loss_ratios, coefficients,\n                    vfun['dist'])\n    return vmodel", "response": "This function returns a list of vulnerability functions that are in the vulnerability filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ffconvert(fname, limit_states, ff, min_iml=1E-10):\n    with context(fname, ff):\n        ffs = ff[1:]\n        imls = ff.imls\n    nodamage = imls.attrib.get('noDamageLimit')\n    if nodamage == 0:\n        # use a cutoff to avoid log(0) in GMPE.to_distribution_values\n        logging.warning('Found a noDamageLimit=0 in %s, line %s, '\n                        'using %g instead', fname, ff.lineno, min_iml)\n        nodamage = min_iml\n    with context(fname, imls):\n        attrs = dict(format=ff['format'],\n                     imt=imls['imt'],\n                     id=ff['id'],\n                     nodamage=nodamage)\n\n    LS = len(limit_states)\n    if LS != len(ffs):\n        with context(fname, ff):\n            raise InvalidFile('expected %d limit states, found %d' %\n                              (LS, len(ffs)))\n    if ff['format'] == 'continuous':\n        minIML = float(imls['minIML'])\n        if minIML == 0:\n            # use a cutoff to avoid log(0) in GMPE.to_distribution_values\n            logging.warning('Found minIML=0 in %s, line %s, using %g instead',\n                            fname, imls.lineno, min_iml)\n            minIML = min_iml\n        attrs['minIML'] = minIML\n        attrs['maxIML'] = float(imls['maxIML'])\n        array = numpy.zeros(LS, [('mean', F64), ('stddev', F64)])\n        for i, ls, node in zip(range(LS), limit_states, ff[1:]):\n            if ls != node['ls']:\n                with context(fname, node):\n                    raise InvalidFile('expected %s, found' %\n                                      (ls, node['ls']))\n            array['mean'][i] = node['mean']\n            array['stddev'][i] = node['stddev']\n    elif ff['format'] == 'discrete':\n        attrs['imls'] = ~imls\n        valid.check_levels(attrs['imls'], attrs['imt'], min_iml)\n        num_poes = len(attrs['imls'])\n        array = numpy.zeros((LS, num_poes))\n        for i, ls, node in zip(range(LS), limit_states, ff[1:]):\n            with context(fname, node):\n                if ls != node['ls']:\n                    raise InvalidFile('expected %s, found' %\n                                      (ls, node['ls']))\n                poes = (~node if isinstance(~node, list)\n                        else valid.probabilities(~node))\n                if len(poes) != num_poes:\n                    raise InvalidFile('expected %s, found' %\n                                      (num_poes, len(poes)))\n                array[i, :] = poes\n    # NB: the format is constrained in nrml.FragilityNode to be either\n    # discrete or continuous, there is no third option\n    return array, attrs", "response": "Convert a fragility function into a numpy array plus a bunch\n    of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a fragility model with the given node and file name fname", "response": "def get_fragility_model(node, fname):\n    \"\"\"\n    :param node:\n        a vulnerabilityModel node\n    :param fname:\n        path to the vulnerability file\n    :returns:\n        a dictionary imt, ff_id -> fragility function list\n    \"\"\"\n    with context(fname, node):\n        fid = node['id']\n        asset_category = node['assetCategory']\n        loss_type = node['lossCategory']\n        description = ~node.description\n        limit_states = ~node.limitStates\n        ffs = node[2:]\n    fmodel = scientific.FragilityModel(\n        fid, asset_category, loss_type, description, limit_states)\n    for ff in ffs:\n        array, attrs = ffconvert(fname, limit_states, ff)\n        attrs['id'] = ff['id']\n        ffl = scientific.FragilityFunctionList(array, **attrs)\n        fmodel[ff.imls['imt'], ff['id']] = ffl\n    return fmodel"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts fragility model from NRML 0. 4 to NRML 0. 5", "response": "def convert_fragility_model_04(node, fname, fmcounter=itertools.count(1)):\n    \"\"\"\n    :param node:\n        an :class:`openquake.commonib.node.Node` in NRML 0.4\n    :param fname:\n        path of the fragility file\n    :returns:\n        an :class:`openquake.commonib.node.Node` in NRML 0.5\n    \"\"\"\n    convert_type = {\"lognormal\": \"logncdf\"}\n    new = Node('fragilityModel',\n               dict(assetCategory='building',\n                    lossCategory='structural',\n                    id='fm_%d_converted_from_NRML_04' %\n                    next(fmcounter)))\n    with context(fname, node):\n        fmt = node['format']\n        descr = ~node.description\n        limit_states = ~node.limitStates\n    new.append(Node('description', {}, descr))\n    new.append((Node('limitStates', {}, ' '.join(limit_states))))\n    for ffs in node[2:]:\n        IML = ffs.IML\n        # NB: noDamageLimit = None is different than zero\n        nodamage = ffs.attrib.get('noDamageLimit')\n        ff = Node('fragilityFunction', {'format': fmt})\n        ff['id'] = ~ffs.taxonomy\n        ff['shape'] = convert_type[ffs.attrib.get('type', 'lognormal')]\n        if fmt == 'continuous':\n            with context(fname, IML):\n                attr = dict(imt=IML['IMT'],\n                            minIML=IML['minIML'],\n                            maxIML=IML['maxIML'])\n                if nodamage is not None:\n                    attr['noDamageLimit'] = nodamage\n                ff.append(Node('imls', attr))\n            for ffc in ffs[2:]:\n                with context(fname, ffc):\n                    ls = ffc['ls']\n                    param = ffc.params\n                with context(fname, param):\n                    m, s = param['mean'], param['stddev']\n                ff.append(Node('params', dict(ls=ls, mean=m, stddev=s)))\n        else:  # discrete\n            with context(fname, IML):\n                imls = ' '.join(map(str, (~IML)[1]))\n                attr = dict(imt=IML['IMT'])\n            if nodamage is not None:\n                attr['noDamageLimit'] = nodamage\n            ff.append(Node('imls', attr, imls))\n            for ffd in ffs[2:]:\n                ls = ffd['ls']\n                with context(fname, ffd):\n                    poes = ' '.join(map(str, ~ffd.poEs))\n                ff.append(Node('poes', dict(ls=ls), poes))\n        new.append(ff)\n    return new"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a fragility model in NRML 0. 5.", "response": "def get_fragility_model_04(fmodel, fname):\n    \"\"\"\n    :param fmodel:\n        a fragilityModel node\n    :param fname:\n        path of the fragility file\n    :returns:\n        an :class:`openquake.risklib.scientific.FragilityModel` instance\n    \"\"\"\n    logging.warning('Please upgrade %s to NRML 0.5', fname)\n    node05 = convert_fragility_model_04(fmodel, fname)\n    node05.limitStates.text = node05.limitStates.text.split()\n    return get_fragility_model(node05, fname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the value is a valid taxonomy.", "response": "def taxonomy(value):\n    \"\"\"\n    Any ASCII character goes into a taxonomy, except spaces.\n    \"\"\"\n    try:\n        value.encode('ascii')\n    except UnicodeEncodeError:\n        raise ValueError('tag %r is not ASCII' % value)\n    if re.search(r'\\s', value):\n        raise ValueError('The taxonomy %r contains whitespace chars' % value)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the global nrml. validators dictionary", "response": "def update_validators():\n    \"\"\"\n    Call this to updade the global nrml.validators\n    \"\"\"\n    validators.update({\n        'fragilityFunction.id': valid.utf8,  # taxonomy\n        'vulnerabilityFunction.id': valid.utf8,  # taxonomy\n        'consequenceFunction.id': valid.utf8,  # taxonomy\n        'asset.id': valid.asset_id,\n        'costType.name': valid.cost_type,\n        'costType.type': valid.cost_type_type,\n        'cost.type': valid.cost_type,\n        'area.type': valid.name,\n        'isAbsolute': valid.boolean,\n        'insuranceLimit': valid.positivefloat,\n        'deductible': valid.positivefloat,\n        'occupants': valid.positivefloat,\n        'value': valid.positivefloat,\n        'retrofitted': valid.positivefloat,\n        'number': valid.compose(valid.positivefloat, valid.nonzero),\n        'vulnerabilitySetID': str,  # any ASCII string is fine\n        'vulnerabilityFunctionID': str,  # any ASCII string is fine\n        'lossCategory': valid.utf8,  # a description field\n        'lr': valid.probability,\n        'lossRatio': valid.positivefloats,\n        'coefficientsVariation': valid.positivefloats,\n        'probabilisticDistribution': valid.Choice('LN', 'BT'),\n        'dist': valid.Choice('LN', 'BT', 'PM'),\n        'meanLRs': valid.positivefloats,\n        'covLRs': valid.positivefloats,\n        'format': valid.ChoiceCI('discrete', 'continuous'),\n        'mean': valid.positivefloat,\n        'stddev': valid.positivefloat,\n        'minIML': valid.positivefloat,\n        'maxIML': valid.positivefloat,\n        'limitStates': valid.namelist,\n        'noDamageLimit': valid.NoneOr(valid.positivefloat),\n        'loss_type': valid_loss_types,\n        'losses': valid.positivefloats,\n        'averageLoss': valid.positivefloat,\n        'stdDevLoss': valid.positivefloat,\n        'ffs.type': valid.ChoiceCI('lognormal'),\n        'assetLifeExpectancy': valid.positivefloat,\n        'interestRate': valid.positivefloat,\n        'lossType': valid_loss_types,\n        'aalOrig': valid.positivefloat,\n        'aalRetr': valid.positivefloat,\n        'ratio': valid.positivefloat,\n        'cf': asset_mean_stddev,\n        'damage': damage_triple,\n        'damageStates': valid.namelist,\n        'taxonomy': taxonomy,\n        'tagNames': valid.namelist,\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the information about the in the datastore.", "response": "def get_info(dstore):\n    \"\"\"\n    :returns: {'stats': dic, 'loss_types': dic, 'num_rlzs': R}\n    \"\"\"\n    oq = dstore['oqparam']\n    stats = {stat: s for s, stat in enumerate(oq.hazard_stats())}\n    loss_types = {lt: l for l, lt in enumerate(oq.loss_dt().names)}\n    imt = {imt: i for i, imt in enumerate(oq.imtls)}\n    num_rlzs = dstore['csm_info'].get_num_rlzs()\n    return dict(stats=stats, num_rlzs=num_rlzs, loss_types=loss_types,\n                imtls=oq.imtls, investigation_time=oq.investigation_time,\n                poes=oq.poes, imt=imt, uhs_dt=oq.uhs_dt())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(query_string, info={}):\n    qdic = parse_qs(query_string)\n    loss_types = info.get('loss_types', [])\n    for key, val in qdic.items():  # for instance, convert site_id to an int\n        if key == 'loss_type':\n            qdic[key] = [loss_types[k] for k in val]\n        else:\n            qdic[key] = [lit_eval(v) for v in val]\n    if info:\n        qdic['k'], qdic['kind'], qdic['rlzs'] = _normalize(qdic['kind'], info)\n    return qdic", "response": "parse a query_string into a normalized query_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef barray(iterlines):\n    lst = [line.encode('utf-8') for line in iterlines]\n    arr = numpy.array(lst)\n    return arr", "response": "Returns a numpy array of bytes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract an HDF5 path object from the datastore.", "response": "def extract_(dstore, dspath):\n    \"\"\"\n    Extracts an HDF5 path object from the datastore, for instance\n    extract(dstore, 'sitecol').\n    \"\"\"\n    obj = dstore[dspath]\n    if isinstance(obj, Dataset):\n        return ArrayWrapper(obj.value, obj.attrs)\n    elif isinstance(obj, Group):\n        return ArrayWrapper(numpy.array(list(obj)), obj.attrs)\n    else:\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_realizations(dstore, dummy):\n    rlzs = dstore['csm_info'].rlzs\n    dt = [('ordinal', U32), ('weight', F32), ('gsims', '<S64')]\n    arr = numpy.zeros(len(rlzs), dt)\n    arr['ordinal'] = rlzs['ordinal']\n    arr['weight'] = rlzs['weight']\n    arr['gsims'] = rlzs['branch_path']  # this is used in scenario by QGIS\n    return arr", "response": "Extract an array of realizations. Use it as / extract / realizations"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_exposure_metadata(dstore, what):\n    dic = {}\n    dic1, dic2 = dstore['assetcol/tagcol'].__toh5__()\n    dic.update(dic1)\n    dic.update(dic2)\n    if 'asset_risk' in dstore:\n        dic['multi_risk'] = sorted(\n            set(dstore['asset_risk'].dtype.names) -\n            set(dstore['assetcol/array'].dtype.names))\n    names = [name for name in dstore['assetcol/array'].dtype.names\n             if name.startswith(('value-', 'number', 'occupants_'))\n             and not name.endswith('_None')]\n    return ArrayWrapper(numpy.array(names), dic)", "response": "Extract the loss categories and tags of the exposure. Use it as / extract / exposure_metadata\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts an array of assets optionally filtered by tag. Use it as / extract / assets?taxonomy = RC&occupancy = RES", "response": "def extract_assets(dstore, what):\n    \"\"\"\n    Extract an array of assets, optionally filtered by tag.\n    Use it as /extract/assets?taxonomy=RC&taxonomy=MSBC&occupancy=RES\n    \"\"\"\n    qdict = parse(what)\n    dic = {}\n    dic1, dic2 = dstore['assetcol/tagcol'].__toh5__()\n    dic.update(dic1)\n    dic.update(dic2)\n    arr = dstore['assetcol/array'].value\n    for tag, vals in qdict.items():\n        cond = numpy.zeros(len(arr), bool)\n        for val in vals:\n            tagidx, = numpy.where(dic[tag] == val)\n            cond |= arr[tag] == tagidx\n        arr = arr[cond]\n    return ArrayWrapper(arr, dic)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_asset_values(dstore, sid):\n    if sid:\n        return extract(dstore, 'asset_values')[int(sid)]\n    assetcol = extract(dstore, 'assetcol')\n    asset_refs = assetcol.asset_refs\n    assets_by_site = assetcol.assets_by_site()\n    lts = assetcol.loss_types\n    dt = numpy.dtype([('aref', asset_refs.dtype), ('aid', numpy.uint32)] +\n                     [(str(lt), numpy.float32) for lt in lts])\n    data = []\n    for assets in assets_by_site:\n        vals = numpy.zeros(len(assets), dt)\n        for a, asset in enumerate(assets):\n            vals[a]['aref'] = asset_refs[a]\n            vals[a]['aid'] = asset['ordinal']\n            for lt in lts:\n                vals[a][lt] = asset['value-' + lt]\n        data.append(vals)\n    return data", "response": "Extract an array of asset values for the given sid. Use it as\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_asset_tags(dstore, tagname):\n    tagcol = dstore['assetcol/tagcol']\n    if tagname:\n        yield tagname, barray(tagcol.gen_tags(tagname))\n    for tagname in tagcol.tagnames:\n        yield tagname, barray(tagcol.gen_tags(tagname))", "response": "Extract an array of asset tags for the given tagname. Use it as\n    or taxonomy\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mesh(sitecol, complete=True):\n    sc = sitecol.complete if complete else sitecol\n    if sc.at_sea_level():\n        mesh = numpy.zeros(len(sc), [('lon', F64), ('lat', F64)])\n        mesh['lon'] = sc.lons\n        mesh['lat'] = sc.lats\n    else:\n        mesh = numpy.zeros(len(sc), [('lon', F64), ('lat', F64),\n                                     ('depth', F64)])\n        mesh['lon'] = sc.lons\n        mesh['lat'] = sc.lats\n        mesh['depth'] = sc.depths\n    return mesh", "response": "returns a lon - lat - depth array depending if the site collection\n    is at sea level or not\n    is at sea level"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hazard_items(dic, mesh, *extras, **kw):\n    for item in kw.items():\n        yield item\n    arr = dic[next(iter(dic))]\n    dtlist = [(str(field), arr.dtype) for field in sorted(dic)]\n    for field, dtype, values in extras:\n        dtlist.append((str(field), dtype))\n    array = numpy.zeros(arr.shape, dtlist)\n    for field in dic:\n        array[field] = dic[field]\n    for field, dtype, values in extras:\n        array[field] = values\n    yield 'all', util.compose_arrays(mesh, array)", "response": "Yields the items of the hazard array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_hcurves(dstore, what):\n    info = get_info(dstore)\n    if what == '':  # npz exports for QGIS\n        sitecol = dstore['sitecol']\n        mesh = get_mesh(sitecol, complete=False)\n        dic = _get_dict(dstore, 'hcurves-stats', info['imtls'], info['stats'])\n        yield from hazard_items(\n            dic, mesh, investigation_time=info['investigation_time'])\n        return\n    params = parse(what, info)\n    if 'imt' in params:\n        [imt] = params['imt']\n        slc = info['imtls'](imt)\n    else:\n        slc = ALL\n    sids = params.get('site_id', ALL)\n    if params['rlzs']:\n        dset = dstore['hcurves-rlzs']\n        for k in params['k']:\n            yield 'rlz-%03d' % k, hdf5.extract(dset, sids, k, slc)[:, 0]\n    else:\n        dset = dstore['hcurves-stats']\n        stats = list(info['stats'])\n        for k in params['k']:\n            yield stats[k], hdf5.extract(dset, sids, k, slc)[:, 0]\n    yield from params.items()", "response": "Extracts hazard curves from the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_hmaps(dstore, what):\n    info = get_info(dstore)\n    if what == '':  # npz exports for QGIS\n        sitecol = dstore['sitecol']\n        mesh = get_mesh(sitecol, complete=False)\n        dic = _get_dict(dstore, 'hmaps-stats',\n                        {imt: info['poes'] for imt in info['imtls']},\n                        info['stats'])\n        yield from hazard_items(\n            dic, mesh, investigation_time=info['investigation_time'])\n        return\n    params = parse(what, info)\n    if 'imt' in params:\n        [imt] = params['imt']\n        m = info['imt'][imt]\n        s = slice(m, m + 1)\n    else:\n        s = ALL\n    if params['rlzs']:\n        dset = dstore['hmaps-rlzs']\n        for k in params['k']:\n            yield 'rlz-%03d' % k, hdf5.extract(dset, ALL, k, s, ALL)[:, 0]\n    else:\n        dset = dstore['hmaps-stats']\n        stats = list(info['stats'])\n        for k in params['k']:\n            yield stats[k], hdf5.extract(dset, ALL, k, s, ALL)[:, 0]\n    yield from params.items()", "response": "Extracts hazard maps. Use it as / extract / hmaps?imt = PGA\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting uniform hazard spectra. Use it as / extract / uhs?kind = mean or / extract / uhs?kind = rlz - 0 etc.", "response": "def extract_uhs(dstore, what):\n    \"\"\"\n    Extracts uniform hazard spectra. Use it as /extract/uhs?kind=mean or\n    /extract/uhs?kind=rlz-0, etc\n    \"\"\"\n    info = get_info(dstore)\n    if what == '':  # npz exports for QGIS\n        sitecol = dstore['sitecol']\n        mesh = get_mesh(sitecol, complete=False)\n        dic = {}\n        for stat, s in info['stats'].items():\n            hmap = dstore['hmaps-stats'][:, s]\n            dic[stat] = calc.make_uhs(hmap, info)\n        yield from hazard_items(\n            dic, mesh, investigation_time=info['investigation_time'])\n        return\n    params = parse(what, info)\n    periods = []\n    for m, imt in enumerate(info['imtls']):\n        if imt == 'PGA' or imt.startswith('SA'):\n            periods.append(m)\n    if 'site_id' in params:\n        sids = params['site_id']\n    else:\n        sids = ALL\n    if params['rlzs']:\n        dset = dstore['hmaps-rlzs']\n        for k in params['k']:\n            yield ('rlz-%03d' % k,\n                   hdf5.extract(dset, sids, k, periods, ALL)[:, 0])\n    else:\n        dset = dstore['hmaps-stats']\n        stats = list(info['stats'])\n        for k in params['k']:\n            yield stats[k], hdf5.extract(dset, sids, k, periods, ALL)[:, 0]\n    yield from params.items()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_agg_curves(dstore, what):\n    from openquake.calculators.export.loss_curves import get_loss_builder\n    oq = dstore['oqparam']\n    loss_type, tags = get_loss_type_tags(what)\n    if 'curves-stats' in dstore:  # event_based_risk\n        losses = _get_curves(dstore['curves-stats'], oq.lti[loss_type])\n        stats = dstore['curves-stats'].attrs['stats']\n    elif 'curves-rlzs' in dstore:  # event_based_risk, 1 rlz\n        losses = _get_curves(dstore['curves-rlzs'], oq.lti[loss_type])\n        assert losses.shape[1] == 1, 'There must be a single realization'\n        stats = [b'mean']  # suitable to be stored as hdf5 attribute\n    else:\n        raise KeyError('No curves found in %s' % dstore)\n    res = _filter_agg(dstore['assetcol'], losses, tags, stats)\n    cc = dstore['assetcol/cost_calculator']\n    res.units = cc.get_units(loss_types=[loss_type])\n    res.return_periods = get_loss_builder(dstore).return_periods\n    return res", "response": "Extract the aggregate loss curves of the given loss type and tags for event based risk calculations. Use it as\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\naggregates losses of the given loss type and tags. Use it as", "response": "def extract_agg_losses(dstore, what):\n    \"\"\"\n    Aggregate losses of the given loss type and tags. Use it as\n    /extract/agg_losses/structural?taxonomy=RC&zipcode=20126\n    /extract/agg_losses/structural?taxonomy=RC&zipcode=*\n\n    :returns:\n        an array of shape (T, R) if one of the tag names has a `*` value\n        an array of shape (R,), being R the number of realizations\n        an array of length 0 if there is no data for the given tags\n    \"\"\"\n    loss_type, tags = get_loss_type_tags(what)\n    if not loss_type:\n        raise ValueError('loss_type not passed in agg_losses/<loss_type>')\n    l = dstore['oqparam'].lti[loss_type]\n    if 'losses_by_asset' in dstore:  # scenario_risk\n        stats = None\n        losses = dstore['losses_by_asset'][:, :, l]['mean']\n    elif 'avg_losses-stats' in dstore:  # event_based_risk, classical_risk\n        stats = dstore['avg_losses-stats'].attrs['stats']\n        losses = dstore['avg_losses-stats'][:, :, l]\n    elif 'avg_losses-rlzs' in dstore:  # event_based_risk, classical_risk\n        stats = [b'mean']\n        losses = dstore['avg_losses-rlzs'][:, :, l]\n    else:\n        raise KeyError('No losses found in %s' % dstore)\n    return _filter_agg(dstore['assetcol'], losses, tags, stats)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_agg_damages(dstore, what):\n    loss_type, tags = get_loss_type_tags(what)\n    if 'dmg_by_asset' in dstore:  # scenario_damage\n        lti = dstore['oqparam'].lti[loss_type]\n        losses = dstore['dmg_by_asset'][:, :, lti, 0]\n    else:\n        raise KeyError('No damages found in %s' % dstore)\n    return _filter_agg(dstore['assetcol'], losses, tags)", "response": "Aggregate damages of the given loss type and tags. Use it as\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the aggregate of the asset objects for the given what.", "response": "def extract_aggregate(dstore, what):\n    \"\"\"\n    /extract/aggregate/avg_losses?\n    kind=mean&loss_type=structural&tag=taxonomy&tag=occupancy\n    \"\"\"\n    name, qstring = what.split('?', 1)\n    info = get_info(dstore)\n    qdic = parse(qstring, info)\n    suffix = '-rlzs' if qdic['rlzs'] else '-stats'\n    tagnames = qdic.get('tag', [])\n    assetcol = dstore['assetcol']\n    ltypes = qdic.get('loss_type', [])\n    if ltypes:\n        array = dstore[name + suffix][:, qdic['k'][0], ltypes[0]]\n    else:\n        array = dstore[name + suffix][:, qdic['k'][0]]\n    aw = ArrayWrapper(assetcol.aggregate_by(tagnames, array), {})\n    for tagname in tagnames:\n        setattr(aw, tagname, getattr(assetcol.tagcol, tagname))\n    aw.tagnames = encode(tagnames)\n    if not ltypes:\n        aw.extra = ('loss_type',) + tuple(info['loss_types'])\n    return aw"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_damage_dt(dstore, mean_std=True):\n    oq = dstore['oqparam']\n    damage_states = ['no_damage'] + list(\n        dstore.get_attr('risk_model', 'limit_states'))\n    dt_list = []\n    for ds in damage_states:\n        ds = str(ds)\n        if mean_std:\n            dt_list.append(('%s_mean' % ds, F32))\n            dt_list.append(('%s_stdv' % ds, F32))\n        else:\n            dt_list.append((ds, F32))\n    damage_dt = numpy.dtype(dt_list)\n    loss_types = oq.loss_dt().names\n    return numpy.dtype([(lt, damage_dt) for lt in loss_types])", "response": "Build a composite dtype for the damage states"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_damage_array(data, damage_dt):\n    A, L, MS, D = data.shape\n    dmg = numpy.zeros(A, damage_dt)\n    for a in range(A):\n        for l, lt in enumerate(damage_dt.names):\n            std = any(f for f in damage_dt[lt].names if f.endswith('_stdv'))\n            if MS == 1 or not std:  # there is only the mean value\n                dmg[lt][a] = tuple(data[a, l, 0])\n            else:  # there are both mean and stddev\n                # data[a, l].T has shape (D, 2)\n                dmg[lt][a] = tuple(numpy.concatenate(data[a, l].T))\n    return dmg", "response": "Builds a damage array from data and damage_dt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_mfd(dstore, what):\n    dd = collections.defaultdict(int)\n    for rup in dstore['ruptures'].value:\n        dd[rup['mag']] += 1\n    dt = numpy.dtype([('mag', float), ('freq', int)])\n    magfreq = numpy.array(sorted(dd.items(), key=operator.itemgetter(0)), dt)\n    return magfreq", "response": "Display num_ruptures by magnitude for event based calculations."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the source loss table for a give loss type ordered in decreasing order.", "response": "def extract_src_loss_table(dstore, loss_type):\n    \"\"\"\n    Extract the source loss table for a give loss type, ordered in decreasing\n    order. Example:\n    http://127.0.0.1:8800/v1/calc/30/extract/src_loss_table/structural\n    \"\"\"\n    oq = dstore['oqparam']\n    li = oq.lti[loss_type]\n    source_ids = dstore['source_info']['source_id']\n    idxs = dstore['ruptures'].value[['srcidx', 'grp_id']]\n    losses = dstore['rup_loss_table'][:, li]\n    slt = numpy.zeros(len(source_ids), [('grp_id', U32), (loss_type, F32)])\n    for loss, (srcidx, grp_id) in zip(losses, idxs):\n        slt[srcidx][loss_type] += loss\n        slt[srcidx]['grp_id'] = grp_id\n    slt = util.compose_arrays(source_ids, slt, 'source_id')\n    slt.sort(order=loss_type)\n    return slt[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_mean_std_curves(dstore, what):\n    getter = getters.PmapGetter(dstore)\n    arr = getter.get_mean().array\n    for imt in getter.imtls:\n        yield 'imls/' + imt, getter.imtls[imt]\n        yield 'poes/' + imt, arr[:, getter.imtls(imt)]", "response": "Yields the mean and standard deviation curves for all sites\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the average losses of all the occupancy assets with the given tag.", "response": "def losses_by_tag(dstore, tag):\n    \"\"\"\n    Statistical average losses by tag. For instance call\n\n    $ oq extract losses_by_tag/occupancy\n    \"\"\"\n    dt = [(tag, vstr)] + dstore['oqparam'].loss_dt_list()\n    aids = dstore['assetcol/array'][tag]\n    dset, stats = _get(dstore, 'avg_losses')\n    arr = dset.value\n    tagvalues = dstore['assetcol/tagcol/' + tag][1:]  # except tagvalue=\"?\"\n    for s, stat in enumerate(stats):\n        out = numpy.zeros(len(tagvalues), dt)\n        for li, (lt, lt_dt) in enumerate(dt[1:]):\n            for i, tagvalue in enumerate(tagvalues):\n                out[i][tag] = tagvalue\n                counts = arr[aids == i + 1, s, li].sum()\n                if counts:\n                    out[i][lt] = counts\n        yield stat, out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_rupture(dstore, serial):\n    ridx = list(dstore['ruptures']['serial']).index(int(serial))\n    [getter] = getters.gen_rupture_getters(dstore, slice(ridx, ridx + 1))\n    yield from getter.get_rupdict().items()", "response": "Extract information about the given event index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract information about the given event index.", "response": "def extract_event_info(dstore, eidx):\n    \"\"\"\n    Extract information about the given event index.\n    Example:\n    http://127.0.0.1:8800/v1/calc/30/extract/event_info/0\n    \"\"\"\n    event = dstore['events'][int(eidx)]\n    serial = int(event['eid'] // TWO32)\n    ridx = list(dstore['ruptures']['serial']).index(serial)\n    [getter] = getters.gen_rupture_getters(dstore, slice(ridx, ridx + 1))\n    rupdict = getter.get_rupdict()\n    rlzi = event['rlz']\n    rlzs_assoc = dstore['csm_info'].get_rlzs_assoc()\n    gsim = rlzs_assoc.gsim_by_trt[rlzi][rupdict['trt']]\n    for key, val in rupdict.items():\n        yield key, val\n    yield 'rlzi', rlzi\n    yield 'gsim', repr(gsim)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the ruptures within the given bounding box", "response": "def get_ruptures_within(dstore, bbox):\n    \"\"\"\n    Extract the ruptures within the given bounding box, a string\n    minlon,minlat,maxlon,maxlat.\n    Example:\n    http://127.0.0.1:8800/v1/calc/30/extract/ruptures_with/8,44,10,46\n    \"\"\"\n    minlon, minlat, maxlon, maxlat = map(float, bbox.split(','))\n    hypo = dstore['ruptures']['hypo'].T  # shape (3, N)\n    mask = ((minlon <= hypo[0]) * (minlat <= hypo[1]) *\n            (maxlon >= hypo[0]) * (maxlat >= hypo[1]))\n    return dstore['ruptures'][mask]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the geometry of a given sources", "response": "def extract_source_geom(dstore, srcidxs):\n    \"\"\"\n    Extract the geometry of a given sources\n    Example:\n    http://127.0.0.1:8800/v1/calc/30/extract/source_geom/1,2,3\n    \"\"\"\n    for i in srcidxs.split(','):\n        rec = dstore['source_info'][int(i)]\n        geom = dstore['source_geom'][rec['gidx1']:rec['gidx2']]\n        yield rec['source_id'], geom"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, what):\n        url = '%s/v1/calc/%d/extract/%s' % (self.server, self.calc_id, what)\n        logging.info('GET %s', url)\n        resp = self.sess.get(url)\n        if resp.status_code != 200:\n            raise WebAPIError(resp.text)\n        npz = numpy.load(io.BytesIO(resp.content))\n        attrs = {k: npz[k] for k in npz if k != 'array'}\n        try:\n            arr = npz['array']\n        except KeyError:\n            arr = ()\n        return ArrayWrapper(arr, attrs)", "response": "Get an array from the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(self, fname):\n        url = '%s/v1/calc/%d/datastore' % (self.server, self.calc_id)\n        resp = self.sess.get(url, stream=True)\n        down = 0\n        with open(fname, 'wb') as f:\n            logging.info('Saving %s', fname)\n            for chunk in resp.iter_content(CHUNKSIZE):\n                f.write(chunk)\n                down += len(chunk)\n                println('Downloaded {:,} bytes'.format(down))\n        print()", "response": "Dump the remote datastore on a local path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_C1_term(C, dists):\n\n    c1_dists = np.zeros_like(dists)\n    idx = dists < C['Rc11']\n    c1_dists[idx] = C['phi_11']\n    idx = (dists >= C['Rc11']) & (dists <= C['Rc21'])\n    c1_dists[idx] = C['phi_11'] + (C['phi_21'] - C['phi_11']) * \\\n        ((dists[idx] - C['Rc11']) / (C['Rc21'] - C['Rc11']))\n    idx = dists > C['Rc21']\n    c1_dists[idx] = C['phi_21']\n    return c1_dists", "response": "Compute the C1 term in terms of the Rodriguez - Marek et al 2003."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the small magnitude correction term in equation 1 page 74.", "response": "def _compute_small_mag_correction_term(C, mag, rhypo):\n    \"\"\"\n    small magnitude correction applied to the median values\n    \"\"\"\n    if mag >= 3.00 and mag < 5.5:\n        min_term = np.minimum(rhypo, C['Rm'])\n        max_term = np.maximum(min_term, 10)\n        term_ln = np.log(max_term / 20)\n        term_ratio = ((5.50 - mag) / C['a1'])\n        temp = (term_ratio) ** C['a2'] * (C['b1'] + C['b2'] * term_ln)\n        return 1 / np.exp(temp)\n    else:\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_phi_ss(C, mag, c1_dists, log_phi_ss, mean_phi_ss):\n\n    phi_ss = 0\n\n    if mag < C['Mc1']:\n        phi_ss = c1_dists\n\n    elif mag >= C['Mc1'] and mag <= C['Mc2']:\n        phi_ss = c1_dists + \\\n            (C['C2'] - c1_dists) * \\\n            ((mag - C['Mc1']) / (C['Mc2'] - C['Mc1']))\n    elif mag > C['Mc2']:\n        phi_ss = C['C2']\n\n    return (phi_ss * 0.50 + mean_phi_ss * 0.50) / log_phi_ss", "response": "Compute the phi_ss of a single station in natural logarithm units."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns standard deviations adjusted for single station correlation.", "response": "def _get_corr_stddevs(C, tau_ss, stddev_types, num_sites, phi_ss, NL=None,\n                      tau_value=None):\n    \"\"\"\n    Return standard deviations adjusted for single station sigma\n    as the total standard deviation - as proposed to be used in\n    the Swiss Hazard Model [2014].\n    \"\"\"\n    stddevs = []\n    temp_stddev = phi_ss * phi_ss\n\n    if tau_value is not None and NL is not None:\n        temp_stddev = temp_stddev + tau_value * tau_value * ((1 + NL) ** 2)\n    else:\n        temp_stddev = temp_stddev + C[tau_ss] * C[tau_ss]\n\n    for stddev_type in stddev_types:\n        if stddev_type == const.StdDev.TOTAL:\n            stddevs.append(np.sqrt(temp_stddev) + np.zeros(num_sites))\n    return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts a CompositionInfo instance containing the single model of index sm_id.", "response": "def get_info(self, sm_id):\n        \"\"\"\n        Extract a CompositionInfo instance containing the single\n        model of index `sm_id`.\n        \"\"\"\n        sm = self.source_models[sm_id]\n        num_samples = sm.samples if self.num_samples else 0\n        return self.__class__(\n            self.gsim_lt, self.seed, num_samples, [sm], self.tot_weight)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef classify_gsim_lt(self, source_model):\n        trts = set(sg.trt for sg in source_model.src_groups if sg.eff_ruptures)\n        gsim_lt = self.gsim_lt.reduce(trts)\n        num_branches = list(gsim_lt.get_num_branches().values())\n        num_paths = gsim_lt.get_num_paths()\n        num_gsims = '(%s)' % ','.join(map(str, num_branches))\n        multi_gsim_trts = sum(1 for num_gsim in num_branches if num_gsim > 1)\n        if multi_gsim_trts == 0:\n            return \"trivial\" + num_gsims, num_paths\n        elif multi_gsim_trts == 1:\n            return \"simple\" + num_gsims, num_paths\n        else:\n            return \"complex\" + num_gsims, num_paths", "response": "Classifies the class of the gsim_lt."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary of source_group_id - > source_model. samples", "response": "def get_samples_by_grp(self):\n        \"\"\"\n        :returns: a dictionary src_group_id -> source_model.samples\n        \"\"\"\n        return {grp.id: sm.samples for sm in self.source_models\n                for grp in sm.src_groups}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_rlzs_by_gsim_grp(self, sm_lt_path=None, trts=None):\n        self.rlzs_assoc = self.get_rlzs_assoc(sm_lt_path, trts)\n        dic = {grp.id: self.rlzs_assoc.get_rlzs_by_gsim(grp.id)\n               for sm in self.source_models for grp in sm.src_groups}\n        return dic", "response": "returns a dictionary of source_group_id - > gsim - > rlzs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning trt -> trti", "response": "def trt2i(self):\n        \"\"\"\n        :returns: trt -> trti\n        \"\"\"\n        trts = sorted(set(src_group.trt for sm in self.source_models\n                          for src_group in sm.src_groups))\n        return {trt: i for i, trt in enumerate(trts)}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_num_rlzs(self, source_model=None):\n        if source_model is None:\n            return sum(self.get_num_rlzs(sm) for sm in self.source_models)\n        if self.num_samples:\n            return source_model.samples\n        trts = set(sg.trt for sg in source_model.src_groups if sg.eff_ruptures)\n        if sum(sg.eff_ruptures for sg in source_model.src_groups) == 0:\n            return 0\n        return self.gsim_lt.reduce(trts).get_num_paths()", "response": "returns the number of realizations per source model"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rlzs(self):\n        tups = [(r.ordinal, r.uid, r.weight['weight'])\n                for r in self.get_rlzs_assoc().realizations]\n        return numpy.array(tups, rlz_dt)", "response": "returns an array of realizations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the effective rupture count for all source groups in this source model.", "response": "def update_eff_ruptures(self, count_ruptures):\n        \"\"\"\n        :param count_ruptures: function or dict src_group_id -> num_ruptures\n        \"\"\"\n        for smodel in self.source_models:\n            for sg in smodel.src_groups:\n                sg.eff_ruptures = (count_ruptures(sg.id)\n                                   if callable(count_ruptures)\n                                   else count_ruptures.get(sg.id, 0))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the source model for the given src_group_id", "response": "def get_source_model(self, src_group_id):\n        \"\"\"\n        Return the source model for the given src_group_id\n        \"\"\"\n        for smodel in self.source_models:\n            for src_group in smodel.src_groups:\n                if src_group.id == src_group_id:\n                    return smodel"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of source group IDs for the given source model ID", "response": "def get_grp_ids(self, sm_id):\n        \"\"\"\n        :returns: a list of source group IDs for the given source model ID\n        \"\"\"\n        return [sg.id for sg in self.source_models[sm_id].src_groups]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of source_models with src_groups as keys and source_classes as values", "response": "def get_sm_by_grp(self):\n        \"\"\"\n        :returns: a dictionary grp_id -> sm_id\n        \"\"\"\n        return {grp.id: sm.ordinal for sm in self.source_models\n                for grp in sm.src_groups}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef grp_by(self, name):\n        dic = {}\n        for smodel in self.source_models:\n            for src_group in smodel.src_groups:\n                dic[src_group.id] = getattr(src_group, name)\n        return dic", "response": "returns a dictionary grp_id - > TRT string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grp_by_src(self):\n        smodels = []\n        grp_id = 0\n        for sm in self.source_models:\n            src_groups = []\n            smodel = sm.__class__(sm.names, sm.weight, sm.path, src_groups,\n                                  sm.num_gsim_paths, sm.ordinal, sm.samples)\n            for sg in sm.src_groups:\n                for src in sg.sources:\n                    src.src_group_id = grp_id\n                    src_groups.append(\n                        sourceconverter.SourceGroup(\n                            sg.trt, [src], name=src.source_id, id=grp_id))\n                    grp_id += 1\n            smodels.append(smodel)\n        return self.__class__(self.gsim_lt, self.source_model_lt, smodels,\n                              self.optimize_same_id)", "response": "returns a CompositeSourceModel with one group per source"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_model(self, sm_id):\n        sm = self.source_models[sm_id]\n        if self.source_model_lt.num_samples:\n            self.source_model_lt.num_samples = sm.samples\n        new = self.__class__(self.gsim_lt, self.source_model_lt, [sm],\n                             self.optimize_same_id)\n        new.sm_id = sm_id\n        return new", "response": "Extract a CompositeSourceModel instance containing the single\n        model of index sm_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(self, sources_by_grp):\n        source_models = []\n        for sm in self.source_models:\n            src_groups = []\n            for src_group in sm.src_groups:\n                sg = copy.copy(src_group)\n                sg.sources = sorted(sources_by_grp.get(sg.id, []),\n                                    key=operator.attrgetter('id'))\n                src_groups.append(sg)\n            newsm = logictree.LtSourceModel(\n                sm.names, sm.weight, sm.path, src_groups,\n                sm.num_gsim_paths, sm.ordinal, sm.samples)\n            source_models.append(newsm)\n        new = self.__class__(self.gsim_lt, self.source_model_lt, source_models,\n                             self.optimize_same_id)\n        new.info.update_eff_ruptures(new.get_num_ruptures())\n        new.info.tot_weight = new.get_weight()\n        return new", "response": "Generate a new CompositeSourceModel instance from the given dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_weight(self, weight=operator.attrgetter('weight')):\n        return sum(weight(src) for src in self.get_sources())", "response": "get weight of the source model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of non parametric sources in the composite source model", "response": "def get_nonparametric_sources(self):\n        \"\"\"\n        :returns: list of non parametric sources in the composite source model\n        \"\"\"\n        return [src for sm in self.source_models\n                for src_group in sm.src_groups\n                for src in src_group if hasattr(src, 'data')]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract duplicated sources i. e. sources with the same source_id in different source groups. Raise an exception if there are sources with the same source_id in different source groups.", "response": "def check_dupl_sources(self):  # used in print_csm_info\n        \"\"\"\n        Extracts duplicated sources, i.e. sources with the same source_id in\n        different source groups. Raise an exception if there are sources with\n        the same ID which are not duplicated.\n\n        :returns: a list of list of sources, ordered by source_id\n        \"\"\"\n        dd = collections.defaultdict(list)\n        for src_group in self.src_groups:\n            for src in src_group:\n                try:\n                    srcid = src.source_id\n                except AttributeError:  # src is a Node object\n                    srcid = src['id']\n                dd[srcid].append(src)\n        dupl = []\n        for srcid, srcs in sorted(dd.items()):\n            if len(srcs) > 1:\n                _assert_equal_sources(srcs)\n                dupl.append(srcs)\n        return dupl"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the sources contained in the source models by optionally filtering and splitting them.", "response": "def get_sources(self, kind='all'):\n        \"\"\"\n        Extract the sources contained in the source models by optionally\n        filtering and splitting them, depending on the passed parameter.\n        \"\"\"\n        assert kind in ('all', 'indep', 'mutex'), kind\n        sources = []\n        for sm in self.source_models:\n            for src_group in sm.src_groups:\n                if kind in ('all', src_group.src_interdep):\n                    for src in src_group:\n                        if sm.samples > 1:\n                            src.samples = sm.samples\n                        sources.append(src)\n        return sources"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of pairs [ trt group of sources ]", "response": "def get_trt_sources(self, optimize_same_id=None):\n        \"\"\"\n        :returns: a list of pairs [(trt, group of sources)]\n        \"\"\"\n        atomic = []\n        acc = AccumDict(accum=[])\n        for sm in self.source_models:\n            for grp in sm.src_groups:\n                if grp and grp.atomic:\n                    atomic.append((grp.trt, grp))\n                elif grp:\n                    acc[grp.trt].extend(grp)\n        if optimize_same_id is None:\n            optimize_same_id = self.optimize_same_id\n        if optimize_same_id is False:\n            return atomic + list(acc.items())\n        # extract a single source from multiple sources with the same ID\n        n = 0\n        tot = 0\n        dic = {}\n        for trt in acc:\n            dic[trt] = []\n            for grp in groupby(acc[trt], lambda x: x.source_id).values():\n                src = grp[0]\n                n += 1\n                tot += len(grp)\n                # src.src_group_id can be a list if get_sources_by_trt was\n                # called before\n                if len(grp) > 1 and not isinstance(src.src_group_id, list):\n                    src.src_group_id = [s.src_group_id for s in grp]\n                dic[trt].append(src)\n        if n < tot:\n            logging.info('Reduced %d sources to %d sources with unique IDs',\n                         tot, n)\n        return atomic + list(dic.items())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of ruptures per source group", "response": "def get_num_ruptures(self):\n        \"\"\"\n        :returns: the number of ruptures per source group ID\n        \"\"\"\n        return {grp.id: sum(src.num_ruptures for src in grp)\n                for grp in self.src_groups}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_serials(self, ses_seed):\n        sources = self.get_sources()\n        serial = ses_seed\n        for src in sources:\n            nr = src.num_ruptures\n            src.serial = serial\n            serial += nr", "response": "Generate unique seeds for each rupture with numpy. arange."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_maxweight(self, weight, concurrent_tasks, minweight=MINWEIGHT):\n        totweight = self.get_weight(weight)\n        ct = concurrent_tasks or 1\n        mw = math.ceil(totweight / ct)\n        return max(mw, minweight)", "response": "Return an appropriate maxweight for use in the block splitter\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_floating_spinning_factors(self):\n        data = []\n        for src in self.get_sources():\n            if hasattr(src, 'hypocenter_distribution'):\n                data.append(\n                    (len(src.hypocenter_distribution.data),\n                     len(src.nodal_plane_distribution.data)))\n        if not data:\n            return numpy.array([1, 1])\n        return numpy.array(data).mean(axis=0)", "response": "returns the floating spinning factors of the sources"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef weight_list_to_tuple(data, attr_name):\n    '''\n    Converts a list of values and corresponding weights to a tuple of values\n    '''\n\n    if len(data['Value']) != len(data['Weight']):\n        raise ValueError('Number of weights do not correspond to number of '\n                         'attributes in %s' % attr_name)\n    weight = np.array(data['Weight'])\n    if fabs(np.sum(weight) - 1.) > 1E-7:\n        raise ValueError('Weights do not sum to 1.0 in %s' % attr_name)\n\n    data_tuple = []\n    for iloc, value in enumerate(data['Value']):\n        data_tuple.append((value, weight[iloc]))\n    return data_tuple", "response": "Converts a list of values and corresponding weights to a tuple of values\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the tectonic regionalisation dictionary attributes to tuples", "response": "def parse_tect_region_dict_to_tuples(region_dict):\n    '''\n    Parses the tectonic regionalisation dictionary attributes to tuples\n    '''\n    output_region_dict = []\n    tuple_keys = ['Displacement_Length_Ratio', 'Shear_Modulus']\n    # Convert MSR string name to openquake.hazardlib.scalerel object\n    for region in region_dict:\n        for val_name in tuple_keys:\n            region[val_name] = weight_list_to_tuple(region[val_name],\n                                                    val_name)\n        # MSR works differently - so call get_scaling_relation_tuple\n        region['Magnitude_Scaling_Relation'] = weight_list_to_tuple(\n            region['Magnitude_Scaling_Relation'],\n            'Magnitude Scaling Relation')\n        output_region_dict.append(region)\n    return output_region_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_scaling_relation_tuple(msr_dict):\n    '''\n    For a dictionary of scaling relation values convert string list to\n    object list and then to tuple\n    '''\n\n    # Convert MSR string name to openquake.hazardlib.scalerel object\n    for iloc, value in enumerate(msr_dict['Value']):\n        if not value in SCALE_REL_MAP.keys():\n            raise ValueError('Scaling relation %s not supported!' % value)\n        msr_dict['Value'][iloc] = SCALE_REL_MAP[value]()\n    return weight_list_to_tuple(msr_dict,\n                                'Magnitude Scaling Relation')", "response": "Convert string list to tuple of object list and then to tuple of object list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the file and returns an instance of the FaultSource class.", "response": "def read_file(self, mesh_spacing=1.0):\n        '''\n        Reads the file and returns an instance of the FaultSource class.\n\n        :param float mesh_spacing:\n            Fault mesh spacing (km)\n        '''\n\n        # Process the tectonic regionalisation\n        tectonic_reg = self.process_tectonic_regionalisation()\n\n        model = mtkActiveFaultModel(self.data['Fault_Model_ID'],\n                                    self.data['Fault_Model_Name'])\n        for fault in self.data['Fault_Model']:\n            fault_geometry = self.read_fault_geometry(fault['Fault_Geometry'],\n                                                      mesh_spacing)\n            if fault['Shear_Modulus']:\n                fault['Shear_Modulus'] = weight_list_to_tuple(\n                    fault['Shear_Modulus'], '%s Shear Modulus' % fault['ID'])\n\n            if fault['Displacement_Length_Ratio']:\n                fault['Displacement_Length_Ratio'] = weight_list_to_tuple(\n                    fault['Displacement_Length_Ratio'],\n                    '%s Displacement to Length Ratio' % fault['ID'])\n\n            fault_source = mtkActiveFault(\n                fault['ID'],\n                fault['Fault_Name'],\n                fault_geometry,\n                weight_list_to_tuple(fault['Slip'], '%s - Slip' % fault['ID']),\n                float(fault['Rake']),\n                fault['Tectonic_Region'],\n                float(fault['Aseismic']),\n                weight_list_to_tuple(\n                    fault['Scaling_Relation_Sigma'],\n                    '%s Scaling_Relation_Sigma' % fault['ID']),\n                neotectonic_fault=None,\n                scale_rel=get_scaling_relation_tuple(\n                    fault['Magnitude_Scaling_Relation']),\n                aspect_ratio=fault['Aspect_Ratio'],\n                shear_modulus=fault['Shear_Modulus'],\n                disp_length_ratio=fault['Displacement_Length_Ratio'])\n\n            if tectonic_reg:\n                fault_source.get_tectonic_regionalisation(\n                    tectonic_reg,\n                    fault['Tectonic_Region'])\n            assert isinstance(fault['MFD_Model'], list)\n            fault_source.generate_config_set(fault['MFD_Model'])\n            model.faults.append(fault_source)\n\n        return model, tectonic_reg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the tectonic regionalisation from the yaml file and returns a TectonicRegionalisation object", "response": "def process_tectonic_regionalisation(self):\n        '''\n        Processes the tectonic regionalisation from the yaml file\n        '''\n\n        if 'tectonic_regionalisation' in self.data.keys():\n            tectonic_reg = TectonicRegionalisation()\n            tectonic_reg.populate_regions(\n                parse_tect_region_dict_to_tuples(\n                    self.data['tectonic_regionalisation']))\n        else:\n            tectonic_reg = None\n        return tectonic_reg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the fault geometry from the dictionary.", "response": "def read_fault_geometry(self, geo_dict, mesh_spacing=1.0):\n        '''\n        Creates the fault geometry from the parameters specified in the\n        dictionary.\n\n        :param dict geo_dict:\n            Sub-dictionary of main fault dictionary containing only\n            the geometry attributes\n        :param float mesh_spacing:\n            Fault mesh spacing (km)\n        :returns:\n            Instance of SimpleFaultGeometry or ComplexFaultGeometry, depending\n            on typology\n        '''\n        if geo_dict['Fault_Typology'] == 'Simple':\n            # Simple fault geometry\n            raw_trace = geo_dict['Fault_Trace']\n            trace = Line([Point(raw_trace[ival], raw_trace[ival + 1])\n                          for ival in range(0, len(raw_trace), 2)])\n            geometry = SimpleFaultGeometry(trace,\n                                           geo_dict['Dip'],\n                                           geo_dict['Upper_Depth'],\n                                           geo_dict['Lower_Depth'],\n                                           mesh_spacing)\n\n        elif geo_dict['Fault_Typology'] == 'Complex':\n            # Complex Fault Typology\n            trace = []\n            for raw_trace in geo_dict['Fault_Trace']:\n                fault_edge = Line(\n                    [Point(raw_trace[ival], raw_trace[ival + 1],\n                           raw_trace[ival + 2]) for ival in range(0, len(raw_trace),\n                                                                  3)])\n                trace.append(fault_edge)\n            geometry = ComplexFaultGeometry(trace, mesh_spacing)\n        else:\n            raise ValueError('Unrecognised or unsupported fault geometry!')\n        return geometry"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        C = self.COEFFS[imt]\n\n        imean = (self._get_magnitude_term(C, rup.mag) +\n                 self._get_distance_term(C, dists.rhypo, rup.mag))\n        # Convert mean from cm/s and cm/s/s\n        if imt.name in \"SA PGA\":\n            mean = np.log((10.0 ** (imean - 2.0)) / g)\n        else:\n            mean = np.log(10.0 ** imean)\n        stddevs = self._get_stddevs(C, len(dists.rhypo), stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_distance_term(self, C, rhypo, mag):\n        h_eff = self._get_effective_distance(mag)\n        r_val = np.sqrt(rhypo ** 2.0 + h_eff ** 2.0)\n        return C[\"c3\"] * np.log10(r_val)", "response": "Returns the distance scaling term in equation 3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the mean and standard deviation of the object.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        C = self.COEFFS[imt]\n        C_SIG = self.SIGMA_COEFFS[imt]\n\n        mean = (self.get_magnitude_scaling_term(C, rup.mag) +\n                self.get_distance_scaling_term(C, dists.rhypo))\n\n        std_devs = self.get_stddevs(C_SIG, stddev_types, len(dists.rhypo))\n\n        #: Mean ground motions initially returned in cm/s/s (for PGA, SA)\n        #: and cm/s for PGV\n        if not imt.name == \"PGV\":\n            # Convert mean from log(cm/s/s) to g\n            mean = np.log(np.exp(mean) / (100. * g))\n\n        return mean, std_devs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the magnitude scaling term in equation 1", "response": "def get_magnitude_scaling_term(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling term (equation 1)\n        \"\"\"\n        mval = mag - 3.0\n        return C['b1'] + C['b2'] * mval + C['b3'] * (mval ** 2.0) +\\\n            C['b4'] * (mval ** 3.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_distance_scaling_term(self, C, rhyp):\n        rval = rhyp + C['bh']\n        return C['b5'] * np.log(rval) + C['b6'] * rval", "response": "Returns the distance scaling term"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_stddevs(self, C_SIG, stddev_types, num_sites):\n        stddevs = []\n        intra = C_SIG['phi']\n        inter = (C_SIG['tau_s'] + C_SIG['tau_b']) / 2.0\n        total = sqrt(intra ** 2.0 + inter ** 2.0)\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(total + np.zeros(num_sites))\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(inter + np.zeros(num_sites))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(intra + np.zeros(num_sites))\n        return stddevs", "response": "Return the standard deviations for the entry - class of the entry - class of the entry class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute and return mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extract dictionaries of coefficients specific to required\n        # intensity measure type\n        C = self.COEFFS[imt]\n\n        mean = self._compute_mean(C, rup, dists, sites, imt)\n\n        stddevs = self._get_stddevs(C, stddev_types, sites.vs30.shape[0])\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_distance_scaling_term(self, C, mag, rrup):\n        return (C[\"r1\"] + C[\"r2\"] * mag) * np.log10(rrup + C[\"r3\"])", "response": "Returns the distance scaling term defined in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the style of faulting term in the given attribute.", "response": "def _get_style_of_faulting_term(self, C, rake):\n        \"\"\"\n        Returns the style of faulting term. Cauzzi et al. determind SOF from\n        the plunge of the B-, T- and P-axes. For consistency with existing\n        GMPEs the Wells & Coppersmith model is preferred\n        \"\"\"\n        if rake > -150.0 and rake <= -30.0:\n            return C['fN']\n        elif rake > 30.0 and rake <= 150.0:\n            return C['fR']\n        else:\n            return C['fSS']"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_mean(self, C, rup, dists, sites, imt):\n        mean = (self._get_magnitude_scaling_term(C, rup.mag) +\n                self._get_distance_scaling_term(C, rup.mag, dists.rrup) +\n                self._get_site_amplification_term(C, sites.vs30))\n        # convert from cm/s**2 to g for SA and from m/s**2 to g for PGA (PGV\n        # is already in cm/s) and also convert from base 10 to base e.\n        if imt.name == \"PGA\":\n            mean = np.log((10 ** mean) * ((2 * np.pi / 0.01) ** 2) *\n                          1e-2 / g)\n        elif imt.name == \"SA\":\n            mean = np.log((10 ** mean) * ((2 * np.pi / imt.period) ** 2) *\n                          1e-2 / g)\n        else:\n            mean = np.log(10 ** mean)\n\n        return mean", "response": "Compute the mean ground motion acceleration and velocity for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the site amplification term on the basis of Eurocode 8 site class", "response": "def _get_site_amplification_term(self, C, vs30):\n        \"\"\"\n        Returns the site amplification term on the basis of Eurocode 8\n        site class\n        \"\"\"\n        s_b, s_c, s_d = self._get_site_dummy_variables(vs30)\n        return (C[\"sB\"] * s_b) + (C[\"sC\"] * s_c) + (C[\"sD\"] * s_d)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_site_dummy_variables(self, vs30):\n        s_b = np.zeros_like(vs30)\n        s_c = np.zeros_like(vs30)\n        s_d = np.zeros_like(vs30)\n        s_b[np.logical_and(vs30 >= 360., vs30 < 800.)] = 1.0\n        s_c[np.logical_and(vs30 >= 180., vs30 < 360.)] = 1.0\n        s_d[vs30 < 180] = 1.0\n        return s_b, s_c, s_d", "response": "Returns the Eurocode 8 site class dummy variable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the recurrence model for the given settings as a dict.", "response": "def get_recurrence(self, config):\n        '''\n        Calculates the recurrence model for the given settings as\n        an instance of the openquake.hmtk.models.IncrementalMFD\n\n        :param dict config:\n            Configuration settings of the magnitude frequency distribution.\n        '''\n        model = MFD_MAP[config['Model_Name']]()\n        model.setUp(config)\n        model.get_mmax(config, self.msr, self.rake, self.area)\n        model.mmax = model.mmax + (self.msr_sigma * model.mmax_sigma)\n        # As the Anderson & Luco arbitrary model requires the input of the\n        # displacement to length ratio\n\n        if 'AndersonLucoAreaMmax' in config['Model_Name']:\n            if not self.disp_length_ratio:\n                # If not defined then default to 1.25E-5\n                self.disp_length_ratio = 1.25E-5\n            min_mag, bin_width, occur_rates = model.get_mfd(\n                self.slip,\n                self.area, self.shear_modulus, self.disp_length_ratio)\n\n        else:\n            min_mag, bin_width, occur_rates = model.get_mfd(self.slip,\n                                                            self.area,\n                                                            self.shear_modulus)\n\n        self.recurrence = IncrementalMFD(min_mag, bin_width, occur_rates)\n        self.magnitudes = min_mag + np.cumsum(\n            bin_width *\n            np.ones(len(occur_rates), dtype=float)) - bin_width\n        self.max_mag = np.max(self.magnitudes)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tectonic_regionalisation(self, regionalisation, region_type=None):\n        '''\n        Defines the tectonic region and updates the shear modulus,\n        magnitude scaling relation and displacement to length ratio using\n        the regional values, if not previously defined for the fault\n\n        :param regionalistion:\n            Instance of the :class:\n            openquake.hmtk.faults.tectonic_regionalisaion.TectonicRegionalisation\n        :param str region_type:\n            Name of the region type - if not in regionalisation an error will\n            be raised\n        '''\n        if region_type:\n            self.trt = region_type\n        if not self.trt in regionalisation.key_list:\n            raise ValueError('Tectonic region classification missing or '\n                             'not defined in regionalisation')\n\n        for iloc, key_val in enumerate(regionalisation.key_list):\n            if self.trt in key_val:\n                self.regionalisation = regionalisation.regionalisation[iloc]\n\n                # Update undefined shear modulus from tectonic regionalisation\n                if not self.shear_modulus:\n                    self.shear_modulus = self.regionalisation.shear_modulus\n                # Update undefined scaling relation from tectonic\n                # regionalisation\n                if not self.msr:\n                    self.msr = self.regionalisation.scaling_rel\n                # Update undefined displacement to length ratio from tectonic\n                # regionalisation\n                if not self.disp_length_ratio:\n                    self.disp_length_ratio = \\\n                        self.regionalisation.disp_length_ratio\n                break\n        return", "response": "Returns the tectonic regionalisation for the given region type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting earthquakes within a specied distance of the fault.", "response": "def select_catalogue(self, selector, distance, distance_metric=\"rupture\",\n                         upper_eq_depth=None, lower_eq_depth=None):\n        \"\"\"\n        Select earthquakes within a specied distance of the fault\n        \"\"\"\n        if selector.catalogue.get_number_events() < 1:\n            raise ValueError('No events found in catalogue!')\n\n        # rupture metric is selected\n        if ('rupture' in distance_metric):\n            # Use rupture distance\n            self.catalogue = selector.within_rupture_distance(\n                self.geometry.surface,\n                distance,\n                upper_depth=upper_eq_depth,\n                lower_depth=lower_eq_depth)\n        else:\n            # Use Joyner-Boore distance\n            self.catalogue = selector.within_joyner_boore_distance(\n                self.geometry.surface,\n                distance,\n                upper_depth=upper_eq_depth,\n                lower_depth=lower_eq_depth)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a branching index for the current branching level.", "response": "def _generate_branching_index(self):\n        '''\n        Generates a branching index (i.e. a list indicating the number of\n        branches in each branching level. Current branching levels are:\n        1) Slip\n        2) MSR\n        3) Shear Modulus\n        4) DLR\n        5) MSR_Sigma\n        6) Config\n\n        :returns:\n            * branch_index - A 2-D numpy.ndarray where each row is a pointer\n            to a particular combination of values\n            * number_branches - Total number of branches (int)\n\n        '''\n        branch_count = np.array([len(self.slip),\n                                 len(self.msr),\n                                 len(self.shear_modulus),\n                                 len(self.disp_length_ratio),\n                                 len(self.msr_sigma),\n                                 len(self.config)])\n        n_levels = len(branch_count)\n        number_branches = np.prod(branch_count)\n        branch_index = np.zeros([number_branches, n_levels], dtype=int)\n        cumval = 1\n        dstep = 1E-9\n        for iloc in range(0, n_levels):\n            idx = np.linspace(0.,\n                              float(branch_count[iloc]) - dstep,\n                              number_branches // cumval)\n            branch_index[:, iloc] = np.reshape(np.tile(idx, [cumval, 1]),\n                                               number_branches)\n            cumval *= branch_count[iloc]\n\n        return branch_index.tolist(), number_branches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_config_set(self, config):\n        '''\n        Generates a list of magnitude frequency distributions and renders as\n        a tuple\n\n        :param dict/list config:\n            Configuration paramters of magnitude frequency distribution\n        '''\n        if isinstance(config, dict):\n            # Configuration list contains only one element\n            self.config = [(config, 1.0)]\n        elif isinstance(config, list):\n            # Multiple configurations with correscponding weights\n            total_weight = 0.\n            self.config = []\n            for params in config:\n                weight = params['Model_Weight']\n                total_weight += params['Model_Weight']\n                self.config.append((params, weight))\n            if fabs(total_weight - 1.0) > 1E-7:\n                raise ValueError('MFD config weights do not sum to 1.0 for '\n                                 'fault %s' % self.id)\n        else:\n            raise ValueError('MFD config must be input as dictionary or list!')", "response": "Generates a list of magnitude frequency distributions and renders as\n        a tuple"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the recurrence models for the given MFD and returns the corresponding set of tuples.", "response": "def generate_recurrence_models(\n            self, collapse=False, bin_width=0.1,\n            config=None, rendered_msr=None):\n        '''\n        Iterates over the lists of values defining epistemic uncertainty\n        in the parameters and calculates the corresponding recurrence model\n        At present epistemic uncertainty is supported for: 1) slip rate,\n        2) magnitude scaling relation, 3) shear modulus, 4) displacement\n        to length ratio) and 5) recurrence model.\n\n        :param list config:\n            List of MFD model configurations\n        :param bool collapse:\n            Boolean flag indicating whether to collapse the logic tree branches\n        :param float bin_width:\n            If collapsing the logic tree branches the reference mfd must be\n            defined. The minimum and maximum magnitudes are updated from the\n            model, but the bin width must be specified here\n        :param list/dict config:\n            Configuration (or sets of configurations) of the recurrence\n            calculations\n        :param rendered_msr:\n            If collapsing the logic tree branches a resulting magnitude\n            scaling relation must be defined as instance of\n            :class: openquake.hazardlib.scalerel.base.BaseASR\n        '''\n        if collapse and not rendered_msr:\n            raise ValueError('Collapsing logic tree branches requires input '\n                             'of a single msr for rendering sources')\n\n        # Generate a set of tuples with corresponding weights\n        if config is not None:\n            self.generate_config_set(config)\n        if not isinstance(self.config, list):\n            raise ValueError('MFD configuration missing or incorrectly '\n                             'formatted')\n\n        # Generate the branching index\n        branch_index, _number_branches = self._generate_branching_index()\n        mmin = np.inf\n        mmax = -np.inf\n        for idx in branch_index:\n            tuple_list = []\n            # Get slip\n            tuple_list.append(self.slip[idx[0]])\n            # Get msr\n            tuple_list.append(self.msr[idx[1]])\n            # Get shear modulus\n            tuple_list.append(self.shear_modulus[idx[2]])\n            # Get displacement length ratio\n            tuple_list.append(self.disp_length_ratio[idx[3]])\n            # Get msr sigma\n            tuple_list.append(self.msr_sigma[idx[4]])\n            # Get config\n            tuple_list.append(self.config[idx[5]])\n            # Calculate branch weight as product of tuple weights\n            branch_weight = np.prod(np.array([val[1] for val in tuple_list]))\n            # Instantiate recurrence model\n            model = RecurrenceBranch(self.area,\n                                     tuple_list[0][0],\n                                     tuple_list[1][0],\n                                     self.rake,\n                                     tuple_list[2][0],\n                                     tuple_list[3][0],\n                                     tuple_list[4][0],\n                                     weight=branch_weight)\n            model.get_recurrence(tuple_list[5][0])\n            self.mfd_models.append(model)\n            # Update the total minimum and maximum magnitudes for the fault\n            if model.recurrence.min_mag < mmin:\n                mmin = model.recurrence.min_mag\n            if np.max(model.magnitudes) > mmax:\n                mmax = np.max(model.magnitudes)\n        if collapse:\n            self.mfd = ([self.collapse_branches(mmin, bin_width, mmax)],\n                        [1.0],\n                        [rendered_msr])\n        else:\n            mfd_mods = []\n            mfd_wgts = []\n            mfd_msr = []\n            for model in self.mfd_models:\n                mfd_mods.append(IncrementalMFD(model.recurrence.min_mag,\n                                               model.recurrence.bin_width,\n                                               model.recurrence.occur_rates))\n                mfd_wgts.append(model.weight)\n                mfd_msr.append(model.msr)\n            self.mfd = (mfd_mods, mfd_wgts, mfd_msr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collapse_branches(self, mmin, bin_width, mmax):\n        '''\n        Collapse the logic tree branches into a single IncrementalMFD\n\n        :param float mmin:\n            Minimum magnitude of reference mfd\n        :param float bin_width:\n            Bin width of reference mfd\n        :param float mmax:\n            Maximum magnitude of reference mfd\n\n        :returns:\n            :class: openquake.hmtk.models.IncrementalMFD\n        '''\n        master_mags = np.arange(mmin, mmax + (bin_width / 2.), bin_width)\n        master_rates = np.zeros(len(master_mags), dtype=float)\n        for model in self.mfd_models:\n            id0 = np.logical_and(\n                master_mags >= np.min(model.magnitudes) - 1E-9,\n                master_mags <= np.max(model.magnitudes) + 1E-9)\n            # Use interpolation in log10-y values\n\n            yvals = np.log10(model.recurrence.occur_rates)\n            interp_y = np.interp(master_mags[id0],\n                                 model.magnitudes,\n                                 yvals)\n            master_rates[id0] = master_rates[id0] + (model.weight *\n                                                     10. ** interp_y)\n        return IncrementalMFD(mmin, bin_width, master_rates)", "response": "Collapse the logic tree branches into a single IncrementalMFD."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attrib(self):\n        return dict([\n            ('id', str(self.id)),\n            ('name', str(self.name)),\n            ('tectonicRegion', str(self.trt)),\n        ])", "response": "General XML element attributes for a seismic source as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict of XML element attributes for this MFD.", "response": "def attrib(self):\n        \"\"\"\n        An dict of XML element attributes for this MFD.\n        \"\"\"\n        return dict([\n            ('aValue', str(self.a_val)),\n            ('bValue', str(self.b_val)),\n            ('minMag', str(self.min_mag)),\n            ('maxMag', str(self.max_mag)),\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict of XML element attributes for this NodalPlane.", "response": "def attrib(self):\n        \"\"\"\n        A dict of XML element attributes for this NodalPlane.\n        \"\"\"\n        return dict([\n            ('probability', str(self.probability)),\n            ('strike', str(self.strike)),\n            ('dip', str(self.dip)),\n            ('rake', str(self.rake)),\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef jbcorrelation(sites_or_distances, imt, vs30_clustering=False):\n        if hasattr(sites_or_distances, 'mesh'):\n            distances = sites_or_distances.mesh.get_distance_matrix()\n        else:\n            distances = sites_or_distances\n\n        # formulae are from page 1700\n        if imt.period < 1:\n            if not vs30_clustering:\n                # case 1, eq. (17)\n                b = 8.5 + 17.2 * imt.period\n            else:\n                # case 2, eq. (18)\n                b = 40.7 - 15.0 * imt.period\n        else:\n            # both cases, eq. (19)\n            b = 22.0 + 3.7 * imt.period\n\n        # eq. (20)\n        return numpy.exp((- 3.0 / b) * distances)", "response": "Returns the Jayaram - Baker correlation model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Heresi - Miranda correlation model for the given sites or distances.", "response": "def hmcorrelation(sites_or_distances, imt, uncertainty_multiplier=0):\n    \"\"\"\n    Returns the Heresi-Miranda correlation model.\n\n    :param sites_or_distances:\n        SiteCollection instance o distance matrix\n    :param imt:\n        Intensity Measure Type (PGA or SA)\n    :param uncertainty_multiplier:\n        Value to be multiplied by the uncertainty in the correlation parameter\n        beta. If uncertainty_multiplier = 0 (default), the median value is\n        used as a constant value.\n    \"\"\"\n    if hasattr(sites_or_distances, 'mesh'):\n        distances = sites_or_distances.mesh.get_distance_matrix()\n    else:\n        distances = sites_or_distances\n\n    period = imt.period\n\n    # Eq. (9)\n    if period < 1.37:\n        Med_b = 4.231 * period * period - 5.180 * period + 13.392\n    else:\n        Med_b = 0.140 * period * period - 2.249 * period + 17.050\n\n    # Eq. (10)\n    Std_b = (4.63e-3 * period*period + 0.028 * period + 0.713)\n\n    # Obtain realization of b\n    if uncertainty_multiplier == 0:\n        beta = Med_b\n    else:\n        beta = numpy.random.lognormal(\n            numpy.log(Med_b), Std_b * uncertainty_multiplier)\n\n    # Eq. (8)\n    return numpy.exp(-numpy.power((distances / beta), 0.55))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying correlation to randomly sampled residuals.", "response": "def apply_correlation(self, sites, imt, residuals, stddev_intra=0):\n        \"\"\"\n        Apply correlation to randomly sampled residuals.\n\n        :param sites:\n            :class:`~openquake.hazardlib.site.SiteCollection` residuals were\n            sampled for.\n        :param imt:\n            Intensity measure type object, see :mod:`openquake.hazardlib.imt`.\n        :param residuals:\n            2d numpy array of sampled residuals, where first dimension\n            represents sites (the length as ``sites`` parameter) and\n            second one represents different realizations (samples).\n        :param stddev_intra:\n            Intra-event standard deviation array. Note that different sites do\n            not necessarily have the same intra-event standard deviation.\n        :returns:\n            Array of the same structure and semantics as ``residuals``\n            but with correlations applied.\n\n        NB: the correlation matrix is cached. It is computed only once\n        per IMT for the complete site collection and then the portion\n        corresponding to the sites is multiplied by the residuals.\n        \"\"\"\n        # intra-event residual for a single relization is a product\n        # of lower-triangle decomposed correlation matrix and vector\n        # of N random numbers (where N is equal to number of sites).\n        # we need to do that multiplication once per realization\n        # with the same matrix and different vectors.\n        try:\n            corma = self.cache[imt]\n        except KeyError:\n            corma = self.get_lower_triangle_correlation_matrix(\n                sites.complete, imt)\n            self.cache[imt] = corma\n        if len(sites.complete) == len(sites):\n            return numpy.dot(corma, residuals)\n        # it is important to allocate little memory, this is why I am\n        # accumulating below; if S is the length of the complete sites\n        # the correlation matrix has shape (S, S) and the residuals (N, s),\n        # where s is the number of samples\n        return numpy.sum(corma[sites.sids, sid] * res\n                         for sid, res in zip(sites.sids, residuals))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_lower_triangle_correlation_matrix(self, sites, imt):\n        return numpy.linalg.cholesky(self._get_correlation_matrix(sites, imt))", "response": "Returns the lower - triangular matrix of the correlation matrix for the given sites and intensity measure type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying correlation to randomly sampled residuals.", "response": "def apply_correlation(self, sites, imt, residuals, stddev_intra):\n        \"\"\"\n        Apply correlation to randomly sampled residuals.\n\n        See Parent function\n        \"\"\"\n        # stddev_intra is repeated if it is only 1 value for all the residuals\n        if stddev_intra.shape[0] == 1:\n            stddev_intra = numpy.matlib.repmat(\n                stddev_intra, len(sites.complete), 1)\n        # Reshape 'stddev_intra' if needed\n        stddev_intra = stddev_intra.squeeze()\n        if not stddev_intra.shape:\n            stddev_intra = stddev_intra[None]\n\n        if self.uncertainty_multiplier == 0:   # No uncertainty\n\n            # residuals were sampled from a normal distribution with\n            # stddev_intra standard deviation. 'residuals_norm' are residuals\n            # normalized, sampled from a standard normal distribution.\n            # For this, every row of 'residuals' (every site) is divided by its\n            # corresponding standard deviation element.\n            residuals_norm = residuals / stddev_intra[sites.sids, None]\n\n            # Lower diagonal of the Cholesky decomposition from/to cache\n            try:\n                cormaLow = self.cache[imt]\n            except KeyError:\n                # Note that instead of computing the whole correlation matrix\n                # corresponding to sites.complete, here we compute only the\n                # correlation matrix corresponding to sites.\n                cormaLow = numpy.linalg.cholesky(\n                       numpy.diag(stddev_intra[sites.sids]) *\n                       self._get_correlation_matrix(sites, imt) *\n                       numpy.diag(stddev_intra[sites.sids]))\n                self.cache[imt] = cormaLow\n\n            # Apply correlation\n            return numpy.dot(cormaLow, residuals_norm)\n\n        else:   # Variability (uncertainty) is included\n            nsim = len(residuals[1])\n            nsites = len(residuals)\n\n            # Re-sample all the residuals\n            residuals_correlated = residuals * 0\n            for isim in range(0, nsim):\n                corma = self._get_correlation_matrix(sites, imt)\n                cov = (numpy.diag(stddev_intra[sites.sids]) * corma *\n                       numpy.diag(stddev_intra[sites.sids]))\n                residuals_correlated[0:, isim] = (\n                    numpy.random.multivariate_normal(\n                        numpy.zeros(nsites), cov, 1))\n\n            return residuals_correlated"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting ebrisk tasks for a single rupture.", "response": "def start_ebrisk(rupgetter, srcfilter, param, monitor):\n    \"\"\"\n    Launcher for ebrisk tasks\n    \"\"\"\n    with monitor('weighting ruptures'):\n        rupgetter.set_weights(srcfilter, param['num_taxonomies'])\n    if rupgetter.weights.sum() <= param['maxweight']:\n        yield ebrisk(rupgetter, srcfilter, param, monitor)\n    else:\n        for rgetter in rupgetter.split(param['maxweight']):\n            yield ebrisk, rgetter, srcfilter, param"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ebrisk(rupgetter, srcfilter, param, monitor):\n    riskmodel = param['riskmodel']\n    E = rupgetter.num_events\n    L = len(riskmodel.lti)\n    N = len(srcfilter.sitecol.complete)\n    e1 = rupgetter.first_event\n    with monitor('getting assets', measuremem=False):\n        with datastore.read(srcfilter.filename) as dstore:\n            assetcol = dstore['assetcol']\n        assets_by_site = assetcol.assets_by_site()\n    A = len(assetcol)\n    getter = getters.GmfGetter(rupgetter, srcfilter, param['oqparam'])\n    with monitor('getting hazard'):\n        getter.init()  # instantiate the computers\n        hazard = getter.get_hazard()  # sid -> (rlzi, sid, eid, gmv)\n    mon_risk = monitor('computing risk', measuremem=False)\n    mon_agg = monitor('aggregating losses', measuremem=False)\n    events = rupgetter.get_eid_rlz()\n    # numpy.testing.assert_equal(events['eid'], sorted(events['eid']))\n    eid2idx = dict(zip(events['eid'], range(e1, e1 + E)))\n    tagnames = param['aggregate_by']\n    shape = assetcol.tagcol.agg_shape((E, L), tagnames)\n    elt_dt = [('eid', U64), ('rlzi', U16), ('loss', (F32, shape[1:]))]\n    if param['asset_loss_table']:\n        alt = numpy.zeros((A, E, L), F32)\n    acc = numpy.zeros(shape, F32)  # shape (E, L, T...)\n    if param['avg_losses']:\n        losses_by_A = numpy.zeros((A, L), F32)\n    else:\n        losses_by_A = 0\n    # NB: IMT-dependent weights are not supported in ebrisk\n    times = numpy.zeros(N)  # risk time per site_id\n    num_events_per_sid = 0\n    epspath = param['epspath']\n    for sid, haz in hazard.items():\n        t0 = time.time()\n        assets_on_sid = assets_by_site[sid]\n        if len(assets_on_sid) == 0:\n            continue\n        num_events_per_sid += len(haz)\n        weights = getter.weights[haz['rlzi'], 0]\n        assets_by_taxo = get_assets_by_taxo(assets_on_sid, epspath)\n        eidx = numpy.array([eid2idx[eid] for eid in haz['eid']]) - e1\n        haz['eid'] = eidx + e1\n        with mon_risk:\n            out = riskmodel.get_output(assets_by_taxo, haz)\n        with mon_agg:\n            for a, asset in enumerate(assets_on_sid):\n                aid = asset['ordinal']\n                tagi = asset[tagnames] if tagnames else ()\n                tagidxs = tuple(idx - 1 for idx in tagi)\n                for lti, lt in enumerate(riskmodel.loss_types):\n                    lratios = out[lt][a]\n                    if lt == 'occupants':\n                        losses = lratios * asset['occupants_None']\n                    else:\n                        losses = lratios * asset['value-' + lt]\n                    if param['asset_loss_table']:\n                        alt[aid, eidx, lti] = losses\n                    acc[(eidx, lti) + tagidxs] += losses\n                    if param['avg_losses']:\n                        losses_by_A[aid, lti] += losses @ weights\n            times[sid] = time.time() - t0\n    if hazard:\n        num_events_per_sid /= len(hazard)\n    with monitor('building event loss table'):\n        elt = numpy.fromiter(\n            ((event['eid'], event['rlz'], losses)\n             for event, losses in zip(events, acc) if losses.sum()), elt_dt)\n        agg = general.AccumDict(accum=numpy.zeros(shape[1:], F32))  # rlz->agg\n        for rec in elt:\n            agg[rec['rlzi']] += rec['loss'] * param['ses_ratio']\n    res = {'elt': elt, 'agg_losses': agg, 'times': times,\n           'events_per_sid': num_events_per_sid}\n    if param['avg_losses']:\n        res['losses_by_A'] = losses_by_A * param['ses_ratio']\n    if param['asset_loss_table']:\n        res['alt_eids'] = alt, events['eid']\n    return res", "response": "Compute the risk model for a single base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_loss_curves_maps(filename, builder, rlzi, monitor):\n    with datastore.read(filename) as dstore:\n        rlzs = dstore['losses_by_event']['rlzi']\n        losses = dstore['losses_by_event'][rlzs == rlzi]['loss']\n    return rlzi, builder.build_curves_maps(losses, rlzi)", "response": "Compute the loss curves maps for a given rlzi"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the minimum and maximum magnitudes", "response": "def get_min_max_mag(self):\n        \"Return the minimum and maximum magnitudes\"\n        mag, num_bins = self._get_min_mag_and_num_bins()\n        return mag, mag + self. bin_width * (num_bins - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the constraints for the object.", "response": "def check_constraints(self):\n        \"\"\"\n        Checks the following constraints:\n\n        * minimum magnitude is positive.\n        * ``b`` value is positive.\n        * characteristic magnitude is positive\n        * characteristic rate is positive\n        * bin width is in the range (0, 0.5] to allow for at least one bin\n          representing the characteristic distribution\n        * characteristic magnitude minus 0.25 (that is the maximum magnitude\n          of the G-R distribution) is greater than the minimum magnitude by at\n          least one magnitude bin.\n        * rate of events at the characteristic magnitude is equal to the\n          rate of events for magnitude equal to m_prime - 1. This is done\n          by asserting the equality (up to 7 digit precision) ::\n\n            10 ** (a_incr - b * (m' - 1)) == char_rate / 0.5\n\n          where ``a_incr`` is the incremental a value obtained from the\n          cumulative a value using the following formula ::\n\n            a_incr = a_val + log10(b_val * ln(10))\n\n          and ``m' - 1 = char_mag - 1.25``\n        \"\"\"\n        if not self.min_mag > 0:\n            raise ValueError('minimum magnitude must be positive')\n\n        if not self.b_val > 0:\n            raise ValueError('b value must be positive')\n\n        if not self.char_mag > 0:\n            raise ValueError('characteristic magnitude must be positive')\n\n        if not self.char_rate > 0:\n            raise ValueError('characteristic rate must be positive')\n\n        if not 0 < self.bin_width <= DELTA_CHAR:\n            err_msg = 'bin width must be in the range (0, %s] to allow for ' \\\n                      'at least one magnitude bin representing the ' \\\n                      'characteristic distribution' % DELTA_CHAR\n            raise ValueError(err_msg)\n\n        if not self.char_mag - DELTA_CHAR / 2 >= self.min_mag + self.bin_width:\n            err_msg = 'Maximum magnitude of the G-R distribution (char_mag ' \\\n                      '- 0.25) must be greater than the minimum magnitude ' \\\n                      'by at least one magnitude bin.'\n            raise ValueError(err_msg)\n\n        a_incr = self.a_val + numpy.log10(self.b_val * numpy.log(10))\n        actual = 10 ** (a_incr - self.b_val * (self.char_mag - 1.25))\n        desired = self.char_rate / DELTA_CHAR\n        if not numpy.allclose(actual, desired, rtol=0.0, atol=1e-07):\n            err_msg = 'Rate of events at the characteristic magnitude is ' \\\n                      'not equal to the rate of events for magnitude equal ' \\\n                      'to char_mag - 1.25'\n            raise ValueError(err_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndefines Youngs and Coppersmith 1985 MFD by constraing cumulative a value and characteristic rate from total moment rate. The cumulative a value and characteristic rate are obtained by solving equations (16) and (17), page 954, for the cumulative rate of events with magnitude greater than the minimum magnitude - N(min_mag) - and the cumulative rate of characteristic earthquakes - N(char_mag). The difference ``N(min_mag) - N(char_mag)`` represents the rate of noncharacteristic, exponentially distributed earthquakes and is used to derive the cumulative a value by solving the following equation :: 10 ** (a_val - b_val * min_mag) - 10 ** (a_val - b_val * (char_mag - 0.25)) = N(min_mag) - N(char_mag) which can be written as :: a_val = log10(N(min_mag) - N(char_mag)) / (10 ** (- b_val * min_mag) - 10 ** (- b_val * (char_mag - 0.25)) In the calculation of N(min_mag) and N(char_mag), the Hanks and Kanamori (1979) formula :: M0 = 10 ** (1.5 * Mw + 9.05) is used to convert moment magnitude (Mw) to seismic moment (M0, Newton \u00d7 m) :param min_mag: The lowest magnitude for the MFD. The first bin in the :meth:`result histogram <get_annual_occurrence_rates>` is aligned to make its left border match this value. :param b_val: The Gutenberg-Richter ``b`` value -- the gradient of the loglinear G-R relationship. :param char_mag: The characteristic magnitude defining the middle point of characteristic distribution. That is the boxcar function representing the characteristic distribution is defined in the range [char_mag - 0.25, char_mag + 0.25]. :param total_moment_rate: Total moment rate in N * m / year. :param bin_width: A positive float value -- the width of a single histogram bin. :returns: An instance of :class:`YoungsCoppersmith1985MFD`. Values for ``min_mag`` and the maximum magnitude (char_mag + 0.25) don't have to be aligned with respect to ``bin_width``. They get rounded accordingly anyway so that both are divisible by ``bin_width`` just before converting a function to a histogram. See :meth:`_get_min_mag_and_num_bins`.", "response": "def from_total_moment_rate(cls, min_mag, b_val, char_mag,\n                               total_moment_rate, bin_width):\n        \"\"\"\n        Define Youngs and Coppersmith 1985 MFD by constraing cumulative a\n        value and characteristic rate from total moment rate.\n        The cumulative a value and characteristic rate are obtained by\n        solving equations (16) and (17), page 954, for the cumulative rate of\n        events with magnitude greater than the minimum magnitude - N(min_mag)\n        - and the cumulative rate of characteristic earthquakes - N(char_mag).\n        The difference ``N(min_mag) - N(char_mag)`` represents the rate of\n        noncharacteristic, exponentially distributed earthquakes and is used\n        to derive the cumulative a value by solving the following equation ::\n\n            10 ** (a_val - b_val * min_mag) -\n            10 ** (a_val - b_val * (char_mag - 0.25))\n            = N(min_mag) - N(char_mag)\n\n        which can be written as ::\n\n            a_val =\n            log10(N(min_mag) - N(char_mag)) /\n            (10 ** (- b_val * min_mag) - 10 ** (- b_val * (char_mag - 0.25))\n\n        In the calculation of N(min_mag) and N(char_mag), the Hanks and\n        Kanamori (1979) formula ::\n\n            M0 = 10 ** (1.5 * Mw + 9.05)\n\n        is used to convert moment magnitude (Mw) to seismic moment (M0,\n        Newton \u00d7 m)\n\n        :param min_mag:\n            The lowest magnitude for the MFD. The first bin in the\n            :meth:`result histogram <get_annual_occurrence_rates>` is aligned\n            to make its left border match this value.\n        :param b_val:\n            The Gutenberg-Richter ``b`` value -- the gradient of the loglinear\n            G-R relationship.\n        :param char_mag:\n            The characteristic magnitude defining the middle point of\n            characteristic distribution. That is the boxcar function\n            representing the characteristic distribution is defined in the\n            range [char_mag - 0.25, char_mag + 0.25].\n        :param total_moment_rate:\n            Total moment rate in N * m / year.\n        :param bin_width:\n            A positive float value -- the width of a single histogram bin.\n        :returns:\n            An instance of :class:`YoungsCoppersmith1985MFD`.\n\n        Values for ``min_mag`` and the maximum magnitude (char_mag + 0.25)\n        don't have to be aligned with respect to ``bin_width``. They get\n        rounded accordingly anyway so that both are divisible by ``bin_width``\n        just before converting a function to a histogram.\n        See :meth:`_get_min_mag_and_num_bins`.\n        \"\"\"\n        beta = b_val * numpy.log(10)\n        mu = char_mag + DELTA_CHAR / 2\n        m0 = min_mag\n\n        # seismic moment (in Nm) for the maximum magnitude\n        c = 1.5\n        d = 9.05\n        mo_u = 10 ** (c * mu + d)\n\n        # equations (16) and (17) solved for N(min_mag) and N(char_mag)\n        c1 = numpy.exp(-beta * (mu - m0 - 0.5))\n        c2 = numpy.exp(-beta * (mu - m0 - 1.5))\n        c3 = beta * c2 / (2 * (1 - c1) + beta * c2)\n        c4 = (b_val * (10 ** (-c / 2)) / (c - b_val)) + \\\n             (b_val * numpy.exp(beta) * (1 - (10 ** (-c / 2))) / c)\n        n_min_mag = (1 - c1) * total_moment_rate / ((1 - c3) * c1 * mo_u * c4)\n        n_char_mag = c3 * n_min_mag\n\n        a_val = numpy.log10(\n            (n_min_mag - n_char_mag) /\n            (10 ** (- b_val * min_mag) - 10 ** (- b_val * (char_mag - 0.25)))\n        )\n\n        return cls(min_mag, a_val, b_val, char_mag, n_char_mag, bin_width)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefine Youngs and Coppersmith 1985 MFD by constraing cumulative a value from characteristic rate. The cumulative a value is obtained by making use of the property that the rate of events at m' - 1 must be equal to the rate at the characteristic magnitude, and therefore by first computing the incremental a value, using the following equation:: 10 ** (a_incr - b_val * (m_prime - 1)) == char_rate / 0.5 where ``m' - 1 = char_mag - 1.25``. The cumulative a value is then obtained as :: a_val = a_incr - log10(b_val * ln(10)) :param min_mag: The lowest magnitude for the MFD. The first bin in the :meth:`result histogram <get_annual_occurrence_rates>` is aligned to make its left border match this value. :param b_val: The Gutenberg-Richter ``b`` value -- the gradient of the loglinear G-R relationship. :param char_mag: The characteristic magnitude defining the middle point of characteristic distribution. That is the boxcar function representing the characteristic distribution is defined in the range [char_mag - 0.25, char_mag + 0.25]. :param char_rate: The characteristic rate associated to the characteristic magnitude, to be distributed over the domain of the boxcar function representing the characteristic distribution (that is \u03bb_char = char_rate / 0.5) :param bin_width: A positive float value -- the width of a single histogram bin. :returns: An instance of :class:`YoungsCoppersmith1985MFD`. Values for ``min_mag`` and the maximum magnitude (char_mag + 0.25) don't have to be aligned with respect to ``bin_width``. They get rounded accordingly anyway so that both are divisible by ``bin_width`` just before converting a function to a histogram. See :meth:`_get_min_mag_and_num_bins`.", "response": "def from_characteristic_rate(cls, min_mag, b_val, char_mag, char_rate,\n                                 bin_width):\n        \"\"\"\n        Define Youngs and Coppersmith 1985 MFD by constraing cumulative a\n        value from characteristic rate.\n        The cumulative a value is obtained by making use of the property that\n        the rate of events at m' - 1 must be equal to the rate at the\n        characteristic magnitude, and therefore by first computing the\n        incremental a value, using the following equation::\n\n            10 ** (a_incr - b_val * (m_prime - 1)) == char_rate / 0.5\n\n        where ``m' - 1 = char_mag - 1.25``.\n        The cumulative a value is then obtained as ::\n\n            a_val = a_incr - log10(b_val * ln(10))\n\n        :param min_mag:\n            The lowest magnitude for the MFD. The first bin in the\n            :meth:`result histogram <get_annual_occurrence_rates>` is aligned\n            to make its left border match this value.\n        :param b_val:\n            The Gutenberg-Richter ``b`` value -- the gradient of the loglinear\n            G-R relationship.\n        :param char_mag:\n            The characteristic magnitude defining the middle point of\n            characteristic distribution. That is the boxcar function\n            representing the characteristic distribution is defined in the\n            range [char_mag - 0.25, char_mag + 0.25].\n        :param char_rate:\n            The characteristic rate associated to the characteristic magnitude,\n            to be distributed over the domain of the boxcar function\n            representing the characteristic distribution (that is \u03bb_char =\n            char_rate / 0.5)\n        :param bin_width:\n            A positive float value -- the width of a single histogram bin.\n        :returns:\n            An instance of :class:`YoungsCoppersmith1985MFD`.\n\n        Values for ``min_mag`` and the maximum magnitude (char_mag + 0.25)\n        don't have to be aligned with respect to ``bin_width``. They get\n        rounded accordingly anyway so that both are divisible by ``bin_width``\n        just before converting a function to a histogram.\n        See :meth:`_get_min_mag_and_num_bins`.\n        \"\"\"\n        a_incr = b_val * (char_mag - 1.25) + numpy.log10(char_rate /\n                                                         DELTA_CHAR)\n        a_val = a_incr - numpy.log10(b_val * numpy.log(10))\n\n        return cls(min_mag, a_val, b_val, char_mag, char_rate, bin_width)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_rate(self, mag):\n        mag_lo = mag - self.bin_width / 2.0\n        mag_hi = mag + self.bin_width / 2.0\n\n        if mag >= self.min_mag and mag < self.char_mag - DELTA_CHAR / 2:\n            # return rate according to exponential distribution\n            return (10 ** (self.a_val - self.b_val * mag_lo)\n                    - 10 ** (self.a_val - self.b_val * mag_hi))\n        else:\n            # return characteristic rate (distributed over the characteristic\n            # range) for the given bin width\n            return (self.char_rate / DELTA_CHAR) * self.bin_width", "response": "Calculate and return the annual occurrence rate for a specific bin."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates the number of bins in the histogram and returns it along with the first bin center value and the number of bins in the histogram.", "response": "def _get_min_mag_and_num_bins(self):\n        \"\"\"\n        Estimate the number of bins in the histogram and return it along with\n        the first bin center value.\n\n        Rounds ``min_mag`` and ``max_mag`` with respect to ``bin_width`` to\n        make the distance between them include integer number of bins.\n\n        :returns:\n            A tuple of 2 items: first bin center, and total number of bins.\n        \"\"\"\n        min_mag = round(self.min_mag / self.bin_width) * self.bin_width\n        max_mag = (round((self.char_mag + DELTA_CHAR / 2) /\n                   self.bin_width) * self.bin_width)\n        min_mag += self.bin_width / 2.0\n        max_mag -= self.bin_width / 2.0\n        # here we use math round on the result of division and not just\n        # cast it to integer because for some magnitude values that can't\n        # be represented as an IEEE 754 double precisely the result can\n        # look like 7.999999999999 which would become 7 instead of 8\n        # being naively casted to int so we would lose the last bin.\n        num_bins = int(round((max_mag - min_mag) / self.bin_width)) + 1\n        return min_mag, num_bins"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating and return the annual occurrence rates histogram.", "response": "def get_annual_occurrence_rates(self):\n        \"\"\"\n        Calculate and return the annual occurrence rates histogram.\n\n        :returns:\n            See :meth:\n            `openquake.hazardlib.mfd.base.BaseMFD.get_annual_occurrence_rates`.\n        \"\"\"\n        mag, num_bins = self._get_min_mag_and_num_bins()\n        rates = []\n        for i in range(num_bins):\n            rate = self._get_rate(mag)\n            rates.append((mag, rate))\n            mag += self.bin_width\n        return rates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_geometry(self, input_geometry, upper_depth, lower_depth):\n        '''\n        If geometry is defined as a numpy array then create instance of\n        nhlib.geo.polygon.Polygon class, otherwise if already instance of class\n        accept class\n\n        :param input_geometry:\n            Input geometry (polygon) as either\n            i) instance of nhlib.geo.polygon.Polygon class\n            ii) numpy.ndarray [Longitude, Latitude]\n\n        :param float upper_depth:\n            Upper seismogenic depth (km)\n\n        :param float lower_depth:\n            Lower seismogenic depth (km)\n        '''\n        self._check_seismogenic_depths(upper_depth, lower_depth)\n\n        # Check/create the geometry class\n        if not isinstance(input_geometry, Polygon):\n            if not isinstance(input_geometry, np.ndarray):\n                raise ValueError('Unrecognised or unsupported geometry '\n                                 'definition')\n\n            if np.shape(input_geometry)[0] < 3:\n                raise ValueError('Incorrectly formatted polygon geometry -'\n                                 ' needs three or more vertices')\n            geometry = []\n            for row in input_geometry:\n                geometry.append(Point(row[0], row[1], self.upper_depth))\n            self.geometry = Polygon(geometry)\n        else:\n            self.geometry = input_geometry", "response": "Create the geometry of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting the catalogue of earthquakes attributable to the source.", "response": "def select_catalogue(self, selector, distance=None):\n        '''\n        Selects the catalogue of earthquakes attributable to the source\n\n        :param selector:\n            Populated instance of openquake.hmtk.seismicity.selector.CatalogueSelector\n            class\n        :param float distance:\n            Distance (in km) to extend or contract (if negative) the zone for\n            selecting events\n        '''\n        if selector.catalogue.get_number_events() < 1:\n            raise ValueError('No events found in catalogue!')\n\n        self.catalogue = selector.within_polygon(self.geometry,\n                                                 distance,\n                                                 upper_depth=self.upper_depth,\n                                                 lower_depth=self.lower_depth)\n        if self.catalogue.get_number_events() < 5:\n            # Throw a warning regarding the small number of earthquakes in\n            # the source!\n            warnings.warn('Source %s (%s) has fewer than 5 events'\n                          % (self.id, self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an instance of the oqhazardlib. source. area. AreaSource class based on the current object properties.", "response": "def create_oqhazardlib_source(self, tom, mesh_spacing, area_discretisation,\n                                  use_defaults=False):\n        \"\"\"\n        Converts the source model into an instance of the :class:\n        openquake.hazardlib.source.area.AreaSource\n\n        :param tom:\n            Temporal Occurrence model as instance of :class:\n            openquake.hazardlib.tom.TOM\n        :param float mesh_spacing:\n            Mesh spacing\n        \"\"\"\n        if not self.mfd:\n            raise ValueError(\"Cannot write to hazardlib without MFD\")\n        return AreaSource(\n            self.id,\n            self.name,\n            self.trt,\n            self.mfd,\n            mesh_spacing,\n            conv.mag_scale_rel_to_hazardlib(self.mag_scale_rel, use_defaults),\n            conv.render_aspect_ratio(self.rupt_aspect_ratio, use_defaults),\n            tom,\n            self.upper_depth,\n            self.lower_depth,\n            conv.npd_to_pmf(self.nodal_plane_dist, use_defaults),\n            conv.hdd_to_pmf(self.hypo_depth_dist, use_defaults),\n            self.geometry,\n            area_discretisation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the data for the current object.", "response": "def get_data(self):\n        \"\"\"\n        :returns:\n            an array of dtype perf_dt, with the information\n            of the monitor (operation, time_sec, memory_mb, counts);\n            the lenght of the array can be 0 (for counts=0) or 1 (otherwise).\n        \"\"\"\n        data = []\n        if self.counts:\n            time_sec = self.duration\n            memory_mb = self.mem / 1024. / 1024. if self.measuremem else 0\n            data.append((self.operation, time_sec, memory_mb, self.counts))\n        return numpy.array(data, perf_dt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nflush the monitor and returns the data.", "response": "def flush(self):\n        \"\"\"\n        Save the measurements on the performance file (or on stdout)\n        \"\"\"\n        if not self._flush:\n            raise RuntimeError(\n                'Monitor(%r).flush() must not be called in a worker' %\n                self.operation)\n        for child in self.children:\n            child.hdf5 = self.hdf5\n            child.flush()\n        data = self.get_data()\n        if len(data) == 0:  # no information\n            return []\n        elif self.hdf5:\n            hdf5.extend(self.hdf5['performance_data'], data)\n\n        # reset monitor\n        self.duration = 0\n        self.mem = 0\n        self.counts = 0\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new monitor usable for a different operation.", "response": "def new(self, operation='no operation', **kw):\n        \"\"\"\n        Return a copy of the monitor usable for a different operation.\n        \"\"\"\n        self_vars = vars(self).copy()\n        del self_vars['operation']\n        del self_vars['children']\n        del self_vars['counts']\n        del self_vars['_flush']\n        new = self.__class__(operation)\n        vars(new).update(self_vars)\n        vars(new).update(kw)\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a site collection from a shakemap array.", "response": "def from_shakemap(cls, shakemap_array):\n        \"\"\"\n        Build a site collection from a shakemap array\n        \"\"\"\n        self = object.__new__(cls)\n        self.complete = self\n        n = len(shakemap_array)\n        dtype = numpy.dtype([(p, site_param_dt[p])\n                             for p in 'sids lon lat depth vs30'.split()])\n        self.array = arr = numpy.zeros(n, dtype)\n        arr['sids'] = numpy.arange(n, dtype=numpy.uint32)\n        arr['lon'] = shakemap_array['lon']\n        arr['lat'] = shakemap_array['lat']\n        arr['depth'] = numpy.zeros(n)\n        arr['vs30'] = shakemap_array['vs30']\n        arr.flags.writeable = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_points(cls, lons, lats, depths=None, sitemodel=None,\n                    req_site_params=()):\n        \"\"\"\n        Build the site collection from\n\n        :param lons:\n            a sequence of longitudes\n        :param lats:\n            a sequence of latitudes\n        :param depths:\n            a sequence of depths (or None)\n        :param sitemodel:\n            None or an object containing site parameters as attributes\n        :param req_site_params:\n            a sequence of required site parameters, possibly empty\n        \"\"\"\n        assert len(lons) < U32LIMIT, len(lons)\n        if depths is None:\n            depths = numpy.zeros(len(lons))\n        assert len(lons) == len(lats) == len(depths), (len(lons), len(lats),\n                                                       len(depths))\n        self = object.__new__(cls)\n        self.complete = self\n        req = ['sids', 'lon', 'lat', 'depth'] + sorted(\n            par for par in req_site_params if par not in ('lon', 'lat'))\n        if 'vs30' in req and 'vs30measured' not in req:\n            req.append('vs30measured')\n        self.dtype = numpy.dtype([(p, site_param_dt[p]) for p in req])\n        self.array = arr = numpy.zeros(len(lons), self.dtype)\n        arr['sids'] = numpy.arange(len(lons), dtype=numpy.uint32)\n        arr['lon'] = fix_lon(numpy.array(lons))\n        arr['lat'] = numpy.array(lats)\n        arr['depth'] = numpy.array(depths)\n        if sitemodel is None:\n            pass\n        elif hasattr(sitemodel, 'reference_vs30_value'):\n            # sitemodel is actually an OqParam instance\n            self._set('vs30', sitemodel.reference_vs30_value)\n            self._set('vs30measured',\n                      sitemodel.reference_vs30_type == 'measured')\n            self._set('z1pt0', sitemodel.reference_depth_to_1pt0km_per_sec)\n            self._set('z2pt5', sitemodel.reference_depth_to_2pt5km_per_sec)\n            self._set('siteclass', sitemodel.reference_siteclass)\n        else:\n            for name in sitemodel.dtype.names:\n                if name not in ('lon', 'lat'):\n                    self._set(name, sitemodel[name])\n        return self", "response": "Build the site collection from a set of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filtered(self, indices):\n        if indices is None or len(indices) == len(self):\n            return self\n        new = object.__new__(self.__class__)\n        indices = numpy.uint32(sorted(indices))\n        new.array = self.array[indices]\n        new.complete = self.complete\n        return new", "response": "Returns a new SiteCollection instance with only the specified indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_complete(self):\n        # reset the site indices from 0 to N-1 and set self.complete to self\n        self.array['sids'] = numpy.arange(len(self), dtype=numpy.uint32)\n        self.complete = self", "response": "Turn the site collection into a complete one"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_in_tiles(self, hint):\n        tiles = []\n        for seq in split_in_blocks(range(len(self)), hint or 1):\n            sc = SiteCollection.__new__(SiteCollection)\n            sc.array = self.array[numpy.array(seq, int)]\n            tiles.append(sc)\n        return tiles", "response": "Split a SiteCollection into a set of tiles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting the set into two sets of close sites and far_sites.", "response": "def split(self, location, distance):\n        \"\"\"\n        :returns: (close_sites, far_sites)\n        \"\"\"\n        if distance is None:  # all close\n            return self, None\n        close = location.distance_to_mesh(self) < distance\n        return self.filter(close), self.filter(~close)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter(self, mask):\n        assert len(mask) == len(self), (len(mask), len(self))\n        if mask.all():\n            # all sites satisfy the filter, return\n            # this collection unchanged\n            return self\n        if not mask.any():\n            # no sites pass the filter, return None\n            return None\n        # extract indices of Trues from the mask\n        indices, = mask.nonzero()\n        return self.filtered(indices)", "response": "Create a new SiteCollection with only a subset of sites."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new SiteCollection with only the sites in this collection that are within the given region.", "response": "def within(self, region):\n        \"\"\"\n        :param region: a shapely polygon\n        :returns: a filtered SiteCollection of sites within the region\n        \"\"\"\n        mask = numpy.array([\n            geometry.Point(rec['lon'], rec['lat']).within(region)\n            for rec in self.array])\n        return self.filter(mask)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a mask of the site IDs within a bounding box", "response": "def within_bbox(self, bbox):\n        \"\"\"\n        :param bbox:\n            a quartet (min_lon, min_lat, max_lon, max_lat)\n        :returns:\n            site IDs within the bounding box\n        \"\"\"\n        min_lon, min_lat, max_lon, max_lat = bbox\n        lons, lats = self.array['lon'], self.array['lat']\n        if cross_idl(lons.min(), lons.max()) or cross_idl(min_lon, max_lon):\n            lons = lons % 360\n            min_lon, max_lon = min_lon % 360, max_lon % 360\n        mask = (min_lon < lons) * (lons < max_lon) * \\\n               (min_lat < lats) * (lats < max_lat)\n        return mask.nonzero()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef point_at(self, horizontal_distance, vertical_increment, azimuth):\n        lon, lat = geodetic.point_at(self.longitude, self.latitude,\n                                     azimuth, horizontal_distance)\n        return Point(lon, lat, self.depth + vertical_increment)", "response": "Compute the point at the given horizontal vertical distances and azimuth from this point."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the azimuth between this point and the given point.", "response": "def azimuth(self, point):\n        \"\"\"\n        Compute the azimuth (in decimal degrees) between this point\n        and the given point.\n\n        :param point:\n            Destination point.\n        :type point:\n            Instance of :class:`Point`\n        :returns:\n            The azimuth, value in a range ``[0, 360)``.\n        :rtype:\n            float\n        \"\"\"\n        return geodetic.azimuth(self.longitude, self.latitude,\n                                point.longitude, point.latitude)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef distance(self, point):\n        return geodetic.distance(self.longitude, self.latitude, self.depth,\n                                 point.longitude, point.latitude, point.depth)", "response": "Compute the distance between this point and the given point."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distance_to_mesh(self, mesh, with_depths=True):\n        if with_depths:\n            if mesh.depths is None:\n                mesh_depths = numpy.zeros_like(mesh.lons)\n            else:\n                mesh_depths = mesh.depths\n            return geodetic.distance(self.longitude, self.latitude, self.depth,\n                                     mesh.lons, mesh.lats, mesh_depths)\n        else:\n            return geodetic.geodetic_distance(self.longitude, self.latitude,\n                                              mesh.lons, mesh.lats)", "response": "Compute distance between this point and each point of a mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the set of points equally spaced between this point and the given point.", "response": "def equally_spaced_points(self, point, distance):\n        \"\"\"\n        Compute the set of points equally spaced between this point\n        and the given point.\n\n        :param point:\n            Destination point.\n        :type point:\n            Instance of :class:`Point`\n        :param distance:\n            Distance between points (in km).\n        :type distance:\n            float\n        :returns:\n            The list of equally spaced points.\n        :rtype:\n            list of :class:`Point` instances\n        \"\"\"\n        lons, lats, depths = geodetic.intervals_between(\n            self.longitude, self.latitude, self.depth,\n            point.longitude, point.latitude, point.depth,\n            distance)\n        return [Point(lons[i], lats[i], depths[i]) for i in range(len(lons))]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a shapely polygon from this point and a radius centered in the point.", "response": "def to_polygon(self, radius):\n        \"\"\"\n        Create a circular polygon with specified radius centered in the point.\n\n        :param radius:\n            Required radius of a new polygon, in km.\n        :returns:\n            Instance of :class:`~openquake.hazardlib.geo.polygon.Polygon` that\n            approximates a circle around the point with specified radius.\n        \"\"\"\n        assert radius > 0\n        # avoid circular imports\n        from openquake.hazardlib.geo.polygon import Polygon\n\n        # get a projection that is centered in the point\n        proj = geo_utils.OrthographicProjection(\n            self.longitude, self.longitude, self.latitude, self.latitude)\n        \n        # create a shapely object from a projected point coordinates,\n        # which are supposedly (0, 0)\n        point = shapely.geometry.Point(*proj(self.longitude, self.latitude))\n    \n        # extend the point to a shapely polygon using buffer()\n        # and create openquake.hazardlib.geo.polygon.Polygon object from it\n        return Polygon._from_2d(point.buffer(radius), proj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef closer_than(self, mesh, radius):\n        dists = geodetic.distance(self.longitude, self.latitude, self.depth,\n                                  mesh.lons, mesh.lats,\n                                  0 if mesh.depths is None else mesh.depths)\n        return dists <= radius", "response": "Check if proximity of points in the mesh is not longer than radius km."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef source_model_info(nodes):\n    c = collections.Counter()\n    for node in nodes:\n        for src_group in node:\n            trt = src_group['tectonicRegion']\n            for src in src_group:\n                src_class = src.tag.split('}')[1]\n                c[trt, src_class] += 1\n    trts, classes = zip(*c)\n    trts = sorted(set(trts))\n    classes = sorted(set(classes))\n    dtlist = [('TRT', (bytes, 30))] + [(name, int) for name in classes]\n    out = numpy.zeros(len(trts) + 1, dtlist)  # +1 for the totals\n    for i, trt in enumerate(trts):\n        out[i]['TRT'] = trt\n        for src_class in classes:\n            out[i][src_class] = c[trt, src_class]\n    out[-1]['TRT'] = 'Total'\n    for name in out.dtype.names[1:]:\n        out[-1][name] = out[name][:-1].sum()\n    return rst_table(out)", "response": "Extract information about NRML source models. Returns a table containing the number of TRTs and source classes as rows and source classes as columns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_csm_info(fname):\n    oqparam = readinput.get_oqparam(fname)\n    csm = readinput.get_composite_source_model(oqparam, in_memory=False)\n    print(csm.info)\n    print('See http://docs.openquake.org/oq-engine/stable/'\n          'effective-realizations.html for an explanation')\n    rlzs_assoc = csm.info.get_rlzs_assoc()\n    print(rlzs_assoc)\n    dupl = [(srcs[0]['id'], len(srcs)) for srcs in csm.check_dupl_sources()]\n    if dupl:\n        print(rst_table(dupl, ['source_id', 'multiplicity']))\n    tot, pairs = get_pickled_sizes(rlzs_assoc)\n    print(rst_table(pairs, ['attribute', 'nbytes']))", "response": "Parse the composite source model and print information about its composition and the full logic tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwalks the directory and builds pre - calculation reports for all the the job. ini files found.", "response": "def do_build_reports(directory):\n    \"\"\"\n    Walk the directory and builds pre-calculation reports for all the\n    job.ini files found.\n    \"\"\"\n    for cwd, dirs, files in os.walk(directory):\n        for f in sorted(files):\n            if f in ('job.ini', 'job_h.ini', 'job_haz.ini', 'job_hazard.ini'):\n                job_ini = os.path.join(cwd, f)\n                logging.info(job_ini)\n                try:\n                    reportwriter.build_report(job_ini, cwd)\n                except Exception as e:\n                    logging.error(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint information about the available calculator views exports and extract functions.", "response": "def info(calculators, gsims, views, exports, extracts, parameters,\n         report, input_file=''):\n    \"\"\"\n    Give information. You can pass the name of an available calculator,\n    a job.ini file, or a zip archive with the input files.\n    \"\"\"\n    if calculators:\n        for calc in sorted(base.calculators):\n            print(calc)\n    if gsims:\n        for gs in gsim.get_available_gsims():\n            print(gs)\n    if views:\n        for name in sorted(view):\n            print(name)\n    if exports:\n        dic = groupby(export, operator.itemgetter(0),\n                      lambda group: [r[1] for r in group])\n        n = 0\n        for exporter, formats in dic.items():\n            print(exporter, formats)\n            n += len(formats)\n        print('There are %d exporters defined.' % n)\n    if extracts:\n        for key in extract:\n            func = extract[key]\n            if hasattr(func, '__wrapped__'):\n                fm = FunctionMaker(func.__wrapped__)\n            else:\n                fm = FunctionMaker(func)\n            print('%s(%s)%s' % (fm.name, fm.signature, fm.doc))\n    if parameters:\n        params = []\n        for val in vars(OqParam).values():\n            if hasattr(val, 'name'):\n                params.append(val)\n        params.sort(key=lambda x: x.name)\n        for param in params:\n            print(param.name)\n    if os.path.isdir(input_file) and report:\n        with Monitor('info', measuremem=True) as mon:\n            with mock.patch.object(logging.root, 'info'):  # reduce logging\n                do_build_reports(input_file)\n        print(mon)\n    elif input_file.endswith('.xml'):\n        node = nrml.read(input_file)\n        if node[0].tag.endswith('sourceModel'):\n            if node['xmlns'].endswith('nrml/0.4'):\n                raise InvalidFile(\n                    '%s is in NRML 0.4 format, please run the following '\n                    'command:\\noq upgrade_nrml %s' % (\n                        input_file, os.path.dirname(input_file) or '.'))\n            print(source_model_info([node[0]]))\n        elif node[0].tag.endswith('logicTree'):\n            nodes = [nrml.read(sm_path)[0]\n                     for sm_path in logictree.collect_info(input_file).smpaths]\n            print(source_model_info(nodes))\n        else:\n            print(node.to_str())\n    elif input_file.endswith(('.ini', '.zip')):\n        with Monitor('info', measuremem=True) as mon:\n            if report:\n                print('Generated', reportwriter.build_report(input_file))\n            else:\n                print_csm_info(input_file)\n        if mon.duration > 1:\n            print(mon)\n    elif input_file:\n        print(\"No info for '%s'\" % input_file)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_stddevs(self, C, stddev_types, num_sites, mag_conversion_sigma):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        sigma = np.zeros(num_sites) + C['sigma'] * np.log(10)\n        sigma = np.sqrt(sigma ** 2 + (C['a'] ** 2) * (mag_conversion_sigma ** 2))\n        stddevs = [sigma for _ in stddev_types]\n\n        return stddevs", "response": "Returns the standard deviation as described in paragraph 2 page 1021."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n\n        # get original values\n        hslab = 50  # See info in GMPEt_Inslab_med.dat\n        rjb, rrup = utils.get_equivalent_distance_inslab(rup.mag, dists.repi,\n                                                         hslab)\n        dists.rjb = rjb\n        dists.rrup = rrup\n        mean, stddevs = super().get_mean_and_stddevs(sites, rup, dists, imt,\n                                                     stddev_types)\n        cff = self.SITE_COEFFS[imt]\n        mean_adj = np.log(np.exp(mean) * 10**cff['mf'])\n        stddevs = [np.ones(len(dists.rrup))*get_sigma(imt)]\n        return mean_adj, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_src_ids(sources):\n    src_ids = []\n    for src in sources:\n        long_src_id = src.source_id\n        try:\n            src_id, ext = long_src_id.rsplit(':', 1)\n        except ValueError:\n            src_id = long_src_id\n        src_ids.append(src_id)\n    return ' '.join(set(src_ids))", "response": "returns a string with the source IDs of the given sources stripping the theARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARKARK"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_extreme_poe(array, imtls):\n    return max(array[imtls(imt).stop - 1].max() for imt in imtls)", "response": "Returns the maximum PoE corresponding to the maximum level for all IMTs and GSIMs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting the given sources and filter the subsources and the compute the PoEs. Yield back subtasks if the sources contain more than maxweight ruptures.", "response": "def classical_split_filter(srcs, srcfilter, gsims, params, monitor):\n    \"\"\"\n    Split the given sources, filter the subsources and the compute the\n    PoEs. Yield back subtasks if the split sources contain more than\n    maxweight ruptures.\n    \"\"\"\n    # first check if we are sampling the sources\n    ss = int(os.environ.get('OQ_SAMPLE_SOURCES', 0))\n    if ss:\n        splits, stime = split_sources(srcs)\n        srcs = readinput.random_filtered_sources(splits, srcfilter, ss)\n        yield classical(srcs, srcfilter, gsims, params, monitor)\n        return\n    sources = []\n    with monitor(\"filtering/splitting sources\"):\n        for src, _sites in srcfilter(srcs):\n            if src.num_ruptures >= params['maxweight']:\n                splits, stime = split_sources([src])\n                sources.extend(srcfilter.filter(splits))\n            else:\n                sources.append(src)\n        blocks = list(block_splitter(sources, params['maxweight'],\n                                     operator.attrgetter('num_ruptures')))\n    if blocks:\n        # yield the first blocks (if any) and compute the last block in core\n        # NB: the last block is usually the smallest one\n        for block in blocks[:-1]:\n            yield classical, block, srcfilter, gsims, params\n        yield classical(blocks[-1], srcfilter, gsims, params, monitor)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the hazard stats for a single object.", "response": "def build_hazard_stats(pgetter, N, hstats, individual_curves, monitor):\n    \"\"\"\n    :param pgetter: an :class:`openquake.commonlib.getters.PmapGetter`\n    :param N: the total number of sites\n    :param hstats: a list of pairs (statname, statfunc)\n    :param individual_curves: if True, also build the individual curves\n    :param monitor: instance of Monitor\n    :returns: a dictionary kind -> ProbabilityMap\n\n    The \"kind\" is a string of the form 'rlz-XXX' or 'mean' of 'quantile-XXX'\n    used to specify the kind of output.\n    \"\"\"\n    with monitor('combine pmaps'):\n        pgetter.init()  # if not already initialized\n        try:\n            pmaps = pgetter.get_pmaps()\n        except IndexError:  # no data\n            return {}\n        if sum(len(pmap) for pmap in pmaps) == 0:  # no data\n            return {}\n    R = len(pmaps)\n    imtls, poes, weights = pgetter.imtls, pgetter.poes, pgetter.weights\n    pmap_by_kind = {}\n    hmaps_stats = []\n    hcurves_stats = []\n    with monitor('compute stats'):\n        for statname, stat in hstats.items():\n            pmap = compute_pmap_stats(pmaps, [stat], weights, imtls)\n            hcurves_stats.append(pmap)\n            if pgetter.poes:\n                hmaps_stats.append(\n                    calc.make_hmap(pmap, pgetter.imtls, pgetter.poes))\n            if statname == 'mean' and R > 1 and N <= FEWSITES:\n                pmap_by_kind['rlz_by_sid'] = rlz = {}\n                for sid, pcurve in pmap.items():\n                    rlz[sid] = util.closest_to_ref(\n                        [pm.setdefault(sid, 0).array for pm in pmaps],\n                        pcurve.array)['rlz']\n    if hcurves_stats:\n        pmap_by_kind['hcurves-stats'] = hcurves_stats\n    if hmaps_stats:\n        pmap_by_kind['hmaps-stats'] = hmaps_stats\n    if R > 1 and individual_curves or not hstats:\n        pmap_by_kind['hcurves-rlzs'] = pmaps\n        if pgetter.poes:\n            with monitor('build individual hmaps'):\n                pmap_by_kind['hmaps-rlzs'] = [\n                    calc.make_hmap(pmap, imtls, poes) for pmap in pmaps]\n    return pmap_by_kind"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the mean and standard deviation for the object.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n\n        sites.vs30 = 700 * np.ones(len(sites.vs30))\n\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n\n        C = CauzziFaccioli2008SWISS01.COEFFS\n\n        tau_ss = 'tau'\n        log_phi_ss = np.log(10)\n        mean, stddevs = _apply_adjustments(\n            C, self.COEFFS_FS_ROCK[imt], tau_ss,\n            mean, stddevs, sites, rup, dists.rhypo, imt, stddev_types,\n            log_phi_ss)\n\n        return mean, np.log(10 ** np.array(stddevs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates and returns the rupture length and width for given magnitude and nodal plane.", "response": "def _get_rupture_dimensions(src, mag, nodal_plane):\n    \"\"\"\n    Calculate and return the rupture length and width\n    for given magnitude ``mag`` and nodal plane.\n\n    :param src:\n        a PointSource, AreaSource or MultiPointSource\n    :param mag:\n        a magnitude\n    :param nodal_plane:\n        Instance of :class:`openquake.hazardlib.geo.nodalplane.NodalPlane`.\n    :returns:\n        Tuple of two items: rupture length in width in km.\n\n    The rupture area is calculated using method\n    :meth:`~openquake.hazardlib.scalerel.base.BaseMSR.get_median_area`\n    of source's\n    magnitude-scaling relationship. In any case the returned\n    dimensions multiplication is equal to that value. Than\n    the area is decomposed to length and width with respect\n    to source's rupture aspect ratio.\n\n    If calculated rupture width being inclined by nodal plane's\n    dip angle would not fit in between upper and lower seismogenic\n    depth, the rupture width is shrunken to a maximum possible\n    and rupture length is extended to preserve the same area.\n    \"\"\"\n    area = src.magnitude_scaling_relationship.get_median_area(\n        mag, nodal_plane.rake)\n    rup_length = math.sqrt(area * src.rupture_aspect_ratio)\n    rup_width = area / rup_length\n    seismogenic_layer_width = (src.lower_seismogenic_depth\n                               - src.upper_seismogenic_depth)\n    max_width = (seismogenic_layer_width\n                 / math.sin(math.radians(nodal_plane.dip)))\n    if rup_width > max_width:\n        rup_width = max_width\n        rup_length = area / rup_width\n    return rup_length, rup_width"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts components to Up South East definition.", "response": "def tensor_components_to_use(mrr, mtt, mpp, mrt, mrp, mtp):\n    '''\n    Converts components to Up, South, East definition::\n\n     USE = [[mrr, mrt, mrp],\n            [mtt, mtt, mtp],\n            [mrp, mtp, mpp]]\n    '''\n    return np.array([[mrr, mrt, mrp], [mrt, mtt, mtp], [mrp, mtp, mpp]])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the azimuth and plunge for a given vector in USE format retrieve the azimuth and plunge for a given vector in USE format", "response": "def get_azimuth_plunge(vect, degrees=True):\n    '''\n    For a given vector in USE format, retrieve the azimuth and plunge\n    '''\n    if vect[0] > 0:\n        vect = -1. * np.copy(vect)\n    vect_hor = sqrt(vect[1] ** 2. + vect[2] ** 2.)\n    plunge = atan2(-vect[0], vect_hor)\n    azimuth = atan2(vect[2], -vect[1])\n    if degrees:\n        icr = 180. / pi\n        return icr * azimuth % 360., icr * plunge\n    else:\n        return azimuth % (2. * pi), plunge"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a tensor in USE coordinate sytem to NED", "response": "def use_to_ned(tensor):\n    '''\n    Converts a tensor in USE coordinate sytem to NED\n    '''\n    return np.array(ROT_NED_USE.T * np.matrix(tensor) * ROT_NED_USE)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ned_to_use(tensor):\n    '''\n    Converts a tensor in NED coordinate sytem to USE\n    '''\n    return np.array(ROT_NED_USE * np.matrix(tensor) * ROT_NED_USE.T)", "response": "Converts a tensor in NED coordinate sytem to USE\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tensor to six component vector", "response": "def tensor_to_6component(tensor, frame='USE'):\n    '''\n    Returns a tensor to six component vector [Mrr, Mtt, Mpp, Mrt, Mrp, Mtp]\n    '''\n    if 'NED' in frame:\n        tensor = ned_to_use(tensor)\n\n    return [tensor[0, 0], tensor[1, 1], tensor[2, 2], tensor[0, 1],\n            tensor[0, 2], tensor[1, 2]]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normalise_tensor(tensor):\n    '''\n    Normalise the tensor by dividing it by its norm, defined such that\n    np.sqrt(X:X)\n    '''\n    tensor_norm = np.linalg.norm(tensor)\n    return tensor / tensor_norm, tensor_norm", "response": "Normalise the tensor by dividing it by its norm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform and eigendecomposition of the tensor and orders into descending eigenvalues", "response": "def eigendecompose(tensor, normalise=False):\n    '''\n    Performs and eigendecomposition of the tensor and orders into\n    descending eigenvalues\n    '''\n    if normalise:\n        tensor, tensor_norm = normalise_tensor(tensor)\n    else:\n        tensor_norm = 1.\n\n    eigvals, eigvects = np.linalg.eigh(tensor, UPLO='U')\n\n    isrt = np.argsort(eigvals)\n    eigenvalues = eigvals[isrt] * tensor_norm\n    eigenvectors = eigvects[:, isrt]\n    return eigenvalues, eigenvectors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a unique euler angle triplet.", "response": "def unique_euler(alpha, beta, gamma):\n    '''\n    Uniquify euler angle triplet.\n    Put euler angles into ranges compatible with (dip,strike,-rake)\n    in seismology:\n    alpha (dip) : [0, pi/2]\n    beta (strike) : [0, 2*pi)\n    gamma (-rake) : [-pi, pi)\n    If alpha is near to zero, beta is replaced by beta+gamma and gamma is set\n    to zero, to prevent that additional ambiguity.\n\n    If alpha is near to pi/2, beta is put into the range [0,pi).\n    '''\n\n    alpha = np.mod(alpha, 2.0 * pi)\n\n    if 0.5 * pi < alpha and alpha <= pi:\n        alpha = pi - alpha\n        beta = beta + pi\n        gamma = 2.0 * pi - gamma\n    elif pi < alpha and alpha <= 1.5 * pi:\n        alpha = alpha - pi\n        gamma = pi - gamma\n    elif 1.5 * pi < alpha and alpha <= 2.0 * pi:\n        alpha = 2.0 * pi - alpha\n        beta = beta + pi\n        gamma = pi + gamma\n\n    alpha = np.mod(alpha, 2.0 * pi)\n    beta = np.mod(beta, 2.0 * pi)\n    gamma = np.mod(gamma + pi, 2.0 * pi) - pi\n\n    # If dip is exactly 90 degrees, one is still\n    # free to choose between looking at the plane from either side.\n    # Choose to look at such that beta is in the range [0,180)\n\n    # This should prevent some problems, when dip is close to 90 degrees:\n    if fabs(alpha - 0.5 * pi) < 1e-10:\n        alpha = 0.5 * pi\n    if fabs(beta - pi) < 1e-10:\n        beta = pi\n    if fabs(beta - 2. * pi) < 1e-10:\n        beta = 0.\n    if fabs(beta) < 1e-10:\n        beta = 0.\n\n    if alpha == 0.5 * pi and beta >= pi:\n        gamma = - gamma\n        beta = np.mod(beta - pi, 2.0 * pi)\n        gamma = np.mod(gamma + pi, 2.0 * pi) - pi\n        assert 0. <= beta < pi\n        assert -pi <= gamma < pi\n\n    if alpha < 1e-7:\n        beta = np.mod(beta + gamma, 2.0 * pi)\n        gamma = 0.\n\n    return (alpha, beta, gamma)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef moment_magnitude_scalar(moment):\n    '''\n    Uses Hanks & Kanamori formula for calculating moment magnitude from\n    a scalar moment (Nm)\n    '''\n    if isinstance(moment, np.ndarray):\n        return (2. / 3.) * (np.log10(moment) - 9.05)\n    else:\n        return (2. / 3.) * (log10(moment) - 9.05)", "response": "Calculates the moment magnitude from a scalar moment"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n        C_SR = self.COEFFS_SOIL_RESPONSE[imt]\n\n        # compute PGA on rock conditions - needed to compute non-linear\n        # site amplification term\n        pga4nl = self._get_pga_on_rock(rup, dists, C)\n\n        # equation 1, pag 106, without sigma term, that is only the first 3\n        # terms. The third term (site amplification) is computed as given in\n        # equation (6), that is the sum of a linear term - equation (7) - and\n        # a non-linear one - equations (8a) to (8c).\n        # Mref, Rref values are given in the caption to table 6, pag 119.\n        if imt == PGA():\n            # avoid recomputing PGA on rock, just add site terms\n            mean = np.log(pga4nl) + \\\n                self._get_site_amplification_linear(sites.vs30, C_SR) + \\\n                self._get_site_amplification_non_linear(sites.vs30, pga4nl,\n                                                        C_SR)\n        else:\n            mean = self._compute_magnitude_scaling(rup, C) + \\\n                self._compute_distance_scaling(rup, dists, C) + \\\n                self._get_site_amplification_linear(sites.vs30, C_SR) + \\\n                self._get_site_amplification_non_linear(sites.vs30, pga4nl,\n                                                        C_SR)\n\n        stddevs = self._get_stddevs(C, stddev_types, num_sites=len(sites.vs30))\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_distance_scaling(self, rup, dists, C):\n        Mref = 4.5\n        Rref = 1.0\n        R = np.sqrt(dists.rjb ** 2 + C['h'] ** 2)\n        return (C['c1'] + C['c2'] * (rup.mag - Mref)) * np.log(R / Rref) + \\\n            C['c3'] * (R - Rref)", "response": "Compute distance scaling term in equation 3 and 4 pag 107."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_magnitude_scaling(self, rup, C):\n        U, SS, NS, RS = self._get_fault_type_dummy_variables(rup)\n        if rup.mag <= C['Mh']:\n            return C['e1'] * U + C['e2'] * SS + C['e3'] * NS + C['e4'] * RS + \\\n                C['e5'] * (rup.mag - C['Mh']) + \\\n                C['e6'] * (rup.mag - C['Mh']) ** 2\n        else:\n            return C['e1'] * U + C['e2'] * SS + C['e3'] * NS + C['e4'] * RS + \\\n                C['e7'] * (rup.mag - C['Mh'])", "response": "Compute magnitude scaling term in equation 107 and pag 107."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing and return PGA on rock conditions.", "response": "def _get_pga_on_rock(self, rup, dists, _C):\n        \"\"\"\n        Compute and return PGA on rock conditions (that is vs30 = 760.0 m/s).\n        This is needed to compute non-linear site amplification term\n        \"\"\"\n        # Median PGA in g for Vref = 760.0, without site amplification,\n        # that is equation (1) pag 106, without the third and fourth terms\n        # Mref and Rref values are given in the caption to table 6, pag 119\n        # Note that in the original paper, the caption reads:\n        # \"Distance-scaling coefficients (Mref=4.5 and Rref=1.0 km for all\n        # periods, except Rref=5.0 km for pga4nl)\". However this is a mistake\n        # as reported in http://www.daveboore.com/pubs_online.php:\n        # ERRATUM: 27 August 2008. Tom Blake pointed out that the caption to\n        # Table 6 should read \"Distance-scaling coefficients (Mref=4.5 and\n        # Rref=1.0 km for all periods)\".\n        C_pga = self.COEFFS[PGA()]\n        pga4nl = np.exp(self._compute_magnitude_scaling(rup, C_pga) +\n                        self._compute_distance_scaling(rup, dists, C_pga))\n\n        return pga4nl"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_site_amplification_non_linear(self, vs30, pga4nl, C):\n        # non linear slope\n        bnl = self._compute_non_linear_slope(vs30, C)\n        # compute the actual non-linear term\n        return self._compute_non_linear_term(pga4nl, bnl)", "response": "Compute site amplification non - linear term"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the non - linear slope factor for the given vs30.", "response": "def _compute_non_linear_slope(self, vs30, C):\n        \"\"\"\n        Compute non-linear slope factor,\n        equations (13a) to (13d), pag 108-109.\n        \"\"\"\n        V1 = 180.0\n        V2 = 300.0\n        Vref = 760.0\n\n        # equation (13d), values are zero for vs30 >= Vref = 760.0\n        bnl = np.zeros(vs30.shape)\n\n        # equation (13a)\n        idx = vs30 <= V1\n        bnl[idx] = C['b1']\n\n        # equation (13b)\n        idx = np.where((vs30 > V1) & (vs30 <= V2))\n        bnl[idx] = (C['b1'] - C['b2']) * \\\n                   np.log(vs30[idx] / V2) / np.log(V1 / V2) + C['b2']\n\n        # equation (13c)\n        idx = np.where((vs30 > V2) & (vs30 < Vref))\n        bnl[idx] = C['b2'] * np.log(vs30[idx] / Vref) / np.log(V2 / Vref)\n        return bnl"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_non_linear_term(self, pga4nl, bnl):\n\n        fnl = np.zeros(pga4nl.shape)\n        a1 = 0.03\n        a2 = 0.09\n        pga_low = 0.06\n\n        # equation (8a)\n        idx = pga4nl <= a1\n        fnl[idx] = bnl[idx] * np.log(pga_low / 0.1)\n\n        # equation (8b)\n        idx = np.where((pga4nl > a1) & (pga4nl <= a2))\n        delta_x = np.log(a2 / a1)\n        delta_y = bnl[idx] * np.log(a2 / pga_low)\n        c = (3 * delta_y - bnl[idx] * delta_x) / delta_x ** 2\n        d = -(2 * delta_y - bnl[idx] * delta_x) / delta_x ** 3\n        fnl[idx] = bnl[idx] * np.log(pga_low / 0.1) +\\\n            c * (np.log(pga4nl[idx] / a1) ** 2) + \\\n            d * (np.log(pga4nl[idx] / a1) ** 3)\n\n        # equation (8c)\n        idx = pga4nl > a2\n        fnl[idx] = np.squeeze(bnl[idx]) * np.log(pga4nl[idx] / 0.1)\n\n        return fnl", "response": "Compute the non - linear term for the logarithm of the logarithm of the pga4nl and bnl."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        Using a frequency dependent correction for the mean ground motion.\n        Standard deviation is fixed.\n        \"\"\"\n        mean, stddevs = super().get_mean_and_stddevs(sites, rup, dists,\n                                                     imt, stddev_types)\n        # Defining frequency\n        if imt == PGA():\n            freq = 50.0\n        elif imt == PGV():\n            freq = 2.0\n        else:\n            freq = 1./imt.period\n\n        # Equation 3 of Atkinson (2010)\n        x1 = np.min([-0.18+0.17*np.log10(freq), 0])\n\n        # Equation 4 a-b-c of Atkinson (2010)\n        if rup.hypo_depth < 20.0:\n            x0 = np.max([0.217-0.321*np.log10(freq), 0])\n        elif rup.hypo_depth > 35.0:\n            x0 = np.min([0.263+0.0924*np.log10(freq), 0.35])\n        else:\n            x0 = 0.2\n\n        # Limiting calculation distance to 1km\n        # (as suggested by C. Bruce Worden)\n        rjb = [d if d > 1 else 1 for d in dists.rjb]\n\n        # Equation 2 and 5 of Atkinson (2010)\n        mean += (x0 + x1*np.log10(rjb))/np.log10(np.e)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning total standard deviation as described in paragraph 2 page 1021.", "response": "def _get_stddevs(self, C, stddev_types, num_sites):\n        \"\"\"\n        Return total standard deviation.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        # Using a frequency independent value of sigma as recommended\n        # in the caption of Table 2 of Atkinson (2010)\n        stddevs = [0.26/np.log10(np.e) + np.zeros(num_sites)]\n\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if path is consistent with ref_path False otherwise.", "response": "def accept_path(path, ref_path):\n    \"\"\"\n    :param path: a logic tree path (list or tuple of strings)\n    :param ref_path: reference logic tree path\n    :returns: True if `path` is consistent with `ref_path`, False otherwise\n\n    >>> accept_path(['SM2'], ('SM2', 'a3b1'))\n    False\n    >>> accept_path(['SM2', '@'], ('SM2', 'a3b1'))\n    True\n    >>> accept_path(['@', 'a3b1'], ('SM2', 'a3b1'))\n    True\n    >>> accept_path('@@', ('SM2', 'a3b1'))\n    True\n    \"\"\"\n    if len(path) != len(ref_path):\n        return False\n    for a, b in zip(path, ref_path):\n        if a != '@' and a != b:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an association between two sets of logic tree items", "response": "def get_rlzs_assoc(cinfo, sm_lt_path=None, trts=None):\n    \"\"\"\n    :param cinfo: a :class:`openquake.commonlib.source.CompositionInfo`\n    :param sm_lt_path: logic tree path tuple used to select a source model\n    :param trts: tectonic region types to accept\n    \"\"\"\n    assoc = RlzsAssoc(cinfo)\n    offset = 0\n    trtset = set(cinfo.gsim_lt.values)\n    for smodel in cinfo.source_models:\n        # discard source models with non-acceptable lt_path\n        if sm_lt_path and not accept_path(smodel.path, sm_lt_path):\n            continue\n\n        # collect the effective tectonic region types and ruptures\n        trts_ = set()\n        for sg in smodel.src_groups:\n            if sg.eff_ruptures:\n                if (trts and sg.trt in trts) or not trts:\n                    trts_.add(sg.trt)\n\n        # recompute the GSIM logic tree if needed\n        if trts_ != {'*'} and trtset != trts_:\n            before = cinfo.gsim_lt.get_num_paths()\n            gsim_lt = cinfo.gsim_lt.reduce(trts_)\n            after = gsim_lt.get_num_paths()\n            if sm_lt_path and before > after:\n                # print the warning only when saving the logic tree,\n                # i.e. when called with sm_lt_path in store_rlz_info\n                logging.warning('Reducing the logic tree of %s from %d to %d '\n                                'realizations', smodel.name, before, after)\n            gsim_rlzs = list(gsim_lt)\n            all_trts = list(gsim_lt.values)\n        else:\n            gsim_rlzs = list(cinfo.gsim_lt)\n            all_trts = list(cinfo.gsim_lt.values)\n\n        rlzs = cinfo._get_rlzs(smodel, gsim_rlzs, cinfo.seed + offset)\n        assoc._add_realizations(offset, smodel, all_trts, rlzs)\n        offset += len(rlzs)\n\n    if assoc.realizations:\n        assoc._init()\n    return assoc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of gsim - > realizations for a given source model ID", "response": "def get_rlzs_by_gsim(self, trt_or_grp_id, sm_id=None):\n        \"\"\"\n        :param trt_or_grp_id: a tectonic region type or a source group ID\n        :param sm_id: source model ordinal (or None)\n        :returns: a dictionary gsim -> rlzs\n        \"\"\"\n        if isinstance(trt_or_grp_id, (int, U16, U32)):  # grp_id\n            trt = self.csm_info.trt_by_grp[trt_or_grp_id]\n            sm_id = self.csm_info.get_sm_by_grp()[trt_or_grp_id]\n        else:  # assume TRT string\n            trt = trt_or_grp_id\n        acc = collections.defaultdict(list)\n        if sm_id is None:  # full dictionary\n            for rlz, gsim_by_trt in zip(self.realizations, self.gsim_by_trt):\n                acc[gsim_by_trt[trt]].append(rlz.ordinal)\n        else:  # dictionary for the selected source model\n            for rlz in self.rlzs_by_smodel[sm_id]:\n                gsim_by_trt = self.gsim_by_trt[rlz.ordinal]\n                try:  # if there is a single TRT\n                    [gsim] = gsim_by_trt.values()\n                except ValueError:  # there is more than 1 TRT\n                    gsim = gsim_by_trt[trt]\n                acc[gsim].append(rlz.ordinal)\n        return {gsim: numpy.array(acc[gsim], dtype=U16)\n                for gsim in sorted(acc)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinalizes the initialization of the RlzsAssoc object by setting the weights of the realizations.", "response": "def _init(self):\n        \"\"\"\n        Finalize the initialization of the RlzsAssoc object by setting\n        the (reduced) weights of the realizations.\n        \"\"\"\n        if self.num_samples:\n            assert len(self.realizations) == self.num_samples, (\n                len(self.realizations), self.num_samples)\n            for rlz in self.realizations:\n                for k in rlz.weight.dic:\n                    rlz.weight.dic[k] = 1. / self.num_samples\n        else:\n            tot_weight = sum(rlz.weight for rlz in self.realizations)\n            if not tot_weight.is_one():\n                # this may happen for rounding errors or because of the\n                # logic tree reduction; we ensure the sum of the weights is 1\n                for rlz in self.realizations:\n                    rlz.weight = rlz.weight / tot_weight"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef combine_pmaps(self, pmap_by_grp):\n        grp = list(pmap_by_grp)[0]  # pmap_by_grp must be non-empty\n        num_levels = pmap_by_grp[grp].shape_y\n        pmaps = [probability_map.ProbabilityMap(num_levels, 1)\n                 for _ in self.realizations]\n        array = self.by_grp()\n        for grp in pmap_by_grp:\n            for gsim_idx, rlzis in enumerate(array[grp]):\n                pmap = pmap_by_grp[grp].extract(gsim_idx)\n                for rlzi in rlzis:\n                    pmaps[rlzi] |= pmap\n        return pmaps", "response": "Combine probability maps for each realization in the given dictionary group string into one probability map per realization."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_rlz(self, rlzstr):\n        mo = re.match(r'rlz-(\\d+)', rlzstr)\n        if not mo:\n            return\n        return self.realizations[int(mo.group(1))]", "response": "r Get a Realization instance for a string of the form rlz - \\ d +"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports an output from the datastore.", "response": "def export(datastore_key, calc_id=-1, exports='csv', export_dir='.'):\n    \"\"\"\n    Export an output from the datastore.\n    \"\"\"    \n    dstore = util.read(calc_id)\n    parent_id = dstore['oqparam'].hazard_calculation_id\n    if parent_id:\n        dstore.parent = util.read(parent_id)\n    dstore.export_dir = export_dir\n    with performance.Monitor('export', measuremem=True) as mon:\n        for fmt in exports.split(','):\n            fnames = export_((datastore_key, fmt), dstore)\n            nbytes = sum(os.path.getsize(f) for f in fnames)\n            print('Exported %s in %s' % (general.humansize(nbytes), fnames))\n    if mon.duration > 1:\n        print(mon)\n    dstore.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_UCERFSource(self, node):\n    dirname = os.path.dirname(self.fname)  # where the source_model_file is\n    source_file = os.path.join(dirname, node[\"filename\"])\n    if \"startDate\" in node.attrib and \"investigationTime\" in node.attrib:\n        # Is a time-dependent model - even if rates were originally\n        # poissonian\n        # Verify that the source time span is the same as the TOM time span\n        inv_time = float(node[\"investigationTime\"])\n        if inv_time != self.investigation_time:\n            raise ValueError(\"Source investigation time (%s) is not \"\n                             \"equal to configuration investigation time \"\n                             \"(%s)\" % (inv_time, self.investigation_time))\n        start_date = datetime.strptime(node[\"startDate\"], \"%d/%m/%Y\")\n    else:\n        start_date = None\n    return UCERFSource(\n        source_file,\n        self.investigation_time,\n        start_date,\n        float(node[\"minMag\"]),\n        npd=self.convert_npdist(node),\n        hdd=self.convert_hpdist(node),\n        aspect=~node.ruptAspectRatio,\n        upper_seismogenic_depth=~node.pointGeometry.upperSeismoDepth,\n        lower_seismogenic_depth=~node.pointGeometry.lowerSeismoDepth,\n        msr=valid.SCALEREL[~node.magScaleRel](),\n        mesh_spacing=self.rupture_mesh_spacing,\n        trt=node[\"tectonicRegion\"])", "response": "Converts the Ucerf source node into an SES Control object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_idx_set(branch_id, start_date):\n    code_set = branch_id.split(\"/\")\n    code_set.insert(3, \"Rates\")\n    idx_set = {\n        \"sec\": \"/\".join([code_set[0], code_set[1], \"Sections\"]),\n        \"mag\": \"/\".join([code_set[0], code_set[1], code_set[2], \"Magnitude\"])}\n    idx_set[\"rate\"] = \"/\".join(code_set)\n    idx_set[\"rake\"] = \"/\".join([code_set[0], code_set[1], \"Rake\"])\n    idx_set[\"msr\"] = \"-\".join(code_set[:3])\n    idx_set[\"geol\"] = code_set[0]\n    if start_date:  # time-dependent source\n        idx_set[\"grid_key\"] = \"_\".join(\n            branch_id.replace(\"/\", \"_\").split(\"_\")[:-1])\n    else:  # time-independent source\n        idx_set[\"grid_key\"] = branch_id.replace(\"/\", \"_\")\n    idx_set[\"total_key\"] = branch_id.replace(\"/\", \"|\")\n    return idx_set", "response": "Builds a dictionary of keys based on the branch id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_rupture_dimensions(mag, nodal_plane, msr, rupture_aspect_ratio,\n                           upper_seismogenic_depth, lower_seismogenic_depth):\n    \"\"\"\n    Calculate and return the rupture length and width\n    for given magnitude ``mag`` and nodal plane.\n\n    :param nodal_plane:\n        Instance of :class:`openquake.hazardlib.geo.nodalplane.NodalPlane`.\n    :returns:\n        Tuple of two items: rupture length in width in km.\n\n    The rupture area is calculated using method\n    :meth:`~openquake.hazardlib.scalerel.base.BaseMSR.get_median_area`\n    of source's\n    magnitude-scaling relationship. In any case the returned\n    dimensions multiplication is equal to that value. Than\n    the area is decomposed to length and width with respect\n    to source's rupture aspect ratio.\n\n    If calculated rupture width being inclined by nodal plane's\n    dip angle would not fit in between upper and lower seismogenic\n    depth, the rupture width is shrunken to a maximum possible\n    and rupture length is extended to preserve the same area.\n    \"\"\"\n    area = msr.get_median_area(mag, nodal_plane.rake)\n    rup_length = math.sqrt(area * rupture_aspect_ratio)\n    rup_width = area / rup_length\n    seismogenic_layer_width = (lower_seismogenic_depth -\n                               upper_seismogenic_depth)\n    max_width = (seismogenic_layer_width /\n                 math.sin(math.radians(nodal_plane.dip)))\n    if rup_width > max_width:\n        rup_width = max_width\n        rup_length = area / rup_width\n    return rup_length, rup_width", "response": "Calculates and returns the rupture length and width for given magnitude and nodal plane."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and return a new rupture surface object with given properties.", "response": "def get_rupture_surface(mag, nodal_plane, hypocenter, msr,\n                        rupture_aspect_ratio, upper_seismogenic_depth,\n                        lower_seismogenic_depth, mesh_spacing=1.0):\n    \"\"\"\n    Create and return rupture surface object with given properties.\n\n    :param mag:\n        Magnitude value, used to calculate rupture dimensions,\n        see :meth:`_get_rupture_dimensions`.\n    :param nodal_plane:\n        Instance of :class:`openquake.hazardlib.geo.nodalplane.NodalPlane`\n        describing the rupture orientation.\n    :param hypocenter:\n        Point representing rupture's hypocenter.\n    :returns:\n        Instance of\n        :class:`~openquake.hazardlib.geo.surface.planar.PlanarSurface`.\n    \"\"\"\n    assert (upper_seismogenic_depth <= hypocenter.depth\n            and lower_seismogenic_depth >= hypocenter.depth)\n    rdip = math.radians(nodal_plane.dip)\n\n    # precalculated azimuth values for horizontal-only and vertical-only\n    # moves from one point to another on the plane defined by strike\n    # and dip:\n    azimuth_right = nodal_plane.strike\n    azimuth_down = (azimuth_right + 90) % 360\n    azimuth_left = (azimuth_down + 90) % 360\n    azimuth_up = (azimuth_left + 90) % 360\n\n    rup_length, rup_width = get_rupture_dimensions(\n        mag, nodal_plane, msr, rupture_aspect_ratio, upper_seismogenic_depth,\n        lower_seismogenic_depth)\n    # calculate the height of the rupture being projected\n    # on the vertical plane:\n    rup_proj_height = rup_width * math.sin(rdip)\n    # and it's width being projected on the horizontal one:\n    rup_proj_width = rup_width * math.cos(rdip)\n\n    # half height of the vertical component of rupture width\n    # is the vertical distance between the rupture geometrical\n    # center and it's upper and lower borders:\n    hheight = rup_proj_height / 2\n    # calculate how much shallower the upper border of the rupture\n    # is than the upper seismogenic depth:\n    vshift = upper_seismogenic_depth - hypocenter.depth + hheight\n    # if it is shallower (vshift > 0) than we need to move the rupture\n    # by that value vertically.\n    if vshift < 0:\n        # the top edge is below upper seismogenic depth. now we need\n        # to check that we do not cross the lower border.\n        vshift = lower_seismogenic_depth - hypocenter.depth - hheight\n        if vshift > 0:\n            # the bottom edge of the rupture is above the lower sesmogenic\n            # depth. that means that we don't need to move the rupture\n            # as it fits inside seismogenic layer.\n            vshift = 0\n        # if vshift < 0 than we need to move the rupture up by that value.\n\n    # now we need to find the position of rupture's geometrical center.\n    # in any case the hypocenter point must lie on the surface, however\n    # the rupture center might be off (below or above) along the dip.\n    rupture_center = hypocenter\n    if vshift != 0:\n        # we need to move the rupture center to make the rupture fit\n        # inside the seismogenic layer.\n        hshift = abs(vshift / math.tan(rdip))\n        rupture_center = rupture_center.point_at(\n            horizontal_distance=hshift, vertical_increment=vshift,\n            azimuth=(azimuth_up if vshift < 0 else azimuth_down))\n\n    # from the rupture center we can now compute the coordinates of the\n    # four coorners by moving along the diagonals of the plane. This seems\n    # to be better then moving along the perimeter, because in this case\n    # errors are accumulated that induce distorsions in the shape with\n    # consequent raise of exceptions when creating PlanarSurface objects\n    # theta is the angle between the diagonal of the surface projection\n    # and the line passing through the rupture center and parallel to the\n    # top and bottom edges. Theta is zero for vertical ruptures (because\n    # rup_proj_width is zero)\n    theta = math.degrees(\n        math.atan((rup_proj_width / 2.) / (rup_length / 2.)))\n    hor_dist = math.sqrt(\n        (rup_length / 2.) ** 2 + (rup_proj_width / 2.) ** 2)\n    left_top = rupture_center.point_at(\n        horizontal_distance=hor_dist,\n        vertical_increment=-rup_proj_height / 2,\n        azimuth=(nodal_plane.strike + 180 + theta) % 360)\n    right_top = rupture_center.point_at(\n        horizontal_distance=hor_dist,\n        vertical_increment=-rup_proj_height / 2,\n        azimuth=(nodal_plane.strike - theta) % 360)\n    left_bottom = rupture_center.point_at(\n        horizontal_distance=hor_dist,\n        vertical_increment=rup_proj_height / 2,\n        azimuth=(nodal_plane.strike + 180 - theta) % 360)\n    right_bottom = rupture_center.point_at(\n        horizontal_distance=hor_dist,\n        vertical_increment=rup_proj_height / 2,\n        azimuth=(nodal_plane.strike + theta) % 360)\n    return PlanarSurface(nodal_plane.strike, nodal_plane.dip,\n                         left_top, right_top, right_bottom, left_bottom)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_background_ruptures(tom, locations, occurrence, mag, npd,\n                                 hdd, upper_seismogenic_depth,\n                                 lower_seismogenic_depth, msr=WC1994(),\n                                 aspect=1.5, trt=DEFAULT_TRT):\n    \"\"\"\n    :param tom:\n        Temporal occurrence model as instance of :class:\n        openquake.hazardlib.tom.TOM\n    :param numpy.ndarray locations:\n        Array of locations [Longitude, Latitude] of the point sources\n    :param numpy.ndarray occurrence:\n        Annual rates of occurrence\n    :param float mag:\n        Magnitude\n    :param npd:\n        Nodal plane distribution as instance of :class:\n        openquake.hazardlib.pmf.PMF\n    :param hdd:\n        Hypocentral depth distribution as instance of :class:\n        openquake.hazardlib.pmf.PMF\n    :param float upper_seismogenic_depth:\n        Upper seismogenic depth (km)\n    :param float lower_seismogenic_depth:\n        Lower seismogenic depth (km)\n    :param msr:\n        Magnitude scaling relation\n    :param float aspect:\n        Aspect ratio\n    :param str trt:\n        Tectonic region type\n    :returns:\n        List of ruptures\n    \"\"\"\n    ruptures = []\n    n_vals = len(locations)\n    depths = hdd.sample_pairs(n_vals)\n    nodal_planes = npd.sample_pairs(n_vals)\n    for i, (x, y) in enumerate(locations):\n        hypocentre = Point(x, y, depths[i][1])\n        surface = get_rupture_surface(mag, nodal_planes[i][1],\n                                      hypocentre, msr, aspect,\n                                      upper_seismogenic_depth,\n                                      lower_seismogenic_depth)\n        rupture_probability = (occurrence[i] * nodal_planes[i][0] *\n                               depths[i][0])\n        ruptures.append(ParametricProbabilisticRupture(\n            mag, nodal_planes[i][1].rake, trt, hypocentre, surface,\n            rupture_probability, tom))\n    return ruptures", "response": "Generate a list of ruptures for a given set of locations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_indices(self, src, ridx, mag):\n        centroids = src.get_centroids(ridx)\n        mindistance = min_geodetic_distance(\n            (centroids[:, 0], centroids[:, 1]), self.sitecol.xyz)\n        idist = self.integration_distance(DEFAULT_TRT, mag)\n        indices, = (mindistance <= idist).nonzero()\n        return indices", "response": "Compute the indices of the ruptures close to the ruptures in the source UCERF source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new(self, grp_id, branch_id):\n        new = copy.copy(self)\n        new.orig = new\n        new.src_group_id = grp_id\n        new.source_id = branch_id\n        new.idx_set = build_idx_set(branch_id, self.start_date)\n        with h5py.File(self.source_file, \"r\") as hdf5:\n            new.start = 0\n            new.stop = len(hdf5[new.idx_set[\"mag\"]])\n        return new", "response": "Creates a new UCERFSource object with the same attributes as the current one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ridx(self, iloc):\n        with h5py.File(self.source_file, \"r\") as hdf5:\n            return hdf5[self.idx_set[\"geol\"] + \"/RuptureIndex\"][iloc]", "response": "Returns the rupture index for the given iloc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_centroids(self, ridx):\n        centroids = []\n        with h5py.File(self.source_file, \"r\") as hdf5:\n            for idx in ridx:\n                trace = \"{:s}/{:s}\".format(self.idx_set[\"sec\"], str(idx))\n                centroids.append(hdf5[trace + \"/Centroids\"].value)\n        return numpy.concatenate(centroids)", "response": "returns array of centroids for the given rupture index"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield trace and rupture planes for the given rupture index", "response": "def gen_trace_planes(self, ridx):\n        \"\"\"\n        :yields: trace and rupture planes for the given rupture index\n        \"\"\"\n        with h5py.File(self.source_file, \"r\") as hdf5:\n            for idx in ridx:\n                trace = \"{:s}/{:s}\".format(self.idx_set[\"sec\"], str(idx))\n                plane = hdf5[trace + \"/RupturePlanes\"][:].astype(\"float64\")\n                yield trace, plane"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_background_sids(self, src_filter):\n        branch_key = self.idx_set[\"grid_key\"]\n        idist = src_filter.integration_distance(DEFAULT_TRT)\n        with h5py.File(self.source_file, 'r') as hdf5:\n            bg_locations = hdf5[\"Grid/Locations\"].value\n            distances = min_geodetic_distance(\n                src_filter.sitecol.xyz,\n                (bg_locations[:, 0], bg_locations[:, 1]))\n            # Add buffer equal to half of length of median area from Mmax\n            mmax_areas = self.msr.get_median_area(\n                hdf5[\"/\".join([\"Grid\", branch_key, \"MMax\"])].value, 0.0)\n            # for instance hdf5['Grid/FM0_0_MEANFS_MEANMSR/MMax']\n            mmax_lengths = numpy.sqrt(mmax_areas / self.aspect)\n            ok = distances <= (0.5 * mmax_lengths + idist)\n            # get list of indices from array of booleans\n            return numpy.where(ok)[0].tolist()", "response": "Get the background sites in the rupture."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a rupture object for the iloc", "response": "def get_ucerf_rupture(self, iloc, src_filter):\n        \"\"\"\n        :param iloc:\n            Location of the rupture plane in the hdf5 file\n        :param src_filter:\n            Sites for consideration and maximum distance\n        \"\"\"\n        trt = self.tectonic_region_type\n        ridx = self.get_ridx(iloc)\n        mag = self.orig.mags[iloc]\n        surface_set = []\n        indices = src_filter.get_indices(self, ridx, mag)\n        if len(indices) == 0:\n            return None\n        for trace, plane in self.gen_trace_planes(ridx):\n            # build simple fault surface\n            for jloc in range(0, plane.shape[2]):\n                top_left = Point(\n                    plane[0, 0, jloc], plane[0, 1, jloc], plane[0, 2, jloc])\n                top_right = Point(\n                    plane[1, 0, jloc], plane[1, 1, jloc], plane[1, 2, jloc])\n                bottom_right = Point(\n                    plane[2, 0, jloc], plane[2, 1, jloc], plane[2, 2, jloc])\n                bottom_left = Point(\n                    plane[3, 0, jloc], plane[3, 1, jloc], plane[3, 2, jloc])\n                try:\n                    surface_set.append(\n                        ImperfectPlanarSurface.from_corner_points(\n                            top_left, top_right, bottom_right, bottom_left))\n                except ValueError as err:\n                    raise ValueError(err, trace, top_left, top_right,\n                                     bottom_right, bottom_left)\n\n        rupture = ParametricProbabilisticRupture(\n            mag, self.orig.rake[iloc], trt,\n            surface_set[len(surface_set) // 2].get_middle_point(),\n            MultiSurface(surface_set), self.orig.rate[iloc], self.tom)\n\n        return rupture"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_ruptures(self):\n        assert self.orig, '%s is not fully initialized' % self\n        for ridx in range(self.start, self.stop):\n            if self.orig.rate[ridx]:  # ruptures may have have zero rate\n                rup = self.get_ucerf_rupture(ridx, self.src_filter)\n                if rup:\n                    yield rup", "response": "Iterate over ruptures in the current set of indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_background_sources(self, src_filter, sample_factor=None):\n        background_sids = self.get_background_sids(src_filter)\n        if sample_factor is not None:  # hack for use in the mosaic\n            background_sids = random_filter(\n                background_sids, sample_factor, seed=42)\n        with h5py.File(self.source_file, \"r\") as hdf5:\n            grid_loc = \"/\".join([\"Grid\", self.idx_set[\"grid_key\"]])\n            # for instance Grid/FM0_0_MEANFS_MEANMSR_MeanRates\n            mags = hdf5[grid_loc + \"/Magnitude\"].value\n            mmax = hdf5[grid_loc + \"/MMax\"][background_sids]\n            rates = hdf5[grid_loc + \"/RateArray\"][background_sids, :]\n            locations = hdf5[\"Grid/Locations\"][background_sids, :]\n            sources = []\n            for i, bg_idx in enumerate(background_sids):\n                src_id = \"_\".join([self.idx_set[\"grid_key\"], str(bg_idx)])\n                src_name = \"|\".join([self.idx_set[\"total_key\"], str(bg_idx)])\n                mag_idx = (self.min_mag <= mags) & (mags < mmax[i])\n                src_mags = mags[mag_idx]\n                src_mfd = EvenlyDiscretizedMFD(\n                    src_mags[0],\n                    src_mags[1] - src_mags[0],\n                    rates[i, mag_idx].tolist())\n                ps = PointSource(\n                    src_id, src_name, self.tectonic_region_type, src_mfd,\n                    self.mesh_spacing, self.msr, self.aspect, self.tom,\n                    self.usd, self.lsd,\n                    Point(locations[i, 0], locations[i, 1]),\n                    self.npd, self.hdd)\n                ps.id = self.id\n                ps.src_group_id = self.src_group_id\n                ps.num_ruptures = ps.count_ruptures()\n                sources.append(ps)\n        return sources", "response": "This method takes the background model of a given branch into a set of point sources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit a complex fault source into chunks", "response": "def split(src, chunksize=MINWEIGHT):\n    \"\"\"\n    Split a complex fault source in chunks\n    \"\"\"\n    for i, block in enumerate(block_splitter(src.iter_ruptures(), chunksize,\n                                             key=operator.attrgetter('mag'))):\n        rup = block[0]\n        source_id = '%s:%d' % (src.source_id, i)\n        amfd = mfd.ArbitraryMFD([rup.mag], [rup.mag_occ_rate])\n        rcs = RuptureCollectionSource(\n            source_id, src.name, src.tectonic_region_type, amfd, block)\n        yield rcs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bounding_box(self, maxdist):\n        locations = [rup.hypocenter for rup in self.ruptures]\n        return get_bounding_box(locations, maxdist)", "response": "Returns a bounding box containing all the hypocenters enlarged by the\n        maximum distance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow the attributes of a HDF5 dataset in the datastore.", "response": "def show_attrs(key, calc_id=-1):\n    \"\"\"\n    Show the attributes of a HDF5 dataset in the datastore.\n    \"\"\"\n    ds = util.read(calc_id)\n    try:\n        attrs = h5py.File.__getitem__(ds.hdf5, key).attrs\n    except KeyError:\n        print('%r is not in %s' % (key, ds))\n    else:\n        if len(attrs) == 0:\n            print('%s has no attributes' % key)\n        for name, value in attrs.items():\n            print(name, value)\n    finally:\n        ds.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares the hazard curves coming from two different calculations.", "response": "def compare_mean_curves(calc_ref, calc, nsigma=3):\n    \"\"\"\n    Compare the hazard curves coming from two different calculations.\n    \"\"\"\n    dstore_ref = datastore.read(calc_ref)\n    dstore = datastore.read(calc)\n    imtls = dstore_ref['oqparam'].imtls\n    if dstore['oqparam'].imtls != imtls:\n        raise RuntimeError('The IMTs and levels are different between '\n                           'calculation %d and %d' % (calc_ref, calc))\n    sitecol_ref = dstore_ref['sitecol']\n    sitecol = dstore['sitecol']\n    site_id_ref = {(lon, lat): sid for sid, lon, lat in zip(\n        sitecol_ref.sids, sitecol_ref.lons, sitecol_ref.lats)}\n    site_id = {(lon, lat): sid for sid, lon, lat in zip(\n        sitecol.sids, sitecol.lons, sitecol.lats)}\n    common = set(site_id_ref) & set(site_id)\n    if not common:\n        raise RuntimeError('There are no common sites between calculation '\n                           '%d and %d' % (calc_ref, calc))\n    pmap_ref = PmapGetter(dstore_ref, sids=[site_id_ref[lonlat]\n                                            for lonlat in common]).get_mean()\n    pmap = PmapGetter(dstore, sids=[site_id[lonlat]\n                                    for lonlat in common]).get_mean()\n    for lonlat in common:\n        mean, std = pmap[site_id[lonlat]].array.T  # shape (2, N)\n        mean_ref, std_ref = pmap_ref[site_id_ref[lonlat]].array.T\n        err = numpy.sqrt(std**2 + std_ref**2)\n        for imt in imtls:\n            sl = imtls(imt)\n            ok = (numpy.abs(mean[sl] - mean_ref[sl]) < nsigma * err[sl]).all()\n            if not ok:\n                md = (numpy.abs(mean[sl] - mean_ref[sl])).max()\n                plt.title('point=%s, imt=%s, maxdiff=%.2e' % (lonlat, imt, md))\n                plt.loglog(imtls[imt], mean_ref[sl] + std_ref[sl],\n                           label=str(calc_ref), color='black')\n                plt.loglog(imtls[imt], mean_ref[sl] - std_ref[sl],\n                           color='black')\n                plt.loglog(imtls[imt], mean[sl] + std[sl], label=str(calc),\n                           color='red')\n                plt.loglog(imtls[imt], mean[sl] - std[sl], color='red')\n                plt.legend()\n                plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the mean value of a logarithmic entry in the logarithmic table.", "response": "def _get_mean(self, sites, C, ln_y_ref, exp1, exp2):\n        \"\"\"\n        Add site effects to an intensity.\n\n        Implements eq. 13b.\n        \"\"\"\n        # we do not support estimating of basin depth and instead\n        # rely on it being available (since we require it).\n        # centered_z1pt0\n        centered_z1pt0 = self._get_centered_z1pt0(sites)\n        # we consider random variables being zero since we want\n        # to find the exact mean value.\n        eta = epsilon = 0.\n\n        ln_y = (\n            # first line of eq. 12\n            ln_y_ref + eta\n            # second line\n            + C['phi1'] * np.log(sites.vs30 / 1130).clip(-np.inf, 0)\n            # third line\n            + C['phi2'] * (exp1 - exp2)\n            * np.log((np.exp(ln_y_ref) * np.exp(eta) + C['phi4']) / C['phi4'])\n            # fourth line\n            + C['phi5']\n            * (1.0 - np.exp(-1. * centered_z1pt0 / C['phi6']))\n            # fifth line\n            + epsilon\n        )\n\n        return ln_y"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_ln_y_ref(self, rup, dists, C):\n        # reverse faulting flag\n        Frv = 1. if 30 <= rup.rake <= 150 else 0.\n        # normal faulting flag\n        Fnm = 1. if -120 <= rup.rake <= -60 else 0.\n        # hanging wall flag\n\n        Fhw = np.zeros_like(dists.rx)\n        idx = np.nonzero(dists.rx >= 0.)\n        Fhw[idx] = 1.\n\n        # a part in eq. 11\n        mag_test1 = np.cosh(2. * max(rup.mag - 4.5, 0))\n\n        # centered DPP\n        centered_dpp = self._get_centered_cdpp(dists)\n        # centered_ztor\n        centered_ztor = self._get_centered_ztor(rup, Frv)\n        #\n        dist_taper = np.fmax(1 - (np.fmax(dists.rrup - 40,\n                                  np.zeros_like(dists)) / 30.),\n                             np.zeros_like(dists))\n        dist_taper = dist_taper.astype(np.float64)\n        ln_y_ref = (\n            # first part of eq. 11\n            C['c1']\n            + (C['c1a'] + C['c1c'] / mag_test1) * Frv\n            + (C['c1b'] + C['c1d'] / mag_test1) * Fnm\n            + (C['c7'] + C['c7b'] / mag_test1) * centered_ztor\n            + (C['c11'] + C['c11b'] / mag_test1) *\n            np.cos(math.radians(rup.dip)) ** 2\n            # second part\n            + C['c2'] * (rup.mag - 6)\n            + ((C['c2'] - C['c3']) / C['cn'])\n            * np.log(1 + np.exp(C['cn'] * (C['cm'] - rup.mag)))\n            # third part\n            + C['c4']\n            * np.log(dists.rrup + C['c5']\n                     * np.cosh(C['c6'] * max(rup.mag - C['chm'], 0)))\n            + (C['c4a'] - C['c4'])\n            * np.log(np.sqrt(dists.rrup ** 2 + C['crb'] ** 2))\n            # forth part\n            + (C['cg1'] + C['cg2'] / (np.cosh(max(rup.mag - C['cg3'], 0))))\n            * dists.rrup\n            # fifth part\n            + C['c8'] * dist_taper\n            * min(max(rup.mag - 5.5, 0) / 0.8, 1.0)\n            * np.exp(-1 * C['c8a'] * (rup.mag - C['c8b']) ** 2) * centered_dpp\n            # sixth part\n            + C['c9'] * Fhw * np.cos(math.radians(rup.dip)) *\n            (C['c9a'] + (1 - C['c9a']) * np.tanh(dists.rx / C['c9b']))\n            * (1 - np.sqrt(dists.rjb ** 2 + rup.ztor ** 2)\n               / (dists.rrup + 1.0))\n        )\n\n        return ln_y_ref", "response": "Returns the log - likelihood of the entry in the logarithm of the magnetic and hanging wall."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_centered_z1pt0(self, sites):\n        #: California and non-Japan regions\n\n        mean_z1pt0 = (-7.15 / 4.) * np.log(((sites.vs30) ** 4. + 570.94 ** 4.)\n                                           / (1360 ** 4. + 570.94 ** 4.))\n        centered_z1pt0 = sites.z1pt0 - np.exp(mean_z1pt0)\n\n        return centered_z1pt0", "response": "Get centered z1pt0 of the avarage in the Vs30 - dependent avarage z1pt0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_centered_ztor(self, rup, Frv):\n        if Frv == 1:\n\n            mean_ztor = max(2.704 - 1.226 * max(rup.mag - 5.849, 0.0), 0.) ** 2\n            centered_ztor = rup.ztor - mean_ztor\n        else:\n\n            mean_ztor = max(2.673 - 1.136 * max(rup.mag - 4.970, 0.0), 0.) ** 2\n            centered_ztor = rup.ztor - mean_ztor\n\n        return centered_ztor", "response": "Get the centered ztor of the avarage."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the standard deviation for the resource table.", "response": "def _get_stddevs(self, sites, rup, C, stddev_types, ln_y_ref, exp1, exp2):\n        \"\"\"\n        Returns the standard deviation, which is fixed at 0.65 for every site\n        \"\"\"\n        ret = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                # eq. 13\n                ret.append(0.65 * np.ones_like(sites.vs30))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fine_graining(points, steps):\n    if steps < 2:\n        return points\n    ls = numpy.concatenate([numpy.linspace(x, y, num=steps + 1)[:-1]\n                            for x, y in pairwise(points)])\n    return numpy.concatenate([ls, [points[-1]]])", "response": "fine - graining a list of points"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild intensity measure levels from a fragility function.", "response": "def build_imls(ff, continuous_fragility_discretization,\n               steps_per_interval=0):\n    \"\"\"\n    Build intensity measure levels from a fragility function. If the function\n    is continuous, they are produced simply as a linear space between minIML\n    and maxIML. If the function is discrete, they are generated with a\n    complex logic depending on the noDamageLimit and the parameter\n    steps per interval.\n\n    :param ff: a fragility function object\n    :param continuous_fragility_discretization: .ini file parameter\n    :param steps_per_interval:  .ini file parameter\n    :returns: generated imls\n    \"\"\"\n    if ff.format == 'discrete':\n        imls = ff.imls\n        if ff.nodamage and ff.nodamage < imls[0]:\n            imls = [ff.nodamage] + imls\n        if steps_per_interval > 1:\n            gen_imls = fine_graining(imls, steps_per_interval)\n        else:\n            gen_imls = imls\n    else:  # continuous\n        gen_imls = numpy.linspace(ff.minIML, ff.maxIML,\n                                  continuous_fragility_discretization)\n    return gen_imls"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a matrix N * R returns a matrix of the same shape N * R and R samples.", "response": "def make_epsilons(matrix, seed, correlation):\n    \"\"\"\n    Given a matrix N * R returns a matrix of the same shape N * R\n    obtained by applying the multivariate_normal distribution to\n    N points and R samples, by starting from the given seed and\n    correlation.\n    \"\"\"\n    if seed is not None:\n        numpy.random.seed(seed)\n    asset_count = len(matrix)\n    samples = len(matrix[0])\n    if not correlation:  # avoid building the covariance matrix\n        return numpy.random.normal(size=(samples, asset_count)).transpose()\n    means_vector = numpy.zeros(asset_count)\n    covariance_matrix = (\n        numpy.ones((asset_count, asset_count)) * correlation +\n        numpy.diag(numpy.ones(asset_count)) * (1 - correlation))\n    return numpy.random.multivariate_normal(\n        means_vector, covariance_matrix, samples).transpose()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scenario_damage(fragility_functions, gmvs):\n    lst = [numpy.ones_like(gmvs)]\n    for f, ff in enumerate(fragility_functions):  # D - 1 functions\n        lst.append(ff(gmvs))\n    lst.append(numpy.zeros_like(gmvs))\n    # convert a (D + 1, E) array into a (D, E) array\n    arr = pairwise_diff(numpy.array(lst))\n    arr[arr < 1E-7] = 0  # sanity check\n    return arr", "response": "This function is used to calculate the damage fractions of a scenario."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef annual_frequency_of_exceedence(poe, t_haz):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # avoid RuntimeWarning: divide by zero encountered in log\n        return - numpy.log(1. - poe) / t_haz", "response": "Calculates annual frequency of an exceedence of a given probability."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef classical_damage(\n        fragility_functions, hazard_imls, hazard_poes,\n        investigation_time, risk_investigation_time):\n    \"\"\"\n    :param fragility_functions:\n        a list of fragility functions for each damage state\n    :param hazard_imls:\n        Intensity Measure Levels\n    :param hazard_poes:\n        hazard curve\n    :param investigation_time:\n        hazard investigation time\n    :param risk_investigation_time:\n        risk investigation time\n    :returns:\n        an array of M probabilities of occurrence where M is the numbers\n        of damage states.\n    \"\"\"\n    spi = fragility_functions.steps_per_interval\n    if spi and spi > 1:  # interpolate\n        imls = numpy.array(fragility_functions.interp_imls)\n        min_val, max_val = hazard_imls[0], hazard_imls[-1]\n        assert min_val > 0, hazard_imls  # sanity check\n        numpy.putmask(imls, imls < min_val, min_val)\n        numpy.putmask(imls, imls > max_val, max_val)\n        poes = interpolate.interp1d(hazard_imls, hazard_poes)(imls)\n    else:\n        imls = (hazard_imls if fragility_functions.format == 'continuous'\n                else fragility_functions.imls)\n        poes = numpy.array(hazard_poes)\n    afe = annual_frequency_of_exceedence(poes, investigation_time)\n    annual_frequency_of_occurrence = pairwise_diff(\n        pairwise_mean([afe[0]] + list(afe) + [afe[-1]]))\n    poes_per_damage_state = []\n    for ff in fragility_functions:\n        frequency_of_exceedence_per_damage_state = numpy.dot(\n            annual_frequency_of_occurrence, list(map(ff, imls)))\n        poe_per_damage_state = 1. - numpy.exp(\n            - frequency_of_exceedence_per_damage_state *\n            risk_investigation_time)\n        poes_per_damage_state.append(poe_per_damage_state)\n    poos = pairwise_diff([1] + poes_per_damage_state + [0])\n    return poos", "response": "This function computes the classical damage of the given fragility functions at the given time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef classical(vulnerability_function, hazard_imls, hazard_poes, loss_ratios):\n    assert len(hazard_imls) == len(hazard_poes), (\n        len(hazard_imls), len(hazard_poes))\n    vf = vulnerability_function\n    imls = vf.mean_imls()\n    lrem = vf.loss_ratio_exceedance_matrix(loss_ratios)\n\n    # saturate imls to hazard imls\n    min_val, max_val = hazard_imls[0], hazard_imls[-1]\n    numpy.putmask(imls, imls < min_val, min_val)\n    numpy.putmask(imls, imls > max_val, max_val)\n\n    # interpolate the hazard curve\n    poes = interpolate.interp1d(hazard_imls, hazard_poes)(imls)\n\n    # compute the poos\n    pos = pairwise_diff(poes)\n    lrem_po = numpy.empty(lrem.shape)\n    for idx, po in enumerate(pos):\n        lrem_po[:, idx] = lrem[:, idx] * po  # column * po\n    return numpy.array([loss_ratios, lrem_po.sum(axis=1)])", "response": "Compute the classical vulnerability curve for a given vulnerability function and hazard intensity measure type and levels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef conditional_loss_ratio(loss_ratios, poes, probability):\n    assert len(loss_ratios) >= 3, loss_ratios\n    rpoes = poes[::-1]\n    if probability > poes[0]:  # max poes\n        return 0.0\n    elif probability < poes[-1]:  # min PoE\n        return loss_ratios[-1]\n    if probability in poes:\n        return max([loss\n                    for i, loss in enumerate(loss_ratios)\n                    if probability == poes[i]])\n    else:\n        interval_index = bisect.bisect_right(rpoes, probability)\n\n        if interval_index == len(poes):  # poes are all nan\n            return float('nan')\n        elif interval_index == 1:  # boundary case\n            x1, x2 = poes[-2:]\n            y1, y2 = loss_ratios[-2:]\n        else:\n            x1, x2 = poes[-interval_index-1:-interval_index + 1]\n            y1, y2 = loss_ratios[-interval_index-1:-interval_index + 1]\n\n        return (y2 - y1) / (x2 - x1) * (probability - x1) + y1", "response": "This function returns the loss ratio corresponding to the given probability."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the insured losses for the given asset and losses.", "response": "def insured_losses(losses, deductible, insured_limit):\n    \"\"\"\n    :param losses: an array of ground-up loss ratios\n    :param float deductible: the deductible limit in fraction form\n    :param float insured_limit: the insured limit in fraction form\n\n    Compute insured losses for the given asset and losses, from the point\n    of view of the insurance company. For instance:\n\n    >>> insured_losses(numpy.array([3, 20, 101]), 5, 100)\n    array([ 0, 15, 95])\n\n    - if the loss is 3 (< 5) the company does not pay anything\n    - if the loss is 20 the company pays 20 - 5 = 15\n    - if the loss is 101 the company pays 100 - 5 = 95\n    \"\"\"\n    return numpy.piecewise(\n        losses,\n        [losses < deductible, losses > insured_limit],\n        [0, insured_limit - deductible, lambda x: x - deductible])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes an insured loss ratio curve given a loss ratio curve", "response": "def insured_loss_curve(curve, deductible, insured_limit):\n    \"\"\"\n    Compute an insured loss ratio curve given a loss ratio curve\n\n    :param curve: an array 2 x R (where R is the curve resolution)\n    :param float deductible: the deductible limit in fraction form\n    :param float insured_limit: the insured limit in fraction form\n\n    >>> losses = numpy.array([3, 20, 101])\n    >>> poes = numpy.array([0.9, 0.5, 0.1])\n    >>> insured_loss_curve(numpy.array([losses, poes]), 5, 100)\n    array([[ 3.        , 20.        ],\n           [ 0.85294118,  0.5       ]])\n    \"\"\"\n    losses, poes = curve[:, curve[0] <= insured_limit]\n    limit_poe = interpolate.interp1d(\n        *curve, bounds_error=False, fill_value=1)(deductible)\n    return numpy.array([\n        losses,\n        numpy.piecewise(poes, [poes > limit_poe], [limit_poe, lambda x: x])])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bcr(eal_original, eal_retrofitted, interest_rate,\n        asset_life_expectancy, asset_value, retrofitting_cost):\n    \"\"\"\n    Compute the Benefit-Cost Ratio.\n\n    BCR = (EALo - EALr)(1-exp(-r*t))/(r*C)\n\n    Where:\n\n    * BCR -- Benefit cost ratio\n    * EALo -- Expected annual loss for original asset\n    * EALr -- Expected annual loss for retrofitted asset\n    * r -- Interest rate\n    * t -- Life expectancy of the asset\n    * C -- Retrofitting cost\n    \"\"\"\n    return ((eal_original - eal_retrofitted) * asset_value *\n            (1 - numpy.exp(- interest_rate * asset_life_expectancy)) /\n            (interest_rate * retrofitting_cost))", "response": "Compute the Benefit - Cost Ratio."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pairwise_mean(values):\n    \"Averages between a value and the next value in a sequence\"\n    return numpy.array([numpy.mean(pair) for pair in pairwise(values)])", "response": "Averages between a value and the next value in a sequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving an N x M matrix returns mean and std computed on the rows", "response": "def mean_std(fractions):\n    \"\"\"\n    Given an N x M matrix, returns mean and std computed on the rows,\n    i.e. two M-dimensional vectors.\n    \"\"\"\n    n = fractions.shape[0]\n    if n == 1:  # avoid warnings when computing the stddev\n        return fractions[0], numpy.ones_like(fractions[0]) * numpy.nan\n    return numpy.mean(fractions, axis=0), numpy.std(fractions, axis=0, ddof=1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a composite array of loss maps for a given set of curves and a list of conditional loss poes", "response": "def loss_maps(curves, conditional_loss_poes):\n    \"\"\"\n    :param curves: an array of loss curves\n    :param conditional_loss_poes: a list of conditional loss poes\n    :returns: a composite array of loss maps with the same shape\n    \"\"\"\n    loss_maps_dt = numpy.dtype([('poe-%s' % poe, F32)\n                                for poe in conditional_loss_poes])\n    loss_maps = numpy.zeros(curves.shape, loss_maps_dt)\n    for idx, curve in numpy.ndenumerate(curves):\n        for poe in conditional_loss_poes:\n            loss_maps['poe-%s' % poe][idx] = conditional_loss_ratio(\n                curve['losses'], curve['poes'], poe)\n    return loss_maps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef broadcast(func, composite_array, *args):\n    dic = {}\n    dtypes = []\n    for name in composite_array.dtype.names:\n        dic[name] = func(composite_array[name], *args)\n        dtypes.append((name, dic[name].dtype))\n    res = numpy.zeros(dic[name].shape, numpy.dtype(dtypes))\n    for name in dic:\n        res[name] = dic[name]\n    return res", "response": "Broadcast an array function over a composite array"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef average_loss(lc):\n    losses, poes = (lc['loss'], lc['poe']) if lc.dtype.names else lc\n    return -pairwise_diff(losses) @ pairwise_mean(poes)", "response": "Given a loss curve array with poe and loss fields compute the average loss on a period of time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_curves_eb(curves):\n    # we assume non-decreasing losses, so losses[-1] is the maximum loss\n    non_zero_curves = [(losses, poes)\n                       for losses, poes in curves if losses[-1] > 0]\n    if not non_zero_curves:  # no damage. all zero curves\n        return curves[0][0], numpy.array([poes for _losses, poes in curves])\n    else:  # standard case\n        max_losses = [losses[-1] for losses, _poes in non_zero_curves]\n        reference_curve = non_zero_curves[numpy.argmax(max_losses)]\n        loss_ratios = reference_curve[0]\n        curves_poes = [interpolate.interp1d(\n            losses, poes, bounds_error=False, fill_value=0)(loss_ratios)\n            for losses, poes in curves]\n        # fix degenerated case with flat curve\n        for cp in curves_poes:\n            if numpy.isnan(cp[0]):\n                cp[0] = 0\n    return loss_ratios, numpy.array(curves_poes)", "response": "This function is a more sophisticated version of normalize_curves used in the event\nAddon based calculator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the loss curve_dt for the given dictionary loss_type - > curve_resolution - > insured_losses - > loss curve_dt for all the classes in the given dictionary loss_type - > curve_resolution - > insured_losses - > loss curve_dt for all classes in the given dictionary loss_type - > loss curve", "response": "def build_loss_curve_dt(curve_resolution, insured_losses=False):\n    \"\"\"\n    :param curve_resolution:\n        dictionary loss_type -> curve_resolution\n    :param insured_losses:\n        configuration parameter\n    :returns:\n       loss_curve_dt\n    \"\"\"\n    lc_list = []\n    for lt in sorted(curve_resolution):\n        C = curve_resolution[lt]\n        pairs = [('losses', (F32, C)), ('poes', (F32, C))]\n        lc_dt = numpy.dtype(pairs)\n        lc_list.append((str(lt), lc_dt))\n    if insured_losses:\n        for lt in sorted(curve_resolution):\n            C = curve_resolution[lt]\n            pairs = [('losses', (F32, C)), ('poes', (F32, C))]\n            lc_dt = numpy.dtype(pairs)\n            lc_list.append((str(lt) + '_ins', lc_dt))\n    loss_curve_dt = numpy.dtype(lc_list) if lc_list else None\n    return loss_curve_dt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_periods(eff_time, num_losses):\n    assert eff_time >= 2, 'eff_time too small: %s' % eff_time\n    assert num_losses >= 2, 'num_losses too small: %s' % num_losses\n    min_time = eff_time / num_losses\n    period = 1\n    periods = []\n    loop = True\n    while loop:\n        for val in [1, 2, 5]:\n            time = period * val\n            if time >= min_time:\n                if time > eff_time:\n                    loop = False\n                    break\n                periods.append(time)\n        period *= 10\n    return U32(periods)", "response": "This function returns a list of 32 bit periods for a given time range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef losses_by_period(losses, return_periods, num_events=None, eff_time=None):\n    if len(losses) == 0:  # zero-curve\n        return numpy.zeros(len(return_periods))\n    if num_events is None:\n        num_events = len(losses)\n    elif num_events < len(losses):\n        raise ValueError(\n            'There are not enough events (%d) to compute the loss curve '\n            'from %d losses' % (num_events, len(losses)))\n    if eff_time is None:\n        eff_time = return_periods[-1]\n    losses = numpy.sort(losses)\n    num_zeros = num_events - len(losses)\n    if num_zeros:\n        losses = numpy.concatenate(\n            [numpy.zeros(num_zeros, losses.dtype), losses])\n    periods = eff_time / numpy.arange(num_events, 0., -1)\n    rperiods = [rp if periods[0] <= rp <= periods[-1] else numpy.nan\n                for rp in return_periods]\n    curve = numpy.interp(numpy.log(rperiods), numpy.log(periods), losses)\n    return curve", "response": "Compute the loss curve for a given set of simulated losses."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interpolate(self, gmvs):\n        # gmvs are clipped to max(iml)\n        gmvs_curve = numpy.piecewise(\n            gmvs, [gmvs > self.imls[-1]], [self.imls[-1], lambda x: x])\n        idxs = gmvs_curve >= self.imls[0]  # indices over the minimum\n        gmvs_curve = gmvs_curve[idxs]\n        return self._mlr_i1d(gmvs_curve), self._cov_for(gmvs_curve), idxs", "response": "Interpolate the loss ratios for the given intensity measure levels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(self, means, covs, idxs, epsilons=None):\n        if epsilons is None:\n            return means\n        self.set_distribution(epsilons)\n        res = self.distribution.sample(means, covs, means * covs, idxs)\n        return res", "response": "Sample the epsilons and apply the corrections to the means."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new vulnerability function that is strictly increasing.", "response": "def strictly_increasing(self):\n        \"\"\"\n        :returns:\n          a new vulnerability function that is strictly increasing.\n          It is built by removing piece of the function where the mean\n          loss ratio is constant.\n        \"\"\"\n        imls, mlrs, covs = [], [], []\n\n        previous_mlr = None\n        for i, mlr in enumerate(self.mean_loss_ratios):\n            if previous_mlr == mlr:\n                continue\n            else:\n                mlrs.append(mlr)\n                imls.append(self.imls[i])\n                covs.append(self.covs[i])\n                previous_mlr = mlr\n\n        return self.__class__(\n            self.id, self.imt, imls, mlrs, covs, self.distribution_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits the mean loss ratios into two sets of loss ratios.", "response": "def mean_loss_ratios_with_steps(self, steps):\n        \"\"\"\n        Split the mean loss ratios, producing a new set of loss ratios. The new\n        set of loss ratios always includes 0.0 and 1.0\n\n        :param int steps:\n            the number of steps we make to go from one loss\n            ratio to the next. For example, if we have [0.5, 0.7]::\n\n             steps = 1 produces [0.0,  0.5, 0.7, 1]\n             steps = 2 produces [0.0, 0.25, 0.5, 0.6, 0.7, 0.85, 1]\n             steps = 3 produces [0.0, 0.17, 0.33, 0.5, 0.57, 0.63,\n                                 0.7, 0.8, 0.9, 1]\n        \"\"\"\n        loss_ratios = self.mean_loss_ratios\n\n        if min(loss_ratios) > 0.0:\n            # prepend with a zero\n            loss_ratios = numpy.concatenate([[0.0], loss_ratios])\n\n        if max(loss_ratios) < 1.0:\n            # append a 1.0\n            loss_ratios = numpy.concatenate([loss_ratios, [1.0]])\n\n        return fine_graining(loss_ratios, steps)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cov_for(self, imls):\n        return self._covs_i1d(\n            numpy.piecewise(\n                imls,\n                [imls > self.imls[-1], imls < self.imls[0]],\n                [self.imls[-1], self.imls[0], lambda x: x]))", "response": "Calculate the covariance of the given set of imls."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the LREM matrix for the given loss ratios and imls.", "response": "def loss_ratio_exceedance_matrix(self, loss_ratios):\n        \"\"\"\n        Compute the LREM (Loss Ratio Exceedance Matrix).\n        \"\"\"\n        # LREM has number of rows equal to the number of loss ratios\n        # and number of columns equal to the number of imls\n        lrem = numpy.empty((len(loss_ratios), len(self.imls)))\n        for row, loss_ratio in enumerate(loss_ratios):\n            for col, (mean_loss_ratio, stddev) in enumerate(\n                    zip(self.mean_loss_ratios, self.stddevs)):\n                lrem[row, col] = self.distribution.survival(\n                    loss_ratio, mean_loss_ratio, stddev)\n        return lrem"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mean_imls(self):\n        return numpy.array(\n            [max(0, self.imls[0] - (self.imls[1] - self.imls[0]) / 2.)] +\n            [numpy.mean(pair) for pair in pairwise(self.imls)] +\n            [self.imls[-1] + (self.imls[-1] - self.imls[-2]) / 2.])", "response": "Compute the mean IMLs for the given vulnerability function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate(self, gmvs):\n        # gmvs are clipped to max(iml)\n        gmvs_curve = numpy.piecewise(\n            gmvs, [gmvs > self.imls[-1]], [self.imls[-1], lambda x: x])\n        idxs = gmvs_curve >= self.imls[0]  # indices over the minimum\n        gmvs_curve = gmvs_curve[idxs]\n        return self._probs_i1d(gmvs_curve), numpy.zeros_like(gmvs_curve), idxs", "response": "Interpolate the intensity measure levels in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample(self, probs, _covs, idxs, epsilons):\n        self.set_distribution(epsilons)\n        return self.distribution.sample(self.loss_ratios, probs)", "response": "Sample the. loss_ratios with the given probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self, limit_states, discretization, steps_per_interval):\n        new = copy.copy(self)\n        add_zero = (self.format == 'discrete' and\n                    self.nodamage and self.nodamage <= self.imls[0])\n        new.imls = build_imls(new, discretization)\n        if steps_per_interval > 1:\n            new.interp_imls = build_imls(  # passed to classical_damage\n                new, discretization, steps_per_interval)\n        for i, ls in enumerate(limit_states):\n            data = self.array[i]\n            if self.format == 'discrete':\n                if add_zero:\n                    new.append(FragilityFunctionDiscrete(\n                        ls, [self.nodamage] + self.imls,\n                        numpy.concatenate([[0.], data]),\n                        self.nodamage))\n                else:\n                    new.append(FragilityFunctionDiscrete(\n                        ls, self.imls, data, self.nodamage))\n            else:  # continuous\n                new.append(FragilityFunctionContinuous(\n                    ls, data['mean'], data['stddev']))\n        return new", "response": "Builds a new FragilityFunctionList instance from the given limit states and discretization."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a new FragilityModel instance with the values in self. limitStates replaced with FragilityFunctionList instances.", "response": "def build(self, continuous_fragility_discretization, steps_per_interval):\n        \"\"\"\n        Return a new FragilityModel instance, in which the values have been\n        replaced with FragilityFunctionList instances.\n\n        :param continuous_fragility_discretization:\n            configuration parameter\n        :param steps_per_interval:\n            configuration parameter\n        \"\"\"\n        newfm = copy.copy(self)\n        for key, ffl in self.items():\n            newfm[key] = ffl.build(self.limitStates,\n                                   continuous_fragility_discretization,\n                                   steps_per_interval)\n        return newfm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pair(self, array, stats):\n        if len(self.weights) > 1 and stats:\n            statnames, statfuncs = zip(*stats)\n            array_stats = compute_stats2(array, statfuncs, self.weights)\n        else:\n            array_stats = None\n        return array, array_stats", "response": "Pair the array and stats for a single entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a new logarithmic loss table from a list of losses by event.", "response": "def build(self, losses_by_event, stats=()):\n        \"\"\"\n        :param losses_by_event:\n            the aggregate loss table with shape R -> (E, L)\n        :param stats:\n            list of pairs [(statname, statfunc), ...]\n        :returns:\n            two arrays with shape (P, R, L) and (P, S, L)\n        \"\"\"\n        P, R = len(self.return_periods), len(self.weights)\n        L = len(self.loss_dt.names)\n        array = numpy.zeros((P, R, L), F32)\n        for r in losses_by_event:\n            num_events = self.num_events[r]\n            losses = losses_by_event[r]\n            for l, lt in enumerate(self.loss_dt.names):\n                ls = losses[:, l].flatten()  # flatten only in ucerf\n                # NB: do not use squeeze or the gmf_ebrisk tests will break\n                lbp = losses_by_period(\n                    ls, self.return_periods, num_events, self.eff_time)\n                array[:, r, l] = lbp\n        return self.pair(array, stats)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_pair(self, losses, stats):\n        P, R = len(self.return_periods), len(self.weights)\n        assert len(losses) == R, len(losses)\n        array = numpy.zeros((P, R), F32)\n        for r, ls in enumerate(losses):\n            ne = self.num_events.get(r, 0)\n            if ne:\n                array[:, r] = losses_by_period(\n                    ls, self.return_periods, ne, self.eff_time)\n        return self.pair(array, stats)", "response": "Builds a pair of events from a list of lists with R elements."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_maps(self, losses, clp, stats=()):\n        shp = losses.shape[:2] + (len(clp), len(losses.dtype))  # (A, R, C, LI)\n        array = numpy.zeros(shp, F32)\n        for lti, lt in enumerate(losses.dtype.names):\n            for a, losses_ in enumerate(losses[lt]):\n                for r, ls in enumerate(losses_):\n                    for c, poe in enumerate(clp):\n                        clratio = conditional_loss_ratio(ls, self.poes, poe)\n                        array[a, r, c, lti] = clratio\n        return self.pair(array, stats)", "response": "Builds a loss_maps array from a list of losses and a list of conditional loss poes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_loss_maps(self, losses, clp, stats=()):\n        array = numpy.zeros((len(clp), len(losses)), F32)\n        for r, ls in enumerate(losses):\n            if len(ls) < 2:\n                continue\n            for c, poe in enumerate(clp):\n                array[c, r] = conditional_loss_ratio(ls, self.poes, poe)\n        return self.pair(array, stats)", "response": "Builds a list of pairs of loss maps for a set of conditional losses and a set of conditional losses."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef store_rlzs_by_grp(dstore):\n    lst = []\n    assoc = dstore['csm_info'].get_rlzs_assoc()\n    for grp, arr in assoc.by_grp().items():\n        for gsim_id, rlzs in enumerate(arr):\n            lst.append((int(grp[4:]), gsim_id, rlzs))\n    dstore['csm_info/rlzs_by_grp'] = numpy.array(lst, rlzs_by_grp_dt)", "response": "Save in the datastore a composite array with fields ( grp_id gsim_id rlzs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_gmfs(rupgetter, srcfilter, param, monitor):\n    getter = GmfGetter(rupgetter, srcfilter, param['oqparam'])\n    with monitor('getting ruptures'):\n        getter.init()\n    return getter.compute_gmfs_curves(monitor)", "response": "Compute GMFs and optionally hazard curves for a single rupture."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncombines the hazard curves coming from two different calculations.", "response": "def combine_mean_curves(calc_big, calc_small):\n    \"\"\"\n    Combine the hazard curves coming from two different calculations.\n    The result will be the hazard curves of calc_big, updated on the sites\n    in common with calc_small with the PoEs of calc_small. For instance:\n    calc_big = USA, calc_small = California\n    \"\"\"\n    dstore_big = datastore.read(calc_big)\n    dstore_small = datastore.read(calc_small)\n    sitecol_big = dstore_big['sitecol']\n    sitecol_small = dstore_small['sitecol']\n    site_id_big = {(lon, lat): sid for sid, lon, lat in zip(\n        sitecol_big.sids, sitecol_big.lons, sitecol_big.lats)}\n    site_id_small = {(lon, lat): sid for sid, lon, lat in zip(\n        sitecol_small.sids, sitecol_small.lons, sitecol_small.lats)}\n    common = set(site_id_big) & set(site_id_small)\n    if not common:\n        raise RuntimeError('There are no common sites between calculation '\n                           '%d and %d' % (calc_big, calc_small))\n    sids_small = [site_id_small[lonlat] for lonlat in common]\n    pmap_big = PmapGetter(dstore_big).get_mean()  # USA\n    pmap_small = PmapGetter(dstore_big, sids=sids_small).get_mean()  # Cal\n    for lonlat in common:\n        pmap_big[site_id_big[lonlat]] |= pmap_small.get(\n            site_id_small[lonlat], 0)\n    out = 'combine_%d_%d.hdf5' % (calc_big, calc_small)\n    with hdf5.File(out, 'w') as h5:\n        h5['hcurves/mean'] = pmap_big\n        h5['oqparam'] = dstore_big['oqparam']\n        h5['sitecol'] = dstore_big['sitecol']\n    print('Generated %s' % out)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a complex fault surface from a list of fault edges.", "response": "def create_geometry(self, input_geometry,  mesh_spacing=1.0):\n        '''\n        If geometry is defined as a numpy array then create instance of\n        nhlib.geo.line.Line class, otherwise if already instance of class\n        accept class\n\n        :param input_geometry:\n            List of at least two fault edges of the fault source from\n            shallowest to deepest. Each edge can be represented as as either\n            i) instance of nhlib.geo.polygon.Polygon class\n            ii) numpy.ndarray [Longitude, Latitude, Depth]\n\n        :param float mesh_spacing:\n            Spacing of the fault mesh (km) {default = 1.0}\n\n        '''\n        if not isinstance(input_geometry, list) or len(input_geometry) < 2:\n            raise ValueError('Complex fault geometry incorrectly defined')\n\n        self.fault_edges = []\n        for edge in input_geometry:\n            if not isinstance(edge, Line):\n                if not isinstance(edge, np.ndarray):\n                    raise ValueError('Unrecognised or unsupported geometry '\n                                     'definition')\n                else:\n                    self.fault_edges.append(Line([Point(row[0], row[1], row[2])\n                                                  for row in edge]))\n            else:\n                self.fault_edges.append(edge)\n            # Updates the upper and lower sesmogenic depths to reflect geometry\n            self._get_minmax_edges(edge)\n        # Build fault surface\n        self.geometry = ComplexFaultSurface.from_fault_data(self.fault_edges,\n                                                            mesh_spacing)\n        # Get a mean dip\n        self.dip = self.geometry.get_dip()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_minmax_edges(self, edge):\n        '''\n        Updates the upper and lower depths based on the input edges\n        '''\n        if isinstance(edge, Line):\n            # For instance of line class need to loop over values\n            depth_vals = np.array([node.depth for node in edge.points])\n        else:\n            depth_vals = edge[:, 2]\n\n        temp_upper_depth = np.min(depth_vals)\n        if not self.upper_depth:\n            self.upper_depth = temp_upper_depth\n        else:\n            if temp_upper_depth < self.upper_depth:\n                self.upper_depth = temp_upper_depth\n\n        temp_lower_depth = np.max(depth_vals)\n        if not self.lower_depth:\n            self.lower_depth = temp_lower_depth\n        else:\n            if temp_lower_depth > self.lower_depth:\n                self.lower_depth = temp_lower_depth", "response": "Updates the upper and lower depths based on the input edges\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_oqhazardlib_source(self, tom, mesh_spacing, use_defaults=False):\n        if not self.mfd:\n            raise ValueError(\"Cannot write to hazardlib without MFD\")\n        return ComplexFaultSource(\n            self.id,\n            self.name,\n            self.trt,\n            self.mfd,\n            mesh_spacing,\n            conv.mag_scale_rel_to_hazardlib(self.mag_scale_rel, use_defaults),\n            conv.render_aspect_ratio(self.rupt_aspect_ratio, use_defaults),\n            tom,\n            self.fault_edges,\n            self.rake)", "response": "Creates an instance of the ComplexFaultSource class based on the properties of the ComplexFault object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n\n        C = self.COEFFS[imt]\n\n        mean = (self._get_magnitude_term(C, rup.mag) +\n                self._get_distance_term(C, dists.rjb, rup.mag) +\n                self._get_site_term(C, sites.vs30))\n\n        # Units of GMPE are in terms of m/s (corrected in an Erratum)\n        # Convert to g\n        if imt.name in \"SA PGA\":\n            mean = np.log(np.exp(mean) / g)\n        else:\n            # For PGV convert from m/s to cm/s/s\n            mean = np.log(np.exp(mean) * 100.)\n\n        # Get standard deviations\n        stddevs = self._get_stddevs(C, stddev_types, dists.rjb.shape)\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the magnitude scaling term in equation 3.", "response": "def _get_magnitude_term(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling term - equation 3\n        \"\"\"\n        if mag >= self.CONSTS[\"Mh\"]:\n            return C[\"e1\"] + C[\"b3\"] * (mag - self.CONSTS[\"Mh\"])\n        else:\n            return C[\"e1\"] + (C[\"b1\"] * (mag - self.CONSTS[\"Mh\"])) +\\\n                (C[\"b2\"] * (mag - self.CONSTS[\"Mh\"]) ** 2.)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_distance_term(self, C, rjb, mag):\n        c_3 = self._get_anelastic_coeff(C)\n        rval = np.sqrt(rjb ** 2. + C[\"h\"] ** 2.)\n        return (C[\"c1\"] + C[\"c2\"] * (mag - self.CONSTS[\"Mref\"])) *\\\n            np.log(rval / self.CONSTS[\"Rref\"]) +\\\n            c_3 * (rval - self.CONSTS[\"Rref\"])", "response": "Returns the general distance scaling term - equation 2."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn only a linear site amplification term", "response": "def _get_site_term(self, C, vs30):\n        \"\"\"\n        Returns only a linear site amplification term\n        \"\"\"\n        dg1, dg2 = self._get_regional_site_term(C)\n        return (C[\"g1\"] + dg1) + (C[\"g2\"] + dg2) * np.log(vs30)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stddevs(self, C, stddev_types, num_sites):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            for stddev_type in stddev_types)\n        stddevs = [np.zeros(num_sites) + C['SigmaTot'] for _ in stddev_types]\n        return stddevs", "response": "Returns the standard deviations as defined in tables below\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget site type dummy variables which classified the sites into different site classes.", "response": "def _get_site_type_dummy_variables(self, sites):\n        \"\"\"\n        Get site type dummy variables, which classified the sites into\n        different site classes based on the shear wave velocity in the\n        upper 30 m (Vs30) according to the EC8 (CEN 2003):\n        class A: Vs30 > 800 m/s\n        class B: Vs30 = 360 - 800 m/s\n        class C*: Vs30 = 180 - 360 m/s\n        class D: Vs30 < 180 m/s\n        *Not computed by this GMPE\n        \"\"\"\n        ssa = np.zeros(len(sites.vs30))\n        ssb = np.zeros(len(sites.vs30))\n        ssd = np.zeros(len(sites.vs30))\n        # Class D; Vs30 < 180 m/s.\n        idx = (sites.vs30 < 180.0)\n        ssd[idx] = 1.0\n        # Class B; 360 m/s <= Vs30 <= 800 m/s.\n        idx = (sites.vs30 >= 360.0) & (sites.vs30 < 800.0)\n        ssb[idx] = 1.0\n        # Class A; Vs30 > 800 m/s.\n        idx = (sites.vs30 >= 800.0)\n        ssa[idx] = 1.0\n\n        for value in sites.vs30:\n            if 180 <= value < 360:\n                raise Exception(\n                    'GMPE does not consider site class C (Vs30 = 180-360 m/s)')\n\n        return ssa, ssb, ssd"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the distance function equation 5.", "response": "def _compute_distance(self, rup, dists, C):\n        \"\"\"\n        Compute the distance function, equation (5).\n        \"\"\"\n        rval = np.sqrt(dists.repi ** 2 + C['h'] ** 2)\n        return C['c1'] * np.log10(rval)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_distance(self, rup, dists, C):\n        mref = 3.6\n        rref = 1.0\n        rval = np.sqrt(dists.rhypo ** 2 + C['h'] ** 2)\n        return (C['c1'] + C['c2'] * (rup.mag - mref)) *\\\n            np.log10(rval / rref) + C['c3'] * (rval - rref)", "response": "Compute the distance function equation 9"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the mean and standard deviation for the specified intensity measure type and site type.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        # extract dictionaries of coefficients specific to required\n        # intensity measure type\n        C = self.COEFFS[imt]\n\n        # Deltas for Tectonic Region Type and rake angles\n        delta_R, delta_S, delta_V, delta_I = self._get_deltas(rup.rake)\n\n        mean = self._compute_mean(C, rup.mag, dists.rrup, rup.hypo_depth,\n                                  delta_R, delta_S, delta_V, delta_I,\n                                  sites.vs30)\n\n        stddevs = self._get_stddevs(C, stddev_types, sites.vs30.size)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_mean(self, C, mag, rrup, hypo_depth, delta_R, delta_S,\n                      delta_V, delta_I, vs30):\n\n        \"\"\"\n        Compute MMI Intensity Value as per Equation in Table 5 and\n        Table 7 pag 198.\n        \"\"\"\n        # mean is calculated for all the 4 classes using the same equation.\n        # For DowrickRhoades2005SSlab, the coefficients which don't appear in\n        # Model 3 equationare assigned to zero\n\n        mean = (C['A1'] + (C['A2'] + C['A2R'] * delta_R + C['A2V'] * delta_V) *\n                mag + (C['A3'] + C['A3S'] * delta_S + C['A3V'] * delta_V) *\n                np.log10(np.power((rrup**3 + C['d']**3), 1.0 / 3.0)) +\n                C['A4'] * hypo_depth + C['A5'] * delta_I)\n\n        # Get S site class term\n        S = self._get_site_class(vs30, mean)\n\n        # Add S amplification term to mean value\n        mean = mean + S\n\n        return mean", "response": "Compute the mean value for the internal class term in the system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_stddevs(self, C, stddev_types, num_sites):\n\n        \"\"\"\n        Return total standard deviation as described in paragraph 5.2 pag 200.\n        \"\"\"\n\n        # interevent stddev\n        sigma_inter = C['tau'] + np.zeros(num_sites)\n        # intraevent std\n        sigma_intra = C['sigma'] + np.zeros(num_sites)\n        std = []\n\n        for stddev_type in stddev_types:\n            if stddev_type == const.StdDev.TOTAL:\n                # equation in section 5.2 page 200\n                std += [np.sqrt(sigma_intra**2 + sigma_inter**2)]\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                std.append(sigma_intra)\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                std.append(sigma_inter)\n\n        return std", "response": "Return total standard deviation as described in paragraph 5. 2 pag 200."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn site class for given vs30 and MMI.", "response": "def _get_site_class(self, vs30, mmi_mean):\n        \"\"\"\n        Return site class flag for:\n        Class E - Very Soft Soil        vs30 < 180\n        Class D - Deep or Soft Soil     vs30 >= 180 and vs30 <= 360\n        Class C - Shallow Soil          vs30 > 360 and vs30 <= 760\n        Class B - Rock                  vs30 > 760 and vs30 <= 1500\n        Class A - Strong Rock           vs30 >= 180 and vs30 <= 360\n        The S site class is equal to\n            S = c1 if MMI <= 7\n            S = c1 - d *(MMI - 7.0) if 7<MMI<9.5\n            S = c2 if MMI >= 9.5\n        \"\"\"\n\n        if vs30[0] < 180:\n            c1 = 1.0\n            c2 = -0.25\n            d = 0.5\n        elif vs30[0] >= 180 and vs30[0] <= 360:\n            c1 = 0.5\n            c2 = -0.125\n            d = 0.25\n        elif vs30[0] > 360 and vs30[0] <= 760:\n            c1 = 0.\n            c2 = 0.\n            d = 0.\n        elif vs30[0] > 760 and vs30[0] <= 1500:\n            c1 = -0.5\n            c2 = 0.125\n            d = -0.25\n        elif vs30[0] > 1500:\n            c1 = -1.0\n            c2 = 0.25\n            d = -0.5\n\n        S = np.zeros_like(vs30)\n\n        for i in range(vs30.size):\n            if mmi_mean[i] <= 7.0:\n                S[i] += c1\n            elif mmi_mean[i] > 7 and mmi_mean[i] < 9.5:\n                S[i] += c1 - d * (mmi_mean[i] - 7.0)\n            else:\n                S[i] += c2\n\n        return S"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the value of deltas for the given RAK", "response": "def _get_deltas(self, rake):\n        \"\"\"\n        Return the value of deltas (delta_R, delta_S, delta_V, delta_I),\n        as defined in \"Table 5: Model 1\" pag 198\n        \"\"\"\n        # delta_R = 1 for reverse focal mechanism (45<rake<135)\n        # and for interface events, 0 for all other events\n        # delta_S = 1 for Strike-slip focal mechanisms (0<=rake<=45) or\n        # (135<=rake<=180) or (-45<=rake<=0), 0 for all other events\n        # delta_V = 1 for TVZ events, 0 for all other events\n        # delta_I = 1 for interface events, 0 for all other events\n\n        # All deltas = 0 for Model 3: Deep Region, pag 198\n\n        delta_R, delta_S = 0, 0\n        delta_V, delta_I = 0, 0\n\n        if rake > 45.0 and rake < 135.0:\n            delta_R = 1\n\n        if (rake >= 0.0 and rake <= 45.0) or \\\n           (rake >= 135 and rake <= 180.0) or \\\n           (rake >= -180.0 and rake <= -135.0) or \\\n           (rake >= -45.0 and rake < 0.0):\n            delta_S = 1\n\n        return delta_R, delta_S, delta_V, delta_I"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_deltas(self, rake):\n        # All deltas = 0 for DowrickRhoades2005SSlab Model 3: Deep Region,\n        # pag 198\n\n        delta_R, delta_S = 0, 0\n        delta_V, delta_I = 0, 0\n\n        return delta_R, delta_S, delta_V, delta_I", "response": "Return the value of deltas for the current rake entry"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the sites and assets for a given calculation ID.", "response": "def plot_assets(calc_id=-1, site_model=False):\n    \"\"\"\n    Plot the sites and the assets\n    \"\"\"\n    # NB: matplotlib is imported inside since it is a costly import\n    import matplotlib.pyplot as p\n    from openquake.hmtk.plotting.patch import PolygonPatch\n    dstore = util.read(calc_id)\n    try:\n        region = dstore['oqparam'].region\n    except KeyError:\n        region = None\n    sitecol = dstore['sitecol']\n    try:\n        assetcol = dstore['assetcol'].value\n    except AttributeError:\n        assetcol = dstore['assetcol'].array\n    fig = p.figure()\n    ax = fig.add_subplot(111)\n    if region:\n        pp = PolygonPatch(shapely.wkt.loads(region), alpha=0.1)\n        ax.add_patch(pp)\n    ax.grid(True)\n    if site_model and 'site_model' in dstore:\n        sm = dstore['site_model']\n        sm_lons, sm_lats = sm['lon'], sm['lat']\n        if len(sm_lons) > 1 and cross_idl(*sm_lons):\n            sm_lons %= 360\n        p.scatter(sm_lons, sm_lats, marker='.', color='orange')\n    p.scatter(sitecol.complete.lons, sitecol.complete.lats, marker='.',\n              color='gray')\n    p.scatter(assetcol['lon'], assetcol['lat'], marker='.', color='green')\n    p.scatter(sitecol.lons, sitecol.lats, marker='+', color='black')\n    if 'discarded' in dstore:\n        disc = numpy.unique(dstore['discarded'].value[['lon', 'lat']])\n        p.scatter(disc['lon'], disc['lat'], marker='x', color='red')\n    p.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a polygon containing the bounding box of the catalogue", "response": "def get_catalogue_bounding_polygon(catalogue):\n    '''\n    Returns a polygon containing the bounding box of the catalogue\n    '''\n    upper_lon = np.max(catalogue.data['longitude'])\n    upper_lat = np.max(catalogue.data['latitude'])\n    lower_lon = np.min(catalogue.data['longitude'])\n    lower_lat = np.min(catalogue.data['latitude'])\n\n    return Polygon([Point(lower_lon, upper_lat), Point(upper_lon, upper_lat),\n                    Point(upper_lon, lower_lat), Point(lower_lon, lower_lat)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new instance of this class from a catalogue and a spacing and dilate.", "response": "def make_from_catalogue(cls, catalogue, spacing, dilate):\n        '''\n        Defines the grid on the basis of the catalogue\n        '''\n        new = cls()\n        cat_bbox = get_catalogue_bounding_polygon(catalogue)\n\n        if dilate > 0:\n            cat_bbox = cat_bbox.dilate(dilate)\n\n        # Define Grid spacing\n        new.update({'xmin': np.min(cat_bbox.lons),\n                    'xmax': np.max(cat_bbox.lons),\n                    'xspc': spacing,\n                    'ymin': np.min(cat_bbox.lats),\n                    'ymax': np.max(cat_bbox.lats),\n                    'yspc': spacing,\n                    'zmin': 0.,\n                    'zmax': np.max(catalogue.data['depth']),\n                    'zspc': np.max(catalogue.data['depth'])})\n\n        if new['zmin'] == new['zmax'] == new['zspc'] == 0:\n            new['zmax'] = new['zspc'] = 1\n\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_analysis(self, catalogue, config, completeness_table=None,\n                     smoothing_kernel=None):\n        '''\n        Runs an analysis of smoothed seismicity in the manner\n        originally implemented by Frankel (1995)\n\n        :param catalogue:\n            Instance of the openquake.hmtk.seismicity.catalogue.Catalogue class\n            catalogue.data dictionary containing the following -\n            'year' - numpy.ndarray vector of years\n            'longitude' - numpy.ndarray vector of longitudes\n            'latitude' - numpy.ndarray vector of latitudes\n            'depth' - numpy.ndarray vector of depths\n\n        :param dict config:\n            Configuration settings of the algorithm:\n            * 'Length_Limit' - Maximum number of bandwidths for use in\n            smoothing (Float)\n            * 'BandWidth' - Bandwidth (km) of the Smoothing Kernel (Float)\n            * 'increment' - Output incremental (True) or cumulative a-value\n            (False)\n\n        :param np.ndarray completeness_table:\n            Completeness of the catalogue assuming evenly spaced magnitudes\n            from most recent bin to oldest bin [year, magnitude]\n\n        :param smoothing_kernel:\n            Smoothing kernel as instance of :class:\n            `openquake.hmtk.seismicity.smoothing.kernels.base.BaseSmoothingKernel`\n\n        :returns:\n            Full smoothed seismicity data as np.ndarray, of the form\n            [Longitude, Latitude, Depth, Observed, Smoothed]\n        '''\n\n        self.catalogue = catalogue\n        if smoothing_kernel:\n            self.kernel = smoothing_kernel\n        else:\n            self.kernel = IsotropicGaussian()\n\n        # If no grid limits are specified then take from catalogue\n        if isinstance(self.grid_limits, list):\n            self.grid_limits = Grid.make_from_list(self.grid_limits)\n            assert self.grid_limits['xmax'] >= self.grid_limits['xmin']\n            assert self.grid_limits['xspc'] > 0.0\n            assert self.grid_limits['ymax'] >= self.grid_limits['ymin']\n            assert self.grid_limits['yspc'] > 0.0\n        elif isinstance(self.grid_limits, float):\n            self.grid_limits = Grid.make_from_catalogue(\n                self.catalogue, self.grid_limits,\n                config['Length_Limit'] * config['BandWidth'])\n\n        completeness_table, mag_inc = utils.get_even_magnitude_completeness(\n            completeness_table,\n            self.catalogue)\n\n        end_year = self.catalogue.end_year\n\n        # Get Weichert factor\n        t_f, _ = utils.get_weichert_factor(self.beta,\n                                           completeness_table[:, 1],\n                                           completeness_table[:, 0],\n                                           end_year)\n        # Get the grid\n        self.create_3D_grid(self.catalogue, completeness_table, t_f, mag_inc)\n        if config['increment']:\n            # Get Hermann adjustment factors\n            fval, fival = utils.hermann_adjustment_factors(\n                self.bval,\n                completeness_table[0, 1], config['increment'])\n            self.data[:, -1] = fval * fival * self.data[:, -1]\n\n        # Apply smoothing\n        smoothed_data, sum_data, sum_smooth = self.kernel.smooth_data(\n            self.data, config, self.use_3d)\n        print('Smoothing Total Rate Comparison - '\n              'Observed: %.6g, Smoothed: %.6g' % (sum_data, sum_smooth))\n        self.data = np.column_stack([self.data, smoothed_data])\n        return self.data", "response": "Runs an analysis of the seismicity in the manner of Frankel 2005."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a 2D grid of observed rates for the given earthquake longitude and latitude.", "response": "def create_2D_grid_simple(self, longitude, latitude, year, magnitude,\n                              completeness_table, t_f=1., mag_inc=0.1):\n        '''\n        Generates the grid from the limits using an approach closer to that of\n        Frankel (1995)\n\n        :param numpy.ndarray longitude:\n            Vector of earthquake longitudes\n\n        :param numpy.ndarray latitude:\n            Vector of earthquake latitudes\n\n        :param numpy.ndarray year:\n            Vector of earthquake years\n\n        :param numpy.ndarray magnitude:\n            Vector of earthquake magnitudes\n\n        :param numpy.ndarray completeness_table:\n            Completeness table\n\n        :param float t_f:\n            Weichert adjustment factor\n\n        :returns:\n           Two-dimensional spatial grid of observed rates\n\n        '''\n        assert mag_inc > 0.\n\n        xlim = np.ceil(\n            (self.grid_limits['xmax'] - self.grid_limits['xmin']) /\n            self.grid_limits['xspc'])\n        ylim = np.ceil(\n            (self.grid_limits['ymax'] - self.grid_limits['ymin']) /\n            self.grid_limits['yspc'])\n        ncolx = int(xlim)\n        ncoly = int(ylim)\n        grid_count = np.zeros(ncolx * ncoly, dtype=float)\n        for iloc in range(0, len(longitude)):\n            dlon = (longitude[iloc] - self.grid_limits['xmin']) /\\\n                self.grid_limits['xspc']\n            if (dlon < 0.) or (dlon > xlim):\n                # Earthquake outside longitude limits\n                continue\n            xcol = int(dlon)\n            if xcol == ncolx:\n                # If longitude is directly on upper grid line then retain\n                xcol = ncolx - 1\n            dlat = fabs(self.grid_limits['ymax'] - latitude[iloc]) /\\\n                self.grid_limits['yspc']\n            if (dlat < 0.) or (dlat > ylim):\n                # Earthquake outside latitude limits\n                continue\n            ycol = int(dlat)  # Correct for floating precision\n            if ycol == ncoly:\n                # If latitude is directly on upper grid line then retain\n                ycol = ncoly - 1\n            kmarker = (ycol * int(xlim)) + xcol\n            adjust = _get_adjustment(magnitude[iloc],\n                                     year[iloc],\n                                     completeness_table[0, 1],\n                                     completeness_table[:, 0],\n                                     t_f,\n                                     mag_inc)\n            if adjust:\n                grid_count[kmarker] = grid_count[kmarker] + adjust\n        return grid_count"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a 3D grid of earthquakes observed in a three - dimensional grid of observed rates.", "response": "def create_3D_grid(self, catalogue, completeness_table, t_f=1.0,\n                       mag_inc=0.1):\n        '''\n        Counts the earthquakes observed in a three dimensional grid\n\n\n        :param catalogue:\n            Instance of the openquake.hmtk.seismicity.catalogue.Catalogue class\n            catalogue.data dictionary containing the following -\n            'year' - numpy.ndarray vector of years\n            'longitude' - numpy.ndarray vector of longitudes\n            'latitude' - numpy.ndarray vector of latitudes\n            'depth' - numpy.ndarray vector of depths\n\n        :param np.ndarray completeness_table:\n            Completeness of the catalogue assuming evenly spaced magnitudes\n            from most recent bin to oldest bin [year, magnitude]\n\n        :param float t_f:\n            Weichert adjustment factor\n\n        :param float mag_inc:\n            Increment of the completeness magnitude (rendered 0.1)\n\n        :returns:\n           Three-dimensional spatial grid of observed rates (or two dimensional\n           if only one depth layer is considered)\n\n        '''\n        x_bins = np.arange(self.grid_limits['xmin'],\n                           self.grid_limits['xmax'],\n                           self.grid_limits['xspc'])\n        if x_bins[-1] < self.grid_limits['xmax']:\n            x_bins = np.hstack([x_bins, x_bins[-1] + self.grid_limits['xspc']])\n\n        y_bins = np.arange(self.grid_limits['ymin'],\n                           self.grid_limits['ymax'],\n                           self.grid_limits['yspc'])\n        if y_bins[-1] < self.grid_limits['ymax']:\n            y_bins = np.hstack([y_bins, y_bins[-1] + self.grid_limits['yspc']])\n\n        z_bins = np.arange(self.grid_limits['zmin'],\n                           self.grid_limits['zmax'] + self.grid_limits['zspc'],\n                           self.grid_limits['zspc'])\n\n        if z_bins[-1] < self.grid_limits['zmax']:\n            z_bins = np.hstack([z_bins, z_bins[-1] + self.grid_limits['zspc']])\n\n        # Define centre points of grid cells\n        gridx, gridy = np.meshgrid((x_bins[1:] + x_bins[:-1]) / 2.,\n                                   (y_bins[1:] + y_bins[:-1]) / 2.)\n\n        n_x, n_y = np.shape(gridx)\n        gridx = np.reshape(gridx, [n_x * n_y, 1])\n        gridy = np.reshape(np.flipud(gridy), [n_x * n_y, 1])\n\n        # Only one depth range\n        idx = np.logical_and(catalogue.data['depth'] >= z_bins[0],\n                             catalogue.data['depth'] < z_bins[1])\n        mid_depth = (z_bins[0] + z_bins[1]) / 2.\n\n        data_grid = np.column_stack([\n            gridx,\n            gridy,\n            mid_depth * np.ones(n_x * n_y, dtype=float),\n            self.create_2D_grid_simple(catalogue.data['longitude'][idx],\n                                       catalogue.data['latitude'][idx],\n                                       catalogue.data['year'][idx],\n                                       catalogue.data['magnitude'][idx],\n                                       completeness_table,\n                                       t_f,\n                                       mag_inc)])\n\n        if len(z_bins) < 3:\n            # Only one depth range\n            self.data = data_grid\n            return\n\n        # Multiple depth layers - append to grid\n        for iloc in range(1, len(z_bins) - 1):\n            idx = np.logical_and(catalogue.data['depth'] >= z_bins[iloc],\n                                 catalogue.data['depth'] < z_bins[iloc + 1])\n            mid_depth = (z_bins[iloc] + z_bins[iloc + 1]) / 2.\n\n            temp_grid = np.column_stack([\n                gridx,\n                gridy,\n                mid_depth * np.ones(n_x * n_y, dtype=float),\n                self.create_2D_grid_simple(catalogue.data['longitude'][idx],\n                                           catalogue.data['latitude'][idx],\n                                           catalogue.data['year'][idx],\n                                           catalogue.data['magnitude'][idx],\n                                           completeness_table,\n                                           t_f,\n                                           mag_inc)])\n\n            data_grid = np.vstack([data_grid, temp_grid])\n        self.data = data_grid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the data to a CSV file.", "response": "def write_to_csv(self, filename):\n        '''\n        Exports to simple csv\n\n        :param str filename:\n            Path to file for export\n        '''\n        fid = open(filename, 'wt')\n        # Create header list\n        header_info = ['Longitude', 'Latitude', 'Depth', 'Observed Count',\n                       'Smoothed Rate', 'b-value']\n        writer = csv.DictWriter(fid, fieldnames=header_info)\n        headers = dict((name0, name0) for name0 in header_info)\n        # Write to file\n        writer.writerow(headers)\n        for row in self.data:\n            # institute crude compression by omitting points with no seismicity\n            # and taking advantage of the %g format\n            if row[4] == 0:\n                continue\n            row_dict = {'Longitude': '%g' % row[0],\n                        'Latitude': '%g' % row[1],\n                        'Depth': '%g' % row[2],\n                        'Observed Count': '%d' % row[3],\n                        'Smoothed Rate': '%.6g' % row[4],\n                        'b-value': '%g' % self.bval}\n            writer.writerow(row_dict)\n        fid.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_hazard_metadata(md):\n    if (md.get('statistics') is not None and (\n            md.get('smlt_path') is not None or\n            md.get('gsimlt_path') is not None)):\n        raise ValueError('Cannot specify both `statistics` and logic tree '\n                         'paths')\n\n    if md.get('statistics') is not None:\n        # make sure only valid statistics types are specified\n        if md.get('statistics') not in ('mean', 'max', 'quantile', 'std'):\n            raise ValueError('`statistics` must be either `mean`, `max`, or '\n                             '`quantile`')\n    else:\n        # must specify both logic tree paths\n        if md.get('smlt_path') is None or md.get('gsimlt_path') is None:\n            raise ValueError('Both logic tree paths are required for '\n                             'non-statistical results')\n\n    if md.get('statistics') == 'quantile':\n        if md.get('quantile_value') is None:\n            raise ValueError('quantile stastics results require a quantile'\n                             ' value to be specified')\n\n    if not md.get('statistics') == 'quantile':\n        if md.get('quantile_value') is not None:\n            raise ValueError('Quantile value must be specified with '\n                             'quantile statistics')\n\n    if md.get('imt') == 'SA':\n        if md.get('sa_period') is None:\n            raise ValueError('`sa_period` is required for IMT == `SA`')\n        if md.get('sa_damping') is None:\n            raise ValueError('`sa_damping` is required for IMT == `SA`')", "response": "Validate the metadata dictionary of attributes which are more or less the same for the hazard curves hazard maps and disaggregation histograms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the metadata attributes on a given element.", "response": "def _set_metadata(element, metadata, attr_map, transform=str):\n    \"\"\"\n    Set metadata attributes on a given ``element``.\n\n    :param element:\n        :class:`xml.etree.ElementTree.Element` instance\n    :param metadata:\n        Dictionary of metadata items containing attribute data for ``element``.\n    :param attr_map:\n        Dictionary mapping of metadata key->attribute name.\n    :param transform:\n        A function accepting and returning a single value to be applied to each\n        attribute value. Defaults to `str`.\n    \"\"\"\n    for kw, attr in attr_map.items():\n        value = metadata.get(kw)\n        if value is not None:\n            element.set(attr, transform(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_gmfs(gmf_set):\n    for gmf in gmf_set:\n        gmf_node = Node('gmf')\n        gmf_node['IMT'] = gmf.imt\n        if gmf.imt == 'SA':\n            gmf_node['saPeriod'] = str(gmf.sa_period)\n            gmf_node['saDamping'] = str(gmf.sa_damping)\n        gmf_node['ruptureId'] = gmf.event_id\n        sorted_nodes = sorted(gmf)\n        gmf_node.nodes = (\n            Node('node', dict(gmv=n.gmv, lon=n.location.x, lat=n.location.y))\n            for n in sorted_nodes)\n        yield gmf_node", "response": "Generates GMF nodes from a set of GMF objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rupture_to_element(rup, parent=None):\n    if parent is None:\n        parent = et.Element('root')\n    rup_elem = et.SubElement(parent, rup.typology)\n    elem = et.SubElement(rup_elem, 'stochasticEventSets')\n    n = 0\n    for ses in rup.events_by_ses:\n        eids = rup.events_by_ses[ses]['eid']\n        n += len(eids)\n        ses_elem = et.SubElement(elem, 'SES', id=ses)\n        ses_elem.text = ' '.join(str(eid) for eid in eids)\n    rup_elem.set('id', rup.rupid)\n    rup_elem.set('multiplicity', str(n))\n    sub_elems(rup_elem, rup, 'magnitude',  'strike', 'dip', 'rake')\n    h = rup.hypocenter\n    et.SubElement(rup_elem, 'hypocenter', dict(lon=h.x, lat=h.y, depth=h.z))\n    if rup.is_from_fault_source:\n        # rup is from a simple or complex fault source\n        # the rup geometry is represented by a mesh of 3D\n        # points\n        mesh_elem = et.SubElement(rup_elem, 'mesh')\n\n        # we assume the mesh components (lons, lats, depths)\n        # are of uniform shape\n        for i, row in enumerate(rup.lons):\n            for j, col in enumerate(row):\n                node_elem = et.SubElement(mesh_elem, 'node')\n                node_elem.set('row', str(i))\n                node_elem.set('col', str(j))\n                node_elem.set('lon', str(rup.lons[i][j]))\n                node_elem.set('lat', str(rup.lats[i][j]))\n                node_elem.set('depth', str(rup.depths[i][j]))\n\n        # if we never entered the loop above, it's possible\n        # that i and j will be undefined\n        mesh_elem.set('rows', str(i + 1))\n        mesh_elem.set('cols', str(j + 1))\n    elif rup.is_gridded_surface:\n        # the rup geometry is represented by a mesh of (1, N) points\n        mesh_elem = et.SubElement(rup_elem, 'mesh')\n        for j, _ in enumerate(rup.lons):\n            node_elem = et.SubElement(mesh_elem, 'node')\n            node_elem.set('row', '0')\n            node_elem.set('col', str(j))\n            node_elem.set('lon', str(rup.lons[j]))\n            node_elem.set('lat', str(rup.lats[j]))\n            node_elem.set('depth', str(rup.depths[j]))\n    else:\n        # rupture is from a multi surface fault source\n        if rup.is_multi_surface:\n            # the arrays lons, lats and depths contain 4*N elements,\n            # where N is the number of planar surfaces contained in the\n            # multisurface; each planar surface if characterised by 4\n            # vertices top_left, top_right, bottom_left, bottom_right\n            assert len(rup.lons) % 4 == 0\n            assert len(rup.lons) == len(rup.lats) == len(rup.depths)\n\n            for offset in range(len(rup.lons) // 4):\n                # looping on the coordinates of the sub surfaces, one\n                # planar surface at the time\n                start = offset * 4\n                end = offset * 4 + 4\n                lons = rup.lons[start:end]  # 4 lons of the current surface\n                lats = rup.lats[start:end]  # 4 lats of the current surface\n                depths = rup.depths[start:end]  # 4 depths\n\n                ps_elem = et.SubElement(\n                    rup_elem, 'planarSurface')\n\n                top_left, top_right, bottom_left, bottom_right = \\\n                    zip(lons, lats, depths)\n\n                for el_name, corner in (\n                        ('topLeft', top_left),\n                        ('topRight', top_right),\n                        ('bottomLeft', bottom_left),\n                        ('bottomRight', bottom_right)):\n\n                    corner_elem = et.SubElement(ps_elem, el_name)\n                    corner_elem.set('lon', '%.7f' % corner[0])\n                    corner_elem.set('lat', '%.7f' % corner[1])\n                    corner_elem.set('depth', '%.7f' % corner[2])\n        else:\n            # rupture is from a point or area source\n            # the rupture geometry is represented by four 3D\n            # corner points\n            ps_elem = et.SubElement(rup_elem, 'planarSurface')\n\n            # create the corner point elements, in the order of:\n            # * top left\n            # * top right\n            # * bottom left\n            # * bottom right\n            for el_name, corner in (\n                    ('topLeft', rup.top_left_corner),\n                    ('topRight', rup.top_right_corner),\n                    ('bottomLeft', rup.bottom_left_corner),\n                    ('bottomRight', rup.bottom_right_corner)):\n\n                corner_elem = et.SubElement(ps_elem, el_name)\n                corner_elem.set('lon', '%.7f' % corner[0])\n                corner_elem.set('lat', '%.7f' % corner[1])\n                corner_elem.set('depth', '%.7f' % corner[2])\n    return parent", "response": "Convert a rupture object into an Element object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize a sequence of hazard curves to the specified file.", "response": "def serialize(self, data):\n        \"\"\"\n        Write a sequence of hazard curves to the specified file.\n\n        :param data:\n            Iterable of hazard curve data. Each datum must be an object with\n            the following attributes:\n\n            * poes: A list of probability of exceedence values (floats).\n            * location: An object representing the location of the curve; must\n              have `x` and `y` to represent lon and lat, respectively.\n        \"\"\"\n        with open(self.dest, 'wb') as fh:\n            root = et.Element('nrml')\n            self.add_hazard_curves(root, self.metadata, data)\n            nrml.write(list(root), fh)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd hazard curves stored into data as child of the root element with metadata.", "response": "def add_hazard_curves(self, root, metadata, data):\n        \"\"\"\n        Add hazard curves stored into `data` as child of the `root`\n        element with `metadata`. See the documentation of the method\n        `serialize` and the constructor for a description of `data`\n        and `metadata`, respectively.\n        \"\"\"\n        hazard_curves = et.SubElement(root, 'hazardCurves')\n\n        _set_metadata(hazard_curves, metadata, _ATTR_MAP)\n\n        imls_elem = et.SubElement(hazard_curves, 'IMLs')\n        imls_elem.text = ' '.join(map(scientificformat, metadata['imls']))\n        gml_ns = nrml.SERIALIZE_NS_MAP['gml']\n\n        for hc in data:\n            hc_elem = et.SubElement(hazard_curves, 'hazardCurve')\n            gml_point = et.SubElement(hc_elem, '{%s}Point' % gml_ns)\n            gml_pos = et.SubElement(gml_point, '{%s}pos' % gml_ns)\n            gml_pos.text = '%s %s' % (hc.location.x, hc.location.y)\n            poes_elem = et.SubElement(hc_elem, 'poEs')\n            poes_elem.text = ' '.join(map(scientificformat, hc.poes))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing a collection of ground motion fields to XML.", "response": "def serialize(self, data, fmt='%10.7E'):\n        \"\"\"\n        Serialize a collection of ground motion fields to XML.\n\n        :param data:\n            An iterable of \"GMF set\" objects.\n            Each \"GMF set\" object should:\n\n            * have an `investigation_time` attribute\n            * have an `stochastic_event_set_id` attribute\n            * be iterable, yielding a sequence of \"GMF\" objects\n\n            Each \"GMF\" object should:\n\n            * have an `imt` attribute\n            * have an `sa_period` attribute (only if `imt` is 'SA')\n            * have an `sa_damping` attribute (only if `imt` is 'SA')\n            * have a `event_id` attribute (to indicate which rupture\n              contributed to this gmf)\n            * be iterable, yielding a sequence of \"GMF node\" objects\n\n            Each \"GMF node\" object should have:\n\n            * a `gmv` attribute (to indicate the ground motion value\n            * `lon` and `lat` attributes (to indicate the geographical location\n              of the ground motion field)\n        \"\"\"\n        gmf_set_nodes = []\n        for gmf_set in data:\n            gmf_set_node = Node('gmfSet')\n            if gmf_set.investigation_time:\n                gmf_set_node['investigationTime'] = str(\n                    gmf_set.investigation_time)\n            gmf_set_node['stochasticEventSetId'] = str(\n                gmf_set.stochastic_event_set_id)\n            gmf_set_node.nodes = gen_gmfs(gmf_set)\n            gmf_set_nodes.append(gmf_set_node)\n\n        gmf_container = Node('gmfCollection')\n        gmf_container[SM_TREE_PATH] = self.sm_lt_path\n        gmf_container[GSIM_TREE_PATH] = self.gsim_lt_path\n        gmf_container.nodes = gmf_set_nodes\n\n        with open(self.dest, 'wb') as dest:\n            nrml.write([gmf_container], dest, fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserializes a collection of stochastic event sets to XML.", "response": "def serialize(self, data, investigation_time):\n        \"\"\"\n        Serialize a collection of stochastic event sets to XML.\n\n        :param data:\n            A dictionary src_group_id -> list of\n            :class:`openquake.commonlib.calc.Rupture` objects.\n            Each Rupture should have the following attributes:\n\n            * `rupid`\n            * `events_by_ses`\n            * `magnitude`\n            * `strike`\n            * `dip`\n            * `rake`\n            * `tectonic_region_type`\n            * `is_from_fault_source` (a `bool`)\n            * `is_multi_surface` (a `bool`)\n            * `lons`\n            * `lats`\n            * `depths`\n\n            If `is_from_fault_source` is `True`, the rupture originated from a\n            simple or complex fault sources. In this case, `lons`, `lats`, and\n            `depths` should all be 2D arrays (of uniform shape). These\n            coordinate triples represent nodes of the rupture mesh.\n\n            If `is_from_fault_source` is `False`, the rupture originated from a\n            point or area source. In this case, the rupture is represented by a\n            quadrilateral planar surface. This planar surface is defined by 3D\n            vertices. In this case, the rupture should have the following\n            attributes:\n\n            * `top_left_corner`\n            * `top_right_corner`\n            * `bottom_right_corner`\n            * `bottom_left_corner`\n\n            Each of these should be a triple of `lon`, `lat`, `depth`.\n\n            If `is_multi_surface` is `True`, the rupture originated from a\n            multi-surface source. In this case, `lons`, `lats`, and `depths`\n            should have uniform length. The length should be a multiple of 4,\n            where each segment of 4 represents the corner points of a planar\n            surface in the following order:\n\n            * top left\n            * top right\n            * bottom left\n            * bottom right\n\n            Each of these should be a triple of `lon`, `lat`, `depth`.\n\n        :param investigation_time:\n            Investigation time parameter specified in the job.ini\n        \"\"\"\n        with open(self.dest, 'wb') as fh:\n            root = et.Element('nrml')\n            ses_container = et.SubElement(root, 'ruptureCollection')\n            ses_container.set('investigationTime', str(investigation_time))\n            for grp_id in sorted(data):\n                attrs = dict(\n                    id=grp_id,\n                    tectonicRegion=data[grp_id][0].tectonic_region_type)\n                sg = et.SubElement(ses_container, 'ruptureGroup', attrs)\n                for rupture in data[grp_id]:\n                    rupture_to_element(rupture, sg)\n            nrml.write(list(root), fh)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize the data to XML.", "response": "def serialize(self, data):\n        \"\"\"\n        Serialize hazard map data to XML.\n\n        See :meth:`HazardMapWriter.serialize` for details about the expected\n        input.\n        \"\"\"\n        with open(self.dest, 'wb') as fh:\n            root = et.Element('nrml')\n            hazard_map = et.SubElement(root, 'hazardMap')\n            _set_metadata(hazard_map, self.metadata, _ATTR_MAP)\n\n            for lon, lat, iml in data:\n                node = et.SubElement(hazard_map, 'node')\n                node.set('lon', str(lon))\n                node.set('lat', str(lat))\n                node.set('iml', str(iml))\n\n            nrml.write(list(root), fh)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(self, data):\n\n        with open(self.dest, 'wb') as fh, floatformat('%.6E'):\n            root = et.Element('nrml')\n\n            diss_matrices = et.SubElement(root, 'disaggMatrices')\n\n            _set_metadata(diss_matrices, self.metadata, _ATTR_MAP)\n\n            transform = lambda val: ', '.join(map(scientificformat, val))\n            _set_metadata(diss_matrices, self.metadata, self.BIN_EDGE_ATTR_MAP,\n                          transform=transform)\n\n            for result in data:\n                diss_matrix = et.SubElement(diss_matrices, 'disaggMatrix')\n\n                # Check that we have bin edges defined for each dimension label\n                # (mag, dist, lon, lat, eps, TRT)\n                for label in result.dim_labels:\n                    bin_edge_attr = self.DIM_LABEL_TO_BIN_EDGE_MAP.get(label)\n                    assert self.metadata.get(bin_edge_attr) is not None, (\n                        \"Writer is missing '%s' metadata\" % bin_edge_attr\n                    )\n\n                result_type = ','.join(result.dim_labels)\n                diss_matrix.set('type', result_type)\n\n                dims = ','.join(str(x) for x in result.matrix.shape)\n                diss_matrix.set('dims', dims)\n\n                diss_matrix.set('poE', scientificformat(result.poe))\n                diss_matrix.set('iml', scientificformat(result.iml))\n\n                for idxs, value in numpy.ndenumerate(result.matrix):\n                    prob = et.SubElement(diss_matrix, 'prob')\n\n                    index = ','.join([str(x) for x in idxs])\n                    prob.set('index', index)\n                    prob.set('value', scientificformat(value))\n\n            nrml.write(list(root), fh)", "response": "Serialize the data into a new N - dimensional numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize a sequence of uniform hazard spectra to the specified file.", "response": "def serialize(self, data):\n        \"\"\"\n        Write a sequence of uniform hazard spectra to the specified file.\n\n        :param data:\n            Iterable of UHS data. Each datum must be an object with the\n            following attributes:\n\n            * imls: A sequence of Intensity Measure Levels\n            * location: An object representing the location of the curve; must\n              have `x` and `y` to represent lon and lat, respectively.\n        \"\"\"\n        gml_ns = nrml.SERIALIZE_NS_MAP['gml']\n\n        with open(self.dest, 'wb') as fh:\n            root = et.Element('nrml')\n\n            uh_spectra = et.SubElement(root, 'uniformHazardSpectra')\n\n            _set_metadata(uh_spectra, self.metadata, _ATTR_MAP)\n\n            periods_elem = et.SubElement(uh_spectra, 'periods')\n            periods_elem.text = ' '.join([str(x)\n                                          for x in self.metadata['periods']])\n\n            for uhs in data:\n                uhs_elem = et.SubElement(uh_spectra, 'uhs')\n                gml_point = et.SubElement(uhs_elem, '{%s}Point' % gml_ns)\n                gml_pos = et.SubElement(gml_point, '{%s}pos' % gml_ns)\n                gml_pos.text = '%s %s' % (uhs.location.x, uhs.location.y)\n                imls_elem = et.SubElement(uhs_elem, 'IMLs')\n                imls_elem.text = ' '.join(['%10.7E' % x for x in uhs.imls])\n\n            nrml.write(list(root), fh)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_cumulative_moment(year, mag, figure_size=(8, 6),\n                           filename=None, filetype='png', dpi=300, ax=None):\n    '''Calculation of Mmax using aCumulative Moment approach, adapted from\n    the cumulative strain energy method of Makropoulos & Burton (1983)\n    :param year: Year of Earthquake\n    :type year: numpy.ndarray\n    :param mag: Magnitude of Earthquake\n    :type mag: numpy.ndarray\n    :keyword iplot: Include cumulative moment plot\n    :type iplot: Boolean\n    :return mmax: Returns Maximum Magnitude\n    :rtype mmax: Float\n    '''\n    # Calculate seismic moment\n    m_o = 10. ** (9.05 + 1.5 * mag)\n    year_range = np.arange(np.min(year), np.max(year) + 1, 1)\n    nyr = np.int(np.shape(year_range)[0])\n    morate = np.zeros(nyr, dtype=float)\n    # Get moment release per year\n    for loc, tyr in enumerate(year_range):\n        idx = np.abs(year - tyr) < 1E-5\n        if np.sum(idx) > 0:\n            # Some moment release in that year\n            morate[loc] = np.sum(m_o[idx])\n    ave_morate = np.sum(morate) / float(nyr)\n\n    # Average moment rate vector\n    exp_morate = np.cumsum(ave_morate * np.ones(nyr))\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    ax.step(year_range, np.cumsum(morate), 'b-', linewidth=2)\n    ax.plot(year_range, exp_morate, 'r-', linewidth=2)\n    # Get offsets\n    upper_morate = exp_morate + (np.max(np.cumsum(morate) - exp_morate))\n    lower_morate = exp_morate + (np.min(np.cumsum(morate) - exp_morate))\n    ax.plot(year_range, upper_morate, 'r--', linewidth=1)\n    ax.plot(year_range, lower_morate, 'r--', linewidth=1)\n    ax.axis([np.min(year), np.max(year), 0.0, np.sum(morate)])\n    _save_image(fig, filename, filetype, dpi)", "response": "Plots the cumulative Moment for a given year and magnitude."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_config(config, data):\n    '''Check config file inputs\n\n    :param dict config:\n         Configuration settings for the function\n\n    '''\n    essential_keys = ['input_mmin', 'b-value', 'sigma-b']\n    for key in essential_keys:\n        if not key in config.keys():\n            raise ValueError('For KijkoSellevolBayes the key %s needs to '\n                             'be set in the configuation' % key)\n    if 'tolerance' not in config.keys() or not config['tolerance']:\n        config['tolerance'] = 1E-5\n\n    if not config.get('maximum_iterations', False):\n        config['maximum_iterations'] = 1000\n\n    if config['input_mmin'] < np.min(data['magnitude']):\n        config['input_mmin'] = np.min(data['magnitude'])\n\n    if fabs(config['sigma-b'] < 1E-15):\n        raise ValueError('Sigma-b must be greater than zero!')\n\n    return config", "response": "Check config file inputs for KijkoSellevolBayes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_mean(self, C, mag, rjb):\n        # line 1686 in hazgridXnga2.f\n        ffc = self._compute_finite_fault_correction(mag)\n        d = np.sqrt(rjb ** 2 + (C['c7'] ** 2) * (ffc ** 2))\n\n        # lines 1663, 1694-1696 in hazgridXnga2.f\n        mean = (\n            C['c1'] + C['c2'] * (mag - 6.) +\n            C['c3'] * ((mag - 6.) ** 2) -\n            C['c4'] * np.log(d) - C['c6'] * d\n        )\n\n        factor = np.log(rjb / 100.)\n        idx = factor > 0\n        mean[idx] -= (C['c5'] - C['c4']) * factor[idx]\n\n        return mean", "response": "Compute ground motion mean value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_finite_fault_correction(self, mag):\n        mw_j96 = mblg_to_mw_johnston_96(mag)\n        mw_ab87 = mblg_to_mw_atkinson_boore_87(mag)\n\n        t1 = np.exp(-1.25 + 0.227 * mw_j96)\n        t2 = np.exp(-1.25 + 0.227 * mw_ab87)\n\n        return np.sqrt(t1 * t2)", "response": "Compute finite fault correction term as geometric mean of correction\n        terms obtained from Mw values calculated with Johnston 1996 and Atkinson and Boore 1987 conversion equations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_vulnerability_functions_04(fname):\n    categories = dict(assetCategory=set(), lossCategory=set(),\n                      vulnerabilitySetID=set())\n    imts = set()\n    taxonomies = set()\n    vf_dict = {}  # imt, taxonomy -> vulnerability function\n    for vset in nrml.read(fname).vulnerabilityModel:\n        categories['assetCategory'].add(vset['assetCategory'])\n        categories['lossCategory'].add(vset['lossCategory'])\n        categories['vulnerabilitySetID'].add(vset['vulnerabilitySetID'])\n        IML = vset.IML\n        imt_str = IML['IMT']\n        imls = ~IML\n        imts.add(imt_str)\n        for vfun in vset.getnodes('discreteVulnerability'):\n            taxonomy = vfun['vulnerabilityFunctionID']\n            if taxonomy in taxonomies:\n                raise InvalidFile(\n                    'Duplicated vulnerabilityFunctionID: %s: %s, line %d' %\n                    (taxonomy, fname, vfun.lineno))\n            taxonomies.add(taxonomy)\n            with context(fname, vfun):\n                loss_ratios = ~vfun.lossRatio\n                coefficients = ~vfun.coefficientsVariation\n            if len(loss_ratios) != len(imls):\n                raise InvalidFile(\n                    'There are %d loss ratios, but %d imls: %s, line %d' %\n                    (len(loss_ratios), len(imls), fname,\n                     vfun.lossRatio.lineno))\n            if len(coefficients) != len(imls):\n                raise InvalidFile(\n                    'There are %d coefficients, but %d imls: %s, line %d' %\n                    (len(coefficients), len(imls), fname,\n                     vfun.coefficientsVariation.lineno))\n            with context(fname, vfun):\n                vf_dict[imt_str, taxonomy] = scientific.VulnerabilityFunction(\n                    taxonomy, imt_str, imls, loss_ratios, coefficients,\n                    vfun['probabilisticDistribution'])\n    categories['id'] = '_'.join(sorted(categories['vulnerabilitySetID']))\n    del categories['vulnerabilitySetID']\n    return vf_dict, categories", "response": "Parses the vulnerability functions in NRML 0. 4 format and returns a dictionary of the vulnerability functions and the vulnerability set that are associated with the vulnerability set."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupgrades to the latest NRML version", "response": "def upgrade_file(path, multipoint):\n    \"\"\"Upgrade to the latest NRML version\"\"\"\n    node0 = nrml.read(path, chatty=False)[0]\n    shutil.copy(path, path + '.bak')  # make a backup of the original file\n    tag = striptag(node0.tag)\n    gml = True\n    if tag == 'vulnerabilityModel':\n        vf_dict, cat_dict = get_vulnerability_functions_04(path)\n        # below I am converting into a NRML 0.5 vulnerabilityModel\n        node0 = Node(\n            'vulnerabilityModel', cat_dict,\n            nodes=[obj_to_node(val) for val in vf_dict.values()])\n        gml = False\n    elif tag == 'fragilityModel':\n        node0 = read_nrml.convert_fragility_model_04(\n            nrml.read(path)[0], path)\n        gml = False\n    elif tag == 'sourceModel':\n        node0 = nrml.read(path)[0]\n        dic = groupby(node0.nodes, operator.itemgetter('tectonicRegion'))\n        node0.nodes = [Node('sourceGroup',\n                            dict(tectonicRegion=trt, name=\"group %s\" % i),\n                            nodes=srcs)\n                       for i, (trt, srcs) in enumerate(dic.items(), 1)]\n        if multipoint:\n            sourceconverter.update_source_model(node0, path + '.bak')\n    with open(path, 'wb') as f:\n        nrml.write([node0], f, gml=gml)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupgrade all the NRML files contained in the given directory to the latest NRML version.", "response": "def upgrade_nrml(directory, dry_run, multipoint):\n    \"\"\"\n    Upgrade all the NRML files contained in the given directory to the latest\n    NRML version. Works by walking all subdirectories.\n    WARNING: there is no downgrade!\n    \"\"\"\n    for cwd, dirs, files in os.walk(directory):\n        for f in files:\n            path = os.path.join(cwd, f)\n            if f.endswith('.xml'):\n                ip = iterparse(path, events=('start',))\n                next(ip)  # read node zero\n                try:\n                    fulltag = next(ip)[1].tag  # tag of the first node\n                    xmlns, tag = fulltag.split('}')\n                except Exception:  # not a NRML file\n                    xmlns, tag = '', ''\n                if xmlns[1:] == NRML05:  # already upgraded\n                    if 'sourceModel' in tag and multipoint:\n                        print('upgrading to multiPointSources', path)\n                        node0 = nrml.read(path)[0]\n                        sourceconverter.update_source_model(node0, path)\n                        with open(path, 'wb') as f:\n                            nrml.write([node0], f, gml=True)\n                elif 'nrml/0.4' in xmlns and (\n                        'vulnerability' in tag or 'fragility' in tag or\n                        'sourceModel' in tag):\n                    if not dry_run:\n                        print('Upgrading', path)\n                        try:\n                            upgrade_file(path, multipoint)\n                        except Exception as exc:\n                            raise\n                            print(exc)\n                    else:\n                        print('Not upgrading', path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mag_scale_rel_to_hazardlib(mag_scale_rel, use_default=False):\n    if isinstance(mag_scale_rel, BaseMSR):\n        return mag_scale_rel\n    elif isinstance(mag_scale_rel, str):\n        if not mag_scale_rel in SCALE_RELS.keys():\n            raise ValueError('Magnitude scaling relation %s not supported!'\n                             % mag_scale_rel)\n        else:\n            return SCALE_RELS[mag_scale_rel]()\n    else:\n        if use_default:\n            # Returns the Wells and Coppersmith string\n            return WC1994()\n        else:\n            raise ValueError('Magnitude Scaling Relation Not Defined!')", "response": "Returns the magnitude scaling relation in a format readable by openquake. hazardlib\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a Nodal Plane distribution to a PMF class.", "response": "def npd_to_pmf(nodal_plane_dist, use_default=False):\n    \"\"\"\n    Returns the nodal plane distribution as an instance of the PMF class\n    \"\"\"\n    if isinstance(nodal_plane_dist, PMF):\n        # Aready in PMF format - return\n        return nodal_plane_dist\n    else:\n        if use_default:\n            return PMF([(1.0, NodalPlane(0.0, 90.0, 0.0))])\n        else:\n            raise ValueError('Nodal Plane distribution not defined')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting hypocentral depth distribution to a PMF object.", "response": "def hdd_to_pmf(hypo_depth_dist, use_default=False):\n    \"\"\"\n    Returns the hypocentral depth distribtuion as an instance of the :class:\n    openquake.hazardlib.pmf. \n    \"\"\"\n    if isinstance(hypo_depth_dist, PMF):\n        # Is already instance of PMF\n        return hypo_depth_dist\n    else:\n        if use_default:\n            # Default value of 10 km accepted\n            return PMF([(1.0, 10.0)])\n        else:\n            # Out of options - raise error!\n            raise ValueError('Hypocentral depth distribution not defined!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a simple fault trace to well - known text format", "response": "def simple_trace_to_wkt_linestring(trace):\n    '''\n    Coverts a simple fault trace to well-known text format\n\n    :param trace:\n        Fault trace as instance of :class: openquake.hazardlib.geo.line.Line\n\n    :returns:\n        Well-known text (WKT) Linstring representation of the trace\n    '''\n    trace_str = \"\"\n    for point in trace:\n        trace_str += ' %s %s,' % (point.longitude, point.latitude)\n    trace_str = trace_str.lstrip(' ')\n    return 'LINESTRING (' + trace_str.rstrip(',') + ')'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simple_edge_to_wkt_linestring(edge):\n    '''\n    Coverts a simple fault trace to well-known text format\n\n    :param trace:\n        Fault trace as instance of :class: openquake.hazardlib.geo.line.Line\n\n    :returns:\n        Well-known text (WKT) Linstring representation of the trace\n    '''\n    trace_str = \"\"\n    for point in edge:\n        trace_str += ' %s %s %s,' % (point.longitude, point.latitude,\n                                     point.depth)\n    trace_str = trace_str.lstrip(' ')\n    return 'LINESTRING (' + trace_str.rstrip(',') + ')'", "response": "Converts a simple fault trace to well - known text format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef checksum(thing):\n    try:\n        job_id = int(thing)\n        job_file = None\n    except ValueError:\n        job_id = None\n        job_file = thing\n        if not os.path.exists(job_file):\n            sys.exit('%s does not correspond to an existing file' % job_file)\n    if job_id:\n        dstore = util.read(job_id)\n        checksum = dstore['/'].attrs['checksum32']\n    elif job_file.endswith('.xml'):  # assume it is a smlt file\n        inputs = {'source_model_logic_tree': job_file}\n        checksum = readinput.get_checksum32(mock.Mock(inputs=inputs))\n    else:\n        oq = readinput.get_oqparam(job_file)\n        checksum = readinput.get_checksum32(oq)\n    print(checksum)", "response": "Get the checksum of a single item from the calculation ID or from the source model logic tree file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a job using the specified config file.", "response": "def run_job(job_ini, log_level='info', log_file=None, exports='',\n            username=getpass.getuser(), **kw):\n    \"\"\"\n    Run a job using the specified config file and other options.\n\n    :param str job_ini:\n        Path to calculation config (INI-style) files.\n    :param str log_level:\n        'debug', 'info', 'warn', 'error', or 'critical'\n    :param str log_file:\n        Path to log file.\n    :param exports:\n        A comma-separated string of export types requested by the user.\n    :param username:\n        Name of the user running the job\n    :param kw:\n        Extra parameters like hazard_calculation_id and calculation_mode\n    \"\"\"\n    job_id = logs.init('job', getattr(logging, log_level.upper()))\n    with logs.handle(job_id, log_level, log_file):\n        job_ini = os.path.abspath(job_ini)\n        oqparam = eng.job_from_file(job_ini, job_id, username, **kw)\n        kw['username'] = username\n        eng.run_calc(job_id, oqparam, exports, **kw)\n        for line in logs.dbcmd('list_outputs', job_id, False):\n            safeprint(line)\n    return job_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a job in a tiling job.", "response": "def run_tile(job_ini, sites_slice):\n    \"\"\"\n    Used in tiling calculations\n    \"\"\"\n    return run_job(job_ini, sites_slice=(sites_slice.start, sites_slice.stop))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef del_calculation(job_id, confirmed=False):\n    if logs.dbcmd('get_job', job_id) is None:\n        print('There is no job %d' % job_id)\n        return\n\n    if confirmed or confirm(\n            'Are you sure you want to (abort and) delete this calculation and '\n            'all associated outputs?\\nThis action cannot be undone. (y/n): '):\n        try:\n            abort(job_id)\n            resp = logs.dbcmd('del_calc', job_id, getpass.getuser())\n        except RuntimeError as err:\n            safeprint(err)\n        else:\n            if 'success' in resp:\n                print('Removed %d' % job_id)\n            else:\n                print(resp['error'])", "response": "Delete a calculation and all associated outputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the whole job.", "response": "def smart_run(job_ini, oqparam, log_level, log_file, exports, reuse_hazard):\n    \"\"\"\n    Run calculations by storing their hazard checksum and reusing previous\n    calculations if requested.\n    \"\"\"\n    haz_checksum = readinput.get_checksum32(oqparam, hazard=True)\n    # retrieve an old calculation with the right checksum, if any\n    job = logs.dbcmd('get_job_from_checksum', haz_checksum)\n    reuse = reuse_hazard and job and os.path.exists(job.ds_calc_dir + '.hdf5')\n    # recompute the hazard and store the checksum\n    ebr = (oqparam.calculation_mode == 'event_based_risk' and\n           'gmfs' not in oqparam.inputs)\n    if ebr:\n        kw = dict(calculation_mode='event_based')\n        if (oqparam.sites or 'sites' in oqparam.inputs or\n                'site_model' in oqparam.inputs):\n            # remove exposure from the hazard\n            kw['exposure_file'] = ''\n    else:\n        kw = {}\n    if not reuse:\n        hc_id = run_job(job_ini, log_level, log_file, exports, **kw)\n        if job is None:\n            logs.dbcmd('add_checksum', hc_id, haz_checksum)\n        elif not reuse_hazard or not os.path.exists(job.ds_calc_dir + '.hdf5'):\n            logs.dbcmd('update_job_checksum', hc_id, haz_checksum)\n        if ebr:\n            run_job(job_ini, log_level, log_file,\n                    exports, hazard_calculation_id=hc_id)\n    else:\n        hc_id = job.id\n        logging.info('Reusing job #%d', job.id)\n        run_job(job_ini, log_level, log_file,\n                exports, hazard_calculation_id=hc_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a calculation using the traditional command line API", "response": "def engine(log_file, no_distribute, yes, config_file, make_html_report,\n           upgrade_db, db_version, what_if_I_upgrade, run,\n           list_hazard_calculations, list_risk_calculations,\n           delete_calculation, delete_uncompleted_calculations,\n           hazard_calculation_id, list_outputs, show_log,\n           export_output, export_outputs, exports='',\n           log_level='info', reuse_hazard=False):\n    \"\"\"\n    Run a calculation using the traditional command line API\n    \"\"\"\n    if not run:\n        # configure a basic logging\n        logs.init()\n\n    if config_file:\n        config.read(os.path.abspath(os.path.expanduser(config_file)),\n                    soft_mem_limit=int, hard_mem_limit=int, port=int,\n                    multi_user=valid.boolean, multi_node=valid.boolean)\n\n    if no_distribute:\n        os.environ['OQ_DISTRIBUTE'] = 'no'\n\n    # check if the datadir exists\n    datadir = datastore.get_datadir()\n    if not os.path.exists(datadir):\n        os.makedirs(datadir)\n\n    dbserver.ensure_on()\n    # check if we are talking to the right server\n    err = dbserver.check_foreign()\n    if err:\n        sys.exit(err)\n\n    if upgrade_db:\n        msg = logs.dbcmd('what_if_I_upgrade', 'read_scripts')\n        if msg.startswith('Your database is already updated'):\n            pass\n        elif yes or confirm('Proceed? (y/n) '):\n            logs.dbcmd('upgrade_db')\n        sys.exit(0)\n\n    if db_version:\n        safeprint(logs.dbcmd('db_version'))\n        sys.exit(0)\n\n    if what_if_I_upgrade:\n        safeprint(logs.dbcmd('what_if_I_upgrade', 'extract_upgrade_scripts'))\n        sys.exit(0)\n\n    # check if the db is outdated\n    outdated = logs.dbcmd('check_outdated')\n    if outdated:\n        sys.exit(outdated)\n\n    # hazard or hazard+risk\n    if hazard_calculation_id == -1:\n        # get the latest calculation of the current user\n        hc_id = get_job_id(hazard_calculation_id, getpass.getuser())\n    elif hazard_calculation_id:\n        # make it possible to use calculations made by another user\n        hc_id = get_job_id(hazard_calculation_id)\n    else:\n        hc_id = None\n    if run:\n        log_file = os.path.expanduser(log_file) \\\n            if log_file is not None else None\n        job_inis = [os.path.expanduser(f) for f in run]\n        if len(job_inis) == 1 and not hc_id:\n            # init logs before calling get_oqparam\n            logs.init('nojob', getattr(logging, log_level.upper()))\n            # not using logs.handle that logs on the db\n            oq = readinput.get_oqparam(job_inis[0])\n            smart_run(job_inis[0], oq, log_level, log_file,\n                      exports, reuse_hazard)\n            return\n        for i, job_ini in enumerate(job_inis):\n            open(job_ini, 'rb').read()  # IOError if the file does not exist\n            job_id = run_job(job_ini, log_level, log_file,\n                             exports, hazard_calculation_id=hc_id)\n            if not hc_id:  # use the first calculation as base for the others\n                hc_id = job_id\n    # hazard\n    elif list_hazard_calculations:\n        for line in logs.dbcmd(\n                'list_calculations', 'hazard', getpass.getuser()):\n            safeprint(line)\n    elif delete_calculation is not None:\n        del_calculation(delete_calculation, yes)\n    # risk\n    elif list_risk_calculations:\n        for line in logs.dbcmd('list_calculations', 'risk', getpass.getuser()):\n            safeprint(line)\n\n    # export\n    elif make_html_report:\n        safeprint('Written %s' % make_report(make_html_report))\n        sys.exit(0)\n\n    elif list_outputs is not None:\n        hc_id = get_job_id(list_outputs)\n        for line in logs.dbcmd('list_outputs', hc_id):\n            safeprint(line)\n    elif show_log is not None:\n        hc_id = get_job_id(show_log)\n        for line in logs.dbcmd('get_log', hc_id):\n            safeprint(line)\n\n    elif export_output is not None:\n        output_id, target_dir = export_output\n        dskey, calc_id, datadir = logs.dbcmd('get_output', int(output_id))\n        for line in core.export_output(\n                dskey, calc_id, datadir, os.path.expanduser(target_dir),\n                exports or 'csv,xml'):\n            safeprint(line)\n\n    elif export_outputs is not None:\n        job_id, target_dir = export_outputs\n        hc_id = get_job_id(job_id)\n        for line in core.export_outputs(\n                hc_id, os.path.expanduser(target_dir), exports or 'csv,xml'):\n            safeprint(line)\n\n    elif delete_uncompleted_calculations:\n        logs.dbcmd('delete_uncompleted_calculations', getpass.getuser())\n\n    else:\n        engine.parentparser.prog = 'oq engine'\n        engine.parentparser.print_usage()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extract dictionaries of coefficients specific to required\n        # intensity measure type and for PGA\n        C = self.COEFFS[imt]\n        C_PGA = self.COEFFS[PGA()]\n\n        # compute median pga on rock (vs30=1100), needed for site response\n        # term calculation\n        # For spectral accelerations at periods between 0.0 and 0.25 s, Sa (T)\n        # cannot be less than PGA on soil, therefore if the IMT is in this\n        # period range it is necessary to calculate PGA on soil\n        if imt.name == 'SA' and imt.period > 0.0 and imt.period < 0.25:\n            get_pga_site = True\n        else:\n            get_pga_site = False\n        pga1100, pga_site = self._compute_imt1100(C_PGA,\n                                                  sites,\n                                                  rup,\n                                                  dists,\n                                                  get_pga_site)\n\n        # Get the median ground motion\n        mean = (self._compute_magnitude_term(C, rup.mag) +\n                self._compute_distance_term(C, rup, dists) +\n                self._compute_style_of_faulting_term(C, rup) +\n                self._compute_hanging_wall_term(C, rup, dists) +\n                self._compute_shallow_site_response(C, sites, pga1100) +\n                self._compute_basin_response_term(C, sites.z2pt5))\n\n        # If it is necessary to ensure that Sa(T) >= PGA (see previous comment)\n        if get_pga_site:\n            idx = mean < np.log(pga_site)\n            mean[idx] = np.log(pga_site[idx])\n\n        stddevs = self._get_stddevs(C,\n                                    sites,\n                                    pga1100,\n                                    C_PGA['s_lny'],\n                                    stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the IMT 1100 term and the PGA at the site.", "response": "def _compute_imt1100(self, C, sites, rup, dists, get_pga_site=False):\n        \"\"\"\n        Computes the PGA on reference (Vs30 = 1100 m/s) rock.\n        \"\"\"\n        # Calculates simple site response term assuming all sites 1100 m/s\n        fsite = (C['c10'] + (C['k2'] * C['n'])) * log(1100. / C['k1'])\n        # Calculates the PGA on rock\n        pga1100 = np.exp(self._compute_magnitude_term(C, rup.mag) +\n                         self._compute_distance_term(C, rup, dists) +\n                         self._compute_style_of_faulting_term(C, rup) +\n                         self._compute_hanging_wall_term(C, rup, dists) +\n                         self._compute_basin_response_term(C, sites.z2pt5) +\n                         fsite)\n        # If PGA at the site is needed then remove factor for rock and\n        # re-calculate on correct site condition\n        if get_pga_site:\n            pga_site = np.exp(np.log(pga1100) - fsite)\n            fsite = self._compute_shallow_site_response(C, sites, pga1100)\n            pga_site = np.exp(np.log(pga_site) + fsite)\n        else:\n            pga_site = None\n        return pga1100, pga_site"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the magnitude term in equation ( 2 page 148 )", "response": "def _compute_magnitude_term(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling factor (equation (2), page 144)\n        \"\"\"\n        fmag = C['c0'] + C['c1'] * mag\n        if mag <= 5.5:\n            return fmag\n        elif mag > 6.5:\n            return fmag + (C['c2'] * (mag - 5.5)) + (C['c3'] * (mag - 6.5))\n        else:\n            return fmag + (C['c2'] * (mag - 5.5))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing and return the distance scaling factor", "response": "def _compute_distance_term(self, C, rup, dists):\n        \"\"\"\n        Returns the distance scaling factor (equation (3), page 145)\n        \"\"\"\n        return (C['c4'] + C['c5'] * rup.mag) * \\\n            np.log(np.sqrt(dists.rrup ** 2. + C['c6'] ** 2.))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_style_of_faulting_term(self, C, rup):\n        frv, fnm = self._get_fault_type_dummy_variables(rup.rake)\n\n        if frv > 0.:\n            # Top of rupture depth term only applies to reverse faults\n            if rup.ztor < 1.:\n                ffltz = rup.ztor\n            else:\n                ffltz = 1.\n        else:\n            ffltz = 0.\n        return (C['c7'] * frv * ffltz) + (C['c8'] * fnm)", "response": "Compute and return the style of faulting term in equation 1 page 146 - 146"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_hanging_wall_term(self, C, rup, dists):\n        return (C['c9'] *\n                self._get_hanging_wall_distance_term(dists, rup.ztor) *\n                self._get_hanging_wall_magnitude_term(rup.mag) *\n                self._get_hanging_wall_depth_term(rup.ztor) *\n                self._get_hanging_wall_dip_term(rup.dip))", "response": "Compute and return the hanging wall scaling term in equation 6 - 10 page 146."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the hanging wall distance scaling term", "response": "def _get_hanging_wall_distance_term(self, dists, ztor):\n        \"\"\"\n        Returns the hanging wall distance scaling term (equation 7, page 146)\n        \"\"\"\n        fhngr = np.ones_like(dists.rjb, dtype=float)\n        idx = dists.rjb > 0.\n        if ztor < 1.:\n            temp_rjb = np.sqrt(dists.rjb[idx] ** 2. + 1.)\n            r_max = np.max(np.column_stack([dists.rrup[idx], temp_rjb]),\n                           axis=1)\n            fhngr[idx] = (r_max - dists.rjb[idx]) / r_max\n        else:\n            fhngr[idx] = (dists.rrup[idx] - dists.rjb[idx]) / dists.rrup[idx]\n        return fhngr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_shallow_site_response(self, C, sites, pga1100):\n        stiff_factor = C['c10'] + (C['k2'] * C['n'])\n        # Initially default all sites to intermediate rock value\n        fsite = stiff_factor * np.log(sites.vs30 / C['k1'])\n        # Check for soft soil sites\n        idx = sites.vs30 < C['k1']\n        if np.any(idx):\n            pga_scale = np.log(pga1100[idx] +\n                               (C['c'] * ((sites.vs30[idx] / C['k1']) **\n                                C['n']))) - np.log(pga1100[idx] + C['c'])\n            fsite[idx] = C['c10'] * np.log(sites.vs30[idx] / C['k1']) + \\\n                (C['k2'] * pga_scale)\n        # Any very hard rock sites are rendered to the constant amplification\n        # factor\n        idx = sites.vs30 >= 1100.\n        if np.any(idx):\n            fsite[idx] = stiff_factor * log(1100. / C['k1'])\n\n        return fsite", "response": "Compute the shallow site response term for the given site set and site set parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_basin_response_term(self, C, z2pt5):\n        fsed = np.zeros_like(z2pt5, dtype=float)\n        idx = z2pt5 < 1.0\n        if np.any(idx):\n            fsed[idx] = C['c11'] * (z2pt5[idx] - 1.0)\n\n        idx = z2pt5 > 3.0\n        if np.any(idx):\n            fsed[idx] = (C['c12'] * C['k3'] * exp(-0.75)) *\\\n                (1.0 - np.exp(-0.25 * (z2pt5[idx] - 3.0)))\n        return fsed", "response": "Compute the basin response term in equation 12 page 146"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stddevs(self, C, sites, pga1100, sigma_pga, stddev_types):\n        std_intra = self._compute_intra_event_std(C,\n                                                  sites.vs30,\n                                                  pga1100,\n                                                  sigma_pga)\n\n        std_inter = C['t_lny'] * np.ones_like(sites.vs30)\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(self._get_total_sigma(C, std_intra, std_inter))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(std_intra)\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(std_inter)\n        return stddevs", "response": "Returns the standard deviations as described in the Aleateryunser page 147."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the intra - event standard deviation at the site as defined in the equation 15 page 147", "response": "def _compute_intra_event_std(self, C, vs30, pga1100, sigma_pga):\n        \"\"\"\n        Returns the intra-event standard deviation at the site, as defined in\n        equation 15, page 147\n        \"\"\"\n        # Get intra-event standard deviation at the base of the site profile\n        sig_lnyb = np.sqrt(C['s_lny'] ** 2. - C['s_lnAF'] ** 2.)\n        sig_lnab = np.sqrt(sigma_pga ** 2. - C['s_lnAF'] ** 2.)\n        # Get linearised relationship between f_site and ln PGA\n        alpha = self._compute_intra_event_alpha(C, vs30, pga1100)\n\n        return np.sqrt(\n            (sig_lnyb ** 2.) +\n            (C['s_lnAF'] ** 2.) +\n            ((alpha ** 2.) * (sig_lnab ** 2.)) +\n            (2.0 * alpha * C['rho'] * sig_lnyb * sig_lnab))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_intra_event_alpha(self, C, vs30, pga1100):\n        alpha = np.zeros_like(vs30, dtype=float)\n        idx = vs30 < C['k1']\n        if np.any(idx):\n            temp1 = (pga1100[idx] +\n                     C['c'] * (vs30[idx] / C['k1']) ** C['n']) ** -1.\n            temp1 = temp1 - ((pga1100[idx] + C['c']) ** -1.)\n            alpha[idx] = C['k2'] * pga1100[idx] * temp1\n\n        return alpha", "response": "Compute the linearised functional relationship between fsite and anomaly of the logarithmic event."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_total_sigma(self, C, std_intra, std_inter):\n        return np.sqrt(std_intra ** 2. + std_inter ** 2. + C['c_lny'] ** 2.)", "response": "Returns the total sigma term for the arbitrary horizontal component of\n        ground motion defined by equation 18 page 150."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_event_set(ucerf, background_sids, src_filter, ses_idx, seed):\n    serial = seed + ses_idx * TWO16\n    # get rates from file\n    with h5py.File(ucerf.source_file, 'r') as hdf5:\n        occurrences = ucerf.tom.sample_number_of_occurrences(ucerf.rate, seed)\n        indices, = numpy.where(occurrences)\n        logging.debug(\n            'Considering \"%s\", %d ruptures', ucerf.source_id, len(indices))\n\n        # get ruptures from the indices\n        ruptures = []\n        rupture_occ = []\n        for iloc, n_occ in zip(indices, occurrences[indices]):\n            ucerf_rup = ucerf.get_ucerf_rupture(iloc, src_filter)\n            if ucerf_rup:\n                ucerf_rup.serial = serial\n                serial += 1\n                ruptures.append(ucerf_rup)\n                rupture_occ.append(n_occ)\n\n        # sample background sources\n        background_ruptures, background_n_occ = sample_background_model(\n            hdf5, ucerf.idx_set[\"grid_key\"], ucerf.tom, seed,\n            background_sids, ucerf.min_mag, ucerf.npd, ucerf.hdd, ucerf.usd,\n            ucerf.lsd, ucerf.msr, ucerf.aspect, ucerf.tectonic_region_type)\n        for i, brup in enumerate(background_ruptures):\n            brup.serial = serial\n            serial += 1\n            ruptures.append(brup)\n        rupture_occ.extend(background_n_occ)\n\n    assert len(ruptures) < TWO16, len(ruptures)  # < 2^16 ruptures per SES\n    return ruptures, rupture_occ", "response": "Generates the event set corresponding to a particular branch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample_background_model(\n        hdf5, branch_key, tom, seed, filter_idx, min_mag, npd, hdd,\n        upper_seismogenic_depth, lower_seismogenic_depth, msr=WC1994(),\n        aspect=1.5, trt=DEFAULT_TRT):\n    \"\"\"\n    Generates a rupture set from a sample of the background model\n\n    :param branch_key:\n        Key to indicate the branch for selecting the background model\n    :param tom:\n        Temporal occurrence model as instance of :class:\n        openquake.hazardlib.tom.TOM\n    :param seed:\n        Random seed to use in the call to tom.sample_number_of_occurrences\n    :param filter_idx:\n        Sites for consideration (can be None!)\n    :param float min_mag:\n        Minimim magnitude for consideration of background sources\n    :param npd:\n        Nodal plane distribution as instance of :class:\n        openquake.hazardlib.pmf.PMF\n    :param hdd:\n        Hypocentral depth distribution as instance of :class:\n        openquake.hazardlib.pmf.PMF\n    :param float aspect:\n        Aspect ratio\n    :param float upper_seismogenic_depth:\n        Upper seismogenic depth (km)\n    :param float lower_seismogenic_depth:\n        Lower seismogenic depth (km)\n    :param msr:\n        Magnitude scaling relation\n    :param float integration_distance:\n        Maximum distance from rupture to site for consideration\n    \"\"\"\n    bg_magnitudes = hdf5[\"/\".join([\"Grid\", branch_key, \"Magnitude\"])].value\n    # Select magnitudes above the minimum magnitudes\n    mag_idx = bg_magnitudes >= min_mag\n    mags = bg_magnitudes[mag_idx]\n    rates = hdf5[\"/\".join([\"Grid\", branch_key, \"RateArray\"])][filter_idx, :]\n    rates = rates[:, mag_idx]\n    valid_locs = hdf5[\"Grid/Locations\"][filter_idx, :]\n    # Sample remaining rates\n    sampler = tom.sample_number_of_occurrences(rates, seed)\n    background_ruptures = []\n    background_n_occ = []\n    for i, mag in enumerate(mags):\n        rate_idx = numpy.where(sampler[:, i])[0]\n        rate_cnt = sampler[rate_idx, i]\n        occurrence = rates[rate_idx, i]\n        locations = valid_locs[rate_idx, :]\n        ruptures = generate_background_ruptures(\n            tom, locations, occurrence,\n            mag, npd, hdd, upper_seismogenic_depth,\n            lower_seismogenic_depth, msr, aspect, trt)\n        background_ruptures.extend(ruptures)\n        background_n_occ.extend(rate_cnt.tolist())\n    return background_ruptures, background_n_occ", "response": "This function generates a sample of the background model from the given hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_ruptures(sources, src_filter, param, monitor):\n    [src] = sources\n    res = AccumDict()\n    res.calc_times = []\n    sampl_mon = monitor('sampling ruptures', measuremem=True)\n    res.trt = DEFAULT_TRT\n    background_sids = src.get_background_sids(src_filter)\n    sitecol = src_filter.sitecol\n    samples = getattr(src, 'samples', 1)\n    n_occ = AccumDict(accum=0)\n    t0 = time.time()\n    with sampl_mon:\n        for sam_idx in range(samples):\n            for ses_idx, ses_seed in param['ses_seeds']:\n                seed = sam_idx * TWO16 + ses_seed\n                rups, occs = generate_event_set(\n                    src, background_sids, src_filter, ses_idx, seed)\n                for rup, occ in zip(rups, occs):\n                    n_occ[rup] += occ\n    tot_occ = sum(n_occ.values())\n    dic = {'eff_ruptures': {src.src_group_id: src.num_ruptures}}\n    eb_ruptures = [EBRupture(rup, src.id, src.src_group_id, n, samples)\n                   for rup, n in n_occ.items()]\n    dic['rup_array'] = stochastic.get_rup_array(eb_ruptures, src_filter)\n    dt = time.time() - t0\n    dic['calc_times'] = {src.id: numpy.array([tot_occ, len(sitecol), dt], F32)}\n    return dic", "response": "Builds a list of UCERF sources with a single UCERF source and a list of UCERF sources with a single UCERF source."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the median area of the rupture - types in the specified rake.", "response": "def get_median_area(self, mag, rake):\n        \"\"\"\n        The values are a function of both magnitude and rake.\n\n        Setting the rake to ``None`` causes their \"All\" rupture-types\n        to be applied.\n        \"\"\"\n        assert rake is None or -180 <= rake <= 180\n        if rake is None:\n            # their \"All\" case\n            return 10.0 ** (-3.49 + 0.91 * mag)\n        elif (-45 <= rake <= 45) or (rake >= 135) or (rake <= -135):\n            # strike slip\n            return 10.0 ** (-3.42 + 0.90 * mag)\n        elif rake > 0:\n            # thrust/reverse\n            return 10.0 ** (-3.99 + 0.98 * mag)\n        else:\n            # normal\n            return 10.0 ** (-2.87 + 0.82 * mag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the standard deviation area for WC1994.", "response": "def get_std_dev_area(self, mag, rake):\n        \"\"\"\n        Standard deviation for WC1994. Magnitude is ignored.\n        \"\"\"\n        assert rake is None or -180 <= rake <= 180\n        if rake is None:\n            # their \"All\" case\n            return 0.24\n        elif (-45 <= rake <= 45) or (rake >= 135) or (rake <= -135):\n            # strike slip\n            return 0.22\n        elif rake > 0:\n            # thrust/reverse\n            return 0.26\n        else:\n            # normal\n            return 0.22"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_std_dev_mag(self, rake):\n        assert rake is None or -180 <= rake <= 180\n        if rake is None:\n            # their \"All\" case\n            return 0.24\n        elif (-45 <= rake <= 45) or (rake >= 135) or (rake <= -135):\n            # strike slip\n            return 0.23\n        elif rake > 0:\n            # thrust/reverse\n            return 0.25\n        else:\n            # normal\n            return 0.25", "response": "Returns the standard deviation on the magnitude for the WC1994 area relation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the median magnitude given the area and rake.", "response": "def get_median_mag(self, area, rake):\n        \"\"\"\n        Return magnitude (Mw) given the area and rake.\n\n        Setting the rake to ``None`` causes their \"All\" rupture-types\n        to be applied.\n\n        :param area:\n            Area in square km.\n        :param rake:\n            Rake angle (the rupture propagation direction) in degrees,\n            from -180 to 180.\n        \"\"\"\n        assert rake is None or -180 <= rake <= 180\n        if rake is None:\n            # their \"All\" case\n            return 4.07 + 0.98 * log10(area)\n        elif (-45 <= rake <= 45) or (rake > 135) or (rake < -135):\n            # strike slip\n            return 3.98 + 1.02 * log10(area)\n        elif rake > 0:\n            # thrust/reverse\n            return 4.33 + 0.90 * log10(area)\n        else:\n            # normal\n            return 3.93 + 1.02 * log10(area)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_parameters(self):\n        for key in dir(self):\n            if key.startswith('REQUIRES_'):\n                setattr(self, key, getattr(self.gmpe, key))\n            if key.startswith('DEFINED_'):\n                if not key.endswith('FOR_INTENSITY_MEASURE_TYPES'):\n                    setattr(self, key, getattr(self.gmpe, key))", "response": "Sets the parameters of the object based on the GMPE parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stds_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n\n        mean_list = []\n        stddvs_list = []\n\n        # Loop over averaging periods\n        for period in self.avg_periods:\n            imt_local = SA(float(period))\n            # compute mean and standard deviation\n            mean, stddvs = self.gmpe.get_mean_and_stddevs(sites, rup, dists,\n                                                          imt_local,\n                                                          stds_types)\n            mean_list.append(mean)\n            stddvs_list.append(stddvs[0])  # Support only for total!\n\n        mean_avgsa = 0.\n        stddvs_avgsa = 0.\n\n        for i1 in range(self.tnum):\n            mean_avgsa += mean_list[i1]\n            for i2 in range(self.tnum):\n                rho = self.corr_func.get_correlation(self.avg_periods[i1],\n                                                     self.avg_periods[i2])\n                stddvs_avgsa += rho * stddvs_list[i1] * stddvs_list[i2]\n\n        mean_avgsa /= self.tnum\n        stddvs_avgsa = np.sqrt(stddvs_avgsa)/self.tnum\n\n        return mean_avgsa, [stddvs_avgsa]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the correlation coefficient for the specified periods.", "response": "def get_correlation(self, t1, t2):\n        \"\"\"\n        Computes the correlation coefficient for the specified periods.\n\n        :param float t1:\n            First period of interest.\n\n        :param float t2:\n            Second period of interest.\n\n        :return float rho:\n            The predicted correlation coefficient.\n        \"\"\"\n\n        t_min = min(t1, t2)\n        t_max = max(t1, t2)\n\n        c1 = 1.0\n        c1 -= np.cos(np.pi / 2.0 - np.log(t_max / max(t_min, 0.109)) * 0.366)\n\n        if t_max < 0.2:\n            c2 = 0.105 * (1.0 - 1.0 / (1.0 + np.exp(100.0 * t_max - 5.0)))\n            c2 = 1.0 - c2 * (t_max - t_min) / (t_max - 0.0099)\n        else:\n            c2 = 0\n\n        if t_max < 0.109:\n            c3 = c2\n        else:\n            c3 = c1\n\n        c4 = c1\n        c4 += 0.5 * (np.sqrt(c3) - c3) * (1.0 + np.cos(np.pi * t_min / 0.109))\n\n        if t_max <= 0.109:\n            rho = c2\n        elif t_min > 0.109:\n            rho = c1\n        elif t_max < 0.2:\n            rho = min(c2, c4)\n        else:\n            rho = c4\n\n        return rho"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the correlation coefficient for the specified periods.", "response": "def get_correlation(self, t1, t2):\n        \"\"\"\n        Computes the correlation coefficient for the specified periods.\n\n        :param float t1:\n            First period of interest.\n\n        :param float t2:\n            Second period of interest.\n\n        :return float:\n            The predicted correlation coefficient.\n        \"\"\"\n\n        if t1 not in act.periods:\n            raise ValueError('t1 not a valid period')\n\n        if t2 not in act.periods:\n            raise ValueError('t2 not a valid period')\n\n        return act.coeff_table[act.periods.index(t1)][act.periods.index(t2)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a Surface object to a 3D array of shape N M", "response": "def surface_to_array(surface):\n    \"\"\"\n    :param surface: a Surface object\n    :returns: a 3D array of shape (3, N, M)\n    \"\"\"\n    if hasattr(surface, 'surfaces'):  # multiplanar surfaces\n        n = len(surface.surfaces)\n        arr = numpy.zeros((3, n, 4), F32)\n        for i, surf in enumerate(surface.surfaces):\n            arr[:, i] = surf.mesh.array\n        return arr\n    mesh = surface.mesh\n    if len(mesh.lons.shape) == 1:  # 1D mesh\n        shp = (3, 1) + mesh.lons.shape\n    else:  # 2D mesh\n        shp = (3,) + mesh.lons.shape\n    return mesh.array.reshape(shp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_coords(cls, coords, sort=True):\n        coords = list(coords)\n        if sort:\n            coords.sort()\n        if len(coords[0]) == 2:  # 2D coordinates\n            lons, lats = zip(*coords)\n            depths = None\n        else:  # 3D coordinates\n            lons, lats, depths = zip(*coords)\n            depths = numpy.array(depths)\n        return cls(numpy.array(lons), numpy.array(lats), depths)", "response": "Create a mesh object from a list of 3D coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a mesh object from a collection of points.", "response": "def from_points_list(cls, points):\n        \"\"\"\n        Create a mesh object from a collection of points.\n\n        :param point:\n            List of :class:`~openquake.hazardlib.geo.point.Point` objects.\n        :returns:\n            An instance of :class:`Mesh` with one-dimensional arrays\n            of coordinates from ``points``.\n        \"\"\"\n        lons = numpy.zeros(len(points), dtype=float)\n        lats = lons.copy()\n        depths = lons.copy()\n        for i in range(len(points)):\n            lons[i] = points[i].longitude\n            lats[i] = points[i].latitude\n            depths[i] = points[i].depth\n        if not depths.any():\n            # all points have zero depth, no need to waste memory\n            depths = None\n        return cls(lons, lats, depths)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an array of shape N with the cartesian coordinates", "response": "def xyz(self):\n        \"\"\"\n        :returns: an array of shape (N, 3) with the cartesian coordinates\n        \"\"\"\n        return geo_utils.spherical_to_cartesian(\n            self.lons.flat, self.lats.flat, self.depths.flat)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_min_distance(self, mesh):\n        return cdist(self.xyz, mesh.xyz).min(axis=0)", "response": "Compute and return the minimum distance from each point of the target mesh to each found entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_closest_points(self, mesh):\n        min_idx = cdist(self.xyz, mesh.xyz).argmin(axis=0)  # lose shape\n        if hasattr(mesh, 'shape'):\n            min_idx = min_idx.reshape(mesh.shape)\n        lons = self.lons.take(min_idx)\n        lats = self.lats.take(min_idx)\n        deps = self.depths.take(min_idx)\n        return Mesh(lons, lats, deps)", "response": "Find closest point of this mesh for each point in the other mesh."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_distance_matrix(self):\n        assert self.lons.ndim == 1\n        distances = geodetic.geodetic_distance(\n            self.lons.reshape(self.lons.shape + (1, )),\n            self.lats.reshape(self.lats.shape + (1, )),\n            self.lons,\n            self.lats)\n        return numpy.matrix(distances, copy=False)", "response": "Compute and return the distance matrix between each pair of points in the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_proj_convex_hull(self):\n        # create a projection centered in the center of points collection\n        proj = geo_utils.OrthographicProjection(\n            *geo_utils.get_spherical_bounding_box(self.lons, self.lats))\n\n        # project all the points and create a shapely multipoint object.\n        # need to copy an array because otherwise shapely misinterprets it\n        coords = numpy.transpose(proj(self.lons.flat, self.lats.flat)).copy()\n        multipoint = shapely.geometry.MultiPoint(coords)\n        # create a 2d polygon from a convex hull around that multipoint\n        return proj, multipoint.convex_hull", "response": "Create a projection function and a convex hull around all the points in the mesh and define\n        a convex polygon in that projection and a 2d polygon."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_joyner_boore_distance(self, mesh):\n        # we perform a hybrid calculation (geodetic mesh-to-mesh distance\n        # and distance on the projection plane for close points). first,\n        # we find the closest geodetic distance for each point of target\n        # mesh to this one. in general that distance is greater than\n        # the exact distance to enclosing polygon of this mesh and it\n        # depends on mesh spacing. but the difference can be neglected\n        # if calculated geodetic distance is over some threshold.\n        # get the highest slice from the 3D mesh\n        distances = geodetic.min_geodetic_distance(\n            (self.lons, self.lats), (mesh.lons, mesh.lats))\n        # here we find the points for which calculated mesh-to-mesh\n        # distance is below a threshold. this threshold is arbitrary:\n        # lower values increase the maximum possible error, higher\n        # values reduce the efficiency of that filtering. the maximum\n        # error is equal to the maximum difference between a distance\n        # from site to two adjacent points of the mesh and distance\n        # from site to the line connecting them. thus the error is\n        # a function of distance threshold and mesh spacing. the error\n        # is maximum when the site lies on a perpendicular to the line\n        # connecting points of the mesh and that passes the middle\n        # point between them. the error then can be calculated as\n        # ``err = trsh - d = trsh - \\sqrt(trsh^2 - (ms/2)^2)``, where\n        # ``trsh`` and ``d`` are distance to mesh points (the one\n        # we found on the previous step) and distance to the line\n        # connecting them (the actual distance) and ``ms`` is mesh\n        # spacing. the threshold of 40 km gives maximum error of 314\n        # meters for meshes with spacing of 10 km and 5.36 km for\n        # meshes with spacing of 40 km. if mesh spacing is over\n        # ``(trsh / \\sqrt(2)) * 2`` then points lying in the middle\n        # of mesh cells (that is inside the polygon) will be filtered\n        # out by the threshold and have positive distance instead of 0.\n        # so for threshold of 40 km mesh spacing should not be more\n        # than 56 km (typical values are 5 to 10 km).\n        idxs = (distances < 40).nonzero()[0]  # indices on the first dimension\n        if not len(idxs):\n            # no point is close enough, return distances as they are\n            return distances\n\n        # for all the points that are closer than the threshold we need\n        # to recalculate the distance and set it to zero, if point falls\n        # inside the enclosing polygon of the mesh. for doing that we\n        # project both this mesh and the points of the second mesh--selected\n        # by distance threshold--to the same Cartesian space, define\n        # minimum shapely polygon enclosing the mesh and calculate point\n        # to polygon distance, which gives the most accurate value\n        # of distance in km (and that value is zero for points inside\n        # the polygon).\n        proj, polygon = self._get_proj_enclosing_polygon()\n        if not isinstance(polygon, shapely.geometry.Polygon):\n            # either line or point is our enclosing polygon. draw\n            # a square with side of 10 m around in order to have\n            # a proper polygon instead.\n            polygon = polygon.buffer(self.DIST_TOLERANCE, 1)\n        mesh_xx, mesh_yy = proj(mesh.lons[idxs], mesh.lats[idxs])\n        # replace geodetic distance values for points-closer-than-the-threshold\n        # by more accurate point-to-polygon distance values.\n        distances[idxs] = geo_utils.point_to_polygon_distance(\n            polygon, mesh_xx, mesh_yy)\n\n        return distances", "response": "Compute and return Joyner - Boore distance to each point of mesh."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_proj_enclosing_polygon(self):\n        if self.lons.size < 4:\n            # the mesh doesn't contain even a single cell\n            return self._get_proj_convex_hull()\n\n        proj = geo_utils.OrthographicProjection(\n            *geo_utils.get_spherical_bounding_box(self.lons, self.lats))\n        if len(self.lons.shape) == 1:  # 1D mesh\n            lons = self.lons.reshape(len(self.lons), 1)\n            lats = self.lats.reshape(len(self.lats), 1)\n        else:  # 2D mesh\n            lons = self.lons.T\n            lats = self.lats.T\n        mesh2d = numpy.array(proj(lons, lats)).T\n        lines = iter(mesh2d)\n        # we iterate over horizontal stripes, keeping the \"previous\"\n        # line of points. we keep it reversed, such that together\n        # with the current line they define the sequence of points\n        # around the stripe.\n        prev_line = next(lines)[::-1]\n        polygons = []\n        for i, line in enumerate(lines):\n            coords = numpy.concatenate((prev_line, line, prev_line[0:1]))\n            # create the shapely polygon object from the stripe\n            # coordinates and simplify it (remove redundant points,\n            # if there are any lying on the straight line).\n            stripe = shapely.geometry.LineString(coords) \\\n                                     .simplify(self.DIST_TOLERANCE) \\\n                                     .buffer(self.DIST_TOLERANCE, 2)\n            polygons.append(shapely.geometry.Polygon(stripe.exterior))\n            prev_line = line[::-1]\n        try:\n            # create a final polygon as the union of all the stripe ones\n            polygon = shapely.ops.cascaded_union(polygons) \\\n                                 .simplify(self.DIST_TOLERANCE)\n        except ValueError:\n            # NOTE(larsbutler): In some rare cases, we've observed ValueErrors\n            # (\"No Shapely geometry can be created from null value\") with very\n            # specific sets of polygons such that there are two unique\n            # and many duplicates of one.\n            # This bug is very difficult to reproduce consistently (except on\n            # specific platforms) so the work around here is to remove the\n            # duplicate polygons. In fact, we only observed this error on our\n            # CI/build machine. None of our dev environments or production\n            # machines has encountered this error, at least consistently. >:(\n            polygons = [shapely.wkt.loads(x) for x in\n                        list(set(p.wkt for p in polygons))]\n            polygon = shapely.ops.cascaded_union(polygons) \\\n                                 .simplify(self.DIST_TOLERANCE)\n        return proj, polygon", "response": "Returns the shapely polygon object that is the projection of the current region of the current region."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a convex polygon object that contains all the points in the original mesh.", "response": "def get_convex_hull(self):\n        \"\"\"\n        Get a convex polygon object that contains projections of all the points\n        of the mesh.\n\n        :returns:\n            Instance of :class:`openquake.hazardlib.geo.polygon.Polygon` that\n            is a convex hull around all the points in this mesh. If the\n            original mesh had only one point, the resulting polygon has a\n            square shape with a side length of 10 meters. If there were only\n            two points, resulting polygon is a stripe 10 meters wide.\n        \"\"\"\n        proj, polygon2d = self._get_proj_convex_hull()\n        # if mesh had only one point, the convex hull is a point. if there\n        # were two, it is a line string. we need to return a convex polygon\n        # object, so extend that area-less geometries by some arbitrarily\n        # small distance.\n        if isinstance(polygon2d, (shapely.geometry.LineString,\n                                  shapely.geometry.Point)):\n            polygon2d = polygon2d.buffer(self.DIST_TOLERANCE, 1)\n\n        # avoid circular imports\n        from openquake.hazardlib.geo.polygon import Polygon\n        return Polygon._from_2d(polygon2d, proj)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a rectangular mesh object from a list of points.", "response": "def from_points_list(cls, points):\n        \"\"\"\n        Create a rectangular mesh object from a list of lists of points.\n        Lists in a list are supposed to have the same length.\n\n        :param point:\n            List of lists of :class:`~openquake.hazardlib.geo.point.Point`\n            objects.\n        \"\"\"\n        assert points is not None and len(points) > 0 and len(points[0]) > 0, \\\n            'list of at least one non-empty list of points is required'\n        lons = numpy.zeros((len(points), len(points[0])), dtype=float)\n        lats = lons.copy()\n        depths = lons.copy()\n        num_cols = len(points[0])\n        for i, row in enumerate(points):\n            assert len(row) == num_cols, \\\n                   'lists of points are not of uniform length'\n            for j, point in enumerate(row):\n                lons[i, j] = point.longitude\n                lats[i, j] = point.latitude\n                depths[i, j] = point.depth\n        if not depths.any():\n            depths = None\n        return cls(lons, lats, depths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the middle point of the mesh.", "response": "def get_middle_point(self):\n        \"\"\"\n        Return the middle point of the mesh.\n\n        :returns:\n            An instance of :class:`~openquake.hazardlib.geo.point.Point`.\n\n        The middle point is taken from the middle row and a middle column\n        of the mesh if there are odd number of both. Otherwise the geometric\n        mean point of two or four middle points.\n        \"\"\"\n        num_rows, num_cols = self.lons.shape\n        mid_row = num_rows // 2\n        depth = 0\n        if num_rows & 1 == 1:\n            # there are odd number of rows\n            mid_col = num_cols // 2\n            if num_cols & 1 == 1:\n                # odd number of columns, we can easily take\n                # the middle point\n                depth = self.depths[mid_row, mid_col]\n                return Point(self.lons[mid_row, mid_col],\n                             self.lats[mid_row, mid_col], depth)\n            else:\n                # even number of columns, need to take two middle\n                # points on the middle row\n                lon1, lon2 = self.lons[mid_row, mid_col - 1: mid_col + 1]\n                lat1, lat2 = self.lats[mid_row, mid_col - 1: mid_col + 1]\n                depth1 = self.depths[mid_row, mid_col - 1]\n                depth2 = self.depths[mid_row, mid_col]\n        else:\n            # there are even number of rows. take the row just above\n            # and the one just below the middle and find middle point\n            # of each\n            submesh1 = self[mid_row - 1: mid_row]\n            submesh2 = self[mid_row: mid_row + 1]\n            p1, p2 = submesh1.get_middle_point(), submesh2.get_middle_point()\n            lon1, lat1, depth1 = p1.longitude, p1.latitude, p1.depth\n            lon2, lat2, depth2 = p2.longitude, p2.latitude, p2.depth\n\n        # we need to find the middle between two points\n        depth = (depth1 + depth2) / 2.0\n        lon, lat = geo_utils.get_middle_point(lon1, lat1, lon2, lat2)\n        return Point(lon, lat, depth)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the weighted average inclination and azimuth of the mesh surface.", "response": "def get_mean_inclination_and_azimuth(self):\n        \"\"\"\n        Calculate weighted average inclination and azimuth of the mesh surface.\n\n        :returns:\n            Tuple of two float numbers: inclination angle in a range [0, 90]\n            and azimuth in range [0, 360) (in decimal degrees).\n\n        The mesh is triangulated, the inclination and azimuth for each triangle\n        is computed and average values weighted on each triangle's area\n        are calculated. Azimuth is always defined in a way that inclination\n        angle doesn't exceed 90 degree.\n        \"\"\"\n        assert 1 not in self.lons.shape, (\n            \"inclination and azimuth are only defined for mesh of more than \"\n            \"one row and more than one column of points\")\n        assert ((self.depths[1:] - self.depths[:-1]) >= 0).all(), (\n            \"get_mean_inclination_and_azimuth() requires next mesh row \"\n            \"to be not shallower than the previous one\")\n\n        points, along_azimuth, updip, diag = self.triangulate()\n\n        # define planes that are perpendicular to each point's vector\n        # as normals to those planes\n        earth_surface_tangent_normal = geo_utils.normalized(points)\n\n        # calculating triangles' area and normals for top-left triangles\n        e1 = along_azimuth[:-1]\n        e2 = updip[:, :-1]\n        tl_area = geo_utils.triangle_area(e1, e2, diag)\n        tl_normal = geo_utils.normalized(numpy.cross(e1, e2))\n        # ... and bottom-right triangles\n        e1 = along_azimuth[1:]\n        e2 = updip[:, 1:]\n        br_area = geo_utils.triangle_area(e1, e2, diag)\n        br_normal = geo_utils.normalized(numpy.cross(e1, e2))\n\n        if (self.depths == 0).all():\n            # mesh is on earth surface, inclination is zero\n            inclination = 0\n        else:\n            # inclination calculation\n            # top-left triangles\n            en = earth_surface_tangent_normal[:-1, :-1]\n            # cosine of inclination of the triangle is scalar product\n            # of vector normal to triangle plane and (normalized) vector\n            # pointing to top left corner of a triangle from earth center\n            incl_cos = numpy.sum(en * tl_normal, axis=-1).clip(-1.0, 1.0)\n            # we calculate average angle using mean of circular quantities\n            # formula: define 2d vector for each triangle where length\n            # of the vector corresponds to triangle's weight (we use triangle\n            # area) and angle is equal to inclination angle. then we calculate\n            # the angle of vector sum of all those vectors and that angle\n            # is the weighted average.\n            xx = numpy.sum(tl_area * incl_cos)\n            # express sine via cosine using Pythagorean trigonometric identity,\n            # this is a bit faster than sin(arccos(incl_cos))\n            yy = numpy.sum(tl_area * sqrt(1 - incl_cos * incl_cos))\n\n            # bottom-right triangles\n            en = earth_surface_tangent_normal[1:, 1:]\n            # we need to clip scalar product values because in some cases\n            # they might exceed range where arccos is defined ([-1, 1])\n            # because of floating point imprecision\n            incl_cos = numpy.sum(en * br_normal, axis=-1).clip(-1.0, 1.0)\n            # weighted angle vectors are calculated independently for top-left\n            # and bottom-right triangles of each cell in a mesh. here we\n            # combine both and finally get the weighted mean angle\n            xx += numpy.sum(br_area * incl_cos)\n            yy += numpy.sum(br_area * sqrt(1 - incl_cos * incl_cos))\n            inclination = numpy.degrees(numpy.arctan2(yy, xx))\n\n        # azimuth calculation is done similar to one for inclination. we also\n        # do separate calculations for top-left and bottom-right triangles\n        # and also combine results using mean of circular quantities approach\n\n        # unit vector along z axis\n        z_unit = numpy.array([0.0, 0.0, 1.0])\n\n        # unit vectors pointing west from each point of the mesh, they define\n        # planes that contain meridian of respective point\n        norms_west = geo_utils.normalized(numpy.cross(points + z_unit, points))\n        # unit vectors parallel to planes defined by previous ones. they are\n        # directed from each point to a point lying on z axis on the same\n        # distance from earth center\n        norms_north = geo_utils.normalized(numpy.cross(points, norms_west))\n        # need to normalize triangles' azimuthal edges because we will project\n        # them on other normals and thus calculate an angle in between\n        along_azimuth = geo_utils.normalized(along_azimuth)\n\n        # process top-left triangles\n        # here we identify the sign of direction of the triangles' azimuthal\n        # edges: is edge pointing west or east? for finding that we project\n        # those edges to vectors directing to west by calculating scalar\n        # product and get the sign of resulting value: if it is negative\n        # than the resulting azimuth should be negative as top edge is pointing\n        # west.\n        sign = numpy.sign(numpy.sign(\n            numpy.sum(along_azimuth[:-1] * norms_west[:-1, :-1], axis=-1))\n            # we run numpy.sign(numpy.sign(...) + 0.1) to make resulting values\n            # be only either -1 or 1 with zero values (when edge is pointing\n            # strictly north or south) expressed as 1 (which means \"don't\n            # change the sign\")\n            + 0.1)\n\n        # the length of projection of azimuthal edge on norms_north is cosine\n        # of edge's azimuth\n        az_cos = numpy.sum(along_azimuth[:-1] * norms_north[:-1, :-1], axis=-1)\n        # use the same approach for finding the weighted mean\n        # as for inclination (see above)\n        xx = numpy.sum(tl_area * az_cos)\n        # the only difference is that azimuth is defined in a range\n        # [0, 360), so we need to have two reference planes and change\n        # sign of projection on one normal to sign of projection to another one\n        yy = numpy.sum(tl_area * sqrt(1 - az_cos * az_cos) * sign)\n        # bottom-right triangles\n        sign = numpy.sign(numpy.sign(\n            numpy.sum(along_azimuth[1:] * norms_west[1:, 1:], axis=-1))\n            + 0.1)\n        az_cos = numpy.sum(along_azimuth[1:] * norms_north[1:, 1:], axis=-1)\n        xx += numpy.sum(br_area * az_cos)\n        yy += numpy.sum(br_area * sqrt(1 - az_cos * az_cos) * sign)\n\n        azimuth = numpy.degrees(numpy.arctan2(yy, xx))\n        if azimuth < 0:\n            azimuth += 360\n\n        if inclination > 90:\n            # average inclination is over 90 degree, that means that we need\n            # to reverse azimuthal direction in order for inclination to be\n            # in range [0, 90]\n            inclination = 180 - inclination\n            azimuth = (azimuth + 180) % 360\n\n        return inclination, azimuth"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the centroid width length and area of each mesh cell.", "response": "def get_cell_dimensions(self):\n        \"\"\"\n        Calculate centroid, width, length and area of each mesh cell.\n\n        :returns:\n            Tuple of four elements, each being 2d numpy array.\n            Each array has both dimensions less by one the dimensions\n            of the mesh, since they represent cells, not vertices.\n            Arrays contain the following cell information:\n\n            #. centroids, 3d vectors in a Cartesian space,\n            #. length (size along row of points) in km,\n            #. width (size along column of points) in km,\n            #. area in square km.\n        \"\"\"\n        points, along_azimuth, updip, diag = self.triangulate()\n        top = along_azimuth[:-1]\n        left = updip[:, :-1]\n        tl_area = geo_utils.triangle_area(top, left, diag)\n        top_length = numpy.sqrt(numpy.sum(top * top, axis=-1))\n        left_length = numpy.sqrt(numpy.sum(left * left, axis=-1))\n\n        bottom = along_azimuth[1:]\n        right = updip[:, 1:]\n        br_area = geo_utils.triangle_area(bottom, right, diag)\n        bottom_length = numpy.sqrt(numpy.sum(bottom * bottom, axis=-1))\n        right_length = numpy.sqrt(numpy.sum(right * right, axis=-1))\n\n        cell_area = tl_area + br_area\n\n        tl_center = (points[:-1, :-1] + points[:-1, 1:] + points[1:, :-1]) / 3\n        br_center = (points[:-1, 1:] + points[1:, :-1] + points[1:, 1:]) / 3\n\n        cell_center = ((tl_center * tl_area.reshape(tl_area.shape + (1, ))\n                        + br_center * br_area.reshape(br_area.shape + (1, )))\n                       / cell_area.reshape(cell_area.shape + (1, )))\n\n        cell_length = ((top_length * tl_area + bottom_length * br_area)\n                       / cell_area)\n        cell_width = ((left_length * tl_area + right_length * br_area)\n                      / cell_area)\n\n        return cell_center, cell_length, cell_width, cell_area"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef triangulate(self):\n        points = geo_utils.spherical_to_cartesian(self.lons, self.lats,\n                                                  self.depths)\n        # triangulate the mesh by defining vectors of triangles edges:\n        # \u2192\n        along_azimuth = points[:, 1:] - points[:, :-1]\n        # \u2191\n        updip = points[:-1] - points[1:]\n        # \u2197\n        diag = points[:-1, 1:] - points[1:, :-1]\n\n        return points, along_azimuth, updip, diag", "response": "Convert the mesh points to vectors in Cartesian space."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_width(self):\n        assert 1 not in self.lons.shape, (\n            \"mean width is only defined for mesh of more than \"\n            \"one row and more than one column of points\")\n\n        _, cell_length, cell_width, cell_area = self.get_cell_dimensions()\n\n        # compute widths along each mesh column\n        widths = numpy.sum(cell_width, axis=0)\n\n        # compute (weighted) mean cell length along each mesh column\n        column_areas = numpy.sum(cell_area, axis=0)\n        mean_cell_lengths = numpy.sum(cell_length * cell_area, axis=0) / \\\n            column_areas\n\n        # compute and return weighted mean\n        return numpy.sum(widths * mean_cell_lengths) / \\\n            numpy.sum(mean_cell_lengths)", "response": "Calculate and return the mean width of a mesh surface."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef smooth_data(self, data, config, is_3d=False):\n        '''\n        Applies the smoothing kernel to the data\n\n        :param np.ndarray data:\n            Raw earthquake count in the form [Longitude, Latitude, Depth,\n                Count]\n        :param dict config:\n            Configuration parameters must contain:\n            * BandWidth: The bandwidth of the kernel (in km) (float)\n            * Length_Limit: Maximum number of standard deviations\n\n        :returns:\n            * smoothed_value: np.ndarray vector of smoothed values\n            * Total (summed) rate of the original values\n            * Total (summed) rate of the smoothed values\n        '''\n        max_dist = config['Length_Limit'] * config['BandWidth']\n        smoothed_value = np.zeros(len(data), dtype=float)\n        for iloc in range(0, len(data)):\n            dist_val = haversine(data[:, 0], data[:, 1],\n                                 data[iloc, 0], data[iloc, 1])\n\n            if is_3d:\n                dist_val = np.sqrt(dist_val.flatten() ** 2.0 +\n                                   (data[:, 2] - data[iloc, 2]) ** 2.0)\n            id0 = np.where(dist_val <= max_dist)[0]\n            w_val = (np.exp(-(dist_val[id0] ** 2.0) /\n                            (config['BandWidth'] ** 2.))).flatten()\n            smoothed_value[iloc] = np.sum(w_val * data[id0, 3]) / np.sum(w_val)\n        return smoothed_value, np.sum(data[:, -1]), np.sum(smoothed_value)", "response": "Applies the smoothing kernel to the data array."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the magnitude scaling term provided in Equation 5", "response": "def _get_magnitude_term(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling term provided in Equation (5)\n        \"\"\"\n        dmag = mag - 8.0\n        return C[\"c0\"] + C[\"c3\"] * dmag + C[\"c4\"] * (dmag ** 2.)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the distance scaling term given in Equation ( 4 ) page 1569", "response": "def _get_distance_term(self, C, rrup, mag):\n        \"\"\"\n        Returns the distance scaling given in Equation (4), page 1569,\n        with distance adjusted by the magnitude-dependent depth scaling\n        factor given in Equation (6)\n        \"\"\"\n        r_adj = np.sqrt(rrup ** 2.0 + (mag ** 2.0 - 3.1 * mag - 14.55) ** 2.)\n        return C[\"c1\"] * np.log10(r_adj) + C[\"c2\"] * r_adj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving one calculation ID from the database and remove its datastore", "response": "def purge_one(calc_id, user):\n    \"\"\"\n    Remove one calculation ID from the database and remove its datastore\n    \"\"\"\n    filename = os.path.join(datadir, 'calc_%s.hdf5' % calc_id)\n    err = dbcmd('del_calc', calc_id, user)\n    if err:\n        print(err)\n    elif os.path.exists(filename):  # not removed yet\n        os.remove(filename)\n        print('Removed %s' % filename)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef purge_all(user=None, fast=False):\n    user = user or getpass.getuser()\n    if os.path.exists(datadir):\n        if fast:\n            shutil.rmtree(datadir)\n            print('Removed %s' % datadir)\n        else:\n            for fname in os.listdir(datadir):\n                mo = re.match('calc_(\\d+)\\.hdf5', fname)\n                if mo is not None:\n                    calc_id = int(mo.group(1))\n                    purge_one(calc_id, user)", "response": "Remove all calculations of the given user from the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef purge(calc_id):\n    if calc_id < 0:\n        try:\n            calc_id = datastore.get_calc_ids(datadir)[calc_id]\n        except IndexError:\n            print('Calculation %d not found' % calc_id)\n            return\n    purge_one(calc_id, getpass.getuser())", "response": "Remove the given calculation from the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a matplotlib. patches. PathPatch object from a Shapely or GeoJSON - like polygon.", "response": "def PolygonPatch(polygon, **kwargs):\n    \"\"\"Constructs a matplotlib patch from a geometric object\n\n    The `polygon` may be a Shapely or GeoJSON-like object possibly with holes.\n    The `kwargs` are those supported by the matplotlib.patches.Polygon class\n    constructor. Returns an instance of matplotlib.patches.PathPatch.\n\n    Example (using Shapely Point and a matplotlib axes):\n\n      >> b = Point(0, 0).buffer(1.0)\n      >> patch = PolygonPatch(b, fc='blue', ec='blue', alpha=0.5)\n      >> axis.add_patch(patch)\n    \"\"\"\n    def coding(ob):\n        # The codes will be all \"LINETO\" commands, except for \"MOVETO\"s at the\n        # beginning of each subpath\n        n = len(getattr(ob, 'coords', None) or ob)\n        vals = ones(n, dtype=Path.code_type) * Path.LINETO\n        vals[0] = Path.MOVETO\n        return vals\n\n    if hasattr(polygon, 'geom_type'):  # Shapely\n        ptype = polygon.geom_type\n        if ptype == 'Polygon':\n            polygon = [Polygon(polygon)]\n        elif ptype == 'MultiPolygon':\n            polygon = [Polygon(p) for p in polygon]\n        else:\n            raise ValueError(\n                \"A polygon or multi-polygon representation is required\")\n\n    else:  # GeoJSON\n        polygon = getattr(polygon, '__geo_interface__', polygon)\n        ptype = polygon[\"type\"]\n        if ptype == 'Polygon':\n            polygon = [Polygon(polygon)]\n        elif ptype == 'MultiPolygon':\n            polygon = [Polygon(p) for p in polygon['coordinates']]\n        else:\n            raise ValueError(\n                \"A polygon or multi-polygon representation is required\")\n\n    vertices = concatenate([\n        concatenate([asarray(t.exterior)[:, :2]] +\n                    [asarray(r)[:, :2] for r in t.interiors])\n        for t in polygon])\n    codes = concatenate([\n        concatenate([coding(t.exterior)] +\n                    [coding(r) for r in t.interiors]) for t in polygon])\n\n    return PathPatch(Path(vertices, codes),  **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n\n        mean = (self.get_magnitude_scaling(C, rup.mag) +\n                self.get_distance_term(C, rup, dists.rjb, imt) +\n                self.get_site_amplification(C, sites))\n        # GMPE originally in cm/s/s - convert to g\n        if imt.name in \"PGA SA\":\n            mean -= np.log(100.0 * g)\n        stddevs = self.get_stddevs(C, dists.rjb.shape, stddev_types, sites)\n        if self.sigma_mu_epsilon:\n            # Apply the epistemic uncertainty factor (sigma_mu) multiplied by\n            # the number of standard deviations\n            sigma_mu = self.get_sigma_mu_adjustment(C, imt, rup, dists)\n            # Cap sigma_mu at 0.5 ln units\n            sigma_mu[sigma_mu > 0.5] = 0.5\n            # Sigma mu should not be less than the standard deviation of the\n            # fault-to-fault variability\n            sigma_mu[sigma_mu < C[\"tau_fault\"]] = C[\"tau_fault\"]\n            mean += (self.sigma_mu_epsilon * sigma_mu)\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the magnitude scaling term in equation 1 page 970.", "response": "def get_magnitude_scaling(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling term\n        \"\"\"\n        d_m = mag - self.CONSTANTS[\"Mh\"]\n        if mag < self.CONSTANTS[\"Mh\"]:\n            return C[\"e1\"] + C[\"b1\"] * d_m + C[\"b2\"] * (d_m ** 2.0)\n        else:\n            return C[\"e1\"] + C[\"b3\"] * d_m"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_distance_term(self, C, rup, rjb, imt):\n        h = self._get_h(C, rup.hypo_depth)\n        rval = np.sqrt(rjb ** 2. + h ** 2.)\n        c3 = self.get_distance_coefficients(C, imt)\n\n        f_r = (C[\"c1\"] + C[\"c2\"] * (rup.mag - self.CONSTANTS[\"Mref\"])) *\\\n            np.log(rval / self.CONSTANTS[\"Rref\"]) +\\\n            c3 * (rval - self.CONSTANTS[\"Rref\"])\n        return f_r", "response": "Returns the distance attenuation factor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_distance_coefficients(self, C, imt):\n        c3 = self.c3[imt][\"c3\"] if self.c3 else C[\"c3\"]\n        return c3", "response": "Returns the distance coefficients for the given term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sigma_mu_adjustment(self, C, imt, rup, dists):\n        if imt.name in \"PGA PGV\":\n            # PGA and PGV are 2D arrays of dimension [nmags, ndists]\n            sigma_mu = getattr(self, imt.name.lower())\n            if rup.mag <= self.mags[0]:\n                sigma_mu_m = sigma_mu[0, :]\n            elif rup.mag >= self.mags[-1]:\n                sigma_mu_m = sigma_mu[-1, :]\n            else:\n                intpl1 = interp1d(self.mags, sigma_mu, axis=0)\n                sigma_mu_m = intpl1(rup.mag)\n            # Linear interpolation with distance\n            intpl2 = interp1d(self.dists, sigma_mu_m, bounds_error=False,\n                              fill_value=(sigma_mu_m[0], sigma_mu_m[-1]))\n            return intpl2(dists.rjb)\n        # In the case of SA the array is of dimension [nmags, ndists, nperiods]\n        # Get values for given magnitude\n        if rup.mag <= self.mags[0]:\n            sigma_mu_m = self.s_a[0, :, :]\n        elif rup.mag >= self.mags[-1]:\n            sigma_mu_m = self.s_a[-1, :, :]\n        else:\n            intpl1 = interp1d(self.mags, self.s_a, axis=0)\n            sigma_mu_m = intpl1(rup.mag)\n        # Get values for period - N.B. ln T, linear sigma mu interpolation\n        if imt.period <= self.periods[0]:\n            sigma_mu_t = sigma_mu_m[:, 0]\n        elif imt.period >= self.periods[-1]:\n            sigma_mu_t = sigma_mu_m[:, -1]\n        else:\n            intpl2 = interp1d(np.log(self.periods), sigma_mu_m, axis=1)\n            sigma_mu_t = intpl2(np.log(imt.period))\n        intpl3 = interp1d(self.dists, sigma_mu_t, bounds_error=False,\n                          fill_value=(sigma_mu_t[0], sigma_mu_t[-1]))\n        return intpl3(dists.rjb)", "response": "Returns the sigma mu adjustment factor for the entry point in the site site"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_site_amplification(self, C, sites):\n        ampl = np.zeros(sites.vs30.shape)\n        # For observed vs30 sites\n        ampl[sites.vs30measured] = (C[\"d0_obs\"] + C[\"d1_obs\"] *\n                                    np.log(sites.vs30[sites.vs30measured]))\n        # For inferred Vs30 sites\n        idx = np.logical_not(sites.vs30measured)\n        ampl[idx] = (C[\"d0_inf\"] + C[\"d1_inf\"] * np.log(sites.vs30[idx]))\n        return ampl", "response": "Returns the linear site amplification term depending on whether the Vs30 is observed of inferred\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the standard deviations for inferred vs. observed vs30 sites.", "response": "def get_stddevs(self, C, stddev_shape, stddev_types, sites):\n        \"\"\"\n        Returns the standard deviations, with different site standard\n        deviation for inferred vs. observed vs30 sites.\n        \"\"\"\n        stddevs = []\n        tau = C[\"tau_event\"]\n        sigma_s = np.zeros(sites.vs30measured.shape, dtype=float)\n        sigma_s[sites.vs30measured] += C[\"sigma_s_obs\"]\n        sigma_s[np.logical_not(sites.vs30measured)] += C[\"sigma_s_inf\"]\n        phi = np.sqrt(C[\"phi0\"] ** 2.0 + sigma_s ** 2.)\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt(tau ** 2. + phi ** 2.) +\n                               np.zeros(stddev_shape))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(phi + np.zeros(stddev_shape))\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(tau + np.zeros(stddev_shape))\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef geodetic_distance(lons1, lats1, lons2, lats2, diameter=2*EARTH_RADIUS):\n    lons1, lats1, lons2, lats2 = _prepare_coords(lons1, lats1, lons2, lats2)\n    distance = numpy.arcsin(numpy.sqrt(\n        numpy.sin((lats1 - lats2) / 2.0) ** 2.0\n        + numpy.cos(lats1) * numpy.cos(lats2)\n        * numpy.sin((lons1 - lons2) / 2.0) ** 2.0\n    ))\n    return diameter * distance", "response": "Calculate the geodetic distance between two points or two collections\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the azimuth between two points or two collections of points.", "response": "def azimuth(lons1, lats1, lons2, lats2):\n    \"\"\"\n    Calculate the azimuth between two points or two collections of points.\n\n    Parameters are the same as for :func:`geodetic_distance`.\n\n    Implements an \"alternative formula\" from\n    http://williams.best.vwh.net/avform.htm#Crs\n\n    :returns:\n        Azimuth as an angle between direction to north from first point and\n        direction to the second point measured clockwise in decimal degrees.\n    \"\"\"\n    lons1, lats1, lons2, lats2 = _prepare_coords(lons1, lats1, lons2, lats2)\n    cos_lat2 = numpy.cos(lats2)\n    true_course = numpy.degrees(numpy.arctan2(\n        numpy.sin(lons1 - lons2) * cos_lat2,\n        numpy.cos(lats1) * numpy.sin(lats2)\n        - numpy.sin(lats1) * cos_lat2 * numpy.cos(lons1 - lons2)\n    ))\n    return (360 - true_course) % 360"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef distance(lons1, lats1, depths1, lons2, lats2, depths2):\n    hdist = geodetic_distance(lons1, lats1, lons2, lats2)\n    vdist = depths1 - depths2\n    return numpy.sqrt(hdist ** 2 + vdist ** 2)", "response": "Calculates a distance between two points or collections of points considering points s depth."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spherical_to_cartesian(lons, lats, depths=None):\n    phi = numpy.radians(lons)\n    theta = numpy.radians(lats)\n    if depths is None:\n        rr = EARTH_RADIUS\n    else:\n        rr = EARTH_RADIUS - numpy.array(depths)\n    cos_theta_r = rr * numpy.cos(theta)\n    try:\n        shape = lons.shape\n    except AttributeError:  # a list/tuple was passed\n        try:\n            shape = (len(lons),)\n        except TypeError:  # a scalar was passed\n            shape = ()\n    arr = numpy.zeros(shape + (3,))\n    arr[..., 0] = cos_theta_r * numpy.cos(phi)\n    arr[..., 1] = cos_theta_r * numpy.sin(phi)\n    arr[..., 2] = rr * numpy.sin(theta)\n    return arr", "response": "Converts a list of spherical coordinates to a 3D array of shape N."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the minimum geodetic distance between two points.", "response": "def min_geodetic_distance(a, b):\n    \"\"\"\n    Compute the minimum distance between first mesh and each point\n    of the second mesh when both are defined on the earth surface.\n\n    :param a: a pair of (lons, lats) or an array of cartesian coordinates\n    :param b: a pair of (lons, lats) or an array of cartesian coordinates\n    \"\"\"\n    if isinstance(a, tuple):\n        a = spherical_to_cartesian(a[0].flatten(), a[1].flatten())\n    if isinstance(b, tuple):\n        b = spherical_to_cartesian(b[0].flatten(), b[1].flatten())\n    return cdist(a, b).min(axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distance_matrix(lons, lats, diameter=2*EARTH_RADIUS):\n    m = len(lons)\n    assert m == len(lats), (m, len(lats))\n    lons = numpy.radians(lons)\n    lats = numpy.radians(lats)\n    cos_lats = numpy.cos(lats)\n    result = numpy.zeros((m, m))\n    for i in range(len(lons)):\n        a = numpy.sin((lats[i] - lats) / 2.0)\n        b = numpy.sin((lons[i] - lons) / 2.0)\n        result[i, :] = numpy.arcsin(\n            numpy.sqrt(a * a + cos_lats[i] * cos_lats * b * b)) * diameter\n    return numpy.matrix(result, copy=False)", "response": "Compute the distance matrix for a single resource in a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of points that lie on the same great circle arc and are equally spaced by length km.", "response": "def intervals_between(lon1, lat1, depth1, lon2, lat2, depth2, length):\n    \"\"\"\n    Find a list of points between two given ones that lie on the same\n    great circle arc and are equally spaced by ``length`` km.\n\n    :param float lon1, lat1, depth1:\n        Coordinates of a point to start placing intervals from. The first\n        point in the resulting list has these coordinates.\n    :param float lon2, lat2, depth2:\n        Coordinates of the other end of the great circle arc segment\n        to put intervals on. The last resulting point might be closer\n        to the first reference point than the second one or further,\n        since the number of segments is taken as rounded division of\n        length between two reference points and ``length``.\n    :param length:\n        Required distance between two subsequent resulting points, in km.\n    :returns:\n        Tuple of three 1d numpy arrays: longitudes, latitudes and depths\n        of resulting points respectively.\n\n    Rounds the distance between two reference points with respect\n    to ``length`` and calls :func:`npoints_towards`.\n    \"\"\"\n    assert length > 0\n    hdist = geodetic_distance(lon1, lat1, lon2, lat2)\n    vdist = depth2 - depth1\n    # if this method is called multiple times with coordinates that are\n    # separated by the same distance, because of floating point imprecisions\n    # the total distance may have slightly different values (for instance if\n    # the distance between two set of points is 65 km, total distance can be\n    # 64.9999999999989910 and 65.0000000000020322). These two values bring to\n    # two different values of num_intervals (32 in the first case and 33 in\n    # the second), and this is a problem because for the same distance we\n    # should have the same number of intervals. To reduce potential differences\n    # due to floating point errors, we therefore round total_distance to a\n    # fixed precision (7)\n    total_distance = round(numpy.sqrt(hdist ** 2 + vdist ** 2), 7)\n    num_intervals = int(round(total_distance / length))\n    if num_intervals == 0:\n        return numpy.array([lon1]), numpy.array([lat1]), numpy.array([depth1])\n    dist_factor = (length * num_intervals) / total_distance\n    return npoints_towards(\n        lon1, lat1, depth1, azimuth(lon1, lat1, lon2, lat2),\n        hdist * dist_factor, vdist * dist_factor, num_intervals + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef npoints_between(lon1, lat1, depth1, lon2, lat2, depth2, npoints):\n    hdist = geodetic_distance(lon1, lat1, lon2, lat2)\n    vdist = depth2 - depth1\n    rlons, rlats, rdepths = npoints_towards(\n        lon1, lat1, depth1, azimuth(lon1, lat1, lon2, lat2),\n        hdist, vdist, npoints\n    )\n    # the last point should be left intact\n    rlons[-1] = lon2\n    rlats[-1] = lat2\n    rdepths[-1] = depth2\n    return rlons, rlats, rdepths", "response": "This function returns a list of specified number of points that are equally spaced along the great circle arc connecting given points."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef npoints_towards(lon, lat, depth, azimuth, hdist, vdist, npoints):\n    assert npoints > 1\n    rlon, rlat = numpy.radians(lon), numpy.radians(lat)\n    tc = numpy.radians(360 - azimuth)\n    hdists = numpy.arange(npoints, dtype=float)\n    hdists *= (hdist / EARTH_RADIUS) / (npoints - 1)\n    vdists = numpy.arange(npoints, dtype=float)\n    vdists *= vdist / (npoints - 1)\n\n    sin_dists = numpy.sin(hdists)\n    cos_dists = numpy.cos(hdists)\n    sin_lat = numpy.sin(rlat)\n    cos_lat = numpy.cos(rlat)\n\n    sin_lats = sin_lat * cos_dists + cos_lat * sin_dists * numpy.cos(tc)\n    lats = numpy.degrees(numpy.arcsin(sin_lats))\n\n    dlon = numpy.arctan2(numpy.sin(tc) * sin_dists * cos_lat,\n                         cos_dists - sin_lat * sin_lats)\n    lons = numpy.mod(rlon - dlon + numpy.pi, 2 * numpy.pi) - numpy.pi\n    lons = numpy.degrees(lons)\n\n    depths = vdists + depth\n\n    # the first point should be left intact\n    lons[0] = lon\n    lats[0] = lat\n    depths[0] = depth\n\n    return lons, lats, depths", "response": "This function finds a list of points starting from a given one holding a great circle arc with a given azimuth measured in a given point."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef point_at(lon, lat, azimuth, distance):\n    # this is a simplified version of npoints_towards().\n    # code duplication is justified by performance reasons.\n    lon, lat = numpy.radians(lon), numpy.radians(lat)\n    tc = numpy.radians(360 - azimuth)\n    sin_dists = numpy.sin(distance / EARTH_RADIUS)\n    cos_dists = numpy.cos(distance / EARTH_RADIUS)\n    sin_lat = numpy.sin(lat)\n    cos_lat = numpy.cos(lat)\n\n    sin_lats = sin_lat * cos_dists + cos_lat * sin_dists * numpy.cos(tc)\n    lats = numpy.degrees(numpy.arcsin(sin_lats))\n\n    dlon = numpy.arctan2(numpy.sin(tc) * sin_dists * cos_lat,\n                         cos_dists - sin_lat * sin_lats)\n    lons = numpy.mod(lon - dlon + numpy.pi, 2 * numpy.pi) - numpy.pi\n    lons = numpy.degrees(lons)\n\n    return lons, lats", "response": "Perform a forward geodetic transformation on a given point in km."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distance_to_semi_arc(alon, alat, aazimuth, plons, plats):\n\n    if type(plons) is float:\n        plons = numpy.array([plons])\n        plats = numpy.array([plats])\n\n    azimuth_to_target = azimuth(alon, alat, plons, plats)\n\n    # Find the indexes of the points in the positive y halfspace\n    idx = numpy.nonzero(numpy.cos(\n        numpy.radians((aazimuth-azimuth_to_target))) > 0.0)\n\n    # Find the indexes of the points in the negative y halfspace\n    idx_not = numpy.nonzero(numpy.cos(\n        numpy.radians((aazimuth-azimuth_to_target))) <= 0.0)\n\n    idx_ll_quadr = numpy.nonzero(\n        (numpy.cos(numpy.radians((aazimuth-azimuth_to_target))) <= 0.0) &\n        (numpy.sin(numpy.radians((aazimuth-azimuth_to_target))) > 0.0))\n\n    # Initialise the array containing the final distances\n    distance = numpy.zeros_like(plons)\n\n    # Compute the distance between the semi-arc with 'aazimuth' direction\n    # and the set of sites in the positive half-space. The shortest distance to\n    # the semi-arc in this case can be computed using the function\n    # :func:`openquake.hazardlib.geo.geodetic.distance_to_arc`.\n    if len(idx):\n        distance_to_target = geodetic_distance(alon, alat,\n                                               plons[idx], plats[idx])\n        t_angle = (azimuth_to_target[idx] - aazimuth + 360) % 360\n        angle = numpy.arccos((numpy.sin(numpy.radians(t_angle)) *\n                              numpy.sin(distance_to_target /\n                                        EARTH_RADIUS)))\n        distance[idx] = (numpy.pi / 2 - angle) * EARTH_RADIUS\n\n    # Compute the distance between the reference point and the set of sites\n    # in the negative half-space. The shortest distance for the semi-arc for\n    # all the points in the negative semi-space simply corresponds to the\n    # shortest distance to its origin.\n    if len(idx_not):\n        distance[idx_not] = geodetic_distance(alon, alat,\n                                              plons[idx_not], plats[idx_not])\n        distance[idx_ll_quadr] = -1 * distance[idx_ll_quadr]\n\n    return distance", "response": "This function calculates the distance between a semi - arc with a given azimuth and a set of plons and plats."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates a closest distance between a great circle arc and a target point.", "response": "def distance_to_arc(alon, alat, aazimuth, plons, plats):\n    \"\"\"\n    Calculate a closest distance between a great circle arc and a point\n    (or a collection of points).\n\n    :param float alon, alat:\n        Arc reference point longitude and latitude, in decimal degrees.\n    :param azimuth:\n        Arc azimuth (an angle between direction to a north and arc in clockwise\n        direction), measured in a reference point, in decimal degrees.\n    :param float plons, plats:\n        Longitudes and latitudes of points to measure distance. Either scalar\n        values or numpy arrays of decimal degrees.\n    :returns:\n        Distance in km, a scalar value or numpy array depending on ``plons``\n        and ``plats``. A distance is negative if the target point lies on the\n        right hand side of the arc.\n\n    Solves a spherical triangle formed by reference point, target point and\n    a projection of target point to a reference great circle arc.\n    \"\"\"\n    azimuth_to_target = azimuth(alon, alat, plons, plats)\n    distance_to_target = geodetic_distance(alon, alat, plons, plats)\n\n    # find an angle between an arc and a great circle arc connecting\n    # arc's reference point and a target point\n    t_angle = (azimuth_to_target - aazimuth + 360) % 360\n\n    # in a spherical right triangle cosine of the angle of a cathetus\n    # augmented to pi/2 is equal to sine of an opposite angle times\n    # sine of hypotenuse, see\n    # http://en.wikipedia.org/wiki/Spherical_trigonometry#Napier.27s_Pentagon\n    angle = numpy.arccos(\n        (numpy.sin(numpy.radians(t_angle))\n         * numpy.sin(distance_to_target / EARTH_RADIUS))\n    )\n    return (numpy.pi / 2 - angle) * EARTH_RADIUS"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare_coords(lons1, lats1, lons2, lats2):\n    lons1 = numpy.radians(lons1)\n    lats1 = numpy.radians(lats1)\n    assert lons1.shape == lats1.shape\n    lons2 = numpy.radians(lons2)\n    lats2 = numpy.radians(lats2)\n    assert lons2.shape == lats2.shape\n    return lons1, lats1, lons2, lats2", "response": "Convert two pairs of spherical coordinates in decimal degrees to numpy arrays of radians."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the seismogenic depths for physical consistency.", "response": "def _check_seismogenic_depths(self, upper_depth, lower_depth):\n        '''\n        Checks the seismic depths for physical consistency\n        :param float upper_depth:\n            Upper seismogenic depth (km)\n\n        :param float lower_depth:\n            Lower seismogenic depth (km)\n        '''\n        # Simple check on depths\n        if upper_depth:\n            if upper_depth < 0.:\n                raise ValueError('Upper seismogenic depth must be greater than'\n                                 ' or equal to 0.0!')\n            else:\n                self.upper_depth = upper_depth\n        else:\n            self.upper_depth = 0.0\n\n        if not lower_depth:\n            raise ValueError('Lower seismogenic depth must be defined for '\n                             'simple fault source!')\n        if lower_depth < self.upper_depth:\n            raise ValueError('Lower seismogenic depth must take a greater'\n                             ' value than upper seismogenic depth')\n\n        self.lower_depth = lower_depth"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating fault surface from input geometry.", "response": "def create_geometry(self, input_geometry, dip, upper_depth, lower_depth,\n                        mesh_spacing=1.0):\n        '''\n        If geometry is defined as a numpy array then create instance of\n        nhlib.geo.line.Line class, otherwise if already instance of class\n        accept class\n\n        :param input_geometry:\n            Trace (line) of the fault source as either\n            i) instance of nhlib.geo.line.Line class\n            ii) numpy.ndarray [Longitude, Latitude]\n\n        :param float dip:\n            Dip of fault surface (in degrees)\n\n        :param float upper_depth:\n            Upper seismogenic depth (km)\n\n        :param float lower_depth:\n            Lower seismogenic depth (km)\n\n        :param float mesh_spacing:\n            Spacing of the fault mesh (km) {default = 1.0}\n        '''\n        assert((dip > 0.) and (dip <= 90.))\n        self.dip = dip\n        self._check_seismogenic_depths(upper_depth, lower_depth)\n\n        if not isinstance(input_geometry, Line):\n            if not isinstance(input_geometry, np.ndarray):\n                raise ValueError('Unrecognised or unsupported geometry '\n                                 'definition')\n            else:\n                self.fault_trace = Line([Point(row[0], row[1]) for row in\n                                         input_geometry])\n        else:\n            self.fault_trace = input_geometry\n        # Build fault surface\n        self.geometry = SimpleFaultSurface.from_fault_data(self.fault_trace,\n                                                           self.upper_depth,\n                                                           self.lower_depth,\n                                                           self.dip,\n                                                           mesh_spacing)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting earthquakes within a distance of the fault source.", "response": "def select_catalogue(self, selector, distance,\n                         distance_metric='joyner-boore', upper_eq_depth=None,\n                         lower_eq_depth=None):\n        '''\n        Selects earthquakes within a distance of the fault\n\n        :param selector:\n            Populated instance of :class:\n            `openquake.hmtk.seismicity.selector.CatalogueSelector`\n\n        :param distance:\n            Distance from point (km) for selection\n\n        :param str distance_metric\n            Choice of fault source distance metric 'joyner-boore' or 'rupture'\n\n        :param float upper_eq_depth:\n            Upper hypocentral depth of hypocentres to be selected\n\n        :param float lower_eq_depth:\n            Lower hypocentral depth of hypocentres to be selected\n\n        '''\n        if selector.catalogue.get_number_events() < 1:\n            raise ValueError('No events found in catalogue!')\n\n        # rupture metric is selected and dip != 90 or 'rupture'\n        if ('rupture' in distance_metric) and (fabs(self.dip - 90) > 1E-5):\n            # Use rupture distance\n            self.catalogue = selector.within_rupture_distance(\n                self.geometry,\n                distance,\n                upper_depth=upper_eq_depth,\n                lower_depth=lower_eq_depth)\n        else:\n            # Use Joyner-Boore distance\n            self.catalogue = selector.within_joyner_boore_distance(\n                self.geometry,\n                distance,\n                upper_depth=upper_eq_depth,\n                lower_depth=lower_eq_depth)\n\n        if self.catalogue.get_number_events() < 5:\n            # Throw a warning regarding the small number of earthquakes in\n            # the source!\n            warnings.warn('Source %s (%s) has fewer than 5 events'\n                          % (self.id, self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_oqhazardlib_source(self, tom, mesh_spacing, use_defaults=False):\n        if not self.mfd:\n            raise ValueError(\"Cannot write to hazardlib without MFD\")\n        return SimpleFaultSource(\n            self.id,\n            self.name,\n            self.trt,\n            self.mfd,\n            mesh_spacing,\n            conv.mag_scale_rel_to_hazardlib(self.mag_scale_rel, use_defaults),\n            conv.render_aspect_ratio(self.rupt_aspect_ratio, use_defaults),\n            tom,\n            self.upper_depth,\n            self.lower_depth,\n            self.fault_trace,\n            self.dip,\n            self.rake)", "response": "Creates an instance of the OpenQuake. hazardlib. source. simple_fault. SimpleFaultSource class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots a set of recurrence models for a given set of recurrence resources.", "response": "def plot_recurrence_models(\n        configs, area, slip, msr, rake,\n        shear_modulus=30.0, disp_length_ratio=1.25E-5, msr_sigma=0.,\n        figure_size=(8, 6), filename=None, filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Plots a set of recurrence models\n\n    :param list configs:\n        List of configuration dictionaries\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    for config in configs:\n        model = RecurrenceBranch(area, slip, msr, rake, shear_modulus,\n                                 disp_length_ratio, msr_sigma, weight=1.0)\n        model.get_recurrence(config)\n        occurrence = model.recurrence.occur_rates\n        cumulative = np.array([np.sum(occurrence[iloc:])\n                               for iloc in range(0, len(occurrence))])\n        if 'AndersonLuco' in config['Model_Name']:\n            flt_label = config['Model_Name'] + ' - ' + config['Model_Type'] +\\\n                ' Type'\n        else:\n            flt_label = config['Model_Name']\n        flt_color = np.random.uniform(0.1, 1.0, 3)\n        ax.semilogy(model.magnitudes, cumulative, '-', label=flt_label,\n                    color=flt_color, linewidth=2.)\n        ax.semilogy(model.magnitudes, model.recurrence.occur_rates, '--',\n                    color=flt_color, linewidth=2.)\n\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Annual Rate')\n    ax.legend(bbox_to_anchor=(1.1, 1.0))\n    _save_image(fig, filename, filetype, dpi)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the area source geometry as a Node object", "response": "def build_area_source_geometry(area_source):\n    \"\"\"\n    Returns the area source geometry as a Node\n\n    :param area_source:\n        Area source model as an instance of the :class:\n        `openquake.hazardlib.source.area.AreaSource`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    geom = []\n    for lon_lat in zip(area_source.polygon.lons, area_source.polygon.lats):\n        geom.extend(lon_lat)\n    poslist_node = Node(\"gml:posList\", text=geom)\n    linear_ring_node = Node(\"gml:LinearRing\", nodes=[poslist_node])\n    exterior_node = Node(\"gml:exterior\", nodes=[linear_ring_node])\n    polygon_node = Node(\"gml:Polygon\", nodes=[exterior_node])\n    upper_depth_node = Node(\n        \"upperSeismoDepth\", text=area_source.upper_seismogenic_depth)\n    lower_depth_node = Node(\n        \"lowerSeismoDepth\", text=area_source.lower_seismogenic_depth)\n    return Node(\n        \"areaGeometry\", {'discretization': area_source.area_discretization},\n        nodes=[polygon_node, upper_depth_node, lower_depth_node])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_point_source_geometry(point_source):\n    xy = point_source.location.x, point_source.location.y\n    pos_node = Node(\"gml:pos\", text=xy)\n    point_node = Node(\"gml:Point\", nodes=[pos_node])\n    upper_depth_node = Node(\n        \"upperSeismoDepth\", text=point_source.upper_seismogenic_depth)\n    lower_depth_node = Node(\n        \"lowerSeismoDepth\", text=point_source.lower_seismogenic_depth)\n    return Node(\n        \"pointGeometry\",\n        nodes=[point_node, upper_depth_node, lower_depth_node])", "response": "Builds the poing source geometry as a Node\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a line into a Node class", "response": "def build_linestring_node(line, with_depth=False):\n    \"\"\"\n    Parses a line to a Node class\n\n    :param line:\n        Line as instance of :class:`openquake.hazardlib.geo.line.Line`\n    :param bool with_depth:\n        Include the depth values (True) or not (False):\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    geom = []\n    for p in line.points:\n        if with_depth:\n            geom.extend((p.x, p.y, p.z))\n        else:\n            geom.extend((p.x, p.y))\n    poslist_node = Node(\"gml:posList\", text=geom)\n    return Node(\"gml:LineString\", nodes=[poslist_node])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the simple fault source geometry as a Node", "response": "def build_simple_fault_geometry(fault_source):\n    \"\"\"\n    Returns the simple fault source geometry as a Node\n\n    :param fault_source:\n        Simple fault source model as an instance of the :class:\n        `openquake.hazardlib.source.simple_fault.SimpleFaultSource`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    linestring_node = build_linestring_node(fault_source.fault_trace,\n                                            with_depth=False)\n    dip_node = Node(\"dip\", text=fault_source.dip)\n    upper_depth_node = Node(\n        \"upperSeismoDepth\", text=fault_source.upper_seismogenic_depth)\n    lower_depth_node = Node(\n        \"lowerSeismoDepth\", text=fault_source.lower_seismogenic_depth)\n    return Node(\"simpleFaultGeometry\",\n                nodes=[linestring_node, dip_node, upper_depth_node,\n                       lower_depth_node])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the complex fault source geometry as a Node object", "response": "def build_complex_fault_geometry(fault_source):\n    \"\"\"\n    Returns the complex fault source geometry as a Node\n\n    :param fault_source:\n        Complex fault source model as an instance of the :class:\n        `openquake.hazardlib.source.complex_fault.ComplexFaultSource`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    num_edges = len(fault_source.edges)\n    edge_nodes = []\n    for iloc, edge in enumerate(fault_source.edges):\n        if iloc == 0:\n            # Top Edge\n            node_name = \"faultTopEdge\"\n\n        elif iloc == (num_edges - 1):\n            # Bottom edge\n            node_name = \"faultBottomEdge\"\n        else:\n            # Intermediate edge\n            node_name = \"intermediateEdge\"\n        edge_nodes.append(\n            Node(node_name,\n                 nodes=[build_linestring_node(edge, with_depth=True)]))\n    return Node(\"complexFaultGeometry\", nodes=edge_nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_evenly_discretised_mfd(mfd):\n    occur_rates = Node(\"occurRates\", text=mfd.occurrence_rates)\n    return Node(\"incrementalMFD\",\n                {\"binWidth\": mfd.bin_width, \"minMag\": mfd.min_mag},\n                nodes=[occur_rates])", "response": "Builds the evenly discretized MFD as a Node"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_truncated_gr_mfd(mfd):\n    return Node(\"truncGutenbergRichterMFD\",\n                {\"aValue\": mfd.a_val, \"bValue\": mfd.b_val,\n                 \"minMag\": mfd.min_mag, \"maxMag\": mfd.max_mag})", "response": "Parses the truncated Gutenberg Richter MFD as a Node\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the arbitrary MFD as a Node", "response": "def build_arbitrary_mfd(mfd):\n    \"\"\"\n    Parses the arbitrary MFD as a Node\n\n    :param mfd:\n        MFD as instance of :class:\n        `openquake.hazardlib.mfd.arbitrary.ArbitraryMFD`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    magnitudes = Node(\"magnitudes\", text=mfd.magnitudes)\n    occur_rates = Node(\"occurRates\", text=mfd.occurrence_rates)\n    return Node(\"arbitraryMFD\", nodes=[magnitudes, occur_rates])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the Youngs & Coppersmith MFD as a node.", "response": "def build_youngs_coppersmith_mfd(mfd):\n    \"\"\"\n    Parses the Youngs & Coppersmith MFD as a node. Note that the MFD does\n    not hold the total moment rate, but only the characteristic rate. Therefore\n    the node is written to the characteristic rate version regardless of\n    whether or not it was originally created from total moment rate\n\n    :param mfd:\n        MFD as instance of :class:\n        `openquake.hazardlib.mfd.youngs_coppersmith_1985.\n        YoungsCoppersmith1985MFD`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    return Node(\"YoungsCoppersmithMFD\",\n                {\"minMag\": mfd.min_mag, \"bValue\": mfd.b_val,\n                 \"characteristicMag\": mfd.char_mag,\n                 \"characteristicRate\": mfd.char_rate,\n                 \"binWidth\": mfd.bin_width})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the MultiMFD as a Node object.", "response": "def build_multi_mfd(mfd):\n    \"\"\"\n    Parses the MultiMFD as a Node\n\n    :param mfd:\n        MFD as instance of :class:\n        `openquake.hazardlib.mfd.multi_mfd.MultiMFD`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    node = Node(\"multiMFD\", dict(kind=mfd.kind, size=mfd.size))\n    for name in sorted(mfd.kwargs):\n        values = mfd.kwargs[name]\n        if name in ('magnitudes', 'occurRates'):\n            if len(values[0]) > 1:  # tested in multipoint_test.py\n                values = list(numpy.concatenate(values))\n            else:\n                values = sum(values, [])\n        node.append(Node(name, text=values))\n    if 'occurRates' in mfd.kwargs:\n        lengths = [len(rates) for rates in mfd.kwargs['occurRates']]\n        node.append(Node('lengths', text=lengths))\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_nodal_plane_dist(npd):\n    npds = []\n    for prob, npd in npd.data:\n        nodal_plane = Node(\n            \"nodalPlane\", {\"dip\": npd.dip, \"probability\": prob,\n                           \"strike\": npd.strike, \"rake\": npd.rake})\n        npds.append(nodal_plane)\n    return Node(\"nodalPlaneDist\", nodes=npds)", "response": "Builds the nodal plane distribution as a Node instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_hypo_depth_dist(hdd):\n    hdds = []\n    for (prob, depth) in hdd.data:\n        hdds.append(\n            Node(\"hypoDepth\", {\"depth\": depth, \"probability\": prob}))\n    return Node(\"hypoDepthDist\", nodes=hdds)", "response": "Build the hypocentral depth distribution as a Node instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn list of nodes of attributes common to all distributed seismicity source classes.", "response": "def get_distributed_seismicity_source_nodes(source):\n    \"\"\"\n    Returns list of nodes of attributes common to all distributed seismicity\n    source classes\n\n    :param source:\n        Seismic source as instance of :class:\n        `openquake.hazardlib.source.area.AreaSource` or :class:\n        `openquake.hazardlib.source.point.PointSource`\n    :returns:\n        List of instances of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    source_nodes = []\n    #  parse msr\n    source_nodes.append(\n        Node(\"magScaleRel\",\n             text=source.magnitude_scaling_relationship.__class__.__name__))\n    # Parse aspect ratio\n    source_nodes.append(\n        Node(\"ruptAspectRatio\", text=source.rupture_aspect_ratio))\n    # Parse MFD\n    source_nodes.append(obj_to_node(source.mfd))\n    # Parse nodal plane distribution\n    source_nodes.append(\n        build_nodal_plane_dist(source.nodal_plane_distribution))\n    # Parse hypocentral depth distribution\n    source_nodes.append(\n        build_hypo_depth_dist(source.hypocenter_distribution))\n    return source_nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_hypo_list_node(hypo_list):\n    hypolist = Node('hypoList', {})\n    for row in hypo_list:\n        n = Node(\n            'hypo', dict(alongStrike=row[0], downDip=row[1], weight=row[2]))\n        hypolist.append(n)\n    return hypolist", "response": "build a hypoList node from a list of hypo objects"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a hypoList node from a list of slip nodes", "response": "def build_slip_list_node(slip_list):\n    \"\"\"\n    :param slip_list:\n       an array of shape (N, 2) with columns (slip, weight)\n    :returns:\n        a hypoList node containing N slip nodes\n    \"\"\"\n    sliplist = Node('slipList', {})\n    for row in slip_list:\n        sliplist.append(\n            Node('slip', dict(weight=row[1]), row[0]))\n    return sliplist"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns list of nodes of attributes common to all fault source classes.", "response": "def get_fault_source_nodes(source):\n    \"\"\"\n    Returns list of nodes of attributes common to all fault source classes\n\n    :param source:\n        Fault source as instance of :class:\n        `openquake.hazardlib.source.simple_fault.SimpleFaultSource` or :class:\n        `openquake.hazardlib.source.complex_fault.ComplexFaultSource`\n    :returns:\n        List of instances of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    source_nodes = []\n    #  parse msr\n    source_nodes.append(\n        Node(\n            \"magScaleRel\",\n            text=source.magnitude_scaling_relationship.__class__.__name__))\n    # Parse aspect ratio\n    source_nodes.append(\n        Node(\"ruptAspectRatio\", text=source.rupture_aspect_ratio))\n    # Parse MFD\n    source_nodes.append(obj_to_node(source.mfd))\n    # Parse Rake\n    source_nodes.append(Node(\"rake\", text=source.rake))\n    if len(getattr(source, 'hypo_list', [])):\n        source_nodes.append(build_hypo_list_node(source.hypo_list))\n    if len(getattr(source, 'slip_list', [])):\n        source_nodes.append(build_slip_list_node(source.slip_list))\n    return source_nodes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of source attributes from the source class", "response": "def get_source_attributes(source):\n    \"\"\"\n    Retreives a dictionary of source attributes from the source class\n\n    :param source:\n        Seismic source as instance of :class:\n        `openquake.hazardlib.source.base.BaseSeismicSource`\n    :returns:\n        Dictionary of source attributes\n    \"\"\"\n    attrs = {\"id\": source.source_id,\n             \"name\": source.name,\n             \"tectonicRegion\": source.tectonic_region_type}\n    if isinstance(source, NonParametricSeismicSource):\n        if source.data[0][0].weight is not None:\n            weights = []\n            for data in source.data:\n                weights.append(data[0].weight)\n            attrs['rup_weights'] = numpy.array(weights)\n    print(attrs)\n    return attrs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_area_source_node(area_source):\n    # parse geometry\n    source_nodes = [build_area_source_geometry(area_source)]\n    # parse common distributed attributes\n    source_nodes.extend(get_distributed_seismicity_source_nodes(area_source))\n    return Node(\n        \"areaSource\", get_source_attributes(area_source), nodes=source_nodes)", "response": "Parses an area source to a Node class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a node for a rupture object", "response": "def build_rupture_node(rupt, probs_occur):\n    \"\"\"\n    :param rupt: a hazardlib rupture object\n    :param probs_occur: a list of floats with sum 1\n    \"\"\"\n    s = sum(probs_occur)\n    if abs(s - 1) > pmf.PRECISION:\n        raise ValueError('The sum of %s is not 1: %s' % (probs_occur, s))\n    h = rupt.hypocenter\n    hp_dict = dict(lon=h.longitude, lat=h.latitude, depth=h.depth)\n    rupt_nodes = [Node('magnitude', {}, rupt.mag),\n                  Node('rake', {}, rupt.rake),\n                  Node('hypocenter', hp_dict)]\n    rupt_nodes.extend(rupt.surface.surface_nodes)\n    geom = rupt.surface.surface_nodes[0].tag\n    if len(rupt.surface.surface_nodes) > 1:\n        name = 'multiPlanesRupture'\n    elif geom == 'planarSurface':\n        name = 'singlePlaneRupture'\n    elif geom == 'simpleFaultGeometry':\n        name = 'simpleFaultRupture'\n    elif geom == 'complexFaultGeometry':\n        name = 'complexFaultRupture'\n    elif geom == 'griddedSurface':\n        name = 'griddedRupture'\n    return Node(name, {'probs_occur': probs_occur}, nodes=rupt_nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a MultiPointSource object into a Node class", "response": "def build_multi_point_source_node(multi_point_source):\n    \"\"\"\n    Parses a point source to a Node class\n\n    :param point_source:\n        MultiPoint source as instance of :class:\n        `openquake.hazardlib.source.point.MultiPointSource`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    # parse geometry\n    pos = []\n    for p in multi_point_source.mesh:\n        pos.append(p.x)\n        pos.append(p.y)\n    mesh_node = Node('gml:posList', text=pos)\n    upper_depth_node = Node(\n        \"upperSeismoDepth\", text=multi_point_source.upper_seismogenic_depth)\n    lower_depth_node = Node(\n        \"lowerSeismoDepth\", text=multi_point_source.lower_seismogenic_depth)\n    source_nodes = [Node(\n        \"multiPointGeometry\",\n        nodes=[mesh_node, upper_depth_node, lower_depth_node])]\n    # parse common distributed attributes\n    source_nodes.extend(get_distributed_seismicity_source_nodes(\n        multi_point_source))\n    return Node(\"multiPointSource\",\n                get_source_attributes(multi_point_source),\n                nodes=source_nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_point_source_node(point_source):\n    # parse geometry\n    source_nodes = [build_point_source_geometry(point_source)]\n    # parse common distributed attributes\n    source_nodes.extend(get_distributed_seismicity_source_nodes(point_source))\n    return Node(\"pointSource\",\n                get_source_attributes(point_source),\n                nodes=source_nodes)", "response": "Parses a point source to a Node class\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_simple_fault_source_node(fault_source):\n    # Parse geometry\n    source_nodes = [build_simple_fault_geometry(fault_source)]\n    # Parse common fault source attributes\n    source_nodes.extend(get_fault_source_nodes(fault_source))\n    return Node(\"simpleFaultSource\",\n                get_source_attributes(fault_source),\n                nodes=source_nodes)", "response": "Parses a simple fault source into a Node class"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a complex fault source into a Node class", "response": "def build_complex_fault_source_node(fault_source):\n    \"\"\"\n    Parses a complex fault source to a Node class\n\n    :param fault_source:\n        Simple fault source as instance of :class:\n        `openquake.hazardlib.source.complex_fault.ComplexFaultSource`\n    :returns:\n        Instance of :class:`openquake.baselib.node.Node`\n    \"\"\"\n    # Parse geometry\n    source_nodes = [build_complex_fault_geometry(fault_source)]\n    # Parse common fault source attributes\n    source_nodes.extend(get_fault_source_nodes(fault_source))\n    return Node(\"complexFaultSource\",\n                get_source_attributes(fault_source),\n                nodes=source_nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a source model to XML.", "response": "def write_source_model(dest, sources_or_groups, name=None,\n                       investigation_time=None):\n    \"\"\"\n    Writes a source model to XML.\n\n    :param dest:\n        Destination path\n    :param sources_or_groups:\n        Source model in different formats\n    :param name:\n        Name of the source model (if missing, extracted from the filename)\n    \"\"\"\n    if isinstance(sources_or_groups, nrml.SourceModel):\n        with open(dest, 'wb') as f:\n            nrml.write([obj_to_node(sources_or_groups)], f, '%s')\n        return\n    if isinstance(sources_or_groups[0], sourceconverter.SourceGroup):\n        groups = sources_or_groups\n    else:  # passed a list of sources\n        srcs_by_trt = groupby(\n            sources_or_groups, operator.attrgetter('tectonic_region_type'))\n        groups = [sourceconverter.SourceGroup(trt, srcs_by_trt[trt])\n                  for trt in srcs_by_trt]\n    name = name or os.path.splitext(os.path.basename(dest))[0]\n    nodes = list(map(obj_to_node, sorted(groups)))\n    attrs = {\"name\": name}\n    if investigation_time is not None:\n        attrs['investigation_time'] = investigation_time\n    source_model = Node(\"sourceModel\", attrs, nodes=nodes)\n    with open(dest, 'wb') as f:\n        nrml.write([source_model], f, '%s')\n    return dest"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hdf5write(h5file, obj, root=''):\n    dic = node_to_dict(obj_to_node(obj))\n    h5file.save(dic, root)", "response": "Write a generic object serializable to a Node - like object into a openquake. baselib. hdf5. File object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for specification of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for specification of input and result values.\n        \"\"\"\n\n        # extract dictionary of coefficients specific to required\n        # intensity measure type\n        coeffs = self.COEFFS[imt]\n        coeffs.update(self.CONSTS)\n\n        # equation (1) is in terms of common logarithm\n        log_mean = (self._compute_magnitude(rup, coeffs) +\n                    self._compute_distance(dists, coeffs) +\n                    self._get_site_amplification(sites, coeffs) +\n                    self._get_mechanism(rup, coeffs))\n        # so convert to g and thence to the natural logarithm\n        mean = log_mean*np.log(10.0) - np.log(g)\n\n        # convert standard deviations from common to natural logarithm\n        log_stddevs = self._get_stddevs(coeffs, stddev_types, len(sites.vs30))\n        stddevs = log_stddevs*np.log(10.0)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns total sigma as reported in Table 2 p. 1202.", "response": "def _get_stddevs(self, coeffs, stddev_types, num_sites):\n        \"\"\"\n        Return total sigma as reported in Table 2, p. 1202.\n        \"\"\"\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            stddevs.append(coeffs['sigma'] + np.zeros(num_sites))\n        return np.array(stddevs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the distance between two sets of RJCs.", "response": "def _compute_distance(cls, dists, coeffs):\n        \"\"\"\n        Compute third term of equation (1) on p. 1200:\n\n        ``b3 * log(sqrt(Rjb ** 2 + b4 ** 2))``\n        \"\"\"\n        return coeffs['b3']*np.log10(np.sqrt(dists.rjb**2. + coeffs['b4']**2.))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the amplification of the site types.", "response": "def _get_site_amplification(self, sites, coeffs):\n        \"\"\"\n        Compute fourth term of equation (1) on p. 1200:\n\n        ``b5 * S``\n        \"\"\"\n        is_rock = self.get_site_type_dummy_variables(sites)\n        return coeffs['b5']*is_rock"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_mechanism(self, rup, coeffs):\n        is_strike_slip = self.get_fault_type_dummy_variables(rup)\n        return coeffs['b6']*is_strike_slip", "response": "Compute the mechanism of the fault type"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_site_type_dummy_variables(self, sites):\n        is_rock = np.array(sites.vs30 > self.NEHRP_BC_BOUNDARY)\n        return is_rock", "response": "Returns a list of variables that are used to determine whether a site type is dummy."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fault_type_dummy_variables(self, rup):\n\n        # normal faulting\n        is_normal = np.array(\n            self.RAKE_THRESH < -rup.rake < (180. - self.RAKE_THRESH))\n\n        # reverse raulting\n        is_reverse = np.array(\n            self.RAKE_THRESH < rup.rake < (180. - self.RAKE_THRESH))\n\n        if not self.ALREADY_WARNED and is_normal.any():\n            # make sure that the warning is printed only once to avoid\n            # flooding the terminal\n            msg = ('Normal faulting not supported by %s; '\n                   'treating as strike-slip' % type(self).__name__)\n            warnings.warn(msg, UserWarning)\n            self.ALREADY_WARNED = True\n\n        is_strike_slip = ~is_reverse | is_normal\n        is_strike_slip = is_strike_slip.astype(float)\n\n        return is_strike_slip", "response": "Return a dummy variable for fault - type classification based on the fault - type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the data from the CSV file and returns a dictionary of the data that is ready to be used for the Strain model.", "response": "def read_data(self, scaling_factor=1E-9, strain_headers=None):\n        '''\n        Reads the data from the csv file\n\n        :param float scaling_factor:\n            Scaling factor used for all strain values (default 1E-9 for\n            nanostrain)\n\n        :param list strain_headers:\n            List of the variables in the file that correspond to strain\n            parameters\n\n        :returns:\n            strain - Strain model as an instance of the :class:\n            openquake.hmtk.strain.geodetic_strain.GeodeticStrain\n\n        '''\n        if strain_headers:\n            self.strain.data_variables = strain_headers\n        else:\n            self.strain.data_variables = STRAIN_VARIABLES\n\n        datafile = open(self.filename, 'r')\n        reader = csv.DictReader(datafile)\n        self.strain.data = dict([(name, []) for name in reader.fieldnames])\n        for row in reader:\n            for name in row.keys():\n                if 'region' in name.lower():\n                    self.strain.data[name].append(row[name])\n                elif name in self.strain.data_variables:\n                    self.strain.data[name].append(\n                        scaling_factor * float(row[name]))\n                else:\n                    self.strain.data[name].append(float(row[name]))\n\n        for key in self.strain.data.keys():\n            if 'region' in key:\n                self.strain.data[key] = np.array(self.strain.data[key],\n                                                 dtype='S13')\n            else:\n                self.strain.data[key] = np.array(self.strain.data[key])\n\n        self._check_invalid_longitudes()\n\n        if 'region' not in self.strain.data:\n            print('No tectonic regionalisation found in input file!')\n        self.strain.data_variables = self.strain.data.keys()\n\n        # Update data with secondary data (i.e. 2nd invariant, e1h, e2h etc.\n        self.strain.get_secondary_strain_data()\n        return self.strain"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_invalid_longitudes(self):\n        '''\n        Checks to ensure that all longitudes are in the range -180. to 180\n        '''\n        idlon = self.strain.data['longitude'] > 180.\n        if np.any(idlon):\n            self.strain.data['longitude'][idlon] = \\\n                self.strain.data['longitude'][idlon] - 360.", "response": "Checks to ensure that all longitudes are in the range - 180 to 180."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_file(self, strain, scaling_factor=1E-9):\n        '''\n        Main writer function for the csv file\n\n        :param strain:\n            Instance of :class: openquake.hmtk.strain.geodetic_strain.GeodeticStrain\n        :param float scaling_factor:\n            Scaling factor used for all strain values (default 1E-9 for\n            nanostrain)\n        '''\n        if not isinstance(strain, GeodeticStrain):\n            raise ValueError('Strain data must be instance of GeodeticStrain')\n\n        for key in strain.data.keys():\n            if key in strain.data_variables:\n                # Return strain value back to original scaling\n                if key in ['longitude', 'latitude']:\n                    continue\n                strain.data[key] = strain.data[key] / scaling_factor\n\n        # Slice seismicity rates into separate dictionary vectors\n        strain, output_variables = self.slice_rates_to_data(strain)\n\n        outfile = open(self.filename, 'wt')\n        print('Writing strain data to file %s' % self.filename)\n        writer = csv.DictWriter(outfile,\n                                fieldnames=output_variables)\n        writer.writeheader()\n        for iloc in range(0, strain.get_number_observations()):\n            row_dict = {}\n            for key in output_variables:\n                if len(strain.data[key]) > 0:\n                    # Ignores empty dictionary attributes\n                    row_dict[key] = strain.data[key][iloc]\n            writer.writerow(row_dict)\n        outfile.close()\n        print('done!')", "response": "This function writes the seismicity rates to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef slice_rates_to_data(self, strain):\n        '''\n        For the strain data, checks to see if seismicity rates have been\n        calculated. If so, each column in the array is sliced and stored as a\n        single vector in the strain.data dictionary with the corresponding\n        magnitude as a key.\n\n        :param strain:\n            Instance of :class: openquake.hmtk.strain.geodetic_strain.GeodeticStrain\n\n        :returns:\n            strain - Instance of strain class with updated data dictionary\n            output_variables - Updated list of headers\n        '''\n        output_variables = list(strain.data)\n        cond = (isinstance(strain.target_magnitudes, np.ndarray) or\n                isinstance(strain.target_magnitudes, list))\n        if cond:\n            magnitude_list = ['%.3f' % mag for mag in strain.target_magnitudes]\n        else:\n            return strain, output_variables\n\n        # Ensure that the number of rows in the rate array corresponds to the\n        # number of observations\n        assert np.shape(strain.seismicity_rate)[0] == \\\n            strain.get_number_observations()\n\n        for iloc, magnitude in enumerate(magnitude_list):\n            strain.data[magnitude] = strain.seismicity_rate[:, iloc]\n        output_variables.extend(magnitude_list)\n        return strain, output_variables", "response": "Slice the seismicity rates of the strain data into a single vector in the data dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the configuration file and make each section available in a separate dict.", "response": "def read(*paths, **validators):\n    \"\"\"\n    Load the configuration, make each section available in a separate dict.\n\n    The configuration location can specified via an environment variable:\n       - OQ_CONFIG_FILE\n\n    In the absence of this environment variable the following paths will be\n    used:\n       - sys.prefix + /openquake.cfg when in a virtualenv\n       - /etc/openquake/openquake.cfg outside of a virtualenv\n\n    If those files are missing, the fallback is the source code:\n       - openquake/engine/openquake.cfg\n\n    Please note: settings in the site configuration file are overridden\n    by settings with the same key names in the OQ_CONFIG_FILE openquake.cfg.\n    \"\"\"\n    paths = config.paths + list(paths)\n    parser = configparser.ConfigParser()\n    found = parser.read(os.path.normpath(os.path.expanduser(p)) for p in paths)\n    if not found:\n        raise IOError('No configuration file found in %s' % str(paths))\n    config.found = found\n    config.clear()\n    for section in parser.sections():\n        config[section] = sec = DotDict(parser.items(section))\n        for k, v in sec.items():\n            sec[k] = validators.get(k, lambda x: x)(v)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert string in boolean", "response": "def boolean(flag):\n    \"\"\"\n    Convert string in boolean\n    \"\"\"\n    s = flag.lower()\n    if s in ('1', 'yes', 'true'):\n        return True\n    elif s in ('0', 'no', 'false'):\n        return False\n    raise ValueError('Unknown flag %r' % s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        mean = self._get_mean(sites.vs30, rup.mag, dists.rrup, imt, scale_fac=0)\n        stddevs = self._get_stddevs(stddev_types, num_sites=sites.vs30.size)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and return the mean value for the current object.", "response": "def _get_mean(self, vs30, mag, rrup, imt, scale_fac):\n        \"\"\"\n        Compute and return mean\n        \"\"\"\n        C_HR, C_BC, C_SR, SC = self._extract_coeffs(imt)\n\n        rrup = self._clip_distances(rrup)\n\n        f0 = self._compute_f0_factor(rrup)\n        f1 = self._compute_f1_factor(rrup)\n        f2 = self._compute_f2_factor(rrup)\n\n        pga_bc = self._get_pga_bc(\n            f0, f1, f2, SC, mag, rrup, vs30, scale_fac\n        )\n\n        # compute mean values for hard-rock sites (vs30 >= 2000),\n        # and non-hard-rock sites (vs30 < 2000) and add soil amplification\n        # term\n        mean = np.zeros_like(vs30)\n        self._compute_mean(C_HR, f0, f1, f2, SC, mag, rrup,\n                           vs30 >= 2000.0, mean, scale_fac)\n        self._compute_mean(C_BC, f0, f1, f2, SC, mag, rrup,\n                           vs30 < 2000.0, mean, scale_fac)\n        self._compute_soil_amplification(C_SR, vs30, pga_bc, mean)\n\n        # convert from base 10 to base e\n        if imt == PGV():\n            mean = np.log(10 ** mean)\n        else:\n            # convert from cm/s**2 to g\n            mean = np.log((10 ** mean) * 1e-2 / g)\n\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing and return PGA on BC boundary", "response": "def _get_pga_bc(self, f0, f1, f2, SC, mag, rrup, vs30, scale_fac):\n        \"\"\"\n        Compute and return PGA on BC boundary\n        \"\"\"\n        pga_bc = np.zeros_like(vs30)\n        self._compute_mean(self.COEFFS_BC[PGA()], f0, f1, f2, SC, mag,\n                           rrup, vs30 < 2000.0, pga_bc, scale_fac)\n\n        return (10 ** pga_bc) * 1e-2 / g"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_coeffs(self, imt):\n        C_HR = self.COEFFS_HARD_ROCK[imt]\n        C_BC = self.COEFFS_BC[imt]\n        C_SR = self.COEFFS_SOIL_RESPONSE[imt]\n        SC = self.COEFFS_STRESS[imt]\n\n        return C_HR, C_BC, C_SR, SC", "response": "Extracts the coefficients specific to the required intensity measure type from the COEFFS table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_f0_factor(self, rrup):\n        # f0 = max(log10(R0/rrup),0)\n        f0 = np.log10(self.COEFFS_IMT_INDEPENDENT['R0'] / rrup)\n        f0[f0 < 0] = 0.0\n\n        return f0", "response": "Compute and return factor f0 for the current log10 - R0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute and return factor f1", "response": "def _compute_f1_factor(self, rrup):\n        \"\"\"\n        Compute and return factor f1 - see equation (5), 4th term, p. 2191\n        \"\"\"\n        # f1 = min(log10(rrup),log10(R1))\n        f1 = np.log10(rrup)\n        logR1 = np.log10(self.COEFFS_IMT_INDEPENDENT['R1'])\n        f1[f1 > logR1] = logR1\n\n        return f1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_f2_factor(self, rrup):\n        # f2 = max(log10(rrup/R2),0)\n        f2 = np.log10(rrup / self.COEFFS_IMT_INDEPENDENT['R2'])\n        f2[f2 < 0] = 0.0\n\n        return f2", "response": "Compute and return factor f2 based on rrup"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the stress drop adjustment for the given stress drop.", "response": "def _compute_stress_drop_adjustment(self, SC, mag, scale_fac):\n        \"\"\"\n        Compute equation (6) p. 2200\n        \"\"\"\n        return scale_fac * np.minimum(\n            SC['delta'] + 0.05,\n            0.05 + SC['delta'] * (\n                np.maximum(mag - SC['M1'], 0) / (SC['Mh'] - SC['M1'])\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_mean(self, C, f0, f1, f2, SC, mag, rrup, idxs, mean,\n                      scale_fac):\n        \"\"\"\n        Compute mean value (for a set of indexes) without site amplification\n        terms. This is equation (5), p. 2191, without S term.\n        \"\"\"\n        mean[idxs] = (C['c1'] +\n                      C['c2'] * mag +\n                      C['c3'] * (mag ** 2) +\n                      (C['c4'] + C['c5'] * mag) * f1[idxs] +\n                      (C['c6'] + C['c7'] * mag) * f2[idxs] +\n                      (C['c8'] + C['c9'] * mag) * f0[idxs] +\n                      C['c10'] * rrup[idxs] +\n                      self._compute_stress_drop_adjustment(SC, mag, scale_fac))", "response": "Compute mean value for a set of site amplification log terms. This is the inverse of _compute_log_likelihood_from_f0_f1_f2_f1_f2."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_soil_amplification(self, C, vs30, pga_bc, mean):\n        # convert from base e (as defined in BA2008) to base 10 (as used in\n        # AB2006)\n        sal = np.log10(np.exp(self._get_site_amplification_linear(vs30, C)))\n        sanl = np.log10(np.exp(\n            self._get_site_amplification_non_linear(vs30, pga_bc, C)))\n\n        idxs = vs30 < 2000.0\n        mean[idxs] = mean[idxs] + sal[idxs] + sanl[idxs]", "response": "Compute soil amplification that is S term in equation 5 p. 2191 and p. 2191 and add to mean values for hard rock sites."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stress_drop_scaling_factor(self, magnitude):\n        stress_drop = 10.0 ** (3.45 - 0.2 * magnitude)\n        cap = 10.0 ** (3.45 - 0.2 * 5.0)\n        if stress_drop > cap:\n            stress_drop = cap\n        return log10(stress_drop / 140.0) / log10(2.0)", "response": "Returns the stress drop scaling factor defined in the Atkinson and Boore equation 6 page 1128 of Atkinson & Boore 2001."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclip GMPE mean value at 1. 5 g for PGA and 3 g for short periods.", "response": "def clip_mean(imt, mean):\n    \"\"\"\n    Clip GMPE mean value at 1.5 g for PGA and 3 g for short periods\n    (0.02 < T < 0.55)\n    \"\"\"\n    if imt.period == 0:\n        mean[mean > 0.405] = 0.405\n\n    if 0.02 < imt.period < 0.55:\n        mean[mean > 1.099] = 1.099\n\n    return mean"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a generator of rupture getters for the given slice of the rupture array.", "response": "def gen_rupture_getters(dstore, slc=slice(None),\n                        concurrent_tasks=1, hdf5cache=None):\n    \"\"\"\n    :yields: RuptureGetters\n    \"\"\"\n    if dstore.parent:\n        dstore = dstore.parent\n    csm_info = dstore['csm_info']\n    trt_by_grp = csm_info.grp_by(\"trt\")\n    samples = csm_info.get_samples_by_grp()\n    rlzs_by_gsim = csm_info.get_rlzs_by_gsim_grp()\n    rup_array = dstore['ruptures'][slc]\n    maxweight = numpy.ceil(len(rup_array) / (concurrent_tasks or 1))\n    nr, ne = 0, 0\n    for grp_id, arr in general.group_array(rup_array, 'grp_id').items():\n        if not rlzs_by_gsim[grp_id]:\n            # this may happen if a source model has no sources, like\n            # in event_based_risk/case_3\n            continue\n        for block in general.block_splitter(arr, maxweight):\n            rgetter = RuptureGetter(\n                hdf5cache or dstore.filename, numpy.array(block), grp_id,\n                trt_by_grp[grp_id], samples[grp_id], rlzs_by_gsim[grp_id])\n            rgetter.weight = getattr(block, 'weight', len(block))\n            yield rgetter\n            nr += len(block)\n            ne += rgetter.num_events\n    logging.info('Read %d ruptures and %d events', nr, ne)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_maxloss_rupture(dstore, loss_type):\n    lti = dstore['oqparam'].lti[loss_type]\n    ridx = dstore.get_attr('rup_loss_table', 'ridx')[lti]\n    [rgetter] = gen_rupture_getters(dstore, slice(ridx, ridx + 1))\n    [ebr] = rgetter.get_ruptures()\n    return ebr", "response": "Get the maximum rupture for the current rupture"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init(self):\n        if hasattr(self, 'data'):  # already initialized\n            return\n        if isinstance(self.dstore, str):\n            self.dstore = hdf5.File(self.dstore, 'r')\n        else:\n            self.dstore.open('r')  # if not\n        if self.sids is None:\n            self.sids = self.dstore['sitecol'].sids\n        oq = self.dstore['oqparam']\n        self.imtls = oq.imtls\n        self.poes = self.poes or oq.poes\n        self.data = {}\n        try:\n            hcurves = self.get_hcurves(self.imtls)  # shape (R, N)\n        except IndexError:  # no data\n            return\n        for sid, hcurve_by_rlz in zip(self.sids, hcurves.T):\n            self.data[sid] = datadict = {}\n            for rlzi, hcurve in enumerate(hcurve_by_rlz):\n                datadict[rlzi] = lst = [None for imt in self.imtls]\n                for imti, imt in enumerate(self.imtls):\n                    lst[imti] = hcurve[imt]", "response": "Read the poes and set the. data attribute with the hazard curves\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pmap_by_grp(self):\n        if hasattr(self, '_pmap_by_grp'):  # already called\n            return self._pmap_by_grp\n        # populate _pmap_by_grp\n        self._pmap_by_grp = {}\n        if 'poes' in self.dstore:\n            # build probability maps restricted to the given sids\n            ok_sids = set(self.sids)\n            for grp, dset in self.dstore['poes'].items():\n                ds = dset['array']\n                L, G = ds.shape[1:]\n                pmap = probability_map.ProbabilityMap(L, G)\n                for idx, sid in enumerate(dset['sids'].value):\n                    if sid in ok_sids:\n                        pmap[sid] = probability_map.ProbabilityCurve(ds[idx])\n                self._pmap_by_grp[grp] = pmap\n                self.nbytes += pmap.nbytes\n        return self._pmap_by_grp", "response": "returns a dictionary of probability maps restricted to the given sids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the hazard curves for the given realization index", "response": "def get(self, rlzi, grp=None):\n        \"\"\"\n        :param rlzi: a realization index\n        :param grp: None (all groups) or a string of the form \"grp-XX\"\n        :returns: the hazard curves for the given realization\n        \"\"\"\n        self.init()\n        assert self.sids is not None\n        pmap = probability_map.ProbabilityMap(len(self.imtls.array), 1)\n        grps = [grp] if grp is not None else sorted(self.pmap_by_grp)\n        array = self.rlzs_assoc.by_grp()\n        for grp in grps:\n            for gsim_idx, rlzis in enumerate(array[grp]):\n                for r in rlzis:\n                    if r == rlzi:\n                        pmap |= self.pmap_by_grp[grp].extract(gsim_idx)\n                        break\n        return pmap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_hcurves(self, imtls=None):\n        self.init()\n        if imtls is None:\n            imtls = self.imtls\n        pmaps = [pmap.convert2(imtls, self.sids)\n                 for pmap in self.get_pmaps()]\n        return numpy.array(pmaps)", "response": "returns an array of hazard curves for the given intensity measure types and levels"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield the probability maps for the individual realizations.", "response": "def items(self, kind=''):\n        \"\"\"\n        Extract probability maps from the datastore, possibly generating\n        on the fly the ones corresponding to the individual realizations.\n        Yields pairs (tag, pmap).\n\n        :param kind:\n            the kind of PoEs to extract; if not given, returns the realization\n            if there is only one or the statistics otherwise.\n        \"\"\"\n        num_rlzs = len(self.weights)\n        if not kind or kind == 'all':  # use default\n            if 'hcurves' in self.dstore:\n                for k in sorted(self.dstore['hcurves']):\n                    yield k, self.dstore['hcurves/' + k].value\n            elif num_rlzs == 1:\n                yield 'mean', self.get(0)\n            return\n        if 'poes' in self.dstore and kind in ('rlzs', 'all'):\n            for rlzi in range(num_rlzs):\n                hcurves = self.get(rlzi)\n                yield 'rlz-%03d' % rlzi, hcurves\n        elif 'poes' in self.dstore and kind.startswith('rlz-'):\n            yield kind, self.get(int(kind[4:]))\n        if 'hcurves' in self.dstore and kind == 'stats':\n            for k in sorted(self.dstore['hcurves']):\n                if not k.startswith('rlz'):\n                    yield k, self.dstore['hcurves/' + k].value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean(self, grp=None):\n        self.init()\n        if len(self.weights) == 1:  # one realization\n            # the standard deviation is zero\n            pmap = self.get(0, grp)\n            for sid, pcurve in pmap.items():\n                array = numpy.zeros(pcurve.array.shape[:-1] + (2,))\n                array[:, 0] = pcurve.array[:, 0]\n                pcurve.array = array\n            return pmap\n        else:  # multiple realizations\n            dic = ({g: self.dstore['poes/' + g] for g in self.dstore['poes']}\n                   if grp is None else {grp: self.dstore['poes/' + grp]})\n            pmaps = self.rlzs_assoc.combine_pmaps(dic)\n            return stats.compute_pmap_stats(\n                pmaps, [stats.mean_curve, stats.std_curve],\n                self.weights, self.imtls)", "response": "Compute the mean curve as a ProbabilityMap\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the GMFs for the given realization and the rupture.", "response": "def gen_gmfs(self):\n        \"\"\"\n        Compute the GMFs for the given realization and\n        yields arrays of the dtype (sid, eid, imti, gmv), one for rupture\n        \"\"\"\n        self.sig_eps = []\n        for computer in self.computers:\n            rup = computer.rupture\n            sids = computer.sids\n            eids_by_rlz = rup.get_eids_by_rlz(self.rlzs_by_gsim)\n            data = []\n            for gs, rlzs in self.rlzs_by_gsim.items():\n                num_events = sum(len(eids_by_rlz[rlzi]) for rlzi in rlzs)\n                if num_events == 0:\n                    continue\n                # NB: the trick for performance is to keep the call to\n                # compute.compute outside of the loop over the realizations\n                # it is better to have few calls producing big arrays\n                array, sig, eps = computer.compute(gs, num_events)\n                array = array.transpose(1, 0, 2)  # from M, N, E to N, M, E\n                for i, miniml in enumerate(self.min_iml):  # gmv < minimum\n                    arr = array[:, i, :]\n                    arr[arr < miniml] = 0\n                n = 0\n                for rlzi in rlzs:\n                    eids = eids_by_rlz[rlzi]\n                    e = len(eids)\n                    if not e:\n                        continue\n                    for ei, eid in enumerate(eids):\n                        gmf = array[:, :, n + ei]  # shape (N, M)\n                        tot = gmf.sum(axis=0)  # shape (M,)\n                        if not tot.sum():\n                            continue\n                        sigmas = sig[:, n + ei]\n                        self.sig_eps.append((eid, sigmas, eps[:, n + ei]))\n                        for sid, gmv in zip(sids, gmf):\n                            if gmv.sum():\n                                data.append((rlzi, sid, eid, gmv))\n                    n += e\n            yield numpy.array(data, self.gmv_dt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an array of the dtype sid eid imti gmv", "response": "def get_gmfdata(self):\n        \"\"\"\n        :returns: an array of the dtype (sid, eid, imti, gmv)\n        \"\"\"\n        alldata = list(self.gen_gmfs())\n        if not alldata:\n            return numpy.zeros(0, self.gmv_dt)\n        return numpy.concatenate(alldata)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_hazard(self, data=None):\n        if data is None:\n            data = self.get_gmfdata()\n        return general.group_array(data, 'sid')", "response": "get_hazard - Returns a list of records of type gmf_dt\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_gmfs_curves(self, monitor):\n        oq = self.oqparam\n        with monitor('GmfGetter.init', measuremem=True):\n            self.init()\n        hcurves = {}  # key -> poes\n        if oq.hazard_curves_from_gmfs:\n            hc_mon = monitor('building hazard curves', measuremem=False)\n            duration = oq.investigation_time * oq.ses_per_logic_tree_path\n            with monitor('building hazard', measuremem=True):\n                gmfdata = self.get_gmfdata()  # returned later\n                hazard = self.get_hazard(data=gmfdata)\n            for sid, hazardr in hazard.items():\n                dic = general.group_array(hazardr, 'rlzi')\n                for rlzi, array in dic.items():\n                    with hc_mon:\n                        gmvs = array['gmv']\n                        for imti, imt in enumerate(oq.imtls):\n                            poes = _gmvs_to_haz_curve(\n                                gmvs[:, imti], oq.imtls[imt],\n                                oq.investigation_time, duration)\n                            hcurves[rsi2str(rlzi, sid, imt)] = poes\n        elif oq.ground_motion_fields:  # fast lane\n            with monitor('building hazard', measuremem=True):\n                gmfdata = self.get_gmfdata()\n        else:\n            return {}\n        if len(gmfdata) == 0:\n            return dict(gmfdata=[])\n        indices = []\n        gmfdata.sort(order=('sid', 'rlzi', 'eid'))\n        start = stop = 0\n        for sid, rows in itertools.groupby(gmfdata['sid']):\n            for row in rows:\n                stop += 1\n            indices.append((sid, start, stop))\n            start = stop\n        res = dict(gmfdata=gmfdata, hcurves=hcurves,\n                   sig_eps=numpy.array(self.sig_eps, self.sig_eps_dt),\n                   indices=numpy.array(indices, (U32, 3)))\n        return res", "response": "Compute the GMFs curves for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the weight of the ruptures in the getter.", "response": "def set_weights(self, src_filter, num_taxonomies_by_site):\n        \"\"\"\n        :returns: the weights of the ruptures in the getter\n        \"\"\"\n        weights = []\n        for rup in self.rup_array:\n            sids = src_filter.close_sids(rup, self.trt, rup['mag'])\n            weights.append(num_taxonomies_by_site[sids].sum())\n        self.weights = numpy.array(weights)\n        self.weight = self.weights.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding RuptureGetters with weight < = maxweight.", "response": "def split(self, maxweight):\n        \"\"\"\n        :yields: RuptureGetters with weight <= maxweight\n        \"\"\"\n        # NB: can be called only after .set_weights() has been called\n        idx = {ri: i for i, ri in enumerate(self.rup_indices)}\n        for rup_indices in general.block_splitter(\n                self.rup_indices, maxweight, lambda ri: self.weights[idx[ri]]):\n            if rup_indices:\n                # some indices may have weight 0 and are discarded\n                rgetter = self.__class__(\n                    self.filename, list(rup_indices), self.grp_id,\n                    self.trt, self.samples, self.rlzs_by_gsim,\n                    self.first_event)\n                rgetter.weight = sum([self.weights[idx[ri]]\n                                      for ri in rup_indices])\n                yield rgetter"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a composite array with the associations eid -> rlz", "response": "def get_eid_rlz(self, monitor=None):\n        \"\"\"\n        :returns: a composite array with the associations eid->rlz\n        \"\"\"\n        eid_rlz = []\n        for rup in self.rup_array:\n            ebr = EBRupture(mock.Mock(serial=rup['serial']), rup['srcidx'],\n                            self.grp_id, rup['n_occ'], self.samples)\n            for rlz, eids in ebr.get_eids_by_rlz(self.rlzs_by_gsim).items():\n                for eid in eids:\n                    eid_rlz.append((eid, rlz))\n        return numpy.array(eid_rlz, [('eid', U64), ('rlz', U16)])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary with the parameters of the rupture", "response": "def get_rupdict(self):\n        \"\"\"\n        :returns: a dictionary with the parameters of the rupture\n        \"\"\"\n        assert len(self.rup_array) == 1, 'Please specify a slice of length 1'\n        dic = {'trt': self.trt, 'samples': self.samples}\n        with datastore.read(self.filename) as dstore:\n            rupgeoms = dstore['rupgeoms']\n            source_ids = dstore['source_info']['source_id']\n            rec = self.rup_array[0]\n            geom = rupgeoms[rec['gidx1']:rec['gidx2']].reshape(\n                rec['sy'], rec['sz'])\n            dic['lons'] = geom['lon']\n            dic['lats'] = geom['lat']\n            dic['deps'] = geom['depth']\n            rupclass, surclass = self.code2cls[rec['code']]\n            dic['rupture_class'] = rupclass.__name__\n            dic['surface_class'] = surclass.__name__\n            dic['hypo'] = rec['hypo']\n            dic['occurrence_rate'] = rec['occurrence_rate']\n            dic['grp_id'] = rec['grp_id']\n            dic['n_occ'] = rec['n_occ']\n            dic['serial'] = rec['serial']\n            dic['mag'] = rec['mag']\n            dic['srcid'] = source_ids[rec['srcidx']]\n        return dic"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of ruptures filtered by bounding box", "response": "def get_ruptures(self, srcfilter=calc.filters.nofilter):\n        \"\"\"\n        :returns: a list of EBRuptures filtered by bounding box\n        \"\"\"\n        ebrs = []\n        with datastore.read(self.filename) as dstore:\n            rupgeoms = dstore['rupgeoms']\n            for rec in self.rup_array:\n                if srcfilter.integration_distance:\n                    sids = srcfilter.close_sids(rec, self.trt, rec['mag'])\n                    if len(sids) == 0:  # the rupture is far away\n                        continue\n                else:\n                    sids = None\n                mesh = numpy.zeros((3, rec['sy'], rec['sz']), F32)\n                geom = rupgeoms[rec['gidx1']:rec['gidx2']].reshape(\n                    rec['sy'], rec['sz'])\n                mesh[0] = geom['lon']\n                mesh[1] = geom['lat']\n                mesh[2] = geom['depth']\n                rupture_cls, surface_cls = self.code2cls[rec['code']]\n                rupture = object.__new__(rupture_cls)\n                rupture.serial = rec['serial']\n                rupture.surface = object.__new__(surface_cls)\n                rupture.mag = rec['mag']\n                rupture.rake = rec['rake']\n                rupture.hypocenter = geo.Point(*rec['hypo'])\n                rupture.occurrence_rate = rec['occurrence_rate']\n                rupture.tectonic_region_type = self.trt\n                if surface_cls is geo.PlanarSurface:\n                    rupture.surface = geo.PlanarSurface.from_array(\n                        mesh[:, 0, :])\n                elif surface_cls is geo.MultiSurface:\n                    # mesh has shape (3, n, 4)\n                    rupture.surface.__init__([\n                        geo.PlanarSurface.from_array(mesh[:, i, :])\n                        for i in range(mesh.shape[1])])\n                elif surface_cls is geo.GriddedSurface:\n                    # fault surface, strike and dip will be computed\n                    rupture.surface.strike = rupture.surface.dip = None\n                    rupture.surface.mesh = Mesh(*mesh)\n                else:\n                    # fault surface, strike and dip will be computed\n                    rupture.surface.strike = rupture.surface.dip = None\n                    rupture.surface.__init__(RectangularMesh(*mesh))\n                grp_id = rec['grp_id']\n                ebr = EBRupture(rupture, rec['srcidx'], grp_id,\n                                rec['n_occ'], self.samples)\n                # not implemented: rupture_slip_direction\n                ebr.sids = sids\n                ebrs.append(ebr)\n        return ebrs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef E2R(self, array, rlzi):\n        z = numpy.zeros((self.num_rlzs,) + array.shape[1:], array.dtype)\n        for a, r in zip(array, rlzi):\n            z[self.rlz2idx[r]] += a\n        return z", "response": "E2R - > R"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n\n        C = self.COEFFS[imt]\n        mean = (self._get_magnitude_scaling(C, rup.mag) +\n                self._get_distance_scaling(C, dists, rup.mag) +\n                self._get_site_term(C, sites.vs30))\n\n        # Mean is returned in terms of m/s^2. Need to convert to g\n        mean -= np.log(g)\n        stddevs = self.get_stddevs(C, sites.vs30.shape, stddev_types)\n        return mean + self.adjustment_factor, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the magnitude scaling term in equation 4", "response": "def _get_magnitude_scaling(self, C, mag):\n        \"\"\"\n        Implements the magnitude scaling function F(M) presented in equation 4\n        \"\"\"\n        if mag < self.CONSTANTS[\"mh\"]:\n            return C[\"e1\"] + C[\"b1\"] * (mag - self.CONSTANTS[\"mref\"]) +\\\n                C[\"b2\"] * ((mag - self.CONSTANTS[\"mref\"]) ** 2.)\n        else:\n            d_m = self.CONSTANTS[\"mh\"] - self.CONSTANTS[\"mref\"]\n            return C[\"e1\"] + C[\"b3\"] * (mag - self.CONSTANTS[\"mh\"]) +\\\n                (C[\"b1\"] * d_m) + C[\"b2\"] * (d_m ** 2.)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_distance_scaling(self, C, dists, mag):\n        r_h = self._get_rh(C, dists)\n        return (C[\"c1\"] + C[\"c2\"] * (mag - self.CONSTANTS[\"mref\"])) *\\\n            np.log(r_h / self.CONSTANTS[\"rref\"]) +\\\n            C[\"c3\"] * (r_h - self.CONSTANTS[\"rref\"])", "response": "Returns the distance scaling term F for the resource class in equation 2 and 3"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n\n        # Distance term\n        R = np.sqrt(dists.rjb ** 2 + 11.29 ** 2)\n\n        # Magnitude term\n        M = rup.mag - 6\n\n        # Site term only distinguishes between lava and ash;\n        # since ash sites have Vs30 in the range 60-200m/s,\n        # we use this upper value as class separator\n        S = np.zeros(R.shape)\n        S[sites.vs30 <= 200] = 1\n\n        # Mean ground motion (log10)\n        mean = (0.518 + 0.387*M - np.log10(R) - 0.00256*R + 0.335*S)\n\n        # Converting to natural log\n        mean /= np.log10(np.e)\n\n        # Check for standard deviation type\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        # Constant (total) standard deviation\n        stddevs = [0.237/np.log10(np.e) + np.zeros(R.shape)]\n\n        return mean, stddevs", "response": "This method calculates the mean and standard deviation of the logarithmic logarithm."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n\n        # assign constant\n        log10e = np.log10(np.e)\n\n        # Distance term\n        R = np.sqrt(dists.rjb ** 2 + 11.29 ** 2)\n        # Magnitude term\n        M = rup.mag - 6\n\n        # Site term only distinguishes between lava and ash;\n        # since ash sites have Vs30 in the range 60-200m/s,\n        # we use this upper value as class separator\n        S = np.zeros(R.shape)\n        S[sites.vs30 <= 200] = 1\n\n        # Mean ground motion (natural log)\n        # call super\n        mean, stddevs = super().get_mean_and_stddevs(sites, rup, dists,\n                                                     imt, stddev_types)\n\n        if rup.mag > 7. and rup.mag <= 7.7:\n            mean = (0.171 * (1 - M)) / log10e + mean\n\n        elif rup.mag > 7.7:\n            mean = (0.1512 + 0.387 * (1 - M)) / log10e + mean\n\n        # define natural log of SA 0.3 sec and 0.2 sec\n        if isinstance(imt, SA):\n            if imt.period == 0.3:\n                mean = np.log(2.2) + mean\n\n            if imt.period == 0.2:\n                mean = np.log(2.5) + mean\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsees :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n\n        C = self.COEFFS[imt]\n        imean = (self._compute_magnitude(rup, C) +\n                 self._compute_distance(rup, dists, C) +\n                 self._get_site_amplification(sites, C) +\n                 self._compute_forearc_backarc_term(C, sites, dists, rup))\n\n        istddevs = self._get_stddevs(C,\n                                     stddev_types,\n                                     num_sites=len(sites.vs30))\n\n        # Convert units to g,\n        # but only for PGA and SA (not PGV):\n        if imt.name in \"SA PGA\":\n            mean = np.log((10.0 ** (imean - 2.0)) / g)\n        else:\n            # PGV:\n            mean = np.log(10.0 ** imean)\n        # Return stddevs in terms of natural log scaling\n        stddevs = np.log(10.0 ** np.array(istddevs))\n        # mean_LogNaturale = np.log((10 ** mean) * 1e-2 / g)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the magnitude of the logarithmic entry table in equation 3 pag 1960", "response": "def _compute_magnitude(self, rup, C):\n        \"\"\"\n        equation 3 pag 1960:\n\n        c1 + c2(M-5.5)\n        \"\"\"\n        m_h = 5.5\n        return C['c1'] + (C['c2'] * (rup.mag - m_h))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_site_amplification(self, sites, C):\n        S, SS = self._get_site_type_dummy_variables(sites)\n\n        return (C['c61'] * S) + (C['c62'] * SS)", "response": "Compute the site amplification term for the given sites and category."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_site_type_dummy_variables(self, sites):\n        S = np.zeros(len(sites.vs30))\n        SS = np.zeros(len(sites.vs30))\n\n        # Class C; 180 m/s <= Vs30 <= 360 m/s.\n        idx = (sites.vs30 < 360.0)\n        SS[idx] = 1.0\n        # Class B; 360 m/s <= Vs30 <= 760 m/s. (NEHRP)\n        idx = (sites.vs30 >= 360.0) & (sites.vs30 < 760)\n        S[idx] = 1.0\n\n        return S, SS", "response": "Get the site type dummy variables for the given site set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the back - arc term of Equation 3. 1. 1.", "response": "def _compute_forearc_backarc_term(self, C, sites, dists, rup):\n        \"\"\"\n        Compute back-arc term of Equation 3\n\n        \"\"\"\n        # flag 1 (R < 335 & R >= 205)\n        flag1 = np.zeros(len(dists.rhypo))\n        ind1 = np.logical_and((dists.rhypo < 335), (dists.rhypo >= 205))\n        flag1[ind1] = 1.0\n        # flag 2 (R >= 335)\n        flag2 = np.zeros(len(dists.rhypo))\n        ind2 = (dists.rhypo >= 335)\n        flag2[ind2] = 1.0\n        # flag 3 (R < 240 & R >= 140)\n        flag3 = np.zeros(len(dists.rhypo))\n        ind3 = np.logical_and((dists.rhypo < 240), (dists.rhypo >= 140))\n        flag3[ind3] = 1.0\n        # flag 4 (R >= 240)\n        flag4 = np.zeros(len(dists.rhypo))\n        ind4 = (dists.rhypo >= 240)\n        flag4[ind4] = 1.0\n\n        A = flag1 * ((205 - dists.rhypo)/150) + flag2\n        B = flag3 * ((140 - dists.rhypo)/100) + flag4\n        if (rup.hypo_depth < 80):\n            FHR = A\n        else:\n            FHR = B\n\n        H0 = 100\n        # Heaviside function\n        if (rup.hypo_depth >= H0):\n            H = 1\n        else:\n            H = 0\n\n        # ARC = 0 for back-arc - ARC = 1 for forearc\n        ARC = np.zeros(len(sites.backarc))\n        idxarc = (sites.backarc == 1)\n        ARC[idxarc] = 1.0\n\n        return ((C['c41'] * (1 - ARC) * H) + (C['c42'] * (1 - ARC) * H * FHR) +\n                (C['c51'] * ARC * H) + (C['c52'] * ARC * H * FHR))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_data(self, amplification_group):\n        # Determine shape of the tables\n        n_levels = len(amplification_group)\n        # Checks the first group in the amplification group and returns the\n        # shape of the SA array - implicitly assumes the SA array in all\n        # amplification groups is the same shape\n        level = next(iter(amplification_group))\n        n_d, n_p, n_m = amplification_group[level][\"IMLs/SA\"].shape\n        assert n_d == len(self.distances), (n_d, len(self.distances))\n        assert n_m == len(self.magnitudes), (n_m, len(self.magnitudes))\n        # Instantiate the arrays with ones\n        self.mean = {\"SA\": numpy.ones([n_d, n_p, n_m, n_levels]),\n                     \"PGA\": numpy.ones([n_d, 1, n_m, n_levels]),\n                     \"PGV\": numpy.ones([n_d, 1, n_m, n_levels])}\n        self.sigma = {}\n        for stddev_type in [const.StdDev.TOTAL, const.StdDev.INTER_EVENT,\n                            const.StdDev.INTRA_EVENT]:\n            level = next(iter(amplification_group))\n            if stddev_type in amplification_group[level]:\n                self.sigma[stddev_type] = deepcopy(self.mean)\n\n        for iloc, (level, amp_model) in enumerate(amplification_group.items()):\n            if \"SA\" in amp_model[\"IMLs\"]:\n                if iloc == 0:\n                    self.periods = amp_model[\"IMLs/T\"][:]\n                else:\n                    assert numpy.allclose(self.periods, amp_model[\"IMLs/T\"][:])\n            for imt in [\"SA\", \"PGA\", \"PGV\"]:\n                if imt in amp_model[\"IMLs\"]:\n                    self.mean[imt][:, :, :, self.argidx[iloc]] = \\\n                        amp_model[\"IMLs/\" + imt][:]\n                    for stddev_type in self.sigma:\n                        self.sigma[stddev_type][imt][\n                            :, :, :, self.argidx[iloc]] = \\\n                            amp_model[\"/\".join([stddev_type, imt])][:]\n        self.shape = (n_d, n_p, n_m, n_levels)", "response": "Builds the numpy array tables from the hdf5 tables\n        and the amplification_group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the amplification factors for the given rupture and site.", "response": "def get_amplification_factors(self, imt, sctx, rctx, dists, stddev_types):\n        \"\"\"\n        Returns the amplification factors for the given rupture and site\n        conditions.\n\n        :param imt:\n            Intensity measure type as an instance of the :class:\n            `openquake.hazardlib.imt`\n        :param sctx:\n            SiteCollection instance\n        :param rctx:\n            Rupture instance\n        :param dists:\n            Source to site distances (km)\n        :param stddev_types:\n            List of required standard deviation types\n        :returns:\n            * mean_amp - Amplification factors applied to the median ground\n                         motion\n            * sigma_amps - List of modification factors applied to the\n                         standard deviations of ground motion\n        \"\"\"\n        dist_level_table = self.get_mean_table(imt, rctx)\n        sigma_tables = self.get_sigma_tables(imt, rctx, stddev_types)\n        mean_interpolator = interp1d(self.values,\n                                     numpy.log10(dist_level_table),\n                                     axis=1)\n        sigma_interpolators = [interp1d(self.values, sigma_table, axis=1)\n                               for sigma_table in sigma_tables]\n        if self.element == \"Rupture\":\n            mean_amp = 10.0 ** mean_interpolator(\n                getattr(rctx, self.parameter))[0] * numpy.ones_like(dists)\n            sigma_amps = []\n            for sig_interpolator in sigma_interpolators:\n                sigma_amps.append(sig_interpolator(\n                    getattr(rctx, self.parameter))[0] * numpy.ones_like(dists))\n        else:\n            mean_amp = 10.0 ** mean_interpolator(\n                getattr(sctx, self.parameter))[0, :]\n            sigma_amps = []\n            for sig_interpolator in sigma_interpolators:\n                sigma_amps.append(sig_interpolator(\n                    getattr(sctx, self.parameter))[0, :] *\n                    numpy.ones_like(dists))\n        return mean_amp, sigma_amps"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_table(self, imt, rctx):\n        # Levels by Distances\n        if imt.name in 'PGA PGV':\n            interpolator = interp1d(self.magnitudes,\n                                    numpy.log10(self.mean[imt.name]), axis=2)\n            output_table = 10.0 ** (\n                interpolator(rctx.mag).reshape(self.shape[0], self.shape[3]))\n        else:\n            # For spectral accelerations - need two step process\n            # Interpolate period - log-log space\n            interpolator = interp1d(numpy.log10(self.periods),\n                                    numpy.log10(self.mean[\"SA\"]),\n                                    axis=1)\n            period_table = interpolator(numpy.log10(imt.period))\n            # Interpolate magnitude - linear-log space\n            mag_interpolator = interp1d(self.magnitudes, period_table, axis=1)\n            output_table = 10.0 ** mag_interpolator(rctx.mag)\n        return output_table", "response": "Returns the amplification factors for the mean given the rupture and intensity measure type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sigma_tables(self, imt, rctx, stddev_types):\n        output_tables = []\n        for stddev_type in stddev_types:\n            # For PGA and PGV only needs to apply magnitude interpolation\n            if imt.name in 'PGA PGV':\n                interpolator = interp1d(self.magnitudes,\n                                        self.sigma[stddev_type][imt.name],\n                                        axis=2)\n                output_tables.append(\n                    interpolator(rctx.mag).reshape(self.shape[0],\n                                                   self.shape[3]))\n\n            else:\n                # For spectral accelerations - need two step process\n                # Interpolate period\n                interpolator = interp1d(numpy.log10(self.periods),\n                                        self.sigma[stddev_type][\"SA\"],\n                                        axis=1)\n                period_table = interpolator(numpy.log10(imt.period))\n                mag_interpolator = interp1d(self.magnitudes,\n                                            period_table,\n                                            axis=1)\n                output_tables.append(mag_interpolator(rctx.mag))\n        return output_tables", "response": "Returns the standard deviations for the resource class and rupture."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init(self, fle=None):\n        if fle is None:\n            fname = self.kwargs.get('gmpe_table', self.GMPE_TABLE)\n            if fname is None:\n                raise ValueError('You forgot to set GMPETable.GMPE_TABLE!')\n            elif os.path.isabs(fname):\n                self.GMPE_TABLE = fname\n            else:\n                # NB: (hackish) GMPE_DIR must be set externally\n                self.GMPE_TABLE = os.path.abspath(\n                    os.path.join(self.GMPE_DIR, fname))\n            fle = h5py.File(self.GMPE_TABLE, \"r\")\n        try:\n            # this is the format inside the datastore\n            self.distance_type = fle[\"distance_type\"].value\n        except KeyError:\n            # this is the original format outside the datastore\n            self.distance_type = decode(fle[\"Distances\"].attrs[\"metric\"])\n        self.REQUIRES_DISTANCES = set([self.distance_type])\n        # Load in magnitude\n        self.m_w = fle[\"Mw\"][:]\n        # Load in distances\n        self.distances = fle[\"Distances\"][:]\n        # Load intensity measure types and levels\n        self.imls = hdf_arrays_to_dict(fle[\"IMLs\"])\n        self.DEFINED_FOR_INTENSITY_MEASURE_TYPES = set(self._supported_imts())\n        if \"SA\" in self.imls and \"T\" not in self.imls:\n            raise ValueError(\"Spectral Acceleration must be accompanied by \"\n                             \"periods\")\n        # Get the standard deviations\n        self._setup_standard_deviations(fle)\n        if \"Amplification\" in fle:\n            self._setup_amplification(fle)", "response": "Initializes the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the standard deviations tables from HDF5 and stores them in memory", "response": "def _setup_standard_deviations(self, fle):\n        \"\"\"\n        Reads the standard deviation tables from hdf5 and stores them in\n        memory\n        :param fle:\n            HDF5 Tables as instance of :class:`h5py.File`\n        \"\"\"\n        # Load in total standard deviation\n        self.stddevs = {}\n        self.stddevs[const.StdDev.TOTAL] = hdf_arrays_to_dict(fle[\"Total\"])\n        # If other standard deviations\n        self.DEFINED_FOR_STANDARD_DEVIATION_TYPES = set(\n            self.DEFINED_FOR_STANDARD_DEVIATION_TYPES)\n        for stddev_type in [const.StdDev.INTER_EVENT,\n                            const.StdDev.INTRA_EVENT]:\n            if stddev_type in fle:\n                self.stddevs[stddev_type] = hdf_arrays_to_dict(\n                    fle[stddev_type])\n                self.DEFINED_FOR_STANDARD_DEVIATION_TYPES.add(stddev_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _supported_imts(self):\n        imt_list = []\n        for key in self.imls:\n            if \"SA\" in key:\n                imt_list.append(imt_module.SA)\n            elif key == \"T\":\n                continue\n            else:\n                try:\n                    factory = getattr(imt_module, key)\n                except Exception:\n                    continue\n                imt_list.append(factory)\n        return imt_list", "response": "Updates the list of supported IMTs from the tables\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mean_and_stddevs(self, sctx, rctx, dctx, imt, stddev_types):\n        # Return Distance Tables\n        imls = self._return_tables(rctx.mag, imt, \"IMLs\")\n        # Get distance vector for the given magnitude\n        idx = numpy.searchsorted(self.m_w, rctx.mag)\n        dists = self.distances[:, 0, idx - 1]\n        # Get mean and standard deviations\n        mean = self._get_mean(imls, dctx, dists)\n        stddevs = self._get_stddevs(dists, rctx.mag, dctx, imt, stddev_types)\n        if self.amplification:\n            # Apply amplification\n            mean_amp, sigma_amp = self.amplification.get_amplification_factors(\n                imt,\n                sctx,\n                rctx,\n                getattr(dctx, self.distance_type),\n                stddev_types)\n            mean = numpy.log(mean) + numpy.log(mean_amp)\n            for iloc in range(len(stddev_types)):\n                stddevs[iloc] *= sigma_amp[iloc]\n            return mean, stddevs\n        else:\n            return numpy.log(mean), stddevs", "response": "Returns the mean and standard deviations for the resource class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the mean intensity measure level vector from the tables", "response": "def _get_mean(self, data, dctx, dists):\n        \"\"\"\n        Returns the mean intensity measure level from the tables\n        :param data:\n            The intensity measure level vector for the given magnitude and IMT\n        :param key:\n            The distance type\n        :param distances:\n            The distance vector for the given magnitude and IMT\n        \"\"\"\n        # For values outside of the interpolation range use -999. to ensure\n        # value is identifiable and outside of potential real values\n        interpolator_mean = interp1d(dists, data,\n                                     bounds_error=False,\n                                     fill_value=-999.)\n        mean = interpolator_mean(getattr(dctx, self.distance_type))\n        # For those distances less than or equal to the shortest distance\n        # extrapolate the shortest distance value\n        mean[getattr(dctx, self.distance_type) < (dists[0] + 1.0E-3)] = data[0]\n        # For those distances significantly greater than the furthest distance\n        # set to 1E-20.\n        mean[getattr(dctx, self.distance_type) > (dists[-1] + 1.0E-3)] = 1E-20\n        # If any distance is between the final distance and a margin of 0.001\n        # km then assign to smallest distance\n        mean[mean < -1.] = data[-1]\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the total standard deviation of the intensity measure level given the distance vector and the rupture magnitude.", "response": "def _get_stddevs(self, dists, mag, dctx, imt, stddev_types):\n        \"\"\"\n        Returns the total standard deviation of the intensity measure level\n        from the tables.\n\n        :param fle:\n            HDF5 data stream as instance of :class:`h5py.File`\n        :param distances:\n            The distance vector for the given magnitude and IMT\n        :param key:\n            The distance type\n        :param mag:\n            The rupture magnitude\n        \"\"\"\n        stddevs = []\n        for stddev_type in stddev_types:\n            if stddev_type not in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES:\n                raise ValueError(\"Standard Deviation type %s not supported\"\n                                 % stddev_type)\n            sigma = self._return_tables(mag, imt, stddev_type)\n            interpolator_std = interp1d(dists, sigma,\n                                        bounds_error=False)\n            stddev = interpolator_std(getattr(dctx, self.distance_type))\n            stddev[getattr(dctx, self.distance_type) < dists[0]] = sigma[0]\n            stddev[getattr(dctx, self.distance_type) > dists[-1]] = sigma[-1]\n            stddevs.append(stddev)\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _return_tables(self, mag, imt, val_type):\n        if imt.name in 'PGA PGV':\n            # Get scalar imt\n            if val_type == \"IMLs\":\n                iml_table = self.imls[imt.name][:]\n            else:\n                iml_table = self.stddevs[val_type][imt.name][:]\n            n_d, n_s, n_m = iml_table.shape\n            iml_table = iml_table.reshape([n_d, n_m])\n        else:\n            if val_type == \"IMLs\":\n                periods = self.imls[\"T\"][:]\n                iml_table = self.imls[\"SA\"][:]\n            else:\n                periods = self.stddevs[val_type][\"T\"][:]\n                iml_table = self.stddevs[val_type][\"SA\"][:]\n\n            low_period = round(periods[0], 7)\n            high_period = round(periods[-1], 7)\n            if (round(imt.period, 7) < low_period) or (\n                    round(imt.period, 7) > high_period):\n                raise ValueError(\"Spectral period %.3f outside of valid range \"\n                                 \"(%.3f to %.3f)\" % (imt.period, periods[0],\n                                                     periods[-1]))\n            # Apply log-log interpolation for spectral period\n            interpolator = interp1d(numpy.log10(periods),\n                                    numpy.log10(iml_table),\n                                    axis=1)\n            iml_table = 10. ** interpolator(numpy.log10(imt.period))\n        return self.apply_magnitude_interpolation(mag, iml_table)", "response": "Returns the vector of ground motions or standard deviations for the specific magnitude and intensity measure type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninterpolates the tables to the required magnitude level and returns the result.", "response": "def apply_magnitude_interpolation(self, mag, iml_table):\n        \"\"\"\n        Interpolates the tables to the required magnitude level\n\n        :param float mag:\n            Magnitude\n        :param iml_table:\n            Intensity measure level table\n        \"\"\"\n        # do not allow \"mag\" to exceed maximum table magnitude\n        if mag > self.m_w[-1]:\n            mag = self.m_w[-1]\n\n        # Get magnitude values\n        if mag < self.m_w[0] or mag > self.m_w[-1]:\n            raise ValueError(\"Magnitude %.2f outside of supported range \"\n                             \"(%.2f to %.2f)\" % (mag,\n                                                 self.m_w[0],\n                                                 self.m_w[-1]))\n        # It is assumed that log10 of the spectral acceleration scales\n        # linearly (or approximately linearly) with magnitude\n        m_interpolator = interp1d(self.m_w, numpy.log10(iml_table), axis=1)\n        return 10.0 ** m_interpolator(mag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the mean and standard deviation of the resource table entry.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        # GMPE differentiates strike-slip, reverse and normal ruptures,\n        # but combines normal and strike-slip into one category. See page 180.\n        is_reverse = (45 <= rup.rake <= 135)\n\n        stddevs = [numpy.zeros_like(sites.vs30) for _ in stddev_types]\n        means = numpy.zeros_like(sites.vs30)\n\n        [rocks_i] = (sites.vs30 > self.ROCK_VS30).nonzero()\n        if len(rocks_i):\n            rrup = dists.rrup.take(rocks_i)\n            mean_rock = self._get_mean_rock(rup.mag, rup.rake, rrup,\n                                            is_reverse, imt)\n            means.put(rocks_i, mean_rock)\n            for stddev_arr in stddevs:\n                stddev_rock = self._get_stddev_rock(rup.mag, imt)\n                stddev_arr.put(rocks_i, stddev_rock)\n\n        [soils_i] = (sites.vs30 <= self.ROCK_VS30).nonzero()\n        if len(soils_i):\n            rrup = dists.rrup.take(soils_i)\n            mean_soil = self._get_mean_deep_soil(rup.mag, rup.rake, rrup,\n                                                 is_reverse, imt)\n            means.put(soils_i, mean_soil)\n            for stddev_arr in stddevs:\n                stddev_soil = self._get_stddev_deep_soil(rup.mag, imt)\n                stddev_arr.put(soils_i, stddev_soil)\n\n        return means, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate and return the mean intensity for deep soil sites.", "response": "def _get_mean_deep_soil(self, mag, rake, rrup, is_reverse, imt):\n        \"\"\"\n        Calculate and return the mean intensity for deep soil sites.\n\n        Implements an equation from table 4.\n        \"\"\"\n        if mag <= self.NEAR_FIELD_SATURATION_MAG:\n            c4 = self.COEFFS_SOIL_IMT_INDEPENDENT['c4lowmag']\n            c5 = self.COEFFS_SOIL_IMT_INDEPENDENT['c5lowmag']\n        else:\n            c4 = self.COEFFS_SOIL_IMT_INDEPENDENT['c4himag']\n            c5 = self.COEFFS_SOIL_IMT_INDEPENDENT['c5himag']\n        c2 = self.COEFFS_SOIL_IMT_INDEPENDENT['c2']\n        c3 = self.COEFFS_SOIL_IMT_INDEPENDENT['c3']\n        C = self.COEFFS_SOIL[imt]\n        if is_reverse:\n            c1 = self.COEFFS_SOIL_IMT_INDEPENDENT['c1r']\n            c6 = C['c6r']\n        else:\n            c1 = self.COEFFS_SOIL_IMT_INDEPENDENT['c1ss']\n            c6 = C['c6ss']\n        # clip mag if greater than 8.5. This is to avoid\n        # ValueError: negative number cannot be raised to a fractional power\n        mag = 8.5 if mag > 8.5 else mag\n        return (c1 + c2 * mag + c6 + C['c7'] * ((8.5 - mag) ** 2.5)\n                - c3 * numpy.log(rrup + c4 * numpy.exp(c5 * mag)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates and return the mean intensity for rock sites.", "response": "def _get_mean_rock(self, mag, _rake, rrup, is_reverse, imt):\n        \"\"\"\n        Calculate and return the mean intensity for rock sites.\n\n        Implements an equation from table 2.\n        \"\"\"\n        if mag <= self.NEAR_FIELD_SATURATION_MAG:\n            C = self.COEFFS_ROCK_LOWMAG[imt]\n        else:\n            C = self.COEFFS_ROCK_HIMAG[imt]\n        # clip mag if greater than 8.5. This is to avoid\n        # ValueError: negative number cannot be raised to a fractional power\n        mag = 8.5 if mag > 8.5 else mag\n        mean = (\n            C['c1'] + C['c2'] * mag + C['c3'] * ((8.5 - mag) ** 2.5)\n            + C['c4'] * numpy.log(rrup + numpy.exp(C['c5'] + C['c6'] * mag))\n            + C['c7'] * numpy.log(rrup + 2)\n        )\n        if is_reverse:\n            # footnote in table 2 says that for reverse ruptures\n            # the mean amplitude value should be multiplied by 1.2\n            mean += 0.1823215567939546  # == log(1.2)\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_stddev_rock(self, mag, imt):\n        C = self.COEFFS_ROCK_STDDERR[imt]\n        if mag > C['maxmag']:\n            return C['maxsigma']\n        else:\n            return C['sigma0'] + C['magfactor'] * mag", "response": "Calculate and return total standard deviation for rock sites."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates and return total standard deviation for deep soil sites.", "response": "def _get_stddev_deep_soil(self, mag, imt):\n        \"\"\"\n        Calculate and return total standard deviation for deep soil sites.\n\n        Implements formulae from the last column of table 4.\n        \"\"\"\n        # footnote from table 4 says that stderr for magnitudes over 7\n        # is equal to one of magnitude 7.\n        if mag > 7:\n            mag = 7\n        C = self.COEFFS_SOIL[imt]\n        return C['sigma0'] + C['magfactor'] * mag"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef zip(what, archive_zip='', risk_file=''):\n    if os.path.isdir(what):\n        oqzip.zip_all(what)\n    elif what.endswith('.xml') and '<logicTree' in open(what).read(512):\n        # hack to see if the NRML file is of kind logicTree\n        oqzip.zip_source_model(what, archive_zip)\n    elif what.endswith('.xml') and '<exposureModel' in open(what).read(512):\n        # hack to see if the NRML file is of kind exposureModel\n        oqzip.zip_exposure(what, archive_zip)\n    elif what.endswith('.ini'):  # a job.ini\n        oqzip.zip_job(what, archive_zip, risk_file)\n    else:\n        sys.exit('Cannot zip %s' % what)", "response": "Zip a file into an archive one or two job. ini files with all related files\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reduce(fname, reduction_factor):\n    if fname.endswith('.csv'):\n        with open(fname) as f:\n            line = f.readline()  # read the first line\n            if csv.Sniffer().has_header(line):\n                header = line\n                all_lines = f.readlines()\n            else:\n                header = None\n                f.seek(0)\n                all_lines = f.readlines()\n        lines = general.random_filter(all_lines, reduction_factor)\n        shutil.copy(fname, fname + '.bak')\n        print('Copied the original file in %s.bak' % fname)\n        _save_csv(fname, lines, header)\n        print('Extracted %d lines out of %d' % (len(lines), len(all_lines)))\n        return\n    elif fname.endswith('.npy'):\n        array = numpy.load(fname)\n        shutil.copy(fname, fname + '.bak')\n        print('Copied the original file in %s.bak' % fname)\n        arr = numpy.array(general.random_filter(array, reduction_factor))\n        numpy.save(fname, arr)\n        print('Extracted %d rows out of %d' % (len(arr), len(array)))\n        return\n    node = nrml.read(fname)\n    model = node[0]\n    if model.tag.endswith('exposureModel'):\n        total = len(model.assets)\n        model.assets.nodes = general.random_filter(\n            model.assets, reduction_factor)\n        num_nodes = len(model.assets)\n    elif model.tag.endswith('siteModel'):\n        total = len(model)\n        model.nodes = general.random_filter(model, reduction_factor)\n        num_nodes = len(model)\n    elif model.tag.endswith('sourceModel'):\n        reduce_source_model(fname, reduction_factor)\n        return\n    elif model.tag.endswith('logicTree'):\n        for smpath in logictree.collect_info(fname).smpaths:\n            reduce_source_model(smpath, reduction_factor)\n        return\n    else:\n        raise RuntimeError('Unknown model tag: %s' % model.tag)\n    save_bak(fname, node, num_nodes, total)", "response": "This function creates a submodel from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the activity rate of the fault and returns the minimum magnitude bin width and occurrence rate.", "response": "def get_mfd(self, slip, area, shear_modulus=30.0):\n        '''\n        Calculates activity rate on the fault\n\n        :param float slip:\n            Slip rate in mm/yr\n\n        :param area:\n            Width of the fault (km)\n\n        :param float shear_modulus:\n            Shear modulus of the fault (GPa)\n\n        :returns:\n            * Minimum Magnitude (float)\n            * Bin width (float)\n            * Occurrence Rates (numpy.ndarray)\n        '''\n        # Working in Nm so convert:  shear_modulus - GPa -> Nm\n        # area - km ** 2. -> m ** 2.\n        # slip - mm/yr -> m/yr\n        moment_rate = (shear_modulus * 1.E9) * (area * 1.E6) * (slip / 1000.)\n        moment_mag = _scale_moment(self.mmax, in_nm=True)\n        beta = self.b_value * log(10.)\n        mag = np.arange(self.mmin - (self.bin_width / 2.),\n                        self.mmax + self.bin_width,\n                        self.bin_width)\n        if self.b_value > 1.5:\n            print('b-value larger than 1.5 will produce invalid results in '\n                  'Anderson & Luco models')\n            self.occurrence_rate = np.nan * np.ones(len(mag) - 1)\n            return self.mmin, self.bin_width, self.occurrence_rate\n\n        self.occurrence_rate = np.zeros(len(mag) - 1, dtype=float)\n        for ival in range(0, len(mag) - 1):\n            self.occurrence_rate[ival] = (\n                self.cumulative_value(mag[ival], moment_rate, beta, moment_mag)\n                - self.cumulative_value(\n                    mag[ival + 1], moment_rate, beta, moment_mag))\n\n        return self.mmin, self.bin_width, self.occurrence_rate"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the activity rate of the fault and returns the minimum magnitude bin width and occurrence rate.", "response": "def get_mfd(self, slip, area, shear_modulus=30.0):\n        '''\n        Calculates activity rate on the fault\n\n        :param float slip:\n            Slip rate in mm/yr\n\n        :param area:\n            Area of the fault (km)\n\n        :param float shear_modulus:\n            Shear modulus of the fault (GPa)\n\n        :returns:\n            * Minimum Magnitude (float)\n            * Bin width (float)\n            * Occurrence Rates (numpy.ndarray)\n\n        Behavioural Notes: To use the openquake.hazardlib implementation the\n        magnitudes returned will be the mid_point of the bins and not the\n        original edge points. The minimum magnitude is update to reflect this!\n        '''\n        # Calculate moment rate in N-m / year\n        moment_rate = (shear_modulus * 1.E9) * (area * 1.E6) * (slip / 1000.)\n        # Get Youngs & Coppersmith rate from\n        # youngs_coppersmith.YoungsCoppersmith1985MFD.from_total_moment_rate\n        self.model = YoungsCoppersmith1985MFD.from_total_moment_rate(\n            self.mmin - (self.bin_width / 2.),\n            self.b_value,\n            self.mmax - 0.25,\n            moment_rate,\n            self.bin_width)\n        temp_data = self.model.get_annual_occurrence_rates()\n        self.occurrence_rate = np.array([value[1] for value in temp_data])\n        self.mmin = np.min(np.array([value[0] for value in temp_data]))\n        return self.mmin, self.bin_width, self.occurrence_rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        mean = self._get_mean(sites, rup, dists, imt, stddev_types)\n        stddevs = [np.ones(len(dists.rrup))*get_sigma(imt)]\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_mean(self, sites, rup, dists, imt, stddev_types):\n        # Zhao et al. 2006 - Vs30 + Rrup\n        mean_zh06, stds1 = super().get_mean_and_stddevs(sites, rup, dists, imt,\n                                                        stddev_types)\n        #\n        # Atkinson and Macias (2009) - Rrup\n        gmpe = AtkinsonMacias2009()\n        mean_am09, stds2 = gmpe.get_mean_and_stddevs(sites, rup, dists, imt,\n                                                     stddev_types)\n        #\n        # Abrahamson et al. (2015) - Rrup + vs30 + backarc\n        gmpe = AbrahamsonEtAl2015SInter()\n        mean_ab15, stds3 = gmpe.get_mean_and_stddevs(sites, rup, dists, imt,\n                                                     stddev_types)\n        #\n        # Ghofrani and Atkinson (2014) - Rrup + vs30\n        gmpe = GhofraniAtkinson2014()\n        mean_ga14, stds4 = gmpe.get_mean_and_stddevs(sites, rup, dists, imt,\n                                                     stddev_types)\n        # Computing adjusted mean and stds\n        cff = self.SITE_COEFFS[imt]\n        mean_adj = (np.log(np.exp(mean_zh06)*cff['mf'])*0.1 +\n                    mean_am09*0.5 + mean_ab15*0.2 +\n                    np.log(np.exp(mean_ga14)*cff['mf'])*0.2)\n        return mean_adj", "response": "Compute and return the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_turning_points(mesh, tol=1.0):\n    assert isinstance(mesh, RectangularMesh)\n    azimuths = geodetic.azimuth(mesh.lons[0, :-1], mesh.lats[0, :-1],\n                                mesh.lons[0, 1:], mesh.lats[0, 1:])\n    naz = len(azimuths)\n    azim = azimuths[0]\n    # Retain initial point\n    idx = [0]\n    for i in range(1, naz):\n        if numpy.fabs(azimuths[i] - azim) > tol:\n            idx.append(i)\n            azim = azimuths[i]\n    # Add on last point - if not already in the set\n    if idx[-1] != mesh.lons.shape[1] - 1:\n        idx.append(mesh.lons.shape[1] - 1)\n    return numpy.array(idx)", "response": "Find the turning points in a rectangular mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef downsample_mesh(mesh, tol=1.0):\n    idx = _find_turning_points(mesh, tol)\n    if mesh.depths is not None:\n        return RectangularMesh(lons=mesh.lons[:, idx],\n                               lats=mesh.lats[:, idx],\n                               depths=mesh.depths[:, idx])\n    else:\n        return RectangularMesh(lons=mesh.lons[:, idx],\n                               lats=mesh.lats[:, idx])", "response": "Downsamples a mesh at a lower resolution"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the minimum distance between each point of a mesh and the great circle arcs perpendicular to the average strike direction of the fault trace and passing through the end - points of the fault trace and the end - points of the fault trace.", "response": "def get_ry0_distance(self, mesh):\n        \"\"\"\n        Compute the minimum distance between each point of a mesh and the great\n        circle arcs perpendicular to the average strike direction of the\n        fault trace and passing through the end-points of the trace.\n\n        :param mesh:\n            :class:`~openquake.hazardlib.geo.mesh.Mesh` of points to calculate\n            Ry0-distance to.\n        :returns:\n            Numpy array of distances in km.\n        \"\"\"\n        # This computes ry0 by using an average strike direction\n        top_edge = self.mesh[0:1]\n        mean_strike = self.get_strike()\n\n        dst1 = geodetic.distance_to_arc(top_edge.lons[0, 0],\n                                        top_edge.lats[0, 0],\n                                        (mean_strike + 90.) % 360,\n                                        mesh.lons, mesh.lats)\n\n        dst2 = geodetic.distance_to_arc(top_edge.lons[0, -1],\n                                        top_edge.lats[0, -1],\n                                        (mean_strike + 90.) % 360,\n                                        mesh.lons, mesh.lats)\n        # Find the points on the rupture\n\n        # Get the shortest distance from the two lines\n        idx = numpy.sign(dst1) == numpy.sign(dst2)\n        dst = numpy.zeros_like(dst1)\n        dst[idx] = numpy.fmin(numpy.abs(dst1[idx]), numpy.abs(dst2[idx]))\n\n        return dst"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_rx_distance(self, mesh):\n        top_edge = self.mesh[0:1]\n\n        dists = []\n        if top_edge.lons.shape[1] < 3:\n\n            i = 0\n            p1 = Point(\n                top_edge.lons[0, i],\n                top_edge.lats[0, i],\n                top_edge.depths[0, i]\n            )\n            p2 = Point(\n                top_edge.lons[0, i + 1], top_edge.lats[0, i + 1],\n                top_edge.depths[0, i + 1]\n            )\n            azimuth = p1.azimuth(p2)\n            dists.append(\n                geodetic.distance_to_arc(\n                    p1.longitude, p1.latitude, azimuth,\n                    mesh.lons, mesh.lats\n                )\n            )\n\n        else:\n\n            for i in range(top_edge.lons.shape[1] - 1):\n                p1 = Point(\n                    top_edge.lons[0, i],\n                    top_edge.lats[0, i],\n                    top_edge.depths[0, i]\n                )\n                p2 = Point(\n                    top_edge.lons[0, i + 1],\n                    top_edge.lats[0, i + 1],\n                    top_edge.depths[0, i + 1]\n                )\n                # Swapping\n                if i == 0:\n                    pt = p1\n                    p1 = p2\n                    p2 = pt\n\n                # Computing azimuth and distance\n                if i == 0 or i == top_edge.lons.shape[1] - 2:\n                    azimuth = p1.azimuth(p2)\n                    tmp = geodetic.distance_to_semi_arc(p1.longitude,\n                                                        p1.latitude,\n                                                        azimuth,\n                                                        mesh.lons, mesh.lats)\n                else:\n                    tmp = geodetic.min_distance_to_segment(\n                        numpy.array([p1.longitude, p2.longitude]),\n                        numpy.array([p1.latitude, p2.latitude]),\n                        mesh.lons, mesh.lats)\n                # Correcting the sign of the distance\n                if i == 0:\n                    tmp *= -1\n                dists.append(tmp)\n\n        # Computing distances\n        dists = numpy.array(dists)\n        iii = abs(dists).argmin(axis=0)\n        dst = dists[iii, list(range(dists.shape[1]))]\n\n        return dst", "response": "Calculates the distance between each point of mesh and surface s great circle and the rupture strike."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_top_edge_depth(self):\n        top_edge = self.mesh[0:1]\n        if top_edge.depths is None:\n            return 0\n        else:\n            return numpy.min(top_edge.depths)", "response": "Returns the minimum depth of the surface s top edge in km."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_area(self):\n        mesh = self.mesh\n        _, _, _, area = mesh.get_cell_dimensions()\n\n        return numpy.sum(area)", "response": "Compute the area of the cell values in the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing surface bounding box from the surface mesh representation. That is extracts longitudes and latitudes of mesh points and calls utils. get_spherical_bounding_box.", "response": "def get_bounding_box(self):\n        \"\"\"\n        Compute surface bounding box from surface mesh representation. That is\n        extract longitudes and latitudes of mesh points and calls:\n        :meth:`openquake.hazardlib.geo.utils.get_spherical_bounding_box`\n\n        :return:\n            A tuple of four items. These items represent western, eastern,\n            northern and southern borders of the bounding box respectively.\n            Values are floats in decimal degrees.\n        \"\"\"\n        mesh = self.mesh\n        return utils.get_spherical_bounding_box(mesh.lons, mesh.lats)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the boundaries of the multiplanar surface.", "response": "def get_surface_boundaries(self):\n        \"\"\"\n        Returns the boundaries in the same format as a multiplanar\n        surface, with two one-element lists of lons and lats\n        \"\"\"\n        mesh = self.mesh\n        lons = numpy.concatenate((mesh.lons[0, :],\n                                  mesh.lons[1:, -1],\n                                  mesh.lons[-1, :-1][::-1],\n                                  mesh.lons[:-1, 0][::-1]))\n        lats = numpy.concatenate((mesh.lats[0, :],\n                                  mesh.lats[1:, -1],\n                                  mesh.lats[-1, :-1][::-1],\n                                  mesh.lats[:-1, 0][::-1]))\n        return [lons], [lats]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_azimuth(self, mesh):\n        # Get info about the rupture\n        strike = self.get_strike()\n        hypocenter = self.get_middle_point()\n        # This is the azimuth from the north of each point Vs. the middle of\n        # the rupture\n        azim = geodetic.azimuth(hypocenter.longitude, hypocenter.latitude,\n                                mesh.lons, mesh.lats)\n        # Compute the azimuth from the fault strike\n        rel_azi = (azim - strike) % 360\n        return rel_azi", "response": "This method computes the azimuth of a set of points in a rupture."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_avg_losses(ekey, dstore):\n    dskey = ekey[0]\n    oq = dstore['oqparam']\n    dt = oq.loss_dt()\n    name, value, tags = _get_data(dstore, dskey, oq.hazard_stats().items())\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    assets = get_assets(dstore)\n    for tag, values in zip(tags, value.transpose(1, 0, 2)):\n        dest = dstore.build_fname(name, tag, 'csv')\n        array = numpy.zeros(len(values), dt)\n        for l, lt in enumerate(dt.names):\n            array[lt] = values[:, l]\n        writer.save(compose_arrays(assets, array), dest)\n    return writer.getsaved()", "response": "Exports the average losses of the hazard stats for each class in the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport the aggregation losses for the current hazard.", "response": "def export_agg_losses(ekey, dstore):\n    \"\"\"\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    dskey = ekey[0]\n    oq = dstore['oqparam']\n    dt = oq.loss_dt()\n    name, value, tags = _get_data(dstore, dskey, oq.hazard_stats().items())\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    expvalue = dstore['exposed_value'].value  # shape (T1, T2, ..., L)\n    tagcol = dstore['assetcol/tagcol']\n    tagnames = tuple(dstore['oqparam'].aggregate_by)\n    header = ('loss_type',) + tagnames + (\n        'loss_value', 'exposed_value', 'loss_ratio')\n    for r, tag in enumerate(tags):\n        rows = []\n        for multi_idx, loss in numpy.ndenumerate(value[:, r]):\n            l, *tagidxs = multi_idx\n            evalue = expvalue[tuple(tagidxs) + (l,)]\n            row = tagcol.get_tagvalues(tagnames, tagidxs) + (\n                loss, evalue, loss / evalue)\n            rows.append((dt.names[l],) + row)\n        dest = dstore.build_fname(name, tag, 'csv')\n        writer.save(rows, dest, header)\n    return writer.getsaved()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_avg_losses_ebrisk(ekey, dstore):\n    name = ekey[0]\n    oq = dstore['oqparam']\n    dt = oq.loss_dt()\n    value = dstore[name].value  # shape (A, L)\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    assets = get_assets(dstore)\n    dest = dstore.build_fname(name, 'mean', 'csv')\n    array = numpy.zeros(len(value), dt)\n    for l, lt in enumerate(dt.names):\n        array[lt] = value[:, l]\n    writer.save(compose_arrays(assets, array), dest)\n    return writer.getsaved()", "response": "Exports the average losses of the oq in the datastore as a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the losses of the last rlz that have been observed by an asset.", "response": "def export_losses_by_asset(ekey, dstore):\n    \"\"\"\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    loss_dt = dstore['oqparam'].loss_dt(stat_dt)\n    losses_by_asset = dstore[ekey[0]].value\n    rlzs = dstore['csm_info'].get_rlzs_assoc().realizations\n    assets = get_assets(dstore)\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    for rlz in rlzs:\n        losses = losses_by_asset[:, rlz.ordinal]\n        dest = dstore.build_fname('losses_by_asset', rlz, 'csv')\n        data = compose_arrays(assets, losses.copy().view(loss_dt)[:, 0])\n        writer.save(data, dest)\n    return writer.getsaved()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_losses_by_event(ekey, dstore):\n    oq = dstore['oqparam']\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    dest = dstore.build_fname('losses_by_event', '', 'csv')\n    if oq.calculation_mode.startswith('scenario'):\n        dtlist = [('eid', U64)] + oq.loss_dt_list()\n        arr = dstore['losses_by_event'].value[['eid', 'loss']]\n        writer.save(arr.copy().view(dtlist), dest)\n    elif oq.calculation_mode == 'ebrisk':\n        tagcol = dstore['assetcol/tagcol']\n        lbe = dstore['losses_by_event'].value\n        lbe.sort(order='eid')\n        dic = dict(tagnames=['event_id', 'loss_type'] + oq.aggregate_by)\n        for tagname in oq.aggregate_by:\n            dic[tagname] = getattr(tagcol, tagname)\n        dic['event_id'] = ['?'] + list(lbe['eid'])\n        dic['loss_type'] = ('?',) + oq.loss_dt().names\n        aw = hdf5.ArrayWrapper(lbe['loss'], dic)  # shape (E, L, T...)\n        writer.save(aw.to_table(), dest)\n    else:\n        dtlist = [('event_id', U64), ('rup_id', U32), ('year', U32)] + \\\n                 oq.loss_dt_list()\n        eids = dstore['losses_by_event']['eid']\n        year_of = year_dict(dstore['events']['eid'],\n                            oq.investigation_time, oq.ses_seed)\n        arr = numpy.zeros(len(dstore['losses_by_event']), dtlist)\n        arr['event_id'] = eids\n        arr['rup_id'] = arr['event_id'] / TWO32\n        arr['year'] = [year_of[eid] for eid in eids]\n        loss = dstore['losses_by_event']['loss'].T  # shape (L, E)\n        for losses, loss_type in zip(loss, oq.loss_dt().names):\n            arr[loss_type] = losses\n        writer.save(arr, dest)\n    return writer.getsaved()", "response": "Export the losses for the given event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting the losses of the last batch of losses by asset as a NumPy array.", "response": "def export_losses_by_asset_npz(ekey, dstore):\n    \"\"\"\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    fname = dstore.export_path('%s.%s' % ekey)\n    savez(fname, **dict(extract(dstore, 'losses_by_asset')))\n    return [fname]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting the max loss ruptures for each rupture type in the datastore.", "response": "def export_maxloss_ruptures(ekey, dstore):\n    \"\"\"\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    oq = dstore['oqparam']\n    mesh = get_mesh(dstore['sitecol'])\n    rlzs_by_gsim = dstore['csm_info'].get_rlzs_by_gsim_grp()\n    num_ses = oq.ses_per_logic_tree_path\n    fnames = []\n    for loss_type in oq.loss_dt().names:\n        ebr = getters.get_maxloss_rupture(dstore, loss_type)\n        root = hazard_writers.rupture_to_element(\n            ebr.export(mesh, rlzs_by_gsim[ebr.grp_id], num_ses))\n        dest = dstore.export_path('rupture-%s.xml' % loss_type)\n        with open(dest, 'wb') as fh:\n            nrml.write(list(root), fh)\n        fnames.append(dest)\n    return fnames"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_agg_losses_ebr(ekey, dstore):\n    if 'ruptures' not in dstore:\n        logging.warning('There are no ruptures in the datastore')\n        return []\n    name, ext = export.keyfunc(ekey)\n    agg_losses = dstore['losses_by_event']\n    has_rup_data = 'ruptures' in dstore\n    extra_list = [('magnitude', F32),\n                  ('centroid_lon', F32),\n                  ('centroid_lat', F32),\n                  ('centroid_depth', F32)] if has_rup_data else []\n    oq = dstore['oqparam']\n    lti = oq.lti\n    dtlist = ([('event_id', U64), ('rup_id', U32), ('year', U32)]\n              + extra_list + oq.loss_dt_list())\n    elt_dt = numpy.dtype(dtlist)\n    elt = numpy.zeros(len(agg_losses), elt_dt)\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    events = dstore['events'].value\n    events_by_rupid = collections.defaultdict(list)\n    for event in events:\n        rupid = event['eid'] // TWO32\n        events_by_rupid[rupid].append(event)\n    year_of = year_dict(events['eid'], oq.investigation_time, oq.ses_seed)\n    rup_data = {}\n    event_by_eid = {}  # eid -> event\n    # populate rup_data and event_by_eid\n    # TODO: avoid reading the events twice\n    for rgetter in getters.gen_rupture_getters(dstore):\n        ruptures = rgetter.get_ruptures()\n        for ebr in ruptures:\n            for event in events_by_rupid[ebr.serial]:\n                event_by_eid[event['eid']] = event\n        if has_rup_data:\n            rup_data.update(get_rup_data(ruptures))\n    for r, row in enumerate(agg_losses):\n        rec = elt[r]\n        event = event_by_eid[row['eid']]\n        rec['event_id'] = eid = event['eid']\n        rec['year'] = year_of[eid]\n        if rup_data:\n            rec['rup_id'] = rup_id = event['eid'] // TWO32\n            (rec['magnitude'], rec['centroid_lon'], rec['centroid_lat'],\n             rec['centroid_depth']) = rup_data[rup_id]\n        for lt, i in lti.items():\n            rec[lt] = row['loss'][i]\n    elt.sort(order=['year', 'event_id'])\n    dest = dstore.build_fname('elt', '', 'csv')\n    writer.save(elt, dest)\n    return writer.getsaved()", "response": "Export the aggregation losses for each event in the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting the damage of the events in the next iteration of the next iteration.", "response": "def export_dmg_by_event(ekey, dstore):\n    \"\"\"\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    damage_dt = build_damage_dt(dstore, mean_std=False)\n    dt_list = [('event_id', numpy.uint64), ('rlzi', numpy.uint16)] + [\n        (f, damage_dt.fields[f][0]) for f in damage_dt.names]\n    all_losses = dstore[ekey[0]].value  # shape (E, R, LI)\n    events_by_rlz = group_array(dstore['events'], 'rlz')\n    rlzs = dstore['csm_info'].get_rlzs_assoc().realizations\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    fname = dstore.build_fname('dmg_by_event', '', 'csv')\n    writer.save(numpy.zeros(0, dt_list), fname)\n    with open(fname, 'ab') as dest:\n        for rlz in rlzs:\n            data = all_losses[:, rlz.ordinal].copy().view(damage_dt)  # shape E\n            arr = numpy.zeros(len(data), dt_list)\n            arr['event_id'] = events_by_rlz[rlz.ordinal]['eid']\n            arr['rlzi'] = rlz.ordinal\n            for field in damage_dt.names:\n                arr[field] = data[field].squeeze()\n            writer.save_block(arr, dest)\n    return [fname]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the loss maps for the current risk node in a datastore.", "response": "def get_loss_maps(dstore, kind):\n    \"\"\"\n    :param dstore: a DataStore instance\n    :param kind: 'rlzs' or 'stats'\n    \"\"\"\n    oq = dstore['oqparam']\n    name = 'loss_maps-%s' % kind\n    if name in dstore:  # event_based risk\n        return _to_loss_maps(dstore[name].value, oq.loss_maps_dt())\n    name = 'loss_curves-%s' % kind\n    if name in dstore:  # classical_risk\n        # the loss maps are built on the fly from the loss curves\n        loss_curves = dstore[name]\n        loss_maps = scientific.broadcast(\n            scientific.loss_maps, loss_curves, oq.conditional_loss_poes)\n        return loss_maps\n    raise KeyError('loss_maps/loss_curves missing in %s' % dstore)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary with the paths of the logic tree realization.", "response": "def get_paths(rlz):\n    \"\"\"\n    :param rlz:\n        a logic tree realization (composite or simple)\n    :returns:\n        a dict {'source_model_tree_path': string, 'gsim_tree_path': string}\n    \"\"\"\n    dic = {}\n    if hasattr(rlz, 'sm_lt_path'):  # composite realization\n        dic['source_model_tree_path'] = '_'.join(rlz.sm_lt_path)\n        dic['gsim_tree_path'] = '_'.join(rlz.gsim_lt_path)\n    else:  # simple GSIM realization\n        dic['source_model_tree_path'] = ''\n        dic['gsim_tree_path'] = '_'.join(rlz.lt_path)\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_by_tag_csv(ekey, dstore):\n    token, tag = ekey[0].split('/')\n    data = extract(dstore, token + '/' + tag)\n    fnames = []\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    for stat, arr in data:\n        tup = (ekey[0].replace('/', '-'), stat, ekey[1])\n        path = '%s-%s.%s' % tup\n        fname = dstore.export_path(path)\n        writer.save(arr, fname)\n        fnames.append(fname)\n    return fnames", "response": "Export the data for the current language of the item in a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports the aggregate of the tokens in the datastore as a CSV file.", "response": "def export_aggregate_by_csv(ekey, dstore):\n    \"\"\"\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    token, what = ekey[0].split('/', 1)\n    aw = extract(dstore, 'aggregate/' + what)\n    fnames = []\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    path = '%s.%s' % (sanitize(ekey[0]), ekey[1])\n    fname = dstore.export_path(path)\n    writer.save(aw.to_table(), fname)\n    fnames.append(fname)\n    return fnames"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_asset_risk_csv(ekey, dstore):\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    path = '%s.%s' % (sanitize(ekey[0]), ekey[1])\n    fname = dstore.export_path(path)\n    md = extract(dstore, 'exposure_metadata')\n    tostr = {'taxonomy': md.taxonomy}\n    for tagname in md.tagnames:\n        tostr[tagname] = getattr(md, tagname)\n    arr = extract(dstore, 'asset_risk').array\n    arefs = dstore['assetcol/asset_refs'].value\n    rows = []\n    lossnames = sorted(name for name in arr.dtype.names if 'loss' in name)\n    perilnames = sorted(name for name in arr.dtype.names\n                        if name.upper() == name)\n    expnames = [name for name in arr.dtype.names if name not in md.tagnames\n                and 'loss' not in name and name not in perilnames\n                and name not in 'lon lat']\n    colnames = (['asset_ref'] + sorted(md.tagnames) + ['lon', 'lat'] +\n                expnames + perilnames + lossnames)\n    # sanity check\n    assert len(colnames) == len(arr.dtype.names) + 1\n    for aref, rec in zip(arefs, arr):\n        row = [aref]\n        for name in colnames[1:]:\n            value = rec[name]\n            try:\n                row.append('\"%s\"' % tostr[name][value])\n            except KeyError:\n                row.append(value)\n        rows.append(row)\n    writer.save(rows, fname, colnames)\n    return [fname]", "response": "Export the asset_risk as a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the aggregation risk as a CSV file", "response": "def export_agg_risk_csv(ekey, dstore):\n    \"\"\"\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\n    :param dstore: datastore object\n    \"\"\"\n    writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n    path = '%s.%s' % (sanitize(ekey[0]), ekey[1])\n    fname = dstore.export_path(path)\n    writer.save(dstore['agg_risk'].value, fname)\n    return [fname]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the log of the given calculation ID from the WebUI", "response": "def viewlog(calc_id, host='localhost', port=8000):\n    \"\"\"\n    Extract the log of the given calculation ID from the WebUI\n    \"\"\"\n    base_url = 'http://%s:%s/v1/calc/' % (host, port)\n    start = 0\n    psize = 10  # page size\n    try:\n        while True:\n            url = base_url + '%d/log/%d:%d' % (calc_id, start, start + psize)\n            rows = json.load(urlopen(url))\n            for row in rows:\n                print(' '.join(row))\n            start += len(rows)\n            time.sleep(1)\n    except:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # Check that the GSIM supports the standard deviations requested\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n        coe = self.COEFFS[imt]\n        mean = (self._get_magnitude_scaling(coe, rup.mag) +\n                self._get_distance_scaling(coe, dists.rhypo) +\n                self._get_azimuth_correction(coe, dists.azimuth))\n        # Convert to g\n        if imt.name in \"SA PGA\":\n            mean = np.log(np.exp(mean) / (100.0 * g))\n        # Compute std\n        stddevs = self._compute_std(coe, stddev_types, dists.azimuth.shape)\n        return mean, stddevs", "response": "Returns the mean and standard deviation for a given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_distance_scaling(self, coe, rhypo):\n        return coe[\"a3\"] * np.log(rhypo) + coe[\"a4\"] * rhypo", "response": "Returns the distance scaling term\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_azimuth_correction(self, coe, azimuth):\n        term1 = abs(np.cos(np.radians(2.*azimuth)))\n        term2 = abs(np.sin(np.radians(2.*azimuth)))*coe['a5']\n        return np.log(np.max(np.hstack((term1, term2))))", "response": "This function returns the azimuth correction of the current species."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the pickled sizes of an object and its direct attributes.", "response": "def get_pickled_sizes(obj):\n    \"\"\"\n    Return the pickled sizes of an object and its direct attributes,\n    ordered by decreasing size. Here is an example:\n\n    >> total_size, partial_sizes = get_pickled_sizes(Monitor(''))\n    >> total_size\n    345\n    >> partial_sizes\n    [('_procs', 214), ('exc', 4), ('mem', 4), ('start_time', 4),\n    ('_start_time', 4), ('duration', 4)]\n\n    Notice that the sizes depend on the operating system and the machine.\n    \"\"\"\n    sizes = []\n    attrs = getattr(obj, '__dict__',  {})\n    for name, value in attrs.items():\n        sizes.append((name, len(Pickled(value))))\n    return len(Pickled(obj)), sorted(\n        sizes, key=lambda pair: pair[1], reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pickle_sequence(objects):\n    cache = {}\n    out = []\n    for obj in objects:\n        obj_id = id(obj)\n        if obj_id not in cache:\n            if isinstance(obj, Pickled):  # already pickled\n                cache[obj_id] = obj\n            else:  # pickle the object\n                cache[obj_id] = Pickled(obj)\n        out.append(cache[obj_id])\n    return out", "response": "Convert an iterable of objects into a list of pickled objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_mem_usage(soft_percent=None, hard_percent=None):\n    soft_percent = soft_percent or config.memory.soft_mem_limit\n    hard_percent = hard_percent or config.memory.hard_mem_limit\n    used_mem_percent = psutil.virtual_memory().percent\n    if used_mem_percent > hard_percent:\n        raise MemoryError('Using more memory than allowed by configuration '\n                          '(Used: %d%% / Allowed: %d%%)! Shutting down.' %\n                          (used_mem_percent, hard_percent))\n    elif used_mem_percent > soft_percent:\n        msg = 'Using over %d%% of the memory in %s!'\n        return msg % (used_mem_percent, socket.gethostname())", "response": "Check if we are running out of memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safely_call(func, args, task_no=0, mon=dummy_mon):\n    isgenfunc = inspect.isgeneratorfunction(func)\n    if hasattr(args[0], 'unpickle'):\n        # args is a list of Pickled objects\n        args = [a.unpickle() for a in args]\n    if mon is dummy_mon:  # in the DbServer\n        assert not isgenfunc, func\n        return Result.new(func, args, mon)\n\n    mon = mon.new(operation='total ' + func.__name__, measuremem=True)\n    mon.weight = getattr(args[0], 'weight', 1.)  # used in task_info\n    mon.task_no = task_no\n    if mon.inject:\n        args += (mon,)\n    with Socket(mon.backurl, zmq.PUSH, 'connect') as zsocket:\n        msg = check_mem_usage()  # warn if too much memory is used\n        if msg:\n            zsocket.send(Result(None, mon, msg=msg))\n        if inspect.isgeneratorfunction(func):\n            gfunc = func\n        else:\n            def gfunc(*args):\n                yield func(*args)\n        gobj = gfunc(*args)\n        for count in itertools.count():\n            res = Result.new(next, (gobj,), mon, count=count)\n            # StopIteration -> TASK_ENDED\n            try:\n                zsocket.send(res)\n            except Exception:  # like OverflowError\n                _etype, exc, tb = sys.exc_info()\n                err = Result(exc, mon, ''.join(traceback.format_tb(tb)),\n                             count=count)\n                zsocket.send(err)\n            mon.duration = 0\n            mon.counts = 0\n            mon.children.clear()\n            if res.msg == 'TASK_ENDED':\n                break", "response": "Call the given function with the given arguments safely and return a pair of Result and exception_type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the task info of the object to the HDF5 file.", "response": "def save_task_info(self, res, mem_gb=0):\n    \"\"\"\n    :param self: an object with attributes .hdf5, .argnames, .sent\n    :parent res: a :class:`Result` object\n    :param mem_gb: memory consumption at the saving time (optional)\n    \"\"\"\n    mon = res.mon\n    name = mon.operation[6:]  # strip 'total '\n    if self.hdf5:\n        mon.hdf5 = self.hdf5  # needed for the flush below\n        t = (mon.task_no, mon.weight, mon.duration, len(res.pik), mem_gb)\n        data = numpy.array([t], task_info_dt)\n        hdf5.extend3(self.hdf5.filename, 'task_info/' + name, data,\n                     argnames=self.argnames, sent=self.sent)\n    mon.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits function used to wake up the workers", "response": "def init_workers():\n    \"\"\"Waiting function, used to wake up the process pool\"\"\"\n    setproctitle('oq-worker')\n    # unregister raiseMasterKilled in oq-workers to avoid deadlock\n    # since processes are terminated via pool.terminate()\n    signal.signal(signal.SIGTERM, signal.SIG_DFL)\n    # prctl is still useful (on Linux) to terminate all spawned processes\n    # when master is killed via SIGKILL\n    try:\n        import prctl\n    except ImportError:\n        pass\n    else:\n        # if the parent dies, the children die\n        prctl.set_pdeathsig(signal.SIGKILL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sequential_apply(task, args, concurrent_tasks=cpu_count * 3,\n                     weight=lambda item: 1, key=lambda item: 'Unspecified'):\n    \"\"\"\n    Apply sequentially task to args by splitting args[0] in blocks\n    \"\"\"\n    chunks = split_in_blocks(args[0], concurrent_tasks or 1, weight, key)\n    task_args = [(ch,) + args[1:] for ch in chunks]\n    return itertools.starmap(task, task_args)", "response": "Apply sequentially task to args by splitting args into consecutive tasks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self):\n        val = self.pik.unpickle()\n        if self.tb_str:\n            etype = val.__class__\n            msg = '\\n%s%s: %s' % (self.tb_str, etype.__name__, val)\n            if issubclass(etype, KeyError):\n                raise RuntimeError(msg)  # nicer message\n            else:\n                raise etype(msg)\n        return val", "response": "Returns the underlying value or raises the underlying exception\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new Result instance with the given args mon and count", "response": "def new(cls, func, args, mon, count=0):\n        \"\"\"\n        :returns: a new Result instance\n        \"\"\"\n        try:\n            with mon:\n                val = func(*args)\n        except StopIteration:\n            res = Result(None, mon, msg='TASK_ENDED')\n        except Exception:\n            _etype, exc, tb = sys.exc_info()\n            res = Result(exc, mon, ''.join(traceback.format_tb(tb)),\n                         count=count)\n        else:\n            res = Result(val, mon, count=count)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sum(cls, iresults):\n        res = object.__new__(cls)\n        res.received = []\n        res.sent = 0\n        for iresult in iresults:\n            res.received.extend(iresult.received)\n            res.sent += iresult.sent\n            name = iresult.name.split('#', 1)[0]\n            if hasattr(res, 'name'):\n                assert res.name.split('#', 1)[0] == name, (res.name, name)\n            else:\n                res.name = iresult.name.split('#')[0]\n        return res", "response": "Sum the data transfer information of a set of results\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs the progress of the computation in percentage", "response": "def log_percent(self):\n        \"\"\"\n        Log the progress of the computation in percentage\n        \"\"\"\n        done = self.total - self.todo\n        percent = int(float(done) / self.total * 100)\n        if not hasattr(self, 'prev_percent'):  # first time\n            self.prev_percent = 0\n            self.progress('Sent %s of data in %d %s task(s)',\n                          humansize(self.sent.sum()), self.total, self.name)\n        elif percent > self.prev_percent:\n            self.progress('%s %3d%% [of %d tasks]',\n                          self.name, percent, len(self.tasks))\n            self.prev_percent = percent\n        return done"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubmits the given arguments to the underlying task", "response": "def submit(self, *args, func=None, monitor=None):\n        \"\"\"\n        Submit the given arguments to the underlying task\n        \"\"\"\n        monitor = monitor or self.monitor\n        func = func or self.task_func\n        if not hasattr(self, 'socket'):  # first time\n            self.__class__.running_tasks = self.tasks\n            self.socket = Socket(self.receiver, zmq.PULL, 'bind').__enter__()\n            monitor.backurl = 'tcp://%s:%s' % (\n                config.dbserver.host, self.socket.port)\n        assert not isinstance(args[-1], Monitor)  # sanity check\n        dist = 'no' if self.num_tasks == 1 else self.distribute\n        if dist != 'no':\n            args = pickle_sequence(args)\n            self.sent += numpy.array([len(p) for p in args])\n        res = submit[dist](self, func, args, monitor)\n        self.tasks.append(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef submit_all(self):\n        for args in self.task_args:\n            self.submit(*args)\n        return self.get_results()", "response": "Submit all the task_args to the daemon and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an iterator result instance", "response": "def get_results(self):\n        \"\"\"\n        :returns: an :class:`IterResult` instance\n        \"\"\"\n        return IterResult(self._loop(), self.name, self.argnames,\n                          self.sent, self.monitor.hdf5)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit all tasks and reduce the results.", "response": "def reduce(self, agg=operator.add, acc=None):\n        \"\"\"\n        Submit all tasks and reduce the results\n        \"\"\"\n        return self.submit_all().reduce(agg, acc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes equivalent values of Joyner - Boore and closest distance to the rupture given epoicentral distance.", "response": "def _get_equivalent_distances_east(wid, lng, mag, repi, focal_depth=10.,\n                                   ab06=False):\n    \"\"\"\n    Computes equivalent values of Joyner-Boore and closest distance to the\n    rupture given epoicentral distance. The procedure is described in\n    Atkinson (2012) - Appendix A (page 32).\n\n    :param float wid:\n        Width of rectangular rupture\n    :param float lng:\n        Length of rectangular rupture\n    :param float mag:\n        Magnitude\n    :param repi:\n        A :class:`numpy.ndarray` instance containing repi values\n    :param float focal_depth:\n        Focal depth\n    :param boolean ab06:\n        When true a minimum ztor value is set to force near-source saturation\n    \"\"\"\n    dtop = focal_depth - 0.5*wid\n    # this computes a minimum ztor value - used for AB2006\n    if ab06:\n        ztor_ab06 = 21-2.5*mag\n        dtop = np.max([ztor_ab06, dtop])\n    ztor = max(0, dtop)\n    # find the average distance to the fault projection\n    dsurf = np.max([repi-0.3*lng, 0.1*np.ones_like(repi)], axis=0)\n    # rrup\n    rrup = (dsurf**2+ztor**2)**0.5\n    # return rjb and rrup\n    return dsurf, rrup"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_equivalent_distance_inslab(mag, repi, hslab):\n    area = 10**(-3.225+0.89*mag)\n    radius = (area / scipy.constants.pi)**0.5\n    rjb = np.max([repi-radius, np.zeros_like(repi)], axis=0)\n    rrup = (rjb**2+hslab**2)**0.5\n    return rjb, rrup", "response": "Returns the equivalent distance between a slab and a given site."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the shape of the homogenous probability tree", "response": "def get_shape(pmaps):\n    \"\"\"\n    :param pmaps: a set of homogenous ProbabilityMaps\n    :returns: the common shape (N, L, I)\n    \"\"\"\n    for pmap in pmaps:\n        if pmap:\n            sid = next(iter(pmap))\n            break\n    else:\n        raise AllEmptyProbabilityMaps(pmaps)\n    return (len(pmap),) + pmap[sid].array.shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef combine(pmaps):\n    shape = get_shape(pmaps)\n    res = ProbabilityMap(shape[1], shape[2])\n    for pmap in pmaps:\n        res |= pmap\n    return res", "response": "Returns a combined ProbabilityMap with the given set of homogenous ProbabilityMaps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert(self, imtls, idx=0):\n        curve = numpy.zeros(1, imtls.dt)\n        for imt in imtls:\n            curve[imt] = self.array[imtls(imt), idx]\n        return curve[0]", "response": "Convert a probability curve into a record of dtype imtls. dt."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(cls, shape_y, shape_z, sids, initvalue=0., dtype=F64):\n        dic = cls(shape_y, shape_z)\n        for sid in sids:\n            dic.setdefault(sid, initvalue, dtype)\n        return dic", "response": "Builds a ProbabilityMap from shape_y shape_z sids and initvalue."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a ProbabilityCurve object from an array of N site IDs.", "response": "def from_array(cls, array, sids):\n        \"\"\"\n        :param array: array of shape (N, L) or (N, L, I)\n        :param sids: array of N site IDs\n        \"\"\"\n        n_sites = len(sids)\n        n = len(array)\n        if n_sites != n:\n            raise ValueError('Passed %d site IDs, but the array has length %d'\n                             % (n_sites, n))\n        if len(array.shape) == 2:  # shape (N, L) -> (N, L, 1)\n            array = array.reshape(array.shape + (1,))\n        self = cls(*array.shape[1:])\n        for sid, poes in zip(sids, array):\n            self[sid] = ProbabilityCurve(poes)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the value of the attribute sid to value.", "response": "def setdefault(self, sid, value, dtype=F64):\n        \"\"\"\n        Works like `dict.setdefault`: if the `sid` key is missing, it fills\n        it with an array and returns the associate ProbabilityCurve\n\n        :param sid: site ID\n        :param value: value used to fill the returned ProbabilityCurve\n        :param dtype: dtype used internally (F32 or F64)\n        \"\"\"\n        try:\n            return self[sid]\n        except KeyError:\n            array = numpy.empty((self.shape_y, self.shape_z), dtype)\n            array.fill(value)\n            pc = ProbabilityCurve(array)\n            self[sid] = pc\n            return pc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef array(self):\n        return numpy.array([self[sid].array for sid in sorted(self)])", "response": "Returns the underlying array of shape N L I"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nbytes(self):\n        try:\n            N, L, I = get_shape([self])\n        except AllEmptyProbabilityMaps:\n            return 0\n        return BYTES_PER_FLOAT * N * L * I", "response": "The size of the underlying array."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a probability map into a composite array of length nsites and dtype imtls. dt.", "response": "def convert(self, imtls, nsites, idx=0):\n        \"\"\"\n        Convert a probability map into a composite array of length `nsites`\n        and dtype `imtls.dt`.\n\n        :param imtls:\n            DictArray instance\n        :param nsites:\n            the total number of sites\n        :param idx:\n            index on the z-axis (default 0)\n        \"\"\"\n        curves = numpy.zeros(nsites, imtls.dt)\n        for imt in curves.dtype.names:\n            curves_by_imt = curves[imt]\n            for sid in self:\n                curves_by_imt[sid] = self[sid].array[imtls(imt), idx]\n        return curves"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a probability map into a composite array of shape N.", "response": "def convert2(self, imtls, sids):\n        \"\"\"\n        Convert a probability map into a composite array of shape (N,)\n        and dtype `imtls.dt`.\n\n        :param imtls:\n            DictArray instance\n        :param sids:\n            the IDs of the sites we are interested in\n        :returns:\n            an array of curves of shape (N,)\n        \"\"\"\n        assert self.shape_z == 1, self.shape_z\n        curves = numpy.zeros(len(sids), imtls.dt)\n        for imt in curves.dtype.names:\n            curves_by_imt = curves[imt]\n            for i, sid in numpy.ndenumerate(sids):\n                try:\n                    pcurve = self[sid]\n                except KeyError:\n                    pass  # the poes will be zeros\n                else:\n                    curves_by_imt[i] = pcurve.array[imtls(imt), 0]\n        return curves"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter(self, sids):\n        dic = self.__class__(self.shape_y, self.shape_z)\n        for sid in sids:\n            try:\n                dic[sid] = self[sid]\n            except KeyError:\n                pass\n        return dic", "response": "Returns a submap of self for the given sids."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract(self, inner_idx):\n        out = self.__class__(self.shape_y, 1)\n        for sid in self:\n            curve = self[sid]\n            array = curve.array[:, inner_idx].reshape(-1, 1)\n            out[sid] = ProbabilityCurve(array)\n        return out", "response": "Extracts a component of the underlying ProbabilityCurves specified by the index inner_idx."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an array with C N L values being the first reference value compute the relative differences between the two sites with the first element in the array and discard the one below the tolerance.", "response": "def get_diff_idxs(array, rtol, atol):\n    \"\"\"\n    Given an array with (C, N, L) values, being the first the reference value,\n    compute the relative differences and discard the one below the tolerance.\n    :returns: indices where there are sensible differences.\n    \"\"\"\n    C, N, L = array.shape\n    diff_idxs = set()  # indices of the sites with differences\n    for c in range(1, C):\n        for n in range(N):\n            if not numpy.allclose(array[c, n], array[0, n], rtol, atol):\n                diff_idxs.add(n)\n    return numpy.fromiter(diff_idxs, int)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compare(what, imt, calc_ids, files, samplesites=100, rtol=.1, atol=1E-4):\n    sids, imtls, poes, arrays = getdata(what, calc_ids, samplesites)\n    try:\n        levels = imtls[imt]\n    except KeyError:\n        sys.exit(\n            '%s not found. The available IMTs are %s' % (imt, list(imtls)))\n    imt2idx = {imt: i for i, imt in enumerate(imtls)}\n    head = ['site_id'] if files else ['site_id', 'calc_id']\n    if what == 'hcurves':\n        array_imt = arrays[:, :, imtls(imt)]\n        header = head + ['%.5f' % lvl for lvl in levels]\n    else:  # hmaps\n        array_imt = arrays[:, :, imt2idx[imt]]\n        header = head + [str(poe) for poe in poes]\n    rows = collections.defaultdict(list)\n    diff_idxs = get_diff_idxs(array_imt, rtol, atol)\n    if len(diff_idxs) == 0:\n        print('There are no differences within the tolerance of %d%%' %\n              (rtol * 100))\n        return\n    arr = array_imt.transpose(1, 0, 2)  # shape (N, C, L)\n    for sid, array in sorted(zip(sids[diff_idxs], arr[diff_idxs])):\n        for calc_id, cols in zip(calc_ids, array):\n            if files:\n                rows[calc_id].append([sid] + list(cols))\n            else:\n                rows['all'].append([sid, calc_id] + list(cols))\n    if files:\n        fdict = {calc_id: open('%s.txt' % calc_id, 'w')\n                 for calc_id in calc_ids}\n        for calc_id, f in fdict.items():\n            f.write(views.rst_table(rows[calc_id], header))\n            print('Generated %s' % f.name)\n    else:\n        print(views.rst_table(rows['all'], header))", "response": "Compare the hazard curves or maps of two or more calculations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the filename of the neccesary figure based on the input properties.", "response": "def build_filename(filename, filetype='png', resolution=300):\n    \"\"\"\n    Uses the input properties to create the string of the filename\n\n    :param str filename:\n        Name of the file\n    :param str filetype:\n        Type of file\n    :param int resolution:\n        DPI resolution of the output figure\n    \"\"\"\n    filevals = os.path.splitext(filename)\n    if filevals[1]:\n        filetype = filevals[1][1:]\n    if not filetype:\n        filetype = 'png'\n    filename = filevals[0] + '.' + filetype\n\n    if not resolution:\n        resolution = 300\n    return filename, filetype, resolution"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _save_image(fig, filename, filetype='png', resolution=300):\n    if filename:\n        filename, filetype, resolution = build_filename(filename,\n                                                        filetype,\n                                                        resolution)\n        fig.savefig(filename, dpi=resolution, format=filetype)\n    else:\n        pass", "response": "Save the image of the current figure to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_catalogue_bin_limits(catalogue, dmag):\n    mag_bins = np.arange(\n        float(np.floor(np.min(catalogue.data['magnitude']))) - dmag,\n        float(np.ceil(np.max(catalogue.data['magnitude']))) + dmag,\n        dmag)\n    counter = np.histogram(catalogue.data['magnitude'], mag_bins)[0]\n    idx = np.where(counter > 0)[0]\n    mag_bins = mag_bins[idx[0]:(idx[-1] + 2)]\n    return mag_bins", "response": "Returns the magnitude bins corresponing to the catalogue\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a histogram of the depths in the catalogue.", "response": "def plot_depth_histogram(\n        catalogue, bin_width,\n        normalisation=False, bootstrap=None, filename=None,\n        figure_size=(8, 6), filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Creates a histogram of the depths in the catalogue\n\n    :param catalogue:\n        Earthquake catalogue as instance of :class:\n        openquake.hmtk.seismicity.catalogue.Catalogue\n    :param float bin_width:\n        Width of the histogram for the depth bins\n    :param bool normalisation:\n        Normalise the histogram to give output as PMF (True) or count (False)\n    :param int bootstrap:\n        To sample depth uncertainty choose number of samples\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n    # Create depth range\n    if len(catalogue.data['depth']) == 0:  # pylint: disable=len-as-condition\n        raise ValueError('No depths reported in catalogue!')\n    depth_bins = np.arange(0.,\n                           np.max(catalogue.data['depth']) + bin_width,\n                           bin_width)\n    depth_hist = catalogue.get_depth_distribution(depth_bins,\n                                                  normalisation,\n                                                  bootstrap)\n    ax.bar(depth_bins[:-1],\n           depth_hist,\n           width=0.95 * bin_width,\n           edgecolor='k')\n    ax.set_xlabel('Depth (km)')\n    if normalisation:\n        ax.set_ylabel('Probability Mass Function')\n    else:\n        ax.set_ylabel('Count')\n    ax.set_title('Depth Histogram')\n\n    _save_image(fig, filename, filetype, dpi)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the magnitude and depth distribution of the given species.", "response": "def plot_magnitude_depth_density(\n        catalogue, mag_int, depth_int,\n        logscale=False, normalisation=False, bootstrap=None, filename=None,\n        figure_size=(8, 6), filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Creates a density plot of the magnitude and depth distribution\n\n    :param catalogue:\n        Earthquake catalogue as instance of :class:\n        openquake.hmtk.seismicity.catalogue.Catalogue\n    :param float mag_int:\n        Width of the histogram for the magnitude bins\n    :param float depth_int:\n        Width of the histogram for the depth bins\n    :param bool logscale:\n        Choose to scale the colours in a log-scale (True) or linear (False)\n    :param bool normalisation:\n        Normalise the histogram to give output as PMF (True) or count (False)\n    :param int bootstrap:\n        To sample magnitude and depth uncertainties choose number of samples\n    \"\"\"\n    if len(catalogue.data['depth']) == 0:  # pylint: disable=len-as-condition\n        raise ValueError('No depths reported in catalogue!')\n    depth_bins = np.arange(0.,\n                           np.max(catalogue.data['depth']) + depth_int,\n                           depth_int)\n    mag_bins = _get_catalogue_bin_limits(catalogue, mag_int)\n    mag_depth_dist = catalogue.get_magnitude_depth_distribution(mag_bins,\n                                                                depth_bins,\n                                                                normalisation,\n                                                                bootstrap)\n    vmin_val = np.min(mag_depth_dist[mag_depth_dist > 0.])\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    if logscale:\n        normaliser = LogNorm(vmin=vmin_val, vmax=np.max(mag_depth_dist))\n    else:\n        normaliser = Normalize(vmin=0, vmax=np.max(mag_depth_dist))\n\n    im = ax.pcolor(mag_bins[:-1],\n                   depth_bins[:-1],\n                   mag_depth_dist.T,\n                   norm=normaliser)\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Depth (km)')\n    ax.set_xlim(mag_bins[0], mag_bins[-1])\n    ax.set_ylim(depth_bins[0], depth_bins[-1])\n    fig.colorbar(im, ax=ax)\n    if normalisation:\n        ax.set_title('Magnitude-Depth Density')\n    else:\n        ax.set_title('Magnitude-Depth Count')\n\n    _save_image(fig, filename, filetype, dpi)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the magnitude of the time of the current object", "response": "def plot_magnitude_time_scatter(\n        catalogue, plot_error=False, fmt_string='o', filename=None,\n        figure_size=(8, 6), filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Creates a simple scatter plot of magnitude with time\n\n    :param catalogue:\n        Earthquake catalogue as instance of :class:\n        openquake.hmtk.seismicity.catalogue.Catalogue\n    :param bool plot_error:\n        Choose to plot error bars (True) or not (False)\n    :param str fmt_string:\n        Symbology of plot\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    dtime = catalogue.get_decimal_time()\n    # pylint: disable=len-as-condition\n    if len(catalogue.data['sigmaMagnitude']) == 0:\n        print('Magnitude Error is missing - neglecting error bars!')\n        plot_error = False\n\n    if plot_error:\n        ax.errorbar(dtime,\n                    catalogue.data['magnitude'],\n                    xerr=None,\n                    yerr=catalogue.data['sigmaMagnitude'],\n                    fmt=fmt_string)\n    else:\n        ax.plot(dtime, catalogue.data['magnitude'], fmt_string)\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Magnitude')\n    ax.set_title('Magnitude-Time Plot')\n\n    _save_image(fig, filename, filetype, dpi)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_magnitude_time_density(\n        catalogue, mag_int, time_int, completeness=None,\n        normalisation=False, logscale=True, bootstrap=None, xlim=[], ylim=[],\n        filename=None, figure_size=(8, 6), filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Creates a plot of magnitude-time density\n\n    :param catalogue:\n        Earthquake catalogue as instance of :class:\n        openquake.hmtk.seismicity.catalogue.Catalogue\n    :param float mag_int:\n        Width of the histogram for the magnitude bins\n    :param float time_int:\n        Width of the histogram for the time bin (in decimal years)\n    :param bool normalisation:\n        Normalise the histogram to give output as PMF (True) or count (False)\n    :param int bootstrap:\n        To sample magnitude and depth uncertainties choose number of samples\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    # Create the magnitude bins\n    if isinstance(mag_int, (np.ndarray, list)):\n        mag_bins = mag_int\n    else:\n        mag_bins = np.arange(\n            np.min(catalogue.data['magnitude']),\n            np.max(catalogue.data['magnitude']) + mag_int / 2.,\n            mag_int)\n    # Creates the time bins\n    if isinstance(time_int, (np.ndarray, list)):\n        time_bins = time_int\n    else:\n        time_bins = np.arange(\n            float(np.min(catalogue.data['year'])),\n            float(np.max(catalogue.data['year'])) + 1.,\n            float(time_int))\n    # Get magnitude-time distribution\n    mag_time_dist = catalogue.get_magnitude_time_distribution(\n        mag_bins,\n        time_bins,\n        normalisation,\n        bootstrap)\n    # Get smallest non-zero value\n    vmin_val = np.min(mag_time_dist[mag_time_dist > 0.])\n    # Create plot\n    if logscale:\n        norm_data = LogNorm(vmin=vmin_val, vmax=np.max(mag_time_dist))\n    else:\n        if normalisation:\n            norm_data = Normalize(vmin=vmin_val, vmax=np.max(mag_time_dist))\n        else:\n            norm_data = Normalize(vmin=1.0, vmax=np.max(mag_time_dist))\n\n    im = ax.pcolor(time_bins[:-1],\n                   mag_bins[:-1],\n                   mag_time_dist.T,\n                   norm=norm_data)\n    ax.set_xlabel('Time (year)')\n    ax.set_ylabel('Magnitude')\n    if len(xlim) == 2:\n        ax.set_xlim(xlim[0], xlim[1])\n    else:\n        ax.set_xlim(time_bins[0], time_bins[-1])\n    if len(ylim) == 2:\n        ax.set_ylim(ylim[0], ylim[1])\n    else:\n        ax.set_ylim(mag_bins[0], mag_bins[-1] + (mag_bins[-1] - mag_bins[-2]))\n    # Fix the title\n    if normalisation:\n        fig.colorbar(im, label='Event Density', shrink=0.9, ax=ax)\n    else:\n        fig.colorbar(im, label='Event Count', shrink=0.9, ax=ax)\n    ax.grid(True)\n    # Plot completeness\n    if completeness is not None:\n        _plot_completeness(ax, completeness, time_bins[0], time_bins[-1])\n\n    _save_image(fig, filename, filetype, dpi)", "response": "Plots the magnitude - time density of the given uncertainties."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the completeness intervals of a single node.", "response": "def _plot_completeness(ax, comw, start_time, end_time):\n    '''\n    Adds completeness intervals to a plot\n    '''\n    comw = np.array(comw)\n    comp = np.column_stack([np.hstack([end_time, comw[:, 0], start_time]),\n                            np.hstack([comw[0, 1], comw[:, 1], comw[-1, 1]])])\n    ax.step(comp[:-1, 0], comp[1:, 1], linestyle='-',\n            where=\"post\", linewidth=3, color='brown')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_completeness_adjusted_table(catalogue, completeness, dmag,\n                                    offset=1.0E-5, end_year=None, plot=False,\n                                    figure_size=(8, 6), filename=None,\n                                    filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Counts the number of earthquakes in each magnitude bin and normalises\n    the rate to annual rates, taking into account the completeness\n    \"\"\"\n    if not end_year:\n        end_year = catalogue.end_year\n    # Find the natural bin limits\n    mag_bins = _get_catalogue_bin_limits(catalogue, dmag)\n    obs_time = end_year - completeness[:, 0] + 1.\n    obs_rates = np.zeros_like(mag_bins)\n    durations = np.zeros_like(mag_bins)\n    n_comp = np.shape(completeness)[0]\n    for iloc in range(n_comp):\n        low_mag = completeness[iloc, 1]\n        comp_year = completeness[iloc, 0]\n        if iloc == (n_comp - 1):\n            idx = np.logical_and(\n                catalogue.data['magnitude'] >= low_mag - offset,\n                catalogue.data['year'] >= comp_year)\n            high_mag = mag_bins[-1]\n            obs_idx = mag_bins >= (low_mag - offset)\n        else:\n            high_mag = completeness[iloc + 1, 1]\n            mag_idx = np.logical_and(\n                catalogue.data['magnitude'] >= low_mag - offset,\n                catalogue.data['magnitude'] < (high_mag - offset))\n\n            idx = np.logical_and(mag_idx,\n                                 catalogue.data['year'] >= (comp_year - offset))\n            obs_idx = np.logical_and(mag_bins >= (low_mag - offset),\n                                     mag_bins < (high_mag + offset))\n        temp_rates = np.histogram(catalogue.data['magnitude'][idx],\n                                  mag_bins[obs_idx])[0]\n        temp_rates = temp_rates.astype(float) / obs_time[iloc]\n        obs_rates[obs_idx[:-1]] = temp_rates\n        durations[obs_idx[:-1]] = obs_time[iloc]\n    selector = np.where(obs_rates > 0.)[0]\n    mag_bins = mag_bins[selector]\n    obs_rates = obs_rates[selector]\n    durations = durations[selector]\n    # Get cumulative rates\n    cum_rates = np.array([sum(obs_rates[iloc:])\n                          for iloc in range(0, len(obs_rates))])\n    if plot:\n        plt.figure(figsize=figure_size)\n        plt.semilogy(mag_bins + dmag / 2., obs_rates, \"bo\",\n                     label=\"Incremental\")\n        plt.semilogy(mag_bins + dmag / 2., cum_rates, \"rs\",\n                     label=\"Cumulative\")\n        plt.xlabel(\"Magnitude (M)\", fontsize=16)\n        plt.ylabel(\"Annual Rate\", fontsize=16)\n        plt.grid(True)\n        plt.legend(fontsize=16)\n        if filename:\n            plt.savefig(filename, format=filetype, dpi=dpi,\n                        bbox_inches=\"tight\")\n    return np.column_stack([mag_bins, durations, obs_rates, cum_rates,\n                            np.log10(cum_rates)])", "response": "Returns the table of the completeness adjusted by the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_observed_recurrence(\n        catalogue, completeness, dmag, end_year=None, filename=None,\n        figure_size=(8, 6), filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Plots the observed recurrence taking into account the completeness\n    \"\"\"\n    # Get completeness adjusted recurrence table\n    if isinstance(completeness, float):\n        # Unique completeness\n        completeness = np.array([[np.min(catalogue.data['year']),\n                                  completeness]])\n    if not end_year:\n        end_year = catalogue.update_end_year()\n    catalogue.data[\"dtime\"] = catalogue.get_decimal_time()\n    cent_mag, t_per, n_obs = get_completeness_counts(catalogue,\n                                                     completeness,\n                                                     dmag)\n    obs_rates = n_obs / t_per\n    cum_obs_rates = np.array([np.sum(obs_rates[i:])\n                              for i in range(len(obs_rates))])\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    ax.semilogy(cent_mag, obs_rates, 'bo', label=\"Incremental\")\n    ax.semilogy(cent_mag, cum_obs_rates, 'rs', label=\"Cumulative\")\n    ax.set_xlim([cent_mag[0] - 0.1, cent_mag[-1] + 0.1])\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Annual Rate')\n    ax.legend()\n    _save_image(fig, filename, filetype, dpi)", "response": "Plots the observed recurrence table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_secondary_strain_data(self, strain_data=None):\n        '''\n        Calculate the following and add to data dictionary:\n        1) 2nd invarient of strain\n        2) Dilatation rate\n        3) e1h and e2h\n        4) err\n\n        :param dict strain_data:\n            Strain data dictionary (as described) - will overwrite current\n            data if input\n\n        '''\n        if strain_data:\n            self.data = strain_data\n\n        if not isinstance(self.data, dict):\n            raise ValueError('Strain data not input or incorrectly formatted')\n\n        # Check to ensure essential attributes are in data dictionary\n        for essential_key in DATA_VARIABLES:\n            if essential_key not in self.data:\n                print(self.data)\n                raise ValueError('Essential strain information %s missing!'\n                                 % essential_key)\n        self.data_variables = deepcopy(DATA_VARIABLES)\n\n        # Second Invarient\n        self.data['2nd_inv'] = np.sqrt(\n            (self.data['exx'] ** 2.) +\n            (self.data['eyy'] ** 2.) +\n            2.0 * (self.data['exy'] ** 2.))\n        # Dilatation\n        self.data['dilatation'] = self.data['exx'] + self.data['eyy']\n        # err\n        self.data['err'] = -1. * self.data['dilatation']\n        center_normal_rate = (self.data['exx'] +\n                              self.data['eyy']) / 2.\n        radius_rate = np.sqrt((self.data['exx'] -\n                               center_normal_rate) ** 2. +\n                              (self.data['exy'] ** 2.))\n        # e1h and e2h\n        self.data['e1h'] = center_normal_rate - radius_rate\n        self.data['e2h'] = center_normal_rate + radius_rate\n        self.data['area'] = np.zeros(self.get_number_observations())\n        self.data_variables.extend(['2nd_inv', 'dilatation', 'err', 'e1h',\n                                    'e2h'])", "response": "This method calculates the secondary strain data for the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_number_observations(self):\n        '''\n        Returns the number of observations in the data file\n        '''\n        if isinstance(self.data, dict) and ('exx' in self.data.keys()):\n            return len(self.data['exx'])\n        else:\n            return 0", "response": "Returns the number of observations in the data file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the loss curves given a calculation id and an asset ordinal.", "response": "def plot_lc(calc_id, aid=None):\n    \"\"\"\n    Plot loss curves given a calculation id and an asset ordinal.\n    \"\"\"\n    # read the hazard data\n    dstore = util.read(calc_id)\n    dset = dstore['agg_curves-rlzs']\n    if aid is None:  # plot the global curves\n        plt = make_figure(dset.attrs['return_periods'], dset.value)\n    else:\n        sys.exit('Not implemented yet')\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nga_west2_epistemic_adjustment(magnitude, distance):\n    if magnitude < 6.0:\n        adjustment = 0.22 * np.ones_like(distance)\n        adjustment[distance < 10.0] = 0.37\n    elif magnitude >= 7.0:\n        adjustment = 0.36 * np.ones_like(distance)\n        adjustment[distance < 10.0] = 0.40\n        adjustment[distance >= 30.0] = 0.33\n    else:\n        adjustment = 0.23 * np.ones_like(distance)\n        adjustment[distance < 10.0] = 0.25\n    return adjustment", "response": "Returns the average adjustment factor for the epistemic uncertainty."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_weighted_poes(gsim, sctx, rctx, dctx, imt, imls, truncation_level,\n                      weighting=DEFAULT_WEIGHTING):\n    \"\"\"\n    This function implements the NGA West 2 GMPE epistemic uncertainty\n    adjustment factor without re-calculating the actual GMPE each time.\n\n    :param gsim:\n        Instance of the GMPE\n    :param list weighting:\n        Weightings as a list of tuples of (weight, number standard deviations\n        of the epistemic uncertainty adjustment)\n    \"\"\"\n    if truncation_level is not None and truncation_level < 0:\n        raise ValueError('truncation level must be zero, positive number '\n                         'or None')\n    gsim._check_imt(imt)\n    adjustment = nga_west2_epistemic_adjustment(rctx.mag, dctx.rrup)\n    adjustment = adjustment.reshape(adjustment.shape + (1, ))\n    if truncation_level == 0:\n        # zero truncation mode, just compare imls to mean\n        imls = gsim.to_distribution_values(imls)\n        mean, _ = gsim.get_mean_and_stddevs(sctx, rctx, dctx, imt, [])\n        mean = mean.reshape(mean.shape + (1, ))\n        output = np.zeros([mean.shape[0], imls.shape[0]])\n        for (wgt, fct) in weighting:\n            output += (wgt *\n                       (imls <= (mean + (fct * adjustment))).astype(float))\n        return output\n    else:\n        # use real normal distribution\n        assert (const.StdDev.TOTAL\n                in gsim.DEFINED_FOR_STANDARD_DEVIATION_TYPES)\n        imls = gsim.to_distribution_values(imls)\n        mean, [stddev] = gsim.get_mean_and_stddevs(sctx, rctx, dctx, imt,\n                                                   [const.StdDev.TOTAL])\n        mean = mean.reshape(mean.shape + (1, ))\n        stddev = stddev.reshape(stddev.shape + (1, ))\n        output = np.zeros([mean.shape[0], imls.shape[0]])\n        for (wgt, fct) in weighting:\n            values = (imls - (mean + (fct * adjustment))) / stddev\n            if truncation_level is None:\n                output += (wgt * _norm_sf(values))\n            else:\n                output += (wgt * _truncnorm_sf(truncation_level, values))\n        return output", "response": "This function returns the weighted Poisson terms for a given site."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mean_and_stddevs(self, sctx, rctx, dctx, imt, stddev_types):\n        # Get original mean and standard deviations\n        mean, stddevs = super().get_mean_and_stddevs(\n            sctx, rctx, dctx, imt, stddev_types)\n        cff = SInterCan15Mid.SITE_COEFFS[imt]\n        mean += np.log(cff['mf'])\n        return mean, stddevs", "response": "This method calculates the mean and standard deviations for the resource class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the mean and standard deviations for the entry class.", "response": "def get_mean_and_stddevs(self, sctx, rctx, dctx, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # Get original mean and standard deviations\n        mean, stddevs = super().get_mean_and_stddevs(\n            sctx, rctx, dctx, imt, stddev_types)\n        # Return mean, increased by the adjustment factor,\n        # and standard devation\n        return mean + nga_west2_epistemic_adjustment(rctx.mag, dctx.rrup),\\\n            stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadapt the original `get_poes()` from the :class: openquake.hazardlib.gsim.base.GMPE to call a function that take the weighted sum of the PoEs from the epistemic uncertainty adjustment", "response": "def get_poes(self, sctx, rctx, dctx, imt, imls, truncation_level):\n        \"\"\"\n        Adapts the original `get_poes()` from the :class:\n        openquake.hazardlib.gsim.base.GMPE to call a function that take the\n        weighted sum of the PoEs from the epistemic uncertainty adjustment\n        \"\"\"\n        return get_weighted_poes(self, sctx, rctx, dctx, imt, imls,\n                                 truncation_level)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpanding hazardlib source attribute defined through list of values into dictionary of shapefile parameters.", "response": "def expand_src_param(values, shp_params):\n    \"\"\"\n    Expand hazardlib source attribute (defined through list of values)\n    into dictionary of shapefile parameters.\n    \"\"\"\n    if values is None:\n        return dict([(key, None) for key, _ in shp_params])\n    else:\n        num_values = len(values)\n        return dict(\n            [(key, float(values[i]) if i < num_values else None)\n                for i, (key, _) in enumerate(shp_params)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts params from source object.", "response": "def extract_source_params(src):\n    \"\"\"\n    Extract params from source object.\n    \"\"\"\n    tags = get_taglist(src)\n    data = []\n    for key, param, vtype in BASE_PARAMS:\n        if key in src.attrib:\n            if vtype == \"c\":\n                data.append((param, src.attrib[key]))\n            elif vtype == \"f\":\n                data.append((param, float(src.attrib[key])))\n            else:\n                data.append((param, None))\n        elif key in tags:\n            if vtype == \"c\":\n                data.append((param, src.nodes[tags.index(key)].text))\n            elif vtype == \"f\":\n                data.append((param, float(src.nodes[tags.index(key)].text)))\n            else:\n                data.append((param, None))\n        else:\n            data.append((param, None))\n    return dict(data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_complex_fault_geometry(node):\n    assert \"complexFaultGeometry\" in node.tag\n    # Get general attributes\n    geometry = {\"intermediateEdges\": []}\n    for subnode in node:\n        crds = subnode.nodes[0].nodes[0].text\n        if \"faultTopEdge\" in subnode.tag:\n            geometry[\"faultTopEdge\"] = numpy.array(\n                [[crds[i], crds[i + 1], crds[i + 2]]\n                 for i in range(0, len(crds), 3)])\n            geometry[\"upperSeismoDepth\"] = numpy.min(\n                geometry[\"faultTopEdge\"][:, 2])\n        elif \"faultBottomEdge\" in subnode.tag:\n\n            geometry[\"faultBottomEdge\"] = numpy.array(\n                [[crds[i], crds[i + 1], crds[i + 2]]\n                 for i in range(0, len(crds), 3)])\n            geometry[\"lowerSeismoDepth\"] = numpy.max(\n                geometry[\"faultBottomEdge\"][:, 2])\n        elif \"intermediateEdge\" in subnode.tag:\n            geometry[\"intermediateEdges\"].append(\n                numpy.array([[crds[i], crds[i + 1], crds[i + 2]]\n                             for i in range(0, len(crds), 3)]))\n        else:\n            pass\n    geometry[\"dip\"] = None\n    return geometry", "response": "Parses a complex fault geometry node and returns both the attributes and parameters in a dictionary\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_planar_fault_geometry(node):\n    assert \"planarSurface\" in node.tag\n    geometry = {\"strike\": node.attrib[\"strike\"],\n                \"dip\": node.attrib[\"dip\"]}\n    upper_depth = numpy.inf\n    lower_depth = 0.0\n    tags = get_taglist(node)\n    corner_points = []\n    for locn in [\"topLeft\", \"topRight\", \"bottomRight\", \"bottomLeft\"]:\n        plane = node.nodes[tags.index(locn)]\n        upper_depth = plane[\"depth\"] if plane[\"depth\"] < upper_depth else\\\n            upper_depth\n        lower_depth = plane[\"depth\"] if plane[\"depth\"] > lower_depth else\\\n            lower_depth\n        corner_points.append([plane[\"lon\"], plane[\"lat\"], plane[\"depth\"]])\n    geometry[\"upperSeismoDepth\"] = upper_depth\n    geometry[\"lowerSeismoDepth\"] = lower_depth\n    geometry[\"corners\"] = numpy.array(corner_points)\n    return geometry", "response": "Parses a planar fault geometry node returning both the attributes and parameters in a dictionary\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the MFD parameters from an object src.", "response": "def extract_mfd_params(src):\n    \"\"\"\n    Extracts the MFD parameters from an object\n    \"\"\"\n    tags = get_taglist(src)\n    if \"incrementalMFD\" in tags:\n        mfd_node = src.nodes[tags.index(\"incrementalMFD\")]\n    elif \"truncGutenbergRichterMFD\" in tags:\n        mfd_node = src.nodes[tags.index(\"truncGutenbergRichterMFD\")]\n    elif \"arbitraryMFD\" in tags:\n        mfd_node = src.nodes[tags.index(\"arbitraryMFD\")]\n    elif \"YoungsCoppersmithMFD\" in tags:\n        mfd_node = src.nodes[tags.index(\"YoungsCoppersmithMFD\")]\n    else:\n        raise ValueError(\"Source %s contains no supported MFD type!\" % src.tag)\n    data = []\n    rates = []\n    for key, param, vtype in MFD_PARAMS:\n        if key in mfd_node.attrib and mfd_node.attrib[key] is not None:\n            data.append((param, mfd_node.attrib[key]))\n        else:\n            data.append((param, None))\n    if (\"incrementalMFD\" or \"arbitraryMFD\") in mfd_node.tag:\n        # Extract Rates\n        rates = ~mfd_node.occurRates\n        n_r = len(rates)\n        if n_r > MAX_RATES:\n            raise ValueError(\"Number of rates in source %s too large \"\n                             \"to be placed into shapefile\" % src.tag)\n        rate_dict = dict([(key, rates[i] if i < n_r else None)\n                          for i, (key, _) in enumerate(RATE_PARAMS)])\n    elif \"YoungsCoppersmithMFD\" in mfd_node.tag:\n        rate_dict = dict([(key, mfd_node.attrib['characteristicRate'])\n                          for i, (key, _) in enumerate(RATE_PARAMS)])\n    else:\n        rate_dict = dict([(key, None)\n                          for i, (key, _) in enumerate(RATE_PARAMS)])\n    return dict(data), rate_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting source hypocentral depths.", "response": "def extract_source_hypocentral_depths(src):\n    \"\"\"\n    Extract source hypocentral depths.\n    \"\"\"\n    if \"pointSource\" not in src.tag and \"areaSource\" not in src.tag:\n        hds = dict([(key, None) for key, _ in HDEPTH_PARAMS])\n        hdsw = dict([(key, None) for key, _ in HDW_PARAMS])\n        return hds, hdsw\n\n    tags = get_taglist(src)\n    hdd_nodeset = src.nodes[tags.index(\"hypoDepthDist\")]\n    if len(hdd_nodeset) > MAX_HYPO_DEPTHS:\n        raise ValueError(\"Number of hypocentral depths %s exceeds stated \"\n                         \"maximum of %s\" % (str(len(hdd_nodeset)),\n                                            str(MAX_HYPO_DEPTHS)))\n    if len(hdd_nodeset):\n        hds = []\n        hdws = []\n        for hdd_node in hdd_nodeset:\n            hds.append(float(hdd_node.attrib[\"depth\"]))\n            hdws.append(float(hdd_node.attrib[\"probability\"]))\n\n        hds = expand_src_param(hds, HDEPTH_PARAMS)\n        hdsw = expand_src_param(hdws, HDW_PARAMS)\n    else:\n        hds = dict([(key, None) for key, _ in HDEPTH_PARAMS])\n        hdsw = dict([(key, None) for key, _ in HDW_PARAMS])\n\n    return hds, hdsw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_source_planes_strikes_dips(src):\n    if \"characteristicFaultSource\" not in src.tag:\n        strikes = dict([(key, None) for key, _ in PLANES_STRIKES_PARAM])\n        dips = dict([(key, None) for key, _ in PLANES_DIPS_PARAM])\n        return strikes, dips\n    tags = get_taglist(src)\n    surface_set = src.nodes[tags.index(\"surface\")]\n    strikes = []\n    dips = []\n    num_planes = 0\n    for surface in surface_set:\n        if \"planarSurface\" in surface.tag:\n            strikes.append(float(surface.attrib[\"strike\"]))\n            dips.append(float(surface.attrib[\"dip\"]))\n            num_planes += 1\n    if num_planes > MAX_PLANES:\n        raise ValueError(\"Number of planes in sourcs %s exceededs maximum \"\n                         \"of %s\" % (str(num_planes), str(MAX_PLANES)))\n    if num_planes:\n        strikes = expand_src_param(strikes, PLANES_STRIKES_PARAM)\n        dips = expand_src_param(dips, PLANES_DIPS_PARAM)\n    else:\n        strikes = dict([(key, None) for key, _ in PLANES_STRIKES_PARAM])\n        dips = dict([(key, None) for key, _ in PLANES_DIPS_PARAM])\n\n    return strikes, dips", "response": "Extract strike and dip angles for source defined by multiple planes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_area_geometry(w, src):\n    assert \"areaSource\" in src.tag\n    geometry_node = src.nodes[get_taglist(src).index(\"areaGeometry\")]\n    area_attrs = parse_area_geometry(geometry_node)\n    w.poly(parts=[area_attrs[\"polygon\"].tolist()])", "response": "Set area polygon as shapefile geometry\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_point_geometry(w, src):\n    assert \"pointSource\" in src.tag\n    geometry_node = src.nodes[get_taglist(src).index(\"pointGeometry\")]\n    point_attrs = parse_point_geometry(geometry_node)\n    w.point(point_attrs[\"point\"][0], point_attrs[\"point\"][1])", "response": "Set point location as shapefile geometry."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets simple fault trace coordinates as shapefile geometry.", "response": "def set_simple_fault_geometry(w, src):\n    \"\"\"\n    Set simple fault trace coordinates as shapefile geometry.\n\n    :parameter w:\n        Writer\n    :parameter src:\n        source\n    \"\"\"\n    assert \"simpleFaultSource\" in src.tag\n    geometry_node = src.nodes[get_taglist(src).index(\"simpleFaultGeometry\")]\n    fault_attrs = parse_simple_fault_geometry(geometry_node)\n    w.line(parts=[fault_attrs[\"trace\"].tolist()])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a 3D polygon from a node instance", "response": "def set_simple_fault_geometry_3D(w, src):\n    \"\"\"\n    Builds a 3D polygon from a node instance\n    \"\"\"\n    assert \"simpleFaultSource\" in src.tag\n    geometry_node = src.nodes[get_taglist(src).index(\"simpleFaultGeometry\")]\n    fault_attrs = parse_simple_fault_geometry(geometry_node)\n    build_polygon_from_fault_attrs(w, fault_attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nidentify parameters defined in NRML source model file, so that shapefile contains only source model specific fields.", "response": "def appraise_source_model(self):\n        \"\"\"\n        Identify parameters defined in NRML source model file, so that\n        shapefile contains only source model specific fields.\n        \"\"\"\n        for src in self.sources:\n            # source params\n            src_taglist = get_taglist(src)\n            if \"areaSource\" in src.tag:\n                self.has_area_source = True\n                npd_node = src.nodes[src_taglist.index(\"nodalPlaneDist\")]\n                npd_size = len(npd_node)\n                hdd_node = src.nodes[src_taglist.index(\"hypoDepthDist\")]\n                hdd_size = len(hdd_node)\n                self.num_np = (npd_size if npd_size > self.num_np\n                               else self.num_np)\n                self.num_hd = (hdd_size if hdd_size > self.num_hd\n                               else self.num_hd)\n            elif \"pointSource\" in src.tag:\n                self.has_point_source = True\n                npd_node = src.nodes[src_taglist.index(\"nodalPlaneDist\")]\n                npd_size = len(npd_node)\n                hdd_node = src.nodes[src_taglist.index(\"hypoDepthDist\")]\n                hdd_size = len(hdd_node)\n                self.num_np = (npd_size if npd_size > self.num_np\n                               else self.num_np)\n                self.num_hd = (hdd_size if hdd_size > self.num_hd\n                               else self.num_hd)\n            elif \"simpleFaultSource\" in src.tag:\n                self.has_simple_fault_geometry = True\n            elif \"complexFaultSource\" in src.tag:\n                self.has_complex_fault_geometry = True\n            elif \"characteristicFaultSource\" in src.tag:\n                # Get the surface node\n                surface_node = src.nodes[src_taglist.index(\"surface\")]\n                p_size = 0\n                for surface in surface_node.nodes:\n                    if \"simpleFaultGeometry\" in surface.tag:\n                        self.has_simple_fault_geometry = True\n                    elif \"complexFaultGeometry\" in surface.tag:\n                        self.has_complex_fault_geometry = True\n                    elif \"planarSurface\" in surface.tag:\n                        self.has_planar_geometry = True\n                        p_size += 1\n                self.num_p = p_size if p_size > self.num_p else self.num_p\n            else:\n                pass\n\n            # MFD params\n            if \"truncGutenbergRichterMFD\" in src_taglist:\n                self.has_mfd_gr = True\n            elif \"incrementalMFD\" in src_taglist:\n                self.has_mfd_incremental = True\n                # Get rate size\n                mfd_node = src.nodes[src_taglist.index(\"incrementalMFD\")]\n                r_size = len(mfd_node.nodes[0].text)\n                self.num_r = r_size if r_size > self.num_r else self.num_r\n            else:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the source model from a NRML file.", "response": "def read(self, nrml_file, validate=False,\n             simple_fault_spacing=1.0, complex_mesh_spacing=5.0,\n             mfd_spacing=0.1):\n        \"\"\"\n        Build the source model from nrml format\n        \"\"\"\n        self.source_file = nrml_file\n        if validate:\n            converter = SourceConverter(1.0, simple_fault_spacing,\n                                        complex_mesh_spacing,\n                                        mfd_spacing,\n                                        10.0)\n            converter.fname = nrml_file\n        root = nrml.read(nrml_file)\n        if root['xmlns'] == 'http://openquake.org/xmlns/nrml/0.4':\n            sg_nodes = [root.sourceModel.nodes]\n        else:  # NRML 0.5\n            sg_nodes = root.sourceModel.nodes\n        sources = []\n        for sg_node in sg_nodes:\n            for no, src_node in enumerate(sg_node, 1):\n                if validate:\n                    print(\"Validating Source %s\" % src_node.attrib[\"id\"])\n                    converter.convert_node(src_node)\n                sources.append(src_node)\n        return SourceModel(sources)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport a source model to NRML file.", "response": "def write(self, destination, source_model, name=None):\n        \"\"\"\n        Exports to NRML\n        \"\"\"\n        if os.path.exists(destination):\n            os.remove(destination)\n        self.destination = destination\n        if name:\n            source_model.name = name\n        output_source_model = Node(\"sourceModel\", {\"name\": name})\n        dic = groupby(source_model.sources,\n                      operator.itemgetter('tectonicRegion'))\n        for i, (trt, srcs) in enumerate(dic.items(), 1):\n            output_source_model.append(\n                Node('sourceGroup',\n                     {'tectonicRegion': trt, 'name': 'group %d' % i},\n                     nodes=srcs))\n        print(\"Exporting Source Model to %s\" % self.destination)\n        with open(self.destination, \"wb\") as f:\n            nrml.write([output_source_model], f, \"%s\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove params uneeded by source_model", "response": "def filter_params(self, src_mod):\n        \"\"\"\n        Remove params uneeded by source_model\n        \"\"\"\n        # point and area related params\n        STRIKE_PARAMS[src_mod.num_np:] = []\n        DIP_PARAMS[src_mod.num_np:] = []\n        RAKE_PARAMS[src_mod.num_np:] = []\n        NPW_PARAMS[src_mod.num_np:] = []\n        HDEPTH_PARAMS[src_mod.num_hd:] = []\n        HDW_PARAMS[src_mod.num_hd:] = []\n        # planar rupture related params\n        PLANES_STRIKES_PARAM[src_mod.num_p:] = []\n        PLANES_DIPS_PARAM[src_mod.num_p:] = []\n        # rate params\n        RATE_PARAMS[src_mod.num_r:] = []\n\n        if src_mod.has_simple_fault_geometry is False:\n            GEOMETRY_PARAMS.remove(('dip', 'dip', 'f'))\n\n        if (src_mod.has_simple_fault_geometry is False and\n                src_mod.has_complex_fault_geometry is False and\n                src_mod.has_planar_geometry is False):\n            BASE_PARAMS.remove(('rake', 'rake', 'f'))\n\n        if (src_mod.has_simple_fault_geometry is False and\n                src_mod.has_complex_fault_geometry is False and\n                src_mod.has_area_source is False and\n                src_mod.has_point_source is False):\n            GEOMETRY_PARAMS[:] = []\n\n        if src_mod.has_mfd_incremental is False:\n            MFD_PARAMS.remove(('binWidth', 'bin_width', 'f'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, input_shapefile, validate=False,\n             simple_fault_spacing=1.0, complex_mesh_spacing=5.0,\n             mfd_spacing=0.1):\n        \"\"\"\n        Build the source model from nrml format\n        \"\"\"\n        reader = shapefile.Reader(input_shapefile)\n        fields = [field[0] for field in reader.fields[1:]]\n        shapes = reader.shapes()\n        records = reader.records()\n        sources = []\n        if validate:\n            converter = SourceConverter(1.0, simple_fault_spacing,\n                                        complex_mesh_spacing,\n                                        mfd_spacing,\n                                        10.0)\n        for iloc in range(0, reader.numRecords):\n            # Build record dictionary\n            record = record_to_dict(records[iloc], fields)\n            shape = shapes[iloc]\n            if \"pointSource\" in record[\"sourcetype\"]:\n                src = build_point_source_from_shp(shape, record)\n            elif \"areaSource\" in record[\"sourcetype\"]:\n                src = build_area_source_from_shp(shape, record)\n            elif \"simpleFaultSource\" in record[\"sourcetype\"]:\n                src = build_simple_fault_source_from_shp(shape, record)\n            elif \"complexFaultSource\" in record[\"sourcetype\"]:\n                src = build_complex_fault_source_from_shp(shape, record)\n            elif \"characteristicFaultSource\" in record[\"sourcetype\"]:\n                print(\"Characteristic Fault Source Not Yet Supported - Sorry!\")\n                src = None\n            if src and validate:\n                print(\"Validating Source %s\" % src.attrib[\"id\"])\n                converter.convert_node(src)\n            if src:\n                sources.append(src)\n        return SourceModel(sources)", "response": "Read the source model from a shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, destination, source_model, name=None):\n        if os.path.exists(destination + \".shp\"):\n            os.system(\"rm %s.*\" % destination)\n        self.destination = destination\n        self.filter_params(source_model)\n\n        w_area = shapefile.Writer(shapefile.POLYGON)\n        w_point = shapefile.Writer(shapefile.POINT)\n        w_simple = shapefile.Writer(shapefile.POLYLINE)\n        w_simple3d = shapefile.Writer(shapefile.POLYGONZ)\n        w_complex = shapefile.Writer(shapefile.POLYLINEZ)\n        w_planar = shapefile.Writer(shapefile.POLYGONZ)\n\n        register_fields(w_area)\n        register_fields(w_point)\n        register_fields(w_simple)\n        register_fields(w_simple3d)\n        register_fields(w_complex)\n        register_fields(w_planar)\n\n        for src in source_model.sources:\n\n            # Order is important here\n            if \"areaSource\" in src.tag:\n                set_params(w_area, src)\n                set_area_geometry(w_area, src)\n            elif \"pointSource\" in src.tag:\n                set_params(w_point, src)\n                set_point_geometry(w_point, src)\n            elif \"complexFaultSource\" in src.tag:\n                set_params(w_complex, src)\n                set_complex_fault_geometry(w_complex, src)\n            elif \"simpleFaultSource\" in src.tag:\n                set_params(w_simple, src)\n                set_simple_fault_geometry(w_simple, src)\n                # Create the 3D polygon\n                set_params(w_simple3d, src)\n                set_simple_fault_geometry_3D(w_simple3d, src)\n            elif \"characteristicFaultSource\" in src.tag:\n                src_taglist = get_taglist(src)\n                surface_node = src.nodes[src_taglist.index(\"surface\")]\n                for subnode in surface_node:\n                    if \"simpleFaultGeometry\" in subnode.tag:\n                        set_params(w_simple, src)\n                        set_params(w_simple3d, src)\n                    elif \"complexFaultGeometry\" in subnode.tag:\n                        set_params(w_complex, src)\n                    elif \"planarSurface\" in subnode.tag:\n                        set_params(w_planar, src)\n                    else:\n                        raise ValueError(\n                            'Geometry class %s not recognized'\n                            % subnode.tag)\n                    set_characteristic_geometry(w_simple, w_simple3d,\n                                                w_complex, w_planar, src)\n            else:\n                raise ValueError('Source type %s not recognized'\n                                 % src.tag)\n\n        root = self.destination\n        if len(w_area.shapes()) > 0:\n            w_area.save('%s_area' % root)\n        if len(w_point.shapes()) > 0:\n            w_point.save('%s_point' % root)\n        if len(w_complex.shapes()) > 0:\n            w_complex.save('%s_complex' % root)\n        if len(w_simple.shapes()) > 0:\n            w_simple.save('%s_simple' % root)\n            w_simple3d.save('%s_simple3d' % root)\n        if len(w_planar.shapes()) > 0:\n            w_planar.save('%s_planar' % root)", "response": "Write the source model to a new shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shell(script=None, args=()):\n    if script:\n        sys.argv = sys.argv[2:]  # strip ['oq', 'shell']\n        runpy.run_path(script, run_name='__main__')\n        return\n    o = OpenQuake()  # noqa\n    try:\n        import IPython\n        IPython.embed(banner1='IPython shell with a global object \"o\"')\n    except ImportError:\n        import code\n        code.interact(banner='Python shell with a global object \"o\"',\n                      local=dict(o=o))", "response": "Start an embedded IPython instance with a global object o"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bilinear_residuals_stepp(input_params, xvals, yvals, slope1_fit):\n    '''\n    Returns the residual sum-of-squares value of a bilinear fit to a data\n    set - with a segment - 1 gradient fixed by an input value (slope_1_fit)\n\n    :param list input_params:\n        Input parameters for the bilinear model [slope2, crossover_point,\n                                                 intercept]\n    :param numpy.ndarray xvals:\n        x-values of the data to be fit\n\n    :param numpy.ndarray yvals:\n        y-values of the data to be fit\n\n    :param float slope1_fit:\n        Gradient of the first slope\n\n    :returns:\n        Residual sum-of-squares of fit\n    '''\n    params = np.hstack([slope1_fit, input_params])\n    num_x = len(xvals)\n    y_model = np.zeros(num_x, dtype=float)\n    residuals = np.zeros(num_x, dtype=float)\n    for iloc in range(0, num_x):\n        y_model[iloc] = piecewise_linear_scalar(params, xvals[iloc])\n        residuals[iloc] = (yvals[iloc] - y_model[iloc]) ** 2.0\n    return np.sum(residuals)", "response": "This function computes the residual sum - of - squares value of a bilinear fit to a data tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef floatformat(fmt_string):\n    fmt_defaults = scientificformat.__defaults__\n    scientificformat.__defaults__ = (fmt_string,) + fmt_defaults[1:]\n    try:\n        yield\n    finally:\n        scientificformat.__defaults__ = fmt_defaults", "response": "Context manager to change the default format string for the\n    function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scientificformat(value, fmt='%13.9E', sep=' ', sep2=':'):\n    if isinstance(value, numpy.bool_):\n        return '1' if value else '0'\n    elif isinstance(value, bytes):\n        return value.decode('utf8')\n    elif isinstance(value, str):\n        return value\n    elif hasattr(value, '__len__'):\n        return sep.join((scientificformat(f, fmt, sep2) for f in value))\n    elif isinstance(value, (float, numpy.float64, numpy.float32)):\n        fmt_value = fmt % value\n        if set(fmt_value) <= zeroset:\n            # '-0.0000000E+00' is converted into '0.0000000E+00\n            fmt_value = fmt_value.replace('-', '')\n        return fmt_value\n    return str(value)", "response": "Convert a value into a string in a scientific notation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a node into an XML string by using the StreamingXMLWriter.", "response": "def tostring(node, indent=4, nsmap=None):\n    \"\"\"\n    Convert a node into an XML string by using the StreamingXMLWriter.\n    This is useful for testing purposes.\n\n    :param node: a node object (typically an ElementTree object)\n    :param indent: the indentation to use in the XML (default 4 spaces)\n    \"\"\"\n    out = io.BytesIO()\n    writer = StreamingXMLWriter(out, indent, nsmap=nsmap)\n    writer.serialize(node)\n    return out.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(source, remove_comments=True, **kw):\n    return ElementTree.parse(source, SourceLineParser(), **kw)", "response": "Wrapper around ElementTree. parse"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iterparse(source, events=('end',), remove_comments=True, **kw):\n    return ElementTree.iterparse(source, events, SourceLineParser(), **kw)", "response": "A wrapper around ElementTree. iterparse that wraps ElementTree. iterparse."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _display(node, indent, expandattrs, expandvals, output):\n    attrs = _displayattrs(node.attrib, expandattrs)\n    if node.text is None or not expandvals:\n        val = ''\n    elif isinstance(node.text, str):\n        val = ' %s' % repr(node.text.strip())\n    else:\n        val = ' %s' % repr(node.text)  # node.text can be a tuple\n    output.write(encode(indent + striptag(node.tag) + attrs + val + '\\n'))\n    for sub_node in node:\n        _display(sub_node, indent + '  ', expandattrs, expandvals, output)", "response": "Core function to display a Node object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay the contents of a node object on the output stream.", "response": "def node_display(root, expandattrs=False, expandvals=False, output=sys.stdout):\n    \"\"\"\n    Write an indented representation of the Node object on the output;\n    this is intended for testing/debugging purposes.\n\n    :param root: a Node object\n    :param bool expandattrs: if True, the values of the attributes are\n                             also printed, not only the names\n    :param bool expandvals: if True, the values of the tags are also printed,\n                            not only the names.\n    :param output: stream where to write the string representation of the node\n    \"\"\"\n    _display(root, '', expandattrs, expandvals, output)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_literal(self):\n    if not self.nodes:\n        return (self.tag, self.attrib, self.text, [])\n    else:\n        return (self.tag, self.attrib, self.text,\n                list(map(to_literal, self.nodes)))", "response": "Convert the node into a literal Python object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pprint(self, stream=None, indent=1, width=80, depth=None):\n    pp.pprint(to_literal(self), stream, indent, width, depth)", "response": "Pretty print the object to stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a dictionary with attributes tag attrib text nodes into a Node object.", "response": "def node_from_dict(dic, nodefactory=Node):\n    \"\"\"\n    Convert a (nested) dictionary with attributes tag, attrib, text, nodes\n    into a Node object.\n    \"\"\"\n    tag = dic['tag']\n    text = dic.get('text')\n    attrib = dic.get('attrib', {})\n    nodes = dic.get('nodes', [])\n    if not nodes:\n        return nodefactory(tag, attrib, text)\n    return nodefactory(tag, attrib, nodes=list(map(node_from_dict, nodes)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a Node object into a nested dictionary with attributes tag attrib text nodes.", "response": "def node_to_dict(node):\n    \"\"\"\n    Convert a Node object into a (nested) dictionary\n    with attributes tag, attrib, text, nodes.\n\n    :param node: a Node-compatible object\n    \"\"\"\n    dic = dict(tag=striptag(node.tag))\n    if node.attrib:\n        dic['attrib'] = node.attrib\n    if node.text is not None:\n        dic['text'] = node.text\n    if node.nodes:\n        dic['nodes'] = [node_to_dict(n) for n in node]\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts an ElementTree object into a Node object.", "response": "def node_from_elem(elem, nodefactory=Node, lazy=()):\n    \"\"\"\n    Convert (recursively) an ElementTree object into a Node object.\n    \"\"\"\n    children = list(elem)\n    lineno = getattr(elem, 'lineno', None)\n    if not children:\n        return nodefactory(elem.tag, dict(elem.attrib), elem.text,\n                           lineno=lineno)\n    if striptag(elem.tag) in lazy:\n        nodes = (node_from_elem(ch, nodefactory, lazy) for ch in children)\n    else:\n        nodes = [node_from_elem(ch, nodefactory, lazy) for ch in children]\n    return nodefactory(elem.tag, dict(elem.attrib), nodes=nodes, lineno=lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef node_to_elem(root):\n    def generate_elem(append, node, level):\n        var = \"e\" + str(level)\n        arg = repr(node.tag)\n        if node.attrib:\n            arg += \", **%r\" % node.attrib\n        if level == 1:\n            append(\"e1 = Element(%s)\" % arg)\n        else:\n            append(\"%s = SubElement(e%d, %s)\" % (var, level - 1, arg))\n        if not node.nodes:\n            append(\"%s.text = %r\" % (var, node.text))\n        for x in node:\n            generate_elem(append, x, level + 1)\n    # generate code to create a tree\n    output = []\n    generate_elem(output.append, root, 1)  # print \"\\n\".join(output)\n    namespace = {\"Element\": ElementTree.Element,\n                 \"SubElement\": ElementTree.SubElement}\n    exec(\"\\n\".join(output), globals(), namespace)\n    return namespace[\"e1\"]", "response": "Convert a Node object into an ElementTree object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading an XML file into a lazy iterator over Node objects.", "response": "def read_nodes(fname, filter_elem, nodefactory=Node, remove_comments=True):\n    \"\"\"\n    Convert an XML file into a lazy iterator over Node objects\n    satifying the given specification, i.e. a function element -> boolean.\n\n    :param fname: file name of file object\n    :param filter_elem: element specification\n\n    In case of errors, add the file name to the error message.\n    \"\"\"\n    try:\n        for _, el in iterparse(fname, remove_comments=remove_comments):\n            if filter_elem(el):\n                yield node_from_elem(el, nodefactory)\n                el.clear()  # save memory\n    except Exception:\n        etype, exc, tb = sys.exc_info()\n        msg = str(exc)\n        if not str(fname) in msg:\n            msg = '%s in %s' % (msg, fname)\n        raise_(etype, msg, tb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a. xml file into a Node object.", "response": "def node_from_xml(xmlfile, nodefactory=Node):\n    \"\"\"\n    Convert a .xml file into a Node object.\n\n    :param xmlfile: a file name or file object open for reading\n    \"\"\"\n    root = parse(xmlfile).getroot()\n    return node_from_elem(root, nodefactory)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef node_to_xml(node, output=sys.stdout, nsmap=None):\n    if nsmap:\n        for ns, prefix in nsmap.items():\n            if prefix:\n                node['xmlns:' + prefix[:-1]] = ns\n            else:\n                node['xmlns'] = ns\n    with StreamingXMLWriter(output, nsmap=nsmap) as w:\n        w.serialize(node)", "response": "Convert a Node object into a pretty. xml file without keeping everything in memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a. ini file into a Node object.", "response": "def node_from_ini(ini_file, nodefactory=Node, root_name='ini'):\n    \"\"\"\n    Convert a .ini file into a Node object.\n\n    :param ini_file: a filename or a file like object in read mode\n    \"\"\"\n    fileobj = open(ini_file) if isinstance(ini_file, str) else ini_file\n    cfp = configparser.RawConfigParser()\n    cfp.read_file(fileobj)\n    root = nodefactory(root_name)\n    sections = cfp.sections()\n    for section in sections:\n        params = dict(cfp.items(section))\n        root.append(Node(section, params))\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a Node object with the right structure into a. ini file.", "response": "def node_to_ini(node, output=sys.stdout):\n    \"\"\"\n    Convert a Node object with the right structure into a .ini file.\n\n    :params node: a Node object\n    :params output: a file-like object opened in write mode\n    \"\"\"\n    for subnode in node:\n        output.write(u'\\n[%s]\\n' % subnode.tag)\n        for name, value in sorted(subnode.attrib.items()):\n            output.write(u'%s=%s\\n' % (name, value))\n    output.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef node_copy(node, nodefactory=Node):\n    return nodefactory(node.tag, node.attrib.copy(), node.text,\n                       [node_copy(n, nodefactory) for n in node])", "response": "Make a deep copy of the node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef context(fname, node):\n    try:\n        yield node\n    except Exception:\n        etype, exc, tb = sys.exc_info()\n        msg = 'node %s: %s, line %s of %s' % (\n            striptag(node.tag), exc, getattr(node, 'lineno', '?'), fname)\n        raise_(etype, msg, tb)", "response": "Context manager managing exceptions and adding line number of the current file to the error message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the short representation of a fully qualified XML tag.", "response": "def shorten(self, tag):\n        \"\"\"\n        Get the short representation of a fully qualified tag\n\n        :param str tag: a (fully qualified or not) XML tag\n        \"\"\"\n        if tag.startswith('{'):\n            ns, _tag = tag.rsplit('}')\n            tag = self.nsmap.get(ns[1:], '') + _tag\n        return tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write(self, text):\n        spaces = ' ' * (self.indent * self.indentlevel)\n        t = spaces + text.strip() + '\\n'\n        if hasattr(t, 'encode'):\n            t = t.encode(self.encoding, 'xmlcharrefreplace')\n        self.stream.write(t)", "response": "Write text by respecting the current indentlevel"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an empty element.", "response": "def emptyElement(self, name, attrs):\n        \"\"\"Add an empty element (may have attributes)\"\"\"\n        attr = ' '.join('%s=%s' % (n, quoteattr(scientificformat(v)))\n                        for n, v in sorted(attrs.items()))\n        self._write('<%s %s/>' % (name, attr))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens an XML tag.", "response": "def start_tag(self, name, attrs=None):\n        \"\"\"Open an XML tag\"\"\"\n        if not attrs:\n            self._write('<%s>' % name)\n        else:\n            self._write('<' + name)\n            for (name, value) in sorted(attrs.items()):\n                self._write(\n                    ' %s=%s' % (name, quoteattr(scientificformat(value))))\n            self._write('>')\n        self.indentlevel += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserializes a node object ( typically an ElementTree object.", "response": "def serialize(self, node):\n        \"\"\"Serialize a node object (typically an ElementTree object)\"\"\"\n        if isinstance(node.tag, types.FunctionType):\n            # this looks like a bug of ElementTree: comments are stored as\n            # functions!?? see https://hg.python.org/sandbox/python2.7/file/tip/Lib/xml/etree/ElementTree.py#l458\n            return\n        if self.nsmap is not None:\n            tag = self.shorten(node.tag)\n        else:\n            tag = node.tag\n        with warnings.catch_warnings():  # unwanted ElementTree warning\n            warnings.simplefilter('ignore')\n            leafnode = not node\n        # NB: we cannot use len(node) to identify leafs since nodes containing\n        # an iterator have no length. They are always True, even if empty :-(\n        if leafnode and node.text is None:\n            self.emptyElement(tag, node.attrib)\n            return\n        self.start_tag(tag, node.attrib)\n        if node.text is not None:\n            txt = escape(scientificformat(node.text).strip())\n            if txt:\n                self._write(txt)\n        for subnode in node:\n            self.serialize(subnode)\n        self.end_tag(tag)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getnodes(self, name):\n        \"Return the direct subnodes with name 'name'\"\n        for node in self.nodes:\n            if striptag(node.tag) == name:\n                yield node", "response": "Return the direct subnodes with name name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append(self, node):\n        \"Append a new subnode\"\n        if not isinstance(node, self.__class__):\n            raise TypeError('Expected Node instance, got %r' % node)\n        self.nodes.append(node)", "response": "Append a new subnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_str(self, expandattrs=True, expandvals=True):\n        out = io.BytesIO()\n        node_display(self, expandattrs, expandvals, out)\n        return decode(out.getvalue())", "response": "Convert the node into a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_bytes(self, bytestr, isfinal=True):\n        with self._context():\n            self.filename = None\n            self.p.Parse(bytestr, isfinal)\n        return self._root", "response": "Parse a byte string into a new instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a file or a filename and return a new instance of a base class.", "response": "def parse_file(self, file_or_fname):\n        \"\"\"\n        Parse a file or a filename\n        \"\"\"\n        with self._context():\n            if hasattr(file_or_fname, 'read'):\n                self.filename = getattr(\n                    file_or_fname, 'name', file_or_fname.__class__.__name__)\n                self.p.ParseFile(file_or_fname)\n            else:\n                self.filename = file_or_fname\n                with open(file_or_fname, 'rb') as f:\n                    self.p.ParseFile(f)\n        return self._root"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef completeness(self, catalogue, config, saveplot=False, filetype='png',\n                     timeout=120):\n        '''\n        :param catalogue:\n            Earthquake catalogue as instance of\n            :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n        :param dict config:\n            Configuration parameters of the algorithm, containing the\n            following information:\n            'magnitude_bin' Size of magnitude bin (non-negative float)\n            'time_bin' Size (in dec. years) of the time window (non-negative\n            float)\n            'increment_lock' Boolean to indicate whether to ensure\n            completeness magnitudes always decrease with more\n            recent bins\n        :returns:\n            2-column table indicating year of completeness and corresponding\n            magnitude numpy.ndarray\n        '''\n        if saveplot and not isinstance(saveplot, str):\n            raise ValueError('To save the figures enter a filename: ')\n\n        # Get magntitude bins\n        magnitude_bins = self._get_magnitudes_from_spacing(\n            catalogue.data['magnitude'],\n            config['magnitude_bin'])\n        dec_time = catalogue.get_decimal_time()\n        completeness_table = np.zeros([len(magnitude_bins) - 1, 2],\n                                      dtype=float)\n        min_year = float(np.min(catalogue.data['year']))\n        max_year = float(np.max(catalogue.data['year'])) + 1.0\n        has_completeness = np.zeros(len(magnitude_bins) - 1, dtype=bool)\n        for iloc in range(0, len(magnitude_bins) - 1):\n            lower_mag = magnitude_bins[iloc]\n            upper_mag = magnitude_bins[iloc + 1]\n            idx = np.logical_and(catalogue.data['magnitude'] >= lower_mag,\n                                 catalogue.data['magnitude'] < upper_mag)\n            cumvals = np.cumsum(np.ones(np.sum(idx)))\n            plt.plot(dec_time[idx], cumvals, '.')\n            plt.xlim(min_year, max_year + 5)\n            title_string = 'Magnitude %5.2f to %5.2f' % (lower_mag, upper_mag)\n            plt.title(title_string)\n            pts = pylab.ginput(1, timeout=timeout)[0]\n            if pts[0] <= max_year:\n                 # Magnitude bin has no completeness!\n                has_completeness[iloc] = True\n            completeness_table[iloc, 0] = np.floor(pts[0])\n            completeness_table[iloc, 1] = magnitude_bins[iloc]\n            print(completeness_table[iloc, :], has_completeness[iloc])\n            if config['increment_lock'] and (iloc > 0) and \\\n                    (completeness_table[iloc, 0] > completeness_table[iloc - 1, 0]):\n                completeness_table[iloc, 0] = \\\n                    completeness_table[iloc - 1, 0]\n            # Add marker line to indicate completeness point\n            marker_line = np.array([\n                [0., completeness_table[iloc, 0]],\n                [cumvals[-1], completeness_table[iloc, 0]]])\n            plt.plot(marker_line[:, 0], marker_line[:, 1], 'r-')\n            if saveplot:\n                filename = saveplot + '_' + ('%5.2f' % lower_mag) + (\n                    '%5.2f' % upper_mag) + '.' + filetype\n                plt.savefig(filename, format=filetype)\n            plt.close()\n        return completeness_table[has_completeness, :]", "response": "Calculates the completeness of the current energy in the specified time window."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_magnitudes_from_spacing(self, magnitudes, delta_m):\n        '''If a single magnitude spacing is input then create the bins\n\n        :param numpy.ndarray magnitudes:\n            Vector of earthquake magnitudes\n\n        :param float delta_m:\n            Magnitude bin width\n\n        :returns: Vector of magnitude bin edges (numpy.ndarray)\n        '''\n        min_mag = np.min(magnitudes)\n        max_mag = np.max(magnitudes)\n        if (max_mag - min_mag) < delta_m:\n            raise ValueError('Bin width greater than magnitude range!')\n        mag_bins = np.arange(np.floor(min_mag), np.ceil(max_mag), delta_m)\n        # Check to see if there are magnitudes in lower and upper bins\n        is_mag = np.logical_and(mag_bins - max_mag < delta_m,\n                                min_mag - mag_bins < delta_m)\n        mag_bins = mag_bins[is_mag]\n        return mag_bins", "response": "This function creates the bin edges from the given magnitudes and the given delta_m."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging two catalogue data dictionaries containing catalogue data and the information originally originally included in dat1 and dat2.", "response": "def _merge_data(dat1, dat2):\n    \"\"\"\n    Merge two data dictionaries containing catalogue data\n\n    :parameter dictionary dat1:\n        Catalogue data dictionary\n\n    :parameter dictionary dat2:\n        Catalogue data dictionary\n\n    :returns:\n        A catalogue data dictionary containing the information originally\n        included in dat1 and dat2\n    \"\"\"\n\n    cnt = 0\n    for key in dat1:\n        flg1 = len(dat1[key]) > 0\n        flg2 = len(dat2[key]) > 0\n        if flg1 != flg2:\n            cnt += 1\n\n    if cnt:\n        raise Warning('Cannot merge catalogues with different' +\n                      ' attributes')\n        return None\n    else:\n        for key in dat1:\n            if isinstance(dat1[key], np.ndarray):\n                dat1[key] = np.concatenate((dat1[key], dat2[key]), axis=0)\n            elif isinstance(dat1[key], list):\n                dat1[key] += dat2[key]\n            else:\n                raise ValueError('Unknown type')\n        return dat1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_row_str(self, i):\n        row_data = [\"{:s}\".format(self.data['eventID'][i]),\n                    \"{:g}\".format(self.data['year'][i]),\n                    \"{:g}\".format(self.data['month'][i]),\n                    \"{:g}\".format(self.data['day'][i]),\n                    \"{:g}\".format(self.data['hour'][i]),\n                    \"{:g}\".format(self.data['minute'][i]),\n                    \"{:.1f}\".format(self.data['second'][i]),\n                    \"{:.3f}\".format(self.data['longitude'][i]),\n                    \"{:.3f}\".format(self.data['latitude'][i]),\n                    \"{:.1f}\".format(self.data['depth'][i]),\n                    \"{:.1f}\".format(self.data['magnitude'][i])]\n        return \" \".join(row_data)", "response": "Returns a string representation of the key information in a row"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_catalogue(self, output_file, key_list=SORTED_ATTRIBUTE_LIST):\n\n        with open(output_file, 'w') as of:\n            writer = csv.DictWriter(of, fieldnames=key_list)\n            writer.writeheader()\n            for i in range(self.get_number_events()):\n                row_dict = {}\n                for key in key_list:\n                    if len(self.data[key]) > 0:\n                        data = self.data[key][i]\n                        if key in self.INT_ATTRIBUTE_LIST:\n                            if np.isnan(data):\n                                data = ''\n                            else:\n                                data = int(data)\n                        if key in self.FLOAT_ATTRIBUTE_LIST:\n                            if np.isnan(data):\n                                data = ''\n                            else:\n                                data = float(data)\n                    row_dict[key] = data\n                writer.writerow(row_dict)", "response": "Writes the catalogue to file using HTMK format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_to_array(self, keys):\n        # Preallocate the numpy array\n        data = np.empty((len(self.data[keys[0]]), len(keys)))\n        for i in range(0, len(self.data[keys[0]])):\n            for j, key in enumerate(keys):\n                data[i, j] = self.data[key][i]\n        return data", "response": "This loads the data contained in the catalogue into a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters the catalogue using a magnitude - time table.", "response": "def catalogue_mt_filter(self, mt_table, flag=None):\n        \"\"\"\n        Filter the catalogue using a magnitude-time table. The table has\n        two columns and n-rows.\n\n        :param nump.ndarray mt_table:\n            Magnitude time table with n-rows where column 1 is year and column\n            2 is magnitude\n\n        \"\"\"\n        if flag is None:\n            # No flag defined, therefore all events are initially valid\n            flag = np.ones(self.get_number_events(), dtype=bool)\n\n        for comp_val in mt_table:\n            id0 = np.logical_and(self.data['year'].astype(float) < comp_val[0],\n                                 self.data['magnitude'] < comp_val[1])\n            print(id0)\n            flag[id0] = False\n        if not np.all(flag):\n            self.purge_catalogue(flag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the bounding box of the catalogue", "response": "def get_bounding_box(self):\n        \"\"\"\n        Returns the bounding box of the catalogue\n\n        :returns: (West, East, South, North)\n        \"\"\"\n        return (np.min(self.data[\"longitude\"]),\n                np.max(self.data[\"longitude\"]),\n                np.min(self.data[\"latitude\"]),\n                np.max(self.data[\"latitude\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_observed_mmax_sigma(self, default=None):\n        if not isinstance(self.data['sigmaMagnitude'], np.ndarray):\n            obsmaxsig = default\n        else:\n            obsmaxsig = self.data['sigmaMagnitude'][\n                np.argmax(self.data['magnitude'])]\n        return obsmaxsig", "response": "returns the sigma for the maximum observed magnitude"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_decimal_time(self):\n        '''\n        Returns the time of the catalogue as a decimal\n        '''\n        return decimal_time(self.data['year'],\n                            self.data['month'],\n                            self.data['day'],\n                            self.data['hour'],\n                            self.data['minute'],\n                            self.data['second'])", "response": "Returns the time of the catalogue as a decimal time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsort the catalogue into chronological order", "response": "def sort_catalogue_chronologically(self):\n        '''\n        Sorts the catalogue into chronological order\n        '''\n        dec_time = self.get_decimal_time()\n        idx = np.argsort(dec_time)\n        if np.all((idx[1:] - idx[:-1]) > 0.):\n            # Catalogue was already in chronological order\n            return\n        self.select_catalogue_events(idx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef purge_catalogue(self, flag_vector):\n        '''\n        Purges present catalogue with invalid events defined by flag_vector\n\n        :param numpy.ndarray flag_vector:\n            Boolean vector showing if events are selected (True) or not (False)\n\n        '''\n        id0 = np.where(flag_vector)[0]\n        self.select_catalogue_events(id0)\n        self.get_number_events()", "response": "Purges present catalogue with invalid events defined by flag_vector"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect the events in the catalogue according to an indexing vector.", "response": "def select_catalogue_events(self, id0):\n        '''\n        Orders the events in the catalogue according to an indexing vector.\n\n        :param np.ndarray id0:\n            Pointer array indicating the locations of selected events\n        '''\n        for key in self.data:\n            if isinstance(\n                    self.data[key], np.ndarray) and len(self.data[key]) > 0:\n                # Dictionary element is numpy array - use logical indexing\n                self.data[key] = self.data[key][id0]\n            elif isinstance(\n                    self.data[key], list) and len(self.data[key]) > 0:\n                # Dictionary element is list\n                self.data[key] = [self.data[key][iloc] for iloc in id0]\n            else:\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the depth distribution of the earthquake catalogue to return a single histogram.", "response": "def get_depth_distribution(self, depth_bins, normalisation=False,\n                               bootstrap=None):\n        '''\n        Gets the depth distribution of the earthquake catalogue to return a\n        single histogram. Depths may be normalised. If uncertainties are found\n        in the catalogue the distrbution may be bootstrap sampled\n\n        :param numpy.ndarray depth_bins:\n             getBin edges for the depths\n\n        :param bool normalisation:\n            Choose to normalise the results such that the total contributions\n            sum to 1.0 (True) or not (False)\n\n        :param int bootstrap:\n            Number of bootstrap samples\n\n        :returns:\n            Histogram of depth values\n\n        '''\n        if len(self.data['depth']) == 0:\n            # If depth information is missing\n            raise ValueError('Depths missing in catalogue')\n\n        if len(self.data['depthError']) == 0:\n            self.data['depthError'] = np.zeros(self.get_number_events(),\n                                               dtype=float)\n\n        return bootstrap_histogram_1D(self.data['depth'],\n                                      depth_bins,\n                                      self.data['depthError'],\n                                      normalisation=normalisation,\n                                      number_bootstraps=bootstrap,\n                                      boundaries=(0., None))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the depth distribution of the catalogue as a probability mass.", "response": "def get_depth_pmf(self, depth_bins, default_depth=5.0, bootstrap=None):\n        \"\"\"\n        Returns the depth distribution of the catalogue as a probability mass\n        function\n        \"\"\"\n        if len(self.data['depth']) == 0:\n            # If depth information is missing\n            return PMF([(1.0, default_depth)])\n        # Get the depth distribution\n        depth_hist = self.get_depth_distribution(depth_bins,\n                                                 normalisation=True,\n                                                 bootstrap=bootstrap)\n        # If the histogram does not sum to 1.0 then remove the difference\n        # from the lowest bin\n        depth_hist = np.around(depth_hist, 3)\n        while depth_hist.sum() - 1.0:\n            depth_hist[-1] -= depth_hist.sum() - 1.0\n            depth_hist = np.around(depth_hist, 3)\n\n        pmf_list = []\n        for iloc, prob in enumerate(depth_hist):\n            pmf_list.append((prob,\n                             (depth_bins[iloc] + depth_bins[iloc + 1]) / 2.0))\n        return PMF(pmf_list)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a 2D magnitude - depth histogram for the catalogue.", "response": "def get_magnitude_depth_distribution(self, magnitude_bins, depth_bins,\n                                         normalisation=False, bootstrap=None):\n        '''\n        Returns a 2-D magnitude-depth histogram for the catalogue\n\n        :param numpy.ndarray magnitude_bins:\n             Bin edges for the magnitudes\n\n        :param numpy.ndarray depth_bins:\n            Bin edges for the depths\n\n        :param bool normalisation:\n            Choose to normalise the results such that the total contributions\n            sum to 1.0 (True) or not (False)\n\n        :param int bootstrap:\n            Number of bootstrap samples\n\n        :returns:\n            2D histogram of events in magnitude-depth bins\n        '''\n        if len(self.data['depth']) == 0:\n            # If depth information is missing\n            raise ValueError('Depths missing in catalogue')\n\n        if len(self.data['depthError']) == 0:\n            self.data['depthError'] = np.zeros(self.get_number_events(),\n                                               dtype=float)\n\n        if len(self.data['sigmaMagnitude']) == 0:\n            self.data['sigmaMagnitude'] = np.zeros(self.get_number_events(),\n                                                   dtype=float)\n\n        return bootstrap_histogram_2D(self.data['magnitude'],\n                                      self.data['depth'],\n                                      magnitude_bins,\n                                      depth_bins,\n                                      boundaries=[(0., None), (None, None)],\n                                      xsigma=self.data['sigmaMagnitude'],\n                                      ysigma=self.data['depthError'],\n                                      normalisation=normalisation,\n                                      number_bootstraps=bootstrap)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_magnitude_time_distribution(self, magnitude_bins, time_bins,\n                                        normalisation=False, bootstrap=None):\n        '''\n        Returns a 2-D histogram indicating the number of earthquakes in a\n        set of time-magnitude bins. Time is in decimal years!\n\n        :param numpy.ndarray magnitude_bins:\n             Bin edges for the magnitudes\n\n        :param numpy.ndarray time_bins:\n            Bin edges for the times\n\n        :param bool normalisation:\n            Choose to normalise the results such that the total contributions\n            sum to 1.0 (True) or not (False)\n\n        :param int bootstrap:\n            Number of bootstrap samples\n\n        :returns:\n            2D histogram of events in magnitude-year bins\n        '''\n        return bootstrap_histogram_2D(\n            self.get_decimal_time(),\n            self.data['magnitude'],\n            time_bins,\n            magnitude_bins,\n            xsigma=np.zeros(self.get_number_events()),\n            ysigma=self.data['sigmaMagnitude'],\n            normalisation=normalisation,\n            number_bootstraps=bootstrap)", "response": "Returns a 2D histogram of the number of earthquakes in the time - magnitude bins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef classical_bcr(riskinputs, riskmodel, param, monitor):\n    R = riskinputs[0].hazard_getter.num_rlzs\n    result = AccumDict(accum=numpy.zeros((R, 3), F32))\n    for ri in riskinputs:\n        for out in riskmodel.gen_outputs(ri, monitor):\n            for asset, (eal_orig, eal_retro, bcr) in zip(\n                    ri.assets, out['structural']):\n                aval = asset['value-structural']\n                result[asset['ordinal']][out.rlzi] = numpy.array([\n                    eal_orig * aval, eal_retro * aval, bcr])\n    return {'bcr_data': result}", "response": "Compute and return the average losses for each asset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexposing the outputs of the current object to the datastore.", "response": "def expose_outputs(dstore, owner=getpass.getuser(), status='complete'):\n    \"\"\"\n    Build a correspondence between the outputs in the datastore and the\n    ones in the database.\n\n    :param dstore: datastore\n    \"\"\"\n    oq = dstore['oqparam']\n    exportable = set(ekey[0] for ekey in export.export)\n    calcmode = oq.calculation_mode\n    dskeys = set(dstore) & exportable  # exportable datastore keys\n    dskeys.add('fullreport')\n    rlzs = dstore['csm_info'].rlzs\n    if len(rlzs) > 1:\n        dskeys.add('realizations')\n    if len(dstore['csm_info/sg_data']) > 1:  # export sourcegroups.csv\n        dskeys.add('sourcegroups')\n    hdf5 = dstore.hdf5\n    if 'hcurves-stats' in hdf5 or 'hcurves-rlzs' in hdf5:\n        if oq.hazard_stats() or oq.individual_curves or len(rlzs) == 1:\n            dskeys.add('hcurves')\n        if oq.uniform_hazard_spectra:\n            dskeys.add('uhs')  # export them\n        if oq.hazard_maps:\n            dskeys.add('hmaps')  # export them\n    if 'avg_losses-stats' in dstore or (\n            'avg_losses-rlzs' in dstore and len(rlzs)):\n        dskeys.add('avg_losses-stats')\n    if 'curves-rlzs' in dstore and len(rlzs) == 1:\n        dskeys.add('loss_curves-rlzs')\n    if 'curves-stats' in dstore and len(rlzs) > 1:\n        dskeys.add('loss_curves-stats')\n    if oq.conditional_loss_poes:  # expose loss_maps outputs\n        if 'loss_curves-stats' in dstore:\n            dskeys.add('loss_maps-stats')\n    if 'all_loss_ratios' in dskeys:\n        dskeys.remove('all_loss_ratios')  # export only specific IDs\n    if 'ruptures' in dskeys and 'scenario' in calcmode:\n        exportable.remove('ruptures')  # do not export, as requested by Vitor\n    if 'rup_loss_table' in dskeys:  # keep it hidden for the moment\n        dskeys.remove('rup_loss_table')\n    if 'hmaps' in dskeys and not oq.hazard_maps:\n        dskeys.remove('hmaps')  # do not export the hazard maps\n    if logs.dbcmd('get_job', dstore.calc_id) is None:\n        # the calculation has not been imported in the db yet\n        logs.dbcmd('import_job', dstore.calc_id, oq.calculation_mode,\n                   oq.description + ' [parent]', owner, status,\n                   oq.hazard_calculation_id, dstore.datadir)\n    keysize = []\n    for key in sorted(dskeys & exportable):\n        try:\n            size_mb = dstore.get_attr(key, 'nbytes') / MB\n        except (KeyError, AttributeError):\n            size_mb = None\n        keysize.append((key, size_mb))\n    ds_size = os.path.getsize(dstore.filename) / MB\n    logs.dbcmd('create_outputs', dstore.calc_id, keysize, ds_size)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef raiseMasterKilled(signum, _stack):\n    # Disable further CTRL-C to allow tasks revocation when Celery is used\n    if OQ_DISTRIBUTE.startswith('celery'):\n        signal.signal(signal.SIGINT, inhibitSigInt)\n\n    msg = 'Received a signal %d' % signum\n    if signum in (signal.SIGTERM, signal.SIGINT):\n        msg = 'The openquake master process was killed manually'\n\n    # kill the calculation only if os.getppid() != _PPID, i.e. the controlling\n    # terminal died; in the workers, do nothing\n    # NB: there is no SIGHUP on Windows\n    if hasattr(signal, 'SIGHUP'):\n        if signum == signal.SIGHUP:\n            if os.getppid() == _PPID:\n                return\n            else:\n                msg = 'The openquake master lost its controlling terminal'\n\n    raise MasterKilled(msg)", "response": "Raises a MasterKilled exception with an appropriate error message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef job_from_file(job_ini, job_id, username, **kw):\n    hc_id = kw.get('hazard_calculation_id')\n    try:\n        oq = readinput.get_oqparam(job_ini, hc_id=hc_id)\n    except Exception:\n        logs.dbcmd('finish', job_id, 'failed')\n        raise\n    if 'calculation_mode' in kw:\n        oq.calculation_mode = kw.pop('calculation_mode')\n    if 'description' in kw:\n        oq.description = kw.pop('description')\n    if 'exposure_file' in kw:  # hack used in commands.engine\n        fnames = kw.pop('exposure_file').split()\n        if fnames:\n            oq.inputs['exposure'] = fnames\n        elif 'exposure' in oq.inputs:\n            del oq.inputs['exposure']\n    logs.dbcmd('update_job', job_id,\n               dict(calculation_mode=oq.calculation_mode,\n                    description=oq.description,\n                    user_name=username,\n                    hazard_calculation_id=hc_id))\n    return oq", "response": "Create a full job profile from a job. ini file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef poll_queue(job_id, pid, poll_time):\n    if config.distribution.serialize_jobs:\n        first_time = True\n        while True:\n            jobs = logs.dbcmd(GET_JOBS)\n            failed = [job.id for job in jobs if not psutil.pid_exists(job.pid)]\n            if failed:\n                logs.dbcmd(\"UPDATE job SET status='failed', is_running=0 \"\n                           \"WHERE id in (?X)\", failed)\n            elif any(job.id < job_id for job in jobs):\n                if first_time:\n                    logs.LOG.warn('Waiting for jobs %s', [j.id for j in jobs])\n                    logs.dbcmd('update_job', job_id,\n                               {'status': 'submitted', 'pid': pid})\n                    first_time = False\n                time.sleep(poll_time)\n            else:\n                break\n    logs.dbcmd('update_job', job_id, {'status': 'executing', 'pid': _PID})", "response": "Poll the queue of executing and submitted jobs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a single calculation.", "response": "def run_calc(job_id, oqparam, exports, hazard_calculation_id=None, **kw):\n    \"\"\"\n    Run a calculation.\n\n    :param job_id:\n        ID of the current job\n    :param oqparam:\n        :class:`openquake.commonlib.oqvalidation.OqParam` instance\n    :param exports:\n        A comma-separated string of export types.\n    \"\"\"\n    setproctitle('oq-job-%d' % job_id)\n    calc = base.calculators(oqparam, calc_id=job_id)\n    logging.info('%s running %s [--hc=%s]',\n                 getpass.getuser(),\n                 calc.oqparam.inputs['job_ini'],\n                 calc.oqparam.hazard_calculation_id)\n    logging.info('Using engine version %s', __version__)\n    msg = check_obsolete_version(oqparam.calculation_mode)\n    if msg:\n        logs.LOG.warn(msg)\n    if OQ_DISTRIBUTE.startswith(('celery', 'zmq')):\n        set_concurrent_tasks_default(job_id)\n    calc.from_engine = True\n    tb = 'None\\n'\n    try:\n        if not oqparam.hazard_calculation_id:\n            if 'input_zip' in oqparam.inputs:  # starting from an archive\n                with open(oqparam.inputs['input_zip'], 'rb') as arch:\n                    data = numpy.array(arch.read())\n            else:\n                logs.LOG.info('zipping the input files')\n                bio = io.BytesIO()\n                oqzip.zip_job(oqparam.inputs['job_ini'], bio, (), oqparam,\n                              logging.debug)\n                data = numpy.array(bio.getvalue())\n                del bio\n            calc.datastore['input/zip'] = data\n            calc.datastore.set_attrs('input/zip', nbytes=data.nbytes)\n            del data  # save memory\n\n        poll_queue(job_id, _PID, poll_time=15)\n        t0 = time.time()\n        calc.run(exports=exports,\n                 hazard_calculation_id=hazard_calculation_id,\n                 close=False, **kw)\n        logs.LOG.info('Exposing the outputs to the database')\n        expose_outputs(calc.datastore)\n        duration = time.time() - t0\n        calc._monitor.flush()\n        records = views.performance_view(calc.datastore)\n        logs.dbcmd('save_performance', job_id, records)\n        calc.datastore.close()\n        logs.LOG.info('Calculation %d finished correctly in %d seconds',\n                      job_id, duration)\n        logs.dbcmd('finish', job_id, 'complete')\n    except BaseException as exc:\n        if isinstance(exc, MasterKilled):\n            msg = 'aborted'\n        else:\n            msg = 'failed'\n        tb = traceback.format_exc()\n        try:\n            logs.LOG.critical(tb)\n            logs.dbcmd('finish', job_id, msg)\n        except BaseException:  # an OperationalError may always happen\n            sys.stderr.write(tb)\n        raise\n    finally:\n        # if there was an error in the calculation, this part may fail;\n        # in such a situation, we simply log the cleanup error without\n        # taking further action, so that the real error can propagate\n        try:\n            if OQ_DISTRIBUTE.startswith('celery'):\n                celery_cleanup(TERMINATE)\n        except BaseException:\n            # log the finalization error only if there is no real error\n            if tb == 'None\\n':\n                logs.LOG.error('finalizing', exc_info=True)\n    return calc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef version_triple(tag):\n    groups = re.match(r'v?(\\d+)\\.(\\d+)\\.(\\d+)', tag).groups()\n    return tuple(int(n) for n in groups)", "response": "returns a triple of integers from a version tag"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_obsolete_version(calculation_mode='WebUI'):\n    if os.environ.get('JENKINS_URL') or os.environ.get('TRAVIS'):\n        # avoid flooding our API server with requests from CI systems\n        return\n\n    headers = {'User-Agent': 'OpenQuake Engine %s;%s;%s;%s' %\n               (__version__, calculation_mode, platform.platform(),\n                config.distribution.oq_distribute)}\n    try:\n        req = Request(OQ_API + '/engine/latest', headers=headers)\n        # NB: a timeout < 1 does not work\n        data = urlopen(req, timeout=1).read()  # bytes\n        tag_name = json.loads(decode(data))['tag_name']\n        current = version_triple(__version__)\n        latest = version_triple(tag_name)\n    except Exception:  # page not available or wrong version tag\n        return\n    if current < latest:\n        return ('Version %s of the engine is available, but you are '\n                'still using version %s' % (tag_name, __version__))\n    else:\n        return ''", "response": "Check if there is a newer version of the engine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding a unicode or bytes object into a list of bytes", "response": "def encode(val):\n    \"\"\"\n    Encode a string assuming the encoding is UTF-8.\n\n    :param: a unicode or bytes object\n    :returns: bytes\n    \"\"\"\n    if isinstance(val, (list, tuple)):  # encode a list or tuple of strings\n        return [encode(v) for v in val]\n    elif isinstance(val, str):\n        return val.encode('utf-8')\n    else:\n        # assume it was an already encoded object\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dbserver(cmd, dbhostport=None,\n             dbpath=os.path.expanduser(config.dbserver.file)):\n    \"\"\"\n    start/stop/restart the database server, or return its status\n    \"\"\"\n    if config.dbserver.multi_user and getpass.getuser() != 'openquake':\n        sys.exit('oq dbserver only works in single user mode')\n\n    status = dbs.get_status()\n    if cmd == 'status':\n        print('dbserver ' + status)\n    elif cmd == 'stop':\n        if status == 'running':\n            pid = logs.dbcmd('getpid')\n            os.kill(pid, signal.SIGINT)  # this is trapped by the DbServer\n        else:\n            print('dbserver already stopped')\n    elif cmd == 'start':\n        if status == 'not-running':\n            dbs.run_server(dbpath, dbhostport)\n        else:\n            print('dbserver already running')\n    elif cmd == 'restart':\n        if status == 'running':\n            pid = logs.dbcmd('getpid')\n            os.kill(pid, signal.SIGINT)\n        dbs.run_server(dbpath, dbhostport)", "response": "start stop or restart the database server"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the pyroclastic cloud and the assets", "response": "def plot_pyro(calc_id=-1):\n    \"\"\"\n    Plot the pyroclastic cloud and the assets\n    \"\"\"\n    # NB: matplotlib is imported inside since it is a costly import\n    import matplotlib.pyplot as p\n    dstore = util.read(calc_id)\n    sitecol = dstore['sitecol']\n    asset_risk = dstore['asset_risk'].value\n    pyro, = numpy.where(dstore['multi_peril']['PYRO'] == 1)\n    lons = sitecol.lons[pyro]\n    lats = sitecol.lats[pyro]\n    p.scatter(lons, lats, marker='o', color='red')\n\n    building_pyro, = numpy.where(asset_risk['building-PYRO'] == 1)\n    lons = sitecol.lons[building_pyro]\n    lats = sitecol.lats[building_pyro]\n    p.scatter(lons, lats, marker='.', color='green')\n    p.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of nodes that are part of the griddedSurface.", "response": "def surface_nodes(self):\n        \"\"\"\n        :param points: a list of Point objects\n        :returns: a Node of kind 'griddedSurface'\n        \"\"\"\n        line = []\n        for point in self.mesh:\n            line.append(point.longitude)\n            line.append(point.latitude)\n            line.append(point.depth)\n        return [Node('griddedSurface', nodes=[Node('gml:posList', {}, line)])]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the surface boundaries of the species.", "response": "def get_surface_boundaries(self):\n        \"\"\"\n        :returns: (min_max lons, min_max lats)\n        \"\"\"\n        min_lon, min_lat, max_lon, max_lat = self.get_bounding_box()\n        return [[min_lon, max_lon]], [[min_lat, max_lat]]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing coordinates of surface middle point.", "response": "def get_middle_point(self):\n        \"\"\"\n        Compute coordinates of surface middle point.\n\n        The actual definition of ``middle point`` depends on the type of\n        surface geometry.\n\n        :return:\n            instance of :class:`openquake.hazardlib.geo.point.Point`\n            representing surface middle point.\n        \"\"\"\n        lons = self.mesh.lons.squeeze()\n        lats = self.mesh.lats.squeeze()\n        depths = self.mesh.depths.squeeze()\n        lon_bar = lons.mean()\n        lat_bar = lats.mean()\n        idx = np.argmin((lons - lon_bar)**2 + (lats - lat_bar)**2)\n        return Point(lons[idx], lats[idx], depths[idx])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n\n        # get mean and std using the superclass\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n        \n        A08 = self.A08_COEFFS[imt]\n        f_ena = 10.0 ** (A08[\"c\"] + A08[\"d\"] * dists.rjb)\n\n        return np.log(np.exp(mean)*f_ena), stddevs", "response": "This method returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_country_code(longname):\n    mo = re.search(REGEX, longname, re.I)\n    if mo is None:\n        raise ValueError('Could not find a valid country in %s' % longname)\n    return country2code[COUNTRIES[mo.lastindex - 1]]", "response": "Returns the code of the country contained in longname or a ValuError\n    if the longname is not a valid country."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary E??_ - > country", "response": "def from_exposures(expnames):\n    \"\"\"\n    :returns: a dictionary E??_ -> country\n    \"\"\"\n    dic = {}\n    for i, expname in enumerate(expnames, 1):\n        cc = get_country_code(expname)\n        dic['E%02d_' % i] = cc\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a single modification to an MFD parameters.", "response": "def modify(self, modification, parameters):\n        \"\"\"\n        Apply a single modification to an MFD parameters.\n\n        Reflects the modification method and calls it passing ``parameters``\n        as keyword arguments. See also :attr:`MODIFICATIONS`.\n\n        Modifications can be applied one on top of another. The logic\n        of stacking modifications is up to a specific MFD implementation.\n\n        :param modification:\n            String name representing the type of modification.\n        :param parameters:\n            Dictionary of parameters needed for modification.\n        :raises ValueError:\n            If ``modification`` is missing from :attr:`MODIFICATIONS`.\n        \"\"\"\n        if modification not in self.MODIFICATIONS:\n            raise ValueError('Modification %s is not supported by %s' %\n                             (modification, type(self).__name__))\n        meth = getattr(self, 'modify_%s' % modification)\n        meth(**parameters)\n        self.check_constraints()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n        # Implements mean model (equation 12)\n        mean = (self._compute_magnitude(rup, C) +\n                self._compute_distance(dists, C) +\n                self._get_site_amplification(sites, rup, C) +\n                self._get_mechanism(rup, C))\n\n        stddevs = self._get_stddevs(rup, np.exp(mean), stddev_types, sites)\n\n        return mean, stddevs", "response": "This method computes the mean and standard deviation of the resource table entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the standard deviations as defined in table 1 p. 200.", "response": "def _get_stddevs(self, rup, arias, stddev_types, sites):\n        \"\"\"\n        Return standard deviations as defined in table 1, p. 200.\n        \"\"\"\n        stddevs = []\n        # Magnitude dependent inter-event term (Eq. 13)\n        if rup.mag < 4.7:\n            tau = 0.611\n        elif rup.mag > 7.6:\n            tau = 0.475\n        else:\n            tau = 0.611 - 0.047 * (rup.mag - 4.7)\n\n        # Retrieve site-class dependent sigma\n        sigma1, sigma2 = self._get_intra_event_sigmas(sites)\n        sigma = np.copy(sigma1)\n        # Implements the nonlinear intra-event sigma (Eq. 14)\n        idx = arias >= 0.125\n        sigma[idx] = sigma2[idx]\n        idx = np.logical_and(arias > 0.013, arias < 0.125)\n        sigma[idx] = sigma1[idx] - 0.106 * (np.log(arias[idx]) -\n                                            np.log(0.0132))\n\n        sigma_total = np.sqrt(tau ** 2. + sigma ** 2.)\n\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(sigma_total)\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(sigma)\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(tau * np.ones_like(sites.vs30))\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the intra - event term nonlinear and dependent on both the site class and the expected ground motion.", "response": "def _get_intra_event_sigmas(self, sites):\n        \"\"\"\n        The intra-event term nonlinear and dependent on both the site class\n        and the expected ground motion. In this case the sigma coefficients\n        are determined from the site class as described below Eq. 14\n        \"\"\"\n        sigma1 = 1.18 * np.ones_like(sites.vs30)\n        sigma2 = 0.94 * np.ones_like(sites.vs30)\n\n        idx1 = np.logical_and(sites.vs30 >= 360.0, sites.vs30 < 760.0)\n        idx2 = sites.vs30 < 360.0\n        sigma1[idx1] = 1.17\n        sigma2[idx1] = 0.93\n        sigma1[idx2] = 0.96\n        sigma2[idx2] = 0.73\n        return sigma1, sigma2"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the magnitude of the record in the system", "response": "def _compute_magnitude(self, rup, C):\n        \"\"\"\n        Compute the first term of the equation described on p. 1144:\n\n        ``c1 + c2 * (M - 6) + c3 * log(M / 6)``\n        \"\"\"\n        return C['c1'] + C['c2'] * (rup.mag - 6.0) +\\\n            (C['c3'] * np.log(rup.mag / 6.0))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the second term of the equation described on p. 1144.", "response": "def _compute_distance(self, dists, C):\n        \"\"\"\n        Compute the second term of the equation described on p. 1144:\n\n        `` c4 * np.log(sqrt(R ** 2. + h ** 2.)\n        \"\"\"\n        return C[\"c4\"] * np.log(np.sqrt(dists.rrup ** 2. + C[\"h\"] ** 2.))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_site_amplification(self, sites, rup, C):\n        Sc, Sd = self._get_site_type_dummy_variables(sites)\n        return (C[\"s11\"] + C[\"s12\"] * (rup.mag - 6.0)) * Sc +\\\n            (C[\"s21\"] + C[\"s22\"] * (rup.mag - 6.0)) * Sd", "response": "Returns the amplification term in equation 1 p. 1144"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_site_type_dummy_variables(self, sites):\n        Sc = np.zeros_like(sites.vs30)\n        Sd = np.zeros_like(sites.vs30)\n        # Soft soil; Vs30 < 360 m/s. Page 199.\n        Sd[sites.vs30 < 360.0] = 1\n        # Stiff soil 360 <= Vs30 < 760\n        Sc[np.logical_and(sites.vs30 >= 360.0, sites.vs30 < 760.0)] = 1\n\n        return Sc, Sd", "response": "Get site type dummy variables Sc Sd"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_mechanism(self, rup, C):\n        Fn, Fr = self._get_fault_type_dummy_variables(rup)\n        return (C['f1'] * Fn) + (C['f2'] * Fr)", "response": "Returns the mechanism of the logarithm of the fault"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the fault type dummy variables for faulting types.", "response": "def _get_fault_type_dummy_variables(self, rup):\n        \"\"\"\n        The original classification considers four style of faulting categories\n        (normal, strike-slip, reverse-oblique and reverse).\n        \"\"\"\n\n        Fn, Fr = 0, 0\n        if rup.rake >= -112.5 and rup.rake <= -67.5:\n            # normal\n            Fn = 1\n        elif rup.rake >= 22.5 and rup.rake <= 157.5:\n            # Joins both the reverse and reverse-oblique categories\n            Fr = 1\n        return Fn, Fr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate centred z1. 0 based on the California model", "response": "def california_basin_model(vs30):\n    \"\"\"\n    Returns the centred z1.0 (mu_z1) based on the California model\n    (equation 11)\n    \"\"\"\n    coeff = 570.94 ** 4.0\n    model = (-7.15 / 4.0) * np.log(\n        ((vs30 ** 4.0) + coeff) / ((1360.0 ** 4.0) + coeff)\n        ) - np.log(1000.)\n    return np.exp(model)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the centred z1. 0 model based on the Japan model", "response": "def japan_basin_model(vs30):\n    \"\"\"\n    Returns the centred z1.0 (mu_z1) based on the Japan model\n    (equation 12)\n    \"\"\"\n    coeff = 412.39 ** 2.0\n    model = (-5.23 / 2.0) * np.log(\n        ((vs30 ** 2.0) + coeff) / ((1360.0 ** 2.0) + coeff)\n        ) - np.log(1000.)\n    return np.exp(model)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the mean and standard deviation of the resource table entry.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n        C_PGA = self.COEFFS[PGA()]\n        imt_per = 0 if imt.name == 'PGV' else imt.period\n        pga_rock = self._get_pga_on_rock(C_PGA, rup, dists)\n        mean = (self._get_magnitude_scaling_term(C, rup) +\n                self._get_path_scaling(C, dists, rup.mag) +\n                self._get_site_scaling(C, pga_rock, sites, imt_per, dists.rjb))\n        stddevs = self._get_stddevs(C, rup, dists, sites, stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the median PGA on rock given the magnitude and distance scaling", "response": "def _get_pga_on_rock(self, C, rup, dists):\n        \"\"\"\n        Returns the median PGA on rock, which is a sum of the\n        magnitude and distance scaling\n        \"\"\"\n        return np.exp(self._get_magnitude_scaling_term(C, rup) +\n                      self._get_path_scaling(C, dists, rup.mag))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the magnitude scaling term defined in equation 2", "response": "def _get_magnitude_scaling_term(self, C, rup):\n        \"\"\"\n        Returns the magnitude scling term defined in equation (2)\n        \"\"\"\n        dmag = rup.mag - C[\"Mh\"]\n        if rup.mag <= C[\"Mh\"]:\n            mag_term = (C[\"e4\"] * dmag) + (C[\"e5\"] * (dmag ** 2.0))\n        else:\n            mag_term = C[\"e6\"] * dmag\n        return self._get_style_of_faulting_term(C, rup) + mag_term"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_style_of_faulting_term(self, C, rup):\n        if np.abs(rup.rake) <= 30.0 or (180.0 - np.abs(rup.rake)) <= 30.0:\n            # strike-slip\n            return C[\"e1\"]\n        elif rup.rake > 30.0 and rup.rake < 150.0:\n            # reverse\n            return C[\"e3\"]\n        else:\n            # normal\n            return C[\"e2\"]", "response": "Returns the style of faulting term in the term table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_path_scaling(self, C, dists, mag):\n        rval = np.sqrt((dists.rjb ** 2.0) + (C[\"h\"] ** 2.0))\n        scaling = (C[\"c1\"] + C[\"c2\"] * (mag - self.CONSTS[\"Mref\"])) *\\\n            np.log(rval / self.CONSTS[\"Rref\"])\n        return scaling + ((C[\"c3\"] + C[\"Dc3\"]) * (rval - self.CONSTS[\"Rref\"]))", "response": "Returns the path scaling term given by equation 3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the site - scaling term broken down into a linear scaling nonlinear scaling and basin depth scaling term broken down into a nonlinear scaling and basin depth scaling term.", "response": "def _get_site_scaling(self, C, pga_rock, sites, period, rjb):\n        \"\"\"\n        Returns the site-scaling term (equation 5), broken down into a\n        linear scaling, a nonlinear scaling and a basin scaling term\n        \"\"\"\n        flin = self._get_linear_site_term(C, sites.vs30)\n        fnl = self._get_nonlinear_site_term(C, sites.vs30, pga_rock)\n        fbd = self._get_basin_depth_term(C, sites, period)\n        return flin + fnl + fbd"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the linear site scaling term", "response": "def _get_linear_site_term(self, C, vs30):\n        \"\"\"\n        Returns the linear site scaling term (equation 6)\n        \"\"\"\n        flin = vs30 / self.CONSTS[\"Vref\"]\n        flin[vs30 > C[\"Vc\"]] = C[\"Vc\"] / self.CONSTS[\"Vref\"]\n        return C[\"c\"] * np.log(flin)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the nonlinear site scaling term in equation 7", "response": "def _get_nonlinear_site_term(self, C, vs30, pga_rock):\n        \"\"\"\n        Returns the nonlinear site scaling term (equation 7)\n        \"\"\"\n        v_s = np.copy(vs30)\n        v_s[vs30 > 760.] = 760.\n        # Nonlinear controlling parameter (equation 8)\n        f_2 = C[\"f4\"] * (np.exp(C[\"f5\"] * (v_s - 360.)) -\n                         np.exp(C[\"f5\"] * 400.))\n        fnl = self.CONSTS[\"f1\"] + f_2 * np.log((pga_rock + self.CONSTS[\"f3\"]) /\n                                               self.CONSTS[\"f3\"])\n        return fnl"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_basin_depth_term(self, C, sites, period):\n        return np.zeros(len(sites.vs30), dtype=float)", "response": "Returns the basin depth term in the case of base model the basin depth term is switched off."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_stddevs(self, C, rup, dists, sites, stddev_types):\n        stddevs = []\n        num_sites = len(sites.vs30)\n        tau = self._get_inter_event_tau(C, rup.mag, num_sites)\n        phi = self._get_intra_event_phi(C,\n                                        rup.mag,\n                                        dists.rjb,\n                                        sites.vs30,\n                                        num_sites)\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt((tau ** 2.0) + (phi ** 2.0)))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(phi)\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(tau)\n        return stddevs", "response": "Returns the standard deviations from the aleatory uncertainty terms described in equations 13 to 17 and 17 to 8."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the inter - event standard deviation in terms of the magnitude of the logarithmic event system.", "response": "def _get_inter_event_tau(self, C, mag, num_sites):\n        \"\"\"\n        Returns the inter-event standard deviation (tau), which is dependent\n        on magnitude\n        \"\"\"\n        base_vals = np.zeros(num_sites)\n        if mag <= 4.5:\n            return base_vals + C[\"t1\"]\n        elif mag >= 5.5:\n            return base_vals + C[\"t2\"]\n        else:\n            return base_vals + C[\"t1\"] + (C[\"t2\"] - C[\"t1\"]) * (mag - 4.5)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_intra_event_phi(self, C, mag, rjb, vs30, num_sites):\n        base_vals = np.zeros(num_sites)\n        # Magnitude Dependent phi (Equation 17)\n        if mag <= 4.5:\n            base_vals += C[\"f1\"]\n        elif mag >= 5.5:\n            base_vals += C[\"f2\"]\n        else:\n            base_vals += (C[\"f1\"] + (C[\"f2\"] - C[\"f1\"]) * (mag - 4.5))\n        # Distance dependent phi (Equation 16)\n        idx1 = rjb > C[\"R2\"]\n        base_vals[idx1] += C[\"DfR\"]\n        idx2 = np.logical_and(rjb > C[\"R1\"], rjb <= C[\"R2\"])\n        base_vals[idx2] += (C[\"DfR\"] * (np.log(rjb[idx2] / C[\"R1\"]) /\n                                        np.log(C[\"R2\"] / C[\"R1\"])))\n        # Site-dependent phi (Equation 15)\n        idx1 = vs30 <= self.CONSTS[\"v1\"]\n        base_vals[idx1] -= C[\"DfV\"]\n        idx2 = np.logical_and(vs30 >= self.CONSTS[\"v1\"],\n                              vs30 <= self.CONSTS[\"v2\"])\n        base_vals[idx2] -= (\n            C[\"DfV\"] * (np.log(self.CONSTS[\"v2\"] / vs30[idx2]) /\n                        np.log(self.CONSTS[\"v2\"] / self.CONSTS[\"v1\"])))\n        return base_vals", "response": "Returns the intra - event standard deviation of the class entry in the site."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_basin_depth_term(self, C, sites, period):\n        if period < 0.65:\n            f_dz1 = np.zeros(len(sites.vs30), dtype=float)\n        else:\n            f_dz1 = C[\"f7\"] + np.zeros(len(sites.vs30), dtype=float)\n            f_ratio = C[\"f7\"] / C[\"f6\"]\n            dz1 = (sites.z1pt0 / 1000.0) - california_basin_model(sites.vs30)\n            idx = dz1 <= f_ratio\n            f_dz1[idx] = C[\"f6\"] * dz1[idx]\n        return f_dz1", "response": "Returns the basin depth term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        C = self.COEFFS[imt]\n\n        M = rup.mag - 6\n        R = np.sqrt(dists.rjb ** 2 + C['h'] ** 2)\n\n        # In the original formulation of the GMPE, distinction is only made\n        # between rock and soil sites, which I assumed separated by the Vs30\n        # value of 910m/s (see equation 5 of the paper)\n        gamma = np.array([0 if v > 910. else 1 for v in sites.vs30])\n\n        mean = np.zeros_like(R)\n\n        mean += C['b1'] + \\\n                C['b2'] * M + \\\n                C['b3'] * M ** 2 + \\\n                C['b5'] * np.log10(R) + \\\n                C['b6'] * gamma\n\n        # Convert from base 10 to base e\n        mean /= np.log10(np.e)\n\n        # Converting PSV to PSA\n        if imt != PGA() and imt != PGV():\n            omega = 2.*np.pi/imt.period\n            mean += np.log(omega/(gravity*100))\n\n        # Computing standard deviation\n        stddevs = self._get_stddevs(C, stddev_types,  dists.rjb.shape[0])\n\n        # Convert from base 10 to base e\n        stddevs = [sd/np.log10(np.e) for sd in stddevs]\n\n        return mean, stddevs", "response": "This method calculates the mean and standard deviation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the standard deviation as defined in equation 7 of the paper.", "response": "def _get_stddevs(self, C, stddev_types, num_sites):\n        \"\"\"\n        Return total standard deviation.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        if self.DEFINED_FOR_INTENSITY_MEASURE_COMPONENT == 'Random horizontal':\n            # Using equation 8 of the paper,\n            # corrected as indicated in the erratum\n            Sr = np.sqrt(C['SlZ']**2 + (C['S3']/np.sqrt(2))**2)\n        else:\n            Sr = C['SlZ']\n\n        stddevs = [np.zeros(num_sites) + Sr for _ in stddev_types]\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_min_max_mag(self):\n        m1s, m2s = [], []\n        for mfd in self:\n            m1, m2 = mfd.get_min_max_mag()\n            m1s.append(m1)\n            m2s.append(m2)\n        return min(m1s), max(m2s)", "response": "returns the minimum and maximum magnitudes from the underlying MFDs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying a modification to the underlying point sources with the specified parameters.", "response": "def modify(self, modification, parameters):\n        \"\"\"\n        Apply a modification to the underlying point sources, with the\n        same parameters for all sources\n        \"\"\"\n        for src in self:\n            src.modify(modification, parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes mean value as in subroutine getGeom in hazgridXnga2. f", "response": "def _compute_mean(self, C, mag, ztor, rrup):\n        \"\"\"\n        Compute mean value as in ``subroutine getGeom`` in ``hazgridXnga2.f``\n        \"\"\"\n        gc0 = 0.2418\n        ci = 0.3846\n        gch = 0.00607\n        g4 = 1.7818\n        ge = 0.554\n        gm = 1.414\n\n        mean = (\n            gc0 + ci + ztor * gch + C['gc1'] +\n            gm * mag + C['gc2'] * (10 - mag) ** 3 +\n            C['gc3'] * np.log(rrup + g4 * np.exp(ge * mag))\n        )\n\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef abort(job_id):\n    job = logs.dbcmd('get_job', job_id)  # job_id can be negative\n    if job is None:\n        print('There is no job %d' % job_id)\n        return\n    elif job.status not in ('executing', 'running'):\n        print('Job %d is %s' % (job.id, job.status))\n        return\n    name = 'oq-job-%d' % job.id\n    for p in psutil.process_iter():\n        if p.name() == name:\n            try:\n                os.kill(p.pid, signal.SIGTERM)\n                logs.dbcmd('set_status', job.id, 'aborted')\n                print('Job %d aborted' % job.id)\n            except Exception as exc:\n                print(exc)\n            break\n    else:  # no break\n        # set job as failed if it is set as 'executing' or 'running' in the db\n        # but the corresponding process is not running anymore\n        logs.dbcmd('set_status', job.id, 'failed')\n        print('Unable to find a process for job %d,'\n              ' setting it as failed' % job.id)", "response": "Abort the given job_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_parentparser(parser, description=None, help=True):\n    if parser is None:\n        return argparse.ArgumentParser(\n            description=description, add_help=help)\n    elif hasattr(parser, 'parentparser'):\n        return parser.parentparser\n    else:\n        return parser", "response": "Returns the parent parser if it is None or the parser itself."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a script that combines several different scripts and dispatches to the subparsers depending on the first argument.", "response": "def compose(scripts, name='main', description=None, prog=None,\n            version=None):\n    \"\"\"\n    Collects together different scripts and builds a single\n    script dispatching to the subparsers depending on\n    the first argument, i.e. the name of the subparser to invoke.\n\n    :param scripts: a list of script instances\n    :param name: the name of the composed parser\n    :param description: description of the composed parser\n    :param prog: name of the script printed in the usage message\n    :param version: version of the script printed with --version\n    \"\"\"\n    assert len(scripts) >= 1, scripts\n    parentparser = argparse.ArgumentParser(\n        description=description, add_help=False)\n    parentparser.add_argument(\n        '--version', '-v', action='version', version=version)\n    subparsers = parentparser.add_subparsers(\n        help='available subcommands; use %s help <subcmd>' % prog,\n        prog=prog)\n\n    def gethelp(cmd=None):\n        if cmd is None:\n            print(parentparser.format_help())\n            return\n        subp = subparsers._name_parser_map.get(cmd)\n        if subp is None:\n            print('No help for unknown command %r' % cmd)\n        else:\n            print(subp.format_help())\n    help_script = Script(gethelp, 'help', help=False)\n    progname = '%s ' % prog if prog else ''\n    help_script.arg('cmd', progname + 'subcommand')\n    for s in list(scripts) + [help_script]:\n        subp = subparsers.add_parser(s.name, description=s.description)\n        for args, kw in s.all_arguments:\n            subp.add_argument(*args, **kw)\n        subp.set_defaults(_func=s.func)\n\n    def main(**kw):\n        try:\n            func = kw.pop('_func')\n        except KeyError:\n            parentparser.print_usage()\n        else:\n            return func(**kw)\n    main.__name__ = name\n    return Script(main, name, parentparser)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add(self, name, *args, **kw):\n        argname = list(self.argdict)[self._argno]\n        if argname != name:\n            raise NameError(\n                'Setting argument %s, but it should be %s' % (name, argname))\n        self._group.add_argument(*args, **kw)\n        self.all_arguments.append((args, kw))\n        self.names.append(name)\n        self._argno += 1", "response": "Add an argument to the underlying parser and grow the list\n       . all_arguments and the set. names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef arg(self, name, help, type=None, choices=None, metavar=None,\n            nargs=None):\n        \"\"\"Describe a positional argument\"\"\"\n        kw = dict(help=help, type=type, choices=choices, metavar=metavar,\n                  nargs=nargs)\n        default = self.argdict[name]\n        if default is not NODEFAULT:\n            kw['nargs'] = nargs or '?'\n            kw['default'] = default\n            kw['help'] = kw['help'] + ' [default: %s]' % repr(default)\n        self._add(name, name, **kw)", "response": "Describe a positional argument"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_arguments(self):\n        for name, default in self.argdict.items():\n            if name not in self.names and default is NODEFAULT:\n                raise NameError('Missing argparse specification for %r' % name)", "response": "Make sure all arguments have a specification"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the argv list and call the function underlying the script.", "response": "def callfunc(self, argv=None):\n        \"\"\"\n        Parse the argv list and extract a dictionary of arguments which\n        is then passed to  the function underlying the script.\n        \"\"\"\n        if not self.checked:\n            self.check_arguments()\n            self.checked = True\n        namespace = self.parentparser.parse_args(argv or sys.argv[1:])\n        return self.func(**vars(namespace))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the cumulative value of a specific entry in the system.", "response": "def cumulative_value(self, slip_moment, mmax, mag_value, bbar, dbar):\n        '''\n        Returns the rate of events with M > mag_value\n\n        :param float slip_moment:\n        :param float slip_moment:\n            Product of slip (cm/yr) * Area (cm ^ 2) * shear_modulus (dyne-cm)\n        :param float mmax:\n            Maximum magnitude\n        :param float mag_value:\n            Magnitude value\n        :param float bbar:\n            \\bar{b} parameter (effectively = b * log(10.))\n        :param float dbar:\n            \\bar{d} parameter\n        '''\n        delta_m = mmax - mag_value\n        a_1 = self._get_a1(bbar, dbar, slip_moment, mmax)\n        return a_1 * np.exp(bbar * (delta_m)) * (delta_m > 0.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_a1(bbar, dbar, slip_moment, mmax):\n        return ((dbar - bbar) / dbar) * (slip_moment / _scale_moment(mmax))", "response": "Returns the A1 term in the range bbar dbar slip_moment and mmax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef incremental_value(self, slip_moment, mmax, mag_value, bbar, dbar):\n        delta_m = mmax - mag_value\n        dirac_term = np.zeros_like(mag_value)\n        dirac_term[np.fabs(delta_m) < 1.0E-12] = 1.0\n        a_1 = self._get_a1(bbar, dbar, slip_moment, mmax)\n        return a_1 * (bbar * np.exp(bbar * delta_m) * (delta_m > 0.0)) +\\\n            a_1 * dirac_term", "response": "Calculates the incremental rate of earthquakes with M = mag_value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the cumulative value of a specific entry in the system.", "response": "def cumulative_value(self, slip_moment, mmax, mag_value, bbar, dbar):\n        '''\n        Returns the rate of events with M > mag_value\n\n        :param float slip_moment:\n            Product of slip (cm/yr) * Area (cm ^ 2) * shear_modulus (dyne-cm)\n        :param float mmax:\n            Maximum magnitude\n        :param float mag_value:\n            Magnitude value\n        :param float bbar:\n            \\bar{b} parameter (effectively = b * log(10.))\n        :param float dbar:\n            \\bar{d} parameter\n        '''\n        delta_m = mmax - mag_value\n        a_2 = self._get_a2(bbar, dbar, slip_moment, mmax)\n        return a_2 * (np.exp(bbar * delta_m) - 1.) * (delta_m > 0.0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_a2(bbar, dbar, slip_moment, mmax):\n        return ((dbar - bbar) / bbar) * (slip_moment / _scale_moment(mmax))", "response": "Returns the A2 value defined in II. 4 of Table 2\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cumulative_value(self, slip_moment, mmax, mag_value, bbar, dbar):\n        '''\n        Returns the rate of events with M > mag_value\n\n        :param float slip_moment:\n            Product of slip (cm/yr) * Area (cm ^ 2) * shear_modulus (dyne-cm)\n        :param float mmax:\n            Maximum magnitude\n        :param float mag_value:\n            Magnitude value\n        :param float bbar:\n            \\bar{b} parameter (effectively = b * log(10.))\n        :param float dbar:\n            \\bar{d} parameter\n        '''\n        delta_m = mmax - mag_value\n        a_3 = self._get_a3(bbar, dbar, slip_moment, mmax)\n        central_term = np.exp(bbar * delta_m) - 1.0 - (bbar * delta_m)\n        return a_3 * central_term * (delta_m > 0.0)", "response": "Returns the cumulative value of a specific entry in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the A3 term for the given parameters.", "response": "def _get_a3(bbar, dbar, slip_moment, mmax):\n        \"\"\"\n        Returns the A3 term (III.4 in Table  4)\n        \"\"\"\n        return ((dbar * (dbar - bbar)) / (bbar ** 2.)) * (slip_moment /\n                                                          _scale_moment(mmax))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef incremental_value(self, slip_moment, mmax, mag_value, bbar, dbar):\n        delta_m = mmax - mag_value\n        a_3 = self._get_a3(bbar, dbar, slip_moment, mmax)\n        return a_3 * bbar * (np.exp(bbar * delta_m) - 1.0) * (delta_m > 0.0)", "response": "Calculates the incremental rate of a set of modules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mmax(self, mfd_conf, msr, rake, area):\n        '''\n        Gets the mmax for the fault - reading directly from the config file\n        or using the msr otherwise\n\n        :param dict mfd_config:\n            Configuration file (see setUp for paramters)\n\n        :param msr:\n            Instance of :class:`nhlib.scalerel`\n\n        :param float rake:\n            Rake of the fault (in range -180 to 180)\n\n        :param float area:\n            Area of the fault surface (km^2)\n        '''\n        if mfd_conf['Maximum_Magnitude']:\n            self.mmax = mfd_conf['Maximum_Magnitude']\n        else:\n            self.mmax = msr.get_median_mag(area, rake)\n\n        if ('Maximum_Magnitude_Uncertainty' in mfd_conf and\n                mfd_conf['Maximum_Magnitude_Uncertainty']):\n            self.mmax_sigma = mfd_conf['Maximum_Magnitude_Uncertainty']\n        else:\n            self.mmax_sigma = msr.get_std_dev_mag(rake)", "response": "Gets the mmax for the fault - reading directly from the config file or using the msr otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mfd(self, slip, area, shear_modulus=30.0):\n        '''\n        Calculates activity rate on the fault\n\n        :param float slip:\n            Slip rate in mm/yr\n\n        :param fault_area:\n            Width of the fault (km)\n\n        :param float shear_modulus:\n            Shear modulus of the fault (GPa)\n\n        :returns:\n            * Minimum Magnitude (float)\n            * Bin width (float)\n            * Occurrence Rates (numpy.ndarray)\n        '''\n\n        # Convert shear modulus GPa -> dyne-cm, area km ** 2 -> cm ** 2 and\n        # slip mm/yr -> cm/yr\n        slip_moment = (shear_modulus * 1E10) * (area * 1E10) * (slip / 10.)\n        dbar = D_VALUE * np.log(10.0)\n        bbar = self.b_value * np.log(10.0)\n\n        mags = np.arange(self.mmin - (self.bin_width / 2.),\n                         self.mmax + self.bin_width,\n                         self.bin_width)\n        if bbar >= dbar:\n            print('b-value larger than 1.5 will produce invalid results in '\n                  'Anderson & Luco models')\n            self.occurrence_rate = np.nan * np.ones(len(mags) - 1)\n            return self.mmin, self.bin_width, self.occurrence_rate\n        self.occurrence_rate = np.zeros(len(mags) - 1, dtype=float)\n        for ival in range(0, len(mags) - 1):\n            self.occurrence_rate[ival] = \\\n                RECURRENCE_MAP[self.mfd_type].cumulative_value(\n                    slip_moment, self.mmax, mags[ival], bbar, dbar) - \\\n                RECURRENCE_MAP[self.mfd_type].cumulative_value(\n                    slip_moment, self.mmax, mags[ival + 1], bbar, dbar)\n        return self.mmin, self.bin_width, self.occurrence_rate", "response": "Calculates the activity rate of the fault and returns the minimum magnitude bin width and occurrence rate of the fault."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reduce_sm(calc_id):\n    with datastore.read(calc_id) as dstore:\n        oqparam = dstore['oqparam']\n        info = dstore['source_info'].value\n        ok = info['weight'] > 0\n        source_ids = set(info[ok]['source_id'])\n    with performance.Monitor() as mon:\n        readinput.reduce_source_model(\n            oqparam.inputs['source_model_logic_tree'], source_ids)\n    print(mon)", "response": "Reduce the source model of the given calculation by discarding all sources that do not contribute to the hazard."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the mean and standard deviation for a specific intensity model.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        C = self.COEFFS[imt]\n        mean = (\n            self._get_magnitude_term(C, rup.mag) +\n            self._get_distance_term(C, rup.mag, dists.rrup) +\n            self._get_style_of_faulting_term(C, rup.rake) +\n            self._get_site_response_term(C, sites.vs30))\n\n        stddevs = self._get_stddevs(C, stddev_types, len(sites.vs30))\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the magnitude scaling term in equation 1 page 74.", "response": "def _get_magnitude_term(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling term.\n        \"\"\"\n        lny = C['C1'] + (C['C3'] * ((8.5 - mag) ** 2.))\n        if mag > 6.3:\n            return lny + (-C['H'] * C['C5']) * (mag - 6.3)\n        else:\n            return lny + C['C2'] * (mag - 6.3)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_distance_term(self, C, mag, rrup):\n        return (C['C4'] + C['C5'] * (mag - 6.3)) *\\\n            np.log(np.sqrt(rrup ** 2. + np.exp(C['H']) ** 2.))", "response": "Returns the distance scaling term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the style of faulting factor in terms of faulting type", "response": "def _get_style_of_faulting_term(self, C, rake):\n        \"\"\"\n        Returns the style of faulting factor\n        \"\"\"\n        f_n, f_r = self._get_fault_type_dummy_variables(rake)\n        return C['C6'] * f_n + C['C7'] * f_r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the fault type dummy variables for normal faulting and reverse faulting.", "response": "def _get_fault_type_dummy_variables(self, rake):\n        \"\"\"\n        Defines the fault type dummy variables for normal faulting (f_n) and\n        reverse faulting (f_r) from rake. Classification based on that\n        found in the original fortran code of Lin (2009)\n        \"\"\"\n        f_n, f_r = 0, 0\n        if rake >= -120 and rake <= -60:\n            # normal\n            f_n = 1\n        elif rake >= 30 and rake <= 150:\n            # reverse\n            f_r = 1\n        return f_n, f_r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_stddevs(self, C, stddev_types, nsites):\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(C['sigma'] + np.zeros(nsites, dtype=float))\n        return stddevs", "response": "Compute total standard deviation for the resource table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef imt2tup(string):\n    s = string.strip()\n    if not s.endswith(')'):\n        # no parenthesis, PGA is considered the same as PGA()\n        return (s,)\n    name, rest = s.split('(', 1)\n    return (name,) + tuple(float(x) for x in ast.literal_eval(rest[:-1] + ','))", "response": "Convert string to a sequence of IANA types."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an arrayWrapper over a 4D array of shape N M P with the given curves.", "response": "def make_iml4(R, iml_disagg, imtls=None, poes_disagg=(None,), curves=()):\n    \"\"\"\n    :returns: an ArrayWrapper over a 4D array of shape (N, R, M, P)\n    \"\"\"\n    if imtls is None:\n        imtls = {imt: [iml] for imt, iml in iml_disagg.items()}\n    N = len(curves) or 1\n    M = len(imtls)\n    P = len(poes_disagg)\n    arr = numpy.zeros((N, R, M, P))\n    imts = [from_string(imt) for imt in imtls]\n    for m, imt in enumerate(imtls):\n        imls = imtls[imt]\n        for p, poe in enumerate(poes_disagg):\n            for r in range(R):\n                arr[:, r, m, p] = _imls(curves, poe, imt, imls, r)\n    return ArrayWrapper(arr, dict(poes_disagg=poes_disagg, imts=imts))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collect_bin_data(ruptures, sitecol, cmaker, iml4,\n                     truncation_level, n_epsilons, monitor=Monitor()):\n    \"\"\"\n    :param ruptures: a list of ruptures\n    :param sitecol: a SiteCollection instance\n    :param cmaker: a ContextMaker instance\n    :param iml4: an ArrayWrapper of intensities of shape (N, R, M, P)\n    :param truncation_level: the truncation level\n    :param n_epsilons: the number of epsilons\n    :param monitor: a Monitor instance\n    :returns: a dictionary (poe, imt, rlzi) -> probabilities of shape (N, E)\n    \"\"\"\n    # NB: instantiating truncnorm is slow and calls the infamous \"doccer\"\n    truncnorm = scipy.stats.truncnorm(-truncation_level, truncation_level)\n    epsilons = numpy.linspace(truncnorm.a, truncnorm.b, n_epsilons + 1)\n    acc = cmaker.disaggregate(\n        sitecol, ruptures, iml4, truncnorm, epsilons, monitor)\n    return pack(acc, 'mags dists lons lats'.split())", "response": "Collect bin data from a list of ruptures and sitecol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining bin edges for disaggregation histograms. Given bins data as provided by :func:`collect_bin_data`, this function finds edges of histograms, taking into account maximum and minimum values of magnitude, distance and coordinates as well as requested sizes/numbers of bins.", "response": "def lon_lat_bins(bb, coord_bin_width):\n    \"\"\"\n    Define bin edges for disaggregation histograms.\n\n    Given bins data as provided by :func:`collect_bin_data`, this function\n    finds edges of histograms, taking into account maximum and minimum values\n    of magnitude, distance and coordinates as well as requested sizes/numbers\n    of bins.\n    \"\"\"\n    west, south, east, north = bb\n    west = numpy.floor(west / coord_bin_width) * coord_bin_width\n    east = numpy.ceil(east / coord_bin_width) * coord_bin_width\n    lon_extent = get_longitudinal_extent(west, east)\n    lon_bins, _, _ = npoints_between(\n        west, 0, 0, east, 0, 0,\n        numpy.round(lon_extent / coord_bin_width + 1))\n    lat_bins = coord_bin_width * numpy.arange(\n        int(numpy.floor(south / coord_bin_width)),\n        int(numpy.ceil(north / coord_bin_width) + 1))\n    return lon_bins, lat_bins"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the shape of the disaggregation matrix for the given site.", "response": "def get_shape(bin_edges, sid):\n    \"\"\"\n    :returns:\n        the shape of the disaggregation matrix for the given site, of form\n        (#mags-1, #dists-1, #lons-1, #lats-1, #eps-1)\n    \"\"\"\n    mag_bins, dist_bins, lon_bins, lat_bins, eps_bins = bin_edges\n    return (len(mag_bins) - 1, len(dist_bins) - 1,\n            len(lon_bins[sid]) - 1, len(lat_bins[sid]) - 1, len(eps_bins) - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_disagg_matrix(bdata, bin_edges, sid, mon=Monitor):\n    with mon('build_disagg_matrix'):\n        mag_bins, dist_bins, lon_bins, lat_bins, eps_bins = bin_edges\n        dim1, dim2, dim3, dim4, dim5 = shape = get_shape(bin_edges, sid)\n\n        # find bin indexes of rupture attributes; bins are assumed closed\n        # on the lower bound, and open on the upper bound, that is [ )\n        # longitude values need an ad-hoc method to take into account\n        # the 'international date line' issue\n        # the 'minus 1' is needed because the digitize method returns the\n        # index of the upper bound of the bin\n        mags_idx = numpy.digitize(bdata.mags+pmf.PRECISION, mag_bins) - 1\n        dists_idx = numpy.digitize(bdata.dists[:, sid], dist_bins) - 1\n        lons_idx = _digitize_lons(bdata.lons[:, sid], lon_bins[sid])\n        lats_idx = numpy.digitize(bdata.lats[:, sid], lat_bins[sid]) - 1\n\n        # because of the way numpy.digitize works, values equal to the last bin\n        # edge are associated to an index equal to len(bins) which is not a\n        # valid index for the disaggregation matrix. Such values are assumed\n        # to fall in the last bin\n        mags_idx[mags_idx == dim1] = dim1 - 1\n        dists_idx[dists_idx == dim2] = dim2 - 1\n        lons_idx[lons_idx == dim3] = dim3 - 1\n        lats_idx[lats_idx == dim4] = dim4 - 1\n\n        out = {}\n        cache = {}\n        cache_hit = 0\n        num_zeros = 0\n        for k, allpnes in bdata.items():\n            pnes = allpnes[:, sid, :]  # shape (U, N, E)\n            cache_key = pnes.sum()\n            if cache_key == pnes.size:  # all pnes are 1\n                num_zeros += 1\n                continue  # zero matrices are not transferred\n            try:\n                matrix = cache[cache_key]\n                cache_hit += 1\n            except KeyError:\n                mat = numpy.ones(shape)\n                for i_mag, i_dist, i_lon, i_lat, pne in zip(\n                        mags_idx, dists_idx, lons_idx, lats_idx, pnes):\n                    mat[i_mag, i_dist, i_lon, i_lat] *= pne\n                matrix = 1. - mat\n                cache[cache_key] = matrix\n            out[k] = matrix\n\n        # operations, hits, num_zeros\n        if hasattr(mon, 'cache_info'):\n            mon.cache_info += numpy.array([len(bdata), cache_hit, num_zeros])\n        else:\n            mon.cache_info = numpy.array([len(bdata), cache_hit, num_zeros])\n    return out", "response": "Builds a disaggregation matrix for a single site."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _digitize_lons(lons, lon_bins):\n    if cross_idl(lon_bins[0], lon_bins[-1]):\n        idx = numpy.zeros_like(lons, dtype=numpy.int)\n        for i_lon in range(len(lon_bins) - 1):\n            extents = get_longitudinal_extent(lons, lon_bins[i_lon + 1])\n            lon_idx = extents > 0\n            if i_lon != 0:\n                extents = get_longitudinal_extent(lon_bins[i_lon], lons)\n                lon_idx &= extents >= 0\n            idx[lon_idx] = i_lon\n        return numpy.array(idx)\n    else:\n        return numpy.digitize(lons, lon_bins) - 1", "response": "Return indices of the bins to which each value in lons belongs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the disaggregation matrix for a given rupture.", "response": "def disaggregation(\n        sources, site, imt, iml, gsim_by_trt, truncation_level,\n        n_epsilons, mag_bin_width, dist_bin_width, coord_bin_width,\n        source_filter=filters.nofilter, filter_distance='rjb'):\n    \"\"\"\n    Compute \"Disaggregation\" matrix representing conditional probability of an\n    intensity mesaure type ``imt`` exceeding, at least once, an intensity\n    measure level ``iml`` at a geographical location ``site``, given rupture\n    scenarios classified in terms of:\n\n    - rupture magnitude\n    - Joyner-Boore distance from rupture surface to site\n    - longitude and latitude of the surface projection of a rupture's point\n      closest to ``site``\n    - epsilon: number of standard deviations by which an intensity measure\n      level deviates from the median value predicted by a GSIM, given the\n      rupture parameters\n    - rupture tectonic region type\n\n    In other words, the disaggregation matrix allows to compute the probability\n    of each scenario with the specified properties (e.g., magnitude, or the\n    magnitude and distance) to cause one or more exceedences of a given hazard\n    level.\n\n    For more detailed information about the disaggregation, see for instance\n    \"Disaggregation of Seismic Hazard\", Paolo Bazzurro, C. Allin Cornell,\n    Bulletin of the Seismological Society of America, Vol. 89, pp. 501-520,\n    April 1999.\n\n    :param sources:\n        Seismic source model, as for\n        :mod:`PSHA <openquake.hazardlib.calc.hazard_curve>` calculator it\n        should be an iterator of seismic sources.\n    :param site:\n        :class:`~openquake.hazardlib.site.Site` of interest to calculate\n        disaggregation matrix for.\n    :param imt:\n        Instance of :mod:`intensity measure type <openquake.hazardlib.imt>`\n        class.\n    :param iml:\n        Intensity measure level. A float value in units of ``imt``.\n    :param gsim_by_trt:\n        Tectonic region type to GSIM objects mapping.\n    :param truncation_level:\n        Float, number of standard deviations for truncation of the intensity\n        distribution.\n    :param n_epsilons:\n        Integer number of epsilon histogram bins in the result matrix.\n    :param mag_bin_width:\n        Magnitude discretization step, width of one magnitude histogram bin.\n    :param dist_bin_width:\n        Distance histogram discretization step, in km.\n    :param coord_bin_width:\n        Longitude and latitude histograms discretization step,\n        in decimal degrees.\n    :param source_filter:\n        Optional source-site filter function. See\n        :mod:`openquake.hazardlib.calc.filters`.\n\n    :returns:\n        A tuple of two items. First is itself a tuple of bin edges information\n        for (in specified order) magnitude, distance, longitude, latitude,\n        epsilon and tectonic region types.\n\n        Second item is 6d-array representing the full disaggregation matrix.\n        Dimensions are in the same order as bin edges in the first item\n        of the result tuple. The matrix can be used directly by pmf-extractor\n        functions.\n    \"\"\"\n    trts = sorted(set(src.tectonic_region_type for src in sources))\n    trt_num = dict((trt, i) for i, trt in enumerate(trts))\n    rlzs_by_gsim = {gsim_by_trt[trt]: [0] for trt in trts}\n    iml4 = make_iml4(1, {str(imt): iml})\n    by_trt = groupby(sources, operator.attrgetter('tectonic_region_type'))\n    bdata = {}\n    sitecol = SiteCollection([site])\n    for trt, srcs in by_trt.items():\n        ruptures = []\n        for src in srcs:\n            ruptures.extend(src.iter_ruptures())\n        cmaker = ContextMaker(\n            trt, rlzs_by_gsim, source_filter.integration_distance,\n            {'filter_distance': filter_distance})\n        bdata[trt] = collect_bin_data(\n            ruptures, sitecol, cmaker, iml4, truncation_level, n_epsilons)\n    if sum(len(bd.mags) for bd in bdata.values()) == 0:\n        warnings.warn(\n            'No ruptures have contributed to the hazard at site %s'\n            % site, RuntimeWarning)\n        return None, None\n\n    min_mag = min(bd.mags.min() for bd in bdata.values())\n    max_mag = max(bd.mags.max() for bd in bdata.values())\n    mag_bins = mag_bin_width * numpy.arange(\n        int(numpy.floor(min_mag / mag_bin_width)),\n        int(numpy.ceil(max_mag / mag_bin_width) + 1))\n\n    min_dist = min(bd.dists.min() for bd in bdata.values())\n    max_dist = max(bd.dists.max() for bd in bdata.values())\n    dist_bins = dist_bin_width * numpy.arange(\n        int(numpy.floor(min_dist / dist_bin_width)),\n        int(numpy.ceil(max_dist / dist_bin_width) + 1))\n\n    bb = (min(bd.lons.min() for bd in bdata.values()),\n          min(bd.lats.min() for bd in bdata.values()),\n          max(bd.lons.max() for bd in bdata.values()),\n          max(bd.lats.max() for bd in bdata.values()))\n    lon_bins, lat_bins = lon_lat_bins(bb, coord_bin_width)\n\n    eps_bins = numpy.linspace(-truncation_level, truncation_level,\n                              n_epsilons + 1)\n\n    bin_edges = (mag_bins, dist_bins, [lon_bins], [lat_bins], eps_bins)\n    matrix = numpy.zeros((len(mag_bins) - 1, len(dist_bins) - 1,\n                          len(lon_bins) - 1, len(lat_bins) - 1,\n                          len(eps_bins) - 1, len(trts)))\n    for trt in bdata:\n        dic = build_disagg_matrix(bdata[trt], bin_edges, sid=0)\n        if dic:  # (poe, imt, rlzi) -> matrix\n            [mat] = dic.values()\n            matrix[..., trt_num[trt]] = mat\n    return bin_edges + (trts,), matrix"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfolding full disaggregation matrix to magnitude PMF.", "response": "def mag_pmf(matrix):\n    \"\"\"\n    Fold full disaggregation matrix to magnitude PMF.\n\n    :returns:\n        1d array, a histogram representing magnitude PMF.\n    \"\"\"\n    nmags, ndists, nlons, nlats, neps = matrix.shape\n    mag_pmf = numpy.zeros(nmags)\n    for i in range(nmags):\n        mag_pmf[i] = numpy.prod(\n            [1. - matrix[i, j, k, l, m]\n             for j in range(ndists)\n             for k in range(nlons)\n             for l in range(nlats)\n             for m in range(neps)])\n    return 1. - mag_pmf"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfolding full disaggregation matrix to tectonic region type PMF.", "response": "def trt_pmf(matrices):\n    \"\"\"\n    Fold full disaggregation matrix to tectonic region type PMF.\n\n    :param matrices:\n        a matrix with T submatrices\n    :returns:\n        an array of T probabilities one per each tectonic region type\n    \"\"\"\n    ntrts, nmags, ndists, nlons, nlats, neps = matrices.shape\n    pmf = numpy.zeros(ntrts)\n    for t in range(ntrts):\n        pmf[t] = 1. - numpy.prod(\n            [1. - matrices[t, i, j, k, l, m]\n             for i in range(nmags)\n             for j in range(ndists)\n             for k in range(nlons)\n             for l in range(nlats)\n             for m in range(neps)])\n    return pmf"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfold full disaggregation matrices to lon lat TRT PMF.", "response": "def lon_lat_trt_pmf(matrices):\n    \"\"\"\n    Fold full disaggregation matrices to lon / lat / TRT PMF.\n\n    :param matrices:\n        a matrix with T submatrices\n    :returns:\n        3d array. First dimension represents longitude histogram bins,\n        second one latitude histogram bins, third one trt histogram bins.\n    \"\"\"\n    res = numpy.array([lon_lat_pmf(mat) for mat in matrices])\n    return res.transpose(1, 2, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef db(cmd, args=()):\n    if cmd not in commands:\n        okcmds = '\\n'.join(\n            '%s %s' % (name, repr(' '.join(args)) if args else '')\n            for name, args in sorted(commands.items()))\n        print('Invalid command \"%s\": choose one from\\n%s' % (cmd, okcmds))\n    elif len(args) != len(commands[cmd]):\n        print('Wrong number of arguments, expected %s, got %s' % (\n            commands[cmd], args))\n    else:\n        dbserver.ensure_on()\n        res = logs.dbcmd(cmd, *convert(args))\n        if hasattr(res, '_fields') and res.__class__.__name__ != 'Row':\n            print(rst_table(res))\n        else:\n            print(res)", "response": "Run a database command"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_area(self):\n        '''\n        Calculates the area of the fault (km ** 2.) as the product of length\n        (km) and downdip width (km)\n        '''\n        d_z = self.lower_depth - self.upper_depth\n        self.downdip_width = d_z / np.sin(self.dip * np.pi / 180.)\n        self.surface_width = self.downdip_width * np.cos(self.dip *\n                                                         np.pi / 180.)\n        self.area = self.length * self.downdip_width\n        return self.area", "response": "Calculates the area of the fault as the product of length and downdip width."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading the sequence of objects from the shakemap database", "response": "def download_shakemap(id):\n    \"\"\"\n    Example of usage: utils/shakemap usp000fjta\n    \"\"\"\n    with performance.Monitor('shakemap', measuremem=True) as mon:\n        dest = '%s.npy' % id\n        numpy.save(dest, download_array(id))\n    print(mon)\n    print('Saved %s' % dest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mean_curve(values, weights=None):\n    if weights is None:\n        weights = [1. / len(values)] * len(values)\n    if not isinstance(values, numpy.ndarray):\n        values = numpy.array(values)\n    return numpy.average(values, axis=0, weights=weights)", "response": "Compute the mean curve by using numpy. average on the first axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the weighted quantile aggregate of a set of curves.", "response": "def quantile_curve(quantile, curves, weights=None):\n    \"\"\"\n    Compute the weighted quantile aggregate of a set of curves.\n\n    :param quantile:\n        Quantile value to calculate. Should be in the range [0.0, 1.0].\n    :param curves:\n        Array of R PoEs (possibly arrays)\n    :param weights:\n        Array-like of weights, 1 for each input curve, or None\n    :returns:\n        A numpy array representing the quantile aggregate\n    \"\"\"\n    if not isinstance(curves, numpy.ndarray):\n        curves = numpy.array(curves)\n    R = len(curves)\n    if weights is None:\n        weights = numpy.ones(R) / R\n    else:\n        weights = numpy.array(weights)\n        assert len(weights) == R, (len(weights), R)\n    result = numpy.zeros(curves.shape[1:])\n    for idx, _ in numpy.ndenumerate(result):\n        data = numpy.array([a[idx] for a in curves])\n        sorted_idxs = numpy.argsort(data)\n        sorted_weights = weights[sorted_idxs]\n        sorted_data = data[sorted_idxs]\n        cum_weights = numpy.cumsum(sorted_weights)\n        # get the quantile from the interpolated CDF\n        result[idx] = numpy.interp(quantile, cum_weights, sorted_data)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the stats for the probability maps and returns a new probability map with the S internal values.", "response": "def compute_pmap_stats(pmaps, stats, weights, imtls):\n    \"\"\"\n    :param pmaps:\n        a list of R probability maps\n    :param stats:\n        a sequence of S statistic functions\n    :param weights:\n        a list of ImtWeights\n    :param imtls:\n        a DictArray of intensity measure types\n    :returns:\n        a probability map with S internal values\n    \"\"\"\n    sids = set()\n    p0 = next(iter(pmaps))\n    L = p0.shape_y\n    for pmap in pmaps:\n        sids.update(pmap)\n        assert pmap.shape_y == L, (pmap.shape_y, L)\n    if len(sids) == 0:\n        raise ValueError('All empty probability maps!')\n    sids = numpy.array(sorted(sids), numpy.uint32)\n    nstats = len(stats)\n    curves = numpy.zeros((len(pmaps), len(sids), L), numpy.float64)\n    for i, pmap in enumerate(pmaps):\n        for j, sid in enumerate(sids):\n            if sid in pmap:\n                curves[i, j] = pmap[sid].array[:, 0]\n    out = p0.__class__.build(L, nstats, sids)\n    for imt in imtls:\n        slc = imtls(imt)\n        w = [weight[imt] for weight in weights]\n        for i, array in enumerate(compute_stats(curves[:, :, slc], stats, w)):\n            for j, sid in numpy.ndenumerate(sids):\n                out[sid].array[slc, i] = array[j]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_stats(array, stats, weights):\n    result = numpy.zeros((len(stats),) + array.shape[1:], array.dtype)\n    for i, func in enumerate(stats):\n        result[i] = apply_stat(func, array, weights)\n    return result", "response": "Compute the statistic of the given array of S elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_stats2(arrayNR, stats, weights):\n    newshape = list(arrayNR.shape)\n    if newshape[1] != len(weights):\n        raise ValueError('Got %d weights but %d values!' %\n                         (len(weights), newshape[1]))\n    newshape[1] = len(stats)  # number of statistical outputs\n    newarray = numpy.zeros(newshape, arrayNR.dtype)\n    data = [arrayNR[:, i] for i in range(len(weights))]\n    for i, func in enumerate(stats):\n        newarray[:, i] = apply_stat(func, data, weights)\n    return newarray", "response": "Compute the statistical outputs of the N - R array"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_stat(f, arraylist, *extra, **kw):\n    dtype = arraylist[0].dtype\n    shape = arraylist[0].shape\n    if dtype.names:  # composite array\n        new = numpy.zeros(shape, dtype)\n        for name in dtype.names:\n            new[name] = f([arr[name] for arr in arraylist], *extra, **kw)\n        return new\n    else:  # simple array\n        return f(arraylist, *extra, **kw)", "response": "Apply a statistical function f to a list of arrays of the same shape and dtype."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_rlzs_stats(dstore, prefix, arrayNR=None):\n    if arrayNR is None:\n        # assume the -rlzs array is already stored\n        arrayNR = dstore[prefix + '-rlzs'].value\n    else:\n        # store passed the -rlzs array\n        dstore[prefix + '-rlzs'] = arrayNR\n    R = arrayNR.shape[1]\n    if R > 1:\n        stats = dstore['oqparam'].hazard_stats()\n        statnames, statfuncs = zip(*stats.items())\n        weights = dstore['weights'][:, 0]\n        dstore[prefix + '-stats'] = compute_stats2(arrayNR, statfuncs, weights)\n        dstore.set_attrs(prefix + '-stats', stats=encode(statnames))", "response": "Set the stats for the - rlzs array"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the mean and standard deviation for the resource table entry.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        C = self.COEFFS[imt]\n        mean = self._compute_mean(C, rup.mag, dists.rjb)\n        stddevs = self._compute_stddevs(C, rup.mag, dists.rjb, imt,\n                                        stddev_types)\n\n        # apply decay factor for 3 and 4 seconds (not originally supported\n        # by the equations)\n        if imt.period == 3.0:\n            mean /= 0.612\n        if imt.period == 4.0:\n            mean /= 0.559\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_term1(self, C, mag):\n        mag_diff = mag - 6\n\n        return C['c2'] * mag_diff + C['c3'] * mag_diff ** 2", "response": "Compute the first term in the hierarchy of terms in equation 3."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_term2(self, C, mag, rjb):\n        RM = np.sqrt(rjb ** 2 + (C['c7'] ** 2) *\n                     np.exp(-1.25 + 0.227 * mag) ** 2)\n\n        return (-C['c4'] * np.log(RM) -\n                (C['c5'] - C['c4']) *\n                np.maximum(np.log(RM / 100), 0) - C['c6'] * RM)", "response": "Compute distance dependent terms in equation 3. 4. 2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_mean(self, C, mag, rjb):\n        mean = (C['c1'] +\n                self._compute_term1(C, mag) +\n                self._compute_term2(C, mag, rjb))\n        return mean", "response": "Compute mean value according to equation 3 page 46."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes total standard deviation as described in equation 5 and 6 page 48.", "response": "def _compute_stddevs(self, C, mag, rjb, imt, stddev_types):\n        \"\"\"\n        Compute total standard deviation, equations 5 and 6, page 48.\n        \"\"\"\n        # aleatory uncertainty\n        sigma_ale_m = np.interp(mag, [5.0, 5.5, 8.0],\n                                [C['m50'], C['m55'], C['m80']])\n        sigma_ale_rjb = np.interp(rjb, [5.0, 20.0], [C['r5'], C['r20']])\n        sigma_ale = np.sqrt(sigma_ale_m ** 2 + sigma_ale_rjb ** 2)\n\n        # epistemic uncertainty\n        if imt.period < 1:\n            sigma_epi = 0.36 + 0.07 * (mag - 6)\n        else:\n            sigma_epi = 0.34 + 0.06 * (mag - 6)\n\n        sigma_total = np.sqrt(sigma_ale ** 2 + sigma_epi ** 2)\n\n        stddevs = []\n        for _ in stddev_types:\n            stddevs.append(sigma_total)\n\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stds_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # Prepare sites\n        sites_rock = copy.deepcopy(sites)\n        sites_rock.vs30 = np.ones_like(sites_rock.vs30) * 760.\n        # compute mean and standard deviation\n        mean, stddvs = self.gmpe.get_mean_and_stddevs(sites_rock, rup, dists,\n                                                      imt, stds_types)\n        if not str(imt) == 'PGA':\n            # compute mean and standard deviation on rock\n            mean_rock, stddvs_rock = self.gmpe.get_mean_and_stddevs(\n                sites_rock, rup, dists, imt, stds_types)\n        else:\n            mean_rock = mean\n        fa = self.BA08_AB06(sites.vs30, imt, np.exp(mean_rock))\n        mean = np.log(np.exp(mean) * fa)\n        return mean, stddvs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef BA08_AB06(self, vs30, imt, pgar):\n        fa = np.ones_like(vs30)\n        if np.isscalar(vs30):\n            vs30 = np.array([vs30])\n        if np.isscalar(pgar):\n            pgar = np.array([pgar])\n        #\n        # Fixing vs30 for hard rock to 1999 m/s. Beyond this threshold the\n        # motion will not be deamplified further\n        vs = copy.copy(vs30)\n        vs[vs >= 2000] = 1999.\n        #\n        # Computing motion on rock\n        idx = np.where(vs30 > 760)\n        if np.size(idx) > 0:\n            \"\"\"\n            # This is the original implementation - Since this code is\n            # experimental we keep it for possible further developments\n            # For values of Vs30 greater than 760 a linear interpolation is\n            # used between the gm factor at 2000 m/s and 760 m/s\n            C2 = self.COEFFS_AB06r[imt]\n            fa[idx] = 10**(np.interp(np.log10(vs[idx]),\n                                     np.log10([760.0, 2000.0]),\n                                     np.log10([1.0, C2['c']])))\n            \"\"\"\n            C = self.COEFFS_BA08[imt]\n            nl = BooreAtkinson2008()._get_site_amplification_non_linear(\n                vs[idx], pgar[idx], C)\n            lin = BooreAtkinson2008()._get_site_amplification_linear(\n                vs[idx], C)\n            tmp = np.exp(nl+lin)\n            fa[idx] = tmp\n        #\n        # For values of Vs30 lower than 760 the amplification is computed\n        # using the site term of Boore and Atkinson (2008)\n        idx = np.where(vs < 760.)\n        if np.size(idx) > 0:\n            C = self.COEFFS_BA08[imt]\n            nl = BooreAtkinson2008()._get_site_amplification_non_linear(\n                vs[idx], pgar[idx], C)\n            lin = BooreAtkinson2008()._get_site_amplification_linear(\n                vs[idx], C)\n            fa[idx] = np.exp(nl+lin)\n        return fa", "response": "This function computes the amplification factor similar to what is done in the 2015 - Michal - Kolaj - Geological Survey of Canada."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncoring function for a damage computation. :param riskinputs: :class:`openquake.risklib.riskinput.RiskInput` objects :param riskmodel: a :class:`openquake.risklib.riskinput.CompositeRiskModel` instance :param monitor: :class:`openquake.baselib.performance.Monitor` instance :param param: dictionary of extra parameters :returns: a dictionary {'d_asset': [(l, r, a, mean-stddev), ...], 'd_event': damage array of shape R, L, E, D, 'c_asset': [(l, r, a, mean-stddev), ...], 'c_event': damage array of shape R, L, E} `d_asset` and `d_tag` are related to the damage distributions whereas `c_asset` and `c_tag` are the consequence distributions. If there is no consequence model `c_asset` is an empty list and `c_tag` is a zero-valued array.", "response": "def scenario_damage(riskinputs, riskmodel, param, monitor):\n    \"\"\"\n    Core function for a damage computation.\n\n    :param riskinputs:\n        :class:`openquake.risklib.riskinput.RiskInput` objects\n    :param riskmodel:\n        a :class:`openquake.risklib.riskinput.CompositeRiskModel` instance\n    :param monitor:\n        :class:`openquake.baselib.performance.Monitor` instance\n    :param param:\n        dictionary of extra parameters\n    :returns:\n        a dictionary {'d_asset': [(l, r, a, mean-stddev), ...],\n                      'd_event': damage array of shape R, L, E, D,\n                      'c_asset': [(l, r, a, mean-stddev), ...],\n                      'c_event': damage array of shape R, L, E}\n\n    `d_asset` and `d_tag` are related to the damage distributions\n    whereas `c_asset` and `c_tag` are the consequence distributions.\n    If there is no consequence model `c_asset` is an empty list and\n    `c_tag` is a zero-valued array.\n    \"\"\"\n    L = len(riskmodel.loss_types)\n    D = len(riskmodel.damage_states)\n    E = param['number_of_ground_motion_fields']\n    R = riskinputs[0].hazard_getter.num_rlzs\n    result = dict(d_asset=[], d_event=numpy.zeros((E, R, L, D), F64),\n                  c_asset=[], c_event=numpy.zeros((E, R, L), F64))\n    for ri in riskinputs:\n        for out in riskmodel.gen_outputs(ri, monitor):\n            r = out.rlzi\n            for l, loss_type in enumerate(riskmodel.loss_types):\n                for asset, fractions in zip(ri.assets, out[loss_type]):\n                    dmg = fractions[:, :D] * asset['number']  # shape (E, D)\n                    result['d_event'][:, r, l] += dmg\n                    result['d_asset'].append(\n                        (l, r, asset['ordinal'], scientific.mean_std(dmg)))\n                    if riskmodel.consequences:\n                        csq = fractions[:, D] * asset['value-' + loss_type]\n                        result['c_asset'].append(\n                            (l, r, asset['ordinal'], scientific.mean_std(csq)))\n                        result['c_event'][:, r, l] += csq\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef form(value):\n    if isinstance(value, FLOAT + INT):\n        if value <= 0:\n            return str(value)\n        elif value < .001:\n            return '%.3E' % value\n        elif value < 10 and isinstance(value, FLOAT):\n            return '%.5f' % value\n        elif value > 1000:\n            return '{:,d}'.format(int(round(value)))\n        elif numpy.isnan(value):\n            return 'NaN'\n        else:  # in the range 10-1000\n            return str(int(value))\n    elif isinstance(value, bytes):\n        return decode(value)\n    elif isinstance(value, str):\n        return value\n    elif isinstance(value, numpy.object_):\n        return str(value)\n    elif hasattr(value, '__len__') and len(value) > 1:\n        return ' '.join(map(form, value))\n    return str(value)", "response": "Format a number in a nice way."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a. rst table from a numpy array.", "response": "def rst_table(data, header=None, fmt=None):\n    \"\"\"\n    Build a .rst table from a matrix.\n    \n    >>> tbl = [['a', 1], ['b', 2]]\n    >>> print(rst_table(tbl, header=['Name', 'Value']))\n    ==== =====\n    Name Value\n    ==== =====\n    a    1    \n    b    2    \n    ==== =====\n    \"\"\"\n    if header is None and hasattr(data, '_fields'):\n        header = data._fields\n    try:\n        # see if data is a composite numpy array\n        data.dtype.fields\n    except AttributeError:\n        # not a composite array\n        header = header or ()\n    else:\n        if not header:\n            header = [col.split(':')[0] for col in build_header(data.dtype)]\n    if header:\n        col_sizes = [len(col) for col in header]\n    else:\n        col_sizes = [len(str(col)) for col in data[0]]\n    body = []\n    fmt = functools.partial(scientificformat, fmt=fmt) if fmt else form\n    for row in data:\n        tup = tuple(fmt(c) for c in row)\n        for (i, col) in enumerate(tup):\n            col_sizes[i] = max(col_sizes[i], len(col))\n        if len(tup) != len(col_sizes):\n            raise ValueError('The header has %d fields but the row %d fields!'\n                             % (len(col_sizes), len(tup)))\n        body.append(tup)\n\n    sepline = ' '.join(('=' * size for size in col_sizes))\n    templ = ' '.join(('%-{}s'.format(size) for size in col_sizes))\n    if header:\n        lines = [sepline, templ % tuple(header), sepline]\n    else:\n        lines = [sepline]\n    for row in body:\n        lines.append(templ % row)\n    lines.append(sepline)\n    return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\naggregate a composite array and compute the totals on a given key.", "response": "def sum_tbl(tbl, kfield, vfields):\n    \"\"\"\n    Aggregate a composite array and compute the totals on a given key.\n\n    >>> dt = numpy.dtype([('name', (bytes, 10)), ('value', int)])\n    >>> tbl = numpy.array([('a', 1), ('a', 2), ('b', 3)], dt)\n    >>> sum_tbl(tbl, 'name', ['value'])['value']\n    array([3, 3])\n    \"\"\"\n    pairs = [(n, tbl.dtype[n]) for n in [kfield] + vfields]\n    dt = numpy.dtype(pairs + [('counts', int)])\n\n    def sum_all(group):\n        vals = numpy.zeros(1, dt)[0]\n        for rec in group:\n            for vfield in vfields:\n                vals[vfield] += rec[vfield]\n            vals['counts'] += 1\n        vals[kfield] = rec[kfield]\n        return vals\n    rows = groupby(tbl, operator.itemgetter(kfield), sum_all).values()\n    array = numpy.zeros(len(rows), dt)\n    for i, row in enumerate(rows):\n        for j, name in enumerate(dt.names):\n            array[i][name] = row[j]\n    return array"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the slowest sources in the datastore", "response": "def view_slow_sources(token, dstore, maxrows=20):\n    \"\"\"\n    Returns the slowest sources\n    \"\"\"\n    info = dstore['source_info'].value\n    info.sort(order='calc_time')\n    return rst_table(info[::-1][:maxrows])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the size of the contents of the datastore and its total size", "response": "def view_contents(token, dstore):\n    \"\"\"\n    Returns the size of the contents of the datastore and its total size\n    \"\"\"\n    try:\n        desc = dstore['oqparam'].description\n    except KeyError:\n        desc = ''\n    data = sorted((dstore.getsize(key), key) for key in dstore)\n    rows = [(key, humansize(nbytes)) for nbytes, key in data]\n    total = '\\n%s : %s' % (\n        dstore.filename, humansize(os.path.getsize(dstore.filename)))\n    return rst_table(rows, header=(desc, '')) + total"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef view_job_info(token, dstore):\n    data = [['task', 'sent', 'received']]\n    for task in dstore['task_info']:\n        dset = dstore['task_info/' + task]\n        if 'argnames' in dset.attrs:\n            argnames = dset.attrs['argnames'].split()\n            totsent = dset.attrs['sent']\n            sent = ['%s=%s' % (a, humansize(s))\n                    for s, a in sorted(zip(totsent, argnames), reverse=True)]\n            recv = dset['received'].sum()\n            data.append((task, ' '.join(sent), humansize(recv)))\n    return rst_table(data)", "response": "Display the amount of data transferred from the controller node to the workers and back in a classical calculation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the average losses transferred from the workers to the controller node in a risk calculation.", "response": "def avglosses_data_transfer(token, dstore):\n    \"\"\"\n    Determine the amount of average losses transferred from the workers to the\n    controller node in a risk calculation.\n    \"\"\"\n    oq = dstore['oqparam']\n    N = len(dstore['assetcol'])\n    R = dstore['csm_info'].get_num_rlzs()\n    L = len(dstore.get_attr('risk_model', 'loss_types'))\n    ct = oq.concurrent_tasks\n    size_bytes = N * R * L * 8 * ct  # 8 byte floats\n    return (\n        '%d asset(s) x %d realization(s) x %d loss type(s) losses x '\n        '8 bytes x %d tasks = %s' % (N, R, L, ct, humansize(size_bytes)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ebr_data_transfer(token, dstore):\n    attrs = dstore['losses_by_event'].attrs\n    sent = humansize(attrs['sent'])\n    received = humansize(attrs['tot_received'])\n    return 'Event Based Risk: sent %s, received %s' % (sent, received)", "response": "Display the data transferred in an event based risk calculation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying the total losses of the asset in the datastore.", "response": "def view_totlosses(token, dstore):\n    \"\"\"\n    This is a debugging view. You can use it to check that the total\n    losses, i.e. the losses obtained by summing the average losses on\n    all assets are indeed equal to the aggregate losses. This is a\n    sanity check for the correctness of the implementation.\n    \"\"\"\n    oq = dstore['oqparam']\n    tot_losses = dstore['losses_by_asset']['mean'].sum(axis=0)\n    return rst_table(tot_losses.view(oq.loss_dt()), fmt='%.6E')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef view_portfolio_losses(token, dstore):\n    oq = dstore['oqparam']\n    loss_dt = oq.loss_dt()\n    data = portfolio_loss(dstore).view(loss_dt)[:, 0]\n    rlzids = [str(r) for r in range(len(data))]\n    array = util.compose_arrays(numpy.array(rlzids), data, 'rlz')\n    # this is very sensitive to rounding errors, so I am using a low precision\n    return rst_table(array, fmt='%.5E')", "response": "Returns the losses for the full portfolio for each realization and loss type extracted from the event loss table."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the mean and stddev loss for the full portfolio for each loss type extracted from the event loss table averaged over the realizations", "response": "def view_portfolio_loss(token, dstore):\n    \"\"\"\n    The mean and stddev loss for the full portfolio for each loss type,\n    extracted from the event loss table, averaged over the realizations\n    \"\"\"\n    data = portfolio_loss(dstore)  # shape (R, L)\n    loss_types = list(dstore['oqparam'].loss_dt().names)\n    header = ['portfolio_loss'] + loss_types\n    mean = ['mean'] + [row.mean() for row in data.T]\n    stddev = ['stddev'] + [row.std(ddof=1) for row in data.T]\n    return rst_table([mean, stddev], header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses to compute summaries for the numeric fields.", "response": "def sum_table(records):\n    \"\"\"\n    Used to compute summaries. The records are assumed to have numeric\n    fields, except the first field which is ignored, since it typically\n    contains a label. Here is an example:\n\n    >>> sum_table([('a', 1), ('b', 2)])\n    ['total', 3]\n    \"\"\"\n    size = len(records[0])\n    result = [None] * size\n    firstrec = records[0]\n    for i in range(size):\n        if isinstance(firstrec[i], (numbers.Number, numpy.ndarray)):\n            result[i] = sum(rec[i] for rec in records)\n        else:\n            result[i] = 'total'\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef view_exposure_info(token, dstore):\n    assetcol = dstore['assetcol/array'][:]\n    taxonomies = sorted(set(dstore['assetcol'].taxonomies))\n    cc = dstore['assetcol/cost_calculator']\n    ra_flag = ['relative', 'absolute']\n    data = [('#assets', len(assetcol)),\n            ('#taxonomies', len(taxonomies)),\n            ('deductibile', ra_flag[int(cc.deduct_abs)]),\n            ('insurance_limit', ra_flag[int(cc.limit_abs)]),\n            ]\n    return rst_table(data) + '\\n\\n' + view_assets_by_site(token, dstore)", "response": "Display info about the exposure model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef view_fullreport(token, dstore):\n    # avoid circular imports\n    from openquake.calculators.reportwriter import ReportWriter\n    return ReportWriter(dstore).make_report()", "response": "Display an. rst report about the computation\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the performance view as a numpy array.", "response": "def performance_view(dstore):\n    \"\"\"\n    Returns the performance view as a numpy array.\n    \"\"\"\n    data = sorted(dstore['performance_data'], key=operator.itemgetter(0))\n    out = []\n    for operation, group in itertools.groupby(data, operator.itemgetter(0)):\n        counts = 0\n        time = 0\n        mem = 0\n        for _operation, time_sec, memory_mb, counts_ in group:\n            counts += counts_\n            time += time_sec\n            mem = max(mem, memory_mb)\n        out.append((operation, time, mem, counts))\n    out.sort(key=operator.itemgetter(1), reverse=True)  # sort by time\n    return numpy.array(out, perf_dt)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stats(name, array, *extras):\n    std = numpy.nan if len(array) == 1 else numpy.std(array, ddof=1)\n    return (name, numpy.mean(array), std,\n            numpy.min(array), numpy.max(array), len(array)) + extras", "response": "Returns statistics from an array of numbers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying the number of units by taxonomy", "response": "def view_num_units(token, dstore):\n    \"\"\"\n    Display the number of units by taxonomy\n    \"\"\"\n    taxo = dstore['assetcol/tagcol/taxonomy'].value\n    counts = collections.Counter()\n    for asset in dstore['assetcol']:\n        counts[taxo[asset['taxonomy']]] += asset['number']\n    data = sorted(counts.items())\n    data.append(('*ALL*', sum(d[1] for d in data)))\n    return rst_table(data, header=['taxonomy', 'num_units'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying statistical information about the distribution of the assets by site", "response": "def view_assets_by_site(token, dstore):\n    \"\"\"\n    Display statistical information about the distribution of the assets\n    \"\"\"\n    taxonomies = dstore['assetcol/tagcol/taxonomy'].value\n    assets_by_site = dstore['assetcol'].assets_by_site()\n    data = ['taxonomy mean stddev min max num_sites num_assets'.split()]\n    num_assets = AccumDict()\n    for assets in assets_by_site:\n        num_assets += {k: [len(v)] for k, v in group_array(\n            assets, 'taxonomy').items()}\n    for taxo in sorted(num_assets):\n        val = numpy.array(num_assets[taxo])\n        data.append(stats(taxonomies[taxo], val, val.sum()))\n    if len(num_assets) > 1:  # more than one taxonomy, add a summary\n        n_assets = numpy.array([len(assets) for assets in assets_by_site])\n        data.append(stats('*ALL*', n_assets, n_assets.sum()))\n    return rst_table(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying the required parameters needed by each tectonic region type.", "response": "def view_required_params_per_trt(token, dstore):\n    \"\"\"\n    Display the parameters needed by each tectonic region type\n    \"\"\"\n    csm_info = dstore['csm_info']\n    tbl = []\n    for grp_id, trt in sorted(csm_info.grp_by(\"trt\").items()):\n        gsims = csm_info.gsim_lt.get_gsims(trt)\n        maker = ContextMaker(trt, gsims)\n        distances = sorted(maker.REQUIRES_DISTANCES)\n        siteparams = sorted(maker.REQUIRES_SITES_PARAMETERS)\n        ruptparams = sorted(maker.REQUIRES_RUPTURE_PARAMETERS)\n        tbl.append((grp_id, ' '.join(map(repr, map(repr, gsims))),\n                    distances, siteparams, ruptparams))\n    return rst_table(\n        tbl, header='grp_id gsims distances siteparams ruptparams'.split(),\n        fmt=scientificformat)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisplaying statistical information about the tasks performance", "response": "def view_task_info(token, dstore):\n    \"\"\"\n    Display statistical information about the tasks performance.\n    It is possible to get full information about a specific task\n    with a command like this one, for a classical calculation::\n\n      $ oq show task_info:classical\n    \"\"\"\n    args = token.split(':')[1:]  # called as task_info:task_name\n    if args:\n        [task] = args\n        array = dstore['task_info/' + task].value\n        rduration = array['duration'] / array['weight']\n        data = util.compose_arrays(rduration, array, 'rduration')\n        data.sort(order='duration')\n        return rst_table(data)\n\n    data = ['operation-duration mean stddev min max outputs'.split()]\n    for task in dstore['task_info']:\n        val = dstore['task_info/' + task]['duration']\n        if len(val):\n            data.append(stats(task, val))\n    if len(data) == 1:\n        return 'Not available'\n    return rst_table(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef view_task_durations(token, dstore):\n    task = token.split(':')[1]  # called as task_duration:task_name\n    array = dstore['task_info/' + task]['duration']\n    return '\\n'.join(map(str, array))", "response": "Display the raw task durations. Here is an example of usage ::\n     "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef view_task_hazard(token, dstore):\n    tasks = set(dstore['task_info'])\n    if 'source_data' not in dstore:\n        return 'Missing source_data'\n    if 'classical_split_filter' in tasks:\n        data = dstore['task_info/classical_split_filter'].value\n    else:\n        data = dstore['task_info/compute_gmfs'].value\n    data.sort(order='duration')\n    rec = data[int(token.split(':')[1])]\n    taskno = rec['taskno']\n    arr = get_array(dstore['source_data'].value, taskno=taskno)\n    st = [stats('nsites', arr['nsites']), stats('weight', arr['weight'])]\n    sources = dstore['task_sources'][taskno - 1].split()\n    srcs = set(decode(s).split(':', 1)[0] for s in sources)\n    res = 'taskno=%d, weight=%d, duration=%d s, sources=\"%s\"\\n\\n' % (\n        taskno, rec['weight'], rec['duration'], ' '.join(sorted(srcs)))\n    return res + rst_table(st, header='variable mean stddev min max n'.split())", "response": "Display info about a given task in the hazard."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef view_task_risk(token, dstore):\n    [key] = dstore['task_info']\n    data = dstore['task_info/' + key].value\n    data.sort(order='duration')\n    rec = data[int(token.split(':')[1])]\n    taskno = rec['taskno']\n    res = 'taskno=%d, weight=%d, duration=%d s' % (\n        taskno, rec['weight'], rec['duration'])\n    return res", "response": "Display info about a given risk task."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef view_hmap(token, dstore):\n    try:\n        poe = valid.probability(token.split(':')[1])\n    except IndexError:\n        poe = 0.1\n    mean = dict(extract(dstore, 'hcurves?kind=mean'))['mean']\n    oq = dstore['oqparam']\n    hmap = calc.make_hmap_array(mean, oq.imtls, [poe], len(mean))\n    dt = numpy.dtype([('sid', U32)] + [(imt, F32) for imt in oq.imtls])\n    array = numpy.zeros(len(hmap), dt)\n    for i, vals in enumerate(hmap):\n        array[i] = (i, ) + tuple(vals)\n    array.sort(order=list(oq.imtls)[0])\n    return rst_table(array[:20])", "response": "Display the highest 20 points of the mean hazard map. Called as\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay the global hazard curves for the calculation.", "response": "def view_global_hcurves(token, dstore):\n    \"\"\"\n    Display the global hazard curves for the calculation. They are\n    used for debugging purposes when comparing the results of two\n    calculations. They are the mean over the sites of the mean hazard\n    curves.\n    \"\"\"\n    oq = dstore['oqparam']\n    nsites = len(dstore['sitecol'])\n    rlzs_assoc = dstore['csm_info'].get_rlzs_assoc()\n    mean = getters.PmapGetter(dstore, rlzs_assoc).get_mean()\n    array = calc.convert_to_array(mean, nsites, oq.imtls)\n    res = numpy.zeros(1, array.dtype)\n    for name in array.dtype.names:\n        res[name] = array[name].mean()\n    return rst_table(res)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay the time spent computing duplicated sources", "response": "def view_dupl_sources_time(token, dstore):\n    \"\"\"\n    Display the time spent computing duplicated sources\n    \"\"\"\n    info = dstore['source_info']\n    items = sorted(group_array(info.value, 'source_id').items())\n    tbl = []\n    tot_time = 0\n    for source_id, records in items:\n        if len(records) > 1:  # dupl\n            calc_time = records['calc_time'].sum()\n            tot_time += calc_time + records['split_time'].sum()\n            tbl.append((source_id, calc_time, len(records)))\n    if tbl and info.attrs.get('has_dupl_sources'):\n        tot = info['calc_time'].sum() + info['split_time'].sum()\n        percent = tot_time / tot * 100\n        m = '\\nTotal time in duplicated sources: %d/%d (%d%%)' % (\n            tot_time, tot, percent)\n        return rst_table(tbl, ['source_id', 'calc_time', 'num_dupl']) + m\n    else:\n        return 'There are no duplicated sources'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef view_global_poes(token, dstore):\n    tbl = []\n    imtls = dstore['oqparam'].imtls\n    header = ['grp_id'] + [str(poe) for poe in imtls.array]\n    for grp in sorted(dstore['poes']):\n        poes = dstore['poes/' + grp]\n        nsites = len(poes)\n        site_avg = sum(poes[sid].array for sid in poes) / nsites\n        gsim_avg = site_avg.sum(axis=1) / poes.shape_z\n        tbl.append([grp] + list(gsim_avg))\n    return rst_table(tbl, header=header)", "response": "Display global probabilities averaged on all sites and all GMPEs\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays the global hazard maps for the calculation.", "response": "def view_global_hmaps(token, dstore):\n    \"\"\"\n    Display the global hazard maps for the calculation. They are\n    used for debugging purposes when comparing the results of two\n    calculations. They are the mean over the sites of the mean hazard\n    maps.\n    \"\"\"\n    oq = dstore['oqparam']\n    dt = numpy.dtype([('%s-%s' % (imt, poe), F32)\n                      for imt in oq.imtls for poe in oq.poes])\n    array = dstore['hmaps/mean'].value.view(dt)[:, 0]\n    res = numpy.zeros(1, array.dtype)\n    for name in array.dtype.names:\n        res[name] = array[name].mean()\n    return rst_table(res)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying GMFs averaged on everything for debugging purposes", "response": "def view_global_gmfs(token, dstore):\n    \"\"\"\n    Display GMFs averaged on everything for debugging purposes\n    \"\"\"\n    imtls = dstore['oqparam'].imtls\n    row = dstore['gmf_data/data']['gmv'].mean(axis=0)\n    return rst_table([row], header=imtls)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying mean quantities for the disaggregation. Useful for checking differences between two calculations.", "response": "def view_mean_disagg(token, dstore):\n    \"\"\"\n    Display mean quantities for the disaggregation. Useful for checking\n    differences between two calculations.\n    \"\"\"\n    tbl = []\n    for key, dset in sorted(dstore['disagg'].items()):\n        vals = [ds.value.mean() for k, ds in sorted(dset.items())]\n        tbl.append([key] + vals)\n    header = ['key'] + sorted(dset)\n    return rst_table(sorted(tbl), header=header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef view_elt(token, dstore):\n    oq = dstore['oqparam']\n    R = len(dstore['csm_info'].rlzs)\n    dic = group_array(dstore['losses_by_event'].value, 'rlzi')\n    header = oq.loss_dt().names\n    tbl = []\n    for rlzi in range(R):\n        if rlzi in dic:\n            tbl.append(dic[rlzi]['loss'].mean(axis=0))\n        else:\n            tbl.append([0.] * len(header))\n    return rst_table(tbl, header)", "response": "Display the event loss table averaged by event"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef view_pmap(token, dstore):\n    grp = token.split(':')[1]  # called as pmap:grp\n    pmap = {}\n    rlzs_assoc = dstore['csm_info'].get_rlzs_assoc()\n    pgetter = getters.PmapGetter(dstore, rlzs_assoc)\n    pmap = pgetter.get_mean(grp)\n    return str(pmap)", "response": "Display the mean ProbabilityMap associated to a given source group name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying the actual number of ruptures by source in event based calculations", "response": "def view_act_ruptures_by_src(token, dstore):\n    \"\"\"\n    Display the actual number of ruptures by source in event based calculations\n    \"\"\"\n    data = dstore['ruptures'].value[['srcidx', 'serial']]\n    counts = sorted(countby(data, 'srcidx').items(),\n                    key=operator.itemgetter(1), reverse=True)\n    src_info = dstore['source_info'].value[['grp_id', 'source_id']]\n    table = [['src_id', 'grp_id', 'act_ruptures']]\n    for srcidx, act_ruptures in counts:\n        src = src_info[srcidx]\n        table.append([src['source_id'], src['grp_id'], act_ruptures])\n    return rst_table(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef view_dupl_sources(token, dstore):\n    fields = ['source_id', 'code', 'gidx1', 'gidx2', 'num_ruptures']\n    dic = group_array(dstore['source_info'].value[fields], 'source_id')\n    sameid = []\n    dupl = []\n    for source_id, group in dic.items():\n        if len(group) > 1:  # same ID sources\n            sources = []\n            for rec in group:\n                geom = dstore['source_geom'][rec['gidx1']:rec['gidx2']]\n                src = Source(source_id, rec['code'], geom, rec['num_ruptures'])\n                sources.append(src)\n            if all_equal(sources):\n                dupl.append(source_id)\n            sameid.append(source_id)\n    if not dupl:\n        return ''\n    msg = str(dupl) + '\\n'\n    msg += ('Found %d source(s) with the same ID and %d true duplicate(s)'\n            % (len(sameid), len(dupl)))\n    fakedupl = set(sameid) - set(dupl)\n    if fakedupl:\n        msg += '\\nHere is a fake duplicate: %s' % fakedupl.pop()\n    return msg", "response": "Show the sources with the same ID and truly duplicated sources\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef view_extreme_groups(token, dstore):\n    data = dstore['disagg_by_grp'].value\n    data.sort(order='extreme_poe')\n    return rst_table(data[::-1])", "response": "Display the source groups contributing the most to the highest"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the mean and standard deviation for the resource table entry.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        C = self.COEFFS[imt]\n        mag = self._convert_magnitude(rup.mag)\n\n        mean = (\n            C['c1'] + C['c2'] * mag + C['c10'] * (mag - 6) ** 2 +\n            (C['c6'] + C['c7'] * mag) * np.log(dists.rjb + np.exp(C['c4']))\n        )\n        mean = clip_mean(imt, mean)\n\n        stddevs = self._compute_stddevs(C, dists.rjb.size, stddev_types)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zip_all(directory):\n    zips = []\n    for cwd, dirs, files in os.walk(directory):\n        if 'ssmLT.xml' in files:\n            zips.append(zip_source_model(os.path.join(cwd, 'ssmLT.xml')))\n        for f in files:\n            if f.endswith('.xml') and 'exposure' in f.lower():\n                zips.append(zip_exposure(os.path.join(cwd, f)))\n    total = sum(os.path.getsize(z) for z in zips)\n    logging.info('Generated %s of zipped data', general.humansize(total))", "response": "Zip all source models and exposures in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef zip_source_model(ssmLT, archive_zip='', log=logging.info):\n    basedir = os.path.dirname(ssmLT)\n    if os.path.basename(ssmLT) != 'ssmLT.xml':\n        orig = ssmLT\n        ssmLT = os.path.join(basedir, 'ssmLT.xml')\n        with open(ssmLT, 'wb') as f:\n            f.write(open(orig, 'rb').read())\n\n    archive_zip = archive_zip or os.path.join(basedir, 'ssmLT.zip')\n    if os.path.exists(archive_zip):\n        sys.exit('%s exists already' % archive_zip)\n    oq = mock.Mock(inputs={'source_model_logic_tree': ssmLT})\n    checksum = readinput.get_checksum32(oq)\n    checkfile = os.path.join(os.path.dirname(ssmLT), 'CHECKSUM.txt')\n    with open(checkfile, 'w') as f:\n        f.write(str(checksum))\n    files = logictree.collect_info(ssmLT).smpaths + [\n        os.path.abspath(ssmLT), os.path.abspath(checkfile)]\n    general.zipfiles(files, archive_zip, log=log, cleanup=True)\n    return archive_zip", "response": "Zip the source model files starting from the smmLT. xml file and the checksum. txt file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nzipping an exposure. xml file with all its. csv subfiles", "response": "def zip_exposure(exposure_xml, archive_zip='', log=logging.info):\n    \"\"\"\n    Zip an exposure.xml file with all its .csv subfiles (if any)\n    \"\"\"\n    archive_zip = archive_zip or exposure_xml[:-4] + '.zip'\n    if os.path.exists(archive_zip):\n        sys.exit('%s exists already' % archive_zip)\n    [exp] = Exposure.read_headers([exposure_xml])\n    files = [exposure_xml] + exp.datafiles\n    general.zipfiles(files, archive_zip, log=log, cleanup=True)\n    return archive_zip"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zip_job(job_ini, archive_zip='', risk_ini='', oq=None, log=logging.info):\n    if not os.path.exists(job_ini):\n        sys.exit('%s does not exist' % job_ini)\n    archive_zip = archive_zip or 'job.zip'\n    if isinstance(archive_zip, str):  # actually it should be path-like\n        if not archive_zip.endswith('.zip'):\n            sys.exit('%s does not end with .zip' % archive_zip)\n        if os.path.exists(archive_zip):\n            sys.exit('%s exists already' % archive_zip)\n    # do not validate to avoid permissions error on the export_dir\n    oq = oq or readinput.get_oqparam(job_ini, validate=False)\n    if risk_ini:\n        risk_ini = os.path.normpath(os.path.abspath(risk_ini))\n        risk_inputs = readinput.get_params([risk_ini])['inputs']\n        del risk_inputs['job_ini']\n        oq.inputs.update(risk_inputs)\n    files = readinput.get_input_files(oq)\n    if risk_ini:\n        files = [risk_ini] + files\n    return general.zipfiles(files, archive_zip, log=log)", "response": "Zip the given job. ini file into the given archive."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_report(job_ini, output_dir=None):\n    calc_id = logs.init()\n    oq = readinput.get_oqparam(job_ini)\n    if oq.calculation_mode == 'classical':\n        oq.calculation_mode = 'preclassical'\n    oq.ground_motion_fields = False\n    output_dir = output_dir or os.path.dirname(job_ini)\n    from openquake.calculators import base  # ugly\n    calc = base.calculators(oq, calc_id)\n    calc.save_params()  # needed to save oqparam\n\n    # some taken is care so that the real calculation is not run:\n    # the goal is to extract information about the source management only\n    calc.pre_execute()\n    if oq.calculation_mode == 'preclassical':\n        calc.execute()\n    rw = ReportWriter(calc.datastore)\n    rw.make_report()\n    report = (os.path.join(output_dir, 'report.rst') if output_dir\n              else calc.datastore.export_path('report.rst'))\n    try:\n        rw.save(report)\n    except IOError as exc:  # permission error\n        sys.stderr.write(str(exc) + '\\n')\n    readinput.exposure = None  # ugly hack\n    return report", "response": "Write a report. csv file with information about the calculation and the source management only of the calculation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the view named name to the report text", "response": "def add(self, name, obj=None):\n        \"\"\"Add the view named `name` to the report text\"\"\"\n        if obj:\n            text = '\\n::\\n\\n' + indent(str(obj))\n        else:\n            text = views.view(name, self.dstore)\n        if text:\n            title = self.title[name]\n            line = '-' * len(title)\n            self.text += '\\n'.join(['\\n\\n' + title, line, text])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the report and return a restructed text string", "response": "def make_report(self):\n        \"\"\"Build the report and return a restructed text string\"\"\"\n        oq, ds = self.oq, self.dstore\n        for name in ('params', 'inputs'):\n            self.add(name)\n        if 'csm_info' in ds:\n            self.add('csm_info')\n            if ds['csm_info'].source_models[0].name != 'scenario':\n                # required_params_per_trt makes no sense for GMFs from file\n                self.add('required_params_per_trt')\n            self.add('rlzs_assoc', ds['csm_info'].get_rlzs_assoc())\n        if 'csm_info' in ds:\n            self.add('ruptures_per_trt')\n        if 'rup_data' in ds:\n            self.add('ruptures_events')\n        if oq.calculation_mode in ('event_based_risk',):\n            self.add('avglosses_data_transfer')\n        if 'exposure' in oq.inputs:\n            self.add('exposure_info')\n        if 'source_info' in ds:\n            self.add('slow_sources')\n            self.add('times_by_source_class')\n            self.add('dupl_sources')\n        if 'task_info' in ds:\n            self.add('task_info')\n            tasks = set(ds['task_info'])\n            if 'classical' in tasks:\n                self.add('task_hazard:0')\n                self.add('task_hazard:-1')\n            self.add('job_info')\n        if 'performance_data' in ds:\n            self.add('performance')\n        return self.text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the report to a file", "response": "def save(self, fname):\n        \"\"\"Save the report\"\"\"\n        with open(fname, 'wb') as f:\n            f.write(encode(self.text))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_to_LHC(imt):\n    # get period t\n    if isinstance(imt, SA):\n        t = imt.period\n    else:\n        t = 0.01\n\n    T1 = 0.08\n    T2 = 0.56\n    T3 = 4.40\n    T4 = 8.70\n    R1 = 1.106\n    R2 = 1.158\n    R3 = 1.178\n    R4 = 1.241\n    R5 = 1.241\n\n    Ratio = max(R1,\n                max(min(R1+(R2-R1)/np.log(T2/T1)*np.log(t/T1),\n                        R2+(R3-R2)/np.log(T3/T2)*np.log(t/T2)),\n                    min(R3+(R4-R3)/np.log(T4/T3)*np.log(t/T3), R5)))\n    SF = np.log(Ratio)\n\n    return SF", "response": "Converts from GMRotI50 to Larger of two horizontal components using the global equation of GMRotI50 and D and Kishida T."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the mean value of a single entry in the logarithmic equation.", "response": "def _get_mean(self, sites, C, ln_y_ref, exp1, exp2, v1):\n        \"\"\"\n        Add site effects to an intensity.\n\n        Implements eq. 5\n        \"\"\"\n        # we do not support estimating of basin depth and instead\n        # rely on it being available (since we require it).\n        z1pt0 = sites.z1pt0\n\n        # we consider random variables being zero since we want\n        # to find the exact mean value.\n        eta = epsilon = 0\n\n        ln_y = (\n            # first line of eq. 13b\n            ln_y_ref + C['phi1'] *\n            np.log(np.clip(sites.vs30, -np.inf, v1) / 1130)\n            # second line\n            + C['phi2'] * (exp1 - exp2)\n            * np.log((np.exp(ln_y_ref) + C['phi4']) / C['phi4'])\n            # third line\n            + C['phi5']\n            * (1.0 - 1.0 / np.cosh(\n                C['phi6'] * (z1pt0 - C['phi7']).clip(0, np.inf)))\n            + C['phi8'] / np.cosh(0.15 * (z1pt0 - 15).clip(0, np.inf))\n            # fourth line\n            + eta + epsilon\n        )\n\n        return ln_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_v1(self, imt):\n\n        if imt == PGA():\n            v1 = 1800.\n\n        else:\n            T = imt.period\n            v1a = np.clip((1130 * (T / 0.75)**-0.11), 1130, np.inf)\n            v1 = np.clip(v1a, -np.inf, 1800.)\n\n        return v1", "response": "Calculates and returns the Bradley s V1 term based on the period of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the mean and standard deviation for the given intensity measure type and site.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n        # intensity on a reference soil is used for both mean\n        # and stddev calculations.\n        ln_y_ref = self._get_ln_y_ref(rup, dists, C)\n        # exp1 and exp2 are parts of eq. 7\n        exp1 = np.exp(C['phi3'] * (sites.vs30.clip(-np.inf, 1130) - 360))\n        exp2 = np.exp(C['phi3'] * (1130 - 360))\n        # v1 is the period dependent site term. The Vs30 above which, the\n        # amplification is constant\n        v1 = self._get_v1(imt)\n\n        mean = self._get_mean(sites, C, ln_y_ref, exp1, exp2, v1)\n        mean += convert_to_LHC(imt)\n\n        stddevs = self._get_stddevs(sites, rup, C, stddev_types,\n                                    ln_y_ref, exp1, exp2)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        if imt not in self.IMTS_TABLES:\n            raise ValueError(\n                'IMT %s not supported in FrankelEtAl1996NSHMP. ' % repr(imt) +\n                'FrankelEtAl1996NSHMP does not allow interpolation for ' +\n                'unsupported periods.'\n            )\n\n        mean = self._compute_mean(imt, rup.mag, dists.rhypo.copy())\n        mean = clip_mean(imt, mean)\n\n        stddevs = self._compute_stddevs(imt, dists.rhypo.shape, stddev_types)\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the given sites and dists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute mean value from lookup table.", "response": "def _compute_mean(self, imt, mag, rhypo):\n        \"\"\"\n        Compute mean value from lookup table.\n\n        Lookup table defines log10(IMT) (in g) for combinations of Mw and\n        log10(rhypo) values. ``mag`` is therefore converted from Mblg to Mw\n        using Atkinson and Boore 1987 conversion equation. Mean value is\n        finally converted from base 10 to base e.\n        \"\"\"\n        mag = np.zeros_like(rhypo) + self._convert_magnitude(mag)\n\n        # to avoid run time warning in case rhypo is zero set minimum distance\n        # to 10, which is anyhow the minimum distance allowed by the tables\n        rhypo[rhypo < 10] = 10\n        rhypo = np.log10(rhypo)\n\n        # create lookup table and interpolate it at magnitude/distance values\n        table = RectBivariateSpline(\n            self.MAGS, self.DISTS, self.IMTS_TABLES[imt].T\n        )\n        mean = table.ev(mag, rhypo)\n\n        # convert mean from base 10 to base e\n        return mean * np.log(10)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_recurrence_model(input_model):\n    if not isinstance(input_model, (TruncatedGRMFD,\n                                    EvenlyDiscretizedMFD,\n                                    YoungsCoppersmith1985MFD)):\n        raise ValueError('Recurrence model not recognised')\n    # Get model annual occurrence rates\n    annual_rates = input_model.get_annual_occurrence_rates()\n    annual_rates = np.array([[val[0], val[1]] for val in annual_rates])\n    # Get cumulative rates\n    cumulative_rates = np.array([np.sum(annual_rates[iloc:, 1])\n                                 for iloc in range(0, len(annual_rates), 1)])\n    return annual_rates, cumulative_rates", "response": "Returns the annual occurrence rates predicted by the recurrence model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_completeness_table(completeness, catalogue):\n    if isinstance(completeness, np.ndarray) and np.shape(completeness)[1] == 2:\n        return completeness\n    elif isinstance(completeness, float):\n        return np.array([[float(np.min(catalogue.data['year'])),\n                          completeness]])\n    elif completeness is None:\n        return np.array([[float(np.min(catalogue.data['year'])),\n                          np.min(catalogue.data['magnitude'])]])\n    else:\n        raise ValueError('Completeness representation not recognised')", "response": "Checks the completeness table for the given catalogue"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_recurrence_model(\n        input_model, catalogue, completeness, dmag=0.1, filename=None,\n        figure_size=(8, 6), filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Plot a calculated recurrence model over an observed catalogue, adjusted for\n    time-varying completeness\n    \"\"\"\n    annual_rates, cumulative_rates = _get_recurrence_model(input_model)\n\n    # Get observed annual recurrence\n    if not catalogue.end_year:\n        catalogue.update_end_year()\n    cent_mag, t_per, n_obs = get_completeness_counts(catalogue,\n                                                     completeness,\n                                                     dmag)\n    obs_rates = n_obs / t_per\n    cum_obs_rates = np.array([np.sum(obs_rates[i:])\n                              for i in range(len(obs_rates))])\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    ax.semilogy(cent_mag, obs_rates, 'bo')\n    ax.semilogy(annual_rates[:, 0], annual_rates[:, 1], 'b-')\n    ax.semilogy(cent_mag, cum_obs_rates, 'rs')\n    ax.semilogy(annual_rates[:, 0], cumulative_rates, 'r-')\n    ax.grid(which='both')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Annual Rate')\n    ax.legend(['Observed Incremental Rate',\n               'Model Incremental Rate',\n               'Observed Cumulative Rate',\n               'Model Cumulative Rate'])\n    ax.tick_params(labelsize=12)\n    _save_image(fig, filename, filetype, dpi)", "response": "Plots a calculated recurrence model over an observed catalogue adjusted for\n        time - varying completeness"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting a Gutenberg - Richter model of the recurrence hierarchy.", "response": "def plot_trunc_gr_model(\n        aval, bval, min_mag, max_mag, dmag,\n        catalogue=None, completeness=None, filename=None,\n        figure_size=(8, 6), filetype='png', dpi=300, ax=None):\n    \"\"\"\n    Plots a Gutenberg-Richter model\n    \"\"\"\n    input_model = TruncatedGRMFD(min_mag, max_mag, dmag, aval, bval)\n    if not catalogue:\n        # Plot only the modelled recurrence\n        annual_rates, cumulative_rates = _get_recurrence_model(input_model)\n\n        if ax is None:\n            fig, ax = plt.subplots(figsize=figure_size)\n        else:\n            fig = ax.get_figure()\n\n        ax.semilogy(annual_rates[:, 0], annual_rates[:, 1], 'b-')\n        ax.semilogy(annual_rates[:, 0], cumulative_rates, 'r-')\n        ax.xlabel('Magnitude')\n        ax.set_ylabel('Annual Rate')\n        ax.set_legend(['Incremental Rate', 'Cumulative Rate'])\n        _save_image(fig, filename, filetype, dpi)\n\n    else:\n        completeness = _check_completeness_table(completeness, catalogue)\n        plot_recurrence_model(\n            input_model, catalogue, completeness, dmag, filename=filename,\n            figure_size=figure_size, filetype=filetype, dpi=dpi, ax=ax)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tag_version(nrml_node):\n    version, tag = re.search(r'(nrml/[\\d\\.]+)\\}(\\w+)', nrml_node.tag).groups()\n    return tag, version", "response": "Extract the tag and version from a node of kind NRML"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a NRML file and return an associated Python object.", "response": "def to_python(fname, *args):\n    \"\"\"\n    Parse a NRML file and return an associated Python object. It works by\n    calling nrml.read() and node_to_obj() in sequence.\n    \"\"\"\n    [node] = read(fname)\n    return node_to_obj(node, fname, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading source models from a list of source files.", "response": "def read_source_models(fnames, converter, monitor):\n    \"\"\"\n    :param fnames:\n        list of source model files\n    :param converter:\n        a SourceConverter instance\n    :param monitor:\n        a :class:`openquake.performance.Monitor` instance\n    :yields:\n        SourceModel instances\n    \"\"\"\n    for fname in fnames:\n        if fname.endswith(('.xml', '.nrml')):\n            sm = to_python(fname, converter)\n        elif fname.endswith('.hdf5'):\n            sm = sourceconverter.to_python(fname, converter)\n        else:\n            raise ValueError('Unrecognized extension in %s' % fname)\n        sm.fname = fname\n        yield sm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a NRML file into a validated Node object.", "response": "def read(source, chatty=True, stop=None):\n    \"\"\"\n    Convert a NRML file into a validated Node object. Keeps\n    the entire tree in memory.\n\n    :param source:\n        a file name or file object open for reading\n    \"\"\"\n    vparser = ValidatingXmlParser(validators, stop)\n    nrml = vparser.parse_file(source)\n    if striptag(nrml.tag) != 'nrml':\n        raise ValueError('%s: expected a node of kind nrml, got %s' %\n                         (source, nrml.tag))\n    # extract the XML namespace URL ('http://openquake.org/xmlns/nrml/0.5')\n    xmlns = nrml.tag.split('}')[0][1:]\n    if xmlns != NRML05 and chatty:\n        # for the moment NRML04 is still supported, so we hide the warning\n        logging.debug('%s is at an outdated version: %s', source, xmlns)\n    nrml['xmlns'] = xmlns\n    nrml['xmlns:gml'] = GML_NAMESPACE\n    return nrml"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(nodes, output=sys.stdout, fmt='%.7E', gml=True, xmlns=None):\n    root = Node('nrml', nodes=nodes)\n    namespaces = {xmlns or NRML05: ''}\n    if gml:\n        namespaces[GML_NAMESPACE] = 'gml:'\n    with floatformat(fmt):\n        node_to_xml(root, output, namespaces)\n    if hasattr(output, 'mode') and '+' in output.mode:  # read-write mode\n        output.seek(0)\n        read(output)", "response": "Convert a list of nodes into a NRML file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a node into a string in NRML format", "response": "def to_string(node):\n    \"\"\"\n    Convert a node into a string in NRML format\n    \"\"\"\n    with io.BytesIO() as f:\n        write([node], f)\n        return f.getvalue().decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        C = self.COEFFS[imt]\n        # Get the mean\n        mean = self.get_mean(C, rup, sites, dists)\n        if imt.name == \"PGV\":\n            # Convert from log10 m/s to ln cm/s\n            mean = np.log((10.0 ** mean) * 100.)\n        else:\n            # convert from log10 m/s/s to ln g\n            mean = np.log((10.0 ** mean) / g)\n\n        # Get the standard deviations\n        stddevs = self.get_stddevs(C, mean.shape, stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mean(self, C, rup, sites, dists):\n        # W2 needs to be a 1 by 5 matrix (not a vector\n        w_2 = np.array([\n            [C[\"W_21\"], C[\"W_22\"], C[\"W_23\"], C[\"W_24\"], C[\"W_25\"]]\n            ])\n        # Gets the style of faulting dummy variable\n        sof = self._get_sof_dummy_variable(rup.rake)\n        # Get the normalised coefficients\n        p_n = self.get_pn(rup, sites, dists, sof)\n        mean = np.zeros_like(dists.rjb)\n        # Need to loop over sites - maybe this can be improved in future?\n        # ndenumerate is used to allow for application to 2-D arrays\n        for idx, rval in np.ndenumerate(p_n[0]):\n            # Place normalised coefficients into a single array\n            p_n_i = np.array([rval, p_n[1], p_n[2][idx], p_n[3], p_n[4]])\n            # Executes the main ANN model\n            mean_i = np.dot(w_2, np.tanh(np.dot(self.W_1, p_n_i) + self.B_1))\n            mean[idx] = (0.5 * (mean_i + C[\"B_2\"] + 1.0) *\n                         (C[\"tmax\"] - C[\"tmin\"])) + C[\"tmin\"]\n        return mean", "response": "Returns the mean ground motion in terms of log10 m/s / s implementing\n        equation 2 ( page 502 )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n        mean = (self._compute_style_of_faulting_term(rup, C) +\n                self._compute_magnitude_scaling(rup.mag, C) +\n                self._compute_distance_scaling(dists.rjb, C) +\n                self._compute_site_term(sites.vs30, C))\n\n        stddevs = self._get_stddevs(C, stddev_types, num_sites=len(sites.vs30))\n\n        return mean, stddevs", "response": "This method computes the mean and standard deviation of the resource table entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_distance_scaling(self, rjb, C):\n        # Calculate distance according to Page 141, Eq 2.\n        rdist = np.sqrt((rjb ** 2.) + (C['h'] ** 2.))\n        return C['B5'] * np.log(rdist)", "response": "Compute distance scaling term according to page 141 Eq 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes magnitude scaling term in equation 1 page 141.", "response": "def _compute_magnitude_scaling(self, mag, C):\n        \"\"\"\n        Compute magnitude-scaling term (Page 141, Eq 1)\n        \"\"\"\n        dmag = mag - 6.\n        return (C['B2'] * dmag) + (C['B3'] * (dmag ** 2.))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_style_of_faulting_term(self, rup, C):\n        if np.abs(rup.rake) <= 30.0 or (180.0 - np.abs(rup.rake)) <= 30.0:\n            # strike-slip\n            return C['B1ss']\n        elif rup.rake > 30.0 and rup.rake < 150.0:\n            # reverse\n            return C['B1rv']\n        else:\n            # unspecified (also includes Normal faulting!)\n            return C['B1all']", "response": "Compute the style of faulting term in equation 1 page 103."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extract dictionaries of coefficients specific to required\n        # intensity measure type and for PGA\n        C = self.COEFFS[imt]\n        C_PGA = self.COEFFS[PGA()]\n\n        # Get mean and standard deviation of PGA on rock (Vs30 1100 m/s^2)\n        pga1100 = np.exp(self.get_mean_values(C_PGA, sites, rup, dists, None))\n        # Get mean and standard deviations for IMT\n        mean = self.get_mean_values(C, sites, rup, dists, pga1100)\n        if imt.name == \"SA\" and imt.period <= 0.25:\n            # According to Campbell & Bozorgnia (2013) [NGA West 2 Report]\n            # If Sa (T) < PGA for T < 0.25 then set mean Sa(T) to mean PGA\n            # Get PGA on soil\n            pga = self.get_mean_values(C_PGA, sites, rup, dists, pga1100)\n            idx = mean <= pga\n            mean[idx] = pga[idx]\n        # Get standard deviations\n        stddevs = self._get_stddevs(C,\n                                    C_PGA,\n                                    rup,\n                                    sites,\n                                    pga1100,\n                                    stddev_types)\n        return mean, stddevs", "response": "This method returns the mean and standard deviation of the object for the given intensity measure type and intensity measure period."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_mean_values(self, C, sites, rup, dists, a1100):\n        if isinstance(a1100, np.ndarray):\n            # Site model defined\n            temp_vs30 = sites.vs30\n            temp_z2pt5 = sites.z2pt5\n        else:\n            # Default site and basin model\n            temp_vs30 = 1100.0 * np.ones(len(sites.vs30))\n            temp_z2pt5 = self._select_basin_model(1100.0) *\\\n                np.ones_like(temp_vs30)\n\n        return (self._get_magnitude_term(C, rup.mag) +\n                self._get_geometric_attenuation_term(C, rup.mag, dists.rrup) +\n                self._get_style_of_faulting_term(C, rup) +\n                self._get_hanging_wall_term(C, rup, dists) +\n                self._get_shallow_site_response_term(C, temp_vs30, a1100) +\n                self._get_basin_response_term(C, temp_z2pt5) +\n                self._get_hypocentral_depth_term(C, rup) +\n                self._get_fault_dip_term(C, rup) +\n                self._get_anelastic_attenuation_term(C, dists.rrup))", "response": "Returns the mean values for a specific IMTVersion."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_magnitude_term(self, C, mag):\n        f_mag = C[\"c0\"] + C[\"c1\"] * mag\n        if (mag > 4.5) and (mag <= 5.5):\n            return f_mag + (C[\"c2\"] * (mag - 4.5))\n        elif (mag > 5.5) and (mag <= 6.5):\n            return f_mag + (C[\"c2\"] * (mag - 4.5)) + (C[\"c3\"] * (mag - 5.5))\n        elif mag > 6.5:\n            return f_mag + (C[\"c2\"] * (mag - 4.5)) + (C[\"c3\"] * (mag - 5.5)) +\\\n                (C[\"c4\"] * (mag - 6.5))\n        else:\n            return f_mag", "response": "Returns the magnitude scaling term defined in equation 2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_geometric_attenuation_term(self, C, mag, rrup):\n        return (C[\"c5\"] + C[\"c6\"] * mag) * np.log(np.sqrt((rrup ** 2.) +\n                                                          (C[\"c7\"] ** 2.)))", "response": "Returns the geometric attenuation term defined in equation 3. 1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the style - of - faulting scaling term defined in equations 4 to 6", "response": "def _get_style_of_faulting_term(self, C, rup):\n        \"\"\"\n        Returns the style-of-faulting scaling term defined in equations 4 to 6\n        \"\"\"\n        if (rup.rake > 30.0) and (rup.rake < 150.):\n            frv = 1.0\n            fnm = 0.0\n        elif (rup.rake > -150.0) and (rup.rake < -30.0):\n            fnm = 1.0\n            frv = 0.0\n        else:\n            fnm = 0.0\n            frv = 0.0\n\n        fflt_f = (self.CONSTS[\"c8\"] * frv) + (C[\"c9\"] * fnm)\n        if rup.mag <= 4.5:\n            fflt_m = 0.0\n        elif rup.mag > 5.5:\n            fflt_m = 1.0\n        else:\n            fflt_m = rup.mag - 4.5\n        return fflt_f * fflt_m"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_hanging_wall_term(self, C, rup, dists):\n        return (C[\"c10\"] *\n                self._get_hanging_wall_coeffs_rx(C, rup, dists.rx) *\n                self._get_hanging_wall_coeffs_rrup(dists) *\n                self._get_hanging_wall_coeffs_mag(C, rup.mag) *\n                self._get_hanging_wall_coeffs_ztor(rup.ztor) *\n                self._get_hanging_wall_coeffs_dip(rup.dip))", "response": "Returns the hanging wall scaling term defined in equations 7 to 16\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_hanging_wall_coeffs_rx(self, C, rup, r_x):\n        # Define coefficients R1 and R2\n        r_1 = rup.width * cos(radians(rup.dip))\n        r_2 = 62.0 * rup.mag - 350.0\n        fhngrx = np.zeros(len(r_x))\n        # Case when 0 <= Rx <= R1\n        idx = np.logical_and(r_x >= 0., r_x < r_1)\n        fhngrx[idx] = self._get_f1rx(C, r_x[idx], r_1)\n        # Case when Rx > R1\n        idx = r_x >= r_1\n        f2rx = self._get_f2rx(C, r_x[idx], r_1, r_2)\n        f2rx[f2rx < 0.0] = 0.0\n        fhngrx[idx] = f2rx\n        return fhngrx", "response": "Returns the hanging wall r - x caling term defined in equation 7 to 12"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the f1 scaling coefficient defined in equation 9. 1.", "response": "def _get_f1rx(self, C, r_x, r_1):\n        \"\"\"\n        Defines the f1 scaling coefficient defined in equation 9\n        \"\"\"\n        rxr1 = r_x / r_1\n        return C[\"h1\"] + (C[\"h2\"] * rxr1) + (C[\"h3\"] * (rxr1 ** 2.))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the f2 scaling coefficient defined in equation 10", "response": "def _get_f2rx(self, C, r_x, r_1, r_2):\n        \"\"\"\n        Defines the f2 scaling coefficient defined in equation 10\n        \"\"\"\n        drx = (r_x - r_1) / (r_2 - r_1)\n        return self.CONSTS[\"h4\"] + (C[\"h5\"] * drx) + (C[\"h6\"] * (drx ** 2.))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the hanging wall rrup term defined in equation 13", "response": "def _get_hanging_wall_coeffs_rrup(self, dists):\n        \"\"\"\n        Returns the hanging wall rrup term defined in equation 13\n        \"\"\"\n        fhngrrup = np.ones(len(dists.rrup))\n        idx = dists.rrup > 0.0\n        fhngrrup[idx] = (dists.rrup[idx] - dists.rjb[idx]) / dists.rrup[idx]\n        return fhngrrup"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the hanging wall magnitude term defined in equation 14", "response": "def _get_hanging_wall_coeffs_mag(self, C, mag):\n        \"\"\"\n        Returns the hanging wall magnitude term defined in equation 14\n        \"\"\"\n        if mag < 5.5:\n            return 0.0\n        elif mag > 6.5:\n            return 1.0 + C[\"a2\"] * (mag - 6.5)\n        else:\n            return (mag - 5.5) * (1.0 + C[\"a2\"] * (mag - 6.5))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_hypocentral_depth_term(self, C, rup):\n        if rup.hypo_depth <= 7.0:\n            fhyp_h = 0.0\n        elif rup.hypo_depth > 20.0:\n            fhyp_h = 13.0\n        else:\n            fhyp_h = rup.hypo_depth - 7.0\n\n        if rup.mag <= 5.5:\n            fhyp_m = C[\"c17\"]\n        elif rup.mag > 6.5:\n            fhyp_m = C[\"c18\"]\n        else:\n            fhyp_m = C[\"c17\"] + ((C[\"c18\"] - C[\"c17\"]) * (rup.mag - 5.5))\n        return fhyp_h * fhyp_m", "response": "Returns the hypocentral depth scaling term defined in equations 21 - 23"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the fault dip term defined in equation 24", "response": "def _get_fault_dip_term(self, C, rup):\n        \"\"\"\n        Returns the fault dip term, defined in equation 24\n        \"\"\"\n        if rup.mag < 4.5:\n            return C[\"c19\"] * rup.dip\n        elif rup.mag > 5.5:\n            return 0.0\n        else:\n            return C[\"c19\"] * (5.5 - rup.mag) * rup.dip"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the anelastic attenuation term defined in equation 25", "response": "def _get_anelastic_attenuation_term(self, C, rrup):\n        \"\"\"\n        Returns the anelastic attenuation term defined in equation 25\n        \"\"\"\n        f_atn = np.zeros(len(rrup))\n        idx = rrup >= 80.0\n        f_atn[idx] = (C[\"c20\"] + C[\"Dc20\"]) * (rrup[idx] - 80.0)\n        return f_atn"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _select_basin_model(self, vs30):\n        if self.CONSTS[\"SJ\"]:\n            # Japan Basin Model - Equation 34 of Campbell & Bozorgnia (2014)\n            return np.exp(5.359 - 1.102 * np.log(vs30))\n        else:\n            # California Basin Model - Equation 33 of\n            # Campbell & Bozorgnia (2014)\n            return np.exp(7.089 - 1.144 * np.log(vs30))", "response": "Select the preferred basin model for the given set of Vs30."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the basin response term defined in equation 20", "response": "def _get_basin_response_term(self, C, z2pt5):\n        \"\"\"\n        Returns the basin response term defined in equation 20\n        \"\"\"\n        f_sed = np.zeros(len(z2pt5))\n        idx = z2pt5 < 1.0\n        f_sed[idx] = (C[\"c14\"] + C[\"c15\"] * float(self.CONSTS[\"SJ\"])) *\\\n            (z2pt5[idx] - 1.0)\n        idx = z2pt5 > 3.0\n        f_sed[idx] = C[\"c16\"] * C[\"k3\"] * exp(-0.75) *\\\n            (1.0 - np.exp(-0.25 * (z2pt5[idx] - 3.0)))\n        return f_sed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_shallow_site_response_term(self, C, vs30, pga_rock):\n        vs_mod = vs30 / C[\"k1\"]\n        # Get linear global site response term\n        f_site_g = C[\"c11\"] * np.log(vs_mod)\n        idx = vs30 > C[\"k1\"]\n        f_site_g[idx] = f_site_g[idx] + (C[\"k2\"] * self.CONSTS[\"n\"] *\n                                         np.log(vs_mod[idx]))\n\n        # Get nonlinear site response term\n        idx = np.logical_not(idx)\n        if np.any(idx):\n            f_site_g[idx] = f_site_g[idx] + C[\"k2\"] * (\n                np.log(pga_rock[idx] +\n                       self.CONSTS[\"c\"] * (vs_mod[idx] ** self.CONSTS[\"n\"])) -\n                np.log(pga_rock[idx] + self.CONSTS[\"c\"])\n                )\n\n        # For Japan sites (SJ = 1) further scaling is needed (equation 19)\n        if self.CONSTS[\"SJ\"]:\n            fsite_j = np.log(vs_mod)\n            idx = vs30 > 200.0\n            if np.any(idx):\n                fsite_j[idx] = (C[\"c13\"] + C[\"k2\"] * self.CONSTS[\"n\"]) *\\\n                    fsite_j[idx]\n            idx = np.logical_not(idx)\n            if np.any(idx):\n                fsite_j[idx] = (C[\"c12\"] + C[\"k2\"] * self.CONSTS[\"n\"]) *\\\n                    (fsite_j[idx] - np.log(200.0 / C[\"k1\"]))\n\n            return f_site_g + fsite_j\n        else:\n            return f_site_g", "response": "Returns the shallow site response term defined in equations 17 18 and 19."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the standard deviations for the basement and intra - event system.", "response": "def _get_stddevs(self, C, C_PGA, rup, sites, pga1100, stddev_types):\n        \"\"\"\n        Returns the inter- and intra-event and total standard deviations\n        \"\"\"\n        # Get stddevs for PGA on basement rock\n        tau_lnpga_b, phi_lnpga_b = self._get_stddevs_pga(C_PGA, rup)\n        num_sites = len(sites.vs30)\n        # Get tau_lny on the basement rock\n        tau_lnyb = self._get_taulny(C, rup.mag)\n        # Get phi_lny on the basement rock\n        phi_lnyb = np.sqrt(self._get_philny(C, rup.mag) ** 2. -\n                           self.CONSTS[\"philnAF\"] ** 2.)\n        # Get site scaling term\n        alpha = self._get_alpha(C, sites.vs30, pga1100)\n        # Evaluate tau according to equation 29\n        tau = np.sqrt(\n            (tau_lnyb ** 2.) +\n            ((alpha ** 2.) * (tau_lnpga_b ** 2.)) +\n            (2.0 * alpha * C[\"rholny\"] * tau_lnyb * tau_lnpga_b))\n\n        # Evaluate phi according to equation 30\n        phi = np.sqrt(\n            (phi_lnyb ** 2.) +\n            (self.CONSTS[\"philnAF\"] ** 2.) +\n            ((alpha ** 2.) * (phi_lnpga_b ** 2.)) +\n            (2.0 * alpha * C[\"rholny\"] * phi_lnyb * phi_lnpga_b))\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt((tau ** 2.) + (phi ** 2.)) +\n                               np.zeros(num_sites))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(phi + np.zeros(num_sites))\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(tau + np.zeros(num_sites))\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_stddevs_pga(self, C, rup):\n        tau_lnpga_b = self._get_taulny(C, rup.mag)\n        phi_lnpga_b = np.sqrt(self._get_philny(C, rup.mag) ** 2. -\n                              self.CONSTS[\"philnAF\"] ** 2.)\n        return tau_lnpga_b, phi_lnpga_b", "response": "Returns the inter - and intra - event coefficients for PGA."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the inter-event random effects coefficient (tau) Equation 28.", "response": "def _get_taulny(self, C, mag):\n        \"\"\"\n        Returns the inter-event random effects coefficient (tau)\n        Equation 28.\n        \"\"\"\n        if mag <= 4.5:\n            return C[\"tau1\"]\n        elif mag >= 5.5:\n            return C[\"tau2\"]\n        else:\n            return C[\"tau2\"] + (C[\"tau1\"] - C[\"tau2\"]) * (5.5 - mag)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_philny(self, C, mag):\n        if mag <= 4.5:\n            return C[\"phi1\"]\n        elif mag >= 5.5:\n            return C[\"phi2\"]\n        else:\n            return C[\"phi2\"] + (C[\"phi1\"] - C[\"phi2\"]) * (5.5 - mag)", "response": "Returns the intra - event random effects coefficient ( phi"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_alpha(self, C, vs30, pga_rock):\n        alpha = np.zeros(len(pga_rock))\n        idx = vs30 < C[\"k1\"]\n        if np.any(idx):\n            af1 = pga_rock[idx] +\\\n                self.CONSTS[\"c\"] * ((vs30[idx] / C[\"k1\"]) ** self.CONSTS[\"n\"])\n            af2 = pga_rock[idx] + self.CONSTS[\"c\"]\n            alpha[idx] = C[\"k2\"] * pga_rock[idx] * ((1.0 / af1) - (1.0 / af2))\n        return alpha", "response": "Returns the alpha linearised functional relationship between the theraculiine and the PGA on rock. Equation 31."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decimal_year(year, month, day):\n    marker = np.array([0., 31., 59., 90., 120., 151., 181.,\n                       212., 243., 273., 304., 334.])\n    tmonth = (month - 1).astype(int)\n    day_count = marker[tmonth] + day - 1.\n    dec_year = year + (day_count / 365.)\n\n    return dec_year", "response": "Calculates the decimal year for a vector of dates\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decimal_time(year, month, day, hour, minute, second):\n    tmo = np.ones_like(year, dtype=int)\n    tda = np.ones_like(year, dtype=int)\n    tho = np.zeros_like(year, dtype=int)\n    tmi = np.zeros_like(year, dtype=int)\n    tse = np.zeros_like(year, dtype=float)\n    #\n    # Checking inputs\n    if any(month < 1) or any(month > 12):\n        raise ValueError('Month must be in [1, 12]')\n    if any(day < 1) or any(day > 31):\n        raise ValueError('Day must be in [1, 31]')\n    if any(hour < 0) or any(hour > 24):\n        raise ValueError('Hour must be in [0, 24]')\n    if any(minute < 0) or any(minute > 60):\n        raise ValueError('Minute must be in [0, 60]')\n    if any(second < 0) or any(second > 60):\n        raise ValueError('Second must be in [0, 60]')\n    #\n    # Initialising values\n    if any(month):\n        tmo = month\n    if any(day):\n        tda = day\n    if any(hour):\n        tho = hour\n    if any(minute):\n        tmi = minute\n    if any(second):\n        tse = second\n    #\n    # Computing decimal\n    tmonth = tmo - 1\n    day_count = MARKER_NORMAL[tmonth] + tda - 1\n    id_leap = leap_check(year)\n    leap_loc = np.where(id_leap)[0]\n    day_count[leap_loc] = MARKER_LEAP[tmonth[leap_loc]] + tda[leap_loc] - 1\n    year_secs = ((day_count.astype(float) * SECONDS_PER_DAY) + tse +\n                 (60. * tmi.astype(float)) + (3600. * tho.astype(float)))\n    dtime = year.astype(float) + (year_secs / (365. * 24. * 3600.))\n    dtime[leap_loc] = year[leap_loc].astype(float) + \\\n        (year_secs[leap_loc] / (366. * 24. * 3600.))\n    return dtime", "response": "Returns the full time as a decimal value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef haversine(lon1, lat1, lon2, lat2, radians=False, earth_rad=6371.227):\n    if not radians:\n        cfact = np.pi / 180.\n        lon1 = cfact * lon1\n        lat1 = cfact * lat1\n        lon2 = cfact * lon2\n        lat2 = cfact * lat2\n\n    # Number of locations in each set of points\n    if not np.shape(lon1):\n        nlocs1 = 1\n        lon1 = np.array([lon1])\n        lat1 = np.array([lat1])\n    else:\n        nlocs1 = np.max(np.shape(lon1))\n    if not np.shape(lon2):\n        nlocs2 = 1\n        lon2 = np.array([lon2])\n        lat2 = np.array([lat2])\n    else:\n        nlocs2 = np.max(np.shape(lon2))\n    # Pre-allocate array\n    distance = np.zeros((nlocs1, nlocs2))\n    i = 0\n    while i < nlocs2:\n        # Perform distance calculation\n        dlat = lat1 - lat2[i]\n        dlon = lon1 - lon2[i]\n        aval = (np.sin(dlat / 2.) ** 2.) + (np.cos(lat1) * np.cos(lat2[i]) *\n                                            (np.sin(dlon / 2.) ** 2.))\n        distance[:, i] = (2. * earth_rad * np.arctan2(np.sqrt(aval),\n                                                      np.sqrt(1 - aval))).T\n        i += 1\n    return distance", "response": "Calculates the geographical distance between two sets of locations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef greg2julian(year, month, day, hour, minute, second):\n    year = year.astype(float)\n    month = month.astype(float)\n    day = day.astype(float)\n\n    timeut = hour.astype(float) + (minute.astype(float) / 60.0) + \\\n        (second / 3600.0)\n\n    julian_time = ((367.0 * year) -\n                   np.floor(\n                       7.0 * (year + np.floor((month + 9.0) / 12.0)) / 4.0) -\n                   np.floor(3.0 *\n                            (np.floor((year + (month - 9.0) / 7.0) / 100.0) +\n                             1.0) / 4.0) +\n                   np.floor((275.0 * month) / 9.0) +\n                   day + 1721028.5 + (timeut / 24.0))\n    return julian_time", "response": "Function to convert a date from Gregorian to Julian format"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the piecewise linear function for a scalar variable xval.", "response": "def piecewise_linear_scalar(params, xval):\n    '''Piecewise linear function for a scalar variable xval (float).\n\n    :param params:\n        Piecewise linear parameters (numpy.ndarray) in the following form:\n        [slope_i,... slope_n, turning_point_i, ..., turning_point_n, intercept]\n        Length params === 2 * number_segments, e.g.\n        [slope_1, slope_2, slope_3, turning_point1, turning_point_2, intercept]\n    :param xval:\n        Value for evaluation of function (float)\n    :returns:\n        Piecewise linear function evaluated at point xval (float)\n    '''\n    n_params = len(params)\n    n_seg, remainder = divmod(n_params, 2)\n    if remainder:\n        raise ValueError(\n            'Piecewise Function requires 2 * nsegments parameters')\n\n    if n_seg == 1:\n        return params[1] + params[0] * xval\n\n    gradients = params[0:n_seg]\n    turning_points = params[n_seg: -1]\n    c_val = np.array([params[-1]])\n\n    for iloc in range(1, n_seg):\n        c_val = np.hstack(\n            [c_val, (c_val[iloc - 1] + gradients[iloc - 1] *\n                     turning_points[iloc - 1]) - (gradients[iloc] *\n                                                  turning_points[iloc - 1])])\n\n    if xval <= turning_points[0]:\n        return gradients[0] * xval + c_val[0]\n    elif xval > turning_points[-1]:\n        return gradients[-1] * xval + c_val[-1]\n    else:\n        select = np.nonzero(turning_points <= xval)[0][-1] + 1\n    return gradients[select] * xval + c_val[select]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample_truncated_gaussian_vector(data, uncertainties, bounds=None):\n    '''\n    Samples a Gaussian distribution subject to boundaries on the data\n\n    :param numpy.ndarray data:\n        Vector of N data values\n    :param numpy.ndarray uncertainties:\n        Vector of N data uncertainties\n    :param int number_bootstraps:\n        Number of bootstrap samples\n    :param tuple bounds:\n        (Lower, Upper) bound of data space\n    '''\n    nvals = len(data)\n    if bounds:\n        # if bounds[0] or (fabs(bounds[0]) < PRECISION):\n        if bounds[0] is not None:\n            lower_bound = (bounds[0] - data) / uncertainties\n        else:\n            lower_bound = -np.inf * np.ones_like(data)\n\n        # if bounds[1] or (fabs(bounds[1]) < PRECISION):\n        if bounds[1] is not None:\n            upper_bound = (bounds[1] - data) / uncertainties\n        else:\n            upper_bound = np.inf * np.ones_like(data)\n        sample = hmtk_truncnorm.rvs(lower_bound, upper_bound, size=nvals)\n\n    else:\n        sample = np.random.normal(0., 1., nvals)\n    return data + uncertainties * sample", "response": "Samples a Gaussian distribution subject to boundaries on the data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hmtk_histogram_2D(xvalues, yvalues, bins, x_offset=1.0E-10,\n                      y_offset=1.0E-10):\n    \"\"\"\n    See the explanation for the 1D case - now applied to 2D.\n\n    :param numpy.ndarray xvalues:\n        Values of x-data\n    :param numpy.ndarray yvalues:\n        Values of y-data\n    :param tuple bins:\n        Tuple containing bin intervals for x-data and y-data (as numpy arrays)\n    :param float x_offset:\n        Small amount to offset the x-bins for floating point precision\n    :param float y_offset:\n        Small amount to offset the y-bins for floating point precision\n    :returns:\n        Count in each bin (as float)\n    \"\"\"\n    xbins, ybins = (bins[0] - x_offset, bins[1] - y_offset)\n    n_x = len(xbins) - 1\n    n_y = len(ybins) - 1\n    counter = np.zeros([n_y, n_x], dtype=float)\n    for j in range(n_y):\n        y_idx = np.logical_and(yvalues >= ybins[j], yvalues < ybins[j + 1])\n        x_vals = xvalues[y_idx]\n        for i in range(n_x):\n            idx = np.logical_and(x_vals >= xbins[i], x_vals < xbins[i + 1])\n            counter[j, i] += float(np.sum(idx))\n    return counter.T", "response": "This function calculates the histogram of the 2D histogram of the log - likelihood of the two sets of data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bootstrap_histogram_1D(\n        values, intervals, uncertainties=None,\n        normalisation=False, number_bootstraps=None, boundaries=None):\n    '''\n    Bootstrap samples a set of vectors\n\n    :param numpy.ndarray values:\n        The data values\n    :param numpy.ndarray intervals:\n        The bin edges\n    :param numpy.ndarray uncertainties:\n        The standard deviations of each observation\n    :param bool normalisation:\n        If True then returns the histogram as a density function\n    :param int number_bootstraps:\n        Number of bootstraps\n    :param tuple boundaries:\n        (Lower, Upper) bounds on the data\n\n    :param returns:\n        1-D histogram of data\n\n    '''\n    if not number_bootstraps or np.all(np.fabs(uncertainties < PRECISION)):\n        # No bootstraps or all uncertaintes are zero - return ordinary\n        # histogram\n        #output = np.histogram(values, intervals)[0]\n        output = hmtk_histogram_1D(values, intervals)\n        if normalisation:\n            output = output / float(np.sum(output))\n        else:\n            output = output\n        return output\n    else:\n        temp_hist = np.zeros([len(intervals) - 1, number_bootstraps],\n                             dtype=float)\n        for iloc in range(0, number_bootstraps):\n            sample = sample_truncated_gaussian_vector(values,\n                                                      uncertainties,\n                                                      boundaries)\n            #output = np.histogram(sample, intervals)[0]\n            output = hmtk_histogram_1D(sample, intervals)\n            temp_hist[:, iloc] = output\n        output = np.sum(temp_hist, axis=1)\n        if normalisation:\n            output = output / float(np.sum(output))\n        else:\n            output = output / float(number_bootstraps)\n        return output", "response": "This function calculates the 1 - D histogram of the data for a set of bootstraps."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating a 2D histogram of data for a single bootstrap of the current set of resources.", "response": "def bootstrap_histogram_2D(\n        xvalues, yvalues, xbins, ybins,\n        boundaries=[None, None], xsigma=None, ysigma=None,\n        normalisation=False, number_bootstraps=None):\n    '''\n    Calculates a 2D histogram of data, allowing for normalisation and\n    bootstrap sampling\n\n    :param numpy.ndarray xvalues:\n        Data values of the first variable\n\n    :param numpy.ndarray yvalues:\n        Data values of the second variable\n\n    :param numpy.ndarray xbins:\n        Bin edges for the first variable\n\n    :param numpy.ndarray ybins:\n        Bin edges for the second variable\n\n    :param list boundaries:\n        List of (Lower, Upper) tuples corresponding to the bounds of the\n        two data sets\n\n    :param numpy.ndarray xsigma:\n        Error values (standard deviatons) on first variable\n\n    :param numpy.ndarray ysigma:\n        Error values (standard deviatons) on second variable\n\n    :param bool normalisation:\n        If True then returns the histogram as a density function\n\n    :param int number_bootstraps:\n        Number of bootstraps\n\n    :param returns:\n        2-D histogram of data\n    '''\n    if (xsigma is None and ysigma is None) or not number_bootstraps:\n        # No sampling - return simple 2-D histrogram\n        #output = np.histogram2d(xvalues, yvalues, bins=[xbins, ybins])[0]\n        output = hmtk_histogram_2D(xvalues, yvalues, bins=(xbins, ybins))\n        if normalisation:\n            output = output / float(np.sum(output))\n        return output\n\n    else:\n        if xsigma is None:\n            xsigma = np.zeros(len(xvalues), dtype=float)\n        if ysigma is None:\n            ysigma = np.zeros(len(yvalues), dtype=float)\n        temp_hist = np.zeros(\n            [len(xbins) - 1, len(ybins) - 1, number_bootstraps],\n            dtype=float)\n        for iloc in range(0, number_bootstraps):\n            xsample = sample_truncated_gaussian_vector(xvalues, xsigma,\n                                                       boundaries[0])\n            ysample = sample_truncated_gaussian_vector(yvalues, ysigma,\n                                                       boundaries[0])\n\n            # temp_hist[:, :, iloc] = np.histogram2d(xsample,\n            #                                       ysample,\n            #                                       bins=[xbins, ybins])[0]\n            temp_hist[:, :, iloc] = hmtk_histogram_2D(xsample,\n                                                      ysample,\n                                                      bins=(xbins, ybins))\n        if normalisation:\n            output = np.sum(temp_hist, axis=2)\n            output = output / np.sum(output)\n        else:\n            output = np.sum(temp_hist, axis=2) / float(number_bootstraps)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lonlat_to_laea(lon, lat, lon0, lat0, f_e=0.0, f_n=0.0):\n    lon = np.radians(lon)\n    lat = np.radians(lat)\n    lon0 = np.radians(lon0)\n    lat0 = np.radians(lat0)\n    q_0 = TO_Q(lat0)\n    q_p = TO_Q(np.pi / 2.)\n    q_val = TO_Q(lat)\n    beta = np.arcsin(q_val / q_p)\n    beta0 = np.arcsin(q_0 / q_p)\n    r_q = WGS84[\"a\"] * np.sqrt(q_p / 2.)\n    dval = WGS84[\"a\"] * (\n        np.cos(lat0) / np.sqrt(1.0 - (WGS84[\"e2\"] * (np.sin(lat0) ** 2.))) /\n        (r_q * np.cos(beta0)))\n    bval = r_q * np.sqrt(\n        2. / (1.0 + (np.sin(beta0) * np.sin(beta)) + (np.cos(beta) *\n                                                      np.cos(beta0) * np.cos(lon - lon0))))\n    easting = f_e + ((bval * dval) * (np.cos(beta) * np.sin(lon - lon0)))\n    northing = f_n + (bval / dval) * ((np.cos(beta0) * np.sin(beta)) -\n                                      (np.sin(beta0) * np.cos(beta) * np.cos(lon - lon0)))\n    return easting, northing", "response": "This function converts vectors of longitude and latitude into Lambert Azimuthal Azimuth"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the area of an OpenQuake polygon in square kilometres", "response": "def area_of_polygon(polygon):\n    \"\"\"\n    Returns the area of an OpenQuake polygon in square kilometres\n    \"\"\"\n    lon0 = np.mean(polygon.lons)\n    lat0 = np.mean(polygon.lats)\n    # Transform to lamber equal area projection\n    x, y = lonlat_to_laea(polygon.lons, polygon.lats, lon0, lat0)\n    # Build shapely polygons\n    poly = geometry.Polygon(zip(x, y))\n    return poly.area"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef input_dir(self):\n        return os.path.abspath(os.path.dirname(self.inputs['job_ini']))", "response": "Returns the absolute path to where the job. ini is located"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an instance of class : RjbEquivalent", "response": "def get_reqv(self):\n        \"\"\"\n        :returns: an instance of class:`RjbEquivalent` if reqv_hdf5 is set\n        \"\"\"\n        if 'reqv' not in self.inputs:\n            return\n        return {key: valid.RjbEquivalent(value)\n                for key, value in self.inputs['reqv'].items()}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the given set of GSIMs is valid", "response": "def check_gsims(self, gsims):\n        \"\"\"\n        :param gsims: a sequence of GSIM instances\n        \"\"\"\n        imts = set(from_string(imt).name for imt in self.imtls)\n        for gsim in gsims:\n            restrict_imts = gsim.DEFINED_FOR_INTENSITY_MEASURE_TYPES\n            if restrict_imts:\n                names = set(cls.__name__ for cls in restrict_imts)\n                invalid_imts = ', '.join(imts - names)\n                if invalid_imts:\n                    raise ValueError(\n                        'The IMT %s is not accepted by the GSIM %s' %\n                        (invalid_imts, gsim))\n\n            if 'site_model' not in self.inputs:\n                # look at the required sites parameters: they must have\n                # a valid value; the other parameters can keep a NaN\n                # value since they are not used by the calculator\n                for param in gsim.REQUIRES_SITES_PARAMETERS:\n                    if param in ('lon', 'lat'):  # no check\n                        continue\n                    param_name = self.siteparam[param]\n                    param_value = getattr(self, param_name)\n                    if (isinstance(param_value, float) and\n                            numpy.isnan(param_value)):\n                        raise ValueError(\n                            'Please set a value for %r, this is required by '\n                            'the GSIM %s' % (param_name, gsim))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ses_ratio(self):\n        if self.investigation_time is None:\n            raise ValueError('Missing investigation_time in the .ini file')\n        return (self.risk_investigation_time or self.investigation_time) / (\n            self.investigation_time * self.ses_per_logic_tree_path)", "response": "The ratio of the risk investigation time and the ses per logic tree path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef all_cost_types(self):\n        # rt has the form 'vulnerability/structural', 'fragility/...', ...\n        costtypes = sorted(rt.rsplit('/')[1] for rt in self.risk_files)\n        if not costtypes and self.hazard_calculation_id:\n            with util.read(self.hazard_calculation_id) as ds:\n                parent = ds['oqparam']\n            self._risk_files = get_risk_files(parent.inputs)\n            costtypes = sorted(rt.rsplit('/')[1] for rt in self.risk_files)\n        return costtypes", "response": "Return the cost types of the computation including occupants and vulnerability and fragility."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef min_iml(self):\n        mini = self.minimum_intensity\n        if mini:\n            for imt in self.imtls:\n                try:\n                    mini[imt] = calc.filters.getdefault(mini, imt)\n                except KeyError:\n                    raise ValueError(\n                        'The parameter `minimum_intensity` in the job.ini '\n                        'file is missing the IMT %r' % imt)\n        if 'default' in mini:\n            del mini['default']\n        return F32([mini.get(imt, 0) for imt in self.imtls])", "response": "returns a numpy array of intensities one per IMT\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_risk_imtls(self, risk_models):\n        # NB: different loss types may have different IMLs for the same IMT\n        # in that case we merge the IMLs\n        imtls = {}\n        for taxonomy, risk_functions in risk_models.items():\n            for risk_type, rf in risk_functions.items():\n                imt = rf.imt\n                from_string(imt)  # make sure it is a valid IMT\n                imls = list(rf.imls)\n                if imt in imtls and imtls[imt] != imls:\n                    logging.debug(\n                        'Different levels for IMT %s: got %s, expected %s',\n                        imt, imls, imtls[imt])\n                    imtls[imt] = sorted(set(imls + imtls[imt]))\n                else:\n                    imtls[imt] = imls\n        self.risk_imtls = imtls\n        if self.uniform_hazard_spectra:\n            self.check_uniform_hazard_spectra()", "response": "Set the attribute risk_imtls."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a composite dtype for the HMAP table.", "response": "def hmap_dt(self):  # used for CSV export\n        \"\"\"\n        :returns: a composite dtype (imt, poe)\n        \"\"\"\n        return numpy.dtype([('%s-%s' % (imt, poe), F32)\n                            for imt in self.imtls for poe in self.poes])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uhs_dt(self):  # used for CSV and NPZ export\n        imts_dt = numpy.dtype([(imt, F32) for imt in self.imtls\n                               if imt.startswith(('PGA', 'SA'))])\n        return numpy.dtype([(str(poe), imts_dt) for poe in self.poes])", "response": "returns a composity dtype for the uhs table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the IMTs with a period as objects", "response": "def imt_periods(self):\n        \"\"\"\n        :returns: the IMTs with a period, as objects\n        \"\"\"\n        imts = []\n        for im in self.imtls:\n            imt = from_string(im)\n            if hasattr(imt, 'period'):\n                imts.append(imt)\n        return imts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef imt_dt(self, dtype=F64):\n        return numpy.dtype([(imt, dtype) for imt in self.imtls])", "response": "returns a numpy dtype of the intensity table"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lti(self):\n        return {lt: i for i, (lt, dt) in enumerate(self.loss_dt_list())}", "response": "Dictionary extended_loss_type -> extended_loss_type index\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loss_dt_list(self, dtype=F32):\n        loss_types = self.all_cost_types\n        dts = [(str(lt), dtype) for lt in loss_types]\n        return dts", "response": "Return a data type list of all loss types in the current node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loss_maps_dt(self, dtype=F32):\n        ltypes = self.loss_dt(dtype).names\n        lst = [('poe-%s' % poe, dtype) for poe in self.conditional_loss_poes]\n        return numpy.dtype([(lt, lst) for lt in ltypes])", "response": "Return a composite data type for loss maps"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gmf_data_dt(self):\n        return numpy.dtype(\n            [('rlzi', U16), ('sid', U32),\n             ('eid', U64), ('gmv', (F32, (len(self.imtls),)))])", "response": "Return a composite data type for the GMFs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if there are no intensity measure levels in this category.", "response": "def no_imls(self):\n        \"\"\"\n        Return True if there are no intensity measure levels\n        \"\"\"\n        return all(numpy.isnan(ls).any() for ls in self.imtls.values())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef correl_model(self):\n        correl_name = self.ground_motion_correlation_model\n        if correl_name is None:  # no correlation model\n            return\n        correl_model_cls = getattr(\n            correlation, '%sCorrelationModel' % correl_name)\n        return correl_model_cls(**self.ground_motion_correlation_params)", "response": "Return a correlation model object. See OpenQuake. hazardlib. correlation. CorrelationModel for more info."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding the names of the possible types of the user s data.", "response": "def get_kinds(self, kind, R):\n        \"\"\"\n        Yield 'rlz-000', 'rlz-001', ...', 'mean', 'quantile-0.1', ...\n        \"\"\"\n        stats = self.hazard_stats()\n        if kind == 'stats':\n            yield from stats\n            return\n        elif kind == 'rlzs':\n            for r in range(R):\n                yield 'rlz-%d' % r\n            return\n        elif kind:\n            yield kind\n            return\n        # default: yield stats (and realizations if required)\n        if R > 1 and self.individual_curves or not stats:\n            for r in range(R):\n                yield 'rlz-%03d' % r\n        yield from stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hazard_stats(self):\n        names = []  # name of statistical functions\n        funcs = []  # statistical functions of kind func(values, weights)\n        if self.mean_hazard_curves:\n            names.append('mean')\n            funcs.append(stats.mean_curve)\n        if self.std_hazard_curves:\n            names.append('std')\n            funcs.append(stats.std_curve)\n        for q in self.quantiles:\n            names.append('quantile-%s' % q)\n            funcs.append(functools.partial(stats.quantile_curve, q))\n        if self.max_hazard_curves:\n            names.append('max')\n            funcs.append(stats.max_curve)\n        return dict(zip(names, funcs))", "response": "Return a list of item with the statistical functions defined for the the\n            class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_valid_geometry(self):\n        has_sites = (self.sites is not None or 'sites' in self.inputs\n                     or 'site_model' in self.inputs)\n        if not has_sites and not self.ground_motion_fields:\n            # when generating only the ruptures you do not need the sites\n            return True\n        if ('gmfs' in self.inputs and not has_sites and\n                not self.inputs['gmfs'].endswith('.xml')):\n            raise ValueError('Missing sites or sites_csv in the .ini file')\n        elif ('risk' in self.calculation_mode or\n                'damage' in self.calculation_mode or\n                'bcr' in self.calculation_mode):\n            return True  # no check on the sites for risk\n        flags = dict(\n            sites=bool(self.sites),\n            sites_csv=self.inputs.get('sites', 0),\n            hazard_curves_csv=self.inputs.get('hazard_curves', 0),\n            gmfs_csv=self.inputs.get('gmfs', 0),\n            region=bool(self.region and self.region_grid_spacing))\n        # NB: below we check that all the flags\n        # are mutually exclusive\n        return sum(bool(v) for v in flags.values()) == 1 or self.inputs.get(\n            'exposure') or self.inputs.get('site_model')", "response": "Check if the geometry of the rupture is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the poes list is non - empty.", "response": "def is_valid_poes(self):\n        \"\"\"\n        When computing hazard maps and/or uniform hazard spectra,\n        the poes list must be non-empty.\n        \"\"\"\n        if self.hazard_maps or self.uniform_hazard_spectra:\n            return bool(self.poes)\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the maximum distance is valid.", "response": "def is_valid_maximum_distance(self):\n        \"\"\"\n        Invalid maximum_distance={maximum_distance}: {error}\n        \"\"\"\n        if 'gsim_logic_tree' not in self.inputs:\n            return True  # don't apply validation\n        gsim_lt = self.inputs['gsim_logic_tree']\n        trts = set(self.maximum_distance)\n        unknown = ', '.join(trts - set(self._gsims_by_trt) - set(['default']))\n        if unknown:\n            self.error = ('setting the maximum_distance for %s which is '\n                          'not in %s' % (unknown, gsim_lt))\n            return False\n        for trt, val in self.maximum_distance.items():\n            if val <= 0:\n                self.error = '%s=%r < 0' % (trt, val)\n                return False\n            elif trt not in self._gsims_by_trt and trt != 'default':\n                self.error = 'tectonic region %r not in %s' % (trt, gsim_lt)\n                return False\n        if 'default' not in trts and trts < set(self._gsims_by_trt):\n            missing = ', '.join(set(self._gsims_by_trt) - trts)\n            self.error = 'missing distance for %s and no default' % missing\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the intensity measure types are set directly and if not raises an exception.", "response": "def is_valid_intensity_measure_types(self):\n        \"\"\"\n        If the IMTs and levels are extracted from the risk models,\n        they must not be set directly. Moreover, if\n        `intensity_measure_types_and_levels` is set directly,\n        `intensity_measure_types` must not be set.\n        \"\"\"\n        if self.ground_motion_correlation_model:\n            for imt in self.imtls:\n                if not (imt.startswith('SA') or imt == 'PGA'):\n                    raise ValueError(\n                        'Correlation model %s does not accept IMT=%s' % (\n                            self.ground_motion_correlation_model, imt))\n        if self.risk_files:  # IMTLs extracted from the risk files\n            return (self.intensity_measure_types is None and\n                    self.intensity_measure_types_and_levels is None)\n        elif not hasattr(self, 'hazard_imtls') and not hasattr(\n                self, 'risk_imtls'):\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_valid_intensity_measure_levels(self):\n        invalid = self.no_imls() and not self.risk_files and (\n            self.hazard_curves_from_gmfs or self.calculation_mode in\n            ('classical', 'disaggregation'))\n        return not invalid", "response": "Returns True if intensity measure types and levels are set and not extracted from the risk models."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the export_dir is set and if it exists create it if it does not exist.", "response": "def is_valid_export_dir(self):\n        \"\"\"\n        export_dir={export_dir} must refer to a directory,\n        and the user must have the permission to write on it.\n        \"\"\"\n        if self.export_dir and not os.path.isabs(self.export_dir):\n            self.export_dir = os.path.normpath(\n                os.path.join(self.input_dir, self.export_dir))\n        if not self.export_dir:\n            self.export_dir = os.path.expanduser('~')  # home directory\n            logging.warning('export_dir not specified. Using export_dir=%s'\n                            % self.export_dir)\n            return True\n        elif not os.path.exists(self.export_dir):\n            try:\n                os.makedirs(self.export_dir)\n            except PermissionError:\n                return False\n            return True\n        return os.path.isdir(self.export_dir) and os.access(\n            self.export_dir, os.W_OK)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the sites are valid.", "response": "def is_valid_sites(self):\n        \"\"\"\n        The sites are overdetermined\n        \"\"\"\n        if 'site_model' in self.inputs and 'sites' in self.inputs:\n            return False\n        elif 'site_model' in self.inputs and self.sites:\n            return False\n        elif 'sites' in self.inputs and self.sites:\n            return False\n        elif self.sites and self.region and self.region_grid_spacing:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the complex_fault_mesh_spacing attribute is set and sets it to self. rupture_mesh_spacing.", "response": "def is_valid_complex_fault_mesh_spacing(self):\n        \"\"\"\n        The `complex_fault_mesh_spacing` parameter can be None only if\n        `rupture_mesh_spacing` is set. In that case it is identified with it.\n        \"\"\"\n        rms = getattr(self, 'rupture_mesh_spacing', None)\n        if rms and not getattr(self, 'complex_fault_mesh_spacing', None):\n            self.complex_fault_mesh_spacing = self.rupture_mesh_spacing\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if optimize_same_id_sources is set to True.", "response": "def is_valid_optimize_same_id_sources(self):\n        \"\"\"\n        The `optimize_same_id_sources` can be true only in the classical\n        calculators.\n        \"\"\"\n        if (self.optimize_same_id_sources and\n                'classical' in self.calculation_mode or\n                'disagg' in self.calculation_mode):\n            return True\n        elif self.optimize_same_id_sources:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the given parameter is missing in the job. ini file.", "response": "def check_missing(self, param, action):\n        \"\"\"\n        Make sure the given parameter is missing in the job.ini file\n        \"\"\"\n        assert action in ('debug', 'info', 'warn', 'error'), action\n        if self.inputs.get(param):\n            msg = '%s_file in %s is ignored in %s' % (\n                param, self.inputs['job_ini'], self.calculation_mode)\n            if action == 'error':\n                raise InvalidFile(msg)\n            else:\n                getattr(logging, action)(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the hazard is precomputed False otherwise.", "response": "def hazard_precomputed(self):\n        \"\"\"\n        :returns: True if the hazard is precomputed\n        \"\"\"\n        if 'gmfs' in self.inputs or 'hazard_curves' in self.inputs:\n            return True\n        elif self.hazard_calculation_id:\n            parent = list(util.read(self.hazard_calculation_id))\n            return 'gmf_data' in parent or 'poes' in parent"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the number of ruptures and set it in the src object", "response": "def get_set_num_ruptures(src):\n    \"\"\"\n    Extract the number of ruptures and set it\n    \"\"\"\n    if not src.num_ruptures:\n        t0 = time.time()\n        src.num_ruptures = src.count_ruptures()\n        dt = time.time() - t0\n        clsname = src.__class__.__name__\n        if dt > 10:\n            if 'Area' in clsname:\n                logging.warning(\n                    '%s.count_ruptures took %d seconds, perhaps the '\n                    'area discretization is too small', src, dt)\n            elif 'ComplexFault' in clsname:\n                logging.warning(\n                    '%s.count_ruptures took %d seconds, perhaps the '\n                    'complex_fault_mesh_spacing is too small', src, dt)\n            elif 'SimpleFault' in clsname:\n                logging.warning(\n                    '%s.count_ruptures took %d seconds, perhaps the '\n                    'rupture_mesh_spacing is too small', src, dt)\n            else:\n                # multiPointSource\n                logging.warning('count_ruptures %s took %d seconds', src, dt)\n    return src.num_ruptures"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning hpdist npdist and magScaleRel from the given pointSource node", "response": "def dists(node):\n    \"\"\"\n    :returns: hpdist, npdist and magScaleRel from the given pointSource node\n    \"\"\"\n    hd = tuple((node['probability'], node['depth'])\n               for node in node.hypoDepthDist)\n    npd = tuple(\n        ((node['probability'], node['rake'], node['strike'], node['dip']))\n        for node in node.nodalPlaneDist)\n    return hd, npd, ~node.magScaleRel"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a list of MFD nodes into a single MultiMFD node", "response": "def mfds2multimfd(mfds):\n    \"\"\"\n    Convert a list of MFD nodes into a single MultiMFD node\n    \"\"\"\n    _, kind = mfds[0].tag.split('}')\n    node = Node('multiMFD', dict(kind=kind, size=len(mfds)))\n    lengths = None\n    for field in mfd.multi_mfd.ASSOC[kind][1:]:\n        alias = mfd.multi_mfd.ALIAS.get(field, field)\n        if field in ('magnitudes', 'occurRates'):\n            data = [~getattr(m, field) for m in mfds]\n            lengths = [len(d) for d in data]\n            data = sum(data, [])  # list of lists\n        else:\n            try:\n                data = [m[alias] for m in mfds]\n            except KeyError:\n                if alias == 'binWidth':\n                    # missing bindWidth in GR MDFs is ok\n                    continue\n                else:\n                    raise\n        node.append(Node(field, text=collapse(data)))\n        if lengths:  # this is the last field if present\n            node.append(Node('lengths', text=collapse(lengths)))\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the source model with the sourceGroups of the given sm_node.", "response": "def update_source_model(sm_node, fname):\n    \"\"\"\n    :param sm_node: a sourceModel Node object containing sourceGroups\n    \"\"\"\n    i = 0\n    for group in sm_node:\n        if 'srcs_weights' in group.attrib:\n            raise InvalidFile('srcs_weights must be removed in %s' % fname)\n        if not group.tag.endswith('sourceGroup'):\n            raise InvalidFile('wrong NRML, got %s instead of '\n                              'sourceGroup in %s' % (group.tag, fname))\n        psrcs = []\n        others = []\n        for src in group:\n            try:\n                del src.attrib['tectonicRegion']  # make the trt implicit\n            except KeyError:\n                pass  # already missing\n            if src.tag.endswith('pointSource'):\n                psrcs.append(src)\n            else:\n                others.append(src)\n        others.sort(key=lambda src: (src.tag, src['id']))\n        i, sources = _pointsources2multipoints(psrcs, i)\n        group.nodes = sources + others"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a source model. hdf5 file into a source model object.", "response": "def to_python(fname, converter):\n    \"\"\"\n    Convert a source model .hdf5 file into a :class:`SourceModel` object\n    \"\"\"\n    with hdf5.File(fname, 'r') as f:\n        source_model = f['/']\n    for sg in source_model:\n        for src in sg:\n            if hasattr(src, 'mfd'):\n                # multipoint source\n                # src.tom = converter.tom\n                kwargs = getattr(src.mfd, 'kwargs', {})\n                if 'bin_width' not in kwargs:\n                    kwargs['bin_width'] = [converter.width_of_mfd_bin]\n    return source_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect(cls, sources):\n        source_stats_dict = {}\n        for src in sources:\n            trt = src['tectonicRegion']\n            if trt not in source_stats_dict:\n                source_stats_dict[trt] = SourceGroup(trt)\n            sg = source_stats_dict[trt]\n            if not sg.sources:\n                # we append just one source per SourceGroup, so that\n                # the memory occupation is insignificant\n                sg.sources.append(src)\n\n        # return SourceGroups, ordered by TRT string\n        return sorted(source_stats_dict.values())", "response": "collects all the source groups for a given set of sources"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the attributes sources min_mag max_mag according to the given source.", "response": "def update(self, src):\n        \"\"\"\n        Update the attributes sources, min_mag, max_mag\n        according to the given source.\n\n        :param src:\n            an instance of :class:\n            `openquake.hazardlib.source.base.BaseSeismicSource`\n        \"\"\"\n        assert src.tectonic_region_type == self.trt, (\n            src.tectonic_region_type, self.trt)\n        if not src.min_mag:  # if not set already\n            src.min_mag = self.min_mag.get(self.trt) or self.min_mag['default']\n        # checking mutex ruptures\n        if (not isinstance(src, NonParametricSeismicSource) and\n                self.rup_interdep == 'mutex'):\n            msg = \"Mutually exclusive ruptures can only be \"\n            msg += \"modelled using non-parametric sources\"\n            raise ValueError(msg)\n\n        nr = get_set_num_ruptures(src)\n        if nr == 0:  # the minimum_magnitude filters all ruptures\n            return\n        self.tot_ruptures += nr\n        self.sources.append(src)\n        _, max_mag = src.get_min_max_mag()\n        prev_max_mag = self.max_mag\n        if prev_max_mag is None or max_mag > prev_max_mag:\n            self.max_mag = max_mag"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the given rupture node into a hazardlib rupture depending on the node tag.", "response": "def convert_node(self, node):\n        \"\"\"\n        Convert the given rupture node into a hazardlib rupture, depending\n        on the node tag.\n\n        :param node: a node representing a rupture\n        \"\"\"\n        convert = getattr(self, 'convert_' + striptag(node.tag))\n        return convert(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef geo_lines(self, edges):\n        lines = []\n        for edge in edges:\n            with context(self.fname, edge):\n                coords = split_coords_3d(~edge.LineString.posList)\n            lines.append(geo.Line([geo.Point(*p) for p in coords]))\n        return lines", "response": "Utility function to convert a list of edges into a list of openquake. hazardlib. geo. Line instances."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_surfaces(self, surface_nodes):\n        surface_node = surface_nodes[0]\n        if surface_node.tag.endswith('simpleFaultGeometry'):\n            surface = geo.SimpleFaultSurface.from_fault_data(\n                self.geo_line(surface_node),\n                ~surface_node.upperSeismoDepth,\n                ~surface_node.lowerSeismoDepth,\n                ~surface_node.dip,\n                self.rupture_mesh_spacing)\n        elif surface_node.tag.endswith('complexFaultGeometry'):\n            surface = geo.ComplexFaultSurface.from_fault_data(\n                self.geo_lines(surface_node),\n                self.complex_fault_mesh_spacing)\n        elif surface_node.tag.endswith('griddedSurface'):\n            with context(self.fname, surface_node):\n                coords = split_coords_3d(~surface_node.posList)\n            points = [geo.Point(*p) for p in coords]\n            surface = geo.GriddedSurface.from_points_list(points)\n        else:  # a collection of planar surfaces\n            planar_surfaces = list(map(self.geo_planar, surface_nodes))\n            surface = geo.MultiSurface(planar_surfaces)\n        return surface", "response": "Utility to convert a list of surface nodes into a single hazardlib\n            object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_simpleFaultRupture(self, node):\n        mag, rake, hypocenter = self.get_mag_rake_hypo(node)\n        with context(self.fname, node):\n            surfaces = [node.simpleFaultGeometry]\n        rupt = source.rupture.BaseRupture(\n            mag=mag, rake=rake, tectonic_region_type=None,\n            hypocenter=hypocenter,\n            surface=self.convert_surfaces(surfaces))\n        return rupt", "response": "Convert a simpleFaultRupture node into a rupture object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_multiPlanesRupture(self, node):\n        mag, rake, hypocenter = self.get_mag_rake_hypo(node)\n        with context(self.fname, node):\n            surfaces = list(node.getnodes('planarSurface'))\n        rupt = source.rupture.BaseRupture(\n            mag=mag, rake=rake,\n            tectonic_region_type=None,\n            hypocenter=hypocenter,\n            surface=self.convert_surfaces(surfaces))\n        return rupt", "response": "Convert a multiPlanesRupture node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a ruptureCollection node into a dictionary of EBRuptures", "response": "def convert_ruptureCollection(self, node):\n        \"\"\"\n        :param node: a ruptureCollection node\n        :returns: a dictionary grp_id -> EBRuptures\n        \"\"\"\n        coll = {}\n        for grpnode in node:\n            grp_id = int(grpnode['id'])\n            coll[grp_id] = ebrs = []\n            for node in grpnode:\n                rup = self.convert_node(node)\n                rup.serial = int(node['id'])\n                sesnodes = node.stochasticEventSets\n                n = 0  # number of events\n                for sesnode in sesnodes:\n                    with context(self.fname, sesnode):\n                        n += len(sesnode.text.split())\n                ebr = source.rupture.EBRupture(rup, 0, 0, numpy.array([n]))\n                ebrs.append(ebr)\n        return coll"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tom(self, node):\n        if 'tom' in node.attrib:\n            tom_cls = tom.registry[node['tom']]\n        else:\n            tom_cls = tom.registry['PoissonTOM']\n        return tom_cls(time_span=self.investigation_time,\n                       occurrence_rate=node.get('occurrence_rate'))", "response": "Convert the given node into a Temporal Occurrence Model object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_mfdist(self, node):\n        with context(self.fname, node):\n            [mfd_node] = [subnode for subnode in node\n                          if subnode.tag.endswith(\n                              ('incrementalMFD', 'truncGutenbergRichterMFD',\n                               'arbitraryMFD', 'YoungsCoppersmithMFD',\n                               'multiMFD'))]\n            if mfd_node.tag.endswith('incrementalMFD'):\n                return mfd.EvenlyDiscretizedMFD(\n                    min_mag=mfd_node['minMag'], bin_width=mfd_node['binWidth'],\n                    occurrence_rates=~mfd_node.occurRates)\n            elif mfd_node.tag.endswith('truncGutenbergRichterMFD'):\n                return mfd.TruncatedGRMFD(\n                    a_val=mfd_node['aValue'], b_val=mfd_node['bValue'],\n                    min_mag=mfd_node['minMag'], max_mag=mfd_node['maxMag'],\n                    bin_width=self.width_of_mfd_bin)\n            elif mfd_node.tag.endswith('arbitraryMFD'):\n                return mfd.ArbitraryMFD(\n                    magnitudes=~mfd_node.magnitudes,\n                    occurrence_rates=~mfd_node.occurRates)\n            elif mfd_node.tag.endswith('YoungsCoppersmithMFD'):\n                if \"totalMomentRate\" in mfd_node.attrib.keys():\n                    # Return Youngs & Coppersmith from the total moment rate\n                    return mfd.YoungsCoppersmith1985MFD.from_total_moment_rate(\n                        min_mag=mfd_node[\"minMag\"], b_val=mfd_node[\"bValue\"],\n                        char_mag=mfd_node[\"characteristicMag\"],\n                        total_moment_rate=mfd_node[\"totalMomentRate\"],\n                        bin_width=mfd_node[\"binWidth\"])\n                elif \"characteristicRate\" in mfd_node.attrib.keys():\n                    # Return Youngs & Coppersmith from the total moment rate\n                    return mfd.YoungsCoppersmith1985MFD.\\\n                        from_characteristic_rate(\n                            min_mag=mfd_node[\"minMag\"],\n                            b_val=mfd_node[\"bValue\"],\n                            char_mag=mfd_node[\"characteristicMag\"],\n                            char_rate=mfd_node[\"characteristicRate\"],\n                            bin_width=mfd_node[\"binWidth\"])\n            elif mfd_node.tag.endswith('multiMFD'):\n                return mfd.multi_mfd.MultiMFD.from_node(\n                    mfd_node, self.width_of_mfd_bin)", "response": "Convert the given node into a Magnitude - Frequency Distribution\n        object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the given node into a Nodal Plane Distribution.", "response": "def convert_npdist(self, node):\n        \"\"\"\n        Convert the given node into a Nodal Plane Distribution.\n\n        :param node: a nodalPlaneDist node\n        :returns: a :class:`openquake.hazardlib.geo.NodalPlane` instance\n        \"\"\"\n        with context(self.fname, node):\n            npdist = []\n            for np in node.nodalPlaneDist:\n                prob, strike, dip, rake = (\n                    np['probability'], np['strike'], np['dip'], np['rake'])\n                npdist.append((prob, geo.NodalPlane(strike, dip, rake)))\n            if not self.spinning_floating:\n                npdist = [(1, npdist[0][1])]  # consider the first nodal plane\n            return pmf.PMF(npdist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_hpdist(self, node):\n        with context(self.fname, node):\n            hcdist = [(hd['probability'], hd['depth'])\n                      for hd in node.hypoDepthDist]\n            if not self.spinning_floating:  # consider the first hypocenter\n                hcdist = [(1, hcdist[0][1])]\n            return pmf.PMF(hcdist)", "response": "Convert the given node into a probability mass function for the\n        hypo depth distribution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_areaSource(self, node):\n        geom = node.areaGeometry\n        coords = split_coords_2d(~geom.Polygon.exterior.LinearRing.posList)\n        polygon = geo.Polygon([geo.Point(*xy) for xy in coords])\n        msr = valid.SCALEREL[~node.magScaleRel]()\n        area_discretization = geom.attrib.get(\n            'discretization', self.area_source_discretization)\n        if area_discretization is None:\n            raise ValueError(\n                'The source %r has no `discretization` parameter and the job.'\n                'ini file has no `area_source_discretization` parameter either'\n                % node['id'])\n        return source.AreaSource(\n            source_id=node['id'],\n            name=node['name'],\n            tectonic_region_type=node.attrib.get('tectonicRegion'),\n            mfd=self.convert_mfdist(node),\n            rupture_mesh_spacing=self.rupture_mesh_spacing,\n            magnitude_scaling_relationship=msr,\n            rupture_aspect_ratio=~node.ruptAspectRatio,\n            upper_seismogenic_depth=~geom.upperSeismoDepth,\n            lower_seismogenic_depth=~geom.lowerSeismoDepth,\n            nodal_plane_distribution=self.convert_npdist(node),\n            hypocenter_distribution=self.convert_hpdist(node),\n            polygon=polygon,\n            area_discretization=area_discretization,\n            temporal_occurrence_model=self.get_tom(node))", "response": "Converts the given node into an area source object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the given node into a point source object.", "response": "def convert_pointSource(self, node):\n        \"\"\"\n        Convert the given node into a point source object.\n\n        :param node: a node with tag pointGeometry\n        :returns: a :class:`openquake.hazardlib.source.PointSource` instance\n        \"\"\"\n        geom = node.pointGeometry\n        lon_lat = ~geom.Point.pos\n        msr = valid.SCALEREL[~node.magScaleRel]()\n        return source.PointSource(\n            source_id=node['id'],\n            name=node['name'],\n            tectonic_region_type=node.attrib.get('tectonicRegion'),\n            mfd=self.convert_mfdist(node),\n            rupture_mesh_spacing=self.rupture_mesh_spacing,\n            magnitude_scaling_relationship=msr,\n            rupture_aspect_ratio=~node.ruptAspectRatio,\n            upper_seismogenic_depth=~geom.upperSeismoDepth,\n            lower_seismogenic_depth=~geom.lowerSeismoDepth,\n            location=geo.Point(*lon_lat),\n            nodal_plane_distribution=self.convert_npdist(node),\n            hypocenter_distribution=self.convert_hpdist(node),\n            temporal_occurrence_model=self.get_tom(node))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the given node into a MultiPointSource object.", "response": "def convert_multiPointSource(self, node):\n        \"\"\"\n        Convert the given node into a MultiPointSource object.\n\n        :param node: a node with tag multiPointGeometry\n        :returns: a :class:`openquake.hazardlib.source.MultiPointSource`\n        \"\"\"\n        geom = node.multiPointGeometry\n        lons, lats = zip(*split_coords_2d(~geom.posList))\n        msr = valid.SCALEREL[~node.magScaleRel]()\n        return source.MultiPointSource(\n            source_id=node['id'],\n            name=node['name'],\n            tectonic_region_type=node.attrib.get('tectonicRegion'),\n            mfd=self.convert_mfdist(node),\n            magnitude_scaling_relationship=msr,\n            rupture_aspect_ratio=~node.ruptAspectRatio,\n            upper_seismogenic_depth=~geom.upperSeismoDepth,\n            lower_seismogenic_depth=~geom.lowerSeismoDepth,\n            nodal_plane_distribution=self.convert_npdist(node),\n            hypocenter_distribution=self.convert_hpdist(node),\n            mesh=geo.Mesh(F32(lons), F32(lats)),\n            temporal_occurrence_model=self.get_tom(node))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_simpleFaultSource(self, node):\n        geom = node.simpleFaultGeometry\n        msr = valid.SCALEREL[~node.magScaleRel]()\n        fault_trace = self.geo_line(geom)\n        mfd = self.convert_mfdist(node)\n        with context(self.fname, node):\n            try:\n                hypo_list = valid.hypo_list(node.hypoList)\n            except AttributeError:\n                hypo_list = ()\n            try:\n                slip_list = valid.slip_list(node.slipList)\n            except AttributeError:\n                slip_list = ()\n            simple = source.SimpleFaultSource(\n                source_id=node['id'],\n                name=node['name'],\n                tectonic_region_type=node.attrib.get('tectonicRegion'),\n                mfd=mfd,\n                rupture_mesh_spacing=self.rupture_mesh_spacing,\n                magnitude_scaling_relationship=msr,\n                rupture_aspect_ratio=~node.ruptAspectRatio,\n                upper_seismogenic_depth=~geom.upperSeismoDepth,\n                lower_seismogenic_depth=~geom.lowerSeismoDepth,\n                fault_trace=fault_trace,\n                dip=~geom.dip,\n                rake=~node.rake,\n                temporal_occurrence_model=self.get_tom(node),\n                hypo_list=hypo_list,\n                slip_list=slip_list)\n        return simple", "response": "Convert the given node into a simple fault object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the given node into a complex fault object.", "response": "def convert_complexFaultSource(self, node):\n        \"\"\"\n        Convert the given node into a complex fault object.\n\n        :param node: a node with tag areaGeometry\n        :returns: a :class:`openquake.hazardlib.source.ComplexFaultSource`\n                  instance\n        \"\"\"\n        geom = node.complexFaultGeometry\n        edges = self.geo_lines(geom)\n        mfd = self.convert_mfdist(node)\n        msr = valid.SCALEREL[~node.magScaleRel]()\n        with context(self.fname, node):\n            cmplx = source.ComplexFaultSource(\n                source_id=node['id'],\n                name=node['name'],\n                tectonic_region_type=node.attrib.get('tectonicRegion'),\n                mfd=mfd,\n                rupture_mesh_spacing=self.complex_fault_mesh_spacing,\n                magnitude_scaling_relationship=msr,\n                rupture_aspect_ratio=~node.ruptAspectRatio,\n                edges=edges,\n                rake=~node.rake,\n                temporal_occurrence_model=self.get_tom(node))\n        return cmplx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the given node into a characteristic fault object.", "response": "def convert_characteristicFaultSource(self, node):\n        \"\"\"\n        Convert the given node into a characteristic fault object.\n\n        :param node:\n            a node with tag areaGeometry\n        :returns:\n            a :class:`openquake.hazardlib.source.CharacteristicFaultSource`\n            instance\n        \"\"\"\n        char = source.CharacteristicFaultSource(\n            source_id=node['id'],\n            name=node['name'],\n            tectonic_region_type=node.attrib.get('tectonicRegion'),\n            mfd=self.convert_mfdist(node),\n            surface=self.convert_surfaces(node.surface),\n            rake=~node.rake,\n            temporal_occurrence_model=self.get_tom(node))\n        return char"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_nonParametricSeismicSource(self, node):\n        trt = node.attrib.get('tectonicRegion')\n        rup_pmf_data = []\n        rups_weights = None\n        if 'rup_weights' in node.attrib:\n            tmp = node.attrib.get('rup_weights')\n            rups_weights = numpy.array([float(s) for s in tmp.split()])\n        for i, rupnode in enumerate(node):\n            probs = pmf.PMF(valid.pmf(rupnode['probs_occur']))\n            rup = RuptureConverter.convert_node(self, rupnode)\n            rup.tectonic_region_type = trt\n            rup.weight = None if rups_weights is None else rups_weights[i]\n            rup_pmf_data.append((rup, probs))\n        nps = source.NonParametricSeismicSource(\n            node['id'], node['name'], trt, rup_pmf_data)\n        nps.splittable = 'rup_weights' not in node.attrib\n        return nps", "response": "Convert the given node into a non parametric seismic source object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the given node into a SourceGroup object.", "response": "def convert_sourceGroup(self, node):\n        \"\"\"\n        Convert the given node into a SourceGroup object.\n\n        :param node:\n            a node with tag sourceGroup\n        :returns:\n            a :class:`SourceGroup` instance\n        \"\"\"\n        trt = node['tectonicRegion']\n        srcs_weights = node.attrib.get('srcs_weights')\n        grp_attrs = {k: v for k, v in node.attrib.items()\n                     if k not in ('name', 'src_interdep', 'rup_interdep',\n                                  'srcs_weights')}\n        sg = SourceGroup(trt, min_mag=self.minimum_magnitude)\n        sg.temporal_occurrence_model = self.get_tom(node)\n        sg.name = node.attrib.get('name')\n        # Set attributes related to occurrence\n        sg.src_interdep = node.attrib.get('src_interdep', 'indep')\n        sg.rup_interdep = node.attrib.get('rup_interdep', 'indep')\n        sg.grp_probability = node.attrib.get('grp_probability')\n        # Set the cluster attribute\n        sg.cluster = node.attrib.get('cluster') == 'true'\n        # Filter admitted cases\n        # 1. The source group is a cluster. In this case the cluster must have\n        #    the attributes required to define its occurrence in time.\n        if sg.cluster:\n            msg = 'A cluster group requires the definition of a temporal'\n            msg += ' occurrence model'\n            assert 'tom' in node.attrib, msg\n            if isinstance(tom, PoissonTOM):\n                assert hasattr(sg, 'occurrence_rate')\n        #\n        for src_node in node:\n            if self.source_id and self.source_id != src_node['id']:\n                continue  # filter by source_id\n            src = self.convert_node(src_node)\n            # transmit the group attributes to the underlying source\n            for attr, value in grp_attrs.items():\n                if attr == 'tectonicRegion':\n                    src_trt = src_node.get('tectonicRegion')\n                    if src_trt and src_trt != trt:\n                        with context(self.fname, src_node):\n                            raise ValueError('Found %s, expected %s' %\n                                             (src_node['tectonicRegion'], trt))\n                    src.tectonic_region_type = trt\n                elif attr == 'grp_probability':\n                    pass  # do not transmit\n                else:  # transmit as it is\n                    setattr(src, attr, node[attr])\n            sg.update(src)\n        if srcs_weights is not None:\n            if len(node) and len(srcs_weights) != len(node):\n                raise ValueError(\n                    'There are %d srcs_weights but %d source(s) in %s'\n                    % (len(srcs_weights), len(node), self.fname))\n            for src, sw in zip(sg, srcs_weights):\n                src.mutex_weight = sw\n        # check that, when the cluster option is set, the group has a temporal\n        # occurrence model properly defined\n        if sg.cluster and not hasattr(sg, 'temporal_occurrence_model'):\n            msg = 'The Source Group is a cluster but does not have a '\n            msg += 'temporal occurrence model'\n            raise ValueError(msg)\n        return sg"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_list_weights(parameter, name):\n    '''\n    Checks that the weights in a list of tuples sums to 1.0\n    '''\n    if not isinstance(parameter, list):\n        raise ValueError('%s must be formatted with a list of tuples' % name)\n    weight = np.sum([val[1] for val in parameter])\n    if fabs(weight - 1.) > 1E-8:\n        raise ValueError('%s weights do not sum to 1.0!' % name)\n    return parameter", "response": "Checks that the weights in a list sums to 1. 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef populate_regions(self, tectonic_region_dict):\n        '''\n        Populates the tectonic region from the list of dictionaries, where each\n        region is a dictionary of with the following format::\n\n         region = {'Shear_Modulus': [(val1, weight1), (val2, weight2), ...],\n                   'Displacement_Length_Ratio': [(val1, weight1), ...],\n                   'Magnitude_Scaling_Relation': [(val1, weight1), ...]}\n        '''\n        for tect_reg in tectonic_region_dict:\n            if 'Shear_Modulus' in tect_reg.keys():\n                shear_modulus = tect_reg['Shear_Modulus']\n            else:\n                shear_modulus = DEFAULT_SHEAR_MODULUS\n\n            if 'Displacement_Length_Ratio' in tect_reg.keys():\n                disp_length_ratio = tect_reg['Displacement_Length_Ratio']\n            else:\n                disp_length_ratio = DEFAULT_DLR\n\n            if 'Magnitude_Scaling_Relation' in tect_reg.keys():\n                scaling_relation = tect_reg['Magnitude_Scaling_Relation']\n            else:\n                scaling_relation = DEFAULT_MSR\n\n            self.regionalisation.append(\n                TectonicRegion(\n                    tect_reg['Code'], tect_reg['Name'],\n                    shear_modulus, disp_length_ratio, scaling_relation))\n            self.key_list.append(tect_reg['Name'])", "response": "Populates the tectonic region list from the given dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_fault_model(self, collapse=False, rendered_msr=WC1994(),\n                          mfd_config=None):\n        '''\n        Constructs a full fault model with epistemic uncertainty by\n        enumerating all the possible recurrence models of each fault as\n        separate faults, with the recurrence rates multiplied by the\n        corresponding weights.\n\n        :param bool collapse:\n            Determines whether or not to collapse the branches\n        :param rendered_msr:\n            If the option is taken to collapse the branches then a recurrence\n            model for rendering must be defined\n        :param list/dict mfd_config:\n            Universal list or dictionay of configuration parameters for the\n            magnitude frequency distribution - will overwrite whatever is\n            previously defined for the fault!\n        '''\n        self.source_model = mtkSourceModel(self.id, self.name)\n        for fault in self.faults:\n            fault.generate_recurrence_models(collapse,\n                                             config=mfd_config,\n                                             rendered_msr=rendered_msr)\n            src_model, src_weight = fault.generate_fault_source_model()\n            for iloc, model in enumerate(src_model):\n\n                new_model = deepcopy(model)\n                new_model.id = str(model.id) + '_%g' % (iloc + 1)\n                new_model.mfd.occurrence_rates = \\\n                    (np.array(new_model.mfd.occurrence_rates) *\n                     src_weight[iloc]).tolist()\n                self.source_model.sources.append(new_model)", "response": "Builds a full fault model for this topic."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the date from a string in the format YYYYMMDD and returns", "response": "def _read_date_from_string(str1):\n    \"\"\"\n    Reads the date from a string in the format YYYY/MM/DD and returns\n    :class: datetime.date\n    \"\"\"\n    full_date = [int(x) for x in str1.split('/')]\n    return datetime.date(full_date[0], full_date[1], full_date[2])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_time_from_string(str1):\n    full_time = [float(x) for x in str1.split(':')]\n    hour = int(full_time[0])\n    minute = int(full_time[1])\n    if full_time[2] > 59.99:\n        minute += 1\n        second = 0\n    else:\n        second = int(full_time[2])\n    microseconds = int((full_time[2] - floor(full_time[2])) * 1000000)\n    return datetime.time(hour, minute, second, microseconds)", "response": "Reads the time from a string in the format HHMMSS. S and returns a datetime. time object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the moment tensor from the ndk_string representation.", "response": "def _read_moment_tensor_from_ndk_string(ndk_string, system='USE'):\n    \"\"\"\n    Reads the moment tensor from the ndk_string representation\n    ndk_string = [Mrr, sigMrr, Mtt, sigMtt, Mpp, sigMpp, Mrt, sigMrt, Mrp,\n                  sigMrp, Mtp, sigMtp]\n    Output tensors should be of format:\n        expected = [[Mtt, Mtp, Mtr],\n                    [Mtp, Mpp, Mpr],\n                    [Mtr, Mpr, Mrr]]\n        sigma = [[sigMtt, sigMtp, sigMtr],\n                 [sigMtp, sigMpp, sigMpr],\n                 [sigMtr, sigMpr, sigMrr]]\n    Exponent returned in Nm\n\n    :param str ndk_string:\n        String of data in ndk format (line 4 of event)\n    :param str system:\n        Reference frame of tensor Up, South, East {USE} or North, East, Down\n        (NED)\n    \"\"\"\n    exponent = float(ndk_string[0:2]) - 7.\n    mkr = np.array([2, 9, 15], dtype=int)\n    vector = []\n    for i in range(0, 6):\n        vector.extend([\n            float(ndk_string[mkr[0]:mkr[1]]),\n            float(ndk_string[mkr[1]:mkr[2]])])\n        mkr = mkr + 13\n    vector = np.array(vector)\n    mrr, mtt, mpp, mrt, mrp, mtp = tuple(vector[np.arange(0, 12, 2)])\n    sig_mrr, sig_mtt, sig_mpp, sig_mrt, sig_mrp, sig_mtp = \\\n        tuple(vector[np.arange(1, 13, 2)])\n\n    tensor = utils.COORD_SYSTEM[system](mrr, mtt, mpp, mrt, mrp, mtp)\n    tensor = (10. ** exponent) * tensor\n\n    sigma = utils.COORD_SYSTEM[system](sig_mrr, sig_mtt, sig_mpp,\n                                       sig_mrt, sig_mrp, sig_mtp)\n    sigma = (10. ** exponent) * sigma\n\n    return tensor, sigma, exponent"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the file and returns a list of all the GCMTs in the file.", "response": "def read_file(self, start_year=None, end_year=None, use_centroid=None):\n        \"\"\"\n        Reads the file\n        \"\"\"\n        raw_data = getlines(self.filename)\n        num_lines = len(raw_data)\n        if ((float(num_lines) / 5.) - float(num_lines / 5)) > 1E-9:\n            raise IOError('GCMT represented by 5 lines - number in file not'\n                          ' a multiple of 5!')\n        self.catalogue.number_gcmts = num_lines // 5\n        self.catalogue.gcmts = [None] * self.catalogue.number_gcmts\n        # Pre-allocates list\n        id0 = 0\n        print('Parsing catalogue ...')\n        for iloc in range(0, self.catalogue.number_gcmts):\n            self.catalogue.gcmts[iloc] = self.read_ndk_event(raw_data, id0)\n            id0 += 5\n        print('complete. Contains %s moment tensors'\n              % self.catalogue.get_number_tensors())\n        if not start_year:\n            min_years = []\n            min_years = [cent.centroid.date.year\n                         for cent in self.catalogue.gcmts]\n            self.catalogue.start_year = np.min(min_years)\n\n        if not end_year:\n            max_years = []\n            max_years = [cent.centroid.date.year\n                         for cent in self.catalogue.gcmts]\n            self.catalogue.end_year = np.max(max_years)\n        self.to_hmtk(use_centroid)\n        return self.catalogue"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_ndk_event(self, raw_data, id0):\n        gcmt = GCMTEvent()\n        # Get hypocentre\n        ndkstring = raw_data[id0].rstrip('\\n')\n        gcmt.hypocentre = self._read_hypocentre_from_ndk_string(ndkstring)\n\n        # GCMT metadata\n        ndkstring = raw_data[id0 + 1].rstrip('\\n')\n        gcmt = self._get_metadata_from_ndk_string(gcmt, ndkstring)\n\n        # Get Centroid\n        ndkstring = raw_data[id0 + 2].rstrip('\\n')\n        gcmt.centroid = self._read_centroid_from_ndk_string(ndkstring,\n                                                            gcmt.hypocentre)\n\n        # Get Moment Tensor\n        ndkstring = raw_data[id0 + 3].rstrip('\\n')\n        gcmt.moment_tensor = self._get_moment_tensor_from_ndk_string(ndkstring)\n\n        # Get principal axes\n        ndkstring = raw_data[id0 + 4].rstrip('\\n')\n        gcmt.principal_axes = self._get_principal_axes_from_ndk_string(\n            ndkstring[3:48],\n            exponent=gcmt.moment_tensor.exponent)\n\n        # Get Nodal Planes\n        gcmt.nodal_planes = self._get_nodal_planes_from_ndk_string(\n            ndkstring[57:])\n\n        # Get Moment and Magnitude\n        gcmt.moment, gcmt.version, gcmt.magnitude = \\\n            self._get_moment_from_ndk_string(\n                ndkstring, gcmt.moment_tensor.exponent)\n        return gcmt", "response": "Reads a 5 - line batch of data into a set of GCMTs containing the information for the entry with the specified ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the content of the GCMT catalogue to a HMTK object.", "response": "def to_hmtk(self, use_centroid=True):\n        '''\n        Convert the content of the GCMT catalogue to a HMTK\n        catalogue.\n        '''\n        self._preallocate_data_dict()\n        for iloc, gcmt in enumerate(self.catalogue.gcmts):\n            self.catalogue.data['eventID'][iloc] = iloc\n\n            if use_centroid:\n                self.catalogue.data['year'][iloc] = \\\n                    gcmt.centroid.date.year\n                self.catalogue.data['month'][iloc] = \\\n                    gcmt.centroid.date.month\n                self.catalogue.data['day'][iloc] = \\\n                    gcmt.centroid.date.day\n                self.catalogue.data['hour'][iloc] = \\\n                    gcmt.centroid.time.hour\n                self.catalogue.data['minute'][iloc] = \\\n                    gcmt.centroid.time.minute\n                self.catalogue.data['second'][iloc] = \\\n                    gcmt.centroid.time.second\n                self.catalogue.data['longitude'][iloc] = \\\n                    gcmt.centroid.longitude\n                self.catalogue.data['latitude'][iloc] = \\\n                    gcmt.centroid.latitude\n                self.catalogue.data['depth'][iloc] = \\\n                    gcmt.centroid.depth\n            else:\n                self.catalogue.data['year'][iloc] = \\\n                    gcmt.hypocentre.date.year\n                self.catalogue.data['month'][iloc] = \\\n                    gcmt.hypocentre.date.month\n                self.catalogue.data['day'][iloc] = \\\n                    gcmt.hypocentre.date.day\n                self.catalogue.data['hour'][iloc] = \\\n                    gcmt.hypocentre.time.hour\n                self.catalogue.data['minute'][iloc] = \\\n                    gcmt.hypocentre.time.minute\n                self.catalogue.data['second'][iloc] = \\\n                    gcmt.hypocentre.time.second\n                self.catalogue.data['longitude'][iloc] = \\\n                    gcmt.hypocentre.longitude\n                self.catalogue.data['latitude'][iloc] = \\\n                    gcmt.hypocentre.latitude\n                self.catalogue.data['depth'][iloc] = \\\n                    gcmt.hypocentre.depth\n            # Moment, magnitude and relative errors\n            self.catalogue.data['moment'][iloc] = gcmt.moment\n            self.catalogue.data['magnitude'][iloc] = gcmt.magnitude\n            self.catalogue.data['f_clvd'][iloc] = gcmt.f_clvd\n            self.catalogue.data['e_rel'][iloc] = gcmt.e_rel\n            self.catalogue.data['centroidID'][iloc] = gcmt.identifier\n            # Nodal planes\n            self.catalogue.data['strike1'][iloc] = \\\n                gcmt.nodal_planes.nodal_plane_1['strike']\n            self.catalogue.data['dip1'][iloc] = \\\n                gcmt.nodal_planes.nodal_plane_1['dip']\n            self.catalogue.data['rake1'][iloc] = \\\n                gcmt.nodal_planes.nodal_plane_1['rake']\n            self.catalogue.data['strike2'][iloc] = \\\n                gcmt.nodal_planes.nodal_plane_2['strike']\n            self.catalogue.data['dip2'][iloc] = \\\n                gcmt.nodal_planes.nodal_plane_2['dip']\n            self.catalogue.data['rake2'][iloc] = \\\n                gcmt.nodal_planes.nodal_plane_2['rake']\n            # Principal axes\n            self.catalogue.data['eigenvalue_b'][iloc] = \\\n                gcmt.principal_axes.b_axis['eigenvalue']\n            self.catalogue.data['azimuth_b'][iloc] = \\\n                gcmt.principal_axes.b_axis['azimuth']\n            self.catalogue.data['plunge_b'][iloc] = \\\n                gcmt.principal_axes.b_axis['plunge']\n            self.catalogue.data['eigenvalue_p'][iloc] = \\\n                gcmt.principal_axes.p_axis['eigenvalue']\n            self.catalogue.data['azimuth_p'][iloc] = \\\n                gcmt.principal_axes.p_axis['azimuth']\n            self.catalogue.data['plunge_p'][iloc] = \\\n                gcmt.principal_axes.p_axis['plunge']\n            self.catalogue.data['eigenvalue_t'][iloc] = \\\n                gcmt.principal_axes.t_axis['eigenvalue']\n            self.catalogue.data['azimuth_t'][iloc] = \\\n                gcmt.principal_axes.t_axis['azimuth']\n            self.catalogue.data['plunge_t'][iloc] = \\\n                gcmt.principal_axes.t_axis['plunge']\n        return self.catalogue"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the hypocentre data from the ndk string and returns an analyzed object.", "response": "def _read_hypocentre_from_ndk_string(self, linestring):\n        \"\"\"\n        Reads the hypocentre data from the ndk string to return an\n        instance of the GCMTHypocentre class\n        \"\"\"\n        hypo = GCMTHypocentre()\n        hypo.source = linestring[0:4]\n        hypo.date = _read_date_from_string(linestring[5:15])\n        hypo.time = _read_time_from_string(linestring[16:26])\n        hypo.latitude = float(linestring[27:33])\n        hypo.longitude = float(linestring[34:41])\n        hypo.depth = float(linestring[42:47])\n        magnitudes = [float(x) for x in linestring[48:55].split(' ')]\n        if magnitudes[0] > 0.:\n            hypo.m_b = magnitudes[0]\n        if magnitudes[1] > 0.:\n            hypo.m_s = magnitudes[1]\n        hypo.location = linestring[56:]\n        return hypo"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the GCMT metadata from the line 2 of the ndk batch and returns the GCMT object.", "response": "def _get_metadata_from_ndk_string(self, gcmt, ndk_string):\n        \"\"\"\n        Reads the GCMT metadata from line 2 of the ndk batch\n        \"\"\"\n        gcmt.identifier = ndk_string[:16]\n        inversion_data = re.split('[A-Z:]+', ndk_string[17:61])\n        gcmt.metadata['BODY'] = [float(x) for x in inversion_data[1].split()]\n        gcmt.metadata['SURFACE'] = [\n            float(x) for x in inversion_data[2].split()]\n        gcmt.metadata['MANTLE'] = [float(x) for x in inversion_data[3].split()]\n        further_meta = re.split('[: ]+', ndk_string[62:])\n        gcmt.metadata['CMT'] = int(further_meta[1])\n        gcmt.metadata['FUNCTION'] = {'TYPE': further_meta[2],\n                                     'DURATION': float(further_meta[3])}\n        return gcmt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the centroid data from the ndk string and returns an an instance", "response": "def _read_centroid_from_ndk_string(self, ndk_string, hypocentre):\n        \"\"\"\n        Reads the centroid data from the ndk string to return an\n        instance of the GCMTCentroid class\n        :param str ndk_string:\n            String of data (line 3 of ndk format)\n        :param hypocentre:\n            Instance of the GCMTHypocentre class\n        \"\"\"\n        centroid = GCMTCentroid(hypocentre.date,\n                                hypocentre.time)\n\n        data = ndk_string[:58].split()\n        centroid.centroid_type = data[0].rstrip(':')\n        data = [float(x) for x in data[1:]]\n        time_diff = data[0]\n        if fabs(time_diff) > 1E-6:\n            centroid._get_centroid_time(time_diff)\n        centroid.time_error = data[1]\n        centroid.latitude = data[2]\n        centroid.latitude_error = data[3]\n        centroid.longitude = data[4]\n        centroid.longitude_error = data[5]\n        centroid.depth = data[6]\n        centroid.depth_error = data[7]\n        centroid.depth_type = ndk_string[59:63]\n        centroid.centroid_id = ndk_string[64:]\n        return centroid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the moment tensor from the ndk_string and returns an instance of ontargets. GCMTMomentTensor.", "response": "def _get_moment_tensor_from_ndk_string(self, ndk_string):\n        \"\"\"\n        Reads the moment tensor from the ndk_string and returns an instance of\n        the GCMTMomentTensor class.\n        By default the ndk format uses the Up, South, East (USE) reference\n        system.\n        \"\"\"\n        moment_tensor = GCMTMomentTensor('USE')\n        tensor_data = _read_moment_tensor_from_ndk_string(ndk_string, 'USE')\n        moment_tensor.tensor = tensor_data[0]\n        moment_tensor.tensor_sigma = tensor_data[1]\n        moment_tensor.exponent = tensor_data[2]\n        return moment_tensor"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the principal axes from the ndk string and returns an instance of the GCMTPrincipalAxes class", "response": "def _get_principal_axes_from_ndk_string(self, ndk_string, exponent):\n        \"\"\"\n        Gets the principal axes from the ndk string and returns an instance\n        of the GCMTPrincipalAxes class\n        \"\"\"\n        axes = GCMTPrincipalAxes()\n        # The principal axes is defined in characters 3:48 of the 5th line\n        exponent = 10. ** exponent\n        axes.t_axis = {'eigenvalue': exponent * float(ndk_string[0:8]),\n                       'plunge': float(ndk_string[8:11]),\n                       'azimuth': float(ndk_string[11:15])}\n\n        axes.b_axis = {'eigenvalue': exponent * float(ndk_string[15:23]),\n                       'plunge': float(ndk_string[23:26]),\n                       'azimuth': float(ndk_string[26:30])}\n\n        axes.p_axis = {'eigenvalue': exponent * float(ndk_string[30:38]),\n                       'plunge': float(ndk_string[38:41]),\n                       'azimuth': float(ndk_string[41:])}\n        return axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_nodal_planes_from_ndk_string(self, ndk_string):\n        planes = GCMTNodalPlanes()\n        planes.nodal_plane_1 = {'strike': float(ndk_string[0:3]),\n                                'dip': float(ndk_string[3:6]),\n                                'rake': float(ndk_string[6:11])}\n        planes.nodal_plane_2 = {'strike': float(ndk_string[11:15]),\n                                'dip': float(ndk_string[15:18]),\n                                'rake': float(ndk_string[18:])}\n        return planes", "response": "Reads the nodal planes from the NDK string and returns an instance of the GCMTNodalPlanes class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_moment_from_ndk_string(self, ndk_string, exponent):\n        moment = float(ndk_string[49:56]) * (10. ** exponent)\n        version = ndk_string[:3]\n        magnitude = utils.moment_magnitude_scalar(moment)\n        return moment, version, magnitude", "response": "Returns the moment and the moment magnitude from the NDK string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialise_to_nrml(self, filename, use_defaults=False):\n        '''\n        Writes the source model to a nrml source model file given by the\n        filename\n\n        :param str filename:\n            Path to output file\n\n        :param bool use_defaults:\n            Boolean to indicate whether to use default values (True) or not.\n            If set to False, ValueErrors will be raised when an essential\n            attribute is missing.\n        '''\n        source_model = self.convert_to_oqhazardlib(\n            PoissonTOM(1.0), 2.0, 2.0, 10.0, use_defaults=use_defaults)\n        write_source_model(filename, source_model, name=self.name)", "response": "Serialises the source model of the current object to a nrml source model file given by the filename."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the source model to an iterator of sources of type openquake. hazardlib. source. base. BaseSeismicSource", "response": "def convert_to_oqhazardlib(\n            self, tom, simple_mesh_spacing=1.0,\n            complex_mesh_spacing=2.0, area_discretisation=10.0,\n            use_defaults=False):\n        \"\"\"\n        Converts the source model to an iterator of sources of :class:\n        openquake.hazardlib.source.base.BaseSeismicSource\n        \"\"\"\n        oq_source_model = []\n        for source in self.sources:\n            if isinstance(source, mtkAreaSource):\n                oq_source_model.append(source.create_oqhazardlib_source(\n                    tom,\n                    simple_mesh_spacing,\n                    area_discretisation,\n                    use_defaults))\n            elif isinstance(source, mtkPointSource):\n                oq_source_model.append(source.create_oqhazardlib_source(\n                    tom,\n                    simple_mesh_spacing,\n                    use_defaults))\n            elif isinstance(source, mtkSimpleFaultSource):\n                oq_source_model.append(source.create_oqhazardlib_source(\n                    tom,\n                    simple_mesh_spacing,\n                    use_defaults))\n            elif isinstance(source, mtkComplexFaultSource):\n                oq_source_model.append(source.create_oqhazardlib_source(\n                    tom,\n                    complex_mesh_spacing,\n                    use_defaults))\n            else:\n                raise ValueError('Source type not recognised!')\n        return oq_source_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a recurrence table of each magnitude in catalogue for each magnitude in catalogue and year.", "response": "def recurrence_table(mag, dmag, year, time_interval=None):\n    \"\"\"\n    Table of recurrence statistics for each magnitude\n    [Magnitude, Number of Observations, Cumulative Number\n    of Observations >= M, Number of Observations\n    (normalised to annual value), Cumulative Number of\n    Observations (normalised to annual value)]\n    Counts number and cumulative number of occurrences of\n    each magnitude in catalogue\n\n    :param numpy.ndarray mag:\n        Catalog matrix magnitude column\n    :param numpy.ndarray dmag:\n        Magnitude interval\n    :param numpy.ndarray year:\n        Catalog matrix year column\n\n    :returns numpy.ndarray recurrence table:\n        Recurrence table\n    \"\"\"\n    # Define magnitude vectors\n    if time_interval is None:\n        num_year = np.max(year) - np.min(year) + 1.\n    else:\n        num_year = time_interval\n    upper_m = np.max(np.ceil(10.0 * mag) / 10.0)\n    lower_m = np.min(np.floor(10.0 * mag) / 10.0)\n    mag_range = np.arange(lower_m, upper_m + (1.5 * dmag), dmag)\n    mval = mag_range[:-1] + (dmag / 2.0)\n    # Find number of earthquakes inside range\n    number_obs = np.histogram(mag, mag_range)[0]\n    number_rows = np.shape(number_obs)[0]\n    # Cumulative number of events\n    n_c = np.zeros((number_rows, 1))\n    i = 0\n    while i < number_rows:\n        n_c[i] = np.sum(number_obs[i:], axis=0)\n        i += 1\n    # Normalise to Annual Rate\n    number_obs_annual = number_obs / num_year\n    n_c_annual = n_c / num_year\n    rec_table = np.column_stack([mval, number_obs, n_c, number_obs_annual,\n                                 n_c_annual])\n    return rec_table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef input_checks(catalogue, config, completeness):\n\n    if isinstance(completeness, np.ndarray):\n        # completeness table is a numpy array (i.e. [year, magnitude])\n        if np.shape(completeness)[1] != 2:\n            raise ValueError('Completeness Table incorrectly configured')\n        else:\n            cmag = completeness[:, 1]\n            ctime = completeness[:, 0]\n    elif isinstance(completeness, float):\n        # Completeness corresponds to a single magnitude (i.e. applies to\n        # the entire catalogue)\n        cmag = np.array(completeness)\n        ctime = np.array(np.min(catalogue.data['year']))\n    else:\n        # Everything is valid - i.e. no completeness magnitude\n        cmag = np.array(np.min(catalogue.data['magnitude']))\n        ctime = np.array(np.min(catalogue.data['year']))\n\n    # Set reference magnitude - if not in config then default to M = 0.\n    if not config:\n        # use default reference magnitude of 0.0 and magnitude interval of 0.1\n        ref_mag = 0.0\n        dmag = 0.1\n        config = {'reference_magnitude': None,\n                  'magnitude_interval': 0.1}\n    else:\n        if (not 'reference_magnitude' in config.keys()) or\\\n                (config['reference_magnitude'] is None):\n            ref_mag = 0.\n            config['reference_magnitude'] = None\n        else:\n            ref_mag = config['reference_magnitude']\n\n        if (not 'magnitude_interval' in config.keys()) or \\\n                not config['magnitude_interval']:\n            dmag = 0.1\n        else:\n            dmag = config['magnitude_interval']\n\n    return cmag, ctime, ref_mag, dmag, config", "response": "Performs basic checks on the data of the resource resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_trunc_gr_magnitudes(bval, mmin, mmax, nsamples):\n    '''\n    Generate a random list of magnitudes distributed according to a\n    truncated Gutenberg-Richter model\n\n    :param float bval:\n        b-value\n    :param float mmin:\n        Minimum Magnitude\n    :param float mmax:\n        Maximum Magnitude\n    :param int nsamples:\n        Number of samples\n\n    :returns:\n        Vector of generated magnitudes\n    '''\n    sampler = np.random.uniform(0., 1., nsamples)\n    beta = bval * np.log(10.)\n    return (-1. / beta) * (\n        np.log(1. - sampler * (1 - np.exp(-beta * (mmax - mmin))))) + mmin", "response": "Generate a random list of magnitudes distributed according to truncated Gutenberg - Richter model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_synthetic_magnitudes(aval, bval, mmin, mmax, nyears):\n    '''\n    Generates a synthetic catalogue for a specified number of years, with\n    magnitudes distributed according to a truncated Gutenberg-Richter\n    distribution\n\n    :param float aval:\n        a-value\n    :param float bval:\n        b-value\n    :param float mmin:\n        Minimum Magnitude\n    :param float mmax:\n        Maximum Magnitude\n    :param int nyears:\n        Number of years\n    :returns:\n        Synthetic catalogue (dict) with year and magnitude attributes\n    '''\n    nsamples = int(np.round(nyears * (10. ** (aval - bval * mmin)), 0))\n    year = np.random.randint(0, nyears, nsamples)\n    # Get magnitudes\n    mags = generate_trunc_gr_magnitudes(bval, mmin, mmax, nsamples)\n    return {'magnitude': mags, 'year': np.sort(year)}", "response": "Generates a synthetic catalogue for a specified number of years with a given minimum and maximum magnitude."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef downsample_completeness_table(comp_table, sample_width=0.1, mmax=None):\n    new_comp_table = []\n    for i in range(comp_table.shape[0] - 1):\n        mvals = np.arange(comp_table[i, 1],\n                          comp_table[i + 1, 1], d_m)  # FIXME: d_m is undefined!\n        new_comp_table.extend([[comp_table[i, 0], mval] for mval in mvals])\n    # If mmax > last magnitude in completeness table\n    if mmax and (mmax > comp_table[-1, 1]):\n        new_comp_table.extend(\n            [[comp_table[-1, 0], mval]\n             for mval in np.arange(comp_table[-1, 1], mmax + d_m, d_m)])\n    return np.array(new_comp_table)", "response": "Downsample the completeness table to a specified sample_width"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the number of earthquakes in a set of magnitude bins of specified base.", "response": "def get_completeness_counts(catalogue, completeness, d_m):\n    \"\"\"\n    Returns the number of earthquakes in a set of magnitude bins of specified\n    with, along with the corresponding completeness duration (in years) of the\n    bin\n\n    :param catalogue:\n        Earthquake catalogue as instance of\n        :class: openquake.hmtk.seisimicity.catalogue.Catalogue\n    :param numpy.ndarray completeness:\n        Completeness table [year, magnitude]\n    :param float d_m:\n        Bin size\n    :returns:\n        * cent_mag - array indicating center of magnitude bins\n        * t_per - array indicating total duration (in years) of completeness\n        * n_obs - number of events in completeness period\n    \"\"\"\n    mmax_obs = np.max(catalogue.data[\"magnitude\"])\n    # thw line below was added by Nick Ackerley but it breaks the tests\n    # catalogue.data[\"dtime\"] = catalogue.get_decimal_time()\n    if mmax_obs > np.max(completeness[:, 1]):\n        cmag = np.hstack([completeness[:, 1], mmax_obs])\n    else:\n        cmag = completeness[:, 1]\n    cyear = np.hstack([catalogue.end_year + 1, completeness[:, 0]])\n\n    # When the magnitude value is on the bin edge numpy's histogram function\n    # may assign randomly to one side or the other based on the floating\n    # point value. As catalogues are rounded to the nearest 0.1 this occurs\n    # frequently! So we offset the bin edge by a very tiny amount to ensure\n    # that, for example, M = 4.099999999 is assigned to the bin M = 4.1 and\n    # not 4.0\n    master_bins = np.arange(np.min(cmag) - 1.0E-7,\n                            np.max(cmag) + d_m,\n                            d_m)\n    count_rates = np.zeros(len(master_bins) - 1)\n    count_years = np.zeros_like(count_rates)\n    for i in range(len(cyear) - 1):\n        time_idx = np.logical_and(catalogue.data[\"dtime\"] < cyear[i],\n                                  catalogue.data[\"dtime\"] >= cyear[i + 1])\n        nyrs = cyear[i] - cyear[i + 1]\n        sel_mags = catalogue.data[\"magnitude\"][time_idx]\n        m_idx = np.where(master_bins >= (cmag[i] - (d_m / 2.)))[0]\n        m_bins = master_bins[m_idx]\n        count_rates[m_idx[:-1]] += np.histogram(\n            sel_mags,\n            bins=m_bins)[0].astype(float)\n        count_years[m_idx[:-1]] += float(nyrs)\n    # Removes any zero rates greater than\n    last_loc = np.where(count_rates > 0)[0][-1]\n    n_obs = count_rates[:(last_loc + 1)]\n    t_per = count_years[:(last_loc + 1)]\n    cent_mag = (master_bins[:-1] + master_bins[1:]) / 2.\n    cent_mag = np.around(cent_mag[:(last_loc + 1)], 3)\n    return cent_mag, t_per, n_obs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove all the datastores and the database of the current user", "response": "def reset(yes):\n    \"\"\"\n    Remove all the datastores and the database of the current user\n    \"\"\"\n    ok = yes or confirm('Do you really want to destroy all your data? (y/n) ')\n    if not ok:\n        return\n\n    dbpath = os.path.realpath(os.path.expanduser(config.dbserver.file))\n\n    # user must be able to access and write the databse file to remove it\n    if os.path.isfile(dbpath) and os.access(dbpath, os.W_OK):\n        if dbserver.get_status() == 'running':\n            if config.dbserver.multi_user:\n                sys.exit('The oq dbserver must be stopped '\n                         'before proceeding')\n            else:\n                pid = logs.dbcmd('getpid')\n                os.kill(pid, signal.SIGTERM)\n                time.sleep(.5)  # give time to stop\n                assert dbserver.get_status() == 'not-running'\n                print('dbserver stopped')\n        try:\n            os.remove(dbpath)\n            print('Removed %s' % dbpath)\n        except OSError as exc:\n            print(exc, file=sys.stderr)\n\n    # fast way of removing everything\n    purge_all(fast=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute mean value according to equation 30 page 1021.", "response": "def _compute_mean(self, C, mag, r):\n        \"\"\"\n        Compute mean value according to equation 30, page 1021.\n        \"\"\"\n        mean = (C['c1'] +\n                self._compute_term1(C, mag) +\n                self._compute_term2(C, mag, r))\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_term2(self, C, mag, r):\n        return (C['c4'] + C['c5'] * mag) * \\\n            np.log(np.sqrt(r**2 + C['c6']**2)) + C['c7'] * r", "response": "This computes the term f2 in equation 8 Drouet & Cotton 2015"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute and return mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        C = self.COEFFS[imt]\n        mean = self._compute_mean(C, rup.mag, dists.rrup)\n        if isinstance(imt, SA) or isinstance(imt, PGA):\n            # Convert from m/s**2 to g\n            mean = mean - np.log(g)\n        elif isinstance(imt, PGV):  # Convert from m/s to cm/s\n            mean = mean + np.log(100.0)\n        stddevs = self._get_stddevs(C, stddev_types, rup.mag,\n                                    dists.rrup.shape[0])\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the status of the current resource in the database.", "response": "def set_status(db, job_id, status):\n    \"\"\"\n    Set the status 'created', 'executing', 'complete', 'failed', 'aborted'\n    consistently with `is_running`.\n\n    :param db: a :class:`openquake.server.dbapi.Db` instance\n    :param job_id: ID of the current job\n    :param status: status string\n    \"\"\"\n    assert status in (\n        'created', 'submitted', 'executing', 'complete', 'aborted', 'failed'\n    ), status\n    if status in ('created', 'complete', 'failed', 'aborted'):\n        is_running = 0\n    else:  # 'executing'\n        is_running = 1\n    if job_id < 0:\n        rows = db('SELECT id FROM job ORDER BY id DESC LIMIT ?x', -job_id)\n        if not rows:\n            return 0\n        job_id = rows[-1].id\n    cursor = db('UPDATE job SET status=?x, is_running=?x WHERE id=?x',\n                status, is_running, job_id)\n    return cursor.rowcount"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_job(db, datadir):\n    calc_id = get_calc_id(db, datadir) + 1\n    job = dict(id=calc_id, is_running=1, description='just created',\n               user_name='openquake', calculation_mode='to be set',\n               ds_calc_dir=os.path.join('%s/calc_%s' % (datadir, calc_id)))\n    return db('INSERT INTO job (?S) VALUES (?X)',\n              job.keys(), job.values()).lastrowid", "response": "Create a new job for the given user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert a job into the database.", "response": "def import_job(db, calc_id, calc_mode, description, user_name, status,\n               hc_id, datadir):\n    \"\"\"\n    Insert a calculation inside the database, if calc_id is not taken\n    \"\"\"\n    job = dict(id=calc_id,\n               calculation_mode=calc_mode,\n               description=description,\n               user_name=user_name,\n               hazard_calculation_id=hc_id,\n               is_running=0,\n               status=status,\n               ds_calc_dir=os.path.join('%s/calc_%s' % (datadir, calc_id)))\n    db('INSERT INTO job (?S) VALUES (?X)', job.keys(), job.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a job from the database.", "response": "def get_job(db, job_id, username=None):\n    \"\"\"\n    If job_id is negative, return the last calculation of the current\n    user, otherwise returns the job_id unchanged.\n\n    :param db: a :class:`openquake.server.dbapi.Db` instance\n    :param job_id: a job ID (can be negative and can be nonexisting)\n    :param username: an user name (if None, ignore it)\n    :returns: a valid job or None if the original job ID was invalid\n    \"\"\"\n    job_id = int(job_id)\n\n    if job_id > 0:\n        dic = dict(id=job_id)\n        if username:\n            dic['user_name'] = username\n        try:\n            return db('SELECT * FROM job WHERE ?A', dic, one=True)\n        except NotFound:\n            return\n\n    # else negative job_id\n    if username:\n        joblist = db('SELECT * FROM job WHERE user_name=?x '\n                     'ORDER BY id DESC LIMIT ?x', username, -job_id)\n    else:\n        joblist = db('SELECT * FROM job ORDER BY id DESC LIMIT ?x', -job_id)\n    if not joblist:  # no jobs\n        return\n    else:\n        return joblist[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the latest calc_id by looking both at the datastore and the database.", "response": "def get_calc_id(db, datadir, job_id=None):\n    \"\"\"\n    Return the latest calc_id by looking both at the datastore\n    and the database.\n\n    :param db: a :class:`openquake.server.dbapi.Db` instance\n    :param datadir: the directory containing the datastores\n    :param job_id: a job ID; if None, returns the latest job ID\n    \"\"\"\n    calcs = datastore.get_calc_ids(datadir)\n    calc_id = 0 if not calcs else calcs[-1]\n    if job_id is None:\n        try:\n            job_id = db('SELECT seq FROM sqlite_sequence WHERE name=\"job\"',\n                        scalar=True)\n        except NotFound:\n            job_id = 0\n    return max(calc_id, job_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_calculations(db, job_type, user_name):\n    jobs = db('SELECT *, %s FROM job WHERE user_name=?x '\n              'AND job_type=?x ORDER BY start_time' % JOB_TYPE,\n              user_name, job_type)\n    out = []\n    if len(jobs) == 0:\n        out.append('None')\n    else:\n        out.append('job_id |     status |          start_time | '\n                   '        description')\n        for job in jobs:\n            descr = job.description\n            start_time = job.start_time\n            out.append('%6d | %10s | %s | %s' % (\n                job.id, job.status, start_time, descr))\n    return out", "response": "Yield a summary of past calculations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the outputs for a given job.", "response": "def list_outputs(db, job_id, full=True):\n    \"\"\"\n    List the outputs for a given\n    :class:`~openquake.server.db.models.OqJob`.\n\n    :param db:\n        a :class:`openquake.server.dbapi.Db` instance\n    :param job_id:\n        ID of a calculation.\n    :param bool full:\n        If True produce a full listing, otherwise a short version\n    \"\"\"\n    outputs = get_outputs(db, job_id)\n    out = []\n    if len(outputs) > 0:\n        truncated = False\n        out.append('  id | name')\n        outs = sorted(outputs, key=operator.attrgetter('display_name'))\n        for i, o in enumerate(outs):\n            if not full and i >= 10:\n                out.append(' ... | %d additional output(s)' % (len(outs) - 10))\n                truncated = True\n                break\n            out.append('%4d | %s' % (o.id, o.display_name))\n        if truncated:\n            out.append('Some outputs where not shown. You can see the full '\n                       'list with the command\\n`oq engine --list-outputs`')\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_outputs(db, job_id, keysize, ds_size):\n    rows = [(job_id, DISPLAY_NAME.get(key, key), key, size)\n            for key, size in keysize]\n    db('UPDATE job SET size_mb=?x WHERE id=?x', ds_size, job_id)\n    db.insert('output', 'oq_job_id display_name ds_key size_mb'.split(), rows)", "response": "Create a correspondence between the outputs in the datastore and the current job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef finish(db, job_id, status):\n    db('UPDATE job SET ?D WHERE id=?x',\n       dict(is_running=False, status=status, stop_time=datetime.utcnow()),\n       job_id)", "response": "Set the status of the current job in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef del_calc(db, job_id, user):\n    job_id = int(job_id)\n    dependent = db(\n        'SELECT id FROM job WHERE hazard_calculation_id=?x', job_id)\n    if dependent:\n        return {\"error\": 'Cannot delete calculation %d: there '\n                'are calculations '\n                'dependent from it: %s' % (job_id, [j.id for j in dependent])}\n    try:\n        owner, path = db('SELECT user_name, ds_calc_dir FROM job WHERE id=?x',\n                         job_id, one=True)\n    except NotFound:\n        return {\"error\": 'Cannot delete calculation %d:'\n                ' ID does not exist' % job_id}\n\n    deleted = db('DELETE FROM job WHERE id=?x AND user_name=?x',\n                 job_id, user).rowcount\n    if not deleted:\n        return {\"error\": 'Cannot delete calculation %d: it belongs to '\n                '%s and you are %s' % (job_id, owner, user)}\n\n    # try to delete datastore and associated file\n    # path has typically the form /home/user/oqdata/calc_XXX\n    fname = path + \".hdf5\"\n    try:\n        os.remove(fname)\n    except OSError as exc:  # permission error\n        return {\"error\": 'Could not remove %s: %s' % (fname, exc)}\n    return {\"success\": fname}", "response": "Delete a calculation and all associated outputs if possible."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a log record in the database.", "response": "def log(db, job_id, timestamp, level, process, message):\n    \"\"\"\n    Write a log record in the database.\n\n    :param db:\n        a :class:`openquake.server.dbapi.Db` instance\n    :param job_id:\n        a job ID\n    :param timestamp:\n        timestamp to store in the log record\n    :param level:\n        logging level to store in the log record\n    :param process:\n        process ID to store in the log record\n    :param message:\n        message to store in the log record\n    \"\"\"\n    db('INSERT INTO log (job_id, timestamp, level, process, message) '\n       'VALUES (?X)', (job_id, timestamp, level, process, message))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_log(db, job_id):\n    logs = db('SELECT * FROM log WHERE job_id=?x ORDER BY id', job_id)\n    out = []\n    for log in logs:\n        time = str(log.timestamp)[:-4]  # strip decimals\n        out.append('[%s #%d %s] %s' % (time, job_id, log.level, log.message))\n    return out", "response": "Extract the logs as a big string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the output from the database.", "response": "def get_output(db, output_id):\n    \"\"\"\n    :param db: a :class:`openquake.server.dbapi.Db` instance\n    :param output_id: ID of an Output object\n    :returns: (ds_key, calc_id, dirname)\n    \"\"\"\n    out = db('SELECT output.*, ds_calc_dir FROM output, job '\n             'WHERE oq_job_id=job.id AND output.id=?x', output_id, one=True)\n    return out.ds_key, out.oq_job_id, os.path.dirname(out.ds_calc_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving in the database the performance information about the given job.", "response": "def save_performance(db, job_id, records):\n    \"\"\"\n    Save in the database the performance information about the given job.\n\n    :param db: a :class:`openquake.server.dbapi.Db` instance\n    :param job_id: a job ID\n    :param records: a list of performance records\n    \"\"\"\n    # NB: rec['counts'] is a numpy.uint64 which is not automatically converted\n    # into an int in Ubuntu 12.04, so we convert it manually below\n    rows = [(job_id, rec['operation'], rec['time_sec'], rec['memory_mb'],\n             int(rec['counts'])) for rec in records]\n    db.insert('performance',\n              'job_id operation time_sec memory_mb counts'.split(), rows)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_info(db, calc_id):\n    job = db('SELECT * FROM job WHERE id=?x', calc_id, one=True)\n    response_data = {}\n    response_data['user_name'] = job.user_name\n    response_data['status'] = job.status\n    response_data['start_time'] = str(job.start_time)\n    response_data['stop_time'] = str(job.stop_time)\n    response_data['is_running'] = job.is_running\n    return response_data", "response": "Returns the information about the given calculation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_log_slice(db, job_id, start, stop):\n    start = int(start)\n    stop = int(stop)\n    limit = -1 if stop == 0 else stop - start\n    logs = db('SELECT * FROM log WHERE job_id=?x '\n              'ORDER BY id LIMIT ?s OFFSET ?s',\n              job_id, limit, start)\n    # NB: .isoformat() returns a string like '2016-08-29T15:42:34.984756'\n    # we consider only the first 22 characters, i.e. '2016-08-29T15:42:34.98'\n    return [[log.timestamp.isoformat()[:22], log.level,\n             log.process, log.message] for log in logs]", "response": "Get a slice of the calculation log as a JSON list of rows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the traceback of the given calculation as a list of lines.", "response": "def get_traceback(db, job_id):\n    \"\"\"\n    Return the traceback of the given calculation as a list of lines.\n    The list is empty if the calculation was successful.\n\n    :param db:\n        a :class:`openquake.server.dbapi.Db` instance\n    :param job_id:\n        a job ID\n    \"\"\"\n    # strange: understand why the filter returns two lines or zero lines\n    log = db(\"SELECT * FROM log WHERE job_id=?x AND level='CRITICAL'\",\n             job_id)\n    if not log:\n        return []\n    response_data = log[-1].message.splitlines()\n    return response_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the result from the database.", "response": "def get_result(db, result_id):\n    \"\"\"\n    :param db:\n        a :class:`openquake.server.dbapi.Db` instance\n    :param result_id:\n        a result ID\n    :returns: (job_id, job_status, datadir, datastore_key)\n    \"\"\"\n    job = db('SELECT job.*, ds_key FROM job, output WHERE '\n             'oq_job_id=job.id AND output.id=?x', result_id, one=True)\n    return (job.id, job.status, job.user_name,\n            os.path.dirname(job.ds_calc_dir), job.ds_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_results(db, job_id):\n    ds_calc_dir = db('SELECT ds_calc_dir FROM job WHERE id=?x', job_id,\n                     scalar=True)\n    datadir = os.path.dirname(ds_calc_dir)\n    return datadir, [output.ds_key for output in get_outputs(db, job_id)]", "response": "Get the results of a single job."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_executing_jobs(db):\n    fields = 'id,pid,user_name,start_time'\n    running = List()\n    running._fields = fields.split(',')\n\n    query = ('''-- executing jobs\nSELECT %s FROM job WHERE status='executing' ORDER BY id desc''' % fields)\n    rows = db(query)\n    for r in rows:\n        # if r.pid is 0 it means that such information\n        # is not available in the database\n        if r.pid and psutil.pid_exists(r.pid):\n            running.append(r)\n    return running", "response": "Returns a list of all the running jobs in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the z1pt0 of the current node", "response": "def calculate_z1pt0(vs30):\n    '''\n    Reads an array of vs30 values (in m/s) and\n    returns the depth to the 1.0 km/s velocity horizon (in m)\n    Ref: Chiou & Youngs (2014) California model\n    :param vs30: the shear wave velocity (in m/s) at a depth of 30m\n    '''\n    c1 = 571 ** 4.\n    c2 = 1360.0 ** 4.\n    return numpy.exp((-7.15 / 4.0) * numpy.log((vs30 ** 4. + c1) / (c2 + c1)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the depth to the 2. 5 km / s velocity horizon at a given vs30.", "response": "def calculate_z2pt5_ngaw2(vs30):\n    '''\n    Reads an array of vs30 values (in m/s) and\n    returns the depth to the 2.5 km/s velocity horizon (in km)\n    Ref: Campbell, K.W. & Bozorgnia, Y., 2014.\n    'NGA-West2 ground motion model for the average horizontal components of\n    PGA, PGV, and 5pct damped linear acceleration response spectra.'\n    Earthquake Spectra, 30(3), pp.1087\u20131114.\n\n    :param vs30: the shear wave velocity (in m/s) at a depth of 30 m\n    '''\n    c1 = 7.089\n    c2 = -1.144\n    z2pt5 = numpy.exp(c1 + numpy.log(vs30) * c2)\n    return z2pt5"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_vs30(fnames):\n    data = []\n    for fname in fnames:\n        for line in open(fname, encoding='utf-8-sig'):\n            data.append(tuple(line.split(',')))\n    return numpy.array(data, vs30_dt)", "response": "reads a list of CSV files with fields lon lat vs30"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_site_model(exposure_xml, sites_csv, vs30_csv,\n                       z1pt0, z2pt5, vs30measured, grid_spacing=0,\n                       assoc_distance=5, output='site_model.csv'):\n    \"\"\"\n    Prepare a site_model.csv file from exposure xml files/site csv files,\n    vs30 csv files and a grid spacing which can be 0 (meaning no grid).\n    For each site the closest vs30 parameter is used. The command can also\n    generate (on demand) the additional fields z1pt0, z2pt5 and vs30measured\n    which may be needed by your hazard model, depending on the required GSIMs.\n    \"\"\"\n    hdf5 = datastore.hdf5new()\n    req_site_params = {'vs30'}\n    fields = ['lon', 'lat', 'vs30']\n    if z1pt0:\n        req_site_params.add('z1pt0')\n        fields.append('z1pt0')\n    if z2pt5:\n        req_site_params.add('z2pt5')\n        fields.append('z2pt5')\n    if vs30measured:\n        req_site_params.add('vs30measured')\n        fields.append('vs30measured')\n    with performance.Monitor(hdf5.path, hdf5, measuremem=True) as mon:\n        if exposure_xml:\n            mesh, assets_by_site = Exposure.read(\n                exposure_xml, check_dupl=False).get_mesh_assets_by_site()\n            mon.hdf5['assetcol'] = assetcol = site.SiteCollection.from_points(\n                mesh.lons, mesh.lats, req_site_params=req_site_params)\n            if grid_spacing:\n                grid = mesh.get_convex_hull().dilate(\n                    grid_spacing).discretize(grid_spacing)\n                haz_sitecol = site.SiteCollection.from_points(\n                    grid.lons, grid.lats, req_site_params=req_site_params)\n                logging.info(\n                    'Associating exposure grid with %d locations to %d '\n                    'exposure sites', len(haz_sitecol), len(assets_by_site))\n                haz_sitecol, assets_by, discarded = assoc(\n                    assets_by_site, haz_sitecol,\n                    grid_spacing * SQRT2, 'filter')\n                if len(discarded):\n                    logging.info('Discarded %d sites with assets '\n                                 '[use oq plot_assets]', len(discarded))\n                    mon.hdf5['discarded'] = numpy.array(discarded)\n                haz_sitecol.make_complete()\n            else:\n                haz_sitecol = assetcol\n                discarded = []\n        elif sites_csv:\n            lons, lats = [], []\n            for fname in sites_csv:\n                with open(fname) as csv:\n                    for line in csv:\n                        if line.startswith('lon,lat'):  # possible header\n                            continue\n                        lon, lat = line.split(',')[:2]\n                        lons.append(valid.longitude(lon))\n                        lats.append(valid.latitude(lat))\n            haz_sitecol = site.SiteCollection.from_points(\n                lons, lats, req_site_params=req_site_params)\n            if grid_spacing:\n                grid = mesh.get_convex_hull().dilate(\n                    grid_spacing).discretize(grid_spacing)\n                haz_sitecol = site.SiteCollection.from_points(\n                    grid.lons, grid.lats, req_site_params=req_site_params)\n        else:\n            raise RuntimeError('Missing exposures or missing sites')\n        vs30orig = read_vs30(vs30_csv)\n        logging.info('Associating %d hazard sites to %d site parameters',\n                     len(haz_sitecol), len(vs30orig))\n        sitecol, vs30, _ = assoc(\n            vs30orig, haz_sitecol, assoc_distance, 'warn')\n        sitecol.array['vs30'] = vs30['vs30']\n        if z1pt0:\n            sitecol.array['z1pt0'] = calculate_z1pt0(vs30['vs30'])\n        if z2pt5:\n            sitecol.array['z2pt5'] = calculate_z2pt5_ngaw2(vs30['vs30'])\n        if vs30measured:\n            sitecol.array['vs30measured'] = False  # it is inferred\n        mon.hdf5['sitecol'] = sitecol\n        write_csv(output, sitecol.array[fields])\n    logging.info('Saved %d rows in %s' % (len(sitecol), output))\n    logging.info(mon)\n    return sitecol", "response": "Prepare a site_model. csv file from exposure xml files site csv files vs30 csv files and a grid spacing which can be 0 for no grid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the webui server in foreground or perform other operation on the webui application", "response": "def webui(cmd, hostport='127.0.0.1:8800', skip_browser=False):\n    \"\"\"\n    start the webui server in foreground or perform other operation on the\n    django application\n    \"\"\"\n    dbpath = os.path.realpath(os.path.expanduser(config.dbserver.file))\n    if os.path.isfile(dbpath) and not os.access(dbpath, os.W_OK):\n        sys.exit('This command must be run by the proper user: '\n                 'see the documentation for details')\n    if cmd == 'start':\n        dbserver.ensure_on()  # start the dbserver in a subprocess\n        rundjango('runserver', hostport, skip_browser)\n    elif cmd in commands:\n        rundjango(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # get the necessary set of coefficients\n        C = self.COEFFS[imt]\n        # compute median sa on rock (vs30=1180m/s). Used for site response\n        # term calculation\n        sa1180 = np.exp(self._get_sa_at_1180(C, imt, sites, rup, dists))\n\n        # get the mean value\n        mean = (self._get_basic_term(C, rup, dists) +\n                self._get_faulting_style_term(C, rup) +\n                self._get_site_response_term(C, imt, sites.vs30, sa1180) +\n                self._get_hanging_wall_term(C, dists, rup) +\n                self._get_top_of_rupture_depth_term(C, imt, rup) +\n                self._get_soil_depth_term(C, sites.z1pt0 / METRES_PER_KM,\n                                          sites.vs30)\n                )\n        mean += self._get_regional_term(C, imt, sites.vs30, dists.rrup)\n        # get standard deviations\n        stddevs = self._get_stddevs(C, imt, rup, sites, stddev_types, sa1180,\n                                    dists)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_sa_at_1180(self, C, imt, sites, rup, dists):\n        # reference vs30 = 1180 m/s\n        vs30_1180 = np.ones_like(sites.vs30) * 1180.\n        # reference shaking intensity = 0\n        ref_iml = np.zeros_like(sites.vs30)\n        # fake Z1.0 - Since negative it will be replaced by the default Z1.0\n        # for the corresponding region\n        fake_z1pt0 = np.ones_like(sites.vs30) * -1\n        return (self._get_basic_term(C, rup, dists) +\n                self._get_faulting_style_term(C, rup) +\n                self._get_site_response_term(C, imt, vs30_1180, ref_iml) +\n                self._get_hanging_wall_term(C, dists, rup) +\n                self._get_top_of_rupture_depth_term(C, imt, rup) +\n                self._get_soil_depth_term(C, fake_z1pt0, vs30_1180) +\n                self._get_regional_term(C, imt, vs30_1180, dists.rrup)\n                )", "response": "Compute and return mean Sa at 1180 term"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute and return basic form of the assessment class term defined in equation 1 at page 1030", "response": "def _get_basic_term(self, C, rup, dists):\n        \"\"\"\n        Compute and return basic form, see page 1030.\n        \"\"\"\n        # Fictitious depth calculation\n        if rup.mag > 5.:\n            c4m = C['c4']\n        elif rup.mag > 4.:\n            c4m = C['c4'] - (C['c4']-1.) * (5. - rup.mag)\n        else:\n            c4m = 1.\n        R = np.sqrt(dists.rrup**2. + c4m**2.)\n        # basic form\n        base_term = C['a1'] * np.ones_like(dists.rrup) + C['a17'] * dists.rrup\n        # equation 2 at page 1030\n        if rup.mag >= C['m1']:\n            base_term += (C['a5'] * (rup.mag - C['m1']) +\n                          C['a8'] * (8.5 - rup.mag)**2. +\n                          (C['a2'] + C['a3'] * (rup.mag - C['m1'])) *\n                          np.log(R))\n        elif rup.mag >= self.CONSTS['m2']:\n            base_term += (C['a4'] * (rup.mag - C['m1']) +\n                          C['a8'] * (8.5 - rup.mag)**2. +\n                          (C['a2'] + C['a3'] * (rup.mag - C['m1'])) *\n                          np.log(R))\n        else:\n            base_term += (C['a4'] * (self.CONSTS['m2'] - C['m1']) +\n                          C['a8'] * (8.5 - self.CONSTS['m2'])**2. +\n                          C['a6'] * (rup.mag - self.CONSTS['m2']) +\n                          C['a7'] * (rup.mag - self.CONSTS['m2'])**2. +\n                          (C['a2'] + C['a3'] * (self.CONSTS['m2'] - C['m1'])) *\n                          np.log(R))\n        return base_term"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing and return faulting style term in equation 1 page 74.", "response": "def _get_faulting_style_term(self, C, rup):\n        \"\"\"\n        Compute and return faulting style term, that is the sum of the second\n        and third terms in equation 1, page 74.\n        \"\"\"\n        # this implements equations 5 and 6 at page 1032. f7 is the\n        # coefficient for reverse mechanisms while f8 is the correction\n        # factor for normal ruptures\n        if rup.mag > 5.0:\n            f7 = C['a11']\n            f8 = C['a12']\n        elif rup.mag >= 4:\n            f7 = C['a11'] * (rup.mag - 4.)\n            f8 = C['a12'] * (rup.mag - 4.)\n        else:\n            f7 = 0.0\n            f8 = 0.0\n        # ranges of rake values for each faulting mechanism are specified in\n        # table 2, page 1031\n        return (f7 * float(rup.rake > 30 and rup.rake < 150) +\n                f8 * float(rup.rake > -150 and rup.rake < -30))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing and return the vs30 star value for the current object.", "response": "def _get_vs30star(self, vs30, imt):\n        \"\"\"\n        This computes equations 8 and 9 at page 1034\n        \"\"\"\n        # compute the v1 value (see eq. 9, page 1034)\n        if imt.name == \"SA\":\n            t = imt.period\n            if t <= 0.50:\n                v1 = 1500.0\n            elif t < 3.0:\n                v1 = np.exp(-0.35 * np.log(t / 0.5) + np.log(1500.))\n            else:\n                v1 = 800.0\n        elif imt.name == \"PGA\":\n            v1 = 1500.0\n        else:\n            # This covers the PGV case\n            v1 = 1500.0\n        # set the vs30 star value (see eq. 8, page 1034)\n        vs30_star = np.ones_like(vs30) * vs30\n        vs30_star[vs30 >= v1] = v1\n        return vs30_star"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_site_response_term(self, C, imt, vs30, sa1180):\n        # vs30 star\n        vs30_star = self._get_vs30star(vs30, imt)\n        # compute the site term\n        site_resp_term = np.zeros_like(vs30)\n        gt_vlin = vs30 >= C['vlin']\n        lw_vlin = vs30 < C['vlin']\n        # compute site response term for sites with vs30 greater than vlin\n        vs30_rat = vs30_star / C['vlin']\n        site_resp_term[gt_vlin] = ((C['a10'] + C['b'] * self.CONSTS['n']) *\n                                   np.log(vs30_rat[gt_vlin]))\n        # compute site response term for sites with vs30 lower than vlin\n        site_resp_term[lw_vlin] = (C['a10'] * np.log(vs30_rat[lw_vlin]) -\n                                   C['b'] * np.log(sa1180[lw_vlin] + C['c']) +\n                                   C['b'] * np.log(sa1180[lw_vlin] + C['c'] *\n                                                   vs30_rat[lw_vlin] **\n                                                   self.CONSTS['n']))\n        return site_resp_term", "response": "Compute and return site response model term see page 1033\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and return hanging wall model term see page 1038.", "response": "def _get_hanging_wall_term(self, C, dists, rup):\n        \"\"\"\n        Compute and return hanging wall model term, see page 1038.\n        \"\"\"\n        if rup.dip == 90.0:\n            return np.zeros_like(dists.rx)\n        else:\n            Fhw = np.zeros_like(dists.rx)\n            Fhw[dists.rx > 0] = 1.\n            # Compute taper t1\n            T1 = np.ones_like(dists.rx)\n            T1 *= 60./45. if rup.dip <= 30. else (90.-rup.dip)/45.0\n            # Compute taper t2 (eq 12 at page 1039) - a2hw set to 0.2 as\n            # indicated at page 1041\n            T2 = np.zeros_like(dists.rx)\n            a2hw = 0.2\n            if rup.mag > 6.5:\n                T2 += (1. + a2hw * (rup.mag - 6.5))\n            elif rup.mag > 5.5:\n                T2 += (1. + a2hw * (rup.mag - 6.5) - (1. - a2hw) *\n                       (rup.mag - 6.5)**2)\n            else:\n                T2 *= 0.\n            # Compute taper t3 (eq. 13 at page 1039) - r1 and r2 specified at\n            # page 1040\n            T3 = np.zeros_like(dists.rx)\n            r1 = rup.width * np.cos(np.radians(rup.dip))\n            r2 = 3. * r1\n            #\n            idx = dists.rx < r1\n            T3[idx] = (np.ones_like(dists.rx)[idx] * self.CONSTS['h1'] +\n                       self.CONSTS['h2'] * (dists.rx[idx] / r1) +\n                       self.CONSTS['h3'] * (dists.rx[idx] / r1)**2)\n            #\n            idx = ((dists.rx >= r1) & (dists.rx <= r2))\n            T3[idx] = 1. - (dists.rx[idx] - r1) / (r2 - r1)\n            # Compute taper t4 (eq. 14 at page 1040)\n            T4 = np.zeros_like(dists.rx)\n            #\n            if rup.ztor <= 10.:\n                T4 += (1. - rup.ztor**2. / 100.)\n            # Compute T5 (eq 15a at page 1040) - ry1 computed according to\n            # suggestions provided at page 1040\n            T5 = np.zeros_like(dists.rx)\n            ry1 = dists.rx * np.tan(np.radians(20.))\n            #\n            idx = (dists.ry0 - ry1) <= 0.0\n            T5[idx] = 1.\n            #\n            idx = (((dists.ry0 - ry1) > 0.0) & ((dists.ry0 - ry1) < 5.0))\n            T5[idx] = 1. - (dists.ry0[idx] - ry1[idx]) / 5.0\n            # Finally, compute the hanging wall term\n            return Fhw*C['a13']*T1*T2*T3*T4*T5"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute and return top of rupture depth term in equation 1 page 1042.", "response": "def _get_top_of_rupture_depth_term(self, C, imt, rup):\n        \"\"\"\n        Compute and return top of rupture depth term. See paragraph\n        'Depth-to-Top of Rupture Model', page 1042.\n        \"\"\"\n        if rup.ztor >= 20.0:\n            return C['a15']\n        else:\n            return C['a15'] * rup.ztor / 20.0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_soil_depth_term(self, C, z1pt0, vs30):\n        # Get reference z1pt0\n        z1ref = self._get_z1pt0ref(vs30)\n        # Get z1pt0\n        z10 = copy.deepcopy(z1pt0)\n        # This is used for the calculation of the motion on reference rock\n        idx = z1pt0 < 0\n        z10[idx] = z1ref[idx]\n        factor = np.log((z10 + 0.01) / (z1ref + 0.01))\n        # Here we use a linear interpolation as suggested in the 'Application\n        # guidelines' at page 1044\n        # Above 700 m/s the trend is flat, but we extend the Vs30 range to\n        # 6,000 m/s (basically the upper limit for mantle shear wave velocity\n        # on earth) to allow extrapolation without throwing an error.\n        f2 = interpolate.interp1d(\n            [0.0, 150, 250, 400, 700, 1000, 6000],\n            [C['a43'], C['a43'], C['a44'], C['a45'], C['a46'], C['a46'],\n             C['a46']],\n            kind='linear')\n        return f2(vs30) * factor", "response": "Compute and return soil depth term. See page 1042."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_stddevs(self, C, imt, rup, sites, stddev_types, sa1180, dists):\n        std_intra = self._get_intra_event_std(C, rup.mag, sa1180, sites.vs30,\n                                              sites.vs30measured, dists.rrup)\n        std_inter = self._get_inter_event_std(C, rup.mag, sa1180, sites.vs30)\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt(std_intra ** 2 +\n                                       std_inter ** 2))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(std_intra)\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(std_inter)\n        return stddevs", "response": "Returns the standard deviations as described in paragraph 1046."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the intr event standard deviation in the species as described at pages 1046 and 1047.", "response": "def _get_intra_event_std(self, C, mag, sa1180, vs30, vs30measured,\n                             rrup):\n        \"\"\"\n        Returns Phi as described at pages 1046 and 1047\n        \"\"\"\n        phi_al = self._get_phi_al_regional(C, mag, vs30measured, rrup)\n        derAmp = self._get_derivative(C, sa1180, vs30)\n        phi_amp = 0.4\n        idx = phi_al < phi_amp\n        if np.any(idx):\n            # In the case of small magnitudes and long periods it is possible\n            # for phi_al to take a value less than phi_amp, which would return\n            # a complex value. According to the GMPE authors in this case\n            # phi_amp should be reduced such that it is fractionally smaller\n            # than phi_al\n            phi_amp = 0.4 * np.ones_like(phi_al)\n            phi_amp[idx] = 0.99 * phi_al[idx]\n        phi_b = np.sqrt(phi_al**2 - phi_amp**2)\n        phi = np.sqrt(phi_b**2 * (1 + derAmp)**2 + phi_amp**2)\n        return phi"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_derivative(self, C, sa1180, vs30):\n        derAmp = np.zeros_like(vs30)\n        n = self.CONSTS['n']\n        c = C['c']\n        b = C['b']\n        idx = vs30 < C['vlin']\n        derAmp[idx] = (b * sa1180[idx] * (-1./(sa1180[idx]+c) +\n                       1./(sa1180[idx] + c*(vs30[idx]/C['vlin'])**n)))\n        return derAmp", "response": "Returns the derivative of the logarithmic logarithm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_phi_al_regional(self, C, mag, vs30measured, rrup):\n        phi_al = np.ones((len(vs30measured)))\n        s1 = np.ones_like(phi_al) * C['s1e']\n        s2 = np.ones_like(phi_al) * C['s2e']\n        s1[vs30measured] = C['s1m']\n        s2[vs30measured] = C['s2m']\n        if mag < 4:\n            phi_al *= s1\n        elif mag <= 6:\n            phi_al *= s1 + (s2 - s1) / 2. * (mag - 4.)\n        else:\n            phi_al *= s2\n        return phi_al", "response": "Returns intra - event (Phi ) standard deviation"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_inter_event_std(self, C, mag, sa1180, vs30):\n        if mag < 5:\n            tau_al = C['s3']\n        elif mag <= 7:\n            tau_al = C['s3'] + (C['s4'] - C['s3']) / 2. * (mag - 5.)\n        else:\n            tau_al = C['s4']\n        tau_b = tau_al\n        tau = tau_b * (1 + self._get_derivative(C, sa1180, vs30))\n        return tau", "response": "Returns inter event standard deviation in equation 25 page 1046."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the regional term in equation 1 page 74.", "response": "def _get_regional_term(self, C, imt, vs30, rrup):\n        \"\"\"\n        In accordance with Abrahamson et al. (2014) we assume as the default\n        region California\n        \"\"\"\n        vs30star = self._get_vs30star(vs30, imt)\n        return C['a31'] * np.log(vs30star/C['vlin']) + C['a25'] * rrup"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_regional_term(self, C, imt, vs30, rrup):\n        f3 = interpolate.interp1d(\n            [150, 250, 350, 450, 600, 850, 1150, 2000],\n            [C['a36'], C['a37'], C['a38'], C['a39'], C['a40'], C['a41'],\n             C['a42'], C['a42']],\n            kind='linear')\n        return f3(vs30) + C['a29'] * rrup", "response": "Compute and return the regional term for Japan. See page 1043."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning intra - event Tau standard deviation equation 26 page 1046.", "response": "def _get_phi_al_regional(self, C, mag, vs30measured, rrup):\n        \"\"\"\n        Returns intra-event (Tau) standard deviation (equation 26, page 1046)\n        \"\"\"\n        phi_al = np.ones((len(vs30measured)))\n\n        idx = rrup < 30\n        phi_al[idx] *= C['s5']\n\n        idx = ((rrup <= 80) & (rrup >= 30.))\n        phi_al[idx] *= C['s5'] + (C['s6'] - C['s5']) / 50. * (rrup[idx] - 30.)\n\n        idx = rrup > 80\n        phi_al[idx] *= C['s6']\n\n        return phi_al"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gc(coeff, mag):\n    if mag > 6.5:\n        a1ca = coeff['ua']\n        a1cb = coeff['ub']\n        a1cc = coeff['uc']\n        a1cd = coeff['ud']\n        a1ce = coeff['ue']\n        a2ca = coeff['ia']\n        a2cb = coeff['ib']\n        a2cc = coeff['ic']\n        a2cd = coeff['id']\n        a2ce = coeff['ie']\n    else:\n        a1ca = coeff['a']\n        a1cb = coeff['b']\n        a1cc = coeff['c']\n        a1cd = coeff['d']\n        a1ce = coeff['e']\n        a2ca = coeff['ma']\n        a2cb = coeff['mb']\n        a2cc = coeff['mc']\n        a2cd = coeff['md']\n        a2ce = coeff['me']\n    return a1ca, a1cb, a1cc, a1cd, a1ce, a2ca, a2cb, a2cc, a2cd, a2ce", "response": "Returns the set of coefficients to be used for the calculation of GM\n    as a function of earthquake magnitude."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rbf(ra, coeff, mag):\n    a1ca, a1cb, a1cc, a1cd, a1ce, a2ca, a2cb, a2cc, a2cd, a2ce = gc(coeff, mag)\n    term1 = a1ca + a1cb * mag + a1cc * np.log(ra + a1cd*np.exp(a1ce*mag))\n    term2 = a2ca + a2cb * mag\n    term3 = a2cd*np.exp(a2ce*mag)\n    return np.exp((term1 - term2) / a2cc) - term3", "response": "Calculate the median ground motion for a given magnitude and distance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fnc(ra, *args):\n    #\n    # epicentral distance\n    repi = args[0]\n    #\n    # azimuth\n    theta = args[1]\n    #\n    # magnitude\n    mag = args[2]\n    #\n    # coefficients\n    coeff = args[3]\n    #\n    # compute the difference between epicentral distances\n    rb = rbf(ra, coeff, mag)\n    t1 = ra**2 * (np.sin(np.radians(theta)))**2\n    t2 = rb**2 * (np.cos(np.radians(theta)))**2\n    xx = ra * rb / (t1+t2)**0.5\n    return xx-repi", "response": "Function used in the minimisation problem."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ras(repi, theta, mag, coeff):\n    rx = 100.\n    ras = 200.\n    #\n    # calculate the difference between epicentral distances\n    dff = fnc(ras, repi, theta, mag, coeff)\n    while abs(dff) > 1e-3:\n        # update the value of distance computed\n        if dff > 0.:\n            ras = ras - rx\n        else:\n            ras = ras + rx\n        dff = fnc(ras, repi, theta, mag, coeff)\n        rx = rx / 2.\n        if rx < 1e-3:\n            break\n    return ras", "response": "Compute equivalent distance between epicentral and azimuth value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # Check that the requested standard deviation type is available\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n        #\n        # Set parameters\n        magn = rup.mag\n        epi = dists.repi\n        theta = dists.azimuth\n        #\n        # Convert Mw into Ms\n        if magn < 6.58:\n            mag = (magn - 0.59) / 0.86\n        else:\n            mag = (magn + 2.42) / 1.28\n        #\n        # Set coefficients\n        coeff = self.COEFFS[imt]\n        a1ca, a1cb, a1cc, a1cd, a1ce, a2ca, a2cb, a2cc, a2cd, a2ce = \\\n            gc(coeff, mag)\n        #\n        # Get correction coefficients. Here for each site we find the\n        # the geometry of the ellipses\n        ras = []\n        for epi, theta in zip(dists.repi, dists.azimuth):\n            res = get_ras(epi, theta, mag, coeff)\n            ras.append(res)\n        ras = np.array(ras)\n        rbs = rbf(ras, coeff, mag)\n        #\n        # Compute values of ground motion for the two cases. The value of\n        # 225 is hardcoded under the assumption that the hypocentral depth\n        # corresponds to 15 km (i.e. 15**2)\n        mean1 = (a1ca + a1cb * mag +\n                 a1cc * np.log((ras**2+225)**0.5 +\n                               a1cd * np.exp(a1ce * mag)))\n        mean2 = (a2ca + a2cb * mag +\n                 a2cc * np.log((rbs**2+225)**0.5 +\n                               a2cd * np.exp(a2ce * mag)))\n        #\n        # Get distances\n        x = (mean1 * np.sin(np.radians(dists.azimuth)))**2\n        y = (mean2 * np.cos(np.radians(dists.azimuth)))**2\n        mean = mean1 * mean2 / np.sqrt(x+y)\n        if imt.name == \"PGA\":\n            mean = np.exp(mean)/g/100\n        elif imt.name == \"PGV\":\n            mean = np.exp(mean)\n        else:\n            raise ValueError('Unsupported IMT')\n        #\n        # Get the standard deviation\n        stddevs = self._compute_std(coeff, stddev_types, len(dists.repi))\n        #\n        # Return results\n        return np.log(mean), stddevs", "response": "This method calculates the mean and standard deviation of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the standard deviations as defined in equation 4 and 5 page 744.", "response": "def _get_stddevs(self, C, stddev_types, rup, imt, num_sites):\n        \"\"\"\n        Return standard deviations as defined in eq. 4 and 5, page 744,\n        based on table 8, page 744.\n        Eq. 5 yields std dev in natural log, so convert to log10\n        \"\"\"\n        stddevs = []\n        for stddev_type in stddev_types:\n            sigma_mean = self._compute_standard_dev(rup, imt, C)\n            sigma_tot = np.sqrt((sigma_mean ** 2) + (C['SigmaReg'] ** 2))\n            sigma_tot = np.log10(np.exp(sigma_tot))\n            stddevs.append(sigma_tot + np.zeros(num_sites))\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_standard_dev(self, rup, imt, C):\n        sigma_mean = 0.\n        if imt.name in \"SA PGA\":\n            psi = -6.898E-3\n        else:\n            psi = -3.054E-5\n        if rup.mag <= 6.5:\n            sigma_mean = (C['c12'] * rup.mag) + C['c13']\n        elif rup.mag > 6.5:\n            sigma_mean = (psi * rup.mag) + C['c14']\n        return sigma_mean", "response": "Compute the standard deviation in terms of magnitude and the standard deviation in terms of magnitude."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing and return mean value for rock conditions", "response": "def _compute_pga_rock(self, C, dc1, sites, rup, dists):\n        \"\"\"\n        Compute and return mean imt value for rock conditions\n        (vs30 = 1000 m/s)\n        \"\"\"\n        mean = (self._compute_magnitude_term(C, dc1, rup.mag) +\n                self._compute_distance_term(C, rup.mag, dists) +\n                self._compute_focal_depth_term(C, rup) +\n                self._compute_forearc_backarc_term(C, sites, dists))\n        # Apply linear site term\n        site_response = ((C['theta12'] + C['b'] * self.CONSTS['n']) *\n                         np.log(1000. / C['vlin']))\n        return mean + site_response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the magnitude scaling term given by equation ( 2 )", "response": "def _compute_magnitude_term(self, C, dc1, mag):\n        \"\"\"\n        Computes the magnitude scaling term given by equation (2)\n        \"\"\"\n        base = C['theta1'] + (self.CONSTS['theta4'] * dc1)\n        dmag = self.CONSTS[\"C1\"] + dc1\n        if mag > dmag:\n            f_mag = (self.CONSTS['theta5'] * (mag - dmag)) +\\\n                C['theta13'] * ((10. - mag) ** 2.)\n\n        else:\n            f_mag = (self.CONSTS['theta4'] * (mag - dmag)) +\\\n                C['theta13'] * ((10. - mag) ** 2.)\n\n        return base + f_mag"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_site_response_term(self, C, sites, pga1000):\n        vs_star = sites.vs30.copy()\n        vs_star[vs_star > 1000.0] = 1000.\n        arg = vs_star / C[\"vlin\"]\n        site_resp_term = C[\"theta12\"] * np.log(arg)\n        # Get linear scaling term\n        idx = sites.vs30 >= C[\"vlin\"]\n        site_resp_term[idx] += (C[\"b\"] * self.CONSTS[\"n\"] * np.log(arg[idx]))\n        # Get nonlinear scaling term\n        idx = np.logical_not(idx)\n        site_resp_term[idx] += (\n            -C[\"b\"] * np.log(pga1000[idx] + self.CONSTS[\"c\"]) +\n            C[\"b\"] * np.log(pga1000[idx] + self.CONSTS[\"c\"] *\n                            (arg[idx] ** self.CONSTS[\"n\"])))\n        return site_resp_term", "response": "Compute and return site response model term for the current species."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_focal_depth_term(self, C, rup):\n        if rup.hypo_depth > 120.0:\n            z_h = 120.0\n        else:\n            z_h = rup.hypo_depth\n        return C['theta11'] * (z_h - 60.)", "response": "Compute the hypocentral depth scaling term in equation 3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the FABA term given by equation 4.", "response": "def _compute_forearc_backarc_term(self, C, sites, dists):\n        \"\"\"\n        Computes the forearc/backarc scaling term given by equation (4).\n        \"\"\"\n        f_faba = np.zeros_like(dists.rhypo)\n        # Term only applies to backarc sites (F_FABA = 0. for forearc)\n        max_dist = dists.rhypo[sites.backarc]\n        max_dist[max_dist < 85.0] = 85.0\n        f_faba[sites.backarc] = C['theta7'] +\\\n            (C['theta8'] * np.log(max_dist / 40.0))\n        return f_faba"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match(m_templ, *m_args):\n    # strip commented lines\n    m_templ = '\\n'.join(line for line in m_templ.splitlines()\n                        if not line.lstrip().startswith('--'))\n    if not m_args:\n        return m_templ, ()\n    try:\n        return _Replacer(m_args).match(m_templ)\n    except IndexError:\n        raise ValueError('Incorrect number of ?-parameters in %s, expected %s'\n                         % (m_templ, len(m_args)))", "response": "Match a meta template string with the given list of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninserting several rows with executemany. Return a cursor.", "response": "def insert(self, table, columns, rows):\n        \"\"\"\n        Insert several rows with executemany. Return a cursor.\n        \"\"\"\n        cursor = self.conn.cursor()\n        if len(rows):\n            templ, _args = match('INSERT INTO ?s (?S) VALUES (?X)',\n                                 table, columns, rows[0])\n            cursor.executemany(templ, rows)\n        return cursor"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cluster(param, tom, imtls, gsims, grp_ids, pmap):\n    pmapclu = AccumDict({grp_id: ProbabilityMap(len(imtls.array), len(gsims))\n                         for grp_id in grp_ids})\n    # Get temporal occurrence model\n    # Number of occurrences for the cluster\n    first = True\n    for nocc in range(0, 50):\n        # TODO fix this once the occurrence rate will be used just as\n        # an object attribute\n        ocr = tom.occurrence_rate\n        prob_n_occ = tom.get_probability_n_occurrences(ocr, nocc)\n        if first:\n            pmapclu = prob_n_occ * (~pmap)**nocc\n            first = False\n        else:\n            pmapclu += prob_n_occ * (~pmap)**nocc\n    pmap = ~pmapclu\n    return pmap", "response": "Compute the probability map in case of a cluster group."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the hazard curves for a set of sources belonging to the same tectonic region type for all the GSIMs associated to that TRT. The arguments are the same as in :func:`calc_hazard_curves`, except for ``gsims``, which is a list of GSIM instances. :returns: a dictionary {grp_id: pmap} with attributes .grp_ids, .calc_times, .eff_ruptures", "response": "def classical(group, src_filter, gsims, param, monitor=Monitor()):\n    \"\"\"\n    Compute the hazard curves for a set of sources belonging to the same\n    tectonic region type for all the GSIMs associated to that TRT.\n    The arguments are the same as in :func:`calc_hazard_curves`, except\n    for ``gsims``, which is a list of GSIM instances.\n\n    :returns:\n        a dictionary {grp_id: pmap} with attributes .grp_ids, .calc_times,\n        .eff_ruptures\n    \"\"\"\n    if not hasattr(src_filter, 'sitecol'):  # a sitecol was passed\n        src_filter = SourceFilter(src_filter, {})\n\n    # Get the parameters assigned to the group\n    src_mutex = getattr(group, 'src_interdep', None) == 'mutex'\n    rup_mutex = getattr(group, 'rup_interdep', None) == 'mutex'\n    cluster = getattr(group, 'cluster', None)\n    # Compute the number of ruptures\n    grp_ids = set()\n    for src in group:\n        if not src.num_ruptures:\n            # src.num_ruptures is set when parsing the XML, but not when\n            # the source is instantiated manually, so it is set here\n            src.num_ruptures = src.count_ruptures()\n        # This sets the proper TOM in case of a cluster\n        if cluster:\n            src.temporal_occurrence_model = FatedTOM(time_span=1)\n        # Updating IDs\n        grp_ids.update(src.src_group_ids)\n    # Now preparing context\n    maxdist = src_filter.integration_distance\n    imtls = param['imtls']\n    trunclevel = param.get('truncation_level')\n    cmaker = ContextMaker(\n        src.tectonic_region_type, gsims, maxdist, param, monitor)\n    # Prepare the accumulator for the probability maps\n    pmap = AccumDict({grp_id: ProbabilityMap(len(imtls.array), len(gsims))\n                      for grp_id in grp_ids})\n    rupdata = {grp_id: [] for grp_id in grp_ids}\n    # AccumDict of arrays with 3 elements weight, nsites, calc_time\n    calc_times = AccumDict(accum=numpy.zeros(3, numpy.float32))\n    eff_ruptures = AccumDict(accum=0)  # grp_id -> num_ruptures\n    # Computing hazard\n    for src, s_sites in src_filter(group):  # filter now\n        t0 = time.time()\n        try:\n            poemap = cmaker.poe_map(src, s_sites, imtls, trunclevel,\n                                    rup_indep=not rup_mutex)\n        except Exception as err:\n            etype, err, tb = sys.exc_info()\n            msg = '%s (source id=%s)' % (str(err), src.source_id)\n            raise etype(msg).with_traceback(tb)\n        if src_mutex:  # mutex sources, there is a single group\n            for sid in poemap:\n                pcurve = pmap[src.src_group_id].setdefault(sid, 0)\n                pcurve += poemap[sid] * src.mutex_weight\n        elif poemap:\n            for gid in src.src_group_ids:\n                pmap[gid] |= poemap\n        if len(cmaker.rupdata):\n            for gid in src.src_group_ids:\n                rupdata[gid].append(cmaker.rupdata)\n        calc_times[src.id] += numpy.array(\n            [src.weight, len(s_sites), time.time() - t0])\n        # storing the number of contributing ruptures too\n        eff_ruptures += {gid: getattr(poemap, 'eff_ruptures', 0)\n                         for gid in src.src_group_ids}\n    # Updating the probability map in the case of mutually exclusive\n    # sources\n    group_probability = getattr(group, 'grp_probability', None)\n    if src_mutex and group_probability:\n        pmap[src.src_group_id] *= group_probability\n    # Processing cluster\n    if cluster:\n        tom = getattr(group, 'temporal_occurrence_model')\n        pmap = _cluster(param, tom, imtls, gsims, grp_ids, pmap)\n    # Return results\n    for gid, data in rupdata.items():\n        if len(data):\n            rupdata[gid] = numpy.concatenate(data)\n    return dict(pmap=pmap, calc_times=calc_times, eff_ruptures=eff_ruptures,\n                rup_data=rupdata)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the hazard curves for a set of seismic source objects.", "response": "def calc_hazard_curves(\n        groups, ss_filter, imtls, gsim_by_trt, truncation_level=None,\n        apply=sequential_apply, filter_distance='rjb', reqv=None):\n    \"\"\"\n    Compute hazard curves on a list of sites, given a set of seismic source\n    groups and a dictionary of ground shaking intensity models (one per\n    tectonic region type).\n\n    Probability of ground motion exceedance is computed in different ways\n    depending if the sources are independent or mutually exclusive.\n\n    :param groups:\n        A sequence of groups of seismic sources objects (instances of\n        of :class:`~openquake.hazardlib.source.base.BaseSeismicSource`).\n    :param ss_filter:\n        A source filter over the site collection or the site collection itself\n    :param imtls:\n        Dictionary mapping intensity measure type strings\n        to lists of intensity measure levels.\n    :param gsim_by_trt:\n        Dictionary mapping tectonic region types (members\n        of :class:`openquake.hazardlib.const.TRT`) to\n        :class:`~openquake.hazardlib.gsim.base.GMPE` or\n        :class:`~openquake.hazardlib.gsim.base.IPE` objects.\n    :param truncation_level:\n        Float, number of standard deviations for truncation of the intensity\n        distribution.\n    :param apply:\n        apply function to use (default sequential_apply)\n    :param filter_distance:\n        The distance used to filter the ruptures (default rjb)\n    :param reqv:\n        If not None, an instance of RjbEquivalent\n    :returns:\n        An array of size N, where N is the number of sites, which elements\n        are records with fields given by the intensity measure types; the\n        size of each field is given by the number of levels in ``imtls``.\n    \"\"\"\n    # This is ensuring backward compatibility i.e. processing a list of\n    # sources\n    if not isinstance(groups[0], SourceGroup):  # sent a list of sources\n        odic = groupby(groups, operator.attrgetter('tectonic_region_type'))\n        groups = [SourceGroup(trt, odic[trt], 'src_group', 'indep', 'indep')\n                  for trt in odic]\n    # ensure the sources have the right src_group_id\n    for i, grp in enumerate(groups):\n        for src in grp:\n            if src.src_group_id is None:\n                src.src_group_id = i\n    imtls = DictArray(imtls)\n    param = dict(imtls=imtls, truncation_level=truncation_level,\n                 filter_distance=filter_distance, reqv=reqv,\n                 cluster=grp.cluster)\n    pmap = ProbabilityMap(len(imtls.array), 1)\n    # Processing groups with homogeneous tectonic region\n    gsim = gsim_by_trt[groups[0][0].tectonic_region_type]\n    mon = Monitor()\n    for group in groups:\n        if group.atomic:  # do not split\n            it = [classical(group, ss_filter, [gsim], param, mon)]\n        else:  # split the group and apply `classical` in parallel\n            it = apply(\n                classical, (group.sources, ss_filter, [gsim], param, mon),\n                weight=operator.attrgetter('weight'))\n        for dic in it:\n            for grp_id, pval in dic['pmap'].items():\n                pmap |= pval\n    sitecol = getattr(ss_filter, 'sitecol', ss_filter)\n    return pmap.convert(imtls, len(sitecol.complete))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # compute median PGA on rock, needed to compute non-linear site\n        # amplification\n        C_pga = self.COEFFS[PGA()]\n        median_pga = np.exp(\n            self._compute_mean(C_pga, rup.mag, dists, rup.rake)\n        )\n\n        # compute full mean value by adding nonlinear site amplification terms\n        C = self.COEFFS[imt]\n        mean = (self._compute_mean(C, rup.mag, dists, rup.rake) +\n                self._compute_non_linear_term(C, median_pga, sites))\n\n        stddevs = self._get_stddevs(C, stddev_types, num_sites=sites.vs30.size)\n\n        return mean + self.adjustment_factor, stddevs", "response": "Compute and return the mean value and standard deviation of the site as described in equation 1 page 20."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_faulting_style_term(self, C, rake):\n        Fn = float(rake > -135.0 and rake < -45.0)\n        Fr = float(rake > 45.0 and rake < 135.0)\n\n        return C['a8'] * Fn + C['a9'] * Fr", "response": "Compute faulting style term in equations 2a and 2b pages 20."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_non_linear_term(self, C, pga_only, sites):\n        Vref = 750.0\n        Vcon = 1000.0\n        lnS = np.zeros_like(sites.vs30)\n\n        # equation (3a)\n        idx = sites.vs30 < Vref\n        lnS[idx] = (\n            C['b1'] * np.log(sites.vs30[idx] / Vref) +\n            C['b2'] * np.log(\n                (pga_only[idx] + C['c'] * (sites.vs30[idx] / Vref) ** C['n']) /\n                ((pga_only[idx] + C['c']) * (sites.vs30[idx] / Vref) ** C['n'])\n            )\n        )\n\n        # equation (3b)\n        idx = (sites.vs30 >= Vref) & (sites.vs30 <= Vcon)\n        lnS[idx] = C['b1'] * np.log(sites.vs30[idx]/Vref)\n\n        # equation (3c)\n        idx = sites.vs30 > Vcon\n        lnS[idx] = C['b1'] * np.log(Vcon/Vref)\n\n        return lnS", "response": "Compute non - linear term in equation 3a to page 20."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_mean(self, C, mag, dists, rake):\n        mean = (\n            C['a1'] +\n            self._compute_linear_magnitude_term(C, mag) +\n            self._compute_quadratic_magnitude_term(C, mag) +\n            self._compute_logarithmic_distance_term(C, mag, dists) +\n            self._compute_faulting_style_term(C, rake)\n        )\n\n        return mean", "response": "Compute and return mean value without site conditions"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute and return the logarithmic distance term in equations ( 2a ) and ( 2b ) and ( 2c )", "response": "def _compute_logarithmic_distance_term(self, C, mag, dists):\n        \"\"\"\n        Compute and return fourth term in equations (2a)\n        and (2b), page 20.\n        \"\"\"\n        return (\n            (C['a4'] + C['a5'] * (mag - self.c1)) *\n            np.log(np.sqrt(dists.rhypo ** 2 + C['a6'] ** 2))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the standard deviations as defined in p. 971.", "response": "def _get_stddevs(self, C, rup, shape, stddev_types):\r\n        \"\"\"\r\n        Return standard deviations as defined in p. 971.\r\n        \"\"\"\r\n        weight = self._compute_weight_std(C, rup.mag)\r\n        std_intra = weight * C[\"sd1\"] * np.ones(shape)\r\n        std_inter = weight * C[\"sd2\"] * np.ones(shape)\r\n        \r\n        stddevs = []\r\n        for stddev_type in stddev_types:\r\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\r\n            if stddev_type == const.StdDev.TOTAL:\r\n                stddevs.append(np.sqrt(std_intra ** 2. + std_inter ** 2.))\r\n            elif stddev_type == const.StdDev.INTRA_EVENT:\r\n                stddevs.append(std_intra)\r\n            elif stddev_type == const.StdDev.INTER_EVENT:\r\n                stddevs.append(std_inter)\r\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the weight of the standard deviation of the tag in the specified mag.", "response": "def _compute_weight_std(self, C, mag):\r\n        \"\"\"\r\n        Common part of equations 8 and 9, page 971.\r\n        \"\"\"\r\n        if mag < 6.0:\r\n            return C['a1']\r\n        elif mag >= 6.0 and mag < 6.5:\r\n            return C['a1'] + (C['a2'] - C['a1']) * ((mag - 6.0) / 0.5)\r\n        else:\r\n            return C['a2']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_magnitude_scaling_term(self, C, mag):\r\n        c1 = self.CONSTS['c1']\r\n        if mag <= c1:\r\n            return C['b1'] + C['b2'] * (mag - c1) + C['b3'] * (8.5 - mag) ** 2\r\n        else:\r\n            return C['b1'] + C['b7'] * (mag - c1) + C['b3'] * (8.5 - mag) ** 2", "response": "Returns the magnitude scaling term in equation 2 page 970."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute and return geometric decay term in equation 3 page 970.", "response": "def _compute_geometric_decay_term(self, C, mag, dists):\r\n        \"\"\"\r\n        Compute and return geometric decay term in equation 3,\r\n        page 970.\r\n        \"\"\"\r\n        c1 = self.CONSTS['c1']\r\n        return (\r\n            (C['b4'] + C['b5'] * (mag - c1)) *\r\n            np.log(np.sqrt(dists.rjb ** 2.0 + C['b6'] ** 2.0))\r\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_anelestic_attenuation_term(self, C, dists):\r\n        f_aat = np.zeros_like(dists.rjb)\r\n        idx = dists.rjb > 80.0\r\n        f_aat[idx] = C[\"b10\"] * (dists.rjb[idx] - 80.0)\r\n        return f_aat", "response": "Compute and return anelastic attenuation term in equation 5 page 970."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the non - linear term in the site table.", "response": "def _compute_non_linear_term(self, C, pga_only, sites):\r\n        \"\"\"\r\n        Compute non-linear term, equation 6, page 970.\r\n        \"\"\"\r\n        Vref = self.CONSTS['Vref']\r\n        Vcon = self.CONSTS['Vcon']\r\n        c = self.CONSTS['c']\r\n        n = self.CONSTS['n']\r\n        lnS = np.zeros_like(sites.vs30)\r\n\r\n        # equation (6a)\r\n        idx = sites.vs30 < Vref\r\n        lnS[idx] = (\r\n            C['sb1'] * np.log(sites.vs30[idx] / Vref) +\r\n            C['sb2'] * np.log(\r\n                (pga_only[idx] + c * (sites.vs30[idx] / Vref) ** n) /\r\n                ((pga_only[idx] + c) * (sites.vs30[idx] / Vref) ** n)\r\n            )\r\n        )\r\n\r\n        # equation (6b)\r\n        idx = sites.vs30 >= Vref\r\n        new_sites = sites.vs30[idx]\r\n        new_sites[new_sites > Vcon] = Vcon\r\n        lnS[idx] = C['sb1'] * np.log(new_sites / Vref)\r\n\r\n        return lnS"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes and return mean value with site conditions", "response": "def _compute_mean(self, C, mag, dists, rake):\r\n        \"\"\"\r\n        Compute and return mean value without site conditions,\r\n        that is equations 2-5, page 970.\r\n        \"\"\"\r\n        mean = (\r\n            self._compute_magnitude_scaling_term(C, mag) +\r\n            self._compute_geometric_decay_term(C, mag, dists) +\r\n            self._compute_faulting_style_term(C, rake) +\r\n            self._compute_anelestic_attenuation_term(C, dists)\r\n        )\r\n\r\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the total number of rupture in the Seismic source.", "response": "def count_ruptures(self):\n        \"\"\"\n        See\n        :meth:`openquake.hazardlib.source.base.BaseSeismicSource.count_ruptures`\n        for description of parameters and return value.\n        \"\"\"\n        return (len(self.get_annual_occurrence_rates()) *\n                len(self.nodal_plane_distribution.data) *\n                len(self.hypocenter_distribution.data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bounding_box(self, maxdist):\n        return utils.get_bounding_box([ps.location for ps in self], maxdist)", "response": "Returns a bounding box containing all the point sources enlarged by the maxdist."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef geom(self):\n        return numpy.array([(p.x, p.y, p.z) for p in self.mesh],\n                           numpy.float32)", "response": "returns the geometry as an array of shape N 3"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n\n        # Extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n\n        imean = (self._compute_magnitude(rup, C) +\n                 self._compute_attenuation(rup, dists, imt, C) +\n                 self._compute_distance(rup, dists, imt, C))\n\n        mean = np.log(10.0 ** (imean))\n\n        istddevs = self._get_stddevs(C, stddev_types, rup, imt,\n                                     num_sites=len(dists.rrup))\n\n        stddevs = np.log(10.0 ** np.array(istddevs))\n\n        return mean, stddevs", "response": "This method computes the mean and standard deviation of the resource table entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the second term of the equation described on p. 1866", "response": "def _compute_attenuation(self, rup, dists, imt, C):\n        \"\"\"\n        Compute the second term of the equation described on p. 1866:\n\n        \" [(c4 + c5 * M) * min{ log10(R), log10(70.) }] +\n        [(c4 + c5 * M) * max{ min{ log10(R/70.), log10(140./70.) }, 0.}] +\n        [(c8 + c9 * M) * max{ log10(R/140.), 0}] \"\n        \"\"\"\n\n        vec = np.ones(len(dists.rrup))\n\n        a1 = (np.log10(np.sqrt(dists.rrup ** 2.0 + C['c11'] ** 2.0)),\n              np.log10(70. * vec))\n\n        a = np.column_stack([a1[0], a1[1]])\n\n        b3 = (np.log10(np.sqrt(dists.rrup ** 2.0 + C['c11'] ** 2.0) /\n                      (70. * vec)),\n              np.log10((140. / 70.) * vec))\n\n        b2 = np.column_stack([b3[0], b3[1]])\n        b1 = ([np.min(b2, axis=1), 0. * vec])\n        b = np.column_stack([b1[0], b1[1]])\n\n        c1 = (np.log10(np.sqrt(dists.rrup ** 2.0 + C['c11'] ** 2.0) /\n              (140.) * vec), 0. * vec)\n        c = np.column_stack([c1[0], c1[1]])\n\n        return (((C['c4'] + C['c5'] * rup.mag) * np.min(a, axis=1)) +\n                ((C['c6'] + C['c7'] * rup.mag) * np.max(b, axis=1)) +\n                ((C['c8'] + C['c9'] * rup.mag) * np.max(c, axis=1)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_standard_dev(self, rup, imt, C):\n        sigma_mean = 0.\n        if rup.mag <= 7.0:\n            sigma_mean = (C['c12'] * rup.mag) + C['c13']\n        elif rup.mag > 7.0:\n            sigma_mean = (-0.00695 * rup.mag) + C['c14']\n        return sigma_mean", "response": "Compute the standard deviation in terms of magnitude and the standard deviation in terms of magnitude."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the base rate of the is", "response": "def _get_base_rates(self, base_params):\n        '''\n        Defines the base moment rate that should be assigned to places of\n        zero strain (i.e. Intraplate regions). In Bird et al (2010) this is\n        taken as basic rate of Intraplate events in GCMT catalogue above the\n        threshold magnitude\n\n        :param dict base_params:\n            Parameters needed for calculating the base rate. Requires:\n                'CMT_EVENTS': The number of CMT events\n                'area': Total area (km ^ 2) of the region class\n                'CMT_duration': Duration of reference catalogue\n                'CMT_moment': Moment rate from CMT catalogue\n                'corner_mag': Corner magnitude of Tapered G-R for region\n                'beta': Beta value of tapered G-R for distribution\n        '''\n        base_ipl_rate = base_params['CMT_EVENTS'] / (\n            base_params['area'] * base_params['CMT_duration'])\n        base_rate = np.zeros(self.number_magnitudes, dtype=float)\n\n        for iloc in range(0, self.number_magnitudes):\n            base_rate[iloc] = base_ipl_rate * calculate_taper_function(\n                base_params['CMT_moment'],\n                self.threshold_moment[iloc],\n                moment_function(base_params['corner_mag']),\n                base_params['beta'])\n        return base_rate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the seismicity rate for the hybrid ocean - based ridge condition with the normal and transform condition.", "response": "def get_rate_osr_normal_transform(self, threshold_moment, id0):\n        '''\n        Gets seismicity rate for special case of the ridge condition with\n        spreading and transform component\n\n        :param float threshold_moment:\n            Moment required for calculating activity rate\n\n        :param np.ndarray id0:\n            Logical vector indicating the cells to which this condition applies\n\n        :returns:\n            Activity rates for cells corresponding to the hybrid ocean\n            spreading ridge and oceanic transform condition\n\n        '''\n        # Get normal component\n        e1h_ridge = np.zeros(np.sum(id0), dtype=float)\n        e2h_ridge = self.strain.data['e1h'][id0] + self.strain.data['e2h'][id0]\n        err_ridge = -(e1h_ridge + e2h_ridge)\n\n        calculated_rate_ridge = self.continuum_seismicity(\n            threshold_moment,\n            e1h_ridge,\n            e2h_ridge,\n            err_ridge,\n            self.regionalisation['OSRnor'])\n\n        # Get transform\n        e1h_trans = self.strain.data['e1h'][id0]\n        e2h_trans = -e1h_trans\n        err_trans = np.zeros(np.sum(id0), dtype=float)\n\n        calculated_rate_transform = self.continuum_seismicity(\n            threshold_moment,\n            e1h_trans,\n            e2h_trans,\n            err_trans,\n            self.regionalisation['OTFmed'])\n\n        return (\n            self.regionalisation['OSRnor']['adjustment_factor'] *\n            (calculated_rate_ridge + calculated_rate_transform))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the seismicity rate for the hybrid ocean condition and the transformation condition.", "response": "def get_rate_osr_convergent_transform(self, threshold_moment, id0):\n        '''\n        Calculates seismicity rate for special case of the ridge condition\n        with convergence and transform\n\n        :param float threshold_moment:\n            Moment required for calculating activity rate\n\n        :param np.ndarray id0:\n            Logical vector indicating the cells to which this condition applies\n\n        :returns:\n            Activity rates for cells corresponding to the hybrid ocean\n            convergent boundary and oceanic transform condition\n        '''\n        # Get convergent component\n        e1h_ocb = self.strain.data['e1h'][id0] + self.strain.data['e2h'][id0]\n        e2h_ocb = np.zeros(np.sum(id0), dtype=float)\n        err_ocb = -(e1h_ocb + e2h_ocb)\n\n        calculated_rate_ocb = self.continuum_seismicity(\n            threshold_moment,\n            e1h_ocb,\n            e2h_ocb,\n            err_ocb,\n            self.regionalisation['OCB'])\n\n        # Get transform\n        e2h_trans = self.strain.data['e2h'][id0]\n        e1h_trans = -e2h_trans\n        err_trans = np.zeros(np.sum(id0), dtype=float)\n\n        calculated_rate_transform = self.continuum_seismicity(\n            threshold_moment,\n            e1h_trans,\n            e2h_trans,\n            err_trans,\n            self.regionalisation['OTFmed'])\n\n        return (self.regionalisation['OSRnor']['adjustment_factor'] *\n                (calculated_rate_ocb + calculated_rate_transform))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef continuum_seismicity(self, threshold_moment, e1h, e2h, err,\n                             region_params):\n        '''\n        Function to implement the continuum seismicity calculation given\n        vectors of input rates e1h, e2h [np.ndarray] and a dictionary of\n        the corresponding regionalisation params\n        returns a vector of the corresponding seismicity rates\n        Python implementation of the CONTINUUM_SEISMICITY subroutine of\n        SHIFT_GSRM.f90\n\n        :param float threshold_moment:\n            Target moment for calculation of activity rate\n\n        :param np.ndarray e1h:\n            First principal strain rate\n\n        :param np.ndarray e1h:\n            Second principal strain rate\n\n        :param np.ndarray err:\n            Vertical strain rate\n\n        :param dict region_params:\n            Activity rate parameters specific to the tectonic region under\n            consideration\n\n        :returns:\n            Cumulative seismicity rate greater than or equal to the\n            threshold magnitude\n        '''\n\n        strain_values = np.column_stack([e1h, e2h, err])\n        e1_rate = np.amin(strain_values, axis=1)\n        e3_rate = np.amax(strain_values, axis=1)\n        e2_rate = 0. - e1_rate - e3_rate\n        # Pre-allocate seismicity rate with zeros\n        seismicity_rate = np.zeros(\n            [np.shape(strain_values)[0], len(threshold_moment)],\n            dtype=float)\n        # Calculate moment rate per unit area\n        temp_e_rate = 2.0 * (-e1_rate)\n        id0 = np.where(e2_rate < 0.0)[0]\n        temp_e_rate[id0] = 2.0 * e3_rate[id0]\n        M_persec_per_m2 = (\n            region_params['assumed_mu'] * temp_e_rate *\n            region_params['coupled_thickness'])\n\n        # Calculate seismicity rate at the threshold moment of the CMT\n        # catalogue - Eq 6 in Bird et al (2010)\n        seismicity_at_cmt_threshold = region_params['CMT_pure_event_rate'] * \\\n            (M_persec_per_m2 / region_params['tGR_moment_rate'])\n        # Adjust forecast rate to desired rate using tapered G-R model\n        # Taken from Eq 7 (Bird et al. 2010) and Eq 9 (Bird & Kagan, 2004)\n        for iloc, moment_thresh in enumerate(threshold_moment):\n            g_function = calculate_taper_function(\n                region_params['CMT_moment'],\n                moment_thresh,\n                region_params['corner_moment'],\n                region_params['beta'])\n            seismicity_rate[:, iloc] = g_function * seismicity_at_cmt_threshold\n        return seismicity_rate", "response": "This function implements the continuum seismicity calculation given the input rates e1h e2h and an error and a dictionary of region_params. The function returns the seismicity rate greater than or equal to the threshold magnitude."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reclassify_Bird_regions_with_data(self):\n        '''\n        The SHIFT regionalisation defines only 'C','R','S','O' - need to\n        use strain data to reclassify to sub-categories according to the\n        definition in Bird & Liu (2007)\n        '''\n        # Treat trivial cases of subduction zones and oceanic types\n        self.strain.data['region'][\n            self.strain.data['region'] == b'IPL'] = ['IPL']\n        self.strain.data['region'][\n            self.strain.data['region'] == b'S'] = ['SUB']\n        self.strain.data['region'][\n            self.strain.data['region'] == b'O'] = ['OCB']\n\n        # Continental types\n        id0 = self.strain.data['region'] == b'C'\n        self.strain.data['region'][id0] = ['CTF']\n        id0_pos_err = np.logical_and(\n            self.strain.data['err'] > 0.,\n            self.strain.data['err'] > (0.364 * self.strain.data['e2h']))\n\n        id0_neg_err = np.logical_and(\n            self.strain.data['err'] < 0.,\n            self.strain.data['err'] <= (0.364 * self.strain.data['e1h']))\n\n        self.strain.data['region'][np.logical_and(id0, id0_pos_err)] = 'CCB'\n        self.strain.data['region'][np.logical_and(id0, id0_neg_err)] = 'CRB'\n\n        # Ridge Types\n        id0 = self.strain.data['region'] == b'R'\n        for iloc in np.where(id0)[0]:\n            cond = (self.strain.data['e1h'][iloc] > 0.0 and\n                    self.strain.data['e2h'][iloc] > 0.0)\n            if cond:\n                self.strain.data['region'][iloc] = 'OSRnor'\n            # Effective == 0.0\n            elif fabs(self.strain.data['e1h'][iloc]) < 1E-99:\n                self.strain.data['region'][iloc] = 'OSRnor'\n            elif ((self.strain.data['e1h'][iloc] *\n                   self.strain.data['e2h'][iloc]) < 0.0) and\\\n                 ((self.strain.data['e1h'][iloc] +\n                   self.strain.data['e2h'][iloc]) >= 0.):\n                self.strain.data['region'][iloc] = 'OSR_special_1'\n            elif ((self.strain.data['e1h'][iloc] *\n                   self.strain.data['e2h'][iloc]) < 0.) and\\\n                 ((self.strain.data['e1h'][iloc] +\n                   self.strain.data['e2h'][iloc]) < 0.):\n                self.strain.data['region'][iloc] = 'OSR_special_2'\n            else:\n                self.strain.data['region'][iloc] = 'OCB'", "response": "Reclassify the Bird regions with the data in the current SEQ and the data in the current SEQ."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the log - likelihood of a single object in the format of a log - likelihood and a second - likelihood of a single object in the format of a single object.", "response": "def _get_ln_y_ref(self, rup, dists, C):\n        \"\"\"\n        Get an intensity on a reference soil.\n\n        Implements eq. 13a.\n        \"\"\"\n        # reverse faulting flag\n        Frv = 1 if 30 <= rup.rake <= 150 else 0\n        # normal faulting flag\n        Fnm = 1 if -120 <= rup.rake <= -60 else 0\n        # hanging wall flag\n        Fhw = (dists.rx >= 0)\n        # aftershock flag. always zero since we only consider main shock\n        AS = 0\n\n        ln_y_ref = (\n            # first line of eq. 13a\n            C['c1']\n            + (C['c1a'] * Frv\n               + C['c1b'] * Fnm\n               + C['c7'] * (rup.ztor - 4))\n            * (1 - AS)\n            + (C['c10'] + C['c7a'] * (rup.ztor - 4)) * AS\n            # second line\n            + C['c2'] * (rup.mag - 6)\n            + ((C['c2'] - C['c3']) / C['cn'])\n            * np.log(1 + np.exp(C['cn'] * (C['cm'] - rup.mag)))\n            # third line\n            + C['c4']\n            * np.log(dists.rrup\n                     + C['c5']\n                     * np.cosh(C['c6'] * max(rup.mag - C['chm'], 0)))\n            # fourth line\n            + (C['c4a'] - C['c4'])\n            * np.log(np.sqrt(dists.rrup ** 2 + C['crb'] ** 2))\n            # fifth line\n            + (C['cg1'] + C['cg2'] / (np.cosh(max(rup.mag - C['cg3'], 0))))\n            * dists.rrup\n            # sixth line\n            + C['c9'] * Fhw\n            * np.tanh(dists.rx\n                      * (np.cos(np.radians(rup.dip)) ** 2)\n                      / C['c9a'])\n            * (1 - np.sqrt(dists.rjb ** 2 + rup.ztor ** 2)\n               / (dists.rrup + 0.001))\n        )\n        return ln_y_ref"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the median fault area from the magnitude and rake.", "response": "def get_median_area(self, mag, rake):\n        \"\"\"\n        Calculates median fault area from magnitude.\n        \"\"\"\n        if rake is None:\n            # Return average of strike-slip and dip-slip curves\n            return power(10.0, (mag - 4.185))\n        elif (-45 <= rake <= 45) or (rake >= 135) or (rake <= -135):\n            # strike-slip\n            return power(10.0, (mag - 4.18))\n        else:\n            # Dip-slip (thrust or normal), and undefined rake\n            return power(10.0, (mag - 4.19))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        C = self.COEFFS[imt]\n\n        # clip rhypo at 10 (this is the minimum distance used in\n        # deriving the equation), see page 22, this avoids singularity\n        # in mean value equation\n        rhypo = dists.rhypo.copy()\n        rhypo[rhypo < 10] = 10\n\n        # convert magnitude from Mblg to Mw\n        mag = rup.mag * 0.98 - 0.39 if rup.mag <= 5.5 else \\\n              2.715 - 0.277 * rup.mag + 0.127 * rup.mag * rup.mag\n\n        # functional form as explained in 'Youngs_fit_to_AB95lookup.doc'\n        f1 = np.minimum(np.log(rhypo), np.log(70.))\n        f2 = np.maximum(np.log(rhypo / 130.), 0)\n        mean = (\n            C['c1'] + C['c2'] * mag + C['c3'] * mag ** 2 +\n            (C['c4'] + C['c5'] * mag) * f1 +\n            (C['c6'] + C['c7'] * mag) * f2 +\n            C['c8'] * rhypo\n        )\n\n        stddevs = self._get_stddevs(stddev_types,  dists.rhypo.shape[0])\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation as described in table 2 page 22."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_base_url(request):\n    if request.is_secure():\n        base_url = 'https://%s'\n    else:\n        base_url = 'http://%s'\n    base_url %= request.META['HTTP_HOST']\n    return base_url", "response": "Construct a base URL given a request object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prepare_job(request, candidates):\n    temp_dir = tempfile.mkdtemp()\n    inifiles = []\n    arch = request.FILES.get('archive')\n    if arch is None:\n        # move each file to a new temp dir, using the upload file names,\n        # not the temporary ones\n        for each_file in request.FILES.values():\n            new_path = os.path.join(temp_dir, each_file.name)\n            shutil.move(each_file.temporary_file_path(), new_path)\n            if each_file.name in candidates:\n                inifiles.append(new_path)\n        return inifiles\n    # else extract the files from the archive into temp_dir\n    return readinput.extract_from_zip(arch, candidates)", "response": "Prepares the job file for the given candidates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the login request.", "response": "def ajax_login(request):\n    \"\"\"\n    Accept a POST request to login.\n\n    :param request:\n        `django.http.HttpRequest` object, containing mandatory parameters\n        username and password required.\n    \"\"\"\n    username = request.POST['username']\n    password = request.POST['password']\n    user = authenticate(username=username, password=password)\n    if user is not None:\n        if user.is_active:\n            login(request, user)\n            return HttpResponse(content='Successful login',\n                                content_type='text/plain', status=200)\n        else:\n            return HttpResponse(content='Disabled account',\n                                content_type='text/plain', status=403)\n    else:\n        return HttpResponse(content='Invalid login',\n                            content_type='text/plain', status=403)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of strings with the available GSIMs", "response": "def get_available_gsims(request):\n    \"\"\"\n    Return a list of strings with the available GSIMs\n    \"\"\"\n    gsims = list(gsim.get_available_gsims())\n    return HttpResponse(content=json.dumps(gsims), content_type=JSON)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nleverages oq - risklib to check if a given XML text is a valid NRML object.", "response": "def validate_nrml(request):\n    \"\"\"\n    Leverage oq-risklib to check if a given XML text is a valid NRML\n\n    :param request:\n        a `django.http.HttpRequest` object containing the mandatory\n        parameter 'xml_text': the text of the XML to be validated as NRML\n\n    :returns: a JSON object, containing:\n        * 'valid': a boolean indicating if the provided text is a valid NRML\n        * 'error_msg': the error message, if any error was found\n                       (None otherwise)\n        * 'error_line': line of the given XML where the error was found\n                        (None if no error was found or if it was not a\n                        validation error)\n    \"\"\"\n    xml_text = request.POST.get('xml_text')\n    if not xml_text:\n        return HttpResponseBadRequest(\n            'Please provide the \"xml_text\" parameter')\n    xml_file = gettemp(xml_text, suffix='.xml')\n    try:\n        nrml.to_python(xml_file)\n    except ExpatError as exc:\n        return _make_response(error_msg=str(exc),\n                              error_line=exc.lineno,\n                              valid=False)\n    except Exception as exc:\n        # get the exception message\n        exc_msg = exc.args[0]\n        if isinstance(exc_msg, bytes):\n            exc_msg = exc_msg.decode('utf-8')   # make it a unicode object\n        elif isinstance(exc_msg, str):\n            pass\n        else:\n            # if it is another kind of object, it is not obvious a priori how\n            # to extract the error line from it\n            return _make_response(\n                error_msg=str(exc_msg), error_line=None, valid=False)\n        # if the line is not mentioned, the whole message is taken\n        error_msg = exc_msg.split(', line')[0]\n        # check if the exc_msg contains a line number indication\n        search_match = re.search(r'line \\d+', exc_msg)\n        if search_match:\n            error_line = int(search_match.group(0).split()[1])\n        else:\n            error_line = None\n        return _make_response(\n            error_msg=error_msg, error_line=error_line, valid=False)\n    else:\n        return _make_response(error_msg=None, error_line=None, valid=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a JSON blob containing all of the parameters for the given calculation.", "response": "def calc(request, calc_id):\n    \"\"\"\n    Get a JSON blob containing all of parameters for the given calculation\n    (specified by ``calc_id``). Also includes the current job status (\n    executing, complete, etc.).\n    \"\"\"\n    try:\n        info = logs.dbcmd('calc_info', calc_id)\n        if not utils.user_has_permission(request, info['user_name']):\n            return HttpResponseForbidden()\n    except dbapi.NotFound:\n        return HttpResponseNotFound()\n    return HttpResponse(content=json.dumps(info), content_type=JSON)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of calculations and report their id status calculation_mode is_running description and url where more detailed information can be accessed.", "response": "def calc_list(request, id=None):\n    # view associated to the endpoints /v1/calc/list and /v1/calc/:id/status\n    \"\"\"\n    Get a list of calculations and report their id, status, calculation_mode,\n    is_running, description, and a url where more detailed information\n    can be accessed. This is called several times by the Javascript.\n\n    Responses are in JSON.\n    \"\"\"\n    base_url = _get_base_url(request)\n    calc_data = logs.dbcmd('get_calcs', request.GET,\n                           utils.get_valid_users(request),\n                           utils.get_acl_on(request), id)\n\n    response_data = []\n    username = psutil.Process(os.getpid()).username()\n    for (hc_id, owner, status, calculation_mode, is_running, desc, pid,\n         parent_id, size_mb) in calc_data:\n        url = urlparse.urljoin(base_url, 'v1/calc/%d' % hc_id)\n        abortable = False\n        if is_running:\n            try:\n                if psutil.Process(pid).username() == username:\n                    abortable = True\n            except psutil.NoSuchProcess:\n                pass\n        response_data.append(\n            dict(id=hc_id, owner=owner,\n                 calculation_mode=calculation_mode, status=status,\n                 is_running=bool(is_running), description=desc, url=url,\n                 parent_id=parent_id, abortable=abortable, size_mb=size_mb))\n\n    # if id is specified the related dictionary is returned instead the list\n    if id is not None:\n        [response_data] = response_data\n\n    return HttpResponse(content=json.dumps(response_data),\n                        content_type=JSON)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nabort the given calculation", "response": "def calc_abort(request, calc_id):\n    \"\"\"\n    Abort the given calculation, it is it running\n    \"\"\"\n    job = logs.dbcmd('get_job', calc_id)\n    if job is None:\n        message = {'error': 'Unknown job %s' % calc_id}\n        return HttpResponse(content=json.dumps(message), content_type=JSON)\n\n    if job.status not in ('submitted', 'executing'):\n        message = {'error': 'Job %s is not running' % job.id}\n        return HttpResponse(content=json.dumps(message), content_type=JSON)\n\n    if not utils.user_has_permission(request, job.user_name):\n        message = {'error': ('User %s has no permission to abort job %s' %\n                             (job.user_name, job.id))}\n        return HttpResponse(content=json.dumps(message), content_type=JSON,\n                            status=403)\n\n    if job.pid:  # is a spawned job\n        try:\n            os.kill(job.pid, signal.SIGTERM)\n        except Exception as exc:\n            logging.error(exc)\n        else:\n            logging.warning('Aborting job %d, pid=%d', job.id, job.pid)\n            logs.dbcmd('set_status', job.id, 'aborted')\n        message = {'success': 'Killing job %d' % job.id}\n        return HttpResponse(content=json.dumps(message), content_type=JSON)\n\n    message = {'error': 'PID for job %s not found' % job.id}\n    return HttpResponse(content=json.dumps(message), content_type=JSON)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the calculation id", "response": "def calc_remove(request, calc_id):\n    \"\"\"\n    Remove the calculation id\n    \"\"\"\n    # Only the owner can remove a job\n    user = utils.get_user(request)\n    try:\n        message = logs.dbcmd('del_calc', calc_id, user)\n    except dbapi.NotFound:\n        return HttpResponseNotFound()\n\n    if 'success' in message:\n        return HttpResponse(content=json.dumps(message),\n                            content_type=JSON, status=200)\n    elif 'error' in message:\n        logging.error(message['error'])\n        return HttpResponse(content=json.dumps(message),\n                            content_type=JSON, status=403)\n    else:\n        # This is an untrapped server error\n        logging.error(message)\n        return HttpResponse(content=message,\n                            content_type='text/plain', status=500)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a log record into a list of strings", "response": "def log_to_json(log):\n    \"\"\"Convert a log record into a list of strings\"\"\"\n    return [log.timestamp.isoformat()[:22],\n            log.level, log.process, log.message]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a slice of the calculation log as a JSON list of rows", "response": "def calc_log(request, calc_id, start, stop):\n    \"\"\"\n    Get a slice of the calculation log as a JSON list of rows\n    \"\"\"\n    start = start or 0\n    stop = stop or 0\n    try:\n        response_data = logs.dbcmd('get_log_slice', calc_id, start, stop)\n    except dbapi.NotFound:\n        return HttpResponseNotFound()\n    return HttpResponse(content=json.dumps(response_data), content_type=JSON)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_log_size(request, calc_id):\n    try:\n        response_data = logs.dbcmd('get_log_size', calc_id)\n    except dbapi.NotFound:\n        return HttpResponseNotFound()\n    return HttpResponse(content=json.dumps(response_data), content_type=JSON)", "response": "Get the current number of lines in the log of the given calc_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a risk and the risk. xml file.", "response": "def calc_run(request):\n    \"\"\"\n    Run a calculation.\n\n    :param request:\n        a `django.http.HttpRequest` object.\n        If the request has the attribute `hazard_job_id`, the results of the\n        specified hazard calculations will be re-used as input by the risk\n        calculation.\n        The request also needs to contain the files needed to perform the\n        calculation. They can be uploaded as separate files, or zipped\n        together.\n    \"\"\"\n    hazard_job_id = request.POST.get('hazard_job_id')\n\n    if hazard_job_id:\n        hazard_job_id = int(hazard_job_id)\n        candidates = (\"job_risk.ini\", \"job.ini\")\n    else:\n        candidates = (\"job_hazard.ini\", \"job_haz.ini\", \"job.ini\")\n    result = safely_call(_prepare_job, (request, candidates))\n    if result.tb_str:\n        return HttpResponse(json.dumps(result.tb_str.splitlines()),\n                            content_type=JSON, status=500)\n    inifiles = result.get()\n    if not inifiles:\n        msg = 'Could not find any file of the form %s' % str(candidates)\n        logging.error(msg)\n        return HttpResponse(content=json.dumps([msg]), content_type=JSON,\n                            status=500)\n\n    user = utils.get_user(request)\n    try:\n        job_id, pid = submit_job(inifiles[0], user, hazard_job_id)\n    except Exception as exc:  # no job created, for instance missing .xml file\n        # get the exception message\n        exc_msg = str(exc)\n        logging.error(exc_msg)\n        response_data = exc_msg.splitlines()\n        status = 500\n    else:\n        response_data = dict(job_id=job_id, status='created', pid=pid)\n        status = 200\n    return HttpResponse(content=json.dumps(response_data), content_type=JSON,\n                        status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef submit_job(job_ini, username, hazard_job_id=None):\n    job_id = logs.init('job')\n    oq = engine.job_from_file(\n        job_ini, job_id, username, hazard_calculation_id=hazard_job_id)\n    pik = pickle.dumps(oq, protocol=0)  # human readable protocol\n    code = RUNCALC % dict(job_id=job_id, hazard_job_id=hazard_job_id, pik=pik,\n                          username=username)\n    tmp_py = gettemp(code, suffix='.py')\n    # print(code, tmp_py)  # useful when debugging\n    devnull = subprocess.DEVNULL\n    popen = subprocess.Popen([sys.executable, tmp_py],\n                             stdin=devnull, stdout=devnull, stderr=devnull)\n    threading.Thread(target=popen.wait).start()\n    logs.dbcmd('update_job', job_id, {'pid': popen.pid})\n    return job_id, popen.pid", "response": "Submit a job from a job. ini file in the job directory and return the job ID and PID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a summarized list of calculation results for a given calc_id.", "response": "def calc_results(request, calc_id):\n    \"\"\"\n    Get a summarized list of calculation results for a given ``calc_id``.\n    Result is a JSON array of objects containing the following attributes:\n\n        * id\n        * name\n        * type (hazard_curve, hazard_map, etc.)\n        * url (the exact url where the full result can be accessed)\n    \"\"\"\n    # If the specified calculation doesn't exist OR is not yet complete,\n    # throw back a 404.\n    try:\n        info = logs.dbcmd('calc_info', calc_id)\n        if not utils.user_has_permission(request, info['user_name']):\n            return HttpResponseForbidden()\n    except dbapi.NotFound:\n        return HttpResponseNotFound()\n    base_url = _get_base_url(request)\n\n    # NB: export_output has as keys the list (output_type, extension)\n    # so this returns an ordered map output_type -> extensions such as\n    # {'agg_loss_curve': ['xml', 'csv'], ...}\n    output_types = groupby(export, lambda oe: oe[0],\n                           lambda oes: [e for o, e in oes])\n    results = logs.dbcmd('get_outputs', calc_id)\n    if not results:\n        return HttpResponseNotFound()\n\n    response_data = []\n    for result in results:\n        try:  # output from the datastore\n            rtype = result.ds_key\n            # Catalina asked to remove the .txt outputs (used for the GMFs)\n            outtypes = [ot for ot in output_types[rtype] if ot != 'txt']\n        except KeyError:\n            continue  # non-exportable outputs should not be shown\n        url = urlparse.urljoin(base_url, 'v1/calc/result/%d' % result.id)\n        datum = dict(\n            id=result.id, name=result.display_name, type=rtype,\n            outtypes=outtypes, url=url, size_mb=result.size_mb)\n        response_data.append(datum)\n\n    return HttpResponse(content=json.dumps(response_data))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload a specific result by result_id.", "response": "def calc_result(request, result_id):\n    \"\"\"\n    Download a specific result, by ``result_id``.\n\n    The common abstracted functionality for getting hazard or risk results.\n\n    :param request:\n        `django.http.HttpRequest` object. Can contain a `export_type` GET\n        param (the default is 'xml' if no param is specified).\n    :param result_id:\n        The id of the requested artifact.\n    :returns:\n        If the requested ``result_id`` is not available in the format\n        designated by the `export_type`.\n\n        Otherwise, return a `django.http.HttpResponse` containing the content\n        of the requested artifact.\n\n    Parameters for the GET request can include an `export_type`, such as 'xml',\n    'geojson', 'csv', etc.\n    \"\"\"\n    # If the result for the requested ID doesn't exist, OR\n    # the job which it is related too is not complete,\n    # throw back a 404.\n    try:\n        job_id, job_status, job_user, datadir, ds_key = logs.dbcmd(\n            'get_result', result_id)\n        if not utils.user_has_permission(request, job_user):\n            return HttpResponseForbidden()\n    except dbapi.NotFound:\n        return HttpResponseNotFound()\n\n    etype = request.GET.get('export_type')\n    export_type = etype or DEFAULT_EXPORT_TYPE\n\n    tmpdir = tempfile.mkdtemp()\n    try:\n        exported = core.export_from_db(\n            (ds_key, export_type), job_id, datadir, tmpdir)\n    except DataStoreExportError as exc:\n        # TODO: there should be a better error page\n        return HttpResponse(content='%s: %s' % (exc.__class__.__name__, exc),\n                            content_type='text/plain', status=500)\n    if not exported:\n        # Throw back a 404 if the exact export parameters are not supported\n        return HttpResponseNotFound(\n            'Nothing to export for export_type=%s, %s' % (export_type, ds_key))\n    elif len(exported) > 1:\n        # Building an archive so that there can be a single file download\n        archname = ds_key + '-' + export_type + '.zip'\n        zipfiles(exported, os.path.join(tmpdir, archname))\n        exported = os.path.join(tmpdir, archname)\n    else:  # single file\n        exported = exported[0]\n\n    content_type = EXPORT_CONTENT_TYPE_MAP.get(\n        export_type, DEFAULT_CONTENT_TYPE)\n\n    fname = 'output-%s-%s' % (result_id, os.path.basename(exported))\n    stream = FileWrapper(open(exported, 'rb'))  # 'b' is needed on Windows\n    stream.close = lambda: (\n        FileWrapper.close(stream), shutil.rmtree(tmpdir))\n    response = FileResponse(stream, content_type=content_type)\n    response['Content-Disposition'] = (\n        'attachment; filename=%s' % os.path.basename(fname))\n    response['Content-Length'] = str(os.path.getsize(exported))\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap over the oq extract command.", "response": "def extract(request, calc_id, what):\n    \"\"\"\n    Wrapper over the `oq extract` command. If `setting.LOCKDOWN` is true\n    only calculations owned by the current user can be retrieved.\n    \"\"\"\n    job = logs.dbcmd('get_job', int(calc_id))\n    if job is None:\n        return HttpResponseNotFound()\n    if not utils.user_has_permission(request, job.user_name):\n        return HttpResponseForbidden()\n\n    try:\n        # read the data and save them on a temporary .npz file\n        with datastore.read(job.ds_calc_dir + '.hdf5') as ds:\n            fd, fname = tempfile.mkstemp(\n                prefix=what.replace('/', '-'), suffix='.npz')\n            os.close(fd)\n            n = len(request.path_info)\n            query_string = unquote_plus(request.get_full_path()[n:])\n            aw = _extract(ds, what + query_string)\n            a = {}\n            for key, val in vars(aw).items():\n                key = str(key)  # can be a numpy.bytes_\n                if isinstance(val, str):\n                    # without this oq extract would fail\n                    a[key] = numpy.array(val.encode('utf-8'))\n                elif isinstance(val, dict):\n                    # this is hack: we are losing the values\n                    a[key] = list(val)\n                else:\n                    a[key] = val\n            numpy.savez_compressed(fname, **a)\n    except Exception as exc:\n        tb = ''.join(traceback.format_tb(exc.__traceback__))\n        return HttpResponse(\n            content='%s: %s\\n%s' % (exc.__class__.__name__, exc, tb),\n            content_type='text/plain', status=500)\n\n    # stream the data back\n    stream = FileWrapper(open(fname, 'rb'))\n    stream.close = lambda: (FileWrapper.close(stream), os.remove(fname))\n    response = FileResponse(stream, content_type='application/octet-stream')\n    response['Content-Disposition'] = (\n        'attachment; filename=%s' % os.path.basename(fname))\n    response['Content-Length'] = str(os.path.getsize(fname))\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_datastore(request, job_id):\n    job = logs.dbcmd('get_job', int(job_id))\n    if job is None:\n        return HttpResponseNotFound()\n    if not utils.user_has_permission(request, job.user_name):\n        return HttpResponseForbidden()\n\n    fname = job.ds_calc_dir + '.hdf5'\n    response = FileResponse(\n        FileWrapper(open(fname, 'rb')), content_type=HDF5)\n    response['Content-Disposition'] = (\n        'attachment; filename=%s' % os.path.basename(fname))\n    response['Content-Length'] = str(os.path.getsize(fname))\n    return response", "response": "Download a full datastore file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_oqparam(request, job_id):\n    job = logs.dbcmd('get_job', int(job_id))\n    if job is None:\n        return HttpResponseNotFound()\n    if not utils.user_has_permission(request, job.user_name):\n        return HttpResponseForbidden()\n\n    with datastore.read(job.ds_calc_dir + '.hdf5') as ds:\n        oq = ds['oqparam']\n    return HttpResponse(content=json.dumps(vars(oq)), content_type=JSON)", "response": "Return the calculation parameters as a JSON object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a file is available to a same FS.", "response": "def on_same_fs(request):\n    \"\"\"\n    Accept a POST request to check access to a FS available by a client.\n\n    :param request:\n        `django.http.HttpRequest` object, containing mandatory parameters\n        filename and checksum.\n    \"\"\"\n    filename = request.POST['filename']\n    checksum_in = request.POST['checksum']\n\n    checksum = 0\n    try:\n        data = open(filename, 'rb').read(32)\n        checksum = zlib.adler32(data, checksum) & 0xffffffff\n        if checksum == int(checksum_in):\n            return HttpResponse(content=json.dumps({'success': True}),\n                                content_type=JSON, status=200)\n    except (IOError, ValueError):\n        pass\n\n    return HttpResponse(content=json.dumps({'success': False}),\n                        content_type=JSON, status=200)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef classical_damage(riskinputs, riskmodel, param, monitor):\n    result = AccumDict(accum=AccumDict())\n    for ri in riskinputs:\n        for out in riskmodel.gen_outputs(ri, monitor):\n            for l, loss_type in enumerate(riskmodel.loss_types):\n                ordinals = ri.assets['ordinal']\n                result[l, out.rlzi] += dict(zip(ordinals, out[loss_type]))\n    return result", "response": "This function is used to compute the classical damage of a single item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_losses(calc_id, bins=7):\n    # read the hazard data\n    dstore = util.read(calc_id)\n    losses_by_rlzi = dict(extract(dstore, 'losses_by_event'))\n    oq = dstore['oqparam']\n    plt = make_figure(losses_by_rlzi, oq.loss_dt().names, bins)\n    plt.show()", "response": "Plot the losses of the hazard in a single plot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsort two matrices returning a positive or zero value.", "response": "def cmp_mat(a, b):\n    \"\"\"\n    Sorts two matrices returning a positive or zero value\n    \"\"\"\n    c = 0\n    for x, y in zip(a.flat, b.flat):\n        c = cmp(abs(x), abs(y))\n        if c != 0:\n            return c\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_centroid_time(self, time_diff):\n        source_time = datetime.datetime.combine(self.date, self.time)\n        second_diff = floor(fabs(time_diff))\n        microsecond_diff = int(1000. * (time_diff - second_diff))\n        if time_diff < 0.:\n            source_time = source_time - datetime.timedelta(\n                seconds=int(second_diff), microseconds=microsecond_diff)\n        else:\n            source_time = source_time + datetime.timedelta(\n                seconds=int(second_diff), microseconds=microsecond_diff)\n        self.time = source_time.time()\n        self.date = source_time.date()", "response": "Calculates the time difference between the date - time classes and the time of the class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalise the tensor by dividing it by its norm", "response": "def normalise_tensor(self):\n        \"\"\"\n        Normalise the tensor by dividing it by its norm, defined such that\n        np.sqrt(X:X)\n        \"\"\"\n        self.tensor, tensor_norm = utils.normalise_tensor(self.tensor)\n        return self.tensor / tensor_norm, tensor_norm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nswitching the reference frame to NED", "response": "def _to_ned(self):\n        \"\"\"\n        Switches the reference frame to NED\n        \"\"\"\n        if self.ref_frame is 'USE':\n            # Rotate\n            return utils.use_to_ned(self.tensor), \\\n                utils.use_to_ned(self.tensor_sigma)\n        elif self.ref_frame is 'NED':\n            # Alreadt NED\n            return self.tensor, self.tensor_sigma\n        else:\n            raise ValueError('Reference frame %s not recognised - cannot '\n                             'transform to NED!' % self.ref_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tensor in the USE reference frame", "response": "def _to_use(self):\n        \"\"\"\n        Returns a tensor in the USE reference frame\n        \"\"\"\n        if self.ref_frame is 'NED':\n            # Rotate\n            return utils.ned_to_use(self.tensor), \\\n                utils.ned_to_use(self.tensor_sigma)\n        elif self.ref_frame is 'USE':\n            # Already USE\n            return self.tensor, self.tensor_sigma\n        else:\n            raise ValueError('Reference frame %s not recognised - cannot '\n                             'transform to USE!' % self.ref_frame)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms and eigendecomposition of the tensor and orders into descending eigenvalues", "response": "def eigendecompose(self, normalise=False):\n        \"\"\"\n        Performs and eigendecomposition of the tensor and orders into \n        descending eigenvalues\n        \"\"\"\n        self.eigenvalues, self.eigenvectors = utils.eigendecompose(self.tensor,\n                                                                   normalise)\n        return self.eigenvalues, self.eigenvectors"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nodal_planes(self):\n        # Convert reference frame to NED\n        self.tensor, self.tensor_sigma = self._to_ned()\n        self.ref_frame = 'NED'\n        # Eigenvalue decomposition\n        # Tensor\n        _, evect = utils.eigendecompose(self.tensor)\n        # Rotation matrix\n        _, rot_vec = utils.eigendecompose(np.matrix([[0., 0., -1],\n                                                     [0., 0., 0.],\n                                                     [-1., 0., 0.]]))\n        rotation_matrix = (np.matrix(evect * rot_vec.T)).T\n        if np.linalg.det(rotation_matrix) < 0.:\n            rotation_matrix *= -1.\n        flip_dc = np.matrix([[0., 0., -1.],\n                             [0., -1., 0.],\n                             [-1., 0., 0.]])\n        rotation_matrices = sorted(\n            [rotation_matrix, flip_dc * rotation_matrix],\n            cmp=cmp_mat)\n        nodal_planes = GCMTNodalPlanes()\n        dip, strike, rake = [(180. / pi) * angle\n                             for angle in utils.matrix_to_euler(rotation_matrices[0])]\n        # 1st Nodal Plane\n        nodal_planes.nodal_plane_1 = {'strike': strike % 360,\n                                      'dip': dip,\n                                      'rake': -rake}\n\n        # 2nd Nodal Plane\n        dip, strike, rake = [(180. / pi) * angle\n                             for angle in utils.matrix_to_euler(rotation_matrices[1])]\n        nodal_planes.nodal_plane_2 = {'strike': strike % 360.,\n                                      'dip': dip,\n                                      'rake': -rake}\n        return nodal_planes", "response": "Returns the nodal planes by eigendecomposition of the moment tensor"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_principal_axes(self):\n        # Perform eigendecomposition - returns in order P, B, T\n        _ = self.eigendecompose(normalise=True)\n        principal_axes = GCMTPrincipalAxes()\n        # Eigenvalues\n        principal_axes.p_axis = {'eigenvalue': self.eigenvalues[0]}\n        principal_axes.b_axis = {'eigenvalue': self.eigenvalues[1]}\n        principal_axes.t_axis = {'eigenvalue': self.eigenvalues[2]}\n        # Eigen vectors\n        # 1) P axis\n        azim, plun = utils.get_azimuth_plunge(self.eigenvectors[:, 0], True)\n        principal_axes.p_axis['azimuth'] = azim\n        principal_axes.p_axis['plunge'] = plun\n        # 2) B axis\n        azim, plun = utils.get_azimuth_plunge(self.eigenvectors[:, 1], True)\n        principal_axes.b_axis['azimuth'] = azim\n        principal_axes.b_axis['plunge'] = plun\n        # 3) T axis\n        azim, plun = utils.get_azimuth_plunge(self.eigenvectors[:, 2], True)\n        principal_axes.t_axis['azimuth'] = azim\n        principal_axes.t_axis['plunge'] = plun\n        return principal_axes", "response": "Returns an instance of the GCMTPrincipalAxes class for the given entry - time - domain name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_f_clvd(self):\n        if not self.principal_axes:\n            # Principal axes not yet defined for moment tensor - raises error\n            raise ValueError('Principal Axes not defined!')\n\n        denominator = np.max(np.array([\n            fabs(self.principal_axes.t_axis['eigenvalue']),\n            fabs(self.principal_axes.p_axis['eigenvalue'])\n        ]))\n        self.f_clvd = -self.principal_axes.b_axis['eigenvalue'] / denominator\n        return self.f_clvd", "response": "Returns the statistic f_clvd of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the relative error statistic of the current object.", "response": "def get_relative_error(self):\n        \"\"\"\n        Returns the relative error statistic (e_rel), defined by Frohlich &\n        Davis (1999): `e_rel = sqrt((U:U) / (M:M))` where M is the moment\n        tensor, U is the uncertainty tensor and : is the tensor dot product\n        \"\"\"\n        if not self.moment_tensor:\n            raise ValueError('Moment tensor not defined!')\n\n        numer = np.tensordot(self.moment_tensor.tensor_sigma,\n                             self.moment_tensor.tensor_sigma)\n\n        denom = np.tensordot(self.moment_tensor.tensor,\n                             self.moment_tensor.tensor)\n        self.e_rel = sqrt(numer / denom)\n        return self.e_rel"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nselects the events in the catalogue according to an indexing vector", "response": "def select_catalogue_events(self, id0):\n        '''\n        Orders the events in the catalogue according to an indexing vector\n\n        :param np.ndarray id0:\n            Pointer array indicating the locations of selected events\n        '''\n        for key in self.data.keys():\n            if isinstance(\n                    self.data[key], np.ndarray) and len(self.data[key]) > 0:\n                # Dictionary element is numpy array - use logical indexing\n                self.data[key] = self.data[key][id0]\n            elif isinstance(\n                    self.data[key], list) and len(self.data[key]) > 0:\n                # Dictionary element is list\n                self.data[key] = [self.data[key][iloc] for iloc in id0]\n            else:\n                continue\n\n        if len(self.gcmts) > 0:\n            self.gcmts = [self.gcmts[iloc] for iloc in id0]\n            self.number_gcmts = self.get_number_tensors()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gcmt_to_simple_array(self, centroid_location=True):\n        catalogue = np.zeros([self.get_number_tensors(), 29], dtype=float)\n        for iloc, tensor in enumerate(self.gcmts):\n            catalogue[iloc, 0] = iloc\n            if centroid_location:\n                catalogue[iloc, 1] = float(tensor.centroid.date.year)\n                catalogue[iloc, 2] = float(tensor.centroid.date.month)\n                catalogue[iloc, 3] = float(tensor.centroid.date.day)\n                catalogue[iloc, 4] = float(tensor.centroid.time.hour)\n                catalogue[iloc, 5] = float(tensor.centroid.time.minute)\n                catalogue[iloc, 6] = np.round(\n                    np.float(tensor.centroid.time.second) +\n                    np.float(tensor.centroid.time.microsecond) / 1000000., 2)\n                catalogue[iloc, 7] = tensor.centroid.longitude\n                catalogue[iloc, 8] = tensor.centroid.latitude\n                catalogue[iloc, 9] = tensor.centroid.depth\n            else:\n                catalogue[iloc, 1] = float(tensor.hypocentre.date.year)\n                catalogue[iloc, 2] = float(tensor.hypocentre.date.month)\n                catalogue[iloc, 3] = float(tensor.hypocentre.date.day)\n                catalogue[iloc, 4] = float(tensor.hypocentre.time.hour)\n                catalogue[iloc, 5] = float(tensor.hypocentre.time.minute)\n                catalogue[iloc, 6] = np.round(\n                    np.float(tensor.centroid.time.second) +\n                    np.float(tensor.centroid.time.microsecond) / 1000000., 2)\n                catalogue[iloc, 7] = tensor.hypocentre.longitude\n                catalogue[iloc, 8] = tensor.hypocentre.latitude\n                catalogue[iloc, 9] = tensor.hypocentre.depth\n            catalogue[iloc, 10] = tensor.magnitude\n            catalogue[iloc, 11] = tensor.moment\n            catalogue[iloc, 12] = tensor.f_clvd\n            catalogue[iloc, 13] = tensor.e_rel\n            # Nodal planes\n            catalogue[iloc, 14] = tensor.nodal_planes.nodal_plane_1['strike']\n            catalogue[iloc, 15] = tensor.nodal_planes.nodal_plane_1['dip']\n            catalogue[iloc, 16] = tensor.nodal_planes.nodal_plane_1['rake']\n            catalogue[iloc, 17] = tensor.nodal_planes.nodal_plane_2['strike']\n            catalogue[iloc, 18] = tensor.nodal_planes.nodal_plane_2['dip']\n            catalogue[iloc, 19] = tensor.nodal_planes.nodal_plane_2['rake']\n            # Principal axes\n            catalogue[iloc, 20] = tensor.principal_axes.b_axis['eigenvalue']\n            catalogue[iloc, 21] = tensor.principal_axes.b_axis['azimuth']\n            catalogue[iloc, 22] = tensor.principal_axes.b_axis['plunge']\n            catalogue[iloc, 23] = tensor.principal_axes.p_axis['eigenvalue']\n            catalogue[iloc, 24] = tensor.principal_axes.p_axis['azimuth']\n            catalogue[iloc, 25] = tensor.principal_axes.p_axis['plunge']\n            catalogue[iloc, 26] = tensor.principal_axes.t_axis['eigenvalue']\n            catalogue[iloc, 27] = tensor.principal_axes.t_axis['azimuth']\n            catalogue[iloc, 28] = tensor.principal_axes.t_axis['plunge']\n        return catalogue", "response": "Converts the GCMT catalogue to a simple array of objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients (for soil amplification)\n        # specific to required intensity measure type\n        C_SR = self.COEFFS_SOIL_RESPONSE[imt]\n\n        # compute median PGA on rock (in g), needed to compute non-linear site\n        # amplification\n        C = self.COEFFS_AC10[PGA()]\n        pga4nl = np.exp(\n            self._compute_mean(C, rup.mag, dists.rjb, rup.rake)) * 1e-2 / g\n\n        # compute full mean value by adding site amplification terms\n        # (but avoiding recomputing mean on rock for PGA)\n        if imt == PGA():\n            mean = (np.log(pga4nl) +\n                    self._get_site_amplification_linear(sites.vs30, C_SR) +\n                    self._get_site_amplification_non_linear(sites.vs30, pga4nl,\n                                                            C_SR))\n        else:\n            C = self.COEFFS_AC10[imt]\n            mean = (self._compute_mean(C, rup.mag, dists.rjb, rup.rake) +\n                    self._get_site_amplification_linear(sites.vs30, C_SR) +\n                    self._get_site_amplification_non_linear(sites.vs30, pga4nl,\n                                                            C_SR))\n\n        # convert from cm/s**2 to g for SA (PGA is already computed in g)\n        if imt.name == \"SA\":\n            mean = np.log(np.exp(mean) * 1e-2 / g)\n\n        stddevs = self._get_stddevs(C, stddev_types, num_sites=len(sites.vs30))\n\n        return mean, stddevs", "response": "This method computes the mean value and standard deviation for the site amplification term and returns it as a tuple of the mean and standard deviation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_linear_magnitude_term(self, C, mag):\n        if mag <= self.c1:\n            # this is the second term in eq. (1a), p. 2981\n            return C['a2'] * (mag - self.c1)\n        else:\n            # this is the second term in eq. (1b), p. 2982\n            return C['a3'] * (mag - self.c1)", "response": "Compute and return the linear magnitude term in equations 1a and 1b."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_shapefile(output, input_shp_files, validate):\n    input_parser = shapefileparser.ShapefileParser()\n    source_model = input_parser.read(input_shp_files[0], validate)\n    for f in input_shp_files[1:]:\n        source_model.sources.extend(input_parser.read(f, validate).sources)\n    if not output:\n        output = os.path.splitext(input_shp_files[0])[0]\n    shapefileparser.SourceModelParser().write(output + '.xml', source_model)", "response": "Convert multiple ESRI Shapefile into a single NRML source model file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_magnitude(self, rup, C):\n        return C['b1'] + (C['b2'] * rup.mag) + (C['b3'] * (rup.mag ** 2))", "response": "Compute the magnitude term in equation 19"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the distance between the base and base rupture.", "response": "def _compute_distance(self, rup, dists, imt, C):\n        \"\"\"\n        Compute the second term of the equation described on p. 199:\n\n        ``(b4 + b5 * M) * log(sqrt(Rjb ** 2 + b6 ** 2))``\n        \"\"\"\n        return (((C['b4'] + C['b5'] * rup.mag)\n                 * np.log10((np.sqrt(dists.rjb ** 2.0 + C['b6'] ** 2.0)))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the third term of the equation described on p. 199", "response": "def _get_site_amplification(self, sites, imt, C):\n        \"\"\"\n        Compute the third term of the equation described on p. 199:\n\n        ``b7 * Ss + b8 * Sa``\n        \"\"\"\n        Ss, Sa = self._get_site_type_dummy_variables(sites)\n        return (C['b7'] * Ss) + (C['b8'] * Sa)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_site_type_dummy_variables(self, sites):\n        Ss = np.zeros((len(sites.vs30),))\n        Sa = np.zeros((len(sites.vs30),))\n        # Soft soil; Vs30 < 360 m/s. Page 199.\n        idxSs = (sites.vs30 < 360.0)\n        # Stiff soil Class A; 360 m/s <= Vs30 <= 750 m/s. Page 199.\n        idxSa = (sites.vs30 >= 360.0) & (sites.vs30 <= 750.0)\n        Ss[idxSs] = 1\n        Sa[idxSa] = 1\n        return Ss, Sa", "response": "Get site type dummy variables Ss and Sa."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the mechanism of the logarithmic core.", "response": "def _get_mechanism(self, sites, rup, imt, C):\n        \"\"\"\n        Compute the fourth term of the equation described on p. 199:\n\n        ``b9 * Fn + b10 * Fr``\n        \"\"\"\n        Fn, Fr = self._get_fault_type_dummy_variables(sites, rup, imt)\n        return (C['b9'] * Fn) + (C['b10'] * Fr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning dummy variable values for fault type.", "response": "def _get_fault_type_dummy_variables(self, sites, rup, imt):\n        \"\"\"\n        Same classification of SadighEtAl1997. Akkar and Bommer 2010 is based\n        on Akkar and Bommer 2007b; read Strong-Motion Dataset and Record\n        Processing on p. 514 (Akkar and Bommer 2007b).\n        \"\"\"\n\n        Fn, Fr = 0, 0\n        if rup.rake >= -135 and rup.rake <= -45:\n            # normal\n            Fn = 1\n        elif rup.rake >= 45 and rup.rake <= 135:\n            # reverse\n            Fr = 1\n        return Fn, Fr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nseeing :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n\n        sites.vs30 = 600 * np.ones(len(sites.vs30))\n\n        mean, stddevs = super(AkkarBommer2010SWISS01, self).\\\n            get_mean_and_stddevs(sites, rup, dists, imt, stddev_types)\n\n        tau_ss = 'tau'\n        log_phi_ss = np.log(10)\n        mean, stddevs = _apply_adjustments(\n            AkkarBommer2010.COEFFS, self.COEFFS_FS_ROCK[imt], tau_ss,\n            mean, stddevs, sites, rup, dists.rjb, imt, stddev_types,\n            log_phi_ss)\n\n        return mean,  np.log(10 ** np.array(stddevs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the mesh corresponding to the whole multi surface", "response": "def mesh(self):\n        \"\"\"\n        :returns: mesh corresponding to the whole multi surface\n        \"\"\"\n        meshes = [surface.mesh for surface in self.surfaces]\n        lons = numpy.concatenate([m.lons for m in meshes])\n        lats = numpy.concatenate([m.lats for m in meshes])\n        depths = numpy.concatenate([m.depths for m in meshes])\n        return Mesh(lons, lats, depths)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_edge_set(self, tol=0.1):\n        edges = []\n        for surface in self.surfaces:\n            if isinstance(surface, GriddedSurface):\n                return edges.append(surface.mesh)\n            elif isinstance(surface, PlanarSurface):\n                # Top edge determined from two end points\n                edge = []\n                for pnt in [surface.top_left, surface.top_right]:\n                    edge.append([pnt.longitude, pnt.latitude, pnt.depth])\n                edges.append(numpy.array(edge))\n            elif isinstance(surface,\n                            (ComplexFaultSurface, SimpleFaultSurface)):\n                # Rectangular meshes are downsampled to reduce their\n                # overall size\n                edges.append(downsample_trace(surface.mesh, tol))\n            else:\n                raise ValueError(\"Surface %s not recognised\" % str(surface))\n        return edges", "response": "Retrieve set of top edges from all of the individual surfaces"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_min_distance(self, mesh):\n        dists = [surf.get_min_distance(mesh) for surf in self.surfaces]\n\n        return numpy.min(dists, axis=0)", "response": "Compute the minimum distance to each point in mesh and return the smallest value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_closest_points(self, mesh):\n        # first, for each point in mesh compute minimum distance to each\n        # surface. The distance matrix is flattend, because mesh can be of\n        # an arbitrary shape. By flattening we obtain a ``distances`` matrix\n        # for which the first dimension represents the different surfaces\n        # and the second dimension the mesh points.\n        dists = numpy.array(\n            [surf.get_min_distance(mesh).flatten() for surf in self.surfaces]\n        )\n\n        # find for each point in mesh the index of closest surface\n        idx = dists == numpy.min(dists, axis=0)\n\n        # loop again over surfaces. For each surface compute the closest\n        # points, and associate them to the mesh points for which the surface\n        # is the closest. Note that if a surface is not the closest to any of\n        # the mesh points then the calculation is skipped\n        lons = numpy.empty_like(mesh.lons.flatten())\n        lats = numpy.empty_like(mesh.lats.flatten())\n        depths = None if mesh.depths is None else \\\n            numpy.empty_like(mesh.depths.flatten())\n        for i, surf in enumerate(self.surfaces):\n            if not idx[i, :].any():\n                continue\n            cps = surf.get_closest_points(mesh)\n            lons[idx[i, :]] = cps.lons.flatten()[idx[i, :]]\n            lats[idx[i, :]] = cps.lats.flatten()[idx[i, :]]\n            if depths is not None:\n                depths[idx[i, :]] = cps.depths.flatten()[idx[i, :]]\n        lons = lons.reshape(mesh.lons.shape)\n        lats = lats.reshape(mesh.lats.shape)\n        if depths is not None:\n            depths = depths.reshape(mesh.depths.shape)\n\n        return Mesh(lons, lats, depths)", "response": "This method returns the points that are closest to each point in a mesh."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_top_edge_depth(self):\n        areas = self._get_areas()\n        depths = numpy.array(\n            [surf.get_top_edge_depth() for surf in self.surfaces])\n        return numpy.sum(areas * depths) / numpy.sum(areas)", "response": "Compute the top edge depth of each surface element and return area - weighted\n        average value in km."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_strike(self):\n        areas = self._get_areas()\n        strikes = numpy.array([surf.get_strike() for surf in self.surfaces])\n\n        v1 = (numpy.sum(areas * numpy.sin(numpy.radians(strikes))) /\n              numpy.sum(areas))\n        v2 = (numpy.sum(areas * numpy.cos(numpy.radians(strikes))) /\n              numpy.sum(areas))\n\n        return numpy.degrees(numpy.arctan2(v1, v2)) % 360", "response": "Compute strike of each surface element and return area - weighted average\n        value in range 0 360"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing dip of each surface element and return area - weighted average value.", "response": "def get_dip(self):\n        \"\"\"\n        Compute dip of each surface element and return area-weighted average\n        value (in range ``(0, 90]``).\n\n        Given that dip values are constrained in the range (0, 90], the simple\n        formula for weighted mean is used.\n        \"\"\"\n        areas = self._get_areas()\n        dips = numpy.array([surf.get_dip() for surf in self.surfaces])\n\n        return numpy.sum(areas * dips) / numpy.sum(areas)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing width of each surface element and return area - weighted average value in km.", "response": "def get_width(self):\n        \"\"\"\n        Compute width of each surface element, and return area-weighted\n        average value (in km).\n        \"\"\"\n        areas = self._get_areas()\n        widths = numpy.array([surf.get_width() for surf in self.surfaces])\n\n        return numpy.sum(areas * widths) / numpy.sum(areas)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the bounding box for each surface element and then return the bounding box of all surface elements northern and southern borders of the items.", "response": "def get_bounding_box(self):\n        \"\"\"\n        Compute bounding box for each surface element, and then return\n        the bounding box of all surface elements' bounding boxes.\n\n        :return:\n            A tuple of four items. These items represent western, eastern,\n            northern and southern borders of the bounding box respectively.\n            Values are floats in decimal degrees.\n        \"\"\"\n        lons = []\n        lats = []\n        for surf in self.surfaces:\n            west, east, north, south = surf.get_bounding_box()\n            lons.extend([west, east])\n            lats.extend([north, south])\n        return utils.get_spherical_bounding_box(lons, lats)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_middle_point(self):\n        if len(self.surfaces) == 1:\n            return self.surfaces[0].get_middle_point()\n\n        west, east, north, south = self.get_bounding_box()\n        longitude, latitude = utils.get_middle_point(west, north, east, south)\n\n        dists = []\n        for surf in self.surfaces:\n            dists.append(\n                surf.get_min_distance(Mesh(numpy.array([longitude]),\n                                           numpy.array([latitude]),\n                                           None)))\n        dists = numpy.array(dists).flatten()\n        idx = dists == numpy.min(dists)\n        return numpy.array(self.surfaces)[idx][0].get_middle_point()", "response": "Returns the middle point of the rupture in the surface."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn surface elements area values in a numpy array.", "response": "def _get_areas(self):\n        \"\"\"\n        Return surface elements area values in a numpy array.\n        \"\"\"\n        if self.areas is None:\n            self.areas = []\n            for surf in self.surfaces:\n                self.areas.append(surf.get_area())\n            self.areas = numpy.array(self.areas)\n\n        return self.areas"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_cartesian_edge_set(self):\n        # Get projection space for cartesian projection\n        edge_sets = numpy.vstack(self.edge_set)\n        west, east, north, south = utils.get_spherical_bounding_box(\n            edge_sets[:, 0],\n            edge_sets[:, 1])\n        self.proj = utils.OrthographicProjection(west, east, north, south)\n\n        for edges in self.edge_set:\n            # Project edges into cartesian space\n            px, py = self.proj(edges[:, 0], edges[:, 1])\n            # Store the two end-points of the trace\n            self.cartesian_endpoints.append(\n                numpy.array([[px[0], py[0], edges[0, 2]],\n                             [px[-1], py[-1], edges[-1, 2]]]))\n            self.cartesian_edges.append(numpy.column_stack([px, py,\n                                                            edges[:, 2]]))\n            # Get surface length vector for the trace - easier in cartesian\n            lengths = numpy.sqrt((px[:-1] - px[1:]) ** 2. +\n                                 (py[:-1] - py[1:]) ** 2.)\n            self.length_set.append(lengths)\n            # Get cumulative surface length vector\n            self.cum_length_set.append(\n                numpy.hstack([0., numpy.cumsum(lengths)]))\n        return edge_sets", "response": "Returns a set of cartesian representations of the fault edges."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_gc2_framework(self):\n        # Generate cartesian edge set\n        edge_sets = self._get_cartesian_edge_set()\n        self.gc2_config = {}\n        # Determine furthest two points apart\n        endpoint_set = numpy.vstack([cep for cep in self.cartesian_endpoints])\n        dmat = squareform(pdist(endpoint_set))\n        irow, icol = numpy.unravel_index(numpy.argmax(dmat), dmat.shape)\n        # Join further points to form a vector (a_hat in Spudich & Chiou)\n        # According to Spudich & Chiou, a_vec should be eastward trending\n        if endpoint_set[irow, 0] > endpoint_set[icol, 0]:\n            # Row point is to the east of column point\n            beginning = endpoint_set[icol, :2]\n            ending = endpoint_set[irow, :2]\n        else:\n            # Column point is to the east of row point\n            beginning = endpoint_set[irow, :2]\n            ending = endpoint_set[icol, :2]\n\n        # Convert to unit vector\n        a_vec = ending - beginning\n        self.gc2_config[\"a_hat\"] = a_vec / numpy.linalg.norm(a_vec)\n        # Get e_j set\n        self.gc2_config[\"ejs\"] = []\n        for c_edges in self.cartesian_edges:\n            self.gc2_config[\"ejs\"].append(\n                numpy.dot(c_edges[-1, :2] - c_edges[0, :2],\n                          self.gc2_config[\"a_hat\"]))\n        # A \"total E\" is defined as the sum of the e_j values\n        self.gc2_config[\"e_tot\"] = sum(self.gc2_config[\"ejs\"])\n        sign_etot = numpy.sign(self.gc2_config[\"e_tot\"])\n        b_vec = numpy.zeros(2)\n        self.gc2_config[\"sign\"] = []\n        for i, c_edges in enumerate(self.cartesian_edges):\n            segment_sign = numpy.sign(self.gc2_config[\"ejs\"][i]) * sign_etot\n            self.gc2_config[\"sign\"].append(segment_sign)\n            if segment_sign < 0:\n                # Segment is discordant - reverse the points\n                c_edges = numpy.flipud(c_edges)\n                self.cartesian_edges[i] = c_edges\n                self.cartesian_endpoints[i] = numpy.flipud(\n                    self.cartesian_endpoints[i])\n            b_vec += (c_edges[-1, :2] - c_edges[0, :2])\n\n        # Get unit vector\n        self.gc2_config[\"b_hat\"] = b_vec / numpy.linalg.norm(b_vec)\n        if numpy.dot(a_vec, self.gc2_config[\"b_hat\"]) >= 0.0:\n            self.p0 = beginning\n        else:\n            self.p0 = ending\n        # To later calculate Ry0 it is necessary to determine the maximum\n        # GC2-U coordinate for the fault\n        self._get_gc2_coordinates_for_rupture(edge_sets)", "response": "This method establishes the GC2 framework for a multi - segment and indeed multi - typing traceroute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_gc2_coordinates_for_rupture(self, edge_sets):\n\n        # Establish GC2 length - for use with Ry0\n        rup_gc2t, rup_gc2u = self.get_generalised_coordinates(\n            edge_sets[:, 0], edge_sets[:, 1])\n        # GC2 length should be the largest positive GC2 value of the edges\n        self.gc_length = numpy.max(rup_gc2u)", "response": "Calculates the GC2 coordinates for the nodes of the upper edge of the fault and the edge sets of the fault."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the U and T coordinate for a specific trace segment.", "response": "def _get_ut_i(self, seg, sx, sy):\n        \"\"\"\n        Returns the U and T coordinate for a specific trace segment\n\n        :param seg:\n            End points of the segment edge\n\n        :param sx:\n            Sites longitudes rendered into coordinate system\n\n        :param sy:\n            Sites latitudes rendered into coordinate system\n        \"\"\"\n        p0x, p0y, p1x, p1y = seg[0, 0], seg[0, 1], seg[1, 0], seg[1, 1]\n        # Unit vector normal to strike\n        t_i_vec = [p1y - p0y, -(p1x - p0x), 0.0]\n        t_i_hat = t_i_vec / numpy.linalg.norm(t_i_vec)\n        # Unit vector along strike\n        u_i_vec = [p1x - p0x, p1y - p0y, 0.0]\n        u_i_hat = u_i_vec / numpy.linalg.norm(u_i_vec)\n        # Vectors from P0 to sites\n        rsite = numpy.column_stack([sx - p0x, sy - p0y])\n        return numpy.sum(u_i_hat[:-1] * rsite, axis=1),\\\n            numpy.sum(t_i_hat[:-1] * rsite, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_generalised_coordinates(self, lons, lats):\n        # If the GC2 configuration has not been setup already - do it!\n        if not self.gc2_config:\n            self._setup_gc2_framework()\n        # Initially the weights are set to zero\n        sx, sy = self.proj(lons, lats)\n        sum_w_i = numpy.zeros_like(lons)\n        sum_w_i_t_i = numpy.zeros_like(lons)\n        sum_wi_ui_si = numpy.zeros_like(lons)\n        # Find the cumulative length of the fault up until the given segment\n        # Essentially calculating s_i\n        general_t = numpy.zeros_like(lons)\n        general_u = numpy.zeros_like(lons)\n        on_segment = numpy.zeros_like(lons, dtype=bool)\n        # Loop over the traces\n        for j, edges in enumerate(self.cartesian_edges):\n            # Loop over segments in trace\n            # s_ij_total = 0.0\n            for i in range(edges.shape[0] - 1):\n                # Get u_i and t_i\n                u_i, t_i = self._get_ut_i(edges[i:(i + 2), :], sx, sy)\n                # If t_i is 0 and u_i is within the section length then site is\n                # directly on the edge - therefore general_t is 0\n                w_i = numpy.zeros_like(lons)\n                ti0_check = numpy.fabs(t_i) < 1.0E-3  # < 1 m precision\n                on_segment_range = numpy.logical_and(\n                    u_i >= 0.0,\n                    u_i <= self.length_set[j][i])\n                # Deal with the case in which t_i is 0 and the site is inside\n                # of the segment\n                idx0 = numpy.logical_and(ti0_check, on_segment_range)\n                # In this null case w_i is ignored - however, null sites on\n                # previous segments would not be null sites on this segment,\n                # so we update the list of null sites\n                on_segment[numpy.logical_or(on_segment, idx0)] = True\n                # Also take care of the U case this time using\n                # equation 12 of Spudich and Chiou\n                s_ij = self.cum_length_set[j][i] + numpy.dot(\n                    (edges[0, :2] - self.p0), self.gc2_config[\"b_hat\"])\n                general_u[idx0] = u_i[idx0] + s_ij\n\n                # In the first case, ti = 0, u_i is outside of the segment\n                # this implements equation 5\n                idx1 = numpy.logical_and(ti0_check,\n                                         numpy.logical_not(on_segment_range))\n                w_i[idx1] = ((1.0 / (u_i[idx1] - self.length_set[j][i])) -\n                             (1.0 / u_i[idx1]))\n\n                # In the last case the site is not on the edge (t != 0)\n                # implements equation 4\n                idx2 = numpy.logical_not(ti0_check)\n                w_i[idx2] = ((1. / t_i[idx2]) * (numpy.arctan(\n                             (self.length_set[j][i] - u_i[idx2]) / t_i[idx2]) -\n                             numpy.arctan(-u_i[idx2] / t_i[idx2])))\n\n                idx = numpy.logical_or(idx1, idx2)\n                # Equation 3\n                sum_w_i[idx] += w_i[idx]\n                # Part of equation 2\n                sum_w_i_t_i[idx] += (w_i[idx] * t_i[idx])\n                # Part of equation 9\n                sum_wi_ui_si[idx] += (w_i[idx] * (u_i[idx] + s_ij))\n\n        # For those sites not on the segment edge itself\n        idx_t = numpy.logical_not(on_segment)\n        general_t[idx_t] = (1.0 / sum_w_i[idx_t]) * sum_w_i_t_i[idx_t]\n        general_u[idx_t] = (1.0 / sum_w_i[idx_t]) * sum_wi_ui_si[idx_t]\n        return general_t, general_u", "response": "This function returns the generalised coordinates of the rupture locations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_rx_distance(self, mesh):\n        # If the GC2 calculations have already been computed (by invoking Ry0\n        # first) and the mesh is identical then class has GC2 attributes\n        # already pre-calculated\n        if not self.tmp_mesh or (self.tmp_mesh == mesh):\n            self.gc2t, self.gc2u = self.get_generalised_coordinates(mesh.lons,\n                                                                    mesh.lats)\n            # Update mesh\n            self.tmp_mesh = deepcopy(mesh)\n        # Rx coordinate is taken directly from gc2t\n        return self.gc2t", "response": "This method calculates the corresponding rx distance using the GC2t and gc2u attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ry0_distance(self, mesh):\n        # If the GC2 calculations have already been computed (by invoking Ry0\n        # first) and the mesh is identical then class has GC2 attributes\n        # already pre-calculated\n        if not self.tmp_mesh or (self.tmp_mesh == mesh):\n            # If that's not the case, or the mesh is different then\n            # re-compute GC2 configuration\n            self.gc2t, self.gc2u = self.get_generalised_coordinates(mesh.lons,\n                                                                    mesh.lats)\n            # Update mesh\n            self.tmp_mesh = deepcopy(mesh)\n\n        # Default value ry0 (for sites within fault length) is 0.0\n        ry0 = numpy.zeros_like(self.gc2u, dtype=float)\n\n        # For sites with negative gc2u (off the initial point of the fault)\n        # take the absolute value of gc2u\n        neg_gc2u = self.gc2u < 0.0\n        ry0[neg_gc2u] = numpy.fabs(self.gc2u[neg_gc2u])\n\n        # Sites off the end of the fault have values shifted by the\n        # GC2 length of the fault\n        pos_gc2u = self.gc2u >= self.gc_length\n        ry0[pos_gc2u] = self.gc2u[pos_gc2u] - self.gc_length\n        return ry0", "response": "This method calculates the Ry0 distance for each point in the fault."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the hazard model from a list of NRML files.", "response": "def from_model_files(cls, limits, input_model, investigation_time=1.0,\n                         simple_mesh_spacing=1.0, complex_mesh_spacing=5.0,\n                         mfd_width=0.1, area_discretisation=10.0):\n        \"\"\"\n        Reads the hazard model from a file\n\n        :param list limits:\n             Grid configuration [west, east, xspc, south, north, yspc,\n                                 upper, lower, zspc]\n        :param str input_model:\n            Path to input source model\n        :param float investigation_time:\n            Investigation time of Poisson model\n        :param float simple_mesh_spacing:\n            Rupture mesh spacing of simple fault (km)\n        :param float complex_mesh_spacing:\n            Rupture mesh spacing of complex fault (km)\n        :param float mfd_width:\n            Spacing (in magnitude units) of MFD\n        :param float area_discretisation:\n            Spacing of discretisation of area source (km)\n        \"\"\"\n        converter = SourceConverter(investigation_time,\n                                    simple_mesh_spacing,\n                                    complex_mesh_spacing,\n                                    mfd_width,\n                                    area_discretisation)\n        sources = []\n        for grp in nrml.to_python(input_model, converter):\n            sources.extend(grp.sources)\n        return cls(limits, sources, area_discretisation)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_rates(self, mmin, mmax=np.inf):\n        nsrcs = self.number_sources()\n        for iloc, source in enumerate(self.source_model):\n            print(\"Source Number %s of %s, Name = %s, Typology = %s\" % (\n                iloc + 1,\n                nsrcs,\n                source.name,\n                source.__class__.__name__))\n            if isinstance(source, CharacteristicFaultSource):\n                self._get_fault_rates(source, mmin, mmax)\n            elif isinstance(source, ComplexFaultSource):\n                self._get_fault_rates(source, mmin, mmax)\n            elif isinstance(source, SimpleFaultSource):\n                self._get_fault_rates(source, mmin, mmax)\n            elif isinstance(source, AreaSource):\n                self._get_area_rates(source, mmin, mmax)\n            elif isinstance(source, PointSource):\n                self._get_point_rates(source, mmin, mmax)\n            else:\n                print(\"Source type %s not recognised - skipping!\" % source)\n                continue", "response": "Returns the cumulative rates greater than Mmin\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_point_location(self, location):\n        if (location.longitude < self.xlim[0]) or\\\n                (location.longitude > self.xlim[-1]):\n            return None, None\n        xloc = int(((location.longitude - self.xlim[0]) / self.xspc) + 1E-7)\n        if (location.latitude < self.ylim[0]) or\\\n                (location.latitude > self.ylim[-1]):\n            return None, None\n        yloc = int(((location.latitude - self.ylim[0]) / self.yspc) + 1E-7)\n        return xloc, yloc", "response": "Returns the location in the output grid corresponding to the cell in\n            which the epicentre lays\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_point_rates(self, source, mmin, mmax=np.inf):\n        xloc, yloc = self._get_point_location(source.location)\n        if (xloc is None) or (yloc is None):\n            return\n        # Get annual rates\n        annual_rate = source.get_annual_occurrence_rates()\n        mags = np.array([val[0] for val in annual_rate])\n        annual_rate = np.array([val[1] for val in annual_rate])\n        idx = np.logical_and(mags >= mmin, mags < mmax)\n        annual_rate = np.sum(annual_rate[idx])\n        for hypo_depth in source.hypocenter_distribution.data:\n            zloc = int((hypo_depth[1] - self.zlim[0]) / self.zspc)\n            if (zloc < 0) or (zloc >= (self.nz - 1)):\n                continue\n            else:\n                self.rates[xloc, yloc, zloc] += float(hypo_depth[0]) * \\\n                    annual_rate", "response": "Adds the rates for a point source to the internal rates array."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the rates from the area source by discretising the source and adding the rates from the point sources to the set of point sources .", "response": "def _get_area_rates(self, source, mmin, mmax=np.inf):\n        \"\"\"\n        Adds the rates from the area source by discretising the source\n        to a set of point sources\n\n        :param source:\n            Area source as instance of :class:\n            openquake.hazardlib.source.area.AreaSource\n        \"\"\"\n        points = list(source)\n        for point in points:\n            self._get_point_rates(point, mmin, mmax)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_fault_rates(self, source, mmin, mmax=np.inf):\n        for rupt in list(source.iter_ruptures()):\n            valid_rupt = (rupt.mag >= mmin) and (rupt.mag < mmax)\n            if not valid_rupt:\n                continue\n            grd = np.column_stack([rupt.surface.mesh.lons.flatten(),\n                                   rupt.surface.mesh.lats.flatten(),\n                                   rupt.surface.mesh.depths.flatten()])\n            npts = np.shape(grd)[0]\n            counter = np.histogramdd(grd,\n                                     bins=[self.xlim, self.ylim, self.zlim]\n                                     )[0]\n            point_rate = rupt.occurrence_rate / float(npts)\n            self.rates += (point_rate * counter)", "response": "Adds the rates for a simple or complex fault source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the rates for a point source.", "response": "def _get_point_rates(self, source, mmin, mmax=np.inf):\n        \"\"\"\n        Adds the rates for a point source\n\n        :param source:\n            Point source as instance of :class:\n            openquake.hazardlib.source.point.PointSource\n        :param float mmin:\n            Minimum Magnitude\n        :param float mmax:\n            Maximum Magnitude\n        \"\"\"\n        src_mesh = Mesh.from_points_list([source.location])\n        in_poly = self.limits.intersects(src_mesh)[0]\n        if not in_poly:\n            return\n        else:\n            for (mag, rate) in source.get_annual_occurrence_rates():\n                if (mag < mmin) or (mag > mmax):\n                    return\n                else:\n                    for (prob, depth) in source.hypocenter_distribution.data:\n                        if (depth < self.upper_depth) or\\\n                                (depth > self.lower_depth):\n                            continue\n                        else:\n                            self.rates += (prob * rate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the rates for a simple or complex fault source.", "response": "def _get_fault_rates(self, source, mmin, mmax=np.inf):\n        \"\"\"\n        Adds the rates for a simple or complex fault source\n\n        :param source:\n            Fault source as instance of :class:\n            openquake.hazardlib.source.simple_fault.SimpleFaultSource or\n            openquake.hazardlib.source.complex_fault.ComplexFaultSource\n        \"\"\"\n        for rup in list(source.iter_ruptures()):\n            if (rup.mag < mmin) or (rup.mag > mmax):\n                # Magnitude outside search range\n                continue\n            depths = rup.surface.mesh.depths.flatten()\n            # Generate simple mesh from surface\n            rupt_mesh = Mesh(rup.surface.mesh.lons.flatten(),\n                             rup.surface.mesh.lats.flatten(),\n                             depths)\n            # Mesh points in polygon\n            in_poly = self.limits.intersects(rupt_mesh)\n            in_depth = np.logical_and(depths >= self.upper_depth,\n                                      depths <= self.lower_depth)\n            idx = np.logical_and(in_poly, in_depth)\n            if np.any(idx):\n                node_rate = rup.occurrence_rate / float(len(depths))\n                self.rates += (node_rate * np.sum(idx))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        C = self.COEFFS[imt]\n        mean = (self._get_magnitude_scaling(C, rup.mag) +\n                self._get_distance_scaling(C, rup.mag, dists.rhypo))\n        if imt.name in \"SA PGA\":\n            mean = np.log(np.exp(mean) / (100.0 * g))\n        stddevs = self._compute_std(C, stddev_types, len(dists.rhypo))\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the distance scalig term", "response": "def _get_distance_scaling(self, C, mag, rhypo):\n        \"\"\"\n        Returns the distance scalig term\n        \"\"\"\n        return (C[\"a3\"] * np.log(rhypo)) + (C[\"a4\"] + C[\"a5\"] * mag) * rhypo"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extract dictionaries of coefficients specific to required\n        # intensity measure type\n        C = self.COEFFS[imt]\n        imean = (self._get_magnitude_scaling_term(C, rup.mag) +\n                 self._get_distance_scaling_term(C, dists.rjb, rup.mag))\n        # convert from cm/s**2 to g for SA and from cm/s**2 to g for PGA (PGV\n        # is already in cm/s) and also convert from base 10 to base e.\n        if imt.name in \"SA PGA\":\n            mean = np.log((10.0 ** (imean - 2.0)) / g)\n        else:\n            mean = np.log(10 ** imean)\n\n        stddevs = self._get_stddevs(C, stddev_types, dists.rjb.shape[0])\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the base 10 of the site."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the distance scaling term defined in equation 10 page 63", "response": "def _get_distance_scaling_term(self, C, rjb, mag):\n        \"\"\"\n        Returns the distance scaling component of the model\n        Equation 10, Page 63\n        \"\"\"\n        # Depth adjusted distance, equation 11 (Page 63)\n        rval = np.sqrt(rjb ** 2.0 + C[\"c11\"] ** 2.0)\n        f_0, f_1, f_2 = self._get_distance_segment_coefficients(rval)\n        return ((C[\"c4\"] + C[\"c5\"] * mag) * f_0 +\n                (C[\"c6\"] + C[\"c7\"] * mag) * f_1 +\n                (C[\"c8\"] + C[\"c9\"] * mag) * f_2 +\n                (C[\"c10\"] * rval))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_distance_segment_coefficients(self, rval):\n        # Get distance segment ends\n        nsites = len(rval)\n        # Equation 12a\n        f_0 = np.log10(self.CONSTS[\"r0\"] / rval)\n        f_0[rval > self.CONSTS[\"r0\"]] = 0.0\n\n        # Equation 12b\n        f_1 = np.log10(rval)\n        f_1[rval > self.CONSTS[\"r1\"]] = np.log10(self.CONSTS[\"r1\"])\n        # Equation 12c\n        f_2 = np.log10(rval / self.CONSTS[\"r2\"])\n        f_2[rval <= self.CONSTS[\"r2\"]] = 0.0\n        return f_0, f_1, f_2", "response": "Returns the coefficients describing the distance attenuation shape shape\n        Equation 12a - 12b - 12c - 12c - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d - 12d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef collect_files(dirpath, cond=lambda fullname: True):\n    files = []\n    for fname in os.listdir(dirpath):\n        fullname = os.path.join(dirpath, fname)\n        if os.path.isdir(fullname):  # navigate inside\n            files.extend(collect_files(fullname))\n        else:  # collect files\n            if cond(fullname):\n                files.append(fullname)\n    return files", "response": "Recursively collect the files contained inside a directory and return them as a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a zip archive and a function to detect the presence of a given filename extract the archive into a temporary directory and return the full path of the file.", "response": "def extract_from_zip(path, candidates):\n    \"\"\"\n    Given a zip archive and a function to detect the presence of a given\n    filename, unzip the archive into a temporary directory and return the\n    full path of the file. Raise an IOError if the file cannot be found\n    within the archive.\n\n    :param path: pathname of the archive\n    :param candidates: list of names to search for\n    \"\"\"\n    temp_dir = tempfile.mkdtemp()\n    with zipfile.ZipFile(path) as archive:\n        archive.extractall(temp_dir)\n    return [f for f in collect_files(temp_dir)\n            if os.path.basename(f) in candidates]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_params(job_inis, **kw):\n    input_zip = None\n    if len(job_inis) == 1 and job_inis[0].endswith('.zip'):\n        input_zip = job_inis[0]\n        job_inis = extract_from_zip(\n            job_inis[0], ['job_hazard.ini', 'job_haz.ini',\n                          'job.ini', 'job_risk.ini'])\n\n    not_found = [ini for ini in job_inis if not os.path.exists(ini)]\n    if not_found:  # something was not found\n        raise IOError('File not found: %s' % not_found[0])\n\n    cp = configparser.ConfigParser()\n    cp.read(job_inis)\n\n    # directory containing the config files we're parsing\n    job_ini = os.path.abspath(job_inis[0])\n    base_path = decode(os.path.dirname(job_ini))\n    params = dict(base_path=base_path, inputs={'job_ini': job_ini})\n    if input_zip:\n        params['inputs']['input_zip'] = os.path.abspath(input_zip)\n\n    for sect in cp.sections():\n        _update(params, cp.items(sect), base_path)\n    _update(params, kw.items(), base_path)  # override on demand\n\n    if params['inputs'].get('reqv'):\n        # using pointsource_distance=0 because of the reqv approximation\n        params['pointsource_distance'] = '0'\n    return params", "response": "Parse one or more INI - style config files and return a dictionary of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_oqparam(job_ini, pkg=None, calculators=None, hc_id=None, validate=1,\n                **kw):\n    \"\"\"\n    Parse a dictionary of parameters from an INI-style config file.\n\n    :param job_ini:\n        Path to configuration file/archive or dictionary of parameters\n    :param pkg:\n        Python package where to find the configuration file (optional)\n    :param calculators:\n        Sequence of calculator names (optional) used to restrict the\n        valid choices for `calculation_mode`\n    :param hc_id:\n        Not None only when called from a post calculation\n    :param validate:\n        Flag. By default it is true and the parameters are validated\n    :param kw:\n        String-valued keyword arguments used to override the job.ini parameters\n    :returns:\n        An :class:`openquake.commonlib.oqvalidation.OqParam` instance\n        containing the validate and casted parameters/values parsed from\n        the job.ini file as well as a subdictionary 'inputs' containing\n        absolute paths to all of the files referenced in the job.ini, keyed by\n        the parameter name.\n    \"\"\"\n    # UGLY: this is here to avoid circular imports\n    from openquake.calculators import base\n\n    OqParam.calculation_mode.validator.choices = tuple(\n        calculators or base.calculators)\n    if not isinstance(job_ini, dict):\n        basedir = os.path.dirname(pkg.__file__) if pkg else ''\n        job_ini = get_params([os.path.join(basedir, job_ini)])\n    if hc_id:\n        job_ini.update(hazard_calculation_id=str(hc_id))\n    job_ini.update(kw)\n    oqparam = OqParam(**job_ini)\n    if validate:\n        oqparam.validate()\n    return oqparam", "response": "Parse a dictionary of parameters from a job. ini file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_csv_header(fname, sep=','):\n    with open(fname, encoding='utf-8-sig') as f:\n        return next(f).split(sep)", "response": "returns the first line of a CSV file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a CSV file and returns a structured array of floats", "response": "def read_csv(fname, sep=','):\n    \"\"\"\n    :param fname: a CSV file with an header and float fields\n    :param sep: separato (default the comma)\n    :return: a structured array of floats\n    \"\"\"\n    with open(fname, encoding='utf-8-sig') as f:\n        header = next(f).strip().split(sep)\n        dt = numpy.dtype([(h, numpy.bool if h == 'vs30measured' else float)\n                          for h in header])\n        return numpy.loadtxt(f, dt, delimiter=sep)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mesh(oqparam):\n    global pmap, exposure, gmfs, eids\n    if 'exposure' in oqparam.inputs and exposure is None:\n        # read it only once\n        exposure = get_exposure(oqparam)\n    if oqparam.sites:\n        return geo.Mesh.from_coords(oqparam.sites)\n    elif 'sites' in oqparam.inputs:\n        fname = oqparam.inputs['sites']\n        header = get_csv_header(fname)\n        if 'lon' in header:\n            data = []\n            for i, row in enumerate(\n                    csv.DictReader(open(fname, encoding='utf-8-sig'))):\n                if header[0] == 'site_id' and row['site_id'] != str(i):\n                    raise InvalidFile('%s: expected site_id=%d, got %s' % (\n                        fname, i, row['site_id']))\n                data.append(' '.join([row['lon'], row['lat']]))\n        elif 'gmfs' in oqparam.inputs:\n            raise InvalidFile('Missing header in %(sites)s' % oqparam.inputs)\n        else:\n            data = [line.replace(',', ' ')\n                    for line in open(fname, encoding='utf-8-sig')]\n        coords = valid.coordinates(','.join(data))\n        start, stop = oqparam.sites_slice\n        c = (coords[start:stop] if header[0] == 'site_id'\n             else sorted(coords[start:stop]))\n        return geo.Mesh.from_coords(c)\n    elif 'hazard_curves' in oqparam.inputs:\n        fname = oqparam.inputs['hazard_curves']\n        if isinstance(fname, list):  # for csv\n            mesh, pmap = get_pmap_from_csv(oqparam, fname)\n        elif fname.endswith('.xml'):\n            mesh, pmap = get_pmap_from_nrml(oqparam, fname)\n        else:\n            raise NotImplementedError('Reading from %s' % fname)\n        return mesh\n    elif 'gmfs' in oqparam.inputs:\n        eids, gmfs = _get_gmfs(oqparam)  # sets oqparam.sites\n        return geo.Mesh.from_coords(oqparam.sites)\n    elif oqparam.region_grid_spacing:\n        if oqparam.region:\n            poly = geo.Polygon.from_wkt(oqparam.region)\n        elif 'site_model' in oqparam.inputs:\n            sm = get_site_model(oqparam)\n            poly = geo.Mesh(sm['lon'], sm['lat']).get_convex_hull()\n        elif exposure:\n            poly = exposure.mesh.get_convex_hull()\n        else:\n            raise InvalidFile('There is a grid spacing but not a region, '\n                              'nor a site model, nor an exposure in %s' %\n                              oqparam.inputs['job_ini'])\n        try:\n            mesh = poly.dilate(oqparam.region_grid_spacing).discretize(\n                oqparam.region_grid_spacing)\n            return geo.Mesh.from_coords(zip(mesh.lons, mesh.lats))\n        except Exception:\n            raise ValueError(\n                'Could not discretize region with grid spacing '\n                '%(region_grid_spacing)s' % vars(oqparam))\n    elif 'exposure' in oqparam.inputs:\n        return exposure.mesh", "response": "Extract the mesh of points from the sites and the hazard curves in the input csv file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_site_model(oqparam):\n    req_site_params = get_gsim_lt(oqparam).req_site_params\n    arrays = []\n    for fname in oqparam.inputs['site_model']:\n        if isinstance(fname, str) and fname.endswith('.csv'):\n            sm = read_csv(fname)\n            if 'site_id' in sm.dtype.names:\n                raise InvalidFile('%s: you passed a sites.csv file instead of '\n                                  'a site_model.csv file!' % fname)\n            z = numpy.zeros(len(sm), sorted(sm.dtype.descr))\n            for name in z.dtype.names:  # reorder the fields\n                z[name] = sm[name]\n            arrays.append(z)\n            continue\n        nodes = nrml.read(fname).siteModel\n        params = [valid.site_param(node.attrib) for node in nodes]\n        missing = req_site_params - set(params[0])\n        if 'vs30measured' in missing:  # use a default of False\n            missing -= {'vs30measured'}\n            for param in params:\n                param['vs30measured'] = False\n        if 'backarc' in missing:  # use a default of False\n            missing -= {'backarc'}\n            for param in params:\n                param['backarc'] = False\n        if missing:\n            raise InvalidFile('%s: missing parameter %s' %\n                              (oqparam.inputs['site_model'],\n                               ', '.join(missing)))\n        # NB: the sorted in sorted(params[0]) is essential, otherwise there is\n        # an heisenbug in scenario/test_case_4\n        site_model_dt = numpy.dtype([(p, site.site_param_dt[p])\n                                     for p in sorted(params[0])])\n        sm = numpy.array([tuple(param[name] for name in site_model_dt.names)\n                          for param in params], site_model_dt)\n        arrays.append(sm)\n    return numpy.concatenate(arrays)", "response": "Convert the NRML file into an array of fields lon lat vs30 vs30"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_site_collection(oqparam):\n    mesh = get_mesh(oqparam)\n    req_site_params = get_gsim_lt(oqparam).req_site_params\n    if oqparam.inputs.get('site_model'):\n        sm = get_site_model(oqparam)\n        try:\n            # in the future we could have elevation in the site model\n            depth = sm['depth']\n        except ValueError:\n            # this is the normal case\n            depth = None\n        sitecol = site.SiteCollection.from_points(\n            sm['lon'], sm['lat'], depth, sm, req_site_params)\n        if oqparam.region_grid_spacing:\n            logging.info('Reducing the grid sites to the site '\n                         'parameters within the grid spacing')\n            sitecol, params, _ = geo.utils.assoc(\n                sm, sitecol, oqparam.region_grid_spacing * 1.414, 'filter')\n            sitecol.make_complete()\n        else:\n            params = sm\n        for name in req_site_params:\n            if name in ('vs30measured', 'backarc') \\\n                   and name not in params.dtype.names:\n                sitecol._set(name, 0)  # the default\n            else:\n                sitecol._set(name, params[name])\n    elif mesh is None and oqparam.ground_motion_fields:\n        raise InvalidFile('You are missing sites.csv or site_model.csv in %s'\n                          % oqparam.inputs['job_ini'])\n    elif mesh is None:\n        # a None sitecol is okay when computing the ruptures only\n        return\n    else:  # use the default site params\n        sitecol = site.SiteCollection.from_points(\n            mesh.lons, mesh.lats, mesh.depths, oqparam, req_site_params)\n    ss = os.environ.get('OQ_SAMPLE_SITES')\n    if ss:\n        # debugging tip to reduce the size of a calculation\n        # OQ_SAMPLE_SITES=.1 oq engine --run job.ini\n        # will run a computation with 10 times less sites\n        sitecol.array = numpy.array(random_filter(sitecol.array, float(ss)))\n        sitecol.make_complete()\n    return sitecol", "response": "Returns a SiteCollection instance by looking at the points and the site model defined by the configuration parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_gsim_lt(oqparam, trts=['*']):\n    if 'gsim_logic_tree' not in oqparam.inputs:\n        return logictree.GsimLogicTree.from_(oqparam.gsim)\n    gsim_file = os.path.join(\n        oqparam.base_path, oqparam.inputs['gsim_logic_tree'])\n    gsim_lt = logictree.GsimLogicTree(gsim_file, trts)\n    gmfcorr = oqparam.correl_model\n    for trt, gsims in gsim_lt.values.items():\n        for gsim in gsims:\n            if gmfcorr and (gsim.DEFINED_FOR_STANDARD_DEVIATION_TYPES ==\n                            {StdDev.TOTAL}):\n                raise CorrelationButNoInterIntraStdDevs(gmfcorr, gsim)\n    trts = set(oqparam.minimum_magnitude) - {'default'}\n    expected_trts = set(gsim_lt.values)\n    assert trts <= expected_trts, (trts, expected_trts)\n    imt_dep_w = any(len(branch.weight.dic) > 1 for branch in gsim_lt.branches)\n    if oqparam.number_of_logic_tree_samples and imt_dep_w:\n        raise NotImplementedError('IMT-dependent weights in the logic tree '\n                                  'do not work with sampling!')\n    return gsim_lt", "response": "Get the logic tree for the given tectonic region types."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an ordered dictionary gsim -> realization index. Work for gsim logic trees with a single tectonic region type.", "response": "def get_rlzs_by_gsim(oqparam):\n    \"\"\"\n    Return an ordered dictionary gsim -> [realization index]. Work for\n    gsim logic trees with a single tectonic region type.\n    \"\"\"\n    cinfo = source.CompositionInfo.fake(get_gsim_lt(oqparam))\n    ra = cinfo.get_rlzs_assoc()\n    dic = {}\n    for rlzi, gsim_by_trt in enumerate(ra.gsim_by_trt):\n        dic[gsim_by_trt['*']] = [rlzi]\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the rupture_model file and by filter the site collection rupture by the site collection rupture_mesh_spacing and hazardlib_complex_fault_mesh_spacing parameters.", "response": "def get_rupture(oqparam):\n    \"\"\"\n    Read the `rupture_model` file and by filter the site collection\n\n    :param oqparam:\n        an :class:`openquake.commonlib.oqvalidation.OqParam` instance\n    :returns:\n        an hazardlib rupture\n    \"\"\"\n    rup_model = oqparam.inputs['rupture_model']\n    [rup_node] = nrml.read(rup_model)\n    conv = sourceconverter.RuptureConverter(\n        oqparam.rupture_mesh_spacing, oqparam.complex_fault_mesh_spacing)\n    rup = conv.convert_node(rup_node)\n    rup.tectonic_region_type = '*'  # there is not TRT for scenario ruptures\n    rup.serial = oqparam.random_seed\n    return rup"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new instance of the source model logic tree that is less than the current one.", "response": "def get_source_model_lt(oqparam, validate=True):\n    \"\"\"\n    :param oqparam:\n        an :class:`openquake.commonlib.oqvalidation.OqParam` instance\n    :returns:\n        a :class:`openquake.commonlib.logictree.SourceModelLogicTree`\n        instance\n    \"\"\"\n    fname = oqparam.inputs.get('source_model_logic_tree')\n    if fname:\n        # NB: converting the random_seed into an integer is needed on Windows\n        return logictree.SourceModelLogicTree(\n            fname, validate, seed=int(oqparam.random_seed),\n            num_samples=oqparam.number_of_logic_tree_samples)\n    return logictree.FakeSmlt(oqparam.inputs['source_model'],\n                              int(oqparam.random_seed),\n                              oqparam.number_of_logic_tree_samples)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the source model contains nonparametric sources.", "response": "def check_nonparametric_sources(fname, smodel, investigation_time):\n    \"\"\"\n    :param fname:\n        full path to a source model file\n    :param smodel:\n        source model object\n    :param investigation_time:\n        investigation_time to compare with in the case of\n        nonparametric sources\n    :returns:\n        the nonparametric sources in the model\n    :raises:\n        a ValueError if the investigation_time is different from the expected\n    \"\"\"\n    # NonParametricSeismicSources\n    np = [src for sg in smodel.src_groups for src in sg\n          if hasattr(src, 'data')]\n    if np and smodel.investigation_time != investigation_time:\n        raise ValueError(\n            'The source model %s contains an investigation_time '\n            'of %s, while the job.ini has %s' % (\n                fname, smodel.investigation_time, investigation_time))\n    return np"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring a source model in the cache_XXX. hdf5 file.", "response": "def store_sm(smodel, filename, monitor):\n    \"\"\"\n    :param smodel: a :class:`openquake.hazardlib.nrml.SourceModel` instance\n    :param filename: path to an hdf5 file (cache_XXX.hdf5)\n    :param monitor: a Monitor instance with an .hdf5 attribute\n    \"\"\"\n    h5 = monitor.hdf5\n    with monitor('store source model'):\n        sources = h5['source_info']\n        source_geom = h5['source_geom']\n        gid = len(source_geom)\n        for sg in smodel:\n            if filename:\n                with hdf5.File(filename, 'r+') as hdf5cache:\n                    hdf5cache['grp-%02d' % sg.id] = sg\n            srcs = []\n            geoms = []\n            for src in sg:\n                srcgeom = src.geom()\n                n = len(srcgeom)\n                geom = numpy.zeros(n, point3d)\n                geom['lon'], geom['lat'], geom['depth'] = srcgeom.T\n                srcs.append((sg.id, src.source_id, src.code, gid, gid + n,\n                             src.num_ruptures, 0, 0, 0))\n                geoms.append(geom)\n                gid += n\n            if geoms:\n                hdf5.extend(source_geom, numpy.concatenate(geoms))\n            if sources:\n                hdf5.extend(sources, numpy.array(srcs, source_info_dt))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_source_models(oqparam, gsim_lt, source_model_lt, monitor,\n                      in_memory=True, srcfilter=None):\n    \"\"\"\n    Build all the source models generated by the logic tree.\n\n    :param oqparam:\n        an :class:`openquake.commonlib.oqvalidation.OqParam` instance\n    :param gsim_lt:\n        a :class:`openquake.commonlib.logictree.GsimLogicTree` instance\n    :param source_model_lt:\n        a :class:`openquake.commonlib.logictree.SourceModelLogicTree` instance\n    :param monitor:\n        a `openquake.baselib.performance.Monitor` instance\n    :param in_memory:\n        if True, keep in memory the sources, else just collect the TRTs\n    :param srcfilter:\n        a SourceFilter instance with an .filename pointing to the cache file\n    :returns:\n        an iterator over :class:`openquake.commonlib.logictree.LtSourceModel`\n        tuples\n    \"\"\"\n    make_sm = SourceModelFactory()\n    spinning_off = oqparam.pointsource_distance == {'default': 0.0}\n    if spinning_off:\n        logging.info('Removing nodal plane and hypocenter distributions')\n    dist = 'no' if os.environ.get('OQ_DISTRIBUTE') == 'no' else 'processpool'\n    smlt_dir = os.path.dirname(source_model_lt.filename)\n    converter = sourceconverter.SourceConverter(\n        oqparam.investigation_time,\n        oqparam.rupture_mesh_spacing,\n        oqparam.complex_fault_mesh_spacing,\n        oqparam.width_of_mfd_bin,\n        oqparam.area_source_discretization,\n        oqparam.minimum_magnitude,\n        not spinning_off,\n        oqparam.source_id)\n    if oqparam.calculation_mode.startswith('ucerf'):\n        [grp] = nrml.to_python(oqparam.inputs[\"source_model\"], converter)\n    elif in_memory:\n        logging.info('Reading the source model(s) in parallel')\n        smap = parallel.Starmap(\n            nrml.read_source_models, monitor=monitor, distribute=dist)\n        for sm in source_model_lt.gen_source_models(gsim_lt):\n            for name in sm.names.split():\n                fname = os.path.abspath(os.path.join(smlt_dir, name))\n                smap.submit([fname], converter)\n        dic = {sm.fname: sm for sm in smap}\n\n    # consider only the effective realizations\n    nr = 0\n    idx = 0\n    grp_id = 0\n    if monitor.hdf5:\n        sources = hdf5.create(monitor.hdf5, 'source_info', source_info_dt)\n        hdf5.create(monitor.hdf5, 'source_geom', point3d)\n        filename = (getattr(srcfilter, 'filename', None)\n                    if oqparam.prefilter_sources == 'no' else None)\n    source_ids = set()\n    for sm in source_model_lt.gen_source_models(gsim_lt):\n        apply_unc = functools.partial(\n            source_model_lt.apply_uncertainties, sm.path)\n        src_groups = []\n        for name in sm.names.split():\n            fname = os.path.abspath(os.path.join(smlt_dir, name))\n            if oqparam.calculation_mode.startswith('ucerf'):\n                sg = copy.copy(grp)\n                sg.id = grp_id\n                src = sg[0].new(sm.ordinal, sm.names)  # one source\n                source_ids.add(src.source_id)\n                src.src_group_id = grp_id\n                src.id = idx\n                if oqparam.number_of_logic_tree_samples:\n                    src.samples = sm.samples\n                sg.sources = [src]\n                src_groups.append(sg)\n                idx += 1\n                grp_id += 1\n                data = [((sg.id, src.source_id, src.code, 0, 0,\n                         src.num_ruptures, 0, 0, 0))]\n                hdf5.extend(sources, numpy.array(data, source_info_dt))\n            elif in_memory:\n                newsm = make_sm(fname, dic[fname], apply_unc,\n                                oqparam.investigation_time)\n                for sg in newsm:\n                    nr += sum(src.num_ruptures for src in sg)\n                    # sample a source for each group\n                    if os.environ.get('OQ_SAMPLE_SOURCES'):\n                        sg.sources = random_filtered_sources(\n                            sg.sources, srcfilter, sg.id + oqparam.random_seed)\n                    for src in sg:\n                        source_ids.add(src.source_id)\n                        src.src_group_id = grp_id\n                        src.id = idx\n                        idx += 1\n                    sg.id = grp_id\n                    grp_id += 1\n                    src_groups.append(sg)\n                if monitor.hdf5:\n                    store_sm(newsm, filename, monitor)\n            else:  # just collect the TRT models\n                groups = logictree.read_source_groups(fname)\n                for group in groups:\n                    source_ids.update(src['id'] for src in group)\n                src_groups.extend(groups)\n\n        if grp_id >= TWO16:\n            # the limit is really needed only for event based calculations\n            raise ValueError('There is a limit of %d src groups!' % TWO16)\n\n        for brid, srcids in source_model_lt.info.applytosources.items():\n            for srcid in srcids:\n                if srcid not in source_ids:\n                    raise ValueError(\n                        'The source %s is not in the source model, please fix '\n                        'applyToSources in %s or the source model' %\n                        (srcid, source_model_lt.filename))\n        num_sources = sum(len(sg.sources) for sg in src_groups)\n        sm.src_groups = src_groups\n        trts = [mod.trt for mod in src_groups]\n        source_model_lt.tectonic_region_types.update(trts)\n        logging.info(\n            'Processed source model %d with %d gsim path(s) and %d '\n            'sources', sm.ordinal + 1, sm.num_gsim_paths, num_sources)\n\n        gsim_file = oqparam.inputs.get('gsim_logic_tree')\n        if gsim_file:  # check TRTs\n            for src_group in src_groups:\n                if src_group.trt not in gsim_lt.values:\n                    raise ValueError(\n                        \"Found in %r a tectonic region type %r inconsistent \"\n                        \"with the ones in %r\" % (sm, src_group.trt, gsim_file))\n        yield sm\n\n    logging.info('The composite source model has {:,d} ruptures'.format(nr))\n\n    # log if some source file is being used more than once\n    dupl = 0\n    for fname, hits in make_sm.fname_hits.items():\n        if hits > 1:\n            logging.info('%s has been considered %d times', fname, hits)\n            if not make_sm.changes:\n                dupl += hits\n    if (dupl and not oqparam.optimize_same_id_sources and\n            not oqparam.is_event_based()):\n        logging.warning(\n            'You are doing redundant calculations: please make sure '\n            'that different sources have different IDs and set '\n            'optimize_same_id_sources=true in your .ini file')\n    if make_sm.changes:\n        logging.info('Applied %d changes to the composite source model',\n                     make_sm.changes)", "response": "Build all the source models for a given source model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef random_filtered_sources(sources, srcfilter, seed):\n    random.seed(seed)\n    while sources:\n        src = random.choice(sources)\n        if srcfilter.get_close_sites(src) is not None:\n            return [src]\n        sources.remove(src)\n    return []", "response": "Returns a list of random sources with a single filtered source"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the XML and build a composite source model.", "response": "def get_composite_source_model(oqparam, monitor=None, in_memory=True,\n                               srcfilter=SourceFilter(None, {})):\n    \"\"\"\n    Parse the XML and build a complete composite source model in memory.\n\n    :param oqparam:\n        an :class:`openquake.commonlib.oqvalidation.OqParam` instance\n    :param monitor:\n         a `openquake.baselib.performance.Monitor` instance\n    :param in_memory:\n        if False, just parse the XML without instantiating the sources\n    :param srcfilter:\n        if not None, use it to prefilter the sources\n    \"\"\"\n    ucerf = oqparam.calculation_mode.startswith('ucerf')\n    source_model_lt = get_source_model_lt(oqparam, validate=not ucerf)\n    trts = source_model_lt.tectonic_region_types\n    trts_lower = {trt.lower() for trt in trts}\n    reqv = oqparam.inputs.get('reqv', {})\n    for trt in reqv:  # these are lowercase because they come from the job.ini\n        if trt not in trts_lower:\n            raise ValueError('Unknown TRT=%s in %s [reqv]' %\n                             (trt, oqparam.inputs['job_ini']))\n    gsim_lt = get_gsim_lt(oqparam, trts or ['*'])\n    p = source_model_lt.num_paths * gsim_lt.get_num_paths()\n    if oqparam.number_of_logic_tree_samples:\n        logging.info('Considering {:,d} logic tree paths out of {:,d}'.format(\n            oqparam.number_of_logic_tree_samples, p))\n    else:  # full enumeration\n        if oqparam.is_event_based() and p > oqparam.max_potential_paths:\n            raise ValueError(\n                'There are too many potential logic tree paths (%d) '\n                'use sampling instead of full enumeration' % p)\n        logging.info('Potential number of logic tree paths = {:,d}'.format(p))\n\n    if source_model_lt.on_each_source:\n        logging.info('There is a logic tree on each source')\n    if monitor is None:\n        monitor = performance.Monitor()\n    smodels = []\n    for source_model in get_source_models(\n            oqparam, gsim_lt, source_model_lt, monitor, in_memory, srcfilter):\n        for src_group in source_model.src_groups:\n            src_group.sources = sorted(src_group, key=getid)\n            for src in src_group:\n                # there are two cases depending on the flag in_memory:\n                # 1) src is a hazardlib source and has a src_group_id\n                #    attribute; in that case the source has to be numbered\n                # 2) src is a Node object, then nothing must be done\n                if isinstance(src, Node):\n                    continue\n        smodels.append(source_model)\n    csm = source.CompositeSourceModel(gsim_lt, source_model_lt, smodels,\n                                      oqparam.optimize_same_id_sources)\n    for sm in csm.source_models:\n        counter = collections.Counter()\n        for sg in sm.src_groups:\n            for srcid in map(getid, sg):\n                counter[srcid] += 1\n        dupl = [srcid for srcid in counter if counter[srcid] > 1]\n        if dupl:\n            raise nrml.DuplicatedID('Found duplicated source IDs in %s: %s'\n                                    % (sm, dupl))\n    if not in_memory:\n        return csm\n\n    if oqparam.is_event_based():\n        # initialize the rupture serial numbers before splitting/filtering; in\n        # this way the serials are independent from the site collection\n        csm.init_serials(oqparam.ses_seed)\n\n    if oqparam.disagg_by_src:\n        csm = csm.grp_by_src()  # one group per source\n\n    csm.info.gsim_lt.check_imts(oqparam.imtls)\n    parallel.Starmap.shutdown()  # save memory\n    return csm"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_risk_model(oqparam):\n    tmap = _get_taxonomy_mapping(oqparam.inputs)\n    fragdict = get_risk_models(oqparam, 'fragility')\n    vulndict = get_risk_models(oqparam, 'vulnerability')\n    consdict = get_risk_models(oqparam, 'consequence')\n    if not tmap:  # the risk ids are the taxonomies already\n        d = dict(ids=['?'], weights=[1.0])\n        for risk_id in set(fragdict) | set(vulndict) | set(consdict):\n            tmap[risk_id] = dict(\n                fragility=d, consequence=d, vulnerability=d)\n        for risk_id in consdict:\n            cdict, fdict = consdict[risk_id], fragdict[risk_id]\n            for loss_type, _ in cdict:\n                c = cdict[loss_type, 'consequence']\n                f = fdict[loss_type, 'fragility']\n                csq_dmg_states = len(c.params)\n                if csq_dmg_states != len(f):\n                    raise ValueError(\n                        'The damage states in %s are different from the '\n                        'damage states in the fragility functions, %s'\n                        % (c, fragdict.limit_states))\n    dic = {}\n    dic.update(fragdict)\n    dic.update(vulndict)\n    oqparam.set_risk_imtls(dic)\n    if oqparam.calculation_mode.endswith('_bcr'):\n        retro = get_risk_models(oqparam, 'vulnerability_retrofitted')\n    else:\n        retro = {}\n    return riskinput.CompositeRiskModel(\n        oqparam, tmap, fragdict, vulndict, consdict, retro)", "response": "Returns a composite risk model for the given oqparam."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_exposure(oqparam):\n    exposure = asset.Exposure.read(\n        oqparam.inputs['exposure'], oqparam.calculation_mode,\n        oqparam.region, oqparam.ignore_missing_costs,\n        by_country='country' in oqparam.aggregate_by)\n    exposure.mesh, exposure.assets_by_site = exposure.get_mesh_assets_by_site()\n    return exposure", "response": "Read the full exposure in memory and build a list of AssetCollection\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sitecol_assetcol(oqparam, haz_sitecol=None, cost_types=()):\n    global exposure\n    asset_hazard_distance = oqparam.asset_hazard_distance['default']\n    if exposure is None:\n        # haz_sitecol not extracted from the exposure\n        exposure = get_exposure(oqparam)\n    if haz_sitecol is None:\n        haz_sitecol = get_site_collection(oqparam)\n    if oqparam.region_grid_spacing:\n        haz_distance = oqparam.region_grid_spacing * 1.414\n        if haz_distance != asset_hazard_distance:\n            logging.info('Using asset_hazard_distance=%d km instead of %d km',\n                         haz_distance, asset_hazard_distance)\n    else:\n        haz_distance = asset_hazard_distance\n\n    if haz_sitecol.mesh != exposure.mesh:\n        # associate the assets to the hazard sites\n        sitecol, assets_by, discarded = geo.utils.assoc(\n            exposure.assets_by_site, haz_sitecol,\n            haz_distance, 'filter', exposure.asset_refs)\n        assets_by_site = [[] for _ in sitecol.complete.sids]\n        num_assets = 0\n        for sid, assets in zip(sitecol.sids, assets_by):\n            assets_by_site[sid] = assets\n            num_assets += len(assets)\n        logging.info(\n            'Associated %d assets to %d sites', num_assets, len(sitecol))\n    else:\n        # asset sites and hazard sites are the same\n        sitecol = haz_sitecol\n        assets_by_site = exposure.assets_by_site\n        discarded = []\n        logging.info('Read %d sites and %d assets from the exposure',\n                     len(sitecol), sum(len(a) for a in assets_by_site))\n    assetcol = asset.AssetCollection(\n        exposure, assets_by_site, oqparam.time_event)\n    if assetcol.occupancy_periods:\n        missing = set(cost_types) - set(exposure.cost_types['name']) - set(\n            ['occupants'])\n    else:\n        missing = set(cost_types) - set(exposure.cost_types['name'])\n    if missing and not oqparam.calculation_mode.endswith('damage'):\n        raise InvalidFile('The exposure %s is missing %s' %\n                          (oqparam.inputs['exposure'], missing))\n    if (not oqparam.hazard_calculation_id and 'gmfs' not in oqparam.inputs\n            and 'hazard_curves' not in oqparam.inputs\n            and sitecol is not sitecol.complete):\n        assetcol = assetcol.reduce_also(sitecol)\n    return sitecol, assetcol, discarded", "response": "Get the site collection and asset collection for the given calculation parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_pmap_from_csv(oqparam, fnames):\n    if not oqparam.imtls:\n        oqparam.set_risk_imtls(get_risk_models(oqparam))\n    if not oqparam.imtls:\n        raise ValueError('Missing intensity_measure_types_and_levels in %s'\n                         % oqparam.inputs['job_ini'])\n\n    dic = {wrapper.imt: wrapper.array\n           for wrapper in map(writers.read_composite_array, fnames)}\n    array = dic[next(iter(dic))]\n    mesh = geo.Mesh(array['lon'], array['lat'])\n    num_levels = sum(len(imls) for imls in oqparam.imtls.values())\n    data = numpy.zeros((len(mesh), num_levels))\n    level = 0\n    for im in oqparam.imtls:\n        arr = dic[im]\n        for poe in arr.dtype.names[3:]:\n            data[:, level] = arr[poe]\n            level += 1\n        for field in ('lon', 'lat', 'depth'):  # sanity check\n            numpy.testing.assert_equal(arr[field], array[field])\n    return mesh, ProbabilityMap.from_array(data, range(len(mesh)))", "response": "Reads the. csv files and returns a ProbabilityMap object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread an NRML file containing hazard curves and returns a mesh and array containing the hazard curves.", "response": "def get_pmap_from_nrml(oqparam, fname):\n    \"\"\"\n    :param oqparam:\n        an :class:`openquake.commonlib.oqvalidation.OqParam` instance\n    :param fname:\n        an XML file containing hazard curves\n    :returns:\n        site mesh, curve array\n    \"\"\"\n    hcurves_by_imt = {}\n    oqparam.hazard_imtls = imtls = {}\n    for hcurves in nrml.read(fname):\n        imt = hcurves['IMT']\n        oqparam.investigation_time = hcurves['investigationTime']\n        if imt == 'SA':\n            imt += '(%s)' % hcurves['saPeriod']\n        imtls[imt] = ~hcurves.IMLs\n        data = sorted((~node.Point.pos, ~node.poEs) for node in hcurves[1:])\n        hcurves_by_imt[imt] = numpy.array([d[1] for d in data])\n    lons, lats = [], []\n    for xy, poes in data:\n        lons.append(xy[0])\n        lats.append(xy[1])\n    mesh = geo.Mesh(numpy.array(lons), numpy.array(lats))\n    num_levels = sum(len(v) for v in imtls.values())\n    array = numpy.zeros((len(mesh), num_levels))\n    imtls = DictArray(imtls)\n    for imt_ in hcurves_by_imt:\n        array[:, imtls(imt_)] = hcurves_by_imt[imt_]\n    return mesh, ProbabilityMap.from_array(array, range(len(mesh)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_scenario_from_nrml(oqparam, fname):\n    if not oqparam.imtls:\n        oqparam.set_risk_imtls(get_risk_models(oqparam))\n    imts = sorted(oqparam.imtls)\n    num_imts = len(imts)\n    imt_dt = numpy.dtype([(imt, F32) for imt in imts])\n    gmfset = nrml.read(fname).gmfCollection.gmfSet\n    eids, sitecounts = _extract_eids_sitecounts(gmfset)\n    coords = sorted(sitecounts)\n    oqparam.sites = [(lon, lat, 0) for lon, lat in coords]\n    site_idx = {lonlat: i for i, lonlat in enumerate(coords)}\n    oqparam.number_of_ground_motion_fields = num_events = len(eids)\n    num_sites = len(oqparam.sites)\n    gmf_by_imt = numpy.zeros((num_events, num_sites), imt_dt)\n    counts = collections.Counter()\n    for i, gmf in enumerate(gmfset):\n        if len(gmf) != num_sites:  # there must be one node per site\n            raise InvalidFile('Expected %d sites, got %d nodes in %s, line %d'\n                              % (num_sites, len(gmf), fname, gmf.lineno))\n        counts[gmf['ruptureId']] += 1\n        imt = gmf['IMT']\n        if imt == 'SA':\n            imt = 'SA(%s)' % gmf['saPeriod']\n        for node in gmf:\n            sid = site_idx[node['lon'], node['lat']]\n            gmf_by_imt[imt][i % num_events, sid] = node['gmv']\n\n    for rupid, count in sorted(counts.items()):\n        if count < num_imts:\n            raise InvalidFile(\"Found a missing ruptureId %d in %s\" %\n                              (rupid, fname))\n        elif count > num_imts:\n            raise InvalidFile(\"Found a duplicated ruptureId '%s' in %s\" %\n                              (rupid, fname))\n    expected_gmvs_per_site = num_imts * len(eids)\n    for lonlat, counts in sitecounts.items():\n        if counts != expected_gmvs_per_site:\n            raise InvalidFile(\n                '%s: expected %d gmvs at location %s, found %d' %\n                (fname, expected_gmvs_per_site, lonlat, counts))\n    return eids, gmf_by_imt.T", "response": "This function extracts the scenario from an NRML file and returns the eids and gmf arrays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mesh_hcurves(oqparam):\n    imtls = oqparam.imtls\n    lon_lats = set()\n    data = AccumDict()  # imt -> list of arrays\n    ncols = len(imtls) + 1  # lon_lat + curve_per_imt ...\n    csvfile = oqparam.inputs['hazard_curves']\n    for line, row in enumerate(csv.reader(csvfile), 1):\n        try:\n            if len(row) != ncols:\n                raise ValueError('Expected %d columns, found %d' %\n                                 ncols, len(row))\n            x, y = row[0].split()\n            lon_lat = valid.longitude(x), valid.latitude(y)\n            if lon_lat in lon_lats:\n                raise DuplicatedPoint(lon_lat)\n            lon_lats.add(lon_lat)\n            for i, imt_ in enumerate(imtls, 1):\n                values = valid.decreasing_probabilities(row[i])\n                if len(values) != len(imtls[imt_]):\n                    raise ValueError('Found %d values, expected %d' %\n                                     (len(values), len(imtls([imt_]))))\n                data += {imt_: [numpy.array(values)]}\n        except (ValueError, DuplicatedPoint) as err:\n            raise err.__class__('%s: file %s, line %d' % (err, csvfile, line))\n    lons, lats = zip(*sorted(lon_lats))\n    mesh = geo.Mesh(numpy.array(lons), numpy.array(lats))\n    return mesh, {imt: numpy.array(lst) for imt, lst in data.items()}", "response": "Reads the CSV file and returns a mesh of points and data as a dictionary of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreduces the source model of a single source model.", "response": "def reduce_source_model(smlt_file, source_ids, remove=True):\n    \"\"\"\n    Extract sources from the composite source model\n    \"\"\"\n    found = 0\n    to_remove = []\n    for paths in logictree.collect_info(smlt_file).smpaths.values():\n        for path in paths:\n            logging.info('Reading %s', path)\n            root = nrml.read(path)\n            model = Node('sourceModel', root[0].attrib)\n            origmodel = root[0]\n            if root['xmlns'] == 'http://openquake.org/xmlns/nrml/0.4':\n                for src_node in origmodel:\n                    if src_node['id'] in source_ids:\n                        model.nodes.append(src_node)\n            else:  # nrml/0.5\n                for src_group in origmodel:\n                    sg = copy.copy(src_group)\n                    sg.nodes = []\n                    weights = src_group.get('srcs_weights')\n                    if weights:\n                        assert len(weights) == len(src_group.nodes)\n                    else:\n                        weights = [1] * len(src_group.nodes)\n                    src_group['srcs_weights'] = reduced_weigths = []\n                    for src_node, weight in zip(src_group, weights):\n                        if src_node['id'] in source_ids:\n                            found += 1\n                            sg.nodes.append(src_node)\n                            reduced_weigths.append(weight)\n                    if sg.nodes:\n                        model.nodes.append(sg)\n            shutil.copy(path, path + '.bak')\n            if model:\n                with open(path, 'wb') as f:\n                    nrml.write([model], f, xmlns=root['xmlns'])\n            elif remove:  # remove the files completely reduced\n                to_remove.append(path)\n    if found:\n        for path in to_remove:\n            os.remove(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the list of input files in a specific order.", "response": "def get_input_files(oqparam, hazard=False):\n    \"\"\"\n    :param oqparam: an OqParam instance\n    :param hazard: if True, consider only the hazard files\n    :returns: input path names in a specific order\n    \"\"\"\n    fnames = []  # files entering in the checksum\n    for key in oqparam.inputs:\n        fname = oqparam.inputs[key]\n        if hazard and key not in ('site_model', 'source_model_logic_tree',\n                                  'gsim_logic_tree', 'source'):\n            continue\n        # collect .hdf5 tables for the GSIMs, if any\n        elif key == 'gsim_logic_tree':\n            gsim_lt = get_gsim_lt(oqparam)\n            for gsims in gsim_lt.values.values():\n                for gsim in gsims:\n                    table = getattr(gsim, 'GMPE_TABLE', None)\n                    if table:\n                        fnames.append(table)\n            fnames.append(fname)\n        elif key == 'source_model':  # UCERF\n            f = oqparam.inputs['source_model']\n            fnames.append(f)\n            fname = nrml.read(f).sourceModel.UCERFSource['filename']\n            fnames.append(os.path.join(os.path.dirname(f), fname))\n        elif key == 'exposure':  # fname is a list\n            for exp in asset.Exposure.read_headers(fname):\n                fnames.extend(exp.datafiles)\n            fnames.extend(fname)\n        elif isinstance(fname, dict):\n            fnames.extend(fname.values())\n        elif isinstance(fname, list):\n            for f in fname:\n                if f == oqparam.input_dir:\n                    raise InvalidFile('%s there is an empty path in %s' %\n                                      (oqparam.inputs['job_ini'], key))\n            fnames.extend(fname)\n        elif key == 'source_model_logic_tree':\n            for smpaths in logictree.collect_info(fname).smpaths.values():\n                fnames.extend(smpaths)\n            fnames.append(fname)\n        else:\n            fnames.append(fname)\n    return sorted(fnames)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding an unsigned 32 bit integer from the input files of a calculation.", "response": "def get_checksum32(oqparam, hazard=False):\n    \"\"\"\n    Build an unsigned 32 bit integer from the input files of a calculation.\n\n    :param oqparam: an OqParam instance\n    :param hazard: if True, consider only the hazard files\n    :returns: the checkume\n    \"\"\"\n    # NB: using adler32 & 0xffffffff is the documented way to get a checksum\n    # which is the same between Python 2 and Python 3\n    checksum = 0\n    for fname in get_input_files(oqparam, hazard):\n        checksum = _checksum(fname, checksum)\n    if hazard:\n        hazard_params = []\n        for key, val in vars(oqparam).items():\n            if key in ('rupture_mesh_spacing', 'complex_fault_mesh_spacing',\n                       'width_of_mfd_bin', 'area_source_discretization',\n                       'random_seed', 'ses_seed', 'truncation_level',\n                       'maximum_distance', 'investigation_time',\n                       'number_of_logic_tree_samples', 'imtls',\n                       'ses_per_logic_tree_path', 'minimum_magnitude',\n                       'prefilter_sources', 'sites',\n                       'pointsource_distance', 'filter_distance'):\n                hazard_params.append('%s = %s' % (key, val))\n        data = '\\n'.join(hazard_params).encode('utf8')\n        checksum = zlib.adler32(data, checksum) & 0xffffffff\n    return checksum"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the database to a file.", "response": "def smart_save(dbpath, archive, calc_id):\n    \"\"\"\n    Make a copy of the db, remove the incomplete jobs and add the copy\n    to the archive\n    \"\"\"\n    tmpdir = tempfile.mkdtemp()\n    newdb = os.path.join(tmpdir, os.path.basename(dbpath))\n    shutil.copy(dbpath, newdb)\n    try:\n        with sqlite3.connect(newdb) as conn:\n            conn.execute('DELETE FROM job WHERE status != \"complete\"')\n            if calc_id:\n                conn.execute('DELETE FROM job WHERE id != %d' % calc_id)\n    except:\n        safeprint('Please check the copy of the db in %s' % newdb)\n        raise\n    zipfiles([newdb], archive, 'a', safeprint)\n    shutil.rmtree(tmpdir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndump the database and all the complete calculations into a zip file.", "response": "def dump(archive, calc_id=0, user=None):\n    \"\"\"\n    Dump the openquake database and all the complete calculations into a zip\n    file. In a multiuser installation must be run as administrator.\n    \"\"\"\n    t0 = time.time()\n    assert archive.endswith('.zip'), archive\n    getfnames = 'select ds_calc_dir || \".hdf5\" from job where ?A'\n    param = dict(status='complete')\n    if calc_id:\n        param['id'] = calc_id\n    if user:\n        param['user_name'] = user\n    fnames = [f for f, in db(getfnames, param) if os.path.exists(f)]\n    zipfiles(fnames, archive, 'w', safeprint)\n    pending_jobs = db('select id, status, description from job '\n                      'where status=\"executing\"')\n    if pending_jobs:\n        safeprint('WARNING: there were calculations executing during the dump,'\n                  ' they have been not copied')\n        for job_id, status, descr in pending_jobs:\n            safeprint('%d %s %s' % (job_id, status, descr))\n\n    # this also checks that the copied db is not corrupted\n    smart_save(db.path, archive, calc_id)\n\n    dt = time.time() - t0\n    safeprint('Archived %d calculations into %s in %d seconds'\n              % (len(fnames), archive, dt))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cumulative_value(self, slip, mmax, mag_value, bbar, dbar, beta):\n        '''\n        Returns the rate of events with M > mag_value\n\n        :param float slip:\n            Slip rate in mm/yr\n        :param float mmax:\n            Maximum magnitude\n        :param float mag_value:\n            Magnitude value\n        :param float bbar:\n            \\bar{b} parameter (effectively = b * log(10.))\n        :param float dbar:\n            \\bar{d} parameter\n        :param float beta:\n            Beta value of formula defined in Eq. 20 of Anderson & Luco (1983)\n        '''\n        delta_m = (mmax - mag_value)\n        a_1 = self._get_a1_value(bbar, dbar, slip / 10., beta, mmax)\n        return a_1 * np.exp(bbar * delta_m) * (delta_m > 0.0)", "response": "Returns the cumulative value of a specific entry in the logarithm of the events with M > mag_value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_a1_value(bbar, dbar, slip, beta, mmax):\n        return ((dbar - bbar) / dbar) * (slip / beta) *\\\n            np.exp(-(dbar / 2.) * mmax)", "response": "Returns the A1 value defined in I. 9 Table 2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the cumulative value of a specific entry in the system.", "response": "def cumulative_value(self, slip, mmax, mag_value, bbar, dbar, beta):\n        '''\n        Returns the rate of events with M > mag_value\n\n        :param float slip:\n            Slip rate in mm/yr\n        :param float mmax:\n            Maximum magnitude\n        :param float mag_value:\n            Magnitude value\n        :param float bbar:\n            \\bar{b} parameter (effectively = b * log(10.))\n        :param float dbar:\n            \\bar{d} parameter\n        :param float beta:\n            Beta value of formula defined in Eq. 20 of Anderson & Luco (1983)\n        '''\n        delta_m = mmax - mag_value\n        a_2 = self._get_a2_value(bbar, dbar, slip / 10., beta, mmax)\n        return a_2 * (np.exp(bbar * delta_m) - 1.0) * (delta_m > 0.0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the A2 value defined in II. 8 Table 3.", "response": "def _get_a2_value(bbar, dbar, slip, beta, mmax):\n        \"\"\"\n        Returns the A2 value defined in II.8 (Table 3)\n        \"\"\"\n        return ((dbar - bbar) / bbar) * (slip / beta) *\\\n            np.exp(-(dbar / 2.) * mmax)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the cumulative value of a specific entry in a set of events.", "response": "def cumulative_value(self, slip, mmax, mag_value, bbar, dbar, beta):\n        '''\n        Returns the rate of events with M > mag_value\n\n        :param float slip:\n            Slip rate in mm/yr\n        :param float mmax:\n            Maximum magnitude\n        :param float mag_value:\n            Magnitude value\n        :param float bbar:\n            \\bar{b} parameter (effectively = b * log(10.))\n        :param float dbar:\n            \\bar{d} parameter\n        :param float beta:\n            Beta value of formula defined in Eq. 20 of Anderson & Luco (1983)\n        '''\n        delta_m = mmax - mag_value\n        a_3 = self._get_a3_value(bbar, dbar, slip / 10., beta, mmax)\n        central_term = np.exp(bbar * delta_m) - 1.0 - (bbar * delta_m)\n        return a_3 * central_term * (delta_m > 0.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_a3_value(bbar, dbar, slip, beta, mmax):\n        return (dbar * (dbar - bbar) / (bbar ** 2.)) * (slip / beta) *\\\n            np.exp(-(dbar / 2.) * mmax)", "response": "Returns the A3 value defined in III. 4 Table 4."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the activity rate of the fault and returns the minimum bin width and occurrence rate for the fault.", "response": "def get_mfd(self, slip, fault_width, shear_modulus=30.0,\n                disp_length_ratio=1.25E-5):\n        '''\n        Calculates activity rate on the fault\n\n        :param float slip:\n            Slip rate in mm/yr\n\n        :param fault_width:\n            Width of the fault (km)\n\n        :param float shear_modulus:\n            Shear modulus of the fault (GPa)\n\n        :param float disp_length_ratio:\n            Displacement to length ratio (dimensionless)\n\n        :returns:\n            * Minimum Magnitude (float)\n            * Bin width (float)\n            * Occurrence Rates (numpy.ndarray)\n        '''\n        beta = np.sqrt((disp_length_ratio * (10.0 ** C_VALUE)) /\n                       ((shear_modulus * 1.0E10) * (fault_width * 1E5)))\n        dbar = D_VALUE * np.log(10.0)\n        bbar = self.b_value * np.log(10.0)\n        mag = np.arange(self.mmin - (self.bin_width / 2.),\n                        self.mmax + self.bin_width,\n                        self.bin_width)\n\n        if bbar > dbar:\n            print('b-value larger than 1.5 will produce invalid results in '\n                  'Anderson & Luco models')\n            self.occurrence_rate = np.nan * np.ones(len(mag) - 1)\n            return self.mmin, self.bin_width, self.occurrence_rate\n\n        self.occurrence_rate = np.zeros(len(mag) - 1, dtype=float)\n        for ival in range(0, len(mag) - 1):\n            self.occurrence_rate[ival] = \\\n                RECURRENCE_MAP[self.mfd_type].cumulative_value(\n                    slip, self.mmax, mag[ival], bbar, dbar, beta) - \\\n                RECURRENCE_MAP[self.mfd_type].cumulative_value(\n                    slip, self.mmax, mag[ival + 1], bbar, dbar, beta)\n            if self.occurrence_rate[ival] < 0.:\n                self.occurrence_rate[ival] = 0.\n        return self.mmin, self.bin_width, self.occurrence_rate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a file content", "response": "def load_version():\n    \"\"\"Loads a file content\"\"\"\n    filename = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                            \"cpt\", \"__init__.py\"))\n    with open(filename, \"rt\") as version_file:\n        conan_init = version_file.read()\n        version = re.search(\"__version__ = '([0-9a-z.-]+)'\", conan_init).group(1)\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef builds(self, confs):\n        self._named_builds = {}\n        self._builds = []\n        for values in confs:\n            if len(values) == 2:\n                self._builds.append(BuildConf(values[0], values[1], {}, {}, self.reference))\n            elif len(values) == 4:\n                self._builds.append(BuildConf(values[0], values[1], values[2], values[3],\n                                              self.reference))\n            elif len(values) != 5:\n                raise Exception(\"Invalid build configuration, has to be a tuple of \"\n                                \"(settings, options, env_vars, build_requires, reference)\")\n            else:\n                self._builds.append(BuildConf(*values))", "response": "For retro compatibility directly assigning builds"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef patch_default_base_profile(conan_api, profile_abs_path):\n    text = tools.load(profile_abs_path)\n    if \"include(default)\" in text:  # User didn't specified a custom profile\n        if Version(conan_version) < Version(\"1.12.0\"):\n            cache = conan_api._client_cache\n        else:\n            cache = conan_api._cache\n\n        default_profile_name = os.path.basename(cache.default_profile_path)\n        if not os.path.exists(cache.default_profile_path):\n            conan_api.create_profile(default_profile_name, detect=True)\n\n        if default_profile_name != \"default\":  # User have a different default profile name\n            # https://github.com/conan-io/conan-package-tools/issues/121\n            text = text.replace(\"include(default)\", \"include(%s)\" % default_profile_name)\n            tools.save(profile_abs_path, text)", "response": "Patch the default base profile with include if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a User with the given username iff the User exists.", "response": "def get_user_if_exists(strategy, details, user=None, *args, **kwargs):\n    \"\"\"Return a User with the given username iff the User exists.\"\"\"\n    if user:\n        return {'is_new': False}\n    try:\n        username = details.get('username')\n\n        # Return the user if it exists\n        return {\n            'is_new': False,\n            'user': User.objects.get(username=username)\n        }\n    except User.DoesNotExist:\n        # Fall to the default return value\n        pass\n\n    # Nothing to return since we don't have a user\n    return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_email(strategy, details, user=None, *args, **kwargs):\n    if user:\n        email = details.get('email')\n\n        if email and user.email != email:\n            user.email = email\n            strategy.storage.user.changed(user)", "response": "Update the user s email address using data from provider."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives two dicts merge them into a new dict as a shallow copy.", "response": "def _merge_two_dicts(x, y):\n    \"\"\"\n    Given two dicts, merge them into a new dict as a shallow copy.\n\n    Once Python 3.6+ only is supported, replace method with ``z = {**x, **y}``\n    \"\"\"\n    z = x.copy()\n    z.update(y)\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps key values from the response to key values in the user model.", "response": "def _map_user_details(self, response):\n        \"\"\"Maps key/values from the response to key/values in the user model.\n\n        Does not transfer any key/value that is empty or not present in the response.\n        \"\"\"\n        dest = {}\n        for source_key, dest_key in self.CLAIMS_TO_DETAILS_KEY_MAP.items():\n            value = response.get(source_key)\n            if value is not None:\n                dest[dest_key] = value\n\n        return dest"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user_claims(self, access_token, claims=None, token_type='Bearer'):\n        data = self.get_json(\n            self.USER_INFO_URL,\n            headers={'Authorization': '{token_type} {token}'.format(token_type=token_type, token=access_token)}\n        )\n\n        if claims:\n            claims_names = set(claims)\n            data = {k: v for (k, v) in six.iteritems(data) if k in claims_names}\n\n        return data", "response": "Returns a dictionary with the values for each claim requested."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup_ipc(self):\n        '''\n        Setup the IPC pub and sub.\n        Subscript to the listener IPC\n        and publish to the device specific IPC.\n        '''\n        log.debug('Setting up the server IPC puller to receive from the listener')\n        self.ctx = zmq.Context()\n        # subscribe to listener\n        self.sub = self.ctx.socket(zmq.PULL)\n        self.sub.bind(LST_IPC_URL)\n        try:\n            self.sub.setsockopt(zmq.HWM, self.opts['hwm'])\n            # zmq 2\n        except AttributeError:\n            # zmq 3\n            self.sub.setsockopt(zmq.RCVHWM, self.opts['hwm'])\n        # device publishers\n        log.debug('Creating the router ICP on the server')\n        self.pub = self.ctx.socket(zmq.ROUTER)\n        self.pub.bind(DEV_IPC_URL)\n        try:\n            self.pub.setsockopt(zmq.HWM, self.opts['hwm'])\n            # zmq 2\n        except AttributeError:\n            # zmq 3\n            self.pub.setsockopt(zmq.SNDHWM, self.opts['hwm'])\n        # Pipe to the publishers\n        self.publisher_pub = self.ctx.socket(zmq.PUB)\n        self.publisher_pub.connect(PUB_PX_IPC_URL)\n        try:\n            self.publisher_pub.setsockopt(zmq.HWM, self.opts['hwm'])\n            # zmq 2\n        except AttributeError:\n            # zmq 3\n            self.publisher_pub.setsockopt(zmq.SNDHWM, self.opts['hwm'])", "response": "Setup the IPC pub and sub."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compile_prefixes(self):\n        '''\n        Create a dict of all OS prefixes and their compiled regexs\n        '''\n        self.compiled_prefixes = {}\n        for dev_os, os_config in self.config.items():\n            if not os_config:\n                continue\n            self.compiled_prefixes[dev_os] = []\n            for prefix in os_config.get('prefixes', []):\n                values = prefix.get('values', {})\n                line = prefix.get('line', '')\n                if prefix.get('__python_fun__'):\n                    self.compiled_prefixes[dev_os].append({\n                        '__python_fun__': prefix['__python_fun__'],\n                        '__python_mod__': prefix['__python_mod__']\n                    })\n                    continue  # if python profiler defined for this prefix,\n                    # no need to go further, but jump to the next prefix\n                # Add 'pri' and 'message' to the line, and values\n                line = '{{pri}}{}{{message}}'.format(line)\n                # PRI https://tools.ietf.org/html/rfc5424#section-6.2.1\n                values['pri'] = r'\\<(\\d+)\\>'\n                values['message'] = '(.*)'\n                # We will now figure out which position each value is in so we can use it with the match statement\n                position = {}\n                for key in values.keys():\n                    position[line.find('{' + key + '}')] = key\n                sorted_position = {}\n                for i, elem in enumerate(sorted(position.items())):\n                    sorted_position[elem[1]] = i + 1\n                # Escape the line, then remove the escape for the curly bracets so they can be used when formatting\n                escaped = re.escape(line).replace(r'\\{', '{').replace(r'\\}', '}')\n                # Replace a whitespace with \\s+\n                escaped = escaped.replace(r'\\ ', r'\\s+')\n                self.compiled_prefixes[dev_os].append({\n                    'prefix': re.compile(escaped.format(**values)),\n                    'prefix_positions': sorted_position,\n                    'raw_prefix': escaped.format(**values),\n                    'values': values\n                })", "response": "Create a dict of all OS prefixes and compiled regexs for each OS prefix and the corresponding entry in the config file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _identify_prefix(self, msg, data):\n        '''\n        Check the message again each OS prefix and if matched return the\n        message dict\n        '''\n        prefix_id = -1\n        for prefix in data:\n            msg_dict = {}\n            prefix_id += 1\n            match = None\n            if '__python_fun__' in prefix:\n                log.debug('Trying to match using the %s custom python profiler', prefix['__python_mod__'])\n                try:\n                    match = prefix['__python_fun__'](msg)\n                except Exception:\n                    log.error('Exception while parsing %s with the %s python profiler',\n                              msg, prefix['__python_mod__'], exc_info=True)\n            else:\n                log.debug('Matching using YAML-defined profiler:')\n                log.debug(prefix['raw_prefix'])\n                match = prefix['prefix'].search(msg)\n            if not match:\n                log.debug('Match not found')\n                continue\n            if '__python_fun__' in prefix:\n                log.debug('%s matched using the custom python profiler %s', msg, prefix['__python_mod__'])\n                msg_dict = match  # the output as-is from the custom function\n            else:\n                positions = prefix.get('prefix_positions', {})\n                values = prefix.get('values')\n                msg_dict = {}\n                for key in values.keys():\n                    msg_dict[key] = match.group(positions.get(key))\n            # Remove whitespace from the start or end of the message\n            msg_dict['__prefix_id__'] = prefix_id\n            msg_dict['message'] = msg_dict['message'].strip()\n\n            # The pri has to be an int as it is retrived using regex '\\<(\\d+)\\>'\n            if 'pri' in msg_dict:\n                msg_dict['facility'] = int(int(msg_dict['pri']) / 8)\n                msg_dict['severity'] = int(int(msg_dict['pri']) - (msg_dict['facility'] * 8))\n            return msg_dict", "response": "Identify the OS prefix and return the message dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _identify_os(self, msg):\n        '''\n        Using the prefix of the syslog message,\n        we are able to identify the operating system and then continue parsing.\n        '''\n        ret = []\n        for dev_os, data in self.compiled_prefixes.items():\n            # TODO Should we prevent attepmting to determine the OS for the blacklisted?\n            # [mircea] I think its good from a logging perspective to know at least that\n            #   that the server found the matching and it tells that it won't be processed\n            #   further. Later, we could potentially add an option to control this.\n            log.debug('Matching under %s', dev_os)\n            msg_dict = self._identify_prefix(msg, data)\n            if msg_dict:\n                log.debug('Adding %s to list of matched OS', dev_os)\n                ret.append((dev_os, msg_dict))\n            else:\n                log.debug('No match found for %s', dev_os)\n        if not ret:\n            log.debug('Not matched any OS, returning original log')\n            msg_dict = {'message': msg}\n            ret.append((None, msg_dict))\n        return ret", "response": "Identify the operating system of the message and return a list of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the logging server.", "response": "def start(self):\n        '''\n        Take the messages from the queue,\n        inspect and identify the operating system,\n        then queue the message correspondingly.\n        '''\n        # metric counters\n        napalm_logs_server_messages_received = Counter(\n            \"napalm_logs_server_messages_received\",\n            \"Count of messages received from listener processes\"\n        )\n        napalm_logs_server_skipped_buffered_messages = Counter(\n            'napalm_logs_server_skipped_buffered_messages',\n            'Count of messages skipped as they were already buffered',\n            ['device_os']\n        )\n        napalm_logs_server_messages_with_identified_os = Counter(\n            \"napalm_logs_server_messages_with_identified_os\",\n            \"Count of messages with positive os identification\",\n            ['device_os']\n        )\n        napalm_logs_server_messages_without_identified_os = Counter(\n            \"napalm_logs_server_messages_without_identified_os\",\n            \"Count of messages with negative os identification\"\n        )\n        napalm_logs_server_messages_failed_device_queuing = Counter(\n            \"napalm_logs_server_messages_failed_device_queuing\",\n            \"Count of messages per device os that fail to be queued to a device process\",\n            ['device_os']\n        )\n        napalm_logs_server_messages_device_queued = Counter(\n            \"napalm_logs_server_messages_device_queued\",\n            \"Count of messages queued to device processes\",\n            ['device_os']\n        )\n        napalm_logs_server_messages_unknown_queued = Counter(\n            \"napalm_logs_server_messages_unknown_queued\",\n            \"Count of messages queued as unknown\"\n        )\n        self._setup_ipc()\n        # Start suicide polling thread\n        cleanup = threading.Thread(target=self._cleanup_buffer)\n        cleanup.start()\n        thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))\n        thread.start()\n        signal.signal(signal.SIGTERM, self._exit_gracefully)\n        self.__up = True\n        while self.__up:\n            # Take messages from the main queue\n            try:\n                bin_obj = self.sub.recv()\n                msg, address = umsgpack.unpackb(bin_obj, use_list=False)\n            except zmq.ZMQError as error:\n                if self.__up is False:\n                    log.info('Exiting on process shutdown')\n                    return\n                else:\n                    log.error(error, exc_info=True)\n                    raise NapalmLogsExit(error)\n            if six.PY3:\n                msg = str(msg, 'utf-8')\n            else:\n                msg = msg.encode('utf-8')\n            log.debug('[%s] Dequeued message from %s: %s', address, msg, time.time())\n            napalm_logs_server_messages_received.inc()\n            os_list = self._identify_os(msg)\n\n            for dev_os, msg_dict in os_list:\n                if dev_os and dev_os in self.started_os_proc:\n                    # Identified the OS and the corresponding process is started.\n                    # Then send the message in the right queue\n                    log.debug('Identified OS: %s', dev_os)\n                    log.debug('Queueing message to %s', dev_os)\n                    if six.PY3:\n                        dev_os = bytes(dev_os, 'utf-8')\n                    if self._buffer:\n                        message = '{dev_os}/{host}/{msg}'.format(dev_os=dev_os,\n                                                                 host=msg_dict['host'],\n                                                                 msg=msg_dict['message'])\n                        message_key = base64.b64encode(message)\n                        if self._buffer[message_key]:\n                            log.info('\"%s\" seems to be already buffered, skipping', msg_dict['message'])\n                            napalm_logs_server_skipped_buffered_messages.labels(device_os=dev_os).inc()\n                            continue\n                        log.debug('\"%s\" is not buffered yet, added', msg_dict['message'])\n                        self._buffer[message_key] = 1\n                    self.pub.send_multipart([dev_os,\n                                             umsgpack.packb((msg_dict, address))])\n                    # self.os_pipes[dev_os].send((msg_dict, address))\n                    napalm_logs_server_messages_with_identified_os.labels(device_os=dev_os).inc()\n                    napalm_logs_server_messages_device_queued.labels(device_os=dev_os).inc()\n\n                elif dev_os and dev_os not in self.started_os_proc:\n                    # Identified the OS, but the corresponding process does not seem to be started.\n                    log.info('Unable to queue the message to %s. Is the sub-process started?', dev_os)\n                    napalm_logs_server_messages_with_identified_os.labels(device_os=dev_os).inc()\n                    napalm_logs_server_messages_failed_device_queuing.labels(device_os=dev_os).inc()\n\n                elif not dev_os and self.opts['_server_send_unknown']:\n                    # OS not identified, but the user requested to publish the message as-is\n                    log.debug('Unable to identify the OS, sending directly to the publishers')\n                    to_publish = {\n                        'ip': address,\n                        'host': 'unknown',\n                        'timestamp': int(time.time()),\n                        'message_details': msg_dict,\n                        'os': UNKNOWN_DEVICE_NAME,\n                        'error': 'UNKNOWN',\n                        'model_name': 'unknown'\n                    }\n                    self.publisher_pub.send(umsgpack.packb(to_publish))\n                    napalm_logs_server_messages_unknown_queued.inc()\n                    napalm_logs_server_messages_without_identified_os.inc()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _setup_ipc(self):\n        '''\n        Subscribe to the right topic\n        in the device IPC and publish to the\n        publisher proxy.\n        '''\n        self.ctx = zmq.Context()\n        # subscribe to device IPC\n        log.debug('Creating the dealer IPC for %s', self._name)\n        self.sub = self.ctx.socket(zmq.DEALER)\n        if six.PY2:\n            self.sub.setsockopt(zmq.IDENTITY, self._name)\n        elif six.PY3:\n            self.sub.setsockopt(zmq.IDENTITY, bytes(self._name, 'utf-8'))\n        try:\n            self.sub.setsockopt(zmq.HWM, self.opts['hwm'])\n            # zmq 2\n        except AttributeError:\n            # zmq 3\n            self.sub.setsockopt(zmq.RCVHWM, self.opts['hwm'])\n        # subscribe to the corresponding IPC pipe\n        self.sub.connect(DEV_IPC_URL)\n        # publish to the publisher IPC\n        self.pub = self.ctx.socket(zmq.PUB)\n        self.pub.connect(PUB_PX_IPC_URL)\n        try:\n            self.pub.setsockopt(zmq.HWM, self.opts['hwm'])\n            # zmq 2\n        except AttributeError:\n            # zmq 3\n            self.pub.setsockopt(zmq.SNDHWM, self.opts['hwm'])", "response": "Setup the IPC for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compile_messages(self):\n        '''\n        Create a list of all OS messages and their compiled regexs\n        '''\n        self.compiled_messages = []\n        if not self._config:\n            return\n        for message_dict in self._config.get('messages', {}):\n            error = message_dict['error']\n            tag = message_dict['tag']\n            model = message_dict['model']\n            match_on = message_dict.get('match_on', 'tag')\n            if '__python_fun__' in message_dict:\n                self.compiled_messages.append({\n                    'error': error,\n                    'tag': tag,\n                    'match_on': match_on,\n                    'model': model,\n                    '__python_fun__': message_dict['__python_fun__']\n                })\n                continue\n            values = message_dict['values']\n            line = message_dict['line']\n            mapping = message_dict['mapping']\n            # We will now figure out which position each value is in so we can use it with the match statement\n            position = {}\n            replace = {}\n            for key in values.keys():\n                if '|' in key:\n                    new_key, replace[new_key] = key.replace(' ', '').split('|')\n                    values[new_key] = values.pop(key)\n                    key = new_key\n                position[line.find('{' + key + '}')] = key\n            sorted_position = {}\n            for i, elem in enumerate(sorted(position.items())):\n                sorted_position[elem[1]] = i + 1\n            # Escape the line, then remove the escape for the curly bracets so they can be used when formatting\n            escaped = re.escape(line).replace(r'\\{', '{').replace(r'\\}', '}')\n            # Replace a whitespace with \\s+\n            escaped = escaped.replace(r'\\ ', r'\\s+')\n            self.compiled_messages.append(\n                {\n                    'error': error,\n                    'tag': tag,\n                    'match_on': match_on,\n                    'line': re.compile(escaped.format(**values)),\n                    'positions': sorted_position,\n                    'values': values,\n                    'replace': replace,\n                    'model': model,\n                    'mapping': mapping\n                }\n            )\n        log.debug('Compiled messages:')\n        log.debug(self.compiled_messages)", "response": "Create a list of all OS messages and their compiled regexs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse(self, msg_dict):\n        '''\n        Parse a syslog message and check what OpenConfig object should\n        be generated.\n        '''\n        error_present = False\n        # log.debug('Matching the message:')\n        # log.debug(msg_dict)\n        for message in self.compiled_messages:\n            # log.debug('Matching using:')\n            # log.debug(message)\n            match_on = message['match_on']\n            if match_on not in msg_dict:\n                # log.debug('%s is not a valid key in the partially parsed dict', match_on)\n                continue\n            if message['tag'] != msg_dict[match_on]:\n                continue\n            if '__python_fun__' in message:\n                return {\n                    'model': message['model'],\n                    'error': message['error'],\n                    '__python_fun__': message['__python_fun__']\n                }\n            error_present = True\n            match = message['line'].search(msg_dict['message'])\n            if not match:\n                continue\n            positions = message.get('positions', {})\n            values = message.get('values')\n            ret = {\n                'model': message['model'],\n                'mapping': message['mapping'],\n                'replace': message['replace'],\n                'error': message['error']\n            }\n            for key in values.keys():\n                # Check if the value needs to be replaced\n                if key in message['replace']:\n                    result = napalm_logs.utils.cast(match.group(positions.get(key)), message['replace'][key])\n                else:\n                    result = match.group(positions.get(key))\n                ret[key] = result\n            return ret\n        if error_present is True:\n            log.info('Configured regex did not match for os: %s tag %s', self._name, msg_dict.get('tag', ''))\n        else:\n            log.info('Syslog message not configured for os: %s tag %s', self._name, msg_dict.get('tag', ''))", "response": "Parse a syslog message and check what OpenConfig object should be generated."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _emit(self, **kwargs):\n        '''\n        Emit an OpenConfig object given a certain combination of\n        fields mappeed in the config to the corresponding hierarchy.\n        '''\n        oc_dict = {}\n        for mapping, result_key in kwargs['mapping']['variables'].items():\n            result = kwargs[result_key]\n            oc_dict = napalm_logs.utils.setval(mapping.format(**kwargs), result, oc_dict)\n        for mapping, result in kwargs['mapping']['static'].items():\n            oc_dict = napalm_logs.utils.setval(mapping.format(**kwargs), result, oc_dict)\n\n        return oc_dict", "response": "Emit an OpenConfig object given a certain combination of\n        fields mappeed in the config to the corresponding hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npublishing the object to the pub socket.", "response": "def _publish(self, obj):\n        '''\n        Publish the OC object.\n        '''\n        bin_obj = umsgpack.packb(obj)\n        self.pub.send(bin_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self):\n        '''\n        Start the worker process.\n        '''\n        # metrics\n        napalm_logs_device_messages_received = Counter(\n            'napalm_logs_device_messages_received',\n            \"Count of messages received by the device process\",\n            ['device_os']\n        )\n        napalm_logs_device_raw_published_messages = Counter(\n            'napalm_logs_device_raw_published_messages',\n            \"Count of raw type published messages\",\n            ['device_os']\n        )\n        napalm_logs_device_published_messages = Counter(\n            'napalm_logs_device_published_messages',\n            \"Count of published messages\",\n            ['device_os']\n        )\n        napalm_logs_device_oc_object_failed = Counter(\n            'napalm_logs_device_oc_object_failed',\n            \"Counter of failed OpenConfig object generations\",\n            ['device_os']\n        )\n\n        self._setup_ipc()\n        # Start suicide polling thread\n        thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))\n        thread.start()\n        signal.signal(signal.SIGTERM, self._exit_gracefully)\n        self.__up = True\n        while self.__up:\n            # bin_obj = self.sub.recv()\n            # msg_dict, address = umsgpack.unpackb(bin_obj, use_list=False)\n            try:\n                bin_obj = self.sub.recv()\n                msg_dict, address = umsgpack.unpackb(bin_obj, use_list=False)\n            except zmq.ZMQError as error:\n                if self.__up is False:\n                    log.info('Exiting on process shutdown [%s]', self._name)\n                    return\n                else:\n                    raise NapalmLogsExit(error)\n            log.debug('%s: dequeued %s, received from %s', self._name, msg_dict, address)\n            napalm_logs_device_messages_received.labels(device_os=self._name).inc()\n            host = msg_dict.get('host')\n            prefix_id = msg_dict.pop('__prefix_id__')\n            if 'timestamp' in msg_dict:\n                timestamp = msg_dict.pop('timestamp')\n            else:\n                timestamp = self._format_time(msg_dict.get('time', ''),\n                                              msg_dict.get('date', ''),\n                                              msg_dict.get('timeZone', 'UTC'),\n                                              prefix_id)\n            facility = msg_dict.get('facility')\n            severity = msg_dict.get('severity')\n\n            kwargs = self._parse(msg_dict)\n            if not kwargs:\n                # Unable to identify what model to generate for the message in cause.\n                # But publish the message when the user requested to push raw messages.\n                to_publish = {\n                    'ip': address,\n                    'host': host,\n                    'timestamp': timestamp,\n                    'message_details': msg_dict,\n                    'os': self._name,\n                    'error': 'RAW',\n                    'model_name': 'raw',\n                    'facility': facility,\n                    'severity': severity\n                }\n                log.debug('Queueing to be published:')\n                log.debug(to_publish)\n                # self.pub_pipe.send(to_publish)\n                self.pub.send(umsgpack.packb(to_publish))\n                napalm_logs_device_raw_published_messages.labels(device_os=self._name).inc()\n                continue\n            try:\n                if '__python_fun__' in kwargs:\n                    log.debug('Using the Python parser to determine the YANG-equivalent object')\n                    yang_obj = kwargs['__python_fun__'](msg_dict)\n                else:\n                    yang_obj = self._emit(**kwargs)\n            except Exception:\n                log.exception('Unexpected error when generating the OC object.', exc_info=True)\n                napalm_logs_device_oc_object_failed.labels(device_os=self._name).inc()\n                continue\n            log.debug('Generated OC object:')\n            log.debug(yang_obj)\n            error = kwargs.get('error')\n            model_name = kwargs.get('model')\n            to_publish = {\n                'error': error,\n                'host': host,\n                'ip': address,\n                'timestamp': timestamp,\n                'yang_message': yang_obj,\n                'message_details': msg_dict,\n                'yang_model': model_name,\n                'os': self._name,\n                'facility': facility,\n                'severity': severity\n            }\n            log.debug('Queueing to be published:')\n            log.debug(to_publish)\n            # self.pub_pipe.send(to_publish)\n            self.pub.send(umsgpack.packb(to_publish))\n            # self._publish(to_publish)\n            napalm_logs_device_published_messages.labels(device_os=self._name).inc()", "response": "Start the worker process."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_serializer(name):\n    '''\n    Return the serialize function.\n    '''\n    try:\n        log.debug('Using %s as serializer', name)\n        return SERIALIZER_LOOKUP[name]\n    except KeyError:\n        msg = 'Serializer {} is not available'.format(name)\n        log.error(msg, exc_info=True)\n        raise InvalidSerializerException(msg)", "response": "Get the serialize function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handshake(self, conn, addr):\n        '''\n        Ensures that the client receives the AES key.\n        '''\n        # waiting for the magic request message\n        msg = conn.recv(len(MAGIC_REQ))\n        log.debug('Received message %s from %s', msg, addr)\n        if msg != MAGIC_REQ:\n            log.warning('%s is not a valid REQ message from %s', msg, addr)\n            return\n        log.debug('Sending the private key')\n        conn.send(self.__key)\n        # wait for explicit ACK\n        log.debug('Waiting for the client to confirm')\n        msg = conn.recv(len(MAGIC_ACK))\n        if msg != MAGIC_ACK:\n            return\n        log.debug('Sending the signature key')\n        conn.send(self.__sgn)\n        # wait for explicit ACK\n        log.debug('Waiting for the client to confirm')\n        msg = conn.recv(len(MAGIC_ACK))\n        if msg != MAGIC_ACK:\n            return\n        log.info('%s is now authenticated', addr)\n        self.keep_alive(conn)", "response": "This method is called by the client to send the private key and the signature key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef keep_alive(self, conn):\n        '''\n        Maintains auth sessions\n        '''\n        while self.__up:\n            msg = conn.recv(len(AUTH_KEEP_ALIVE))\n            if msg != AUTH_KEEP_ALIVE:\n                log.error('Received something other than %s', AUTH_KEEP_ALIVE)\n                conn.close()\n                return\n            try:\n                conn.send(AUTH_KEEP_ALIVE_ACK)\n            except (IOError, socket.error) as err:\n                log.error('Unable to send auth keep alive: %s', err)\n                conn.close()\n                return", "response": "Keeps the session alive."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_cert(self):\n        '''\n        Checks that the provided cert and key are valid and usable\n        '''\n        log.debug('Verifying the %s certificate, keyfile: %s',\n                  self.certificate, self.keyfile)\n        try:\n            ssl.create_default_context().load_cert_chain(self.certificate, keyfile=self.keyfile)\n        except ssl.SSLError:\n            error_string = 'SSL certificate and key do not match'\n            log.error(error_string)\n            raise SSLMismatchException(error_string)\n        except IOError:\n            log.error('Unable to open either certificate or key file')\n            raise\n        log.debug('Certificate looks good.')", "response": "Checks that the provided certificate and key are valid and usable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_skt(self):\n        '''\n        Create the authentication socket.\n        '''\n        log.debug('Creating the auth socket')\n        if ':' in self.auth_address:\n            self.socket = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        else:\n            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            self.socket.bind((self.auth_address, self.auth_port))\n        except socket.error as msg:\n            error_string = 'Unable to bind (auth) to port {} on {}: {}'.format(self.auth_port, self.auth_address, msg)\n            log.error(error_string, exc_info=True)\n            raise BindException(error_string)", "response": "Create the authentication socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        '''\n        Listen to auth requests and send the AES key.\n        Each client connection starts a new thread.\n        '''\n        # Start suicide polling thread\n        log.debug('Starting the auth process')\n        self.verify_cert()\n        self._create_skt()\n        log.debug('The auth process can receive at most %d parallel connections', AUTH_MAX_CONN)\n        self.socket.listen(AUTH_MAX_CONN)\n        thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))\n        thread.start()\n        signal.signal(signal.SIGTERM, self._exit_gracefully)\n        self.__up = True\n        while self.__up:\n            try:\n                (clientsocket, address) = self.socket.accept()\n                wrapped_auth_skt = ssl.wrap_socket(clientsocket,\n                                                   server_side=True,\n                                                   certfile=self.certificate,\n                                                   keyfile=self.keyfile)\n            except ssl.SSLError:\n                log.exception('SSL error', exc_info=True)\n                continue\n            except socket.error as error:\n                if self.__up is False:\n                    return\n                else:\n                    msg = 'Received auth socket error: {}'.format(error)\n                    log.error(msg, exc_info=True)\n                    raise NapalmLogsExit(msg)\n            log.info('%s connected', address)\n            log.debug('Starting the handshake')\n            client_thread = threading.Thread(target=self._handshake,\n                                             args=(wrapped_auth_skt, address))\n            client_thread.start()", "response": "Start the auth process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(self):\n        '''\n        Stop the auth proc.\n        '''\n        log.info('Stopping auth process')\n        self.__up = False\n        self.socket.close()", "response": "Stop the auth process."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the serialize function.", "response": "def get_interface(name):\n    '''\n    Return the serialize function.\n    '''\n    try:\n        log.debug('Using %s as buffer interface', name)\n        return BUFFER_LOOKUP[name]\n    except KeyError:\n        msg = 'Buffer interface {} is not available'.format(name)\n        log.error(msg, exc_info=True)\n        raise InvalidBufferException(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self):\n        '''\n        Startup the kafka consumer.\n        '''\n        log.debug('Creating the consumer using the bootstrap servers: %s and the group ID: %s',\n                  self.bootstrap_servers,\n                  self.group_id)\n        try:\n            self.consumer = kafka.KafkaConsumer(bootstrap_servers=self.bootstrap_servers,\n                                                group_id=self.group_id)\n        except kafka.errors.NoBrokersAvailable as err:\n            log.error(err, exc_info=True)\n            raise ListenerException(err)\n        log.debug('Subscribing to the %s topic', self.topic)\n        self.consumer.subscribe(topics=[self.topic])", "response": "Start the kafka consumer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef receive(self):\n        '''\n        Return the message received and the address.\n        '''\n        try:\n            msg = next(self.consumer)\n        except ValueError as error:\n            log.error('Received kafka error: %s', error, exc_info=True)\n            raise ListenerException(error)\n        log_source = msg.key\n        try:\n            decoded = json.loads(msg.value.decode('utf-8'))\n        except ValueError:\n            log.error('Not in json format: %s', msg.value.decode('utf-8'))\n            return '', ''\n        log_message = decoded.get('message')\n        log.debug('[%s] Received %s from %s', log_message, log_source, time.time())\n        return log_message, log_source", "response": "Receive a message from kafka and return the message and the address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the named transport class.", "response": "def get_transport(name):\n    '''\n    Return the transport class.\n    '''\n    try:\n        log.debug('Using %s as transport', name)\n        return TRANSPORT_LOOKUP[name]\n    except KeyError:\n        msg = 'Transport {} is not available. Are the dependencies installed?'.format(name)\n        log.error(msg, exc_info=True)\n        raise InvalidTransportException(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        '''\n        Startup the zmq consumer.\n        '''\n        zmq_uri = '{protocol}://{address}:{port}'.format(\n                       protocol=self.protocol,\n                       address=self.address,\n                       port=self.port\n                   ) if self.port else\\\n                   '{protocol}://{address}'.format(  # noqa\n                       protocol=self.protocol,\n                       address=self.address\n                   )\n        log.debug('ZMQ URI: %s', zmq_uri)\n        self.ctx = zmq.Context()\n        if hasattr(zmq, self.type):\n            skt_type = getattr(zmq, self.type)\n        else:\n            skt_type = zmq.PULL\n        self.sub = self.ctx.socket(skt_type)\n        self.sub.connect(zmq_uri)\n        if self.hwm is not None:\n            try:\n                self.sub.setsockopt(zmq.HWM, self.hwm)\n            except AttributeError:\n                self.sub.setsockopt(zmq.RCVHWM, self.hwm)\n        if self.recvtimeout is not None:\n            log.debug('Setting RCVTIMEO to %d', self.recvtimeout)\n            self.sub.setsockopt(zmq.RCVTIMEO, self.recvtimeout)\n        if self.keepalive is not None:\n            log.debug('Setting TCP_KEEPALIVE to %d', self.keepalive)\n            self.sub.setsockopt(zmq.TCP_KEEPALIVE, self.keepalive)\n        if self.keepalive_idle is not None:\n            log.debug('Setting TCP_KEEPALIVE_IDLE to %d', self.keepalive_idle)\n            self.sub.setsockopt(zmq.TCP_KEEPALIVE_IDLE, self.keepalive_idle)\n        if self.keepalive_interval is not None:\n            log.debug('Setting TCP_KEEPALIVE_INTVL to %d', self.keepalive_interval)\n            self.sub.setsockopt(zmq.TCP_KEEPALIVE_INTVL, self.keepalive_interval)", "response": "Startup the zmq consumer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef receive(self):\n        '''\n        Return the message received.\n\n        ..note::\n            In ZMQ we are unable to get the address where we got the message from.\n        '''\n        try:\n            msg = self.sub.recv()\n        except zmq.Again as error:\n            log.error('Unable to receive messages: %s', error, exc_info=True)\n            raise ListenerException(error)\n        log.debug('[%s] Received %s', time.time(), msg)\n        return msg, ''", "response": "Receive a message from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        '''\n        Create the UDP listener socket.\n        '''\n        if ':' in self.address:\n            self.skt = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        else:\n            self.skt = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        if self.reuse_port:\n            self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            if hasattr(socket, 'SO_REUSEPORT'):\n                self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n            else:\n                log.error('SO_REUSEPORT not supported')\n        try:\n            self.skt.bind((self.address, int(self.port)))\n        except socket.error as msg:\n            error_string = 'Unable to bind to port {} on {}: {}'.format(self.port, self.address, msg)\n            log.error(error_string, exc_info=True)\n            raise BindException(error_string)", "response": "Create the UDP listener socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreceive a message from the listener socket and return the message and the address.", "response": "def receive(self):\n        '''\n        Return the message received and the address.\n        '''\n        try:\n            msg, addr = self.skt.recvfrom(self.buffer_size)\n        except socket.error as error:\n            log.error('Received listener socket error: %s', error, exc_info=True)\n            raise ListenerException(error)\n        log.debug('[%s] Received %s from %s', msg, addr, time.time())\n        return msg, addr[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _suicide_when_without_parent(self, parent_pid):\n        '''\n        Kill this process when the parent died.\n        '''\n        while True:\n            time.sleep(5)\n            try:\n                # Check pid alive\n                os.kill(parent_pid, 0)\n            except OSError:\n                # Forcibly exit\n                # Regular sys.exit raises an exception\n                self.stop()\n                log.warning('The parent is not alive, exiting.')\n                os._exit(999)", "response": "Kill this process when the parent died."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_log(self):\n        '''\n        Setup the log object.\n        '''\n        logging_level = CONFIG.LOGGING_LEVEL.get(self.log_level.lower())\n        logging.basicConfig(format=self.log_format,\n                            level=logging_level)", "response": "Setup the log object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if the OS should be ignored based on the whitelist - blacklist logic.", "response": "def _whitelist_blacklist(self, os_name):\n        '''\n        Determines if the OS should be ignored,\n        depending on the whitelist-blacklist logic\n        configured by the user.\n        '''\n        return napalm_logs.ext.check_whitelist_blacklist(os_name,\n                                                         whitelist=self.device_whitelist,\n                                                         blacklist=self.device_blacklist)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the comments at the top of the YAML file and return the extracted comment as string.", "response": "def _extract_yaml_docstring(stream):\n        '''\n        Extract the comments at the top of the YAML file,\n        from the stream handler.\n        Return the extracted comment as string.\n        '''\n        comment_lines = []\n        lines = stream.read().splitlines()\n        for line in lines:\n            line_strip = line.strip()\n            if not line_strip:\n                continue\n            if line_strip.startswith('#'):\n                comment_lines.append(\n                    line_strip.replace('#', '', 1).strip()\n                )\n            else:\n                break\n        return ' '.join(comment_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the configuration from a specific path and return the object.", "response": "def _load_config(self, path):\n        '''\n        Read the configuration under a specific path\n        and return the object.\n        '''\n        config = {}\n        log.debug('Reading configuration from %s', path)\n        if not os.path.isdir(path):\n            msg = (\n                'Unable to read from {path}: '\n                'the directory does not exist!'\n            ).format(path=path)\n            log.error(msg)\n            raise IOError(msg)\n        # The directory tree should look like the following:\n        # .\n        # \u251c\u2500\u2500 __init__.py\n        # \u251c\u2500\u2500 eos\n        # \u2502\u00a0\u00a0 \u2514\u2500\u2500 init.yml\n        # \u251c\u2500\u2500 iosxr\n        # \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n        # \u251c\u2500\u2500 junos\n        # \u2502\u00a0\u00a0 \u2514\u2500\u2500 init.yml\n        # \u2502\u00a0\u00a0 \u2514\u2500\u2500 bgp_read_message.py\n        # \u2502\u00a0\u00a0 \u2514\u2500\u2500 BGP_PREFIX_THRESH_EXCEEDED.py\n        # \u2514\u2500\u2500 nxos\n        #     \u2514\u2500\u2500 init.yml\n        os_subdirs = [sdpath[0] for sdpath in os.walk(path)][1:]\n        if not os_subdirs:\n            log.error('%s does not contain any OS subdirectories', path)\n        for os_dir in os_subdirs:\n            os_name = os.path.split(os_dir)[1]  # the network OS name\n            if os_name.startswith('__'):\n                log.debug('Ignoring %s', os_name)\n                continue\n            if not self._whitelist_blacklist(os_name):\n                log.debug('Not building config for %s (whitelist-blacklist logic)', os_name)\n                # Ignore devices that are not in the whitelist (if defined),\n                #   or those operating systems that are on the blacklist.\n                # This way we can prevent starting unwanted sub-processes.\n                continue\n            log.debug('Building config for %s:', os_name)\n            log.debug('='*40)\n            if os_name not in config:\n                config[os_name] = {}\n            files = os.listdir(os_dir)\n            # Read all files under the OS dir\n            for file_ in files:\n                log.debug('Inspecting %s', file_)\n                file_name, file_extension = os.path.splitext(file_)\n                file_extension = file_extension.replace('.', '')\n                filepath = os.path.join(os_dir, file_)\n                comment = ''\n                if file_extension in ('yml', 'yaml'):\n                    try:\n                        log.debug('Loading %s as YAML', file_)\n                        with open(filepath, 'r') as fstream:\n                            cfg = yaml.load(fstream)\n                            # Reposition at the top and read the comments.\n                            if file_name not in CONFIG.OS_INIT_FILENAMES:\n                                # If the file name is not a profile init.\n                                fstream.seek(0)\n                                comment = self._extract_yaml_docstring(fstream)\n                                if 'messages' in cfg:\n                                    for message in cfg['messages']:\n                                        message['__doc__'] = comment\n                            napalm_logs.utils.dictupdate(config[os_name], cfg)\n                    except yaml.YAMLError as yamlexc:\n                        log.error('Invalid YAML file: %s', filepath, exc_info=True)\n                        if file_name in CONFIG.OS_INIT_FILENAMES:\n                            # Raise exception and break only when the init file is borked\n                            #   otherwise, it will try loading best efforts.\n                            raise IOError(yamlexc)\n                elif file_extension == 'py':\n                    log.debug('Lazy loading Python module %s', file_)\n                    mod_fp, mod_file, mod_data = imp.find_module(file_name, [os_dir])\n                    mod = imp.load_module(file_name, mod_fp, mod_file, mod_data)\n                    if file_name in CONFIG.OS_INIT_FILENAMES:\n                        # Init file defined as Python module\n                        log.debug('%s seems to be a Python profiler', filepath)\n                        # Init files require to define the `extract` function.\n                        # Sample init file:\n                        # def extract(message):\n                        #     return {'tag': 'A_TAG', 'host': 'hostname'}\n                        if hasattr(mod, CONFIG.INIT_RUN_FUN) and\\\n                           hasattr(getattr(mod, CONFIG.INIT_RUN_FUN), '__call__'):\n                            # if extract is defined and is callable\n                            if 'prefixes' not in config[os_name]:\n                                config[os_name]['prefixes'] = []\n                            config[os_name]['prefixes'].append({\n                                'values': {'tag': ''},\n                                'line': '',\n                                '__python_fun__': getattr(mod, CONFIG.INIT_RUN_FUN),\n                                '__python_mod__': filepath  # Will be used for debugging\n                            })\n                            log.info('Adding the prefix function defined under %s to %s',\n                                     filepath, os_name)\n                        elif file_name != '__init__':\n                            # If __init__.py does not have the extractor function, no problem.\n                            log.warning('%s does not have the \"%s\" function defined. Ignoring.',\n                                        filepath, CONFIG.INIT_RUN_FUN)\n                    else:\n                        # Other python files require the `emit` function.\n                        if hasattr(mod, '__tag__'):\n                            mod_tag = getattr(mod, '__tag__')\n                        else:\n                            log.info('%s does not have __tag__, defaulting the tag to %s', filepath, file_name)\n                            mod_tag = file_name\n                        if hasattr(mod, '__error__'):\n                            mod_err = getattr(mod, '__error__')\n                        else:\n                            log.info('%s does not have __error__, defaulting the error to %s', filepath, file_name)\n                            mod_err = file_name\n                        if hasattr(mod, '__match_on__'):\n                            err_match = getattr(mod, '__match_on__')\n                        else:\n                            err_match = 'tag'\n                        model = CONFIG.OPEN_CONFIG_NO_MODEL\n                        if hasattr(mod, '__yang_model__'):\n                            model = getattr(mod, '__yang_model__')\n                        log.debug('Mathing on %s', err_match)\n                        if hasattr(mod, CONFIG.CONFIG_RUN_FUN) and\\\n                           hasattr(getattr(mod, CONFIG.CONFIG_RUN_FUN), '__call__'):\n                            log.debug('Adding %s with tag:%s, error:%s, matching on:%s',\n                                      file_, mod_tag, mod_err, err_match)\n                            # the structure below must correspond to the VALID_CONFIG structure enforcement\n                            if 'messages' not in config[os_name]:\n                                config[os_name]['messages'] = []\n                            config[os_name]['messages'].append({\n                                'tag': mod_tag,\n                                'error': mod_err,\n                                'match_on': err_match,\n                                '__doc__': mod.__doc__,\n                                '__python_fun__': getattr(mod, CONFIG.CONFIG_RUN_FUN),\n                                '__python_mod__': filepath,  # Will be used for debugging\n                                'line': '',\n                                'model': model,\n                                'values': {},\n                                'mapping': {'variables': {}, 'static': {}}\n                            })\n                        else:\n                            log.warning('%s does not have the \"%s\" function defined. Ignoring.',\n                                        filepath, CONFIG.CONFIG_RUN_FUN)\n                else:\n                    log.info('Ignoring %s (extension not allowed)', filepath)\n            log.debug('-'*40)\n        if not config:\n            msg = 'Could not find proper configuration files under {path}'.format(path=path)\n            log.error(msg)\n            raise IOError(msg)\n        log.debug('Complete config:')\n        log.debug(config)\n        log.debug('ConfigParserg size in bytes: %d', sys.getsizeof(config))\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify if the config dict is valid.", "response": "def _verify_config_dict(self, valid, config, dev_os, key_path=None):\n        '''\n        Verify if the config dict is valid.\n        '''\n        if not key_path:\n            key_path = []\n        for key, value in valid.items():\n            self._verify_config_key(key, value, valid, config, dev_os, key_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _verify_config(self):\n        '''\n        Verify that the config is correct\n        '''\n        if not self.config_dict:\n            self._raise_config_exception('No config found')\n        # Check for device conifg, if there isn't anything then just log, do not raise an exception\n        for dev_os, dev_config in self.config_dict.items():\n            if not dev_config:\n                log.warning('No config found for %s', dev_os)\n                continue\n            # Compare the valid opts with the conifg\n            self._verify_config_dict(CONFIG.VALID_CONFIG, dev_config, dev_os)\n        log.debug('Read the config without error')", "response": "Verify that the config is correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the config of the napalm syslog parser.", "response": "def _build_config(self):\n        '''\n        Build the config of the napalm syslog parser.\n        '''\n        if not self.config_dict:\n            if not self.config_path:\n                # No custom config path requested\n                # Read the native config files\n                self.config_path = os.path.join(\n                    os.path.dirname(os.path.realpath(__file__)),\n                    'config'\n                )\n            log.info('Reading the configuration from %s', self.config_path)\n            self.config_dict = self._load_config(self.config_path)\n        if not self.extension_config_dict and\\\n           self.extension_config_path and\\\n           os.path.normpath(self.extension_config_path) != os.path.normpath(self.config_path):  # same path?\n            # When extension config is not sent as dict\n            # But `extension_config_path` is specified\n            log.info('Reading extension configuration from %s', self.extension_config_path)\n            self.extension_config_dict = self._load_config(self.extension_config_path)\n        if self.extension_config_dict:\n            napalm_logs.utils.dictupdate(self.config_dict, self.extension_config_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _start_auth_proc(self):\n        '''\n        Start the authenticator process.\n        '''\n        log.debug('Computing the signing key hex')\n        verify_key = self.__signing_key.verify_key\n        sgn_verify_hex = verify_key.encode(encoder=nacl.encoding.HexEncoder)\n        log.debug('Starting the authenticator subprocess')\n        auth = NapalmLogsAuthProc(self.certificate,\n                                  self.keyfile,\n                                  self.__priv_key,\n                                  sgn_verify_hex,\n                                  self.auth_address,\n                                  self.auth_port)\n        proc = Process(target=auth.start)\n        proc.start()\n        proc.description = 'Auth process'\n        log.debug('Started auth process as %s with PID %s', proc._name, proc.pid)\n        return proc", "response": "Start the authenticator process."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start_lst_proc(self,\n                        listener_type,\n                        listener_opts):\n        '''\n        Start the listener process.\n        '''\n        log.debug('Starting the listener process for %s', listener_type)\n        listener = NapalmLogsListenerProc(self.opts,\n                                          self.address,\n                                          self.port,\n                                          listener_type,\n                                          listener_opts=listener_opts)\n        proc = Process(target=listener.start)\n        proc.start()\n        proc.description = 'Listener process'\n        log.debug('Started listener process as %s with PID %s', proc._name, proc.pid)\n        return proc", "response": "Start the listener process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the server process.", "response": "def _start_srv_proc(self,\n                        started_os_proc):\n        '''\n        Start the server process.\n        '''\n        log.debug('Starting the server process')\n        server = NapalmLogsServerProc(self.opts,\n                                      self.config_dict,\n                                      started_os_proc,\n                                      buffer=self._buffer)\n        proc = Process(target=server.start)\n        proc.start()\n        proc.description = 'Server process'\n        log.debug('Started server process as %s with PID %s', proc._name, proc.pid)\n        return proc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _start_pub_proc(self,\n                        publisher_type,\n                        publisher_opts,\n                        pub_id):\n        '''\n        Start the publisher process.\n        '''\n        log.debug('Starting the publisher process for %s', publisher_type)\n        publisher = NapalmLogsPublisherProc(self.opts,\n                                            self.publish_address,\n                                            self.publish_port,\n                                            publisher_type,\n                                            self.serializer,\n                                            self.__priv_key,\n                                            self.__signing_key,\n                                            publisher_opts,\n                                            disable_security=self.disable_security,\n                                            pub_id=pub_id)\n        proc = Process(target=publisher.start)\n        proc.start()\n        proc.description = 'Publisher process'\n        log.debug('Started publisher process as %s with PID %s', proc._name, proc.pid)\n        return proc", "response": "Start the publisher process."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the device worker process.", "response": "def _start_dev_proc(self,\n                        device_os,\n                        device_config):\n        '''\n        Start the device worker process.\n        '''\n        log.info('Starting the child process for %s', device_os)\n        dos = NapalmLogsDeviceProc(device_os,\n                                   self.opts,\n                                   device_config)\n        os_proc = Process(target=dos.start)\n        os_proc.start()\n        os_proc.description = '%s device process' % device_os\n        log.debug('Started process %s for %s, having PID %s', os_proc._name, device_os, os_proc.pid)\n        return os_proc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the child processes (one per device OS)", "response": "def start_engine(self):\n        '''\n        Start the child processes (one per device OS)\n        '''\n        if self.disable_security is True:\n            log.warning('***Not starting the authenticator process due to disable_security being set to True***')\n        else:\n            log.debug('Generating the private key')\n            self.__priv_key = nacl.utils.random(nacl.secret.SecretBox.KEY_SIZE)\n            log.debug('Generating the signing key')\n            self.__signing_key = nacl.signing.SigningKey.generate()\n            # start the keepalive thread for the auth sub-process\n            self._processes.append(self._start_auth_proc())\n        log.debug('Starting the internal proxy')\n        proc = self._start_pub_px_proc()\n        self._processes.append(proc)\n        # publisher process start\n        pub_id = 0\n        for pub in self.publisher:\n            publisher_type, publisher_opts = list(pub.items())[0]\n            proc = self._start_pub_proc(publisher_type,\n                                        publisher_opts,\n                                        pub_id)\n            self._processes.append(proc)\n            pub_id += 1\n        # device process start\n        log.info('Starting child processes for each device type')\n        started_os_proc = []\n        for device_os, device_config in self.config_dict.items():\n            if not self._whitelist_blacklist(device_os):\n                log.debug('Not starting process for %s (whitelist-blacklist logic)', device_os)\n                # Ignore devices that are not in the whitelist (if defined),\n                #   or those operating systems that are on the blacklist.\n                # This way we can prevent starting unwanted sub-processes.\n                continue\n            log.debug('Will start %d worker process(es) for %s', self.device_worker_processes, device_os)\n            for proc_index in range(self.device_worker_processes):\n                self._processes.append(self._start_dev_proc(device_os,\n                                                            device_config))\n            started_os_proc.append(device_os)\n        # start the server process\n        self._processes.append(self._start_srv_proc(started_os_proc))\n        # start listener process\n        for lst in self.listener:\n            listener_type, listener_opts = list(lst.items())[0]\n            proc = self._start_lst_proc(listener_type,\n                                        listener_opts)\n            self._processes.append(proc)\n        thread = threading.Thread(target=self._check_children)\n        thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks all of the child processes are still running.", "response": "def _check_children(self):\n        '''\n        Check all of the child processes are still running\n        '''\n        while self.up:\n            time.sleep(1)\n            for process in self._processes:\n                if process.is_alive() is True:\n                    continue\n                log.debug('%s is dead. Stopping the napalm-logs engine.', process.description)\n                self.stop_engine()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the thread that handles the messages and publish them.", "response": "def start(self):\n        '''\n        Listen to messages and publish them.\n        '''\n        self._setup_ipc()\n        # Start suicide polling thread\n        thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))\n        thread.start()\n        signal.signal(signal.SIGTERM, self._exit_gracefully)\n        try:\n            zmq.proxy(self.sub, self.pub)\n        except zmq.ZMQError as error:\n            if self.__up is False:\n                log.info('Exiting on process shutdown')\n                return\n            else:\n                log.error(error, exc_info=True)\n                raise NapalmLogsExit(error)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setup_ipc(self):\n        '''\n        Subscribe to the pub IPC\n        and publish the messages\n        on the right transport.\n        '''\n        self.ctx = zmq.Context()\n        log.debug('Setting up the %s publisher subscriber #%d', self._transport_type, self.pub_id)\n        self.sub = self.ctx.socket(zmq.SUB)\n        self.sub.connect(PUB_IPC_URL)\n        self.sub.setsockopt(zmq.SUBSCRIBE, b'')\n        try:\n            self.sub.setsockopt(zmq.HWM, self.opts['hwm'])\n            # zmq 2\n        except AttributeError:\n            # zmq 3\n            self.sub.setsockopt(zmq.RCVHWM, self.opts['hwm'])", "response": "Setup the IPC subscriber and publish the messages on the right transport."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing the object to be sent over the untrusted channel.", "response": "def _prepare(self, serialized_obj):\n        '''\n        Prepare the object to be sent over the untrusted channel.\n        '''\n        # generating a nonce\n        nonce = nacl.utils.random(nacl.secret.SecretBox.NONCE_SIZE)\n        # encrypting using the nonce\n        encrypted = self.__safe.encrypt(serialized_obj, nonce)\n        # sign the message\n        signed = self.__signing_key.sign(encrypted)\n        return signed"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the event loop.", "response": "def start(self):\n        '''\n        Listen to messages and publish them.\n        '''\n        # metrics\n        napalm_logs_publisher_received_messages = Counter(\n            'napalm_logs_publisher_received_messages',\n            \"Count of messages received by the publisher\",\n            ['publisher_type', 'address', 'port']\n        )\n        napalm_logs_publisher_whitelist_blacklist_check_fail = Counter(\n            'napalm_logs_publisher_whitelist_blacklist_check_fail',\n            \"Count of messages which fail the whitelist/blacklist check\",\n            ['publisher_type', 'address', 'port']\n        )\n        napalm_logs_publisher_messages_published = Counter(\n            'napalm_logs_publisher_messages_published',\n            \"Count of published messages\",\n            ['publisher_type', 'address', 'port']\n        )\n        self._setup_ipc()\n        # Start suicide polling thread\n        thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))\n        thread.start()\n        signal.signal(signal.SIGTERM, self._exit_gracefully)\n        self.transport.start()\n        self.__up = True\n        while self.__up:\n            try:\n                bin_obj = self.sub.recv()\n            except zmq.ZMQError as error:\n                if self.__up is False:\n                    log.info('Exiting on process shutdown')\n                    return\n                else:\n                    log.error(error, exc_info=True)\n                    raise NapalmLogsExit(error)\n            obj = umsgpack.unpackb(bin_obj)\n            if self._strip_message_details:\n                obj.pop('message_details', None)\n                bin_obj = self.serializer_fun(obj)\n            napalm_logs_publisher_received_messages.labels(\n                publisher_type=self._transport_type,\n                address=self.address,\n                port=self.port\n            ).inc()\n            if not napalm_logs.ext.check_whitelist_blacklist(obj['error'],\n                                                             whitelist=self.error_whitelist,\n                                                             blacklist=self.error_blacklist):\n                # Apply the whitelist / blacklist logic\n                # If it doesn't match, jump over.\n                log.debug('This error type is %s. Skipping for %s #%d',\n                          obj['error'],\n                          self._transport_type,\n                          self.pub_id)\n                napalm_logs_publisher_whitelist_blacklist_check_fail.labels(\n                    publisher_type=self._transport_type,\n                    address=self.address,\n                    port=self.port\n                ).inc()\n                continue\n            serialized_obj = self._serialize(obj, bin_obj)\n            log.debug('Publishing the OC object')\n            if not self.disable_security and self.__transport_encrypt:\n                # Encrypt only when needed.\n                serialized_obj = self._prepare(serialized_obj)\n            self.transport.publish(serialized_obj)\n            napalm_logs_publisher_messages_published.labels(\n                publisher_type=self._transport_type,\n                address=self.address,\n                port=self.port\n            ).inc()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_listener(name):\n    '''\n    Return the listener class.\n    '''\n    try:\n        log.debug('Using %s as listener', name)\n        return LISTENER_LOOKUP[name]\n    except KeyError:\n        msg = 'Listener {} is not available. Are the dependencies installed?'.format(name)\n        log.error(msg, exc_info=True)\n        raise InvalidListenerException(msg)", "response": "Get the class of the named listener."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntraversing a dictionary or list using a slash delimiter target string.", "response": "def traverse(data, key, delim=defaults.DEFAULT_DELIM):\n    '''\n    Traverse a dict or list using a slash delimiter target string.\n    The target 'foo/bar/0' will return data['foo']['bar'][0] if\n    this value exists, otherwise will return empty dict.\n    Return None when not found.\n    This can be used to verify if a certain key exists under\n    dictionary hierarchy.\n    '''\n    for each in key.split(delim):\n        if isinstance(data, list):\n            if isinstance(each, six.string_type):\n                embed_match = False\n                # Index was not numeric, lets look at any embedded dicts\n                for embedded in (x for x in data if isinstance(x, dict)):\n                    try:\n                        data = embedded[each]\n                        embed_match = True\n                        break\n                    except KeyError:\n                        pass\n                if not embed_match:\n                    # No embedded dicts matched\n                    return None\n            else:\n                try:\n                    data = data[int(each)]\n                except IndexError:\n                    return None\n        else:\n            try:\n                data = data[each]\n            except (KeyError, TypeError):\n                return None\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _start_keep_alive(self):\n        '''\n        Start the keep alive thread as a daemon\n        '''\n        keep_alive_thread = threading.Thread(target=self.keep_alive)\n        keep_alive_thread.daemon = True\n        keep_alive_thread.start()", "response": "Start the keep alive thread as a daemon"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a keep - alive request to the server.", "response": "def keep_alive(self):\n        '''\n        Send a keep alive request periodically to make sure that the server\n        is still alive. If not then try to reconnect.\n        '''\n        self.ssl_skt.settimeout(defaults.AUTH_KEEP_ALIVE_INTERVAL)\n        while self.__up:\n            try:\n                log.debug('Sending keep-alive message to the server')\n                self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)\n            except socket.error:\n                log.error('Unable to send keep-alive message to the server.')\n                log.error('Re-init the SSL socket.')\n                self.reconnect()\n                log.debug('Trying to re-send the keep-alive message to the server.')\n                self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)\n            msg = self.ssl_skt.recv(len(defaults.AUTH_KEEP_ALIVE_ACK))\n            log.debug('Received %s from the keep-alive server', msg)\n            if msg != defaults.AUTH_KEEP_ALIVE_ACK:\n                log.error('Received %s instead of %s form the auth keep-alive server',\n                          msg, defaults.AUTH_KEEP_ALIVE_ACK)\n                log.error('Re-init the SSL socket.')\n                self.reconnect()\n            time.sleep(defaults.AUTH_KEEP_ALIVE_INTERVAL)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to reconnect and re - authenticate with the server.", "response": "def reconnect(self):\n        '''\n        Try to reconnect and re-authenticate with the server.\n        '''\n        log.debug('Closing the SSH socket.')\n        try:\n            self.ssl_skt.close()\n        except socket.error:\n            log.error('The socket seems to be closed already.')\n        log.debug('Re-opening the SSL socket.')\n        self.authenticate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authenticate(self):\n        '''\n        Authenticate the client and return the private\n        and signature keys.\n\n        Establish a connection through a secured socket,\n        then do the handshake using the napalm-logs\n        auth algorithm.\n        '''\n        log.debug('Authenticate to %s:%d, using the certificate %s',\n                  self.address, self.port, self.certificate)\n        if ':' in self.address:\n            skt_ver = socket.AF_INET6\n        else:\n            skt_ver = socket.AF_INET\n        skt = socket.socket(skt_ver, socket.SOCK_STREAM)\n        self.ssl_skt = ssl.wrap_socket(skt,\n                                       ca_certs=self.certificate,\n                                       cert_reqs=ssl.CERT_REQUIRED)\n        try:\n            self.ssl_skt.connect((self.address, self.port))\n            self.auth_try_id = 0\n        except socket.error as err:\n            log.error('Unable to open the SSL socket.')\n            self.auth_try_id += 1\n            if not self.max_try or self.auth_try_id < self.max_try:\n                log.error('Trying to authenticate again in %d seconds', self.timeout)\n                time.sleep(self.timeout)\n                self.authenticate()\n            log.critical('Giving up, unable to authenticate to %s:%d using the certificate %s',\n                         self.address, self.port, self.certificate)\n            raise ClientConnectException(err)\n\n        # Explicit INIT\n        self.ssl_skt.write(defaults.MAGIC_REQ)\n        # Receive the private key\n        private_key = self.ssl_skt.recv(defaults.BUFFER_SIZE)\n        # Send back explicit ACK\n        self.ssl_skt.write(defaults.MAGIC_ACK)\n        # Read the hex of the verification key\n        verify_key_hex = self.ssl_skt.recv(defaults.BUFFER_SIZE)\n        # Send back explicit ACK\n        self.ssl_skt.write(defaults.MAGIC_ACK)\n        self.priv_key = nacl.secret.SecretBox(private_key)\n        self.verify_key = nacl.signing.VerifyKey(verify_key_hex, encoder=nacl.encoding.HexEncoder)", "response": "Authenticate the client and return the private key and signature keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the connecition with one client.", "response": "def _client_connection(self, conn, addr):\n        '''\n        Handle the connecition with one client.\n        '''\n        log.debug('Established connection with %s:%d', addr[0], addr[1])\n        conn.settimeout(self.socket_timeout)\n        try:\n            while self.__up:\n                msg = conn.recv(self.buffer_size)\n                if not msg:\n                    # log.debug('Received empty message from %s', addr)\n                    # disabled ^ as it was too noisy\n                    continue\n                log.debug('[%s] Received %s from %s. Adding in the queue', time.time(), msg, addr)\n                self.buffer.put((msg, '{}:{}'.format(addr[0], addr[1])))\n        except socket.timeout:\n            if not self.__up:\n                return\n            log.debug('Connection %s:%d timed out', addr[1], addr[0])\n            raise ListenerException('Connection %s:%d timed out' % addr)\n        finally:\n            log.debug('Closing connection with %s', addr)\n            conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _serve_clients(self):\n        '''\n        Accept cients and serve, one separate thread per client.\n        '''\n        self.__up = True\n        while self.__up:\n            log.debug('Waiting for a client to connect')\n            try:\n                conn, addr = self.skt.accept()\n                log.debug('Received connection from %s:%d', addr[0], addr[1])\n            except socket.error as error:\n                if not self.__up:\n                    return\n                msg = 'Received listener socket error: {}'.format(error)\n                log.error(msg, exc_info=True)\n                raise ListenerException(msg)\n            client_thread = threading.Thread(target=self._client_connection, args=(conn, addr,))\n            client_thread.start()", "response": "Accept cients and serve one separate thread per client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        '''\n        Start listening for messages.\n        '''\n        log.debug('Creating the TCP server')\n        if ':' in self.address:\n            self.skt = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        else:\n            self.skt = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        if self.reuse_port:\n            self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            if hasattr(socket, 'SO_REUSEPORT'):\n                self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n            else:\n                log.error('SO_REUSEPORT not supported')\n        try:\n            self.skt.bind((self.address, int(self.port)))\n        except socket.error as msg:\n            error_string = 'Unable to bind to port {} on {}: {}'.format(self.port, self.address, msg)\n            log.error(error_string, exc_info=True)\n            raise BindException(error_string)\n        log.debug('Accepting max %d parallel connections', self.max_clients)\n        self.skt.listen(self.max_clients)\n        self.thread_serve = threading.Thread(target=self._serve_clients)\n        self.thread_serve.start()", "response": "Start listening for messages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn one message dequeued from the listen buffer.", "response": "def receive(self):\n        '''\n        Return one message dequeued from the listen buffer.\n        '''\n        while self.buffer.empty() and self.__up:\n            # This sequence is skipped when the buffer is not empty.\n            sleep_ms = random.randint(0, 1000)\n            # log.debug('The message queue is empty, waiting %d miliseconds', sleep_ms)\n            # disabled ^ as it was too noisy\n            time.sleep(sleep_ms / 1000.0)\n        if not self.buffer.empty():\n            return self.buffer.get(block=False)\n        return '', ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop(self):\n        '''\n        Closing the socket.\n        '''\n        log.info('Stopping the TCP listener')\n        self.__up = False\n        try:\n            self.skt.shutdown(socket.SHUT_RDWR)\n        except socket.error:\n            log.error('The following error may not be critical:', exc_info=True)\n        self.skt.close()", "response": "Stops the TCP listener."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        '''\n        Listen to messages and publish them.\n        '''\n        # counter metrics for messages\n        c_logs_ingested = Counter(\n            'napalm_logs_listener_logs_ingested',\n            'Count of ingested log messages',\n            ['listener_type', 'address', 'port'],\n        )\n        c_messages_published = Counter(\n            'napalm_logs_listener_messages_published',\n            'Count of published messages',\n            ['listener_type', 'address', 'port'],\n        )\n        self._setup_ipc()\n        log.debug('Using the %s listener', self._listener_type)\n        self._setup_listener()\n        self.listener.start()\n        # Start suicide polling thread\n        thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))\n        thread.start()\n        signal.signal(signal.SIGTERM, self._exit_gracefully)\n        self.__up = True\n        while self.__up:\n            try:\n                log_message, log_source = self.listener.receive()\n            except ListenerException as lerr:\n                if self.__up is False:\n                    log.info('Exiting on process shutdown')\n                    return\n                else:\n                    log.error(lerr, exc_info=True)\n                    raise NapalmLogsExit(lerr)\n            log.debug('Received %s from %s. Queueing to the server.', log_message, log_source)\n            if not log_message:\n                log.info('Empty message received from %s. Not queueing to the server.', log_source)\n                continue\n            c_logs_ingested.labels(listener_type=self._listener_type, address=self.address, port=self.port).inc()\n            self.pub.send(umsgpack.packb((log_message, log_source)))\n            c_messages_published.labels(listener_type=self._listener_type, address=self.address, port=self.port).inc()", "response": "Start the listener thread and publish messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef emit(msg_dict):\n    '''\n    Extracts the details from the syslog message\n    and returns an object having the following structure:\n\n    .. code-block:: python\n\n        {\n            u'users': {\n                u'user': {\n                    u'luke': {\n                        u'action': {\n                            u'login': True\n                        },\n                        u'uid': 0\n                    }\n                }\n            }\n        }\n    '''\n    log.debug('Evaluating the message dict:')\n    log.debug(msg_dict)\n    ret = {}\n    extracted = napalm_logs.utils.extract(_RGX, msg_dict['message'], _RGX_PARTS)\n    if not extracted:\n        return ret\n    uid_key_path = 'users//user//{0[user]}//uid'.format(extracted)\n    uid_value = int(extracted['uid'])\n    log.debug('Setting %d under key path %s', uid_value, uid_key_path)\n    ret.update(napalm_logs.utils.setval(uid_key_path, uid_value, dict_=ret))\n    login_key_path = 'users//user//{0[user]}//action//login'.format(extracted)\n    ret.update(napalm_logs.utils.setval(login_key_path, True, dict_=ret))\n    return ret", "response": "Returns the message dict as a dict of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate a line of text against an expression.", "response": "def expr_match(line, expr):\n    '''\n    Evaluate a line of text against an expression. First try a full-string\n    match, next try globbing, and then try to match assuming expr is a regular\n    expression. Originally designed to match minion IDs for\n    whitelists/blacklists.\n    '''\n    if line == expr:\n        return True\n    if fnmatch.fnmatch(line, expr):\n        return True\n    try:\n        if re.match(r'\\A{0}\\Z'.format(expr), line):\n            return True\n    except re.error:\n        pass\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_whitelist_blacklist(value, whitelist=None, blacklist=None):\n    '''\n    Check a whitelist and/or blacklist to see if the value matches it.\n\n    value\n        The item to check the whitelist and/or blacklist against.\n\n    whitelist\n        The list of items that are white-listed. If ``value`` is found\n        in the whitelist, then the function returns ``True``. Otherwise,\n        it returns ``False``.\n\n    blacklist\n        The list of items that are black-listed. If ``value`` is found\n        in the blacklist, then the function returns ``False``. Otherwise,\n        it returns ``True``.\n\n    If both a whitelist and a blacklist are provided, value membership\n    in the blacklist will be examined first. If the value is not found\n    in the blacklist, then the whitelist is checked. If the value isn't\n    found in the whitelist, the function returns ``False``.\n    '''\n    if blacklist is not None:\n        if not hasattr(blacklist, '__iter__'):\n            blacklist = [blacklist]\n        try:\n            for expr in blacklist:\n                if expr_match(value, expr):\n                    return False\n        except TypeError:\n            log.error('Non-iterable blacklist {0}'.format(blacklist))\n\n    if whitelist:\n        if not hasattr(whitelist, '__iter__'):\n            whitelist = [whitelist]\n        try:\n            for expr in whitelist:\n                if expr_match(value, expr):\n                    return True\n        except TypeError:\n            log.error('Non-iterable whitelist {0}'.format(whitelist))\n    else:\n        return True\n\n    return False", "response": "Check a whitelist and blacklist against a value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging dynamically port and host variable in xml Snap! project file", "response": "def set_snap_server_variables(host, port, snap_extension='.xml', path=None):\n    \"\"\" Change dynamically port and host variable in xml Snap! project file\"\"\"\n\n    localdir = os.getcwd()\n    if path is None:\n        os.chdir(os.path.dirname(os.path.realpath(__file__)))\n    else:\n        os.chdir(path)\n    xml_files = [f for f in os.listdir('.') if f.endswith(snap_extension)]\n    for filename in xml_files:\n        with open(filename, 'r') as xf:\n            xml = xf.read()\n        # Change host variable\n        xml = re.sub(r'''<variable name=\"host\"><l>[\\s\\S]*?<\\/l><\\/variable>''',\n                     '''<variable name=\"host\"><l>{}</l></variable>'''.format(host), xml)\n        # Change host argument of \"set $robot host\"\n        xml = re.sub(r'''<custom-block s=\"set \\$robot host to \\%s\"><l>[\\s\\S]*?<\\/l>''',\n                     '''<custom-block s=\"set $robot host to %s\"><l>{}</l>'''.format(host), xml)\n        # Change port variable\n        xml = re.sub(r'''<variable name=\"port\"><l>[\\s\\S]*?<\\/l><\\/variable>''',\n                     '''<variable name=\"port\"><l>{}</l></variable>'''.format(port), xml)\n\n        with open(filename, 'w') as xf:\n            xf.write(xml)\n    os.chdir(localdir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the tornado server run forever.", "response": "def run(self, quiet=None, server=''):\n        \"\"\" Start the tornado server, run forever.\n            'quiet' and 'server' arguments are no longer used, they are keep only for backward compatibility\n        \"\"\"\n\n        try:\n            loop = IOLoop()\n            http_server = HTTPServer(WSGIContainer(self.app))\n            http_server.listen(self.port)\n            loop.start()\n\n        except socket.error as serr:\n            # Re raise the socket error if not \"[Errno 98] Address already in use\"\n            if serr.errno != errno.EADDRINUSE:\n                raise serr\n            else:\n                logger.warning(\"\"\"The webserver port {} is already used.\nThe SnapRobotServer is maybe already run or another software use this port.\"\"\".format(self.port))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_update_loop(thread, update_func):\n    while not thread.should_stop():\n        if thread.should_pause():\n            thread.wait_to_resume()\n\n        start = time.time()\n        if hasattr(thread, '_updated'):\n            thread._updated.clear()\n        update_func()\n        if hasattr(thread, '_updated'):\n            thread._updated.set()\n        end = time.time()\n\n        dt = thread.period - (end - start)\n\n        if dt > 0:\n            time.sleep(dt)", "response": "A function that runs an update function at a predefined frequency."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(self):\n        if self.running:\n            self.stop()\n\n        self._thread = threading.Thread(target=self._wrapped_target)\n        self._thread.daemon = True\n        self._thread.start()", "response": "Start the run method as a new thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop(self, wait=True):\n        if self.started:\n            self._running.clear()\n            self._resume.set()\n\n            # We cannot wait for ourself\n            if wait and (threading.current_thread() != self._thread):\n                while self._thread.is_alive():\n                    self._running.clear()\n                    self._resume.set()\n                    self._thread.join(timeout=1.0)\n\n            self._started.clear()\n            self._resume.clear()", "response": "Stop the thread.\n\n        More precisely, sends the stopping signal to the thread. It is then up to the run method to correctly responds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for the thread to actually start.", "response": "def wait_to_start(self, allow_failure=False):\n        \"\"\" Wait for the thread to actually starts. \"\"\"\n        self._started.wait()\n\n        if self._crashed and not allow_failure:\n            self._thread.join()\n            raise RuntimeError('Setup failed, see {} Traceback'\n                               'for details.'.format(self._thread.name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_vrep(config, vrep_host='127.0.0.1', vrep_port=19997, scene=None,\n              tracked_objects=[], tracked_collisions=[],\n              id=None, shared_vrep_io=None):\n    \"\"\" Create a robot from a V-REP instance.\n\n    :param config: robot configuration (either the path to the json or directly the dictionary)\n    :type config: str or dict\n    :param str vrep_host: host of the V-REP server\n    :param int vrep_port: port of the V-REP server\n    :param str scene: path to the V-REP scene to load and start\n    :param list tracked_objects: list of V-REP dummy object to track\n    :param list tracked_collisions: list of V-REP collision to track\n    :param int id: robot id in simulator (useful when using a scene with multiple robots)\n    :param vrep_io: use an already connected VrepIO (useful when using a scene with multiple robots)\n    :type vrep_io: :class:`~pypot.vrep.io.VrepIO`\n\n    This function tries to connect to a V-REP instance and expects to find motors with names corresponding as the ones found in the config.\n\n    .. note:: The :class:`~pypot.robot.robot.Robot` returned will also provide a convenience reset_simulation method which resets the simulation and the robot position to its intial stance.\n\n    .. note:: Using the same configuration, you should be able to switch from a real to a simulated robot just by switching from :func:`~pypot.robot.config.from_config` to :func:`~pypot.vrep.from_vrep`.\n        For instance::\n\n            import json\n\n            with open('my_config.json') as f:\n                config = json.load(f)\n\n            from pypot.robot import from_config\n            from pypot.vrep import from_vrep\n\n            real_robot = from_config(config)\n            simulated_robot = from_vrep(config, '127.0.0.1', 19997, 'poppy.ttt')\n\n    \"\"\"\n    if shared_vrep_io is None:\n        vrep_io = VrepIO(vrep_host, vrep_port)\n    else:\n        vrep_io = shared_vrep_io\n\n    vreptime = vrep_time(vrep_io)\n    pypot_time.time = vreptime.get_time\n    pypot_time.sleep = vreptime.sleep\n\n    if isinstance(config, basestring):\n        with open(config) as f:\n            config = json.load(f, object_pairs_hook=OrderedDict)\n\n    motors = [motor_from_confignode(config, name)\n              for name in config['motors'].keys()]\n\n    vc = VrepController(vrep_io, scene, motors, id=id)\n    vc._init_vrep_streaming()\n\n    sensor_controllers = []\n\n    if tracked_objects:\n        sensors = [ObjectTracker(name) for name in tracked_objects]\n        vot = VrepObjectTracker(vrep_io, sensors)\n        sensor_controllers.append(vot)\n\n    if tracked_collisions:\n        sensors = [VrepCollisionDetector(name) for name in tracked_collisions]\n        vct = VrepCollisionTracker(vrep_io, sensors)\n        sensor_controllers.append(vct)\n\n    robot = Robot(motor_controllers=[vc],\n                  sensor_controllers=sensor_controllers)\n\n    for m in robot.motors:\n        m.goto_behavior = 'minjerk'\n\n    init_pos = {m: m.goal_position for m in robot.motors}\n\n    make_alias(config, robot)\n\n    def start_simu():\n        vrep_io.start_simulation()\n\n        for m, p in init_pos.iteritems():\n            m.goal_position = p\n\n        vc.start()\n\n        if tracked_objects:\n            vot.start()\n\n        if tracked_collisions:\n            vct.start()\n\n        while vrep_io.get_simulation_current_time() < 1.:\n            sys_time.sleep(0.1)\n\n    def stop_simu():\n        if tracked_objects:\n            vot.stop()\n\n        if tracked_collisions:\n            vct.stop()\n\n        vc.stop()\n        vrep_io.stop_simulation()\n\n    def reset_simu():\n        stop_simu()\n        sys_time.sleep(0.5)\n        start_simu()\n\n    robot.start_simulation = start_simu\n    robot.stop_simulation = stop_simu\n    robot.reset_simulation = reset_simu\n\n    def current_simulation_time(robot):\n        return robot._controllers[0].io.get_simulation_current_time()\n    Robot.current_simulation_time = property(lambda robot: current_simulation_time(robot))\n\n    def get_object_position(robot, object, relative_to_object=None):\n        return vrep_io.get_object_position(object, relative_to_object)\n    Robot.get_object_position = partial(get_object_position, robot)\n\n    def get_object_orientation(robot, object, relative_to_object=None):\n        return vrep_io.get_object_orientation(object, relative_to_object)\n    Robot.get_object_orientation = partial(get_object_orientation, robot)\n\n    return robot", "response": "Create a robot from a V - REP instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_control_mode(self, ids):\n        to_get_ids = [id for id in ids if id not in self._known_mode]\n        limits = self.get_angle_limit(to_get_ids, convert=False)\n        modes = ['wheel' if limit == (0, 0) else 'joint' for limit in limits]\n\n        self._known_mode.update(zip(to_get_ids, modes))\n\n        return tuple(self._known_mode[id] for id in ids)", "response": "Gets the control mode for the specified motors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_wheel_mode(self, ids):\n        self.set_control_mode(dict(zip(ids, itertools.repeat('wheel'))))", "response": "Sets the motors to wheel mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_joint_mode(self, ids):\n        self.set_control_mode(dict(zip(ids, itertools.repeat('joint'))))", "response": "Sets the motors to joint mode."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_angle_limit(self, limit_for_id, **kwargs):\n        convert = kwargs['convert'] if 'convert' in kwargs else self._convert\n\n        if 'wheel' in self.get_control_mode(limit_for_id.keys()):\n            raise ValueError('can not change the angle limit of a motor in wheel mode')\n\n        if (0, 0) in limit_for_id.values():\n            raise ValueError('can not set limit to (0, 0)')\n\n        self._set_angle_limit(limit_for_id, convert=convert)", "response": "Sets the angle limit for motors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        self.stop_sync()\n        [c.io.close() for c in self._controllers if c.io is not None]", "response": "Closes the robot by stopping synchronization and all controllers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts all the synchonization loop.", "response": "def start_sync(self):\n        \"\"\" Starts all the synchonization loop (sensor/effector controllers). \"\"\"\n        if self._syncing:\n            return\n\n        [c.start() for c in self._controllers]\n        [c.wait_to_start() for c in self._controllers]\n        self._primitive_manager.start()\n        self._primitive_manager._running.wait()\n\n        self._syncing = True\n\n        logger.info('Starting robot synchronization.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstop all the synchonization loop.", "response": "def stop_sync(self):\n        \"\"\" Stops all the synchonization loop (sensor/effector controllers). \"\"\"\n        if not self._syncing:\n            return\n\n        if self._primitive_manager.running:\n            self._primitive_manager.stop()\n\n        [c.stop() for c in self._controllers]\n        [s.close() for s in self.sensors if hasattr(s, 'close')]\n\n        self._syncing = False\n\n        logger.info('Stopping robot synchronization.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef goto_position(self, position_for_motors, duration, control=None, wait=False):\n        for i, (motor_name, position) in enumerate(position_for_motors.iteritems()):\n            w = False if i < len(position_for_motors) - 1 else wait\n\n            m = getattr(self, motor_name)\n            m.goto_position(position, duration, control, wait=w)", "response": "Moves a subset of the motors to a position within a specific duration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges all settings to guarantee the motors will be used at maximum power.", "response": "def power_up(self):\n        \"\"\" Changes all settings to guarantee the motors will be used at their maximum power. \"\"\"\n        for m in self.motors:\n            m.compliant = False\n            m.moving_speed = 0\n            m.torque_limit = 100.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_config(self):\n        from ..dynamixel.controller import DxlController\n\n        dxl_controllers = [c for c in self._controllers\n                           if isinstance(c, DxlController)]\n\n        config = {}\n\n        config['controllers'] = {}\n        for i, c in enumerate(dxl_controllers):\n            name = 'dxl_controller_{}'.format(i)\n            config['controllers'][name] = {\n                'port': c.io.port,\n                'sync_read': c.io._sync_read,\n                'attached_motors': [m.name for m in c.motors],\n            }\n\n        config['motors'] = {}\n        for m in self.motors:\n            config['motors'][m.name] = {\n                'id': m.id,\n                'type': m.model,\n                'offset': m.offset,\n                'orientation': 'direct' if m.direct else 'indirect',\n                'angle_limit': m.angle_limit,\n            }\n\n            if m.angle_limit == (0, 0):\n                config['motors']['wheel_mode'] = True\n\n        config['motorgroups'] = {}\n\n        return config", "response": "Generates the config for the current robot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detect_blob(self, img, filters):\n        acc_mask = ones(img.shape[:2], dtype=uint8) * 255\n\n        rgb = img.copy()\n        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n        for c, (min, max) in filters.items():\n            img = rgb if c in 'RGB' else hsv\n\n            mask = img[:, :, self.channels[c]]\n            mask[mask < min] = 0\n            mask[mask > max] = 0\n\n            acc_mask &= mask\n\n        kernel = ones((5, 5), uint8)\n        acc_mask = cv2.dilate(cv2.erode(acc_mask, kernel), kernel)\n\n        circles = cv2.HoughCircles(acc_mask, cv2.HOUGH_GRADIENT, 3, img.shape[0] / 5.)\n        return circles.reshape(-1, 3) if circles is not None else []", "response": "Detects the blob of a given image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup(self):\n        self._init_vrep_streaming()\n\n        # Init lifo for temperature spoofing\n        for m in self.motors:\n            m.__dict__['_load_fifo'] = deque(200 * [1], maxlen=200)\n\n        self.update()", "response": "Setups the controller by reading position for all motors and setting position for all motors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self):\n        # Read all the angle limits\n        h, _, l, _ = self.io.call_remote_api('simxGetObjectGroupData',\n                                             remote_api.sim_object_joint_type,\n                                             16,\n                                             streaming=True)\n        limits4handle = {hh: (ll, lr) for hh, ll, lr in zip(h, l[::2], l[1::2])}\n\n        for m in self.motors:\n            tmax = torque_max[m.model]\n\n            # Read values from V-REP and set them to the Motor\n            p = round(\n                rad2deg(self.io.get_motor_position(motor_name=self._motor_name(m))), 1)\n            m.__dict__['present_position'] = p\n\n            l = 100. * self.io.get_motor_force(motor_name=self._motor_name(m)) / tmax\n            m.__dict__['present_load'] = l\n\n            m.__dict__['_load_fifo'].append(abs(l))\n            m.__dict__['present_temperature'] = 25 + \\\n                round(2.5 * sum(m.__dict__['_load_fifo']) / len(m.__dict__['_load_fifo']), 1)\n\n            ll, lr = limits4handle[self.io._object_handles[self._motor_name(m)]]\n            m.__dict__['lower_limit'] = rad2deg(ll)\n            m.__dict__['upper_limit'] = rad2deg(ll) + rad2deg(lr)\n\n            # Send new values from Motor to V-REP\n            p = deg2rad(round(m.__dict__['goal_position'], 1))\n            self.io.set_motor_position(motor_name=self._motor_name(m), position=p)\n\n            t = m.__dict__['torque_limit'] * tmax / 100.\n\n            if m.__dict__['compliant']:\n                t = 0.\n\n            self.io.set_motor_force(motor_name=self._motor_name(m), force=t)", "response": "Synchronization update loop.\n\n        At each update all motor position are read from vrep and set to the motors. The motors target position are also send to v-rep."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the position and orientation of the tracked objects.", "response": "def update(self):\n        \"\"\" Updates the position and orientation of the tracked objects. \"\"\"\n        for s in self.sensors:\n            s.position = self.io.get_object_position(object_name=s.name)\n            s.orientation = self.io.get_object_orientation(object_name=s.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self):\n\n        for s in self.sensors:\n            s.colliding = self.io.get_collision_state(collision_name=s.name)", "response": "Update the state of the collision detectors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the homogeneous transformation matrix for this link.", "response": "def get_transformation_matrix(self, theta):\n        \"\"\" Computes the homogeneous transformation matrix for this link. \"\"\"\n        ct = numpy.cos(theta + self.theta)\n        st = numpy.sin(theta + self.theta)\n        ca = numpy.cos(self.alpha)\n        sa = numpy.sin(self.alpha)\n\n        return numpy.matrix(((ct, -st * ca, st * sa, self.a * ct),\n                             (st, ct * ca, -ct * sa, self.a * st),\n                             (0, sa, ca, self.d),\n                             (0, 0, 0, 1)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward_kinematics(self, q):\n        q = numpy.array(q).flatten()\n\n        if len(q) != len(self.links):\n            raise ValueError('q must contain as element as the number of links')\n\n        tr = self.base.copy()\n\n        l = []\n\n        for link, theta in zip(self.links, q):\n            tr = tr * link.get_transformation_matrix(theta)\n\n            l.append(tr)\n\n        tr = tr * self.tool\n        l.append(tr)\n        return tr, numpy.asarray(l)", "response": "Computes the homogeneous transformation matrix of the end effector of the chain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inverse_kinematics(self, end_effector_transformation,\n                           q=None,\n                           max_iter=1000, tolerance=0.05,\n                           mask=numpy.ones(6),\n                           use_pinv=False):\n        \"\"\" Computes the joint angles corresponding to the end effector transformation.\n\n        :param end_effector_transformation: the end effector homogeneous transformation matrix\n        :param vector q: initial estimate of the joint angles\n        :param int max_iter: maximum number of iteration\n        :param float tolerance: tolerance before convergence\n        :param mask: specify the cartesian DOF that will be ignore (in the case of a chain with less than 6 joints).\n        :rtype: vector of the joint angles (theta 1, theta 2, ..., theta n)\n\n        \"\"\"\n        if q is None:\n            q = numpy.zeros((len(self.links), 1))\n        q = numpy.matrix(q.reshape(-1, 1))\n\n        best_e = numpy.ones(6) * numpy.inf\n        best_q = None\n        alpha = 1.0\n\n        for _ in range(max_iter):\n            e = numpy.multiply(transform_difference(self.forward_kinematics(q)[0], end_effector_transformation), mask)\n            d = numpy.linalg.norm(e)\n\n            if d < numpy.linalg.norm(best_e):\n                best_e = e.copy()\n                best_q = q.copy()\n                alpha *= 2.0 ** (1.0 / 8.0)\n            else:\n                q = best_q.copy()\n                e = best_e.copy()\n                alpha *= 0.5\n\n            if use_pinv:\n                dq = numpy.linalg.pinv(self._jacob0(q)) * e.reshape((-1, 1))\n            else:\n                dq = self._jacob0(q).T * e.reshape((-1, 1))\n            q += alpha * dq\n\n            # d = numpy.linalg.norm(dq)\n            if d < tolerance:\n                return q\n\n        else:\n            raise ValueError('could not converge d={}'.format(numpy.linalg.norm(best_e)))", "response": "Computes the inverse kinematics of the current chain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self):\n        pos = self._amp * numpy.sin(self._freq * 2.0 * numpy.pi * self.elapsed_time +\n                                    self._phase * numpy.pi / 180.0) + self._offset\n\n        for m in self.motor_list:\n            m.goal_position = pos", "response": "Update the goal position of the motors in the motors list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_available_ports():\n    if platform.system() == 'Darwin':\n        return glob.glob('/dev/tty.usb*')\n\n    elif platform.system() == 'Linux':\n        return glob.glob('/dev/ttyACM*') + glob.glob('/dev/ttyUSB*') + glob.glob('/dev/ttyAMA*')\n\n    elif sys.platform.lower() == 'cygwin':\n        return glob.glob('/dev/com*')\n\n    elif platform.system() == 'Windows':\n        import _winreg\n        import itertools\n\n        ports = []\n        path = 'HARDWARE\\\\DEVICEMAP\\\\SERIALCOMM'\n        key = _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE, path)\n\n        for i in itertools.count():\n            try:\n                ports.append(str(_winreg.EnumValue(key, i)[1]))\n            except WindowsError:\n                return ports\n    else:\n        raise EnvironmentError('{} is an unsupported platform, cannot find serial ports !'.format(platform.system()))\n    return []", "response": "Tries to find the available serial ports on your system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_port_vendor_info(port=None):\n\n    port_info_dict = dict((x[0], x[2]) for x in serial.tools.list_ports.comports())\n    return port_info_dict[port] if port is not None else port_info_dict", "response": "Return the vendor informations of a USB2Serial device."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the port with the specified attached motor ids.", "response": "def find_port(ids, strict=True):\n    \"\"\" Find the port with the specified attached motor ids.\n\n        :param list ids: list of motor ids to find\n        :param bool strict: specify if all ids should be find (when set to False, only half motor must be found)\n\n        .. warning:: If two (or more) ports are attached to the same list of motor ids the first match will be returned.\n\n    \"\"\"\n    ids_founds = []\n    for port in get_available_ports():\n        for DxlIOCls in (DxlIO, Dxl320IO):\n            try:\n                with DxlIOCls(port) as dxl:\n                    _ids_founds = dxl.scan(ids)\n                    ids_founds += _ids_founds\n\n                    if strict and len(_ids_founds) == len(ids):\n                        return port\n\n                    if not strict and len(_ids_founds) >= len(ids) / 2:\n                        logger.warning('Missing ids: {}'.format(ids, list(set(ids) - set(_ids_founds))))\n                        return port\n\n                    if len(ids_founds) > 0:\n                        logger.warning('Port:{} ids found:{}'.format(port, _ids_founds))\n\n            except DxlError:\n                logger.warning('DxlError on port {}'.format(port))\n                continue\n\n    raise IndexError('No suitable port found for ids {}. These ids are missing {} !'.format(\n        ids, list(set(ids) - set(ids_founds))))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a :class:`~pypot.robot.robot.Robot` by detecting dynamixel motors on all available ports.", "response": "def autodetect_robot():\n    \"\"\" Creates a :class:`~pypot.robot.robot.Robot` by detecting dynamixel motors on all available ports. \"\"\"\n    motor_controllers = []\n\n    for port in get_available_ports():\n        for DxlIOCls in (DxlIO, Dxl320IO):\n            dxl_io = DxlIOCls(port)\n            ids = dxl_io.scan()\n\n            if not ids:\n                dxl_io.close()\n                continue\n\n            models = dxl_io.get_model(ids)\n\n            motorcls = {\n                'MX': DxlMXMotor,\n                'RX': DxlAXRXMotor,\n                'AX': DxlAXRXMotor,\n                'XL': DxlXL320Motor,\n                'SR': DxlSRMotor,\n            }\n\n            motors = [motorcls[model[:2]](id, model=model)\n                      for id, model in zip(ids, models)]\n\n            c = BaseDxlController(dxl_io, motors)\n            motor_controllers.append(c)\n            break\n\n    return Robot(motor_controllers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the value from the specified register and sets it to the motors attribute.", "response": "def get_register(self, motors, disable_sync_read=False):\n        \"\"\" Gets the value from the specified register and sets it to the :class:`~pypot.dynamixel.motor.DxlMotor`. \"\"\"\n        if not motors:\n            return False\n\n        ids = [m.id for m in motors]\n        getter = getattr(self.io, 'get_{}'.format(self.regname))\n\n        values = (sum([list(getter([id])) for id in ids], [])\n                  if disable_sync_read else\n                  getter(ids))\n\n        if not values:\n            return False\n\n        for m, val in zip(motors, values):\n            m.__dict__[self.varname] = val\n\n        for m in motors:\n            m._read_synced[self.varname].done()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_register(self, motors):\n        if not motors:\n            return\n        ids = [m.id for m in motors]\n\n        values = (m.__dict__[self.varname] for m in motors)\n        getattr(self.io, 'set_{}'.format(self.regname))(dict(zip(ids, values)))\n\n        for m in motors:\n            m._write_synced[self.varname].done()", "response": "Sets the value of the variable in the specified register."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _filter(self, data):\n        filtered_data = []\n        for queue, data in zip(self._raw_data_queues, data):\n            queue.append(data)\n            filtered_data.append(numpy.median(queue))\n\n        return filtered_data", "response": "Apply a filter to reduce noisy data.\n\n           Return the median value of a heap of data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simxGetJointPosition(clientID, jointHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    position = ct.c_float()\n    return c_GetJointPosition(clientID, jointHandle, ct.byref(position), operationMode), position.value", "response": "Get the current joint position."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the joint position.", "response": "def simxSetJointPosition(clientID, jointHandle, position, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetJointPosition(clientID, jointHandle, position, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simxGetJointMatrix(clientID, jointHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    matrix = (ct.c_float*12)()\n    ret = c_GetJointMatrix(clientID, jointHandle, matrix, operationMode)\n    arr = []\n    for i in range(12):\n        arr.append(matrix[i])\n    return ret, arr", "response": "This function is used to get the matrix of joints for a given client."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the matrix of a spherical joint.", "response": "def simxSetSphericalJointMatrix(clientID, jointHandle, matrix, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    matrix = (ct.c_float*12)(*matrix)\n    return c_SetSphericalJointMatrix(clientID, jointHandle, matrix, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the target velocity of a joint.", "response": "def simxSetJointTargetVelocity(clientID, jointHandle, targetVelocity, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetJointTargetVelocity(clientID, jointHandle, targetVelocity, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxSetJointTargetPosition(clientID, jointHandle, targetPosition, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetJointTargetPosition(clientID, jointHandle, targetPosition, operationMode)", "response": "Set the target position of a joint."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the force of a joint.", "response": "def simxJointGetForce(clientID, jointHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    force = ct.c_float()\n    return c_GetJointForce(clientID, jointHandle, ct.byref(force), operationMode), force.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simxSetJointForce(clientID, jointHandle, force, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    return c_SetJointForce(clientID, jointHandle, force, operationMode)", "response": "Set the force of a joint in a V - REP user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplease have a look at the function description/documentation in the V-REP user manual", "response": "def simxReadForceSensor(clientID, forceSensorHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    state = ct.c_ubyte()\n    forceVector  = (ct.c_float*3)()\n    torqueVector = (ct.c_float*3)()\n    ret = c_ReadForceSensor(clientID, forceSensorHandle, ct.byref(state), forceVector, torqueVector, operationMode)\n    arr1 = []\n    for i in range(3):\n        arr1.append(forceVector[i])\n    arr2 = []\n    for i in range(3):\n        arr2.append(torqueVector[i])\n    #if sys.version_info[0] == 3:\n    #    state=state.value\n    #else:\n    #    state=ord(state.value)\n    return ret, state.value, arr1, arr2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxReadVisionSensor(clientID, sensorHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    detectionState = ct.c_ubyte()\n    auxValues      = ct.POINTER(ct.c_float)()\n    auxValuesCount = ct.POINTER(ct.c_int)()\n    ret = c_ReadVisionSensor(clientID, sensorHandle, ct.byref(detectionState), ct.byref(auxValues), ct.byref(auxValuesCount), operationMode)\n    \n    auxValues2 = []\n    if ret == 0:\n        s = 0\n        for i in range(auxValuesCount[0]):\n            auxValues2.append(auxValues[s:s+auxValuesCount[i+1]])\n            s += auxValuesCount[i+1]\n\n        #free C buffers\n        c_ReleaseBuffer(auxValues)\n        c_ReleaseBuffer(auxValuesCount)\n\n    return ret, bool(detectionState.value!=0), auxValues2", "response": "This function is used to read a single V - REP sensor from a V - REP device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the handle of an object.", "response": "def simxGetObjectHandle(clientID, objectName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    handle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(objectName) is str):\n        objectName=objectName.encode('utf-8')\n    return c_GetObjectHandle(clientID, objectName, ct.byref(handle), operationMode), handle.value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npleasing have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetVisionSensorImage(clientID, sensorHandle, options, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    resolution = (ct.c_int*2)()\n    c_image  = ct.POINTER(ct.c_byte)()\n    bytesPerPixel = 3\n    if (options and 1) != 0:\n        bytesPerPixel = 1\n    ret = c_GetVisionSensorImage(clientID, sensorHandle, resolution, ct.byref(c_image), options, operationMode)\n\n    reso = []\n    image = []\n    if (ret == 0):\n        image = [None]*resolution[0]*resolution[1]*bytesPerPixel\n        for i in range(resolution[0] * resolution[1] * bytesPerPixel):\n            image[i] = c_image[i]\n        for i in range(2):\n            reso.append(resolution[i])\n    return ret, reso, image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simxSetVisionSensorImage(clientID, sensorHandle, image, options, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    size = len(image)\n    image_bytes  = (ct.c_byte*size)(*image)\n    return c_SetVisionSensorImage(clientID, sensorHandle, image_bytes, size, options, operationMode)", "response": "Set the image of a vision sensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxGetVisionSensorDepthBuffer(clientID, sensorHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    c_buffer  = ct.POINTER(ct.c_float)()\n    resolution = (ct.c_int*2)()\n    ret = c_GetVisionSensorDepthBuffer(clientID, sensorHandle, resolution, ct.byref(c_buffer), operationMode)\n    reso = []\n    buffer = []\n    if (ret == 0):\n        buffer = [None]*resolution[0]*resolution[1]\n        for i in range(resolution[0] * resolution[1]):\n            buffer[i] = c_buffer[i]\n        for i in range(2):\n            reso.append(resolution[i])\n    return ret, reso, buffer", "response": "This function returns the depth buffer of a V - REP Vision sensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npleases have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetObjectChild(clientID, parentObjectHandle, childIndex, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    childObjectHandle = ct.c_int()\n    return c_GetObjectChild(clientID, parentObjectHandle, childIndex, ct.byref(childObjectHandle), operationMode), childObjectHandle.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simxGetObjectParent(clientID, childObjectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    parentObjectHandle = ct.c_int()\n    return c_GetObjectParent(clientID, childObjectHandle, ct.byref(parentObjectHandle), operationMode), parentObjectHandle.value", "response": "Get the parent of an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npleases have a look at the function description/documentation in the V-REP user manual", "response": "def simxReadProximitySensor(clientID, sensorHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    detectionState = ct.c_ubyte()\n    detectedObjectHandle = ct.c_int()\n    detectedPoint  = (ct.c_float*3)()\n    detectedSurfaceNormalVector = (ct.c_float*3)()\n    ret = c_ReadProximitySensor(clientID, sensorHandle, ct.byref(detectionState), detectedPoint, ct.byref(detectedObjectHandle), detectedSurfaceNormalVector, operationMode)\n    arr1 = []\n    for i in range(3):\n        arr1.append(detectedPoint[i])\n    arr2 = []\n    for i in range(3):\n        arr2.append(detectedSurfaceNormalVector[i])\n    return ret, bool(detectionState.value!=0), arr1, detectedObjectHandle.value, arr2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxLoadModel(clientID, modelPathAndName, options, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    baseHandle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(modelPathAndName) is str):\n        modelPathAndName=modelPathAndName.encode('utf-8')\n    return c_LoadModel(clientID, modelPathAndName, options, ct.byref(baseHandle), operationMode), baseHandle.value", "response": "Load a model from a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a new V -REP UI.", "response": "def simxLoadUI(clientID, uiPathAndName, options, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    count = ct.c_int()\n    uiHandles = ct.POINTER(ct.c_int)()\n    if (sys.version_info[0] == 3) and (type(uiPathAndName) is str):\n        uiPathAndName=uiPathAndName.encode('utf-8')\n    ret = c_LoadUI(clientID, uiPathAndName, options, ct.byref(count), ct.byref(uiHandles), operationMode)\n    \n    handles = []\n    if ret == 0:\n        for i in range(count.value):\n            handles.append(uiHandles[i])\n        #free C buffers\n        c_ReleaseBuffer(uiHandles)\n\n    return ret, handles"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a scene from a file.", "response": "def simxLoadScene(clientID, scenePathAndName, options, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(scenePathAndName) is str):\n        scenePathAndName=scenePathAndName.encode('utf-8')\n    return c_LoadScene(clientID, scenePathAndName, options, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simxGetUIHandle(clientID, uiName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    handle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(uiName) is str):\n        uiName=uiName.encode('utf-8')\n    return c_GetUIHandle(clientID, uiName, ct.byref(handle), operationMode), handle.value", "response": "Get the handle of a UI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simxGetUISlider(clientID, uiHandle, uiButtonID, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    position = ct.c_int()\n    return c_GetUISlider(clientID, uiHandle, uiButtonID, ct.byref(position), operationMode), position.value", "response": "This function returns the slider value of the specified UI button."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simxSetUISlider(clientID, uiHandle, uiButtonID, position, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetUISlider(clientID, uiHandle, uiButtonID, position, operationMode)", "response": "Set the slider on the specified button."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxGetUIEventButton(clientID, uiHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    uiEventButtonID = ct.c_int()\n    auxValues = (ct.c_int*2)()\n    ret = c_GetUIEventButton(clientID, uiHandle, ct.byref(uiEventButtonID), auxValues, operationMode)\n    arr = []\n    for i in range(2):\n        arr.append(auxValues[i])\n    return ret, uiEventButtonID.value, arr", "response": "This function returns the value of a specific UI event button."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simxGetUIButtonProperty(clientID, uiHandle, uiButtonID, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    prop = ct.c_int()\n    return c_GetUIButtonProperty(clientID, uiHandle, uiButtonID, ct.byref(prop), operationMode), prop.value", "response": "Get the value of a UI button property."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the value of a UI button property.", "response": "def simxSetUIButtonProperty(clientID, uiHandle, uiButtonID, prop, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    \n    return c_SetUIButtonProperty(clientID, uiHandle, uiButtonID, prop, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a statusbar message to the V -REP user s virtual server.", "response": "def simxAddStatusbarMessage(clientID, message, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(message) is str):\n        message=message.encode('utf-8')\n    return c_AddStatusbarMessage(clientID, message, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npleasing have a look at the function description/documentation in the V-REP user manual", "response": "def simxAuxiliaryConsoleOpen(clientID, title, maxLines, mode, position, size, textColor, backgroundColor, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    consoleHandle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(title) is str):\n        title=title.encode('utf-8')\n    if position != None:\n        c_position = (ct.c_int*2)(*position)\n    else:\n        c_position = None\n    if size != None:\n        c_size = (ct.c_int*2)(*size)\n    else:\n        c_size = None\n    if textColor != None:\n        c_textColor = (ct.c_float*3)(*textColor)\n    else:\n        c_textColor = None\n    if backgroundColor != None:\n        c_backgroundColor = (ct.c_float*3)(*backgroundColor)\n    else:\n        c_backgroundColor = None\n    return c_AuxiliaryConsoleOpen(clientID, title, maxLines, mode, c_position, c_size, c_textColor, c_backgroundColor, ct.byref(consoleHandle), operationMode), consoleHandle.value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npleasing have a look at the function description/documentation in the V-REP user manual", "response": "def simxAuxiliaryConsolePrint(clientID, consoleHandle, txt, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(txt) is str):\n        txt=txt.encode('utf-8')\n    return c_AuxiliaryConsolePrint(clientID, consoleHandle, txt, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simxAuxiliaryConsoleShow(clientID, consoleHandle, showState, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_AuxiliaryConsoleShow(clientID, consoleHandle, showState, operationMode)", "response": "This function is used to call c_AuxiliaryConsoleShow in a V - REP user with the same arguments as c_AuxiliaryConsoleShow."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simxGetObjectOrientation(clientID, objectHandle, relativeToObjectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    eulerAngles = (ct.c_float*3)()\n    ret = c_GetObjectOrientation(clientID, objectHandle, relativeToObjectHandle, eulerAngles, operationMode)\n    arr = []\n    for i in range(3):\n        arr.append(eulerAngles[i])\n    return ret, arr", "response": "This function is used to get the orientation of an object in a VREP server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the position of an object in a VREP object.", "response": "def simxGetObjectPosition(clientID, objectHandle, relativeToObjectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    position = (ct.c_float*3)()\n    ret = c_GetObjectPosition(clientID, objectHandle, relativeToObjectHandle, position, operationMode)\n    arr = []\n    for i in range(3):\n        arr.append(position[i])\n    return ret, arr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simxSetObjectOrientation(clientID, objectHandle, relativeToObjectHandle, eulerAngles, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    angles = (ct.c_float*3)(*eulerAngles)\n    return c_SetObjectOrientation(clientID, objectHandle, relativeToObjectHandle, angles, operationMode)", "response": "Set the orientation of an object in the V -REP server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the position of an object in a VREP container.", "response": "def simxSetObjectPosition(clientID, objectHandle, relativeToObjectHandle, position, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    c_position = (ct.c_float*3)(*position)\n    return c_SetObjectPosition(clientID, objectHandle, relativeToObjectHandle, c_position, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the parent object of an object.", "response": "def simxSetObjectParent(clientID, objectHandle, parentObject, keepInPlace, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetObjectParent(clientID, objectHandle, parentObject, keepInPlace, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simxSetUIButtonLabel(clientID, uiHandle, uiButtonID, upStateLabel, downStateLabel, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if sys.version_info[0] == 3:\n        if type(upStateLabel) is str:\n            upStateLabel=upStateLabel.encode('utf-8')\n        if type(downStateLabel) is str:\n            downStateLabel=downStateLabel.encode('utf-8')\n    return c_SetUIButtonLabel(clientID, uiHandle, uiButtonID, upStateLabel, downStateLabel, operationMode)", "response": "Set the UI button label."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simxGetLastErrors(clientID, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    errors =[]\n    errorCnt = ct.c_int()\n    errorStrings = ct.POINTER(ct.c_char)()\n    ret = c_GetLastErrors(clientID, ct.byref(errorCnt), ct.byref(errorStrings), operationMode)\n    if ret == 0:\n        s = 0\n        for i in range(errorCnt.value):\n            a = bytearray()\n            while errorStrings[s] != b'\\0':\n                if sys.version_info[0] == 3:\n                    a.append(int.from_bytes(errorStrings[s],'big'))\n                else:\n                    a.append(errorStrings[s])\n                s += 1\n            s += 1 #skip null\n            if sys.version_info[0] == 3:\n                errors.append(str(a,'utf-8'))\n            else:\n                errors.append(str(a))\n\n    return ret, errors", "response": "This function returns the number of errors that occurred in the last error of a specific client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npleasing have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetArrayParameter(clientID, paramIdentifier, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    paramValues = (ct.c_float*3)()\n    ret = c_GetArrayParameter(clientID, paramIdentifier, paramValues, operationMode)\n    arr = []\n    for i in range(3):\n        arr.append(paramValues[i])\n    return ret, arr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value of an array parameter.", "response": "def simxSetArrayParameter(clientID, paramIdentifier, paramValues, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    c_paramValues = (ct.c_float*3)(*paramValues)\n    return c_SetArrayParameter(clientID, paramIdentifier, c_paramValues, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplease have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetBooleanParameter(clientID, paramIdentifier, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    paramValue = ct.c_ubyte()\n    return c_GetBooleanParameter(clientID, paramIdentifier, ct.byref(paramValue), operationMode), bool(paramValue.value!=0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simxSetBooleanParameter(clientID, paramIdentifier, paramValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetBooleanParameter(clientID, paramIdentifier, paramValue, operationMode)", "response": "Set a boolean parameter on the specified client."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simxGetIntegerParameter(clientID, paramIdentifier, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    paramValue = ct.c_int()\n    return c_GetIntegerParameter(clientID, paramIdentifier, ct.byref(paramValue), operationMode), paramValue.value", "response": "This function returns the value of an integer parameter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting an integer parameter in the specified client.", "response": "def simxSetIntegerParameter(clientID, paramIdentifier, paramValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetIntegerParameter(clientID, paramIdentifier, paramValue, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npleasing have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetFloatingParameter(clientID, paramIdentifier, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    paramValue = ct.c_float()\n    return c_GetFloatingParameter(clientID, paramIdentifier, ct.byref(paramValue), operationMode), paramValue.value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplease have a look at the function description/documentation in the V-REP user manual", "response": "def simxSetFloatingParameter(clientID, paramIdentifier, paramValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetFloatingParameter(clientID, paramIdentifier, paramValue, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simxGetStringParameter(clientID, paramIdentifier, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    paramValue = ct.POINTER(ct.c_char)()\n    ret = c_GetStringParameter(clientID, paramIdentifier, ct.byref(paramValue), operationMode)\n    \n    a = bytearray()\n    if ret == 0:\n        i = 0\n        while paramValue[i] != b'\\0':\n            if sys.version_info[0] == 3:\n                a.append(int.from_bytes(paramValue[i],'big'))\n            else:\n                a.append(paramValue[i])\n            i=i+1\n    if sys.version_info[0] == 3:\n        a=str(a,'utf-8')\n    else:\n        a=str(a)\n    return ret, a", "response": "This function is used to get a string parameter from a V -REP user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simxGetCollisionHandle(clientID, collisionObjectName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    handle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(collisionObjectName) is str):\n        collisionObjectName=collisionObjectName.encode('utf-8')\n    return c_GetCollisionHandle(clientID, collisionObjectName, ct.byref(handle), operationMode), handle.value", "response": "Get the handle of a collision object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simxGetCollectionHandle(clientID, collectionName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    handle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(collectionName) is str):\n        collectionName=collectionName.encode('utf-8')\n    return c_GetCollectionHandle(clientID, collectionName, ct.byref(handle), operationMode), handle.value", "response": "Get the handle of a collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the handle of a given distance object.", "response": "def simxGetDistanceHandle(clientID, distanceObjectName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    handle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(distanceObjectName) is str):\n        distanceObjectName=distanceObjectName.encode('utf-8')\n    return c_GetDistanceHandle(clientID, distanceObjectName, ct.byref(handle), operationMode), handle.value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simxReadCollision(clientID, collisionObjectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    collisionState = ct.c_ubyte()\n    return c_ReadCollision(clientID, collisionObjectHandle, ct.byref(collisionState), operationMode), bool(collisionState.value!=0)", "response": "Reads a collision from a client."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxReadDistance(clientID, distanceObjectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    minimumDistance = ct.c_float()\n    return c_ReadDistance(clientID, distanceObjectHandle, ct.byref(minimumDistance), operationMode), minimumDistance.value", "response": "Reads the distance object from the specified client."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxGetObjects(clientID, objectType, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    objectCount = ct.c_int()\n    objectHandles = ct.POINTER(ct.c_int)()\n\n    ret = c_GetObjects(clientID, objectType, ct.byref(objectCount), ct.byref(objectHandles), operationMode)\n    handles = []\n    if ret == 0:\n        for i in range(objectCount.value):\n            handles.append(objectHandles[i])\n\n    return ret, handles", "response": "This function returns the number of objects in a given object type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simxDisplayDialog(clientID, titleText, mainText, dialogType, initialText, titleColors, dialogColors, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    if titleColors != None:\n        c_titleColors  = (ct.c_float*6)(*titleColors)\n    else:\n        c_titleColors  = None\n    if dialogColors != None:\n        c_dialogColors  = (ct.c_float*6)(*dialogColors)\n    else:\n        c_dialogColors  = None\n\n    c_dialogHandle = ct.c_int()\n    c_uiHandle = ct.c_int()\n    if sys.version_info[0] == 3:\n        if type(titleText) is str:\n            titleText=titleText.encode('utf-8')\n        if type(mainText) is str:\n            mainText=mainText.encode('utf-8')\n        if type(initialText) is str:\n            initialText=initialText.encode('utf-8')\n    return c_DisplayDialog(clientID, titleText, mainText, dialogType, initialText, c_titleColors, c_dialogColors, ct.byref(c_dialogHandle), ct.byref(c_uiHandle), operationMode), c_dialogHandle.value, c_uiHandle.value", "response": "Display a dialog in a V -REP user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npleases have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetDialogInput(clientID, dialogHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    inputText = ct.POINTER(ct.c_char)()\n    ret = c_GetDialogInput(clientID, dialogHandle, ct.byref(inputText), operationMode)\n    \n    a = bytearray()\n    if ret == 0:\n        i = 0\n        while inputText[i] != b'\\0':\n            if sys.version_info[0] == 3:\n                a.append(int.from_bytes(inputText[i],'big'))\n            else:\n                a.append(inputText[i])\n            i = i+1\n\n    if sys.version_info[0] == 3:\n        a=str(a,'utf-8')\n    else:\n        a=str(a)\n    return ret, a"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npleases have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetDialogResult(clientID, dialogHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    result = ct.c_int()\n    return c_GetDialogResult(clientID, dialogHandle, ct.byref(result), operationMode), result.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies paste objects from a list of objects.", "response": "def simxCopyPasteObjects(clientID, objectHandles, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    c_objectHandles  = (ct.c_int*len(objectHandles))(*objectHandles)\n    c_objectHandles = ct.cast(c_objectHandles,ct.POINTER(ct.c_int)) # IronPython needs this\n    newObjectCount   = ct.c_int()\n    newObjectHandles = ct.POINTER(ct.c_int)()\n    ret = c_CopyPasteObjects(clientID, c_objectHandles, len(objectHandles), ct.byref(newObjectHandles), ct.byref(newObjectCount), operationMode)\n\n    newobj = []\n    if ret == 0:\n        for i in range(newObjectCount.value):\n            newobj.append(newObjectHandles[i])\n\n    return ret, newobj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simxGetObjectSelection(clientID, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    objectCount   = ct.c_int()\n    objectHandles = ct.POINTER(ct.c_int)()\n    ret = c_GetObjectSelection(clientID, ct.byref(objectHandles), ct.byref(objectCount), operationMode)\n\n    newobj = []\n    if ret == 0:\n        for i in range(objectCount.value):\n            newobj.append(objectHandles[i])\n\n    return ret, newobj", "response": "Get the object selection of a given client."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simxSetObjectSelection(clientID, objectHandles, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    c_objectHandles  = (ct.c_int*len(objectHandles))(*objectHandles)\n    return c_SetObjectSelection(clientID, c_objectHandles, len(objectHandles), operationMode)", "response": "Set the object selection of the specified objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simxClearFloatSignal(clientID, signalName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    return c_ClearFloatSignal(clientID, signalName, operationMode)", "response": "Clear a signal from a client."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclears an integer signal from the specified client.", "response": "def simxClearIntegerSignal(clientID, signalName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    return c_ClearIntegerSignal(clientID, signalName, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simxClearStringSignal(clientID, signalName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    return c_ClearStringSignal(clientID, signalName, operationMode)", "response": "Clear a string signal from a client."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npleases have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetFloatSignal(clientID, signalName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    signalValue = ct.c_float()\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    return c_GetFloatSignal(clientID, signalName, ct.byref(signalValue), operationMode), signalValue.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npleases have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetIntegerSignal(clientID, signalName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    signalValue = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    return c_GetIntegerSignal(clientID, signalName, ct.byref(signalValue), operationMode), signalValue.value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a string stream from a V -REP client.", "response": "def simxReadStringStream(clientID, signalName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    signalLength = ct.c_int();\n    signalValue = ct.POINTER(ct.c_ubyte)()\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    ret = c_ReadStringStream(clientID, signalName, ct.byref(signalValue), ct.byref(signalLength), operationMode)\n\n    a = bytearray()\n    if ret == 0:\n        for i in range(signalLength.value):\n            a.append(signalValue[i])\n    if sys.version_info[0] != 3:\n        a=str(a)\n\n    return ret, a"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simxSetFloatSignal(clientID, signalName, signalValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    return c_SetFloatSignal(clientID, signalName, signalValue, operationMode)", "response": "Set a signal on a client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the signal value for the specified client ID.", "response": "def simxSetIntegerSignal(clientID, signalName, signalValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(signalName) is str):\n        signalName=signalName.encode('utf-8')\n    return c_SetIntegerSignal(clientID, signalName, signalValue, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplease have a look at the function description/documentation in the V-REP user manual", "response": "def simxSetStringSignal(clientID, signalName, signalValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    sigV=signalValue\n    if sys.version_info[0] == 3:\n        if type(signalName) is str:\n            signalName=signalName.encode('utf-8')\n        if type(signalValue) is bytearray:\n            sigV  = (ct.c_ubyte*len(signalValue))(*signalValue)\n        if type(signalValue) is str:\n            signalValue=signalValue.encode('utf-8')\n            sigV  = (ct.c_ubyte*len(signalValue))(*signalValue)\n    else:\n        if type(signalValue) is bytearray:\n            sigV = (ct.c_ubyte*len(signalValue))(*signalValue)\n        if type(signalValue) is str:\n            signalValue=bytearray(signalValue)\n            sigV = (ct.c_ubyte*len(signalValue))(*signalValue)\n    sigV=ct.cast(sigV,ct.POINTER(ct.c_ubyte)) # IronPython needs this\n    return c_SetStringSignal(clientID, signalName, sigV, len(signalValue), operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simxGetObjectFloatParameter(clientID, objectHandle, parameterID, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    \n    parameterValue = ct.c_float()\n    return c_GetObjectFloatParameter(clientID, objectHandle, parameterID, ct.byref(parameterValue), operationMode), parameterValue.value", "response": "This function returns the value of a float parameter in a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxSetObjectFloatParameter(clientID, objectHandle, parameterID, parameterValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetObjectFloatParameter(clientID, objectHandle, parameterID, parameterValue, operationMode)", "response": "Set the value of a float parameter on an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxGetObjectIntParameter(clientID, objectHandle, parameterID, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    parameterValue = ct.c_int() \n    return c_GetObjectIntParameter(clientID, objectHandle, parameterID, ct.byref(parameterValue), operationMode), parameterValue.value", "response": "Get an integer parameter from an object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simxSetObjectIntParameter(clientID, objectHandle, parameterID, parameterValue, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetObjectIntParameter(clientID, objectHandle, parameterID, parameterValue, operationMode)", "response": "Set an integer parameter on an object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simxGetModelProperty(clientID, objectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    prop = ct.c_int()\n    return c_GetModelProperty(clientID, objectHandle, ct.byref(prop), operationMode), prop.value", "response": "Get the value of a model property."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value of a model property.", "response": "def simxSetModelProperty(clientID, objectHandle, prop, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    return c_SetModelProperty(clientID, objectHandle, prop, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a new connection.", "response": "def simxStart(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(connectionAddress) is str):\n        connectionAddress=connectionAddress.encode('utf-8')\n    return c_Start(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simxGetPingTime(clientID):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    pingTime = ct.c_int()\n    return c_GetPingTime(clientID, ct.byref(pingTime)), pingTime.value", "response": "Get ping time for a client."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxGetInMessageInfo(clientID, infoType):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    info = ct.c_int()\n    return c_GetInMessageInfo(clientID, infoType, ct.byref(info)), info.value", "response": "Get in message info for a given clientID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting out message info for a given clientID.", "response": "def simxGetOutMessageInfo(clientID, infoType):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    info = ct.c_int()\n    return c_GetOutMessageInfo(clientID, infoType, ct.byref(info)), info.value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simxTransferFile(clientID, filePathAndName, fileName_serverSide, timeOut, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(filePathAndName) is str):\n        filePathAndName=filePathAndName.encode('utf-8')\n    return c_TransferFile(clientID, filePathAndName, fileName_serverSide, timeOut, operationMode)", "response": "Transfer a file from a file to a client."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npleasing have a look at the function description/documentation in the V-REP user manual", "response": "def simxEraseFile(clientID, fileName_serverSide, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if (sys.version_info[0] == 3) and (type(fileName_serverSide) is str):\n        fileName_serverSide=fileName_serverSide.encode('utf-8')\n    return c_EraseFile(clientID, fileName_serverSide, operationMode)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a dummy image.", "response": "def simxCreateDummy(clientID, size, color, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    handle = ct.c_int()\n    if color != None:\n        c_color = (ct.c_ubyte*12)(*color)\n    else:\n        c_color = None\n    return c_CreateDummy(clientID, size, c_color, ct.byref(handle), operationMode), handle.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxQuery(clientID, signalName, signalValue, retSignalName, timeOutInMs):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    retSignalLength = ct.c_int();\n    retSignalValue = ct.POINTER(ct.c_ubyte)()\n\n    sigV=signalValue\n    if sys.version_info[0] == 3:\n        if type(signalName) is str:\n            signalName=signalName.encode('utf-8')\n        if type(retSignalName) is str:\n            retSignalName=retSignalName.encode('utf-8')\n        if type(signalValue) is bytearray:\n            sigV  = (ct.c_ubyte*len(signalValue))(*signalValue)\n        if type(signalValue) is str:\n            signalValue=signalValue.encode('utf-8')\n            sigV  = (ct.c_ubyte*len(signalValue))(*signalValue)\n    else:\n        if type(signalValue) is bytearray:\n            sigV = (ct.c_ubyte*len(signalValue))(*signalValue)\n        if type(signalValue) is str:\n            signalValue=bytearray(signalValue)\n            sigV = (ct.c_ubyte*len(signalValue))(*signalValue)\n    sigV=ct.cast(sigV,ct.POINTER(ct.c_ubyte)) # IronPython needs this\n\n    ret = c_Query(clientID, signalName, sigV, len(signalValue), retSignalName, ct.byref(retSignalValue), ct.byref(retSignalLength), timeOutInMs)\n\n    a = bytearray()\n    if ret == 0:\n        for i in range(retSignalLength.value):\n            a.append(retSignalValue[i])\n    if sys.version_info[0] != 3:\n        a=str(a)\n\n    return ret, a", "response": "Query the V -REP server for a given signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplease have a look at the function description/documentation in the V-REP user manual", "response": "def simxGetObjectGroupData(clientID, objectType, dataType, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    handles =[]\n    intData =[]\n    floatData =[]\n    stringData =[]\n    handlesC = ct.c_int()\n    handlesP = ct.POINTER(ct.c_int)()\n    intDataC = ct.c_int()\n    intDataP = ct.POINTER(ct.c_int)()\n    floatDataC = ct.c_int()\n    floatDataP = ct.POINTER(ct.c_float)()\n    stringDataC = ct.c_int()\n    stringDataP = ct.POINTER(ct.c_char)()\n    ret = c_GetObjectGroupData(clientID, objectType, dataType, ct.byref(handlesC), ct.byref(handlesP), ct.byref(intDataC), ct.byref(intDataP), ct.byref(floatDataC), ct.byref(floatDataP), ct.byref(stringDataC), ct.byref(stringDataP), operationMode)\n    \n    if ret == 0:\n        for i in range(handlesC.value):\n            handles.append(handlesP[i])\n        for i in range(intDataC.value):\n            intData.append(intDataP[i])\n        for i in range(floatDataC.value):\n            floatData.append(floatDataP[i])\n        s = 0\n        for i in range(stringDataC.value):\n            a = bytearray()\n            while stringDataP[s] != b'\\0':\n                if sys.version_info[0] == 3:\n                    a.append(int.from_bytes(stringDataP[s],'big'))\n                else:\n                    a.append(stringDataP[s])\n                s += 1\n            s += 1 #skip null\n            if sys.version_info[0] == 3:\n                a=str(a,'utf-8')\n            else:\n                a=str(a)\n            stringData.append(a)\n \n    return ret, handles, intData, floatData, stringData"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simxCallScriptFunction(clientID, scriptDescription, options, functionName, inputInts, inputFloats, inputStrings, inputBuffer, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    inputBufferV=inputBuffer\n    if sys.version_info[0] == 3:\n        if type(scriptDescription) is str:\n            scriptDescription=scriptDescription.encode('utf-8')\n        if type(functionName) is str:\n            functionName=functionName.encode('utf-8')\n        if type(inputBuffer) is bytearray:\n            inputBufferV  = (ct.c_ubyte*len(inputBuffer))(*inputBuffer)\n        if type(inputBuffer) is str:\n            inputBuffer=inputBuffer.encode('utf-8')\n            inputBufferV  = (ct.c_ubyte*len(inputBuffer))(*inputBuffer)\n    else:\n        if type(inputBuffer) is bytearray:\n            inputBufferV = (ct.c_ubyte*len(inputBuffer))(*inputBuffer)\n        if type(inputBuffer) is str:\n            inputBuffer=bytearray(inputBuffer)\n            inputBufferV = (ct.c_ubyte*len(inputBuffer))(*inputBuffer)\n    inputBufferV=ct.cast(inputBufferV,ct.POINTER(ct.c_ubyte)) # IronPython needs this\n\n    c_inInts  = (ct.c_int*len(inputInts))(*inputInts)\n    c_inInts = ct.cast(c_inInts,ct.POINTER(ct.c_int)) # IronPython needs this\n    c_inFloats  = (ct.c_float*len(inputFloats))(*inputFloats)\n    c_inFloats = ct.cast(c_inFloats,ct.POINTER(ct.c_float)) # IronPython needs this\n\n    concatStr=''.encode('utf-8')\n    for i in range(len(inputStrings)):\n        a=inputStrings[i]\n        a=a+'\\0'\n        if type(a) is str:\n            a=a.encode('utf-8')\n        concatStr=concatStr+a\n    c_inStrings  = (ct.c_char*len(concatStr))(*concatStr)\n\n    intDataOut =[]\n    floatDataOut =[]\n    stringDataOut =[]\n    bufferOut =bytearray()\n\n    intDataC = ct.c_int()\n    intDataP = ct.POINTER(ct.c_int)()\n    floatDataC = ct.c_int()\n    floatDataP = ct.POINTER(ct.c_float)()\n    stringDataC = ct.c_int()\n    stringDataP = ct.POINTER(ct.c_char)()\n    bufferS = ct.c_int()\n    bufferP = ct.POINTER(ct.c_ubyte)()\n\n    ret = c_CallScriptFunction(clientID,scriptDescription,options,functionName,len(inputInts),c_inInts,len(inputFloats),c_inFloats,len(inputStrings),c_inStrings,len(inputBuffer),inputBufferV,ct.byref(intDataC),ct.byref(intDataP),ct.byref(floatDataC),ct.byref(floatDataP),ct.byref(stringDataC),ct.byref(stringDataP),ct.byref(bufferS),ct.byref(bufferP),operationMode)\n\n    if ret == 0:\n        for i in range(intDataC.value):\n            intDataOut.append(intDataP[i])\n        for i in range(floatDataC.value):\n            floatDataOut.append(floatDataP[i])\n        s = 0\n        for i in range(stringDataC.value):\n            a = bytearray()\n            while stringDataP[s] != b'\\0':\n                if sys.version_info[0] == 3:\n                    a.append(int.from_bytes(stringDataP[s],'big'))\n                else:\n                    a.append(stringDataP[s])\n                s += 1\n            s += 1 #skip null\n            if sys.version_info[0] == 3:\n                a=str(a,'utf-8')\n            else:\n                a=str(a)\n            stringDataOut.append(a)\n        for i in range(bufferS.value):\n            bufferOut.append(bufferP[i])\n    if sys.version_info[0] != 3:\n        bufferOut=str(bufferOut)\n\n    return ret, intDataOut, floatDataOut, stringDataOut, bufferOut", "response": "This function calls the specified function in the V - REP user and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simxGetObjectVelocity(clientID, objectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    linearVel  = (ct.c_float*3)()\n    angularVel = (ct.c_float*3)()\n    ret = c_GetObjectVelocity(clientID, objectHandle, linearVel, angularVel, operationMode)\n    arr1 = []\n    for i in range(3):\n        arr1.append(linearVel[i])\n    arr2 = []\n    for i in range(3):\n        arr2.append(angularVel[i])\n    return ret, arr1, arr2", "response": "This function is used to get the velocity of an object in a V -REP client."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npacks the given list of integers into a string.", "response": "def simxPackInts(intList):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    \n    if sys.version_info[0] == 3:\n        s=bytes()\n        for i in range(len(intList)):\n            s=s+struct.pack('<i',intList[i])\n        s=bytearray(s)\n    else:\n        s=''\n        for i in range(len(intList)):\n            s+=struct.pack('<i',intList[i])\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nunpack a list of integers into a list of objects", "response": "def simxUnpackInts(intsPackedInString):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    b=[]\n    for i in range(int(len(intsPackedInString)/4)):\n        b.append(struct.unpack('<i',intsPackedInString[4*i:4*(i+1)])[0])\n    return b"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npacking a list of float values into a string", "response": "def simxPackFloats(floatList):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n\n    if sys.version_info[0] == 3:\n        s=bytes()\n        for i in range(len(floatList)):\n            s=s+struct.pack('<f',floatList[i])\n        s=bytearray(s)\n    else:\n        s=''\n        for i in range(len(floatList)):\n            s+=struct.pack('<f',floatList[i])\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simxUnpackFloats(floatsPackedInString):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n    b=[]\n    for i in range(int(len(floatsPackedInString)/4)):\n        b.append(struct.unpack('<f',floatsPackedInString[4*i:4*(i+1)])[0])\n    return b", "response": "Unpacks a list of floats into a list of objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart all the synchronization loops.", "response": "def setup(self):\n        \"\"\" Starts all the synchronization loops. \"\"\"\n        [c.start() for c in self.controllers]\n        [c.wait_to_start() for c in self.controllers]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a kinematic chain from motors in a Poppy Creature.", "response": "def from_poppy_creature(cls, poppy, motors, passiv, tip,\n                            reversed_motors=[]):\n        \"\"\" Creates an kinematic chain from motors of a Poppy Creature.\n\n            :param poppy: PoppyCreature used\n            :param list motors: list of all motors that composed the kinematic chain\n            :param list passiv: list of motors which are passiv in the chain (they will not move)\n            :param list tip: [x, y, z] translation of the tip of the chain (in meters)\n            :param list reversed_motors: list of motors that should be manually reversed (due to a problem in the URDF?)\n\n        \"\"\"\n        chain_elements = get_chain_from_joints(poppy.urdf_file,\n                                               [m.name for m in motors])\n\n        activ = [False] + [m not in passiv for m in motors] + [True]\n\n        chain = cls.from_urdf_file(poppy.urdf_file,\n                                   base_elements=chain_elements,\n                                   last_link_vector=tip,\n                                   active_links_mask=activ)\n\n        chain.motors = [getattr(poppy, l.name) for l in chain.links[1:-1]]\n\n        for m, l in zip(chain.motors, chain.links[1:-1]):\n            # Force an access to angle limit to retrieve real values\n            # This is quite an ugly fix and should be handled better\n            m.angle_limit\n\n            bounds = m.__dict__['lower_limit'], m.__dict__['upper_limit']\n            l.bounds = tuple(map(rad2deg, bounds))\n\n        chain._reversed = array([(-1 if m in reversed_motors else 1)\n                                 for m in motors])\n\n        return chain"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the cartesian position of the end of the chain.", "response": "def end_effector(self):\n        \"\"\" Returns the cartesian position of the end of the chain (in meters). \"\"\"\n        angles = self.convert_to_ik_angles(self.joints_position)\n        return self.forward_kinematics(angles)[:3, 3]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef goto(self, position, duration, wait=False, accurate=False):\n        if len(position) != 3:\n            raise ValueError('Position should be a list [x, y, z]!')\n\n        M = eye(4)\n        M[:3, 3] = position\n        self._goto(M, duration, wait, accurate)", "response": "Goes to a given cartesian position."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngoing to a given cartesian pose.", "response": "def _goto(self, pose, duration, wait, accurate):\n        \"\"\" Goes to a given cartesian pose.\n\n            :param matrix pose: homogeneous matrix representing the target position\n            :param float duration: move duration\n            :param bool wait: whether to wait for the end of the move\n            :param bool accurate: trade-off between accurate solution and computation time. By default, use the not so accurate but fast version.\n\n        \"\"\"\n\n        kwargs = {}\n        if not accurate:\n            kwargs['max_iter'] = 3\n\n        q0 = self.convert_to_ik_angles(self.joints_position)\n        q = self.inverse_kinematics(pose, initial_position=q0, **kwargs)\n\n        joints = self.convert_from_ik_angles(q)\n\n        last = self.motors[-1]\n        for m, pos in list(zip(self.motors, joints)):\n            m.goto_position(pos, duration,\n                            wait=False if m != last else wait)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_to_ik_angles(self, joints):\n        if len(joints) != len(self.motors):\n            raise ValueError('Incompatible data, len(joints) should be {}!'.format(len(self.motors)))\n\n        raw_joints = [(j + m.offset) * (1 if m.direct else -1)\n                      for j, m in zip(joints, self.motors)]\n\n        raw_joints *= self._reversed\n\n        return [0] + [deg2rad(j) for j in raw_joints] + [0]", "response": "Convert from poppy representation to IKPY internal representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_from_ik_angles(self, joints):\n        if len(joints) != len(self.motors) + 2:\n            raise ValueError('Incompatible data, len(joints) should be {}!'.format(len(self.motors) + 2))\n\n        joints = [rad2deg(j) for j in joints[1:-1]]\n        joints *= self._reversed\n\n        return [(j * (1 if m.direct else -1)) - m.offset\n                for j, m in zip(joints, self.motors)]", "response": "Convert from IKPY internal representation to poppy representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef factory_reset(self, ids, except_ids=False, except_baudrate_and_ids=False):\n\n        mode = (0x02 if except_baudrate_and_ids else\n                0x01 if except_ids else 0xFF)\n\n        for id in ids:\n            try:\n                self._send_packet(self._protocol.DxlResetPacket(id, mode))\n\n            except (DxlTimeoutError, DxlCommunicationError):\n                pass", "response": "Reset all motors on the bus to their factory default settings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        if not self.robot._primitive_manager.running:\n            raise RuntimeError('Cannot run a primitive when the sync is stopped!')\n\n        StoppableThread.start(self)\n        self.wait_to_start()\n\n        logger.info(\"Primitive %s started.\", self)", "response": "Start or restart the primitive."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequests the primitive to stop.", "response": "def stop(self, wait=True):\n        \"\"\" Requests the primitive to stop. \"\"\"\n        logger.info(\"Primitive %s stopped.\", self)\n        StoppableThread.stop(self, wait)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the equivalent : class : ~pypot. primitive. mockup. MockupMotor.", "response": "def get_mockup_motor(self, motor):\n        \"\"\" Gets the equivalent :class:`~pypot.primitive.primitive.MockupMotor`. \"\"\"\n        return next((m for m in self.robot.motors if m.name == motor.name), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the 10 most recent update frequencies.", "response": "def recent_update_frequencies(self):\n        \"\"\" Returns the 10 most recent update frequencies.\n\n        The given frequencies are computed as short-term frequencies!\n        The 0th element of the list corresponds to the most recent frequency.\n        \"\"\"\n        return list(reversed([(1.0 / p) for p in numpy.diff(self._recent_updates)]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef goto_position(self, position, duration, control=None, wait=False):\n\n        if control is None:\n            control = self.goto_behavior\n\n        if control == 'minjerk':\n            goto_min_jerk = GotoMinJerk(self, position, duration)\n            goto_min_jerk.start()\n            if wait:\n                goto_min_jerk.wait_to_stop()\n\n        elif control == 'dummy':\n            dp = abs(self.present_position - position)\n            speed = (dp / float(duration)) if duration > 0 else numpy.inf\n\n            self.moving_speed = speed\n            self.goal_position = position\n\n            if wait:\n                time.sleep(duration)", "response": "Goto the specified position within the specified duration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self, file):\n        d = {\n            'framerate': self.framerate,\n            'positions': self._timed_positions,\n        }\n        json.dump(d, file, indent=2)", "response": "Saves the current state of the object to a json file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Move object from a dictionary.", "response": "def create(cls, d):\n        \"\"\" Create a :class:`~pypot.primitive.move.Move` from a dictionary. \"\"\"\n        move = cls(d['framerate'])\n        move._timed_positions.update(d['positions'])\n        return move"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a : class ~pypot. primitive. move. Move from a json file.", "response": "def load(cls, file):\n        \"\"\" Loads a :class:`~pypot.primitive.move.Move` from a json file. \"\"\"\n        d = json.load(file)\n        return cls.create(d)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a : class ~pypot. primitive. move. Move from a json string.", "response": "def loads(cls, str):\n        \"\"\" Loads a :class:`~pypot.primitive.move.Move` from a json string. \"\"\"\n        d = json.loads(str)\n        return cls.create(d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds new motors to the recording", "response": "def add_tracked_motors(self, tracked_motors):\n        \"\"\"Add new motors to the recording\"\"\"\n        new_mockup_motors = map(self.get_mockup_motor, tracked_motors)\n        self.tracked_motors = list(set(self.tracked_motors + new_mockup_motors))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self):\n        with self.syncing:\n            for m in self._motors:\n                to_set = defaultdict(list)\n\n                for p in self._prim:\n                    for key, val in getattr(p.robot, m.name)._to_set.iteritems():\n                        to_set[key].append(val)\n\n                for key, val in to_set.iteritems():\n                    if key == 'led':\n                        colors = set(val)\n                        if len(colors) > 1:\n                            colors -= {'off'}\n                        filtred_val = colors.pop()\n                    else:\n                        filtred_val = self._filter(val)\n\n                    logger.debug('Combined %s.%s from %s to %s',\n                                 m.name, key, val, filtred_val)\n                    setattr(m, key, filtred_val)\n\n            [p._synced.set() for p in self._prim]", "response": "Update the motors with the current motors."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops the primitive manager.", "response": "def stop(self):\n        \"\"\" Stop the primitive manager. \"\"\"\n        for p in self.primitives[:]:\n            p.stop()\n\n        StoppableLoopThread.stop(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_scene(self, scene_path, start=False):\n        self.stop_simulation()\n\n        if not os.path.exists(scene_path):\n            raise IOError(\"No such file or directory: '{}'\".format(scene_path))\n\n        self.call_remote_api('simxLoadScene', scene_path, True)\n\n        if start:\n            self.start_simulation()", "response": "Loads a scene on the V -REP server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the current motor position.", "response": "def get_motor_position(self, motor_name):\n        \"\"\" Gets the motor current position. \"\"\"\n        return self.call_remote_api('simxGetJointPosition',\n                                    self.get_object_handle(motor_name),\n                                    streaming=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the motor target position.", "response": "def set_motor_position(self, motor_name, position):\n        \"\"\" Sets the motor target position. \"\"\"\n        self.call_remote_api('simxSetJointTargetPosition',\n                             self.get_object_handle(motor_name),\n                             position,\n                             sending=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_motor_force(self, motor_name):\n        return self.call_remote_api('simxGetJointForce',\n                                    self.get_object_handle(motor_name),\n                                    streaming=True)", "response": "Retrieves the force or torque applied to a joint along the active axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_motor_force(self, motor_name, force):\n        self.call_remote_api('simxSetJointForce',\n                             self.get_object_handle(motor_name),\n                             force,\n                             sending=True)", "response": "Sets the maximum force or torque that a joint can exert."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the object position.", "response": "def get_object_position(self, object_name, relative_to_object=None):\n        \"\"\" Gets the object position. \"\"\"\n        h = self.get_object_handle(object_name)\n        relative_handle = (-1 if relative_to_object is None\n                           else self.get_object_handle(relative_to_object))\n\n        return self.call_remote_api('simxGetObjectPosition',\n                                    h, relative_handle,\n                                    streaming=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_object_position(self, object_name, position=[0, 0, 0]):\n        h = self.get_object_handle(object_name)\n\n        return self.call_remote_api('simxSetObjectPosition',\n                                    h, -1, position,\n                                    sending=True)", "response": "Sets the position of the object in the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the object handle.", "response": "def get_object_handle(self, obj):\n        \"\"\" Gets the vrep object handle. \"\"\"\n        if obj not in self._object_handles:\n            self._object_handles[obj] = self._get_object_handle(obj=obj)\n\n        return self._object_handles[obj]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the state of a single collision.", "response": "def get_collision_state(self, collision_name):\n        \"\"\" Gets the collision state. \"\"\"\n        return self.call_remote_api('simxReadCollision',\n                                    self.get_collision_handle(collision_name),\n                                    streaming=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_collision_handle(self, collision):\n        if collision not in self._object_handles:\n            h = self._get_collision_handle(collision)\n            self._object_handles[collision] = h\n\n        return self._object_handles[collision]", "response": "Gets a vrep collisions handle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a cube to the log", "response": "def add_cube(self, name, position, sizes, mass):\n        \"\"\" Add Cube \"\"\"\n        self._create_pure_shape(0, 239, sizes, mass, [0, 0])\n        self.set_object_position(\"Cuboid\", position)\n        self.change_object_name(\"Cuboid\", name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new cylinder to the log", "response": "def add_cylinder(self, name, position, sizes, mass, precision=[10, 10]):\n        \"\"\" Add Cylinder \"\"\"\n        self._create_pure_shape(2, 239, sizes, mass, precision)\n        self.set_object_position(\"Cylinder\", position)\n        self.change_object_name(\"Cylinder\", name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the object name of the object.", "response": "def change_object_name(self, old_name, new_name):\n        \"\"\" Change object name \"\"\"\n        h = self._get_object_handle(old_name)\n        if old_name in self._object_handles:\n            self._object_handles.pop(old_name)\n        lua_code = \"simSetObjectName({}, '{}')\".format(h, new_name)\n        self._inject_lua_code(lua_code)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _inject_lua_code(self, lua_code):\n        msg = (ctypes.c_ubyte * len(lua_code)).from_buffer_copy(lua_code.encode())\n        self.call_remote_api('simxWriteStringStream', 'my_lua_code', msg)", "response": "Sends raw lua code and evaluate it wihtout any checking!"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call_remote_api(self, func_name, *args, **kwargs):\n        f = getattr(remote_api, func_name)\n\n        mode = self._extract_mode(kwargs)\n        kwargs['operationMode'] = vrep_mode[mode]\n        # hard_retry = True\n\n        if '_force' in kwargs:\n            del kwargs['_force']\n            _force = True\n        else:\n            _force = False\n\n        for _ in range(VrepIO.MAX_ITER):\n            with self._lock:\n                ret = f(self.client_id, *args, **kwargs)\n\n            if _force:\n                return\n\n            if mode == 'sending' or isinstance(ret, int):\n                err, res = ret, None\n            else:\n                err, res = ret[0], ret[1:]\n                res = res[0] if len(res) == 1 else res\n\n            err = [bool((err >> i) & 1) for i in range(len(vrep_error))]\n\n            if remote_api.simx_return_novalue_flag not in err:\n                break\n\n            time.sleep(VrepIO.TIMEOUT)\n\n        # if any(err) and hard_retry:\n        #     print \"HARD RETRY\"\n        # self.stop_simulation() #nope\n        #\n        #     notconnected = True\n        #     while notconnected:\n        #         self.close()\n        #         close_all_connections()\n        #         time.sleep(0.5)\n        #         try:\n        #             self.open_io()\n        #             notconnected = False\n        #         except:\n        #             print 'CONNECTION ERROR'\n        #             pass\n        #\n        #     self.start_simulation()\n        #\n        #     with self._lock:\n        #         ret = f(self.client_id, *args, **kwargs)\n        #\n        #         if mode == 'sending' or isinstance(ret, int):\n        #             err, res = ret, None\n        #         else:\n        #             err, res = ret[0], ret[1:]\n        #             res = res[0] if len(res) == 1 else res\n        #\n        #         err = [bool((err >> i) & 1) for i in range(len(vrep_error))]\n        #\n        #         return res\n\n        if any(err):\n            msg = ' '.join([vrep_error[2 ** i]\n                            for i, e in enumerate(err) if e])\n            raise VrepIOErrors(msg)\n\n        return res", "response": "Calls any remote API func in a thread - safe way."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the tornado server run forever", "response": "def run(self, **kwargs):\n        \"\"\" Start the tornado server, run forever\"\"\"\n\n        try:\n            loop = IOLoop()\n            app = self.make_app()\n            app.listen(self.port)\n            loop.start()\n\n        except socket.error as serr:\n            # Re raise the socket error if not \"[Errno 98] Address already in use\"\n            if serr.errno != errno.EADDRINUSE:\n                raise serr\n            else:\n                logger.warning('The webserver port {} is already used. May be the HttpRobotServer is already running or another software is using this port.'.format(self.port))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns an infinite REQ or REREP loop.", "response": "def run(self):\n        \"\"\" Run an infinite REQ/REP loop. \"\"\"\n        while True:\n            req = self.socket.recv_json()\n\n            try:\n                answer = self.handle_request(req)\n                self.socket.send(json.dumps(answer))\n\n            except (AttributeError, TypeError) as e:\n                self.socket.send_json({'error': str(e)})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen a new serial connection.", "response": "def open(self, port, baudrate=1000000, timeout=0.05):\n        \"\"\" Opens a new serial communication (closes the previous communication if needed).\n\n            :raises: :py:exc:`~pypot.dynamixel.io.DxlError` if the port is already used.\n\n            \"\"\"\n        self._open(port, baudrate, timeout)\n        logger.info(\"Opening port '%s'\", self.port,\n                    extra={'port': port,\n                           'baudrate': baudrate,\n                           'timeout': timeout})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the serial communication if opened.", "response": "def close(self, _force_lock=False):\n        \"\"\" Closes the serial communication if opened. \"\"\"\n        if not self.closed:\n            with self.__force_lock(_force_lock) or self._serial_lock:\n                self._serial.close()\n                self.__used_ports.remove(self.port)\n\n            logger.info(\"Closing port '%s'\", self.port,\n                        extra={'port': self.port,\n                               'baudrate': self.baudrate,\n                               'timeout': self.timeout})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flush(self, _force_lock=False):\n        if self.closed:\n            raise DxlError('attempt to flush a closed serial communication')\n\n        with self.__force_lock(_force_lock) or self._serial_lock:\n            self._serial.flushInput()\n            self._serial.flushOutput()", "response": "Flushes the serial communication."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ping(self, id):\n        pp = self._protocol.DxlPingPacket(id)\n\n        try:\n            self._send_packet(pp, error_handler=None)\n            return True\n        except DxlTimeoutError:\n            return False", "response": "Ping the motor with the specified id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscan all motors within the specified list.", "response": "def scan(self, ids=range(254)):\n        \"\"\" Pings all ids within the specified list, by default it finds all the motors connected to the bus. \"\"\"\n        return [id for id in ids if self.ping(id)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the model for the specified motors.", "response": "def get_model(self, ids):\n        \"\"\" Gets the model for the specified motors. \"\"\"\n        to_get_ids = [i for i in ids if i not in self._known_models]\n        models = [dxl_to_model(m) for m in self._get_model(to_get_ids, convert=False)]\n        self._known_models.update(zip(to_get_ids, models))\n\n        return tuple(self._known_models[id] for id in ids)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_id(self, new_id_for_id):\n        if len(set(new_id_for_id.values())) < len(new_id_for_id):\n            raise ValueError('each id must be unique.')\n\n        for new_id in new_id_for_id.itervalues():\n            if self.ping(new_id):\n                raise ValueError('id {} is already used.'.format(new_id))\n\n        self._change_id(new_id_for_id)\n\n        for motor_id, new_id in new_id_for_id.iteritems():\n            if motor_id in self._known_models:\n                self._known_models[new_id] = self._known_models[motor_id]\n                del self._known_models[motor_id]\n            if motor_id in self._known_mode:\n                self._known_mode[new_id] = self._known_mode[motor_id]\n                del self._known_mode[motor_id]", "response": "Changes the id of the specified motors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_baudrate(self, baudrate_for_ids):\n        self._change_baudrate(baudrate_for_ids)\n\n        for motor_id in baudrate_for_ids:\n            if motor_id in self._known_models:\n                del self._known_models[motor_id]\n            if motor_id in self._known_mode:\n                del self._known_mode[motor_id]", "response": "Changes the baudrate of the specified motors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_status_return_level(self, ids, **kwargs):\n        convert = kwargs['convert'] if 'convert' in kwargs else self._convert\n        srl = []\n        for id in ids:\n            try:\n                srl.extend(self._get_status_return_level((id, ),\n                                                         error_handler=None, convert=convert))\n            except DxlTimeoutError as e:\n                if self.ping(id):\n                    srl.append('never' if convert else 0)\n                else:\n                    if self._error_handler:\n                        self._error_handler.handle_timeout(e)\n                        return ()\n                    else:\n                        raise e\n\n        return tuple(srl)", "response": "Gets the status level for the motors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_status_return_level(self, srl_for_id, **kwargs):\n        convert = kwargs['convert'] if 'convert' in kwargs else self._convert\n        if convert:\n            srl_for_id = dict(zip(srl_for_id.keys(),\n                                  [('never', 'read', 'always').index(s) for s in srl_for_id.values()]))\n        self._set_status_return_level(srl_for_id, convert=False)", "response": "Sets status return level to the motors in the specified motors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef switch_led_on(self, ids):\n        self._set_LED(dict(zip(ids, itertools.repeat(True))))", "response": "Switches on the LED of motors with the specified ids."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef switch_led_off(self, ids):\n        self._set_LED(dict(zip(ids, itertools.repeat(False))))", "response": "Switches off the LED of motors with the specified ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_torque(self, ids):\n        self._set_torque_enable(dict(zip(ids, itertools.repeat(True))))", "response": "Enables torque of motors with the specified ids."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disable_torque(self, ids):\n        self._set_torque_enable(dict(zip(ids, itertools.repeat(False))))", "response": "Disables torque of motors with the specified ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pid_gain(self, ids, **kwargs):\n        return tuple([tuple(reversed(t)) for t in self._get_pid_gain(ids, **kwargs)])", "response": "Gets the pid gain for the specified motors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the pid gain to the motors in the specified motors.", "response": "def set_pid_gain(self, pid_for_id, **kwargs):\n        \"\"\" Sets the pid gain to the specified motors. \"\"\"\n        pid_for_id = dict(itertools.izip(pid_for_id.iterkeys(),\n                                         [tuple(reversed(t)) for t in pid_for_id.values()]))\n        self._set_pid_gain(pid_for_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_control_table(self, ids, **kwargs):\n        error_handler = kwargs['error_handler'] if ('error_handler' in kwargs) else self._error_handler\n        convert = kwargs['convert'] if ('convert' in kwargs) else self._convert\n\n        bl = ('goal position speed load', 'present position speed load')\n        controls = [c for c in self._AbstractDxlIO__controls if c.name not in bl]\n\n        res = []\n\n        for id, model in zip(ids, self.get_model(ids)):\n            controls = [c for c in controls if model in c.models]\n\n            controls = sorted(controls, key=lambda c: c.address)\n\n            address = controls[0].address\n            length = controls[-1].address + controls[-1].nb_elem * controls[-1].length\n\n            rp = self._protocol.DxlReadDataPacket(id, address, length)\n            sp = self._send_packet(rp, error_handler=error_handler)\n\n            d = OrderedDict()\n            for c in controls:\n                v = dxl_decode_all(sp.parameters[c.address:c.address + c.nb_elem * c.length], c.nb_elem)\n                d[c.name] = c.dxl_to_si(v, model) if convert else v\n\n            res.append(d)\n\n        return tuple(res)", "response": "Gets the full control table for the specified motors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the nearest keys thanks to a cKDTree query", "response": "def nearest_keys(self, key):\n        \"\"\"Find the nearest_keys (l2 distance) thanks to a cKDTree query\"\"\"\n        if not isinstance(key, tuple):\n            _key = (key,)\n        if self.__stale:\n            self.generate_tree()\n        d, idx = self.__tree.query(\n            _key, self.k_neighbors, distance_upper_bound=self.distance_upper_bound)\n\n        try:\n            return [self.__keys[id][0] for id in idx if id < len(self.__keys)]\n        except TypeError:\n            # if k_neighbors = 1 query is not returnng arrays\n            return self.__keys[idx]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninterpolate motors to the actual speed and position of motors.", "response": "def interpolate_motor_positions(self, input_key, nearest_keys):\n        \"\"\" Process linear interpolation to estimate actual speed and position of motors\n            Method specific to the :meth:~pypot.primitive.move.Move.position() structure\n            it is a KDTreeDict[timestamp] = {dict[motor]=(position,speed)}\n        \"\"\"\n\n        # TODO : to be rewrited with more style (map ?)\n\n        if len(nearest_keys) == 1:\n            return self[nearest_keys[0]]\n        elif len(nearest_keys) == 0:\n            raise KeyError('key {} exceed distance_upper_bound {}'.format(\n                input_key, self.distance_upper_bound))\n        elif len(nearest_keys) != 2:\n            raise NotImplementedError(\"interpolation works only for k_neighbors = 2\")\n        elif nearest_keys[0] == nearest_keys[1]:\n            # Bug from nearest key ?\n            return self[nearest_keys[0]]\n\n        # Problem if ValueError: A value in x_new is above the interpolation range.\n        elif input_key < min(nearest_keys):\n            return self[min(nearest_keys)]\n        elif input_key > max(nearest_keys):\n            return self[max(nearest_keys)]\n\n        interpolated_positions = {}\n        for (k, v), (k2, v2) in zip(self[nearest_keys[0]].items(), self[nearest_keys[1]].items()):\n            if k == k2:\n                x = np.array(nearest_keys)\n                y_pos = np.array([v[0], v2[0]])\n                y_speed = np.array([v[1], v2[1]])\n                f_pos = interp1d(x, y_pos, bounds_error=False)\n                f_speed = interp1d(x, y_speed, bounds_error=False)\n                # print k, input_key, (float(f_pos(input_key[0])), float(f_speed(input_key[0])))\n                interpolated_positions[k] = (f_pos(input_key), f_speed(input_key))\n            else:\n                raise IndexError(\"key are not identics. Motor added during the record ?\")\n        return interpolated_positions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse to replace print that doesn t exist for python <3. 3", "response": "def flushed_print(*args, **kwargs):\n    \"\"\"\n    Use to replace print(*args, flush=True) that doesn't exist for python<3.3\n    \"\"\"\n    print(*args, **kwargs)\n    file = kwargs.get('file', sys.stdout)\n    file.flush() if file is not None else sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_config(config, strict=True, sync=True, use_dummy_io=False, **extra):\n    logger.info('Loading config... ', extra={'config': config})\n\n    alias = config['motorgroups']\n\n    # Instatiate the different motor controllers\n    controllers = []\n    for c_name, c_params in config['controllers'].items():\n        motor_names = sum([_motor_extractor(alias, name)\n                           for name in c_params['attached_motors']], [])\n\n        attached_motors = [motor_from_confignode(config, name)\n                           for name in motor_names]\n\n        # at least one of the motor is set as broken\n        if [m for m in attached_motors if m._broken]:\n            strict = False\n\n        attached_ids = [m.id for m in attached_motors]\n        if not use_dummy_io:\n            dxl_io = dxl_io_from_confignode(config, c_params, attached_ids, strict)\n\n            check_motor_eprom_configuration(config, dxl_io, motor_names)\n\n            logger.info('Instantiating controller on %s with motors %s',\n                        dxl_io.port, motor_names,\n                        extra={'config': config})\n\n            syncloop = (c_params['syncloop'] if 'syncloop' in c_params\n                        else 'BaseDxlController')\n            SyncLoopCls = getattr(pypot.dynamixel.syncloop, syncloop)\n\n            c = SyncLoopCls(dxl_io, attached_motors)\n            controllers.append(c)\n        else:\n            controllers.append(DummyController(attached_motors))\n\n    try:\n        robot = Robot(motor_controllers=controllers, sync=sync)\n    except RuntimeError:\n        for c in controllers:\n            c.io.close()\n\n        raise\n\n    make_alias(config, robot)\n\n    # Create all sensors and attached them\n    try:\n        if 'sensors' in config and not use_dummy_io:\n            sensors = []\n            for s_name in config['sensors'].keys():\n                if s_name in extra and extra[s_name] == 'dummy':\n                    config['sensors'][s_name]['type'] = 'Dummy{}'.format(s_name.capitalize())\n\n                sensor = sensor_from_confignode(config, s_name, robot)\n                setattr(robot, s_name, sensor)\n                sensors.append(sensor)\n                robot.sensors.append(sensor)\n\n            [s.start() for s in sensors if hasattr(s, 'start')]\n\n    # If anything goes wrong when adding sensors\n    # We have to make sure we close the robot properly\n    # Otherwise trying to open it again will fail.\n    except Exception:\n        robot.close()\n        raise\n\n    logger.info('Loading complete!',\n                extra={'config': config})\n\n    return robot", "response": "Create a new robot instance from a configuration dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_motor_eprom_configuration(config, dxl_io, motor_names):\n    changed_angle_limits = {}\n    changed_return_delay_time = {}\n\n    for name in motor_names:\n        m = config['motors'][name]\n        id = m['id']\n\n        try:\n            old_limits = dxl_io.get_angle_limit((id, ))[0]\n            old_return_delay_time = dxl_io.get_return_delay_time((id, ))[0]\n        except IndexError:  # probably a broken motor so we just skip\n            continue\n\n        if old_return_delay_time != 0:\n            logger.warning(\"Return delay time of %s changed from %s to 0\",\n                           name, old_return_delay_time)\n            changed_return_delay_time[id] = 0\n\n        new_limits = m['angle_limit']\n        if 'wheel_mode' in m and m['wheel_mode']:\n            dxl_io.set_wheel_mode([m['id']])\n            time.sleep(0.5)\n        else:\n            # TODO: we probably need a better fix for this.\n            # dxl_io.set_joint_mode([m['id']])\n\n            d = numpy.linalg.norm(numpy.asarray(new_limits) - numpy.asarray(old_limits))\n            if d > 1:\n                logger.warning(\"Limits of '%s' changed from %s to %s\",\n                               name, old_limits, new_limits,\n                               extra={'config': config})\n                changed_angle_limits[id] = new_limits\n\n    if changed_angle_limits:\n        dxl_io.set_angle_limit(changed_angle_limits)\n        time.sleep(0.5)\n\n    if changed_return_delay_time:\n        dxl_io.set_return_delay_time(changed_return_delay_time)\n        time.sleep(0.5)", "response": "Check if the return delay time is set to 0."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_json(json_file, sync=True, strict=True, use_dummy_io=False, **extra):\n    with open(json_file) as f:\n        config = json.load(f, object_pairs_hook=OrderedDict)\n\n    return from_config(config, sync=sync, strict=strict, use_dummy_io=use_dummy_io, **extra)", "response": "Creates a new robot from a JSON configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping recording a new record", "response": "def stop_move_recorder(self, move_name):\n        \"\"\"Allow more easily than stop_primitive() to save in a filename the recorded move\"\"\"\n        recorder = getattr(self.robot, '_{}_recorder'.format(move_name))\n        recorder.stop()\n        with open('{}.record'.format(move_name), 'w') as f:\n            recorder.move.save(f)\n\n        # Stop player if running : to discuss\n        # Recording a playing move can produce strange outputs, but could be a good feature\n        try:\n            player = getattr(self.robot, '_{}_player'.format(move_name))\n            if player.running:\n                player.stop()\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_move_player(self, move_name, speed=1.0, backwards=False):\n\n        # check if running\n        try:\n            player = getattr(self.robot, '_{}_player'.format(move_name))\n            if player.running:\n                return\n        except AttributeError:\n            pass\n\n        # if not running, override the play primitive\n        with open('{}.record'.format(move_name)) as f:\n            loaded_move = Move.load(f)\n        player = MovePlayer(self.robot, loaded_move, play_speed=speed, backwards=backwards)\n        self.robot.attach_primitive(player, '_{}_player'.format(move_name))\n\n        player.start()\n        return player.duration()", "response": "Start a move player."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the full path to gdcmconv.", "response": "def _get_gdcmconv():\n    \"\"\"\n    Get the full path to gdcmconv.\n    If not found raise error\n    \"\"\"\n    gdcmconv_executable = settings.gdcmconv_path\n    if gdcmconv_executable is None:\n        gdcmconv_executable = _which('gdcmconv')\n    if gdcmconv_executable is None:\n        gdcmconv_executable = _which('gdcmconv.exe')\n\n    if gdcmconv_executable is None:\n        raise ConversionError('GDCMCONV_NOT_FOUND')\n\n    return gdcmconv_executable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_dicom_file(filename):\n    file_stream = open(filename, 'rb')\n    file_stream.seek(128)\n    data = file_stream.read(4)\n    file_stream.close()\n    if data == b'DICM':\n        return True\n    if settings.pydicom_read_force:\n        try:\n            dicom_headers = pydicom.read_file(filename, defer_size=\"1 KB\", stop_before_pixels=True, force=True)\n            if dicom_headers is not None:\n                return True\n        except:\n            pass\n    return False", "response": "Checks if the file is a dicom file and returns True if it is a dicom file False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_compressed(dicom_file, force=False):\n    header = pydicom.read_file(dicom_file,\n                               defer_size=\"1 KB\",\n                               stop_before_pixels=True,\n                               force=force)\n\n    uncompressed_types = [\"1.2.840.10008.1.2\",\n                          \"1.2.840.10008.1.2.1\",\n                          \"1.2.840.10008.1.2.1.99\",\n                          \"1.2.840.10008.1.2.2\"]\n\n    if 'TransferSyntaxUID' in header.file_meta and header.file_meta.TransferSyntaxUID in uncompressed_types:\n        return False\n    return True", "response": "Check if dicoms are compressed or not"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _decompress_dicom(dicom_file, output_file):\n    gdcmconv_executable = _get_gdcmconv()\n\n    subprocess.check_output([gdcmconv_executable, '-w', dicom_file, output_file])", "response": "This function can be used to convert a jpeg compressed image to an uncompressed one for further conversion"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dicom_diff(file1, file2):\n\n    datasets = compressed_dicom.read_file(file1), compressed_dicom.read_file(file2)\n\n    rep = []\n\n    for dataset in datasets:\n        lines = (str(dataset.file_meta)+\"\\n\"+str(dataset)).split('\\n')\n        lines = [line + '\\n' for line in lines]  # add the newline to the end\n        rep.append(lines)\n\n    diff = difflib.Differ()\n    for line in diff.compare(rep[0], rep[1]):\n        if (line[0] == '+') or (line[0] == '-'):\n            sys.stdout.write(line)", "response": "Show the fields that differ between two DICOM images."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_slice(self, slice_type, slice_number, time_point=0):\n        slice_ = Slice()\n        slice_.slice_number = slice_number\n        # assert that slice_ number is withing the range\n        assert slice_number >= 0\n        assert slice_number < self._get_number_of_slices(slice_type)\n        slice_data = None\n        if slice_type == SliceType.AXIAL:\n            slice_data = self.__get_raw_slice__(slice_number, self.axial_orientation, time_point)\n            slice_.slice_orientation = self.axial_orientation\n        elif slice_type == SliceType.SAGITTAL:\n            slice_data = self.__get_raw_slice__(slice_number, self.sagittal_orientation, time_point)\n            slice_.slice_orientation = self.sagittal_orientation\n        elif slice_type == SliceType.CORONAL:\n            slice_data = self.__get_raw_slice__(slice_number, self.coronal_orientation, time_point)\n            slice_.slice_orientation = self.coronal_orientation\n        # make a copy of the slice_ so we do not modify the orignal\n        slice_.original_data = slice_data\n        return slice_", "response": "Returns a slice of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the number of slices in a certain direction", "response": "def _get_number_of_slices(self, slice_type):\n        \"\"\"\n        Get the number of slices in a certain direction\n        \"\"\"\n        if slice_type == SliceType.AXIAL:\n            return self.dimensions[self.axial_orientation.normal_component]\n        elif slice_type == SliceType.SAGITTAL:\n            return self.dimensions[self.sagittal_orientation.normal_component]\n        elif slice_type == SliceType.CORONAL:\n            return self.dimensions[self.coronal_orientation.normal_component]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dicom_series_to_nifti(original_dicom_directory, output_file=None, reorient_nifti=True):\n    # copy files so we can can modify without altering the original\n    temp_directory = tempfile.mkdtemp()\n    try:\n        dicom_directory = os.path.join(temp_directory, 'dicom')\n        shutil.copytree(original_dicom_directory, dicom_directory)\n\n        dicom_input = common.read_dicom_directory(dicom_directory)\n\n        return dicom_array_to_nifti(dicom_input, output_file, reorient_nifti)\n\n    except AttributeError as exception:\n        reraise(\n            tp=ConversionError,\n            value=ConversionError(str(exception)),\n            tb=sys.exc_info()[2])\n\n    finally:\n        # remove the copied data\n        shutil.rmtree(temp_directory)", "response": "Converts a single series to nifty"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dicom_array_to_nifti(dicom_list, output_file, reorient_nifti=True):\n    # copy files so we can can modify without altering the original\n    if not are_imaging_dicoms(dicom_list):\n        raise ConversionValidationError('NON_IMAGING_DICOM_FILES')\n\n    vendor = _get_vendor(dicom_list)\n    if vendor == Vendor.GENERIC:\n        results = convert_generic.dicom_to_nifti(dicom_list, output_file)\n    elif vendor == Vendor.SIEMENS:\n        results = convert_siemens.dicom_to_nifti(dicom_list, output_file)\n    elif vendor == Vendor.GE:\n        results = convert_ge.dicom_to_nifti(dicom_list, output_file)\n    elif vendor == Vendor.PHILIPS:\n        results = convert_philips.dicom_to_nifti(dicom_list, output_file)\n    elif vendor == Vendor.HITACHI:\n        results = convert_hitachi.dicom_to_nifti(dicom_list, output_file)\n    else:\n        raise ConversionValidationError(\"UNSUPPORTED_DATA\")\n\n    # do image reorientation if needed\n    if reorient_nifti or settings.resample:\n        image_reorientation.reorient_image(results['NII_FILE'], results['NII_FILE'])\n\n    # resampling needs to be after reorientation\n    if settings.resample:\n        if not common.is_orthogonal_nifti(results['NII_FILE']):\n            resample.resample_single_nifti(results['NII_FILE'])\n\n    return results", "response": "Converts a list of dicom objects to nifty"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to get the first dicom file form a directory and return the header WorkItem", "response": "def _get_first_header(dicom_directory):\n    \"\"\"\n    Function to get the first dicom file form a directory and return the header\n    Useful to determine the type of data to convert\n\n    :param dicom_directory: directory with dicom files\n    \"\"\"\n    # looping over all files\n    for root, _, file_names in os.walk(dicom_directory):\n        # go over all the files and try to read the dicom header\n        for file_name in file_names:\n            file_path = os.path.join(root, file_name)\n            # check wither it is a dicom file\n            if not compressed_dicom.is_dicom_file(file_path):\n                continue\n            # read the headers\n            return compressed_dicom.read_file(file_path,\n                                              stop_before_pixels=True,\n                                              force=dicom2nifti.settings.pydicom_read_force)\n    # no dicom files found\n    raise ConversionError('NO_DICOM_FILES_FOUND')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nshrinking a single dicom file into smaller ones", "response": "def _shrink_file(dicom_file_in, subsample_factor):\n    \"\"\"\n    Anonimize a single dicomfile\n    :param dicom_file_in: filepath for input file\n    :param dicom_file_out: filepath for output file\n    :param fields_to_keep: dicom tags to keep\n    \"\"\"\n    # Default meta_fields\n    # Required fields according to reference\n\n    dicom_file_out = dicom_file_in\n\n    # Load dicom_file_in\n    dicom_in = compressed_dicom.read_file(dicom_file_in)\n\n    # Create new dicom file\n    # Set new file meta information\n    file_meta = pydicom.dataset.Dataset()\n    for key, value in dicom_in.file_meta.items():\n        file_meta.add(value)\n\n        # Create the FileDataset instance (initially no data elements, but file_meta supplied)\n    dicom_out = pydicom.dataset.FileDataset(dicom_file_out, {}, file_meta=file_meta, preamble=b'\\0' * 128)\n\n    # Copy transfer syntax\n    dicom_out.is_little_endian = dicom_in.is_little_endian\n    dicom_out.is_implicit_VR = dicom_in.is_implicit_VR\n\n    rows = 0\n    columns = 0\n\n    # Add the data elements\n    for field_key, field_value in dicom_in.items():\n        logging.info(field_key)\n        if field_key == (0x7fe0, 0x0010):\n            pixel_array = dicom_in.pixel_array[::subsample_factor, ::subsample_factor]\n\n            dicom_out.PixelData = pixel_array.tostring()  # = byte array (see pydicom docs)\n            rows = pixel_array.shape[1]\n            columns = pixel_array.shape[0]\n            # noinspection PyPep8Naming\n            dicom_out[0x7fe0, 0x0010].VR = 'OB'\n        else:\n            dicom_out.add(field_value)\n\n    dicom_out.PixelSpacing[0] *= subsample_factor\n    dicom_out.PixelSpacing[1] *= subsample_factor\n    dicom_out.Rows = rows\n    dicom_out.Columns = columns\n\n    # Save dicom_file_out\n    # Make sure we have a directory\n    if not os.path.exists(os.path.dirname(dicom_file_out)):\n        logging.info('Decompressing files')\n\n    # Save the file\n    dicom_out.save_as(dicom_file_out, write_like_original=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreorganizes the image in the NI - Volume space and returns the output image in LAS space.", "response": "def reorient_image(input_image, output_image):\n    \"\"\"\n    Change the orientation of the Image data in order to be in LAS space\n    x will represent the coronal plane, y the sagittal and z the axial plane.\n    x increases from Right (R) to Left (L), y from Posterior (P) to Anterior (A) and z from Inferior (I) to Superior (S)\n\n    :returns: The output image in nibabel form\n    :param output_image: filepath to the nibabel image\n    :param input_image: filepath to the nibabel image\n    \"\"\"\n    # Use the imageVolume module to find which coordinate corresponds to each plane\n    # and get the image data in RAS orientation\n    # print 'Reading nifti'\n    image = load(input_image)\n\n    # 4d have a different conversion to 3d\n    # print 'Reorganizing data'\n    if image.nifti_data.squeeze().ndim == 4:\n        new_image = _reorient_4d(image)\n    elif image.nifti_data.squeeze().ndim == 3:\n        new_image = _reorient_3d(image)\n    else:\n        raise Exception('Only 3d and 4d images are supported')\n\n    # print 'Recreating affine'\n    affine = image.nifti.affine\n    # Based on VolumeImage.py where slice orientation 1 represents the axial plane\n    # Flipping on the data may be needed based on x_inverted, y_inverted, ZInverted\n\n    # Create new affine header by changing the order of the columns of the input image header\n    # the last column with the origin depends on the origin of the original image, the size and the direction of x,y,z\n\n    new_affine = numpy.eye(4)\n    new_affine[:, 0] = affine[:, image.sagittal_orientation.normal_component]\n    new_affine[:, 1] = affine[:, image.coronal_orientation.normal_component]\n    new_affine[:, 2] = affine[:, image.axial_orientation.normal_component]\n    point = [0, 0, 0, 1]\n\n    # If the orientation of coordinates is inverted, then the origin of the \"new\" image\n    # would correspond to the last voxel of the original image\n    # First we need to find which point is the origin point in image coordinates\n    # and then transform it in world coordinates\n    if not image.axial_orientation.x_inverted:\n        new_affine[:, 0] = - new_affine[:, 0]\n        point[image.sagittal_orientation.normal_component] = image.dimensions[\n                                                                image.sagittal_orientation.normal_component] - 1\n        # new_affine[0, 3] = - new_affine[0, 3]\n    if image.axial_orientation.y_inverted:\n        new_affine[:, 1] = - new_affine[:, 1]\n        point[image.coronal_orientation.normal_component] = image.dimensions[\n                                                                image.coronal_orientation.normal_component] - 1\n        # new_affine[1, 3] = - new_affine[1, 3]\n    if image.coronal_orientation.y_inverted:\n        new_affine[:, 2] = - new_affine[:, 2]\n        point[image.axial_orientation.normal_component] = image.dimensions[image.axial_orientation.normal_component] - 1\n        # new_affine[2, 3] = - new_affine[2, 3]\n\n    new_affine[:, 3] = numpy.dot(affine, point)\n\n    # DONE: Needs to update new_affine, so that there is no translation difference between the original\n    # and created image (now there is 1-2 voxels translation)\n    # print 'Creating new nifti image'\n    nibabel.nifti1.Nifti1Image(new_image, new_affine).to_filename(output_image)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreorganizes the data for a 3d nifti image", "response": "def _reorient_3d(image):\n    \"\"\"\n    Reorganize the data for a 3d nifti\n    \"\"\"\n    # Create empty array where x,y,z correspond to LR (sagittal), PA (coronal), IS (axial) directions and the size\n    # of the array in each direction is the same with the corresponding direction of the input image.\n    new_image = numpy.zeros([image.dimensions[image.sagittal_orientation.normal_component],\n                             image.dimensions[image.coronal_orientation.normal_component],\n                             image.dimensions[image.axial_orientation.normal_component]],\n                            dtype=image.nifti_data.dtype)\n\n    # Fill the new image with the values of the input image but with matching the orientation with x,y,z\n    if image.coronal_orientation.y_inverted:\n        for i in range(new_image.shape[2]):\n            new_image[:, :, i] = numpy.fliplr(numpy.squeeze(image.get_slice(SliceType.AXIAL,\n                                                                            new_image.shape[2] - 1 - i).original_data))\n    else:\n        for i in range(new_image.shape[2]):\n            new_image[:, :, i] = numpy.fliplr(numpy.squeeze(image.get_slice(SliceType.AXIAL,\n                                                                            i).original_data))\n\n    return new_image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dicom_to_nifti(dicom_input, output_file=None):\n\n    assert common.is_philips(dicom_input)\n\n    if common.is_multiframe_dicom(dicom_input):\n        _assert_explicit_vr(dicom_input)\n        logger.info('Found multiframe dicom')\n        if _is_multiframe_4d(dicom_input):\n            logger.info('Found sequence type: MULTIFRAME 4D')\n            return _multiframe_to_nifti(dicom_input, output_file)\n\n        if _is_multiframe_anatomical(dicom_input):\n            logger.info('Found sequence type: MULTIFRAME ANATOMICAL')\n            return _multiframe_to_nifti(dicom_input, output_file)\n    else:\n        logger.info('Found singleframe dicom')\n        grouped_dicoms = _get_grouped_dicoms(dicom_input)\n        if _is_singleframe_4d(dicom_input):\n            logger.info('Found sequence type: SINGLEFRAME 4D')\n            return _singleframe_to_nifti(grouped_dicoms, output_file)\n\n    logger.info('Assuming anatomical data')\n    return convert_generic.dicom_to_nifti(dicom_input, output_file)", "response": "This function is the main dicom to nifti conversion fuction for philips images. It will first convert the dicom files to nifti files and then convert them to nifti files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nasserting that explicit vr is used", "response": "def _assert_explicit_vr(dicom_input):\n    \"\"\"\n    Assert that explicit vr is used\n    \"\"\"\n    if settings.validate_multiframe_implicit:\n        header = dicom_input[0]\n        if header.file_meta[0x0002, 0x0010].value == '1.2.840.10008.1.2':\n            raise ConversionError('IMPLICIT_VR_ENHANCED_DICOM')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_multiframe_diffusion_imaging(dicom_input):\n    header = dicom_input[0]\n\n    if \"PerFrameFunctionalGroupsSequence\" not in header:\n        return False\n\n    # check if there is diffusion info in the frame\n    found_diffusion = False\n    diffusion_tag = Tag(0x0018, 0x9117)\n    for frame in header.PerFrameFunctionalGroupsSequence:\n        if diffusion_tag in frame:\n            found_diffusion = True\n            break\n    if not found_diffusion:\n        return False\n\n    return True", "response": "Check if a dicom series is a philips multiframe dti dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a dicom series is a philips multiframe 4D dataset", "response": "def _is_multiframe_4d(dicom_input):\n    \"\"\"\n    Use this function to detect if a dicom series is a philips multiframe 4D dataset\n    \"\"\"\n    # check if it is multi frame dicom\n    if not common.is_multiframe_dicom(dicom_input):\n        return False\n\n    header = dicom_input[0]\n\n    # check if there are multiple stacks\n    number_of_stack_slices = common.get_ss_value(header[Tag(0x2001, 0x105f)][0][Tag(0x2001, 0x102d)])\n    number_of_stacks = int(int(header.NumberOfFrames) / number_of_stack_slices)\n    if number_of_stacks <= 1:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a dicom series is a philips singleframe 4D dataset", "response": "def _is_singleframe_4d(dicom_input):\n    \"\"\"\n    Use this function to detect if a dicom series is a philips singleframe 4D dataset\n    \"\"\"\n    header = dicom_input[0]\n\n    # check if there are stack information\n    slice_number_mr_tag = Tag(0x2001, 0x100a)\n    if slice_number_mr_tag not in header:\n        return False\n\n    # check if there are multiple timepoints\n    grouped_dicoms = _get_grouped_dicoms(dicom_input)\n    if len(grouped_dicoms) <= 1:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_bval_type_a(grouped_dicoms):\n    bval_tag = Tag(0x2001, 0x1003)\n    bvec_x_tag = Tag(0x2005, 0x10b0)\n    bvec_y_tag = Tag(0x2005, 0x10b1)\n    bvec_z_tag = Tag(0x2005, 0x10b2)\n    for group in grouped_dicoms:\n        if bvec_x_tag in group[0] and _is_float(common.get_fl_value(group[0][bvec_x_tag])) and \\\n                bvec_y_tag in group[0] and _is_float(common.get_fl_value(group[0][bvec_y_tag])) and \\\n                bvec_z_tag in group[0] and _is_float(common.get_fl_value(group[0][bvec_z_tag])) and \\\n                bval_tag in group[0] and _is_float(common.get_fl_value(group[0][bval_tag])) and \\\n                common.get_fl_value(group[0][bval_tag]) != 0:\n            return True\n    return False", "response": "Check if the bvals are stored in the first of 2 currently known ways for single frame dti\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_bval_type_b(grouped_dicoms):\n    bval_tag = Tag(0x0018, 0x9087)\n    bvec_tag = Tag(0x0018, 0x9089)\n    for group in grouped_dicoms:\n        if bvec_tag in group[0] and bval_tag in group[0]:\n            bvec = common.get_fd_array_value(group[0][bvec_tag], 3)\n            bval = common.get_fd_value(group[0][bval_tag])\n            if _is_float(bvec[0]) and _is_float(bvec[1]) and _is_float(bvec[2]) and _is_float(bval) and bval != 0:\n                return True\n    return False", "response": "Check if the bvals are stored in the second of 2 currently known ways for single frame dti\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _singleframe_to_nifti(grouped_dicoms, output_file):\n\n    # Create mosaic block\n    logger.info('Creating data block')\n    full_block = _singleframe_to_block(grouped_dicoms)\n\n    logger.info('Creating affine')\n    # Create the nifti header info\n    affine, slice_increment = common.create_affine(grouped_dicoms[0])\n\n    logger.info('Creating nifti')\n    # Convert to nifti\n    nii_image = nibabel.Nifti1Image(full_block, affine)\n    common.set_tr_te(nii_image, float(grouped_dicoms[0][0].RepetitionTime), float(grouped_dicoms[0][0].EchoTime))\n\n    if output_file is not None:\n        # Save to disk\n        logger.info('Saving nifti to disk %s' % output_file)\n        nii_image.to_filename(output_file)\n\n    if _is_singleframe_diffusion_imaging(grouped_dicoms):\n        bval_file = None\n        bvec_file = None\n        # Create the bval en bvec files\n        if output_file is not None:\n            base_name = os.path.splitext(output_file)[0]\n            if base_name.endswith('.nii'):\n                base_name = os.path.splitext(base_name)[0]\n\n            logger.info('Creating bval en bvec files')\n            bval_file = '%s.bval' % base_name\n            bvec_file = '%s.bvec' % base_name\n        nii_image, bval, bvec, bval_file, bvec_file = _create_singleframe_bvals_bvecs(grouped_dicoms,\n                                                                                      bval_file,\n                                                                                      bvec_file,\n                                                                                      nii_image,\n                                                                                      output_file)\n\n        return {'NII_FILE': output_file,\n                'BVAL_FILE': bval_file,\n                'BVEC_FILE': bvec_file,\n                'NII': nii_image,\n                'BVAL': bval,\n                'BVEC': bvec,\n                'MAX_SLICE_INCREMENT': slice_increment}\n\n    return {'NII_FILE': output_file,\n            'NII': nii_image,\n            'MAX_SLICE_INCREMENT': slice_increment}", "response": "This function will convert a philips singleframe series to a nifti header"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a list of timepoints into a full datablock containing all timepoints", "response": "def _singleframe_to_block(grouped_dicoms):\n    \"\"\"\n    Generate a full datablock containing all timepoints\n    \"\"\"\n    # For each slice / mosaic create a data volume block\n    data_blocks = []\n    for index in range(0, len(grouped_dicoms)):\n        logger.info('Creating block %s of %s' % (index + 1, len(grouped_dicoms)))\n        current_block = _stack_to_block(grouped_dicoms[index])\n        current_block = current_block[:, :, :, numpy.newaxis]\n        data_blocks.append(current_block)\n\n    try:\n        full_block = numpy.concatenate(data_blocks, axis=3)\n    except:\n        traceback.print_exc()\n        raise ConversionError(\"MISSING_DICOM_FILES\")\n\n    # Apply the rescaling if needed\n    common.apply_scaling(full_block, grouped_dicoms[0][0])\n\n    return full_block"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_grouped_dicoms(dicom_input):\n    # if all dicoms have an instance number try sorting by instance number else by position\n    if [d for d in dicom_input if 'InstanceNumber' in d]:\n        dicoms = sorted(dicom_input, key=lambda x: x.InstanceNumber)\n    else:\n        dicoms = common.sort_dicoms(dicom_input)\n    # now group per stack\n    grouped_dicoms = [[]]  # list with first element a list\n    timepoint_index = 0\n    previous_stack_position = -1\n\n    # loop over all sorted dicoms\n    stack_position_tag = Tag(0x2001, 0x100a)  # put this there as this is a slow step and used a lot\n    for index in range(0, len(dicoms)):\n        dicom_ = dicoms[index]\n        stack_position = 0\n        if stack_position_tag in dicom_:\n            stack_position = common.get_is_value(dicom_[stack_position_tag])\n        if previous_stack_position == stack_position:\n            # if the stack number is the same we move to the next timepoint\n            timepoint_index += 1\n            if len(grouped_dicoms) <= timepoint_index:\n                grouped_dicoms.append([])\n        else:\n            # if it changes move back to the first timepoint\n            timepoint_index = 0\n        grouped_dicoms[timepoint_index].append(dicom_)\n        previous_stack_position = stack_position\n\n    return grouped_dicoms", "response": "Get all dicoms in the dicom directory and validate them"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction to create the affine matrix for a siemens mosaic dataset", "response": "def _create_affine_multiframe(multiframe_dicom):\n    \"\"\"\n    Function to create the affine matrix for a siemens mosaic dataset\n    This will work for siemens dti and 4D if in mosaic format\n    \"\"\"\n    first_frame = multiframe_dicom[Tag(0x5200, 0x9230)][0]\n    last_frame = multiframe_dicom[Tag(0x5200, 0x9230)][-1]\n    # Create affine matrix (http://nipy.sourceforge.net/nibabel/dicom/dicom_orientation.html#dicom-slice-affine)\n    image_orient1 = numpy.array(first_frame.PlaneOrientationSequence[0].ImageOrientationPatient)[0:3].astype(float)\n    image_orient2 = numpy.array(first_frame.PlaneOrientationSequence[0].ImageOrientationPatient)[3:6].astype(float)\n\n    normal = numpy.cross(image_orient1, image_orient2)\n\n    delta_r = float(first_frame[0x2005, 0x140f][0].PixelSpacing[0])\n    delta_c = float(first_frame[0x2005, 0x140f][0].PixelSpacing[1])\n\n    image_pos = numpy.array(first_frame.PlanePositionSequence[0].ImagePositionPatient).astype(float)\n    last_image_pos = numpy.array(last_frame.PlanePositionSequence[0].ImagePositionPatient).astype(float)\n\n    number_of_stack_slices = int(common.get_ss_value(multiframe_dicom[Tag(0x2001, 0x105f)][0][Tag(0x2001, 0x102d)]))\n    delta_s = abs(numpy.linalg.norm(last_image_pos - image_pos)) / (number_of_stack_slices - 1)\n\n    return numpy.array(\n        [[-image_orient1[0] * delta_c, -image_orient2[0] * delta_r, -delta_s * normal[0], -image_pos[0]],\n         [-image_orient1[1] * delta_c, -image_orient2[1] * delta_r, -delta_s * normal[1], -image_pos[1]],\n         [image_orient1[2] * delta_c, image_orient2[2] * delta_r, delta_s * normal[2], image_pos[2]],\n         [0, 0, 0, 1]])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a full datablock containing all stacks and slices in the multiframe dataset.", "response": "def _multiframe_to_block(multiframe_dicom):\n    \"\"\"\n    Generate a full datablock containing all stacks\n    \"\"\"\n    # Calculate the amount of stacks and slices in the stack\n    number_of_stack_slices = int(common.get_ss_value(multiframe_dicom[Tag(0x2001, 0x105f)][0][Tag(0x2001, 0x102d)]))\n    number_of_stacks = int(int(multiframe_dicom.NumberOfFrames) / number_of_stack_slices)\n\n    # We create a numpy array\n    size_x = multiframe_dicom.pixel_array.shape[2]\n    size_y = multiframe_dicom.pixel_array.shape[1]\n    size_z = number_of_stack_slices\n    size_t = number_of_stacks\n    # get the format\n    format_string = common.get_numpy_type(multiframe_dicom)\n\n    # get header info needed for ordering\n    frame_info = multiframe_dicom[0x5200, 0x9230]\n\n    data_4d = numpy.zeros((size_z, size_y, size_x, size_t), dtype=format_string)\n\n    # loop over each slice and insert in datablock\n    t_location_index = _get_t_position_index(multiframe_dicom)\n    for slice_index in range(0, size_t * size_z):\n\n        z_location = frame_info[slice_index].FrameContentSequence[0].InStackPositionNumber - 1\n        if t_location_index is None:\n            t_location = frame_info[slice_index].FrameContentSequence[0].TemporalPositionIndex - 1\n        else:\n            t_location = frame_info[slice_index].FrameContentSequence[0].DimensionIndexValues[t_location_index] - 1\n\n        block_data = multiframe_dicom.pixel_array[slice_index, :, :]\n        # apply scaling\n        rescale_intercept = frame_info[slice_index].PixelValueTransformationSequence[0].RescaleIntercept\n        rescale_slope = frame_info[slice_index].PixelValueTransformationSequence[0].RescaleSlope\n        block_data = common.do_scaling(block_data,\n                                       rescale_slope, rescale_intercept)\n        # switch to float if needed\n        if block_data.dtype != data_4d.dtype:\n            data_4d = data_4d.astype(block_data.dtype)\n        data_4d[z_location, :, :, t_location] = block_data\n\n    full_block = numpy.zeros((size_x, size_y, size_z, size_t), dtype=data_4d.dtype)\n\n    # loop over each stack and reorganize the data\n    for t_index in range(0, size_t):\n        # transpose the block so the directions are correct\n        data_3d = numpy.transpose(data_4d[:, :, :, t_index], (2, 1, 0))\n        # add the block the the full data\n        full_block[:, :, :, t_index] = data_3d\n\n    return full_block"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating bvals and bvecs from the dicom files.", "response": "def _create_bvals_bvecs(multiframe_dicom, bval_file, bvec_file, nifti, nifti_file):\n    \"\"\"\n    Write the bvals from the sorted dicom files to a bval file\n    Inspired by https://github.com/IBIC/ibicUtils/blob/master/ibicBvalsBvecs.py\n    \"\"\"\n\n    # create the empty arrays\n    number_of_stack_slices = common.get_ss_value(multiframe_dicom[Tag(0x2001, 0x105f)][0][Tag(0x2001, 0x102d)])\n    number_of_stacks = int(int(multiframe_dicom.NumberOfFrames) / number_of_stack_slices)\n\n    bvals = numpy.zeros([number_of_stacks], dtype=numpy.int32)\n    bvecs = numpy.zeros([number_of_stacks, 3])\n\n    # loop over all timepoints and create a list with all bvals and bvecs\n    for stack_index in range(0, number_of_stacks):\n        stack = multiframe_dicom[Tag(0x5200, 0x9230)][stack_index]\n        if str(stack[Tag(0x0018, 0x9117)][0][Tag(0x0018, 0x9075)].value) == 'DIRECTIONAL':\n            bvals[stack_index] = common.get_fd_value(stack[Tag(0x0018, 0x9117)][0][Tag(0x0018, 0x9087)])\n            bvecs[stack_index, :] = common.get_fd_array_value(stack[Tag(0x0018, 0x9117)][0]\n                                                              [Tag(0x0018, 0x9076)][0][Tag(0x0018, 0x9089)], 3)\n\n    # truncate nifti if needed\n    nifti, bvals, bvecs = _fix_diffusion_images(bvals, bvecs, nifti, nifti_file)\n\n    # save the found bvecs to the file\n    if numpy.count_nonzero(bvals) > 0 or numpy.count_nonzero(bvecs) > 0:\n        common.write_bval_file(bvals, bval_file)\n        common.write_bvec_file(bvecs, bvec_file)\n    else:\n        bval_file = None\n        bvec_file = None\n        bvals = None\n        bvecs = None\n\n    return bvals, bvecs, bval_file, bvec_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fix_diffusion_images(bvals, bvecs, nifti, nifti_file):\n    # if all zero continue of if the last bvec is not all zero continue\n    if numpy.count_nonzero(bvecs) == 0 or not numpy.count_nonzero(bvals[-1]) == 0:\n        # nothing needs to be done here\n        return nifti, bvals, bvecs\n    # remove last elements from bvals and bvecs\n    bvals = bvals[:-1]\n    bvecs = bvecs[:-1]\n\n    # remove last elements from the nifti\n    new_nifti = nibabel.Nifti1Image(nifti.get_data()[:, :, :, :-1], nifti.affine)\n    new_nifti.to_filename(nifti_file)\n\n    return new_nifti, bvals, bvecs", "response": "This function will remove the last timepoint from the nifti bvals and bvecs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the bvals and bvecs from the sorted dicom files.", "response": "def _create_singleframe_bvals_bvecs(grouped_dicoms, bval_file, bvec_file, nifti, nifti_file):\n    \"\"\"\n    Write the bvals from the sorted dicom files to a bval file\n    \"\"\"\n\n    # create the empty arrays\n    bvals = numpy.zeros([len(grouped_dicoms)], dtype=numpy.int32)\n    bvecs = numpy.zeros([len(grouped_dicoms), 3])\n\n    # loop over all timepoints and create a list with all bvals and bvecs\n    if _is_bval_type_a(grouped_dicoms):\n        bval_tag = Tag(0x2001, 0x1003)\n        bvec_x_tag = Tag(0x2005, 0x10b0)\n        bvec_y_tag = Tag(0x2005, 0x10b1)\n        bvec_z_tag = Tag(0x2005, 0x10b2)\n        for stack_index in range(0, len(grouped_dicoms)):\n            bvals[stack_index] = common.get_fl_value(grouped_dicoms[stack_index][0][bval_tag])\n            bvecs[stack_index, :] = [common.get_fl_value(grouped_dicoms[stack_index][0][bvec_x_tag]),\n                                     common.get_fl_value(grouped_dicoms[stack_index][0][bvec_y_tag]),\n                                     common.get_fl_value(grouped_dicoms[stack_index][0][bvec_z_tag])]\n    elif _is_bval_type_b(grouped_dicoms):\n        bval_tag = Tag(0x0018, 0x9087)\n        bvec_tag = Tag(0x0018, 0x9089)\n        for stack_index in range(0, len(grouped_dicoms)):\n            bvals[stack_index] = common.get_fd_value(grouped_dicoms[stack_index][0][bval_tag])\n            bvecs[stack_index, :] = common.get_fd_array_value(grouped_dicoms[stack_index][0][bvec_tag], 3)\n\n    # truncate nifti if needed\n    nifti, bvals, bvecs = _fix_diffusion_images(bvals, bvecs, nifti, nifti_file)\n\n    # save the found bvecs to the file\n    if numpy.count_nonzero(bvals) > 0 or numpy.count_nonzero(bvecs) > 0:\n        common.write_bval_file(bvals, bval_file)\n        common.write_bvec_file(bvecs, bvec_file)\n    else:\n        bval_file = None\n        bvec_file = None\n        bvals = None\n        bvecs = None\n    return nifti, bvals, bvecs, bval_file, bvec_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _remove_duplicate_slices(dicoms):\n    # Loop overall files and build dict\n\n    dicoms_dict = {}\n    filtered_dicoms = []\n    for dicom_ in dicoms:\n        if tuple(dicom_.ImagePositionPatient) not in dicoms_dict:\n            dicoms_dict[tuple(dicom_.ImagePositionPatient)] = dicom_\n            filtered_dicoms.append(dicom_)\n        else:\n            if numpy.array_equal(dicom_.pixel_array,\n                                 dicoms_dict[tuple(dicom_.ImagePositionPatient)].pixel_array):\n                logger.warning('Removing duplicate slice from series')\n            else:\n                filtered_dicoms.append(dicom_)\n    return filtered_dicoms", "response": "Remove duplicate slices from a list of dicoms"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _remove_localizers_by_imagetype(dicoms):\n    # Loop overall files and build dict\n    filtered_dicoms = []\n    for dicom_ in dicoms:\n        if 'ImageType' in dicom_ and 'LOCALIZER' in dicom_.ImageType:\n            continue\n        # 'Projection Image' are Localizers for CT only see MSMET-234\n        if 'CT' in dicom_.Modality and 'ImageType' in dicom_ and 'PROJECTION IMAGE' in dicom_.ImageType:\n            continue\n        filtered_dicoms.append(dicom_)\n    return filtered_dicoms", "response": "Remove localizers from a list of dicoms."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves localizers based on the orientation of the dicom files.", "response": "def _remove_localizers_by_orientation(dicoms):\n    \"\"\"\n    Removing localizers based on the orientation.\n    This is needed as in some cases with ct data there are some localizer/projection type images that cannot\n    be distiguished by the dicom headers. This is why we kick out all orientations that do not have more than 4 files\n    4 is the limit anyway for converting to nifti on our case\n    \"\"\"\n    orientations = []\n    sorted_dicoms = {}\n    # Loop overall files and build dict\n    for dicom_header in dicoms:\n        # Create affine matrix (http://nipy.sourceforge.net/nibabel/dicom/dicom_orientation.html#dicom-slice-affine)\n        image_orient1 = numpy.array(dicom_header.ImageOrientationPatient)[0:3]\n        image_orient2 = numpy.array(dicom_header.ImageOrientationPatient)[3:6]\n        image_orient_combined = (image_orient1.tolist(), image_orient2.tolist())\n        found_orientation = False\n        for orientation in orientations:\n            if numpy.allclose(image_orient_combined[0], numpy.array(orientation[0]), rtol=0.001, atol=0.001) \\\n                    and numpy.allclose(image_orient_combined[1], numpy.array(orientation[1]), rtol=0.001,\n                                       atol=0.001):\n                sorted_dicoms[str(orientation)].append(dicom_header)\n                found_orientation = True\n                break\n        if not found_orientation:\n            orientations.append(image_orient_combined)\n            sorted_dicoms[str(image_orient_combined)] = [dicom_header]\n\n    # if there are multiple possible orientations delete orientations where there are less than 4 files\n    # we don't convert anything less that that anyway\n\n    if len(sorted_dicoms) > 1:\n        filtered_dicoms = []\n        for orientation in sorted_dicoms.keys():\n            if len(sorted_dicoms[orientation]) >= 4:\n                filtered_dicoms.extend(sorted_dicoms[orientation])\n        return filtered_dicoms\n    else:\n        return six.next(six.itervalues(sorted_dicoms))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert slice inconsistency in a single image into a single subvolume.", "response": "def _convert_slice_incement_inconsistencies(dicom_input):\n    \"\"\"\n    If there is slice increment inconsistency detected, for the moment CT images, then split the volumes into subvolumes based on the slice increment and process each volume separately using a space constructed based on the highest resolution increment\n    \"\"\"\n\n    #   Estimate the \"first\" slice increment based on the 2 first slices\n    increment = numpy.array(dicom_input[0].ImagePositionPatient) - numpy.array(dicom_input[1].ImagePositionPatient)\n\n    # Create as many volumes as many changes in slice increment. NB Increments might be repeated in different volumes\n    max_slice_increment = 0\n    slice_incement_groups = []\n    current_group = [dicom_input[0], dicom_input[1]]\n    previous_image_position = numpy.array(dicom_input[1].ImagePositionPatient)\n    for dicom in dicom_input[2:]:\n        current_image_position = numpy.array(dicom.ImagePositionPatient)\n        current_increment = previous_image_position - current_image_position\n        max_slice_increment = max(max_slice_increment, numpy.linalg.norm(current_increment))\n        if numpy.allclose(increment, current_increment, rtol=0.05, atol=0.1):\n            current_group.append(dicom)\n        if not numpy.allclose(increment, current_increment, rtol=0.05, atol=0.1):\n            slice_incement_groups.append(current_group)\n            current_group = [current_group[-1], dicom]\n            increment = current_increment\n        previous_image_position = current_image_position\n    slice_incement_groups.append(current_group)\n\n    # Create nibabel objects for each volume based on the corresponding headers\n    slice_incement_niftis = []\n    for dicom_slices in slice_incement_groups:\n        data = common.get_volume_pixeldata(dicom_slices)\n        affine, _ = common.create_affine(dicom_slices)\n        slice_incement_niftis.append(nibabel.Nifti1Image(data, affine))\n\n    nifti_volume = resample.resample_nifti_images(slice_incement_niftis)\n\n    return nifti_volume, max_slice_increment"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_dicom_directory(dicom_directory, stop_before_pixels=False):\n    dicom_input = []\n    for root, _, files in os.walk(dicom_directory):\n        for dicom_file in files:\n            file_path = os.path.join(root, dicom_file)\n            if compressed_dicom.is_dicom_file(file_path):\n                dicom_headers = compressed_dicom.read_file(file_path,\n                                                           defer_size=\"1 KB\",\n                                                           stop_before_pixels=stop_before_pixels,\n                                                           force=dicom2nifti.settings.pydicom_read_force)\n                if is_valid_imaging_dicom(dicom_headers):\n                    dicom_input.append(dicom_headers)\n    return dicom_input", "response": "Read all dicom files in a given directory"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse this function to detect if a dicom series is a hitachi dataset :param dicom_input: directory with dicom files for 1 scan of a dicom_header", "response": "def is_hitachi(dicom_input):\n    \"\"\"\n    Use this function to detect if a dicom series is a hitachi dataset\n\n    :param dicom_input: directory with dicom files for 1 scan of a dicom_header\n    \"\"\"\n    # read dicom header\n    header = dicom_input[0]\n\n    if 'Manufacturer' not in header or 'Modality' not in header:\n        return False  # we try generic conversion in these cases\n\n    # check if Modality is mr\n    if header.Modality.upper() != 'MR':\n        return False\n\n    # check if manufacturer is hitachi\n    if 'HITACHI' not in header.Manufacturer.upper():\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse this function to detect if a dicom series is a GE dataset :param dicom_input: list with dicom objects", "response": "def is_ge(dicom_input):\n    \"\"\"\n    Use this function to detect if a dicom series is a GE dataset\n\n    :param dicom_input: list with dicom objects\n    \"\"\"\n    # read dicom header\n    header = dicom_input[0]\n\n    if 'Manufacturer' not in header or 'Modality' not in header:\n        return False  # we try generic conversion in these cases\n\n    # check if Modality is mr\n    if header.Modality.upper() != 'MR':\n        return False\n\n    # check if manufacturer is GE\n    if 'GE MEDICAL SYSTEMS' not in header.Manufacturer.upper():\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing this function to detect if a dicom series is a philips dataset :param dicom_input: directory with dicom files for 1 scan of a dicom_header", "response": "def is_philips(dicom_input):\n    \"\"\"\n    Use this function to detect if a dicom series is a philips dataset\n\n    :param dicom_input: directory with dicom files for 1 scan of a dicom_header\n    \"\"\"\n    # read dicom header\n    header = dicom_input[0]\n\n    if 'Manufacturer' not in header or 'Modality' not in header:\n        return False  # we try generic conversion in these cases\n\n    # check if Modality is mr\n    if header.Modality.upper() != 'MR':\n        return False\n\n    # check if manufacturer is Philips\n    if 'PHILIPS' not in header.Manufacturer.upper():\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_siemens(dicom_input):\n    # read dicom header\n    header = dicom_input[0]\n\n    # check if manufacturer is Siemens\n    if 'Manufacturer' not in header or 'Modality' not in header:\n        return False  # we try generic conversion in these cases\n\n    # check if Modality is mr\n    if header.Modality.upper() != 'MR':\n        return False\n\n    if 'SIEMENS' not in header.Manufacturer.upper():\n        return False\n\n    return True", "response": "This function detects if a dicom series is a siemens dataset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing this function to detect if a dicom series is a siemens 4D dataset NOTE: Only the first slice will be checked so you can only provide an already sorted dicom directory (containing one series) :param dicom_input: directory with dicom files for 1 scan", "response": "def is_multiframe_dicom(dicom_input):\n    \"\"\"\n    Use this function to detect if a dicom series is a siemens 4D dataset\n    NOTE: Only the first slice will be checked so you can only provide an already sorted dicom directory\n    (containing one series)\n\n    :param dicom_input: directory with dicom files for 1 scan\n    \"\"\"\n    # read dicom header\n    header = dicom_input[0]\n\n    if Tag(0x0002, 0x0002) not in header.file_meta:\n        return False\n    if header.file_meta[0x0002, 0x0002].value == '1.2.840.10008.5.1.4.1.1.4.1':\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction will check if the given dicom header is a valid imaging dicom", "response": "def is_valid_imaging_dicom(dicom_header):\n    \"\"\"\n    Function will do some basic checks to see if this is a valid imaging dicom\n    \"\"\"\n    # if it is philips and multiframe dicom then we assume it is ok\n    try:\n        if is_philips([dicom_header]):\n            if is_multiframe_dicom([dicom_header]):\n                return True\n\n        if \"SeriesInstanceUID\" not in dicom_header:\n            return False\n\n        if \"InstanceNumber\" not in dicom_header:\n            return False\n\n        if \"ImageOrientationPatient\" not in dicom_header or len(dicom_header.ImageOrientationPatient) < 6:\n            return False\n\n        if \"ImagePositionPatient\" not in dicom_header or len(dicom_header.ImagePositionPatient) < 3:\n            return False\n\n        # for all others if there is image position patient we assume it is ok\n        if Tag(0x0020, 0x0037) not in dicom_header:\n            return False\n\n        return True\n    except (KeyError, AttributeError):\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_volume_pixeldata(sorted_slices):\n    slices = []\n    combined_dtype = None\n    for slice_ in sorted_slices:\n        slice_data = _get_slice_pixeldata(slice_)\n        slice_data = slice_data[numpy.newaxis, :, :]\n        slices.append(slice_data)\n        if combined_dtype is None:\n            combined_dtype = slice_data.dtype\n        else:\n            combined_dtype = numpy.promote_types(combined_dtype, slice_data.dtype)\n\n    # create the new volume with with the correct data\n    vol = numpy.concatenate(slices, axis=0)\n\n    # Done\n    vol = numpy.transpose(vol, (2, 1, 0))\n    return vol", "response": "get the data for the given list of slices"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_numpy_type(dicom_header):\n\n    format_string = '%sint%d' % (('u', '')[dicom_header.PixelRepresentation], dicom_header.BitsAllocated)\n    try:\n        numpy.dtype(format_string)\n    except TypeError:\n        raise TypeError(\"Data type not understood by NumPy: format='%s', PixelRepresentation=%d, BitsAllocated=%d\" %\n                        (format_string, dicom_header.PixelRepresentation, dicom_header.BitsAllocated))\n    return format_string", "response": "Make NumPy format code from dicom header"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the value of the array that can be used to read the data from a single file.", "response": "def get_fd_array_value(tag, count):\n    \"\"\"\n    Getters for data that also work with implicit transfersyntax\n\n    :param count: number of items in the array\n    :param tag: the tag to read\n    \"\"\"\n    if tag.VR == 'OB' or tag.VR == 'UN':\n        values = []\n        for i in range(count):\n            start = i * 8\n            stop = (i + 1) * 8\n            values.append(struct.unpack('d', tag.value[start:stop])[0])\n        return numpy.array(values)\n    return tag.value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the value of a FD tag.", "response": "def get_fd_value(tag):\n    \"\"\"\n    Getters for data that also work with implicit transfersyntax\n\n    :param tag: the tag to read\n    \"\"\"\n    if tag.VR == 'OB' or tag.VR == 'UN':\n        value = struct.unpack('d', tag.value)[0]\n        return value\n    return tag.value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the value of the FD field in the given tag.", "response": "def set_fd_value(tag, value):\n    \"\"\"\n    Setters for data that also work with implicit transfersyntax\n\n    :param value: the value to set on the tag\n    :param tag: the tag to read\n    \"\"\"\n    if tag.VR == 'OB' or tag.VR == 'UN':\n        value = struct.pack('d', value)\n    tag.value = value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_fl_value(tag):\n    if tag.VR == 'OB' or tag.VR == 'UN':\n        value = struct.unpack('f', tag.value)[0]\n        return value\n    return tag.value", "response": "Getters for data that also work with implicit transfersyntax\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_is_value(tag):\n    # data is int formatted as string so convert te string first and cast to int\n    if tag.VR == 'OB' or tag.VR == 'UN':\n        value = int(tag.value.decode(\"ascii\").replace(\" \", \"\"))\n        return value\n    return int(tag.value)", "response": "Getters for data that also work with implicit transfersyntax\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ss_value(tag):\n    # data is int formatted as string so convert te string first and cast to int\n    if tag.VR == 'OB' or tag.VR == 'UN':\n        value = struct.unpack('h', tag.value)[0]\n        return value\n    return tag.value", "response": "Getters for data that also work with implicit transfersyntax\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the value of the SS tag.", "response": "def set_ss_value(tag, value):\n    \"\"\"\n    Setter for data that also work with implicit transfersyntax\n\n    :param value: the value to set on the tag\n    :param tag: the tag to read\n    \"\"\"\n    if tag.VR == 'OB' or tag.VR == 'UN':\n        value = struct.pack('h', value)\n    tag.value = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_bvec_file(bvecs, bvec_file):\n    if bvec_file is None:\n        return\n    logger.info('Saving BVEC file: %s' % bvec_file)\n    with open(bvec_file, 'w') as text_file:\n        # Map a dicection to string join them using a space and write to the file\n        text_file.write('%s\\n' % ' '.join(map(str, bvecs[:, 0])))\n        text_file.write('%s\\n' % ' '.join(map(str, bvecs[:, 1])))\n        text_file.write('%s\\n' % ' '.join(map(str, bvecs[:, 2])))", "response": "Write an array of bvecs to a bvec file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_bval_file(bvals, bval_file):\n    if bval_file is None:\n        return\n    logger.info('Saving BVAL file: %s' % bval_file)\n    with open(bval_file, 'w') as text_file:\n        # join the bvals using a space and write to the file\n        text_file.write('%s\\n' % ' '.join(map(str, bvals)))", "response": "Write an array of bvals to a bval file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction to generate the affine matrix for a dicom series", "response": "def create_affine(sorted_dicoms):\n    \"\"\"\n    Function to generate the affine matrix for a dicom series\n    This method was based on (http://nipy.org/nibabel/dicom/dicom_orientation.html)\n\n    :param sorted_dicoms: list with sorted dicom files\n    \"\"\"\n\n    # Create affine matrix (http://nipy.sourceforge.net/nibabel/dicom/dicom_orientation.html#dicom-slice-affine)\n    image_orient1 = numpy.array(sorted_dicoms[0].ImageOrientationPatient)[0:3]\n    image_orient2 = numpy.array(sorted_dicoms[0].ImageOrientationPatient)[3:6]\n\n    delta_r = float(sorted_dicoms[0].PixelSpacing[0])\n    delta_c = float(sorted_dicoms[0].PixelSpacing[1])\n\n    image_pos = numpy.array(sorted_dicoms[0].ImagePositionPatient)\n\n    last_image_pos = numpy.array(sorted_dicoms[-1].ImagePositionPatient)\n\n    if len(sorted_dicoms) == 1:\n        # Single slice\n        step = [0, 0, -1]\n    else:\n        step = (image_pos - last_image_pos) / (1 - len(sorted_dicoms))\n\n    # check if this is actually a volume and not all slices on the same location\n    if numpy.linalg.norm(step) == 0.0:\n        raise ConversionError(\"NOT_A_VOLUME\")\n\n    affine = numpy.array(\n        [[-image_orient1[0] * delta_c, -image_orient2[0] * delta_r, -step[0], -image_pos[0]],\n         [-image_orient1[1] * delta_c, -image_orient2[1] * delta_r, -step[1], -image_pos[1]],\n         [image_orient1[2] * delta_c, image_orient2[2] * delta_r, step[2], image_pos[2]],\n         [0, 0, 0, 1]]\n    )\n    return affine, numpy.linalg.norm(step)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating that volume is orthogonal.", "response": "def is_orthogonal(dicoms, log_details=False):\n    \"\"\"\n    Validate that volume is orthonormal\n\n    :param dicoms: check that we have a volume without skewing\n    \"\"\"\n    first_image_orient1 = numpy.array(dicoms[0].ImageOrientationPatient)[0:3]\n    first_image_orient2 = numpy.array(dicoms[0].ImageOrientationPatient)[3:6]\n    first_image_pos = numpy.array(dicoms[0].ImagePositionPatient)\n\n    last_image_pos = numpy.array(dicoms[-1].ImagePositionPatient)\n\n    first_image_dir = numpy.cross(first_image_orient1, first_image_orient2)\n    first_image_dir /= numpy.linalg.norm(first_image_dir)\n\n    combined_dir = last_image_pos - first_image_pos\n    combined_dir /= numpy.linalg.norm(combined_dir)\n\n    if not numpy.allclose(first_image_dir, combined_dir, rtol=0.05, atol=0.05) \\\n            and not numpy.allclose(first_image_dir, -combined_dir, rtol=0.05, atol=0.05):\n        if log_details:\n            logger.warning('Orthogonality check failed: non cubical image')\n            logger.warning('---------------------------------------------------------')\n            logger.warning(first_image_dir)\n            logger.warning(combined_dir)\n            logger.warning('---------------------------------------------------------')\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_orthogonal_nifti(nifti_file):\n    nifti_image = nibabel.load(nifti_file)\n    affine = nifti_image.affine\n\n    transformed_x = numpy.transpose(numpy.dot(affine, [[1], [0], [0], [0]]))[0][:3]\n    transformed_y = numpy.transpose(numpy.dot(affine, [[0], [1], [0], [0]]))[0][:3]\n    transformed_z = numpy.transpose(numpy.dot(affine, [[0], [0], [1], [0]]))[0][:3]\n\n    transformed_x /= numpy.linalg.norm(transformed_x)\n    transformed_y /= numpy.linalg.norm(transformed_y)\n    transformed_z /= numpy.linalg.norm(transformed_z)\n\n    perpendicular = numpy.cross(transformed_x, transformed_y)\n    perpendicular /= numpy.linalg.norm(perpendicular)\n\n    if not numpy.allclose(transformed_z, perpendicular, rtol=0.05, atol=0.05) \\\n            and not numpy.allclose(transformed_z, -perpendicular, rtol=0.05, atol=0.05):\n        return False\n    return True", "response": "Validate that volume is orthogonal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting the dicoms based om the image possition patient", "response": "def sort_dicoms(dicoms):\n    \"\"\"\n    Sort the dicoms based om the image possition patient\n\n    :param dicoms: list of dicoms\n    \"\"\"\n    # find most significant axis to use during sorting\n    # the original way of sorting (first x than y than z) does not work in certain border situations\n    # where for exampe the X will only slightly change causing the values to remain equal on multiple slices\n    # messing up the sorting completely)\n    dicom_input_sorted_x = sorted(dicoms, key=lambda x: (x.ImagePositionPatient[0]))\n    dicom_input_sorted_y = sorted(dicoms, key=lambda x: (x.ImagePositionPatient[1]))\n    dicom_input_sorted_z = sorted(dicoms, key=lambda x: (x.ImagePositionPatient[2]))\n    diff_x = abs(dicom_input_sorted_x[-1].ImagePositionPatient[0] - dicom_input_sorted_x[0].ImagePositionPatient[0])\n    diff_y = abs(dicom_input_sorted_y[-1].ImagePositionPatient[1] - dicom_input_sorted_y[0].ImagePositionPatient[1])\n    diff_z = abs(dicom_input_sorted_z[-1].ImagePositionPatient[2] - dicom_input_sorted_z[0].ImagePositionPatient[2])\n    if diff_x >= diff_y and diff_x >= diff_z:\n        return dicom_input_sorted_x\n    if diff_y >= diff_x and diff_y >= diff_z:\n        return dicom_input_sorted_y\n    if diff_z >= diff_x and diff_z >= diff_y:\n        return dicom_input_sorted_z"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that the distance between all slices is equal or very close to", "response": "def validate_slice_increment(dicoms):\n    \"\"\"\n    Validate that the distance between all slices is equal (or very close to)\n\n    :param dicoms: list of dicoms\n    \"\"\"\n    first_image_position = numpy.array(dicoms[0].ImagePositionPatient)\n    previous_image_position = numpy.array(dicoms[1].ImagePositionPatient)\n\n    increment = first_image_position - previous_image_position\n    for dicom_ in dicoms[2:]:\n        current_image_position = numpy.array(dicom_.ImagePositionPatient)\n        current_increment = previous_image_position - current_image_position\n        if not numpy.allclose(increment, current_increment, rtol=0.05, atol=0.1):\n            logger.warning('Slice increment not consistent through all slices')\n            logger.warning('---------------------------------------------------------')\n            logger.warning('%s %s' % (previous_image_position, increment))\n            logger.warning('%s %s' % (current_image_position, current_increment))\n            if 'InstanceNumber' in dicom_:\n                logger.warning('Instance Number: %s' % dicom_.InstanceNumber)\n            logger.warning('---------------------------------------------------------')\n            raise ConversionValidationError('SLICE_INCREMENT_INCONSISTENT')\n        previous_image_position = current_image_position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that the distance between all slices is equal or very close to", "response": "def is_slice_increment_inconsistent(dicoms):\n    \"\"\"\n    Validate that the distance between all slices is equal (or very close to)\n\n    :param dicoms: list of dicoms\n    \"\"\"\n    sliceincrement_inconsistent = False\n    first_image_position = numpy.array(dicoms[0].ImagePositionPatient)\n    previous_image_position = numpy.array(dicoms[1].ImagePositionPatient)\n\n    increment = first_image_position - previous_image_position\n    for dicom_ in dicoms[2:]:\n        current_image_position = numpy.array(dicom_.ImagePositionPatient)\n        current_increment = previous_image_position - current_image_position\n        if not numpy.allclose(increment, current_increment, rtol=0.05, atol=0.1):\n            sliceincrement_inconsistent = True\n            break\n    return sliceincrement_inconsistent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_orientation(dicoms):\n    first_image_orient1 = numpy.array(dicoms[0].ImageOrientationPatient)[0:3]\n    first_image_orient2 = numpy.array(dicoms[0].ImageOrientationPatient)[3:6]\n    for dicom_ in dicoms:\n        # Create affine matrix (http://nipy.sourceforge.net/nibabel/dicom/dicom_orientation.html#dicom-slice-affine)\n        image_orient1 = numpy.array(dicom_.ImageOrientationPatient)[0:3]\n        image_orient2 = numpy.array(dicom_.ImageOrientationPatient)[3:6]\n        if not numpy.allclose(image_orient1, first_image_orient1, rtol=0.001, atol=0.001) \\\n                or not numpy.allclose(image_orient2, first_image_orient2, rtol=0.001, atol=0.001):\n            logger.warning('Image orientations not consistent through all slices')\n            logger.warning('---------------------------------------------------------')\n            logger.warning('%s %s' % (image_orient1, first_image_orient1))\n            logger.warning('%s %s' % (image_orient2, first_image_orient2))\n            logger.warning('---------------------------------------------------------')\n            raise ConversionValidationError('IMAGE_ORIENTATION_INCONSISTENT')", "response": "Validate that all dicoms have the same orientation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_tr_te(nifti_image, repetition_time, echo_time):\n    # set the repetition time in pixdim\n    nifti_image.header.structarr['pixdim'][4] = repetition_time / 1000.0\n\n    # set tr and te in db_name field\n    nifti_image.header.structarr['db_name'] = '?TR:%.3f TE:%d' % (repetition_time, echo_time)\n\n    return nifti_image", "response": "Set the tr and te in the nifti image header"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dicom_to_nifti(dicom_input, output_file=None):\n    assert common.is_ge(dicom_input)\n\n    logger.info('Reading and sorting dicom files')\n    grouped_dicoms = _get_grouped_dicoms(dicom_input)\n\n    if _is_4d(grouped_dicoms):\n        logger.info('Found sequence type: 4D')\n        return _4d_to_nifti(grouped_dicoms, output_file)\n\n    logger.info('Assuming anatomical data')\n    return convert_generic.dicom_to_nifti(dicom_input, output_file)", "response": "This function is the main dicom to nifti conversion fuction for ge images. It will first convert the dicom images to nifti and then convert the nifti images to ge images."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a dicom series is a diffusion imaging dataset.", "response": "def _is_diffusion_imaging(grouped_dicoms):\n    \"\"\"\n    Use this function to detect if a dicom series is a ge dti dataset\n    NOTE: We already assume this is a 4D dataset\n    \"\"\"\n    # we already assume 4D images as input\n\n    # check if contains dti bval information\n    bval_tag = Tag(0x0043, 0x1039)  # put this there as this is a slow step and used a lot\n    found_bval = False\n    for header in list(itertools.chain.from_iterable(grouped_dicoms)):\n        if bval_tag in header and int(header[bval_tag].value[0]) != 0:\n            found_bval = True\n            break\n    if not found_bval:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _4d_to_nifti(grouped_dicoms, output_file):\n\n    # Create mosaic block\n    logger.info('Creating data block')\n    full_block = _get_full_block(grouped_dicoms)\n\n    logger.info('Creating affine')\n    # Create the nifti header info\n    affine, slice_increment = common.create_affine(grouped_dicoms[0])\n\n    logger.info('Creating nifti')\n    # Convert to nifti\n    nii_image = nibabel.Nifti1Image(full_block, affine)\n    common.set_tr_te(nii_image, float(grouped_dicoms[0][0].RepetitionTime),\n                     float(grouped_dicoms[0][0].EchoTime))\n    logger.info('Saving nifti to disk %s' % output_file)\n    # Save to disk\n    if output_file is not None:\n        nii_image.to_filename(output_file)\n\n    if _is_diffusion_imaging(grouped_dicoms):\n        bval_file = None\n        bvec_file = None\n        # Create the bval en bevec files\n        if output_file is not None:\n            base_path = os.path.dirname(output_file)\n            base_name = os.path.splitext(os.path.splitext(os.path.basename(output_file))[0])[0]\n\n            logger.info('Creating bval en bvec files')\n            bval_file = '%s/%s.bval' % (base_path, base_name)\n            bvec_file = '%s/%s.bvec' % (base_path, base_name)\n        bval, bvec = _create_bvals_bvecs(grouped_dicoms, bval_file, bvec_file)\n        return {'NII_FILE': output_file,\n                'BVAL_FILE': bval_file,\n                'BVEC_FILE': bvec_file,\n                'NII': nii_image,\n                'BVAL': bval,\n                'BVEC': bvec,\n                'MAX_SLICE_INCREMENT': slice_increment\n                }\n\n    return {'NII_FILE': output_file,\n            'NII': nii_image}", "response": "This function will convert ge 4d series to nifti"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a full datablock containing all timepoints", "response": "def _get_full_block(grouped_dicoms):\n    \"\"\"\n    Generate a full datablock containing all timepoints\n    \"\"\"\n    # For each slice / mosaic create a data volume block\n    data_blocks = []\n    for index in range(0, len(grouped_dicoms)):\n        logger.info('Creating block %s of %s' % (index + 1, len(grouped_dicoms)))\n        data_blocks.append(_timepoint_to_block(grouped_dicoms[index]))\n\n    # Add the data_blocks together to one 4d block\n    size_x = numpy.shape(data_blocks[0])[0]\n    size_y = numpy.shape(data_blocks[0])[1]\n    size_z = numpy.shape(data_blocks[0])[2]\n    size_t = len(data_blocks)\n    full_block = numpy.zeros((size_x, size_y, size_z, size_t), dtype=data_blocks[0].dtype)\n    for index in range(0, size_t):\n        if full_block[:, :, :, index].shape != data_blocks[index].shape:\n            logger.warning('Missing slices (slice count mismatch between timepoint %s and %s)' % (index - 1, index))\n            logger.warning('---------------------------------------------------------')\n            logger.warning(full_block[:, :, :, index].shape)\n            logger.warning(data_blocks[index].shape)\n            logger.warning('---------------------------------------------------------')\n            raise ConversionError(\"MISSING_DICOM_FILES\")\n        full_block[:, :, :, index] = data_blocks[index]\n\n    return full_block"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_grouped_dicoms(dicom_input):\n\n    # Order all dicom files by InstanceNumber\n    dicoms = sorted(dicom_input, key=lambda x: x.InstanceNumber)\n\n    # now group per stack\n    grouped_dicoms = [[]]  # list with first element a list\n    stack_index = 0\n\n    # loop over all sorted dicoms and sort them by stack\n    # for this we use the position and direction of the slices so we can detect a new stack easily\n    previous_position = None\n    previous_direction = None\n    for dicom_ in dicoms:\n        current_direction = None\n        # if the stack number decreases we moved to the next stack\n        if previous_position is not None:\n            current_direction = numpy.array(dicom_.ImagePositionPatient) - previous_position\n            current_direction = current_direction / numpy.linalg.norm(current_direction)\n        if current_direction is not None and \\\n                previous_direction is not None and \\\n                not numpy.allclose(current_direction, previous_direction, rtol=0.05, atol=0.05):\n            previous_position = numpy.array(dicom_.ImagePositionPatient)\n            previous_direction = None\n            stack_index += 1\n        else:\n            previous_position = numpy.array(dicom_.ImagePositionPatient)\n            previous_direction = current_direction\n\n        if stack_index >= len(grouped_dicoms):\n            grouped_dicoms.append([])\n        grouped_dicoms[stack_index].append(dicom_)\n\n    return grouped_dicoms", "response": "Get all dicoms in the dicom directory and sort them by InstanceNumber and validate them"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the bvals and bvecs from the dicom files and write them to a bval file", "response": "def _get_bvals_bvecs(grouped_dicoms):\n    \"\"\"\n    Write the bvals from the sorted dicom files to a bval file\n    \"\"\"\n    # loop over all timepoints and create a list with all bvals and bvecs\n    bvals = numpy.zeros([len(grouped_dicoms)], dtype=numpy.int32)\n    bvecs = numpy.zeros([len(grouped_dicoms), 3])\n\n    for group_index in range(0, len(grouped_dicoms)):\n        dicom_ = grouped_dicoms[group_index][0]\n        # 0019:10bb: Diffusion X\n        # 0019:10bc: Diffusion Y\n        # 0019:10bd: Diffusion Z\n        # 0043:1039: B-values (4 values, 1st value is actual B value)\n\n        # bval can be stored both in string as number format in dicom so implement both\n        # some workarounds needed for implicit transfer syntax to work\n        if isinstance(dicom_[Tag(0x0043, 0x1039)].value, string_types):  # this works for python2.7\n            original_bval = float(dicom_[Tag(0x0043, 0x1039)].value.split('\\\\')[0])\n        elif isinstance(dicom_[Tag(0x0043, 0x1039)].value, bytes):  # this works for python3.o\n            original_bval = float(dicom_[Tag(0x0043, 0x1039)].value.decode(\"utf-8\").split('\\\\')[0])\n        else:\n            original_bval = dicom_[Tag(0x0043, 0x1039)][0]\n        original_bvec = numpy.array([0, 0, 0], dtype=numpy.float)\n        original_bvec[0] = -float(dicom_[Tag(0x0019, 0x10bb)].value)  # invert based upon mricron output\n        original_bvec[1] = float(dicom_[Tag(0x0019, 0x10bc)].value)\n        original_bvec[2] = float(dicom_[Tag(0x0019, 0x10bd)].value)\n\n        # Add calculated B Value\n        if original_bval != 0:  # only normalize if there is a value\n            corrected_bval = original_bval * pow(numpy.linalg.norm(original_bvec), 2)\n            if numpy.linalg.norm(original_bvec) != 0:\n                normalized_bvec = original_bvec / numpy.linalg.norm(original_bvec)\n            else:\n                normalized_bvec = original_bvec\n        else:\n            corrected_bval = original_bval\n            normalized_bvec = original_bvec\n\n        bvals[group_index] = int(round(corrected_bval))  # we want the original numbers back as in the protocol\n        bvecs[group_index, :] = normalized_bvec\n\n    return bvals, bvecs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_bvals_bvecs(grouped_dicoms, bval_file, bvec_file):\n\n    # get the bvals and bvecs\n    bvals, bvecs = _get_bvals_bvecs(grouped_dicoms)\n\n    # save the found bvecs to the file\n    common.write_bval_file(bvals, bval_file)\n    common.write_bvec_file(bvecs, bvec_file)\n\n    return bvals, bvecs", "response": "Create the bvals and bvecs for a single dicom file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetect if a dicom series is a siemens 4d dataset", "response": "def _is_mosaic(dicom_input):\n    \"\"\"\n    Use this function to detect if a dicom series is a siemens 4d dataset\n    NOTE: Only the first slice will be checked so you can only provide an already sorted dicom directory\n    (containing one series)\n    \"\"\"\n    # for grouped dicoms\n    if type(dicom_input) is list and type(dicom_input[0]) is list:\n        header = dicom_input[0][0]\n    else:  # all the others\n        header = dicom_input[0]\n\n    # check if image type contains m and mosaic\n    if 'ImageType' not in header or 'MOSAIC' not in header.ImageType:\n        return False\n\n    if 'AcquisitionMatrix' not in header or header.AcquisitionMatrix is None:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mosaic_4d_to_nifti(dicom_input, output_file):\n    # Get the sorted mosaics\n    logger.info('Sorting dicom slices')\n    sorted_mosaics = _get_sorted_mosaics(dicom_input)\n    common.validate_orientation(sorted_mosaics)\n\n    # Create mosaic block\n    logger.info('Creating data block')\n    full_block = _mosaic_get_full_block(sorted_mosaics)\n\n    logger.info('Creating affine')\n    # Create the nifti header info\n    affine = _create_affine_siemens_mosaic(dicom_input)\n    logger.info('Creating nifti')\n    # Convert to nifti\n    nii_image = nibabel.Nifti1Image(full_block, affine)\n    common.set_tr_te(nii_image, float(sorted_mosaics[0].RepetitionTime), float(sorted_mosaics[0].EchoTime))\n    logger.info('Saving nifti to disk')\n    # Save to disk\n    if output_file is not None:\n        nii_image.to_filename(output_file)\n\n    if _is_diffusion_imaging(dicom_input[0]):\n        # Create the bval en bvec files\n        logger.info('Creating bval en bvec')\n        bval_file = None\n        bvec_file = None\n        if output_file is not None:\n            base_path = os.path.dirname(output_file)\n            base_name = os.path.splitext(os.path.splitext(os.path.basename(output_file))[0])[0]\n            logger.info('Saving bval en bvec files')\n            bval_file = '%s/%s.bval' % (base_path, base_name)\n            bvec_file = '%s/%s.bvec' % (base_path, base_name)\n        bvals = _create_bvals(sorted_mosaics, bval_file)\n        bvecs = _create_bvecs(sorted_mosaics, bvec_file)\n\n        return {'NII_FILE': output_file,\n                'BVAL_FILE': bval_file,\n                'BVEC_FILE': bvec_file,\n                'NII': nii_image,\n                'BVAL': bvals,\n                'BVEC': bvecs}\n\n    return {'NII_FILE': output_file,\n            'NII': nii_image}", "response": "This function will convert siemens 4d series to nifti"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _classic_get_grouped_dicoms(dicom_input):\n    # Loop overall files and build dict\n    # Order all dicom files by InstanceNumber\n    if [d for d in dicom_input if 'InstanceNumber' in d]:\n        dicoms = sorted(dicom_input, key=lambda x: x.InstanceNumber)\n    else:\n        dicoms = common.sort_dicoms(dicom_input)\n\n    # now group per stack\n    grouped_dicoms = []\n\n    # loop over all sorted dicoms\n    stack_position_tag = Tag(0x0020, 0x0012)  # in this case it is the acquisition number\n    for index in range(0, len(dicoms)):\n        dicom_ = dicoms[index]\n        if stack_position_tag not in dicom_:\n            stack_index = 0\n        else:\n            stack_index = dicom_[stack_position_tag].value - 1\n        while len(grouped_dicoms) <= stack_index:\n            grouped_dicoms.append([])\n        grouped_dicoms[stack_index].append(dicom_)\n\n    return grouped_dicoms", "response": "This function is used to group all the dicoms in a classic dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _classic_get_full_block(grouped_dicoms):\n    # For each slice / mosaic create a data volume block\n    data_blocks = []\n    for index in range(0, len(grouped_dicoms)):\n        logger.info('Creating block %s of %s' % (index + 1, len(grouped_dicoms)))\n        data_blocks.append(_classic_timepoint_to_block(grouped_dicoms[index]))\n\n    # Add the data_blocks together to one 4d block\n    size_x = numpy.shape(data_blocks[0])[0]\n    size_y = numpy.shape(data_blocks[0])[1]\n    size_z = numpy.shape(data_blocks[0])[2]\n    size_t = len(data_blocks)\n    full_block = numpy.zeros((size_x, size_y, size_z, size_t), dtype=data_blocks[0].dtype)\n    for index in range(0, size_t):\n        full_block[:, :, :, index] = data_blocks[index]\n\n    return full_block", "response": "Generate a full datablock containing all timepoints\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mosaic_get_full_block(sorted_mosaics):\n    # For each slice / mosaic create a data volume block\n    data_blocks = []\n    for index in range(0, len(sorted_mosaics)):\n        data_blocks.append(_mosaic_to_block(sorted_mosaics[index]))\n\n    # Add the data_blocks together to one 4d block\n    size_x = numpy.shape(data_blocks[0])[0]\n    size_y = numpy.shape(data_blocks[0])[1]\n    size_z = numpy.shape(data_blocks[0])[2]\n    size_t = len(data_blocks)\n    full_block = numpy.zeros((size_x, size_y, size_z, size_t), dtype=data_blocks[0].dtype)\n    for index in range(0, size_t):\n        full_block[:, :, :, index] = data_blocks[index]\n\n    # Apply the rescaling if needed\n    common.apply_scaling(full_block, sorted_mosaics[0])\n\n    return full_block", "response": "Generate a full datablock containing all timepoints"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches all mosaics in the dicom directory sort and validate them", "response": "def _get_sorted_mosaics(dicom_input):\n    \"\"\"\n    Search all mosaics in the dicom directory, sort and validate them\n    \"\"\"\n    # Order all dicom files by acquisition number\n    sorted_mosaics = sorted(dicom_input, key=lambda x: x.AcquisitionNumber)\n\n    for index in range(0, len(sorted_mosaics) - 1):\n        # Validate that there are no duplicate AcquisitionNumber\n        if sorted_mosaics[index].AcquisitionNumber >= sorted_mosaics[index + 1].AcquisitionNumber:\n            raise ConversionValidationError(\"INCONSISTENT_ACQUISITION_NUMBERS\")\n\n    return sorted_mosaics"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the asconv headers from the mosaic", "response": "def _get_asconv_headers(mosaic):\n    \"\"\"\n    Getter for the asconv headers (asci header info stored in the dicom)\n    \"\"\"\n    asconv_headers = re.findall(r'### ASCCONV BEGIN(.*)### ASCCONV END ###',\n                                mosaic[Tag(0x0029, 0x1020)].value.decode(encoding='ISO-8859-1'),\n                                re.DOTALL)[0]\n\n    return asconv_headers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_mosaic_type(mosaic):\n\n    ascconv_headers = _get_asconv_headers(mosaic)\n\n    try:\n        size = int(re.findall(r'sSliceArray\\.lSize\\s*=\\s*(\\d+)', ascconv_headers)[0])\n\n        # get the locations of the slices\n        slice_location = [None] * size\n        for index in range(size):\n            axial_result = re.findall(\n                r'sSliceArray\\.asSlice\\[%s\\]\\.sPosition\\.dTra\\s*=\\s*([-+]?[0-9]*\\.?[0-9]*)' % index,\n                ascconv_headers)\n            if len(axial_result) > 0:\n                axial = float(axial_result[0])\n            else:\n                axial = 0.0\n            slice_location[index] = axial\n\n        # should we invert (https://www.icts.uiowa.edu/confluence/plugins/viewsource/viewpagesrc.action?pageId=54756326)\n        invert = False\n        invert_result = re.findall(r'sSliceArray\\.ucImageNumbTra\\s*=\\s*([-+]?0?x?[0-9]+)', ascconv_headers)\n        if len(invert_result) > 0:\n            invert_value = int(invert_result[0], 16)\n            if invert_value >= 0:\n                invert = True\n\n        # return the correct slice types\n        if slice_location[0] <= slice_location[1]:\n            if not invert:\n                return MosaicType.ASCENDING\n            else:\n                return MosaicType.DESCENDING\n        else:\n            if not invert:\n                return MosaicType.DESCENDING\n            else:\n                return MosaicType.ASCENDING\n    except:\n        traceback.print_exc()\n        raise ConversionError(\"MOSAIC_TYPE_NOT_SUPPORTED\")", "response": "Get the mosaic type based on the slice position and size of the mosaic."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions to create the affine matrix for a siemens mosaic dataset", "response": "def _create_affine_siemens_mosaic(dicom_input):\n    \"\"\"\n    Function to create the affine matrix for a siemens mosaic dataset\n    This will work for siemens dti and 4d if in mosaic format\n    \"\"\"\n    # read dicom series with pds\n    dicom_header = dicom_input[0]\n\n    # Create affine matrix (http://nipy.sourceforge.net/nibabel/dicom/dicom_orientation.html#dicom-slice-affine)\n    image_orient1 = numpy.array(dicom_header.ImageOrientationPatient)[0:3]\n    image_orient2 = numpy.array(dicom_header.ImageOrientationPatient)[3:6]\n\n    normal = numpy.cross(image_orient1, image_orient2)\n\n    delta_r = float(dicom_header.PixelSpacing[0])\n    delta_c = float(dicom_header.PixelSpacing[1])\n\n    image_pos = dicom_header.ImagePositionPatient\n\n    delta_s = dicom_header.SpacingBetweenSlices\n    return numpy.array(\n        [[-image_orient1[0] * delta_c, -image_orient2[0] * delta_r, -delta_s * normal[0], -image_pos[0]],\n         [-image_orient1[1] * delta_c, -image_orient2[1] * delta_r, -delta_s * normal[1], -image_pos[1]],\n         [image_orient1[2] * delta_c, image_orient2[2] * delta_r, delta_s * normal[2], image_pos[2]],\n         [0, 0, 0, 1]])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate bvals from the dicom files.", "response": "def _create_bvals(sorted_dicoms, bval_file):\n    \"\"\"\n    Write the bvals from the sorted dicom files to a bval file\n    \"\"\"\n    bvals = []\n    for index in range(0, len(sorted_dicoms)):\n        if type(sorted_dicoms[0]) is list:\n            dicom_headers = sorted_dicoms[index][0]\n        else:\n            dicom_headers = sorted_dicoms[index]\n\n        bvals.append(common.get_is_value(dicom_headers[Tag(0x0019, 0x100c)]))\n    # save the found bvecs to the file\n    common.write_bval_file(bvals, bval_file)\n    return numpy.array(bvals)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the bvecs for the current image and set the bvec to the bvec file", "response": "def _create_bvecs(sorted_dicoms, bvec_file):\n    \"\"\"\n    Calculate the bvecs and write the to a bvec file\n    # inspired by dicom2nii from mricron\n    # see  http://users.fmrib.ox.ac.uk/~robson/internal/Dicom2Nifti111.m\n    \"\"\"\n    if type(sorted_dicoms[0]) is list:\n        dicom_headers = sorted_dicoms[0][0]\n    else:\n        dicom_headers = sorted_dicoms[0]\n\n    # get the patient orientation\n    image_orientation = dicom_headers.ImageOrientationPatient\n    read_vector = numpy.array([float(image_orientation[0]), float(image_orientation[1]), float(image_orientation[2])])\n    phase_vector = numpy.array([float(image_orientation[3]), float(image_orientation[4]), float(image_orientation[5])])\n    mosaic_vector = numpy.cross(read_vector, phase_vector)\n\n    # normalize the vectors\n    read_vector /= numpy.linalg.norm(read_vector)\n    phase_vector /= numpy.linalg.norm(phase_vector)\n    mosaic_vector /= numpy.linalg.norm(mosaic_vector)\n    # create an empty array for the new bvecs\n    bvecs = numpy.zeros([len(sorted_dicoms), 3])\n    # for each slice calculate the new bvec\n    for index in range(0, len(sorted_dicoms)):\n        if type(sorted_dicoms[0]) is list:\n            dicom_headers = sorted_dicoms[index][0]\n        else:\n            dicom_headers = sorted_dicoms[index]\n\n        # get the bval als this is needed in some checks\n        bval = common.get_is_value(dicom_headers[Tag(0x0019, 0x100c)])\n        # get the bvec if it exists in the headers\n        bvec = numpy.array([0, 0, 0])\n        if Tag(0x0019, 0x100e) in dicom_headers:\n            # in case of implicit VR the private field cannot be split into an array, we do this here\n            bvec = numpy.array(common.get_fd_array_value(dicom_headers[Tag(0x0019, 0x100e)], 3))\n        # if bval is 0 or the vector is 0 no projection is needed and the vector is 0,0,0\n        new_bvec = numpy.array([0, 0, 0])\n\n        if bval > 0 and not (bvec == [0, 0, 0]).all():\n            # project the bvec and invert the y direction\n            new_bvec = numpy.array(\n                [numpy.dot(bvec, read_vector), -numpy.dot(bvec, phase_vector), numpy.dot(bvec, mosaic_vector)])\n            # normalize the bvec\n            new_bvec /= numpy.linalg.norm(new_bvec)\n        bvecs[index, :] = new_bvec\n        # save the found bvecs to the file\n        common.write_bvec_file(bvecs, bvec_file)\n    return numpy.array(bvecs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resample_single_nifti(input_nifti):\n    # read the input image\n    input_image = nibabel.load(input_nifti)\n    output_image = resample_nifti_images([input_image])\n    output_image.to_filename(input_nifti)", "response": "Resample a gantry tilted image in place"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to generate the affine matrix for a dicom series", "response": "def _create_affine(x_axis, y_axis, z_axis, image_pos, voxel_sizes):\n    \"\"\"\n    Function to generate the affine matrix for a dicom series\n    This method was based on (http://nipy.org/nibabel/dicom/dicom_orientation.html)\n\n    :param sorted_dicoms: list with sorted dicom files\n    \"\"\"\n\n    # Create affine matrix (http://nipy.sourceforge.net/nibabel/dicom/dicom_orientation.html#dicom-slice-affine)\n\n    affine = numpy.array(\n        [[x_axis[0] * voxel_sizes[0], y_axis[0] * voxel_sizes[1], z_axis[0] * voxel_sizes[2], image_pos[0]],\n         [x_axis[1] * voxel_sizes[0], y_axis[1] * voxel_sizes[1], z_axis[1] * voxel_sizes[2], image_pos[1]],\n         [x_axis[2] * voxel_sizes[0], y_axis[2] * voxel_sizes[1], z_axis[2] * voxel_sizes[2], image_pos[2]],\n         [0, 0, 0, 1]])\n    return affine"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_directory(dicom_directory, output_folder, compression=True, reorient=True):\n    # sort dicom files by series uid\n    dicom_series = {}\n    for root, _, files in os.walk(dicom_directory):\n        for dicom_file in files:\n            file_path = os.path.join(root, dicom_file)\n            # noinspection PyBroadException\n            try:\n                if compressed_dicom.is_dicom_file(file_path):\n                    # read the dicom as fast as possible\n                    # (max length for SeriesInstanceUID is 64 so defer_size 100 should be ok)\n\n                    dicom_headers = compressed_dicom.read_file(file_path,\n                                                               defer_size=\"1 KB\",\n                                                               stop_before_pixels=False,\n                                                               force=dicom2nifti.settings.pydicom_read_force)\n                    if not _is_valid_imaging_dicom(dicom_headers):\n                        logger.info(\"Skipping: %s\" % file_path)\n                        continue\n                    logger.info(\"Organizing: %s\" % file_path)\n                    if dicom_headers.SeriesInstanceUID not in dicom_series:\n                        dicom_series[dicom_headers.SeriesInstanceUID] = []\n                    dicom_series[dicom_headers.SeriesInstanceUID].append(dicom_headers)\n            except:  # Explicitly capturing all errors here to be able to continue processing all the rest\n                logger.warning(\"Unable to read: %s\" % file_path)\n                traceback.print_exc()\n\n    # start converting one by one\n    for series_id, dicom_input in iteritems(dicom_series):\n        base_filename = \"\"\n        # noinspection PyBroadException\n        try:\n            # construct the filename for the nifti\n            base_filename = \"\"\n            if 'SeriesNumber' in dicom_input[0]:\n                base_filename = _remove_accents('%s' % dicom_input[0].SeriesNumber)\n                if 'SeriesDescription' in dicom_input[0]:\n                    base_filename = _remove_accents('%s_%s' % (base_filename,\n                                                               dicom_input[0].SeriesDescription))\n                elif 'SequenceName' in dicom_input[0]:\n                    base_filename = _remove_accents('%s_%s' % (base_filename,\n                                                               dicom_input[0].SequenceName))\n                elif 'ProtocolName' in dicom_input[0]:\n                    base_filename = _remove_accents('%s_%s' % (base_filename,\n                                                               dicom_input[0].ProtocolName))\n            else:\n                base_filename = _remove_accents(dicom_input[0].SeriesInstanceUID)\n            logger.info('--------------------------------------------')\n            logger.info('Start converting %s' % base_filename)\n            if compression:\n                nifti_file = os.path.join(output_folder, base_filename + '.nii.gz')\n            else:\n                nifti_file = os.path.join(output_folder, base_filename + '.nii')\n            convert_dicom.dicom_array_to_nifti(dicom_input, nifti_file, reorient)\n            gc.collect()\n        except:  # Explicitly capturing app exceptions here to be able to continue processing\n            logger.info(\"Unable to convert: %s\" % base_filename)\n            traceback.print_exc()", "response": "This function will convert all dicom files in a directory to a single nifti folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions will check if the given dicom header is a valid imaging dicom", "response": "def _is_valid_imaging_dicom(dicom_header):\n    \"\"\"\n    Function will do some basic checks to see if this is a valid imaging dicom\n    \"\"\"\n    # if it is philips and multiframe dicom then we assume it is ok\n    try:\n        if common.is_philips([dicom_header]):\n            if common.is_multiframe_dicom([dicom_header]):\n                return True\n\n        if \"SeriesInstanceUID\" not in dicom_header:\n            return False\n\n        if \"InstanceNumber\" not in dicom_header:\n            return False\n\n        if \"ImageOrientationPatient\" not in dicom_header or len(dicom_header.ImageOrientationPatient) < 6:\n            return False\n\n        if \"ImagePositionPatient\" not in dicom_header or len(dicom_header.ImagePositionPatient) < 3:\n            return False\n\n        # for all others if there is image position patient we assume it is ok\n        if Tag(0x0020, 0x0037) not in dicom_header:\n            return False\n\n        return True\n    except (KeyError, AttributeError):\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remove_accents(filename):\n    # noinspection PyBroadException\n    try:\n        filename = filename.replace(\" \", \"_\")\n        if isinstance(filename, type(six.u(''))):\n            unicode_filename = filename\n        else:\n            unicode_filename = six.u(filename)\n        cleaned_filename = unicodedata.normalize('NFKD', unicode_filename).encode('ASCII', 'ignore').decode('ASCII')\n\n        cleaned_filename = re.sub(r'[^\\w\\s-]', '', cleaned_filename.strip().lower())\n        cleaned_filename = re.sub(r'[-\\s]+', '-', cleaned_filename)\n\n        return cleaned_filename\n    except:\n        traceback.print_exc()\n        return filename", "response": "Function that will try to remove accents from a ascii string to be used in a filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning that will try to remove accents from a ascii string to be used in a filename.", "response": "def _remove_accents_(filename):\n    \"\"\"\n    Function that will try to remove accents from a unicode string to be used in a filename.\n    input filename should be either an ascii or unicode string\n    \"\"\"\n    if isinstance(filename, type(six.u(''))):\n        unicode_filename = filename\n    else:\n        unicode_filename = six.u(filename)\n    valid_characters = bytes(b'-_.() 1234567890abcdefghijklmnopqrstuvwxyz')\n    cleaned_filename = unicodedata.normalize('NFKD', unicode_filename).encode('ASCII', 'ignore')\n\n    new_filename = six.u('')\n\n    for char_int in bytes(cleaned_filename):\n        char_byte = bytes([char_int])\n        if char_byte in valid_characters:\n            new_filename += char_byte.decode()\n\n    return new_filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the border style for the specified Menu.", "response": "def set_border_style(self, border_style):\n        \"\"\"\n        Set the border style using the specified MenuBorderStyle instance.\n        :param border_style: the instance of MenuBorderStyle to use for border style formatting.\n        \"\"\"\n        if not isinstance(border_style, MenuBorderStyle):\n            raise TypeError('border_style must be type MenuBorderStyle')\n        self.__header.style.border_style = border_style\n        self.__prologue.style.border_style = border_style\n        self.__items_section.style.border_style = border_style\n        self.__epilogue.style.border_style = border_style\n        self.__footer.style.border_style = border_style\n        self.__prompt.style.border_style = border_style\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_border_style_type(self, border_style_type):\n        style = self.__border_style_factory.create_border(border_style_type)\n        self.set_border_style(style)\n        return self", "response": "Sets the border style type for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the bottom margin of the footer.", "response": "def set_bottom_margin(self, bottom_margin):\n        \"\"\"\n        Set the bottom margin of the menu. This will determine the number of console lines appear between the\n        bottom of the menu border and the menu input prompt.\n        :param bottom_margin: an integer value\n        \"\"\"\n        self.__footer.style.margins.bottom = bottom_margin\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_left_margin(self, left_margin):\n        self.__header.style.margins.left = left_margin\n        self.__prologue.style.margins.left = left_margin\n        self.__items_section.style.margins.left = left_margin\n        self.__epilogue.style.margins.left = left_margin\n        self.__footer.style.margins.left = left_margin\n        self.__prompt.style.margins.left = left_margin\n        return self", "response": "Sets the left margin of the menu."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_right_margin(self, right_margin):\n        self.__header.style.margins.right = right_margin\n        self.__prologue.style.margins.right = right_margin\n        self.__items_section.style.margins.right = right_margin\n        self.__epilogue.style.margins.right = right_margin\n        self.__footer.style.margins.right = right_margin\n        self.__prompt.style.margins.right = right_margin\n        return self", "response": "Sets the right margin of the menu."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_top_margin(self, top_margin):\n        self.__header.style.margins.top = top_margin\n        return self", "response": "Sets the top margin of the menu."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_data(self):\n        self.__header.title = None\n        self.__header.subtitle = None\n        self.__prologue.text = None\n        self.__epilogue.text = None\n        self.__items_section.items = None", "response": "Clear menu data from previous generation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format(self, title=None, subtitle=None, prologue_text=None, epilogue_text=None, items=None):\n        self.clear_data()\n        content = ''\n        # Header Section\n        if title is not None:\n            self.__header.title = title\n        if subtitle is not None:\n            self.__header.subtitle = subtitle\n        sections = [self.__header]\n        # Prologue Section\n        if prologue_text is not None:\n            self.__prologue.text = prologue_text\n            sections.append(self.__prologue)\n        # Items Section\n        if items is not None:\n            self.__items_section.items = items\n            sections.append(self.__items_section)\n        # Epilogue Section\n        if epilogue_text is not None:\n            self.__epilogue.text = epilogue_text\n            sections.append(self.__epilogue)\n        sections.append(self.__footer)\n        sections.append(self.__prompt)\n        for sect in sections:\n            content += \"\\n\".join(sect.generate())\n            # Don't add newline to prompt so input is on same line as prompt\n            if not isinstance(sect, MenuPrompt):\n                content += \"\\n\"\n        return content", "response": "Format the menu and return as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(self, input_string):\n        parsed_url = urlparse(url=input_string)\n        return bool(parsed_url.scheme and parsed_url.netloc)", "response": "Validate url\n\n        :return: True if match / False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_border_width(self):\n        return self.max_dimension.width - self.margins.left - self.margins.right - 1", "response": "Calculates the width of the menu border."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_content_width(self):\n        return self.calculate_border_width() - self.padding.left - self.padding.right - 2", "response": "Calculate the width of the inner content of the menu."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the complete inner horizontal border section including the left and right border verticals.", "response": "def inner_horizontal_border(self):\n        \"\"\"\n        The complete inner horizontal border section, including the left and right border verticals.\n\n        Returns:\n            str: The complete inner horizontal border.\n        \"\"\"\n        return u\"{lm}{lv}{hz}{rv}\".format(lm=' ' * self.margins.left,\n                                          lv=self.border_style.outer_vertical_inner_right,\n                                          rv=self.border_style.outer_vertical_inner_left,\n                                          hz=self.inner_horizontals())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the complete outer bottom horizontal border section including left and right margins.", "response": "def outer_horizontal_border_bottom(self):\n        \"\"\"\n        The complete outer bottom horizontal border section, including left and right margins.\n\n        Returns:\n            str: The bottom menu border.\n        \"\"\"\n        return u\"{lm}{lv}{hz}{rv}\".format(lm=' ' * self.margins.left,\n                                          lv=self.border_style.bottom_left_corner,\n                                          rv=self.border_style.bottom_right_corner,\n                                          hz=self.outer_horizontals())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the complete outer top horizontal border section including left and right margins.", "response": "def outer_horizontal_border_top(self):\n        \"\"\"\n        The complete outer top horizontal border section, including left and right margins.\n\n        Returns:\n            str: The top menu border.\n        \"\"\"\n        return u\"{lm}{lv}{hz}{rv}\".format(lm=' ' * self.margins.left,\n                                          lv=self.border_style.top_left_corner,\n                                          rv=self.border_style.top_right_corner,\n                                          hz=self.outer_horizontals())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef row(self, content='', align='left'):\n        return u\"{lm}{vert}{cont}{vert}\".format(lm=' ' * self.margins.left,\n                                                vert=self.border_style.outer_vertical,\n                                                cont=self._format_content(content, align))", "response": "A row of the menu which comprises the left and right verticals plus the given content."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show_item_bottom_border(self, item_text, flag):\n        if flag:\n            self.__bottom_border_dict[item_text] = True\n        else:\n            self.__bottom_border_dict.pop(item_text, None)", "response": "Sets a flag that will show a bottom border for an item with the specified text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show_item_top_border(self, item_text, flag):\n        if flag:\n            self.__top_border_dict[item_text] = True\n        else:\n            self.__top_border_dict.pop(item_text, None)", "response": "Sets a flag that will show a top border for an item with the specified text."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new MenuBorderStyle instance based on the given border style type.", "response": "def create_border(self, border_style_type):\n        \"\"\"\n        Create a new MenuBorderStyle instance based on the given border style type.\n\n        Args:\n            border_style_type (int):  an integer value from :obj:`MenuBorderStyleType`.\n\n        Returns:\n            :obj:`MenuBorderStyle`: a new MenuBorderStyle instance of the specified style.\n\n        \"\"\"\n        if border_style_type == MenuBorderStyleType.ASCII_BORDER:\n            return self.create_ascii_border()\n        elif border_style_type == MenuBorderStyleType.LIGHT_BORDER:\n            return self.create_light_border()\n        elif border_style_type == MenuBorderStyleType.HEAVY_BORDER:\n            return self.create_heavy_border()\n        elif border_style_type == MenuBorderStyleType.DOUBLE_LINE_BORDER:\n            return self.create_doubleline_border()\n        elif border_style_type == MenuBorderStyleType.HEAVY_OUTER_LIGHT_INNER_BORDER:\n            return self.create_heavy_outer_light_inner_border()\n        elif border_style_type == MenuBorderStyleType.DOUBLE_LINE_OUTER_LIGHT_INNER_BORDER:\n            return self.create_doubleline_outer_light_inner_border()\n        else:\n            # Use ASCII if we don't recognize the type\n            self.logger.info('Unrecognized border style type: {}. Defaulting to ASCII.'.format(border_style_type))\n            return self.create_ascii_border()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_win_python35_or_earlier():\n        return sys.platform.startswith(\"win\") and sys.version_info.major < 3 or (\n                    sys.version_info.major == 3 and sys.version_info.minor < 6)", "response": "Returns True if the current platform is Windows and Python version 3. 5 or earlier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_menu(self, menu):\n        self.menu = menu\n        self.submenu.parent = menu", "response": "Sets the menu of this item."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean_up(self):\n        self.submenu.join()\n        self.menu.clear_screen()\n        self.menu.resume()", "response": "This method is used to clean up the menu screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_prompt(prompt=None, default=None, enable_quit=False, quit_string='q',\n                      quit_message='(enter q to Quit)'):\n        \"\"\"\n        Format the prompt.\n        :param prompt: the prompt message.\n        :param default:  the default answer if user does not provide a response.\n        :param enable_quit: specifies whether the user can cancel out of the input prompt.\n        :param quit_string: the string whcih the user must input in order to quit.\n        :param quit_message: the message to explain how to quit.\n        :return: the formatted prompt string.\n        \"\"\"\n        if prompt is None:\n            return None\n        prompt = prompt.rstrip()\n        prompt = prompt.rstrip(':')\n        if enable_quit:\n            prompt = \"{0} {1}\".format(prompt, quit_message)\n        if default:\n            prompt = \"{0} [{1}]\".format(prompt, default)\n        return \"{0}: \".format(prompt)", "response": "Format a prompt message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef confirm_answer(self, answer, message=None):\n        if message is None:\n            message = \"\\nYou entered {0}.  Is this correct?\".format(answer)\n        return self.prompt_for_yes_or_no(message)", "response": "Prompts the user to confirm a question with a yes or no prompt."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a console prompt with the given message or defaults to Enter to continue", "response": "def enter_to_continue(self, message=None):\n        \"\"\"\n        Creates a console prompt with the given message, or defaults to 'Press [Enter] to continue' if no message\n        is provided.\n        :param message:\n        \"\"\"\n        if message:\n            message = message.rstrip() + ' '\n        else:\n            message = 'Press [Enter] to continue '\n        self.__screen.input(message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprompting the user for input and return a tuple of the input string and validation result.", "response": "def input(self, prompt=None, default=None, validators=None, enable_quit=False, quit_string='q',\n              quit_message='(enter q to Quit)'):\n        \"\"\"\n        Prompt the user for input.\n        :param prompt: the message to prompt the user.\n        :param default: the default value to suggest as an answer.\n        :param validators: list of validators to perform input validation.\n        :param enable_quit: specifies whether the user can cancel out of the input prompt.\n        :param quit_string: the string whcih the user must input in order to quit.\n        :param quit_message: the message to explain how to quit.\n        :return: an InputResult tuple.\n        \"\"\"\n\n        prompt = self.__prompt_formatter.format_prompt(prompt=prompt, default=default, enable_quit=enable_quit,\n                                                       quit_string=quit_string, quit_message=quit_message)\n\n        input_string = self.__screen.input(prompt=prompt)\n\n        if enable_quit and quit_string == input_string:\n            raise UserQuit\n\n        if default is not None and input_string.strip() == '':\n            input_string = default\n\n        validation_result = self.validate_input(input_string, validators)\n\n        return InputResult(input_string=input_string, validation_result=validation_result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprompt the user for a password.", "response": "def input_password(self, message=None):\n        \"\"\"\n        Prompt the user for a password. This is equivalent to the input() method, but does not echo inputted\n        characters to the screen.\n        :param message: the prompt message.\n        \"\"\"\n        message = self.__prompt_formatter.format_prompt(message)\n        try:\n            if message:\n                return getpass.getpass(message)\n            else:\n                return getpass.getpass()\n        except BaseException:\n            self.__screen.println('Warning: Unable to mask input; characters will be echoed to console')\n            return self.input(message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprompt the user for a response that must be one of the given options.", "response": "def prompt_for_bilateral_choice(self, prompt, option1, option2):\n        \"\"\"\n        Prompt the user for a response that must be one of the two supplied choices.\n        NOTE: The user input verification is case-insensitive, but will return the original case provided\n        by the given options.\n        \"\"\"\n        if prompt is None:\n            prompt = ''\n        prompt = prompt.rstrip() + ' (' + option1 + '/' + option2 + ')'\n        while True:\n            user_input = self.__screen.input(prompt)\n            if str(user_input).lower() == option1.lower():\n                return option1\n            elif str(user_input).lower() == option2.lower():\n                return option2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prompt_for_trilateral_choice(self, prompt, option1, option2, option3):\n        if prompt is None:\n            prompt = ''\n        prompt = prompt.rstrip() + ' (' + option1 + '/' + option2 + '/' + option3 + ')'\n        while True:\n            user_input = self.__screen.input(prompt)\n            if str(user_input).lower() == option1.lower():\n                return option1\n            elif str(user_input).lower() == option2.lower():\n                return option2\n            elif str(user_input).lower() == option3.lower():\n                return option3", "response": "Prompt the user for a trilateral choice."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying a numbered vertical list of choices from the provided list of strings.", "response": "def prompt_for_numbered_choice(self, choices, title=None, prompt=\">\"):\n        \"\"\"\n        Displays a numbered vertical list of choices from the provided list of strings.\n        :param choices: list of choices to display\n        :param title: optional title to display above the numbered list\n        :param prompt: prompt string. Default is \">\"\n        :return: an int representing the selected index.\n        \"\"\"\n        if choices is None or len(choices) < 1:\n            raise Exception('choices list must contain at least one element.')\n\n        while True:\n            self.clear()\n\n            if title:\n                self.screen.println(title + \"\\n\")\n\n            for i in range(0, len(choices)):\n                print('   {:<4}{choice}'.format(str(i + 1) + ') ', choice=choices[i]))\n\n            answer = self.screen.input('\\n{} '.format(prompt))\n\n            try:\n                index = int(answer) - 1\n                if 0 <= index < len(choices):\n                    return index\n            except Exception as e:\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the input string against the specified list of validators.", "response": "def validate_input(self, input_string, validators):\n        \"\"\"\n        Validate the given input string against the specified list of validators.\n        :param input_string: the input string to verify.\n        :param validators: the list of validators.\n        :raises InvalidValidator if the list of validators does not provide a valid InputValidator class.\n        :return: a boolean representing the validation result. True if the input string is valid; False otherwise.\n        \"\"\"\n        validation_result = True\n\n        if isinstance(validators, BaseValidator):\n            validators = [validators]\n        elif validators is None:\n            validators = []\n\n        if isinstance(validators, list):\n            validation_results = []\n            for validator in validators:\n                if isinstance(validator, BaseValidator):\n                    validation_results.append(validator.validate(input_string=input_string))\n                else:\n                    raise InvalidValidator(\"Validator {} is not a valid validator\".format(validator))\n\n            validation_result = all(validation_results)\n        else:\n            raise InvalidValidator(\"Validator {} is not a valid validator\".format(validators))\n\n        return validation_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(self, input_string):\n        validation_result = False\n        try:\n            validation_result = bool(match(pattern=self.pattern, string=input_string))\n        except TypeError as e:\n            self.log.error(\n                'Exception while validating Regex, pattern={}, input_string={} - exception: {}'.format(self.pattern,\n                                                                                                       input_string,\n                                                                                                       e))\n        return validation_result", "response": "Validate input_string against a regex pattern\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend an item to the end of the menu before the exit item.", "response": "def append_item(self, item):\n        \"\"\"\n        Add an item to the end of the menu before the exit item.\n\n        Note that Multi-Select Menus will not allow a SubmenuItem to be added, as multi-select menus\n        are expected to be used only for executing multiple actions.\n\n        Args:\n            item (:obj:`MenuItem`): The item to be added\n\n        Raises:\n            TypeError: If the specified MenuIem is a SubmenuItem.\n        \"\"\"\n        if isinstance(item, SubmenuItem):\n            raise TypeError(\"SubmenuItems cannot be added to a MultiSelectMenu\")\n        super(MultiSelectMenu, self).append_item(item)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_user_input(self):\n        user_input = self.screen.input()\n\n        try:\n            indexes = self.__parse_range_list(user_input)\n            # Subtract 1 from each number for its actual index number\n            indexes[:] = [x - 1 for x in indexes if 0 < x < len(self.items) + 1]\n            for index in indexes:\n                self.current_option = index\n                self.select()\n        except Exception as e:\n            return", "response": "This method is used to process the user input of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef selected_item(self):\n        if self.items and self.selected_option != -1:\n            return self.items[self.current_option]\n        else:\n            return None", "response": "Return the item that the user currently selected."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an item to the end of the menu before the exit item.", "response": "def append_item(self, item):\n        \"\"\"\n        Add an item to the end of the menu before the exit item.\n\n        Args:\n            item (MenuItem): The item to be added.\n\n        \"\"\"\n        did_remove = self.remove_exit()\n        item.menu = self\n        self.items.append(item)\n        if did_remove:\n            self.add_exit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the specified item from the menu.", "response": "def remove_item(self, item):\n        \"\"\"\n        Remove the specified item from the menu.\n\n        Args:\n            item (MenuItem): the item to be removed.\n\n        Returns:\n            bool: True if the item was removed; False otherwise.\n        \"\"\"\n        for idx, _item in enumerate(self.items):\n            if item == _item:\n                del self.items[idx]\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the exit item if necessary. Used to make sure we only remove the exit item if necessary.", "response": "def remove_exit(self):\n        \"\"\"\n        Remove the exit item if necessary. Used to make sure we only remove the exit item, not something else.\n\n        Returns:\n            bool: True if item needed to be removed, False otherwise.\n        \"\"\"\n        if self.items:\n            if self.items[-1] is self.exit_item:\n                del self.items[-1]\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the menu in a new thread and allow the user to interact with it.", "response": "def start(self, show_exit_option=None):\n        \"\"\"\n        Start the menu in a new thread and allow the user to interact with it.\n        The thread is a daemon, so :meth:`join()<consolemenu.ConsoleMenu.join>` should be called if there's a\n        possibility that the main thread will exit before the menu is done\n\n        Args:\n            show_exit_option (bool): Specify whether the exit item should be shown, defaults to the value\n                set in the constructor\n\n        \"\"\"\n        self.previous_active_menu = ConsoleMenu.currently_active_menu\n        ConsoleMenu.currently_active_menu = None\n\n        self.should_exit = False\n\n        if show_exit_option is None:\n            show_exit_option = self.show_exit_option\n\n        if show_exit_option:\n            self.add_exit()\n        else:\n            self.remove_exit()\n\n        try:\n            self._main_thread = threading.Thread(target=self._wrap_start, daemon=True)\n        except TypeError:\n            self._main_thread = threading.Thread(target=self._wrap_start)\n            self._main_thread.daemon = True\n\n        self._main_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef draw(self):\n        self.screen.printf(self.formatter.format(title=self.title, subtitle=self.subtitle, items=self.items,\n                                                 prologue_text=self.prologue_text, epilogue_text=self.epilogue_text))", "response": "Draw the menu for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_user_input(self):\n        user_input = self.get_input()\n\n        try:\n            num = int(user_input)\n        except Exception:\n            return\n        if 0 < num < len(self.items) + 1:\n            self.current_option = num - 1\n            self.select()\n\n        return user_input", "response": "Process the user input and decides what to do with it\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef go_down(self):\n        if self.current_option < len(self.items) - 1:\n            self.current_option += 1\n        else:\n            self.current_option = 0\n        self.draw()", "response": "Go down one item and draw it if necessary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngoing up one item and draw it if necessary", "response": "def go_up(self):\n        \"\"\"\n        Go up one, wrap to end if necessary\n        \"\"\"\n        if self.current_option > 0:\n            self.current_option += -1\n        else:\n            self.current_option = len(self.items) - 1\n        self.draw()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show(self, index):\n        if self.menu and self.menu.parent:\n            self.text = \"Return to %s\" % self.menu.parent.title\n            # Check if menu title ends with menu. (Some menus will include Menu in the name).\n            if not self.text.strip().lower().endswith(\"menu\"):\n                self.text += \" menu\"\n        else:\n            self.text = \"Exit\"\n        return super(ExitItem, self).show(index)", "response": "This method overrides this method to show the exit item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_selection(cls, strings, title=\"Select an option\", subtitle=None, exit_option=True, _menu=None):\n        menu = cls(strings, title, subtitle, exit_option)\n        if _menu is not None:\n            _menu.append(menu)\n        menu.show()\n        menu.join()\n        return menu.selected_option", "response": "This method creates a new menu object and returns the index of the selected option in the menu."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dataframe of the long format data for the current language.", "response": "def get_dataframe_from_data(data):\n    \"\"\"\n    Parameters\n    ----------\n    data : string or pandas dataframe.\n        If string, data should be an absolute or relative path to a CSV file\n        containing the long format data for this choice model. Note long format\n        has one row per available alternative for each observation. If pandas\n        dataframe, the dataframe should be the long format data for the choice\n        model.\n\n    Returns\n    -------\n    dataframe : pandas dataframe of the long format data for the choice model.\n    \"\"\"\n    if isinstance(data, str):\n        if data.endswith(\".csv\"):\n            dataframe = pd.read_csv(data)\n        else:\n            msg_1 = \"data = {} is of unknown file type.\"\n            msg_2 = \" Please pass path to csv.\"\n            raise ValueError(msg_1.format(data) + msg_2)\n    elif isinstance(data, pd.DataFrame):\n        dataframe = data\n    else:\n        msg_1 = \"type(data) = {} is an invalid type.\"\n        msg_2 = \" Please pass pandas dataframe or path to csv.\"\n        raise TypeError(msg_1.format(type(data)) + msg_2)\n\n    return dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_object_is_ordered_dict(item, title):\n    assert isinstance(title, str)\n\n    if not isinstance(item, OrderedDict):\n        msg = \"{} must be an OrderedDict. {} passed instead.\"\n        raise TypeError(msg.format(title, type(item)))\n\n    return None", "response": "Checks that the item is an OrderedDict. Raises ValueError."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the item is a string. Raises ValueError.", "response": "def ensure_object_is_string(item, title):\n    \"\"\"\n    Checks that the item is a string. If not, raises ValueError.\n    \"\"\"\n    assert isinstance(title, str)\n\n    if not isinstance(item, str):\n        msg = \"{} must be a string. {} passed instead.\"\n        raise TypeError(msg.format(title, type(item)))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that a given object is a dense numpy array. Raises a helpful TypeError.", "response": "def ensure_object_is_ndarray(item, title):\n    \"\"\"\n    Ensures that a given mapping matrix is a dense numpy array. Raises a\n    helpful TypeError if otherwise.\n    \"\"\"\n    assert isinstance(title, str)\n\n    if not isinstance(item, np.ndarray):\n        msg = \"{} must be a np.ndarray. {} passed instead.\"\n        raise TypeError(msg.format(title, type(item)))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether each column in columns is in dataframe. Raises ValueError if any of the columns are not in dataframe.", "response": "def ensure_columns_are_in_dataframe(columns,\n                                    dataframe,\n                                    col_title='',\n                                    data_title='data'):\n    \"\"\"\n    Checks whether each column in `columns` is in `dataframe`. Raises\n    ValueError if any of the columns are not in the dataframe.\n\n    Parameters\n    ----------\n    columns : list of strings.\n        Each string should represent a column heading in dataframe.\n    dataframe : pandas DataFrame.\n        Dataframe containing the data for the choice model to be estimated.\n    col_title : str, optional.\n        Denotes the title of the columns that were passed to the function.\n    data_title : str, optional.\n        Denotes the title of the dataframe that is being checked to see whether\n        it contains the passed columns. Default == 'data'\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    # Make sure columns is an iterable\n    assert isinstance(columns, Iterable)\n    # Make sure dataframe is a pandas dataframe\n    assert isinstance(dataframe, pd.DataFrame)\n    # Make sure title is a string\n    assert isinstance(col_title, str)\n    assert isinstance(data_title, str)\n\n    problem_cols = [col for col in columns if col not in dataframe.columns]\n    if problem_cols != []:\n        if col_title == '':\n            msg = \"{} not in {}.columns\"\n            final_msg = msg.format(problem_cols, data_title)\n        else:\n            msg = \"The following columns in {} are not in {}.columns: {}\"\n            final_msg = msg.format(col_title, data_title, problem_cols)\n\n        raise ValueError(final_msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that long_form is a pandas dataframe and that specification_dict is an OrderedDict raising a ValueError otherwise.", "response": "def check_argument_type(long_form, specification_dict):\n    \"\"\"\n    Ensures that long_form is a pandas dataframe and that specification_dict\n    is an OrderedDict, raising a ValueError otherwise.\n\n    Parameters\n    ----------\n    long_form : pandas dataframe.\n        Contains one row for each available alternative, for each observation.\n    specification_dict : OrderedDict.\n        Keys are a proper subset of the columns in `long_form_df`. Values are\n        either a list or a single string, `\"all_diff\"` or `\"all_same\"`. If a\n        list, the elements should be:\n\n            - single objects that are within the alternative ID column of\n              `long_form_df`\n            - lists of objects that are within the alternative ID column of\n              `long_form_df`. For each single object in the list, a unique\n              column will be created (i.e. there will be a unique coefficient\n              for that variable in the corresponding utility equation of the\n              corresponding alternative). For lists within the\n              `specification_dict` values, a single column will be created for\n              all the alternatives within iterable (i.e. there will be one\n              common coefficient for the variables in the iterable).\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if not isinstance(long_form, pd.DataFrame):\n        msg = \"long_form should be a pandas dataframe. It is a {}\"\n        raise TypeError(msg.format(type(long_form)))\n\n    ensure_object_is_ordered_dict(specification_dict, \"specification_dict\")\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ensure_alt_id_in_long_form(alt_id_col, long_form):\n    if alt_id_col not in long_form.columns:\n        msg = \"alt_id_col == {} is not a column in long_form.\"\n        raise ValueError(msg.format(alt_id_col))\n\n    return None", "response": "Ensures that the alternative ID in the given column is in the given long_form. Raises a ValueError if not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether each column in specification is in dataframe. Raises ValueError if any of the columns are not in dataframe.", "response": "def ensure_specification_cols_are_in_dataframe(specification, dataframe):\n    \"\"\"\n    Checks whether each column in `specification` is in `dataframe`. Raises\n    ValueError if any of the columns are not in the dataframe.\n\n    Parameters\n    ----------\n    specification : OrderedDict.\n        Keys are a proper subset of the columns in `data`. Values are either a\n        list or a single string, \"all_diff\" or \"all_same\". If a list, the\n        elements should be:\n            - single objects that are in the alternative ID column of `data`\n            - lists of objects that are within the alternative ID column of\n              `data`. For each single object in the list, a unique column will\n              be created (i.e. there will be a unique coefficient for that\n              variable in the corresponding utility equation of the\n              corresponding alternative). For lists within the\n              `specification` values, a single column will be created for all\n              the alternatives within the iterable (i.e. there will be one\n              common coefficient for the variables in the iterable).\n    dataframe : pandas DataFrame.\n        Dataframe containing the data for the choice model to be estimated.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    # Make sure specification is an OrderedDict\n    try:\n        assert isinstance(specification, OrderedDict)\n    except AssertionError:\n        raise TypeError(\"`specification` must be an OrderedDict.\")\n    # Make sure dataframe is a pandas dataframe\n    assert isinstance(dataframe, pd.DataFrame)\n\n    problem_cols = []\n    dataframe_cols = dataframe.columns\n    for key in specification:\n        if key not in dataframe_cols:\n            problem_cols.append(key)\n    if problem_cols != []:\n        msg = \"The following keys in the specification are not in 'data':\\n{}\"\n        raise ValueError(msg.format(problem_cols))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the values of specification_dict have the correct type and values of the set of possible alternatives.", "response": "def check_type_and_values_of_specification_dict(specification_dict,\n                                                unique_alternatives):\n    \"\"\"\n    Verifies that the values of specification_dict have the correct type, have\n    the correct structure, and have valid values (i.e. are actually in the set\n    of possible alternatives). Will raise various errors if / when appropriate.\n\n    Parameters\n    ----------\n    specification_dict : OrderedDict.\n        Keys are a proper subset of the columns in `long_form_df`. Values are\n        either a list or a single string, `\"all_diff\"` or `\"all_same\"`. If a\n        list, the elements should be:\n\n            - single objects that are within the alternative ID column of\n              `long_form_df`\n            - lists of objects that are within the alternative ID column of\n              `long_form_df`. For each single object in the list, a unique\n              column will be created (i.e. there will be a unique coefficient\n              for that variable in the corresponding utility equation of the\n              corresponding alternative). For lists within the\n              `specification_dict` values, a single column will be created for\n              all the alternatives within iterable (i.e. there will be one\n              common coefficient for the variables in the iterable).\n    unique_alternatives : 1D ndarray.\n        Should contain the possible alternative id's for this dataset.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    for key in specification_dict:\n        specification = specification_dict[key]\n        if isinstance(specification, str):\n            if specification not in [\"all_same\", \"all_diff\"]:\n                msg = \"specification_dict[{}] not in ['all_same', 'all_diff']\"\n                raise ValueError(msg.format(key))\n\n        elif isinstance(specification, list):\n            # Imagine that the specification is [[1, 2], 3]\n            # group would be [1, 2]\n            # group_item would be 1 or 2. group_item should never be a list.\n            for group in specification:\n                group_is_list = isinstance(group, list)\n                if group_is_list:\n                    for group_item in group:\n                        if isinstance(group_item, list):\n                            msg = \"Wrong structure for specification_dict[{}]\"\n                            msg_2 = \" Values can be a list of lists of ints,\"\n                            msg_3 = \" not lists of lists of lists of ints.\"\n                            total_msg = msg.format(key) + msg_2 + msg_3\n                            raise ValueError(total_msg)\n\n                        elif group_item not in unique_alternatives:\n                            msg_1 = \"{} in {} in specification_dict[{}]\"\n                            msg_2 = \" is not in long_format[alt_id_col]\"\n                            total_msg = (msg_1.format(group_item, group, key) +\n                                         msg_2)\n                            raise ValueError(total_msg)\n                else:\n                    if group not in unique_alternatives:\n                        msg_1 = \"{} in specification_dict[{}]\"\n                        msg_2 = \" is not in long_format[alt_id_col]\"\n                        raise ValueError(msg_1.format(group, key) + msg_2)\n\n        else:\n            msg = \"specification_dict[{}] must be 'all_same', 'all_diff', or\"\n            msg_2 = \" a list.\"\n            raise TypeError(msg.format(key) + msg_2)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the validity of the keys and values of the names dictionary.", "response": "def check_keys_and_values_of_name_dictionary(names,\n                                             specification_dict,\n                                             num_alts):\n    \"\"\"\n    Check the validity of the keys and values in the names dictionary.\n\n    Parameters\n    ----------\n    names : OrderedDict, optional.\n        Should have the same keys as `specification_dict`. For each key:\n\n            - if the corresponding value in `specification_dict` is \"all_same\",\n              then there should be a single string as the value in names.\n            - if the corresponding value in `specification_dict` is \"all_diff\",\n              then there should be a list of strings as the value in names.\n              There should be one string in the value in names for each\n              possible alternative.\n            - if the corresponding value in `specification_dict` is a list,\n              then there should be a list of strings as the value in names.\n              There should be one string the value in names per item in the\n              value in `specification_dict`.\n    specification_dict : OrderedDict.\n        Keys are a proper subset of the columns in `long_form_df`. Values are\n        either a list or a single string, `\"all_diff\"` or `\"all_same\"`. If a\n        list, the elements should be:\n\n            - single objects that are within the alternative ID column of\n              `long_form_df`\n            - lists of objects that are within the alternative ID column of\n              `long_form_df`. For each single object in the list, a unique\n              column will be created (i.e. there will be a unique coefficient\n              for that variable in the corresponding utility equation of the\n              corresponding alternative). For lists within the\n              `specification_dict` values, a single column will be created for\n              all the alternatives within iterable (i.e. there will be one\n              common coefficient for the variables in the iterable).\n    num_alts : int.\n        The number of alternatives in this dataset's universal choice set.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if names.keys() != specification_dict.keys():\n        msg = \"names.keys() does not equal specification_dict.keys()\"\n        raise ValueError(msg)\n\n    for key in names:\n        specification = specification_dict[key]\n        name_object = names[key]\n        if isinstance(specification, list):\n            try:\n                assert isinstance(name_object, list)\n                assert len(name_object) == len(specification)\n                assert all([isinstance(x, str) for x in name_object])\n            except AssertionError:\n                msg = \"names[{}] must be a list AND it must have the same\"\n                msg_2 = \" number of strings as there are elements of the\"\n                msg_3 = \" corresponding list in specification_dict\"\n                raise ValueError(msg.format(key) + msg_2 + msg_3)\n\n        else:\n            if specification == \"all_same\":\n                if not isinstance(name_object, str):\n                    msg = \"names[{}] should be a string\".format(key)\n                    raise TypeError(msg)\n\n            else:    # This means speciffication == 'all_diff'\n                try:\n                    assert isinstance(name_object, list)\n                    assert len(name_object) == num_alts\n                except AssertionError:\n                    msg_1 = \"names[{}] should be a list with {} elements,\"\n                    msg_2 = \" 1 element for each possible alternative\"\n                    msg = (msg_1.format(key, num_alts) + msg_2)\n                    raise ValueError(msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_all_columns_are_used(num_vars_accounted_for,\n                                dataframe,\n                                data_title='long_data'):\n    \"\"\"\n    Ensure that all of the columns from dataframe are in the list of used_cols.\n    Will raise a helpful UserWarning if otherwise.\n\n    Parameters\n    ----------\n    num_vars_accounted_for : int.\n        Denotes the number of variables used in one's function.\n    dataframe : pandas dataframe.\n        Contains all of the data to be converted from one format to another.\n    data_title : str, optional.\n        Denotes the title by which `dataframe` should be referred in the\n        UserWarning.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    dataframe_vars = set(dataframe.columns.tolist())\n    num_dataframe_vars = len(dataframe_vars)\n\n    if num_vars_accounted_for == num_dataframe_vars:\n        pass\n\n    elif num_vars_accounted_for < num_dataframe_vars:\n        msg = \"Note, there are {:,} variables in {} but the inputs\"\n        msg_2 = \" ind_vars, alt_specific_vars, and subset_specific_vars only\"\n        msg_3 = \" account for {:,} variables.\"\n\n        warnings.warn(msg.format(num_dataframe_vars, data_title) +\n                      msg_2 + msg_3.format(num_vars_accounted_for))\n\n    else:  # This means num_vars_accounted_for > num_dataframe_vars\n        msg = \"There are more variable specified in ind_vars, \"\n        msg_2 = \"alt_specific_vars, and subset_specific_vars ({:,}) than there\"\n        msg_3 = \" are variables in {} ({:,})\"\n        warnings.warn(msg +\n                      msg_2.format(num_vars_accounted_for) +\n                      msg_3.format(data_title, num_dataframe_vars))\n\n    return None", "response": "Ensures that all of the columns in dataframe are used in the list of used_cols."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking a cross - sectional dataframe of long - format data for duplicate observations.", "response": "def check_dataframe_for_duplicate_records(obs_id_col, alt_id_col, df):\n    \"\"\"\n    Checks a cross-sectional dataframe of long-format data for duplicate\n    observations. Duplicate observations are defined as rows with the same\n    observation id value and the same alternative id value.\n\n    Parameters\n    ----------\n    obs_id_col : str.\n        Denotes the column in `df` that contains the observation ID\n        values for each row.\n    alt_id_col : str.\n        Denotes the column in `df` that contains the alternative ID\n        values for each row.\n    df : pandas dataframe.\n        The dataframe of long format data that is to be checked for duplicates.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if df.duplicated(subset=[obs_id_col, alt_id_col]).any():\n        msg = \"One or more observation-alternative_id pairs is not unique.\"\n        raise ValueError(msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the total number of recorded choices equals the total number of recorded observations. Raises helpful ValueError messages if this is not the case.", "response": "def ensure_num_chosen_alts_equals_num_obs(obs_id_col, choice_col, df):\n    \"\"\"\n    Checks that the total number of recorded choices equals the total number of\n    observations. If this is not the case, raise helpful ValueError messages.\n\n    Parameters\n    ----------\n    obs_id_col : str.\n        Denotes the column in `df` that contains the observation ID values for\n        each row.\n    choice_col : str.\n        Denotes the column in `long_data` that contains a one if the\n        alternative pertaining to the given row was the observed outcome for\n        the observation pertaining to the given row and a zero otherwise.\n    df : pandas dataframe.\n        The dataframe whose choices and observations will be checked.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    num_obs = df[obs_id_col].unique().shape[0]\n    num_choices = df[choice_col].sum()\n\n    if num_choices < num_obs:\n        msg = \"One or more observations have not chosen one \"\n        msg_2 = \"of the alternatives available to him/her\"\n        raise ValueError(msg + msg_2)\n    if num_choices > num_obs:\n        msg = \"One or more observations has chosen multiple alternatives\"\n        raise ValueError(msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_type_and_values_of_alt_name_dict(alt_name_dict, alt_id_col, df):\n    if not isinstance(alt_name_dict, dict):\n        msg = \"alt_name_dict should be a dictionary. Passed value was a {}\"\n        raise TypeError(msg.format(type(alt_name_dict)))\n\n    if not all([x in df[alt_id_col].values for x in alt_name_dict.keys()]):\n        msg = \"One or more of alt_name_dict's keys are not \"\n        msg_2 = \"in long_data[alt_id_col]\"\n        raise ValueError(msg + msg_2)\n\n    return None", "response": "Checks that the type of alt_name_dict and values of alt_id_col are met. Raises helpful errors if either of them are not met."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that ridge is either None or a scalar value. Raises a helpful TypeError otherwise.", "response": "def ensure_ridge_is_scalar_or_none(ridge):\n    \"\"\"\n    Ensures that `ridge` is either None or a scalar value. Raises a helpful\n    TypeError otherwise.\n\n    Parameters\n    ----------\n    ridge : int, float, long, or None.\n        Scalar value or None, determining the L2-ridge regression penalty.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if (ridge is not None) and not isinstance(ridge, Number):\n        msg_1 = \"ridge should be None or an int, float, or long.\"\n        msg_2 = \"The passed value of ridge had type: {}\".format(type(ridge))\n        raise TypeError(msg_1 + msg_2)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_design_matrix(long_form,\n                         specification_dict,\n                         alt_id_col,\n                         names=None):\n    \"\"\"\n    Parameters\n    ----------\n    long_form : pandas dataframe.\n        Contains one row for each available alternative, for each observation.\n    specification_dict : OrderedDict.\n        Keys are a proper subset of the columns in `long_form_df`. Values are\n        either a list or a single string, `\"all_diff\"` or `\"all_same\"`. If a\n        list, the elements should be:\n\n            - single objects that are within the alternative ID column of\n              `long_form_df`\n            - lists of objects that are within the alternative ID column of\n              `long_form_df`. For each single object in the list, a unique\n              column will be created (i.e. there will be a unique coefficient\n              for that variable in the corresponding utility equation of the\n              corresponding alternative). For lists within the\n              `specification_dict` values, a single column will be created for\n              all the alternatives within iterable (i.e. there will be one\n              common coefficient for the variables in the iterable).\n    alt_id_col : str.\n        Column name which denotes the column in `long_form` that contains the\n        alternative ID for each row in `long_form`.\n    names : OrderedDict, optional.\n        Should have the same keys as `specification_dict`. For each key:\n\n            - if the corresponding value in `specification_dict` is \"all_same\",\n              then there should be a single string as the value in names.\n            - if the corresponding value in `specification_dict` is \"all_diff\",\n              then there should be a list of strings as the value in names.\n              There should be one string in the value in names for each\n              possible alternative.\n            - if the corresponding value in `specification_dict` is a list,\n              then there should be a list of strings as the value in names.\n              There should be one string the value in names per item in the\n              value in `specification_dict`.\n        Default == None.\n\n    Returns\n    -------\n    design_matrix, var_names: tuple with two elements.\n        First element is the design matrix, a numpy array with some number of\n        columns and as many rows as are in `long_form`. Each column corresponds\n        to a coefficient to be estimated. The second element is a list of\n        strings denoting the names of each coefficient, with one variable name\n        per column in the design matrix.\n    \"\"\"\n    ##########\n    # Check that the arguments meet this functions assumptions.\n    # Fail gracefully if the arguments do not meet the function's requirements.\n    #########\n    check_argument_type(long_form, specification_dict)\n\n    ensure_alt_id_in_long_form(alt_id_col, long_form)\n\n    ensure_specification_cols_are_in_dataframe(specification_dict, long_form)\n\n    # Find out what and how many possible alternatives there are\n    unique_alternatives = np.sort(long_form[alt_id_col].unique())\n    num_alternatives = len(unique_alternatives)\n\n    check_type_and_values_of_specification_dict(specification_dict,\n                                                unique_alternatives)\n\n    # Check the user passed dictionary of names if the user passed such a list\n    if names is not None:\n        ensure_object_is_ordered_dict(names, \"names\")\n\n        check_keys_and_values_of_name_dictionary(names,\n                                                 specification_dict,\n                                                 num_alternatives)\n\n    ##########\n    # Actually create the design matrix\n    ##########\n    # Create a list of the columns of independent variables\n    independent_vars = []\n    # Create a list of variable names\n    var_names = []\n\n    # Create the columns of the design matrix based on the specification dict.\n    for variable in specification_dict:\n        specification = specification_dict[variable]\n        if specification == \"all_same\":\n            # Create the variable column\n            independent_vars.append(long_form[variable].values)\n            # Create the column name\n            var_names.append(variable)\n        elif specification == \"all_diff\":\n            for alt in unique_alternatives:\n                # Create the variable column\n                independent_vars.append((long_form[alt_id_col] == alt).values *\n                                        long_form[variable].values)\n                # create the column name\n                var_names.append(\"{}_{}\".format(variable, alt))\n        else:\n            for group in specification:\n                if isinstance(group, list):\n                    # Create the variable column\n                    independent_vars.append(\n                                     long_form[alt_id_col].isin(group).values *\n                                     long_form[variable].values)\n                    # Create the column name\n                    var_names.append(\"{}_{}\".format(variable, str(group)))\n\n                else:  # the group is an integer\n                    # Create the variable column\n                    new_col_vals = ((long_form[alt_id_col] == group).values *\n                                    long_form[variable].values)\n                    independent_vars.append(new_col_vals)\n                    # Create the column name\n                    var_names.append(\"{}_{}\".format(variable, group))\n\n    # Create the final design matrix\n    design_matrix = np.hstack((x[:, None] for x in independent_vars))\n\n    # Use the list of names passed by the user, if the user passed such a list\n    if names is not None:\n        var_names = []\n        for value in names.values():\n            if isinstance(value, str):\n                var_names.append(value)\n            else:\n                for inner_name in value:\n                    var_names.append(inner_name)\n\n    return design_matrix, var_names", "response": "Creates a design matrix for the given language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the unique ids of id_array in their original order of appearance.", "response": "def get_original_order_unique_ids(id_array):\n    \"\"\"\n    Get the unique id's of id_array, in their original order of appearance.\n\n    Parameters\n    ----------\n    id_array : 1D ndarray.\n        Should contain the ids that we want to extract the unique values from.\n\n    Returns\n    -------\n    original_order_unique_ids : 1D ndarray.\n        Contains the unique ids from `id_array`, in their original order of\n        appearance.\n    \"\"\"\n    assert isinstance(id_array, np.ndarray)\n    assert len(id_array.shape) == 1\n\n    # Get the indices of the unique IDs in their order of appearance\n    # Note the [1] is because the np.unique() call will return both the sorted\n    # unique IDs and the indices\n    original_unique_id_indices =\\\n        np.sort(np.unique(id_array, return_index=True)[1])\n\n    # Get the unique ids, in their original order of appearance\n    original_order_unique_ids = id_array[original_unique_id_indices]\n\n    return original_order_unique_ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a matrix that maps each row of id_array to some id related by the corresponding column.", "response": "def create_row_to_some_id_col_mapping(id_array):\n    \"\"\"\n    Parameters\n    ----------\n    id_array : 1D ndarray.\n        All elements of the array should be ints representing some id related\n        to the corresponding row.\n\n    Returns\n    -------\n    rows_to_ids : 2D scipy sparse array.\n        Will map each row of id_array to the unique values of `id_array`. The\n        columns of the returned sparse array will correspond to the unique\n        values of `id_array`, in the order of appearance for each of these\n        unique values.\n    \"\"\"\n    # Get the unique ids, in their original order of appearance\n    original_order_unique_ids = get_original_order_unique_ids(id_array)\n\n    # Create a matrix with the same number of rows as id_array but a single\n    # column for each of the unique IDs. This matrix will associate each row\n    # as belonging to a particular observation using a one and using a zero to\n    # show non-association.\n    rows_to_ids = (id_array[:, None] ==\n                   original_order_unique_ids[None, :]).astype(int)\n    return rows_to_ids"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a scipy. sparse matrix that maps each row represented by an element in id_array to the corresponding value of the corresponding unique_ids in id_array.", "response": "def create_sparse_mapping(id_array, unique_ids=None):\n    \"\"\"\n    Will create a scipy.sparse compressed-sparse-row matrix that maps\n    each row represented by an element in id_array to the corresponding\n    value of the unique ids in id_array.\n\n    Parameters\n    ----------\n    id_array : 1D ndarray of ints.\n        Each element should represent some id related to the corresponding row.\n    unique_ids : 1D ndarray of ints, or None, optional.\n        If not None, each element should be present in `id_array`. The elements\n        in `unique_ids` should be present in the order in which one wishes them\n        to appear in the columns of the resulting sparse array. For the\n        `row_to_obs` and `row_to_mixers` mappings, this should be the order of\n        appearance in `id_array`. If None, then the unique_ids will be created\n        from `id_array`, in the order of their appearance in `id_array`.\n\n    Returns\n    -------\n    mapping : 2D scipy.sparse CSR matrix.\n        Will contain only zeros and ones. `mapping[i, j] == 1` where\n        `id_array[i] == unique_ids[j]`. The id's corresponding to each column\n        are given by `unique_ids`. The rows correspond to the elements of\n        `id_array`.\n    \"\"\"\n    # Create unique_ids if necessary\n    if unique_ids is None:\n        unique_ids = get_original_order_unique_ids(id_array)\n\n    # Check function arguments for validity\n    assert isinstance(unique_ids, np.ndarray)\n    assert isinstance(id_array, np.ndarray)\n    assert unique_ids.ndim == 1\n    assert id_array.ndim == 1\n\n    # Figure out which ids in id_array are represented in unique_ids\n    represented_ids = np.in1d(id_array, unique_ids)\n    # Determine the number of rows in id_array that are in unique_ids\n    num_non_zero_rows = represented_ids.sum()\n    # Figure out the dimensions of the resulting sparse matrix\n    num_rows = id_array.size\n    num_cols = unique_ids.size\n    # Specify the non-zero values that will be present in the sparse matrix.\n    data = np.ones(num_non_zero_rows, dtype=int)\n    # Specify which rows will have non-zero entries in the sparse matrix.\n    row_indices = np.arange(num_rows)[represented_ids]\n    # Map the unique id's to their respective columns\n    unique_id_dict = dict(zip(unique_ids, np.arange(num_cols)))\n    # Figure out the column indices of the non-zero entries, and do so in a way\n    # that avoids a key error (i.e. only look up ids that are represented)\n    col_indices =\\\n        np.array([unique_id_dict[x] for x in id_array[represented_ids]])\n\n    # Create and return the sparse matrix\n    return csr_matrix((data, (row_indices, col_indices)),\n                      shape=(num_rows, num_cols))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a mapping from the given observation ID to the given alternative ID.", "response": "def create_long_form_mappings(long_form,\n                              obs_id_col,\n                              alt_id_col,\n                              choice_col=None,\n                              nest_spec=None,\n                              mix_id_col=None,\n                              dense=False):\n    \"\"\"\n    Parameters\n    ----------\n    long_form : pandas dataframe.\n        Contains one row for each available alternative for each observation.\n    obs_id_col : str.\n        Denotes the column in `long_form` which contains the choice situation\n        observation ID values for each row of `long_form`. Note each value in\n        this column must be unique (i.e., individuals with repeat observations\n        have unique `obs_id_col` values for each choice situation, and\n        `obs_id_col` values are unique across individuals).\n    alt_id_col : str.\n        Denotes the column in long_form which contains the alternative ID\n        values for each row of `long_form`.\n    choice_col : str, optional.\n        Denotes the column in long_form which contains a one if the alternative\n        pertaining to the given row was the observed outcome for the\n        observation pertaining to the given row and a zero otherwise.\n        Default == None.\n    nest_spec : OrderedDict, or None, optional.\n        Keys are strings that define the name of the nests. Values are lists of\n        alternative ids, denoting which alternatives belong to which nests.\n        Each alternative id must only be associated with a single nest!\n        Default == None.\n    mix_id_col : str, optional.\n        Denotes the column in long_form that contains the identification values\n        used to denote the units of observation over which parameters are\n        randomly distributed.\n    dense : bool, optional.\n        Determines whether or not scipy sparse matrices will be returned or\n        dense numpy arrays.\n\n    Returns\n    -------\n    mapping_dict : OrderedDict.\n        Keys will be `[\"rows_to_obs\", \"rows_to_alts\", \"chosen_row_to_obs\",\n        \"rows_to_nests\"]`. If `choice_col` is None, then the value for\n        `chosen_row_to_obs` will be None. Likewise, if `nest_spec` is None,\n        then the value for `rows_to_nests` will be None. The value for\n        \"rows_to_obs\" will map the rows of the `long_form` to the unique\n        observations (on the columns) in their order of appearance. The value\n        for `rows_to_alts` will map the rows of the `long_form` to the unique\n        alternatives which are possible in the dataset (on the columns), in\n        sorted order--not order of appearance. The value for\n        `chosen_row_to_obs`, if not None, will map the rows of the `long_form`\n        that contain the chosen alternatives to the specific observations those\n        rows are associated with (denoted by the columns). The value of\n        `rows_to_nests`, if not None, will map the rows of the `long_form` to\n        the nest (denoted by the column) that contains the row's alternative.\n        If `dense==True`, the returned values will be dense numpy arrays.\n        Otherwise, the returned values will be scipy sparse arrays.\n    \"\"\"\n    # Get the id_values from the long_form dataframe\n    obs_id_values = long_form[obs_id_col].values\n    alt_id_values = long_form[alt_id_col].values\n\n    # Create a matrix with the same number of rows as long_form but a single\n    # column for each of the unique IDs. This matrix will associate each row\n    # as belonging to a particular observation using a one and using a zero to\n    # show non-association.\n    rows_to_obs = create_sparse_mapping(obs_id_values)\n\n    # Determine all of the unique alternative IDs\n    all_alternatives = np.sort(np.unique(alt_id_values))\n\n    # Create a matrix with the same number of rows as long_form but a single\n    # column for each of the unique alternatives. This matrix will associate\n    # each row as belonging to a particular alternative using a one and using\n    # a zero to show non-association.\n    rows_to_alts = create_sparse_mapping(alt_id_values,\n                                         unique_ids=all_alternatives)\n\n    if choice_col is not None:\n        # Create a matrix to associate each row with the same number of\n        # rows as long_form but a 1 only if that row corresponds to an\n        # alternative that a given observation (denoted by the columns)\n        # chose.\n        chosen_row_to_obs = csr_matrix(rows_to_obs.multiply(\n                                long_form[choice_col].values[:, None]))\n    else:\n        chosen_row_to_obs = None\n\n    if nest_spec is not None:\n        # Determine how many nests there are\n        num_nests = len(nest_spec)\n        # Create a mapping between the alternative ids and their nests\n        alt_id_to_nest_name = {}\n        for key in nest_spec:\n            for element in nest_spec[key]:\n                alt_id_to_nest_name[element] = key\n\n        # Create a mapping between nest names and nest ids\n        nest_ids = np.arange(1, num_nests + 1)\n        nest_name_to_nest_id = dict(zip(nest_spec.keys(), nest_ids))\n\n        # Create an array of the nest ids of each row\n        nest_id_vec = np.array([nest_name_to_nest_id[alt_id_to_nest_name[x]]\n                                for x in alt_id_values])\n\n        # Create the mapping matrix between each row and the nests\n        rows_to_nests = create_sparse_mapping(nest_id_vec, unique_ids=nest_ids)\n\n    else:\n        rows_to_nests = None\n\n    if mix_id_col is not None:\n        # Create a mapping matrix between each row and each 'mixing unit'\n        mix_id_array = long_form[mix_id_col].values\n        rows_to_mixers = create_sparse_mapping(mix_id_array)\n\n    else:\n        rows_to_mixers = None\n\n    # Create the dictionary of mapping matrices that is to be returned\n    mapping_dict = OrderedDict()\n    mapping_dict[\"rows_to_obs\"] = rows_to_obs\n    mapping_dict[\"rows_to_alts\"] = rows_to_alts\n    mapping_dict[\"chosen_row_to_obs\"] = chosen_row_to_obs\n    mapping_dict[\"rows_to_nests\"] = rows_to_nests\n    mapping_dict[\"rows_to_mixers\"] = rows_to_mixers\n\n    # Return the dictionary of mapping matrices.\n    # If desired, convert the mapping matrices to dense matrices\n    if dense:\n        for key in mapping_dict:\n            if mapping_dict[key] is not None:\n                mapping_dict[key] = mapping_dict[key].A\n\n    return mapping_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_long_to_wide(long_data,\n                         ind_vars,\n                         alt_specific_vars,\n                         subset_specific_vars,\n                         obs_id_col,\n                         alt_id_col,\n                         choice_col,\n                         alt_name_dict=None,\n                         null_value=np.nan):\n    \"\"\"\n    Converts a 'long format' dataframe of cross-sectional discrete choice data\n    into a 'wide format' version of the same data.\n\n    Parameters\n    ----------\n    long_data : pandas dataframe.\n        Contains one row for each available alternative for each observation.\n        Should have the specified `[obs_id_col, alt_id_col, choice_col]` column\n        headings. The dtypes of all columns should be numeric.\n    ind_vars : list of strings.\n        Each element should be a column heading in `long_data` that denotes a\n        variable that varies across observations but not across alternatives.\n    alt_specific_vars : list of strings.\n        Each element should be a column heading in `long_data` that denotes a\n        variable that varies not only across observations but also across all\n        alternatives.\n    subset_specific_vars : dict.\n        Each key should be a string that is a column heading of `long_data`.\n        Each value should be a list of alternative ids denoting the subset of\n        alternatives which the variable (i.e. the key) over actually varies.\n        These variables should vary across individuals and across some\n        alternatives.\n    obs_id_col : str.\n        Denotes the column in `long_data` that contains the observation ID\n        values for each row.\n    alt_id_col : str.\n        Denotes the column in `long_data` that contains the alternative ID\n        values for each row.\n    choice_col : str.\n        Denotes the column in `long_data` that contains a one if the\n        alternative pertaining to the given row was the observed outcome for\n        the observation pertaining to the given row and a zero otherwise.\n    alt_name_dict : dict or None, optional\n        If not None, should be a dictionary whose keys are the possible values\n        in `long_data[alt_id_col].unique()`. The values should be the name\n        that one wants to associate with each alternative id. Default == None.\n    null_value : int, float, long, or `np.nan`, optional.\n        The passed value will be used to fill cells in the wide format\n        dataframe when that cell is unknown for a given individual. This is\n        most commonly the case when there is a variable that varies across\n        alternatives and one of the alternatives is not available for a given\n        indvidual. The `null_value` will be inserted for that individual for\n        that variable. Default == `np.nan`.\n\n    Returns\n    -------\n    final_wide_df : pandas dataframe.\n        Will contain one row per observational unit. Will contain an\n        observation id column of the same name as `obs_id_col`. Will also\n        contain a choice column of the same name as `choice_col`. Will contain\n        one availability column per unique, observed alternative in the\n        dataset. Will contain one column per variable in `ind_vars`. Will\n        contain one column per alternative per variable in `alt_specific_vars`.\n        Will contain one column per specified alternative per variable in\n        `subset_specific_vars`.\n    \"\"\"\n    ##########\n    # Check that all columns of long_data are being\n    # used in the conversion to wide format\n    ##########\n    num_vars_accounted_for = sum([len(x) for x in\n                                  [ind_vars, alt_specific_vars,\n                                   subset_specific_vars,\n                                   [obs_id_col, alt_id_col, choice_col]]])\n\n    ensure_all_columns_are_used(num_vars_accounted_for, long_data)\n\n    ##########\n    # Check that all columns one wishes to use are actually in long_data\n    ##########\n    ensure_columns_are_in_dataframe(ind_vars,\n                                    long_data,\n                                    col_title=\"ind_vars\",\n                                    data_title='long_data')\n\n    ensure_columns_are_in_dataframe(alt_specific_vars,\n                                    long_data,\n                                    col_title=\"alt_specific_vars\",\n                                    data_title='long_data')\n\n    ensure_columns_are_in_dataframe(subset_specific_vars.keys(),\n                                    long_data,\n                                    col_title=\"subset_specific_vars\",\n                                    data_title='long_data')\n\n    identifying_cols = [choice_col, obs_id_col, alt_id_col]\n    identifying_col_string = \"[choice_col, obs_id_col, alt_id_col]\"\n    ensure_columns_are_in_dataframe(identifying_cols,\n                                    long_data,\n                                    col_title=identifying_col_string,\n                                    data_title='long_data')\n\n    ##########\n    # Make sure that each observation-alternative pair is unique\n    ##########\n    check_dataframe_for_duplicate_records(obs_id_col, alt_id_col, long_data)\n\n    ##########\n    # Make sure each observation chose an alternative that's available.\n    ##########\n    # Make sure that the number of chosen alternatives equals the number of\n    # individuals.\n    ensure_num_chosen_alts_equals_num_obs(obs_id_col, choice_col, long_data)\n\n    ##########\n    # Check that the alternative ids in the alt_name_dict are actually the\n    # alternative ids used in the long_data alt_id column.\n    ##########\n    if alt_name_dict is not None:\n        check_type_and_values_of_alt_name_dict(alt_name_dict,\n                                               alt_id_col,\n                                               long_data)\n\n    ##########\n    # Figure out how many rows/columns should be in the wide format dataframe\n    ##########\n    # Note that the number of rows in wide format is the number of observations\n    num_obs = long_data[obs_id_col].unique().shape[0]\n\n    # Figure out the total number of possible alternatives for the dataset\n    num_alts = long_data[alt_id_col].unique().shape[0]\n\n    ############\n    # Calculate the needed number of colums\n    ############\n    # For each observation, there is at least one column-- the observation id,\n    num_cols = 1\n    # We should have one availability column per alternative in the dataset\n    num_cols += num_alts\n    # We should also have one column to record the choice of each observation\n    num_cols += 1\n    # We should also have one column for each individual specific variable\n    num_cols += len(ind_vars)\n    # We should also have one column for each alternative specific variable,\n    # for each alternative\n    num_cols += len(alt_specific_vars) * num_alts\n    # We should have one column for each subset alternative specific variable\n    # for each alternative over which the variable varies\n    for col in subset_specific_vars:\n        num_cols += len(subset_specific_vars[col])\n\n    ##########\n    # Create the columns of the new dataframe\n    ##########\n    #####\n    # Create the individual specific variable columns,\n    # along with the observation id column\n    #####\n    new_df = long_data[[obs_id_col] + ind_vars].drop_duplicates()\n\n    # Reset the index so that the index is not based on long_data\n    new_df.reset_index(inplace=True)\n\n    #####\n    # Create the choice column in the wide data format\n    #####\n    new_df[choice_col] = long_data.loc[long_data[choice_col] == 1,\n                                       alt_id_col].values\n\n    #####\n    # Create the availability columns\n    #####\n    # Get the various long form mapping matrices\n    mapping_res = create_long_form_mappings(long_data,\n                                            obs_id_col,\n                                            alt_id_col)\n    row_to_obs = mapping_res[\"rows_to_obs\"]\n    row_to_alt = mapping_res[\"rows_to_alts\"]\n\n    # Get the matrix of observations (rows) to available alternatives (columns)\n    obs_to_alt = row_to_obs.T.dot(row_to_alt).todense()\n\n    # Determine the unique alternative IDs in the order used in obs_to_alt\n    alt_id_values = long_data[alt_id_col].values\n    all_alternatives = np.sort(np.unique(alt_id_values))\n\n    # Create the names for the availability columns\n    if alt_name_dict is None:\n        availability_col_names = [\"availability_{}\".format(int(x))\n                                  for x in all_alternatives]\n    else:\n        availability_col_names = [\"availability_{}\".format(alt_name_dict[x])\n                                  for x in all_alternatives]\n\n    # Create a dataframe containing the availability columns for this dataset\n    availability_df = pd.DataFrame(obs_to_alt,\n                                   columns=availability_col_names)\n\n    #####\n    # Create the alternative specific and subset\n    # alternative specific variable columns\n    #####\n    # For each alternative specific variable, create a wide format dataframe\n    alt_specific_dfs = []\n    for col in alt_specific_vars + list(subset_specific_vars.keys()):\n        # Get the relevant values from the long format dataframe\n        relevant_vals = long_data[col].values[:, None]\n        # Create an wide format array of the relevant values\n        obs_to_var = row_to_obs.T.dot(row_to_alt.multiply(relevant_vals))\n        # Ensure that the wide format array is an ndarray with of dtype float\n        if issparse(obs_to_var):\n            obs_to_var = obs_to_var.toarray()\n        # Ensure that obs_to_var has a float dtype\n        obs_to_var = obs_to_var.astype(float)\n\n        # Place a null value in columns where the alternative is not available\n        # to a given observation\n        if (obs_to_alt == 0).any():\n            obs_to_var[np.where(obs_to_alt == 0)] = null_value\n\n        # Create column names for the alternative specific variable columns\n        if alt_name_dict is None:\n            obs_to_var_names = [\"{}_{}\".format(col, int(x))\n                                for x in all_alternatives]\n        else:\n            obs_to_var_names = [\"{}_{}\".format(col, alt_name_dict[x])\n                                for x in all_alternatives]\n\n        # Subset obs_to_vars and obs_to_var_names if col is in\n        # subset_specific_vars\n        if col in subset_specific_vars:\n            # Calculate the relevant column indices for\n            # the specified subset of alternatives\n            relevant_alt_ids = subset_specific_vars[col]\n            relevant_col_idx = np.where(np.in1d(all_alternatives,\n                                                relevant_alt_ids))[0]\n        else:\n            relevant_col_idx = None\n\n        # Create a dataframe containing the alternative specific variables\n        # or the subset alternative specific variables for the given\n        # variable in the long format dataframe\n        if relevant_col_idx is None:\n            obs_to_var_df = pd.DataFrame(obs_to_var,\n                                         columns=obs_to_var_names)\n        else:\n            obs_to_var_df = pd.DataFrame(obs_to_var[:, relevant_col_idx],\n                                         columns=[obs_to_var_names[x] for\n                                                  x in relevant_col_idx])\n\n        # Store the current alternative specific variable columns/dataframe\n        alt_specific_dfs.append(obs_to_var_df)\n\n    # Combine all of the various alternative specific variable dataframes\n    final_alt_specific_df = pd.concat(alt_specific_dfs, axis=1)\n\n    ##########\n    # Construct the final wide format dataframe to be returned\n    ##########\n    final_wide_df = pd.concat([new_df[[obs_id_col]],\n                               new_df[[choice_col]],\n                               availability_df,\n                               new_df[ind_vars],\n                               final_alt_specific_df],\n                              axis=1)\n\n    # Make sure one has the correct number of rows and columns in\n    # the final dataframe\n    if final_wide_df.shape != (num_obs, num_cols):\n        msg_1 = \"There is an error with the dataframe that will be returned\"\n        msg_2 = \"The shape of the dataframe should be {}\".format((num_obs,\n                                                                  num_cols))\n        msg_3 = \"Instead, the returned dataframe will have shape: {}\"\n        total_msg = msg_1 + msg_2 + msg_3.format(final_wide_df.shape)\n        warnings.warn(total_msg)\n\n    # Return the wide format dataframe\n    return final_wide_df", "response": "Converts a long format dataframe of cross - sectional discrete choice data into a wide format version of the same data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_wide_data_for_blank_choices(choice_col, wide_data):\n    if wide_data[choice_col].isnull().any():\n        msg_1 = \"One or more of the values in wide_data[choice_col] is null.\"\n        msg_2 = \" Remove null values in the choice column or fill them in.\"\n        raise ValueError(msg_1 + msg_2)\n\n    return None", "response": "Checks wide_data for null values in the choice column and raises a\n    helpful ValueError if null values are found."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_unique_obs_ids_in_wide_data(obs_id_col, wide_data):\n    if len(wide_data[obs_id_col].unique()) != wide_data.shape[0]:\n        msg = \"The values in wide_data[obs_id_col] are not unique, \"\n        msg_2 = \"but they need to be.\"\n        raise ValueError(msg + msg_2)\n\n    return None", "response": "Ensures that the observation ID s in the specified column are unique. Raises a helpful ValueError if otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nensure that all chosen alternatives in wide_df are present in availability_vars. Raises a helpful ValueError if not.", "response": "def ensure_chosen_alternatives_are_in_user_alt_ids(choice_col,\n                                                   wide_data,\n                                                   availability_vars):\n    \"\"\"\n    Ensures that all chosen alternatives in `wide_df` are present in the\n    `availability_vars` dict. Raises a helpful ValueError if not.\n\n    Parameters\n    ----------\n    choice_col : str.\n        Denotes the column in `wide_data` that contains a one if the\n        alternative pertaining to the given row was the observed outcome for\n        the observation pertaining to the given row and a zero otherwise.\n    wide_data : pandas dataframe.\n        Contains one row for each observation. Should contain the specified\n        `choice_col` column.\n    availability_vars : dict.\n        There should be one key value pair for each alternative that is\n        observed in the dataset. Each key should be the alternative id for the\n        alternative, and the value should be the column heading in `wide_data`\n        that denotes (using ones and zeros) whether an alternative is\n        available/unavailable, respectively, for a given observation.\n        Alternative id's, i.e. the keys, must be integers.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if not wide_data[choice_col].isin(availability_vars.keys()).all():\n        msg = \"One or more values in wide_data[choice_col] is not in the user \"\n        msg_2 = \"provided alternative ids in availability_vars.keys()\"\n        raise ValueError(msg + msg_2)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether or not each observation with a restricted choice set chose an available alternative.", "response": "def ensure_each_wide_obs_chose_an_available_alternative(obs_id_col,\n                                                        choice_col,\n                                                        availability_vars,\n                                                        wide_data):\n    \"\"\"\n    Checks whether or not each observation with a restricted choice set chose\n    an alternative that was personally available to him or her. Will raise a\n    helpful ValueError if this is not the case.\n\n    Parameters\n    ----------\n    obs_id_col : str.\n        Denotes the column in `wide_data` that contains the observation ID\n        values for each row.\n    choice_col : str.\n        Denotes the column in `wide_data` that contains a one if the\n        alternative pertaining to the given row was the observed outcome for\n        the observation pertaining to the given row and a zero otherwise.\n    availability_vars : dict.\n        There should be one key value pair for each alternative that is\n        observed in the dataset. Each key should be the alternative id for the\n        alternative, and the value should be the column heading in `wide_data`\n        that denotes (using ones and zeros) whether an alternative is\n        available/unavailable, respectively, for a given observation.\n        Alternative id's, i.e. the keys, must be integers.\n    wide_data : pandas dataframe.\n        Contains one row for each observation. Should have the specified\n        `[obs_id_col, choice_col] + availability_vars.values()` columns.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # Determine the various availability values for each observation\n    wide_availability_values = wide_data[list(\n        availability_vars.values())].values\n\n    # Isolate observations for whom one or more alternatives are unavailable\n    unavailable_condition = ((wide_availability_values == 0).sum(axis=1)\n                                                            .astype(bool))\n\n    # Iterate over the observations with one or more unavailable alternatives\n    # Check that each such observation's chosen alternative was available\n    problem_obs = []\n    for idx, row in wide_data.loc[unavailable_condition].iterrows():\n        if row.at[availability_vars[row.at[choice_col]]] != 1:\n            problem_obs.append(row.at[obs_id_col])\n\n    if problem_obs != []:\n        msg = \"The following observations chose unavailable alternatives:\\n{}\"\n        raise ValueError(msg.format(problem_obs))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_all_wide_alt_ids_are_chosen(choice_col,\n                                       alt_specific_vars,\n                                       availability_vars,\n                                       wide_data):\n    \"\"\"\n    Checks to make sure all user-specified alternative id's, both in\n    `alt_specific_vars` and `availability_vars` are observed in the choice\n    column of `wide_data`.\n    \"\"\"\n    sorted_alt_ids = np.sort(wide_data[choice_col].unique())\n    try:\n        problem_ids = [x for x in availability_vars\n                       if x not in sorted_alt_ids]\n        problem_type = \"availability_vars\"\n        assert problem_ids == []\n\n        problem_ids = []\n        for new_column in alt_specific_vars:\n            for alt_id in alt_specific_vars[new_column]:\n                if alt_id not in sorted_alt_ids and alt_id not in problem_ids:\n                    problem_ids.append(alt_id)\n        problem_type = \"alt_specific_vars\"\n        assert problem_ids == []\n    except AssertionError:\n        msg = \"The following alternative ids from {} are not \"\n        msg_2 = \"observed in wide_data[choice_col]:\\n{}\"\n        raise ValueError(msg.format(problem_type) + msg_2.format(problem_ids))\n\n    return None", "response": "Checks to make sure all user - specified alternative ids observed in the choice_col of wide_data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_contiguity_in_observation_rows(obs_id_vector):\n    # Check that the choice situation id for each row is larger than or equal\n    # to the choice situation id of the preceding row.\n    contiguity_check_array = (obs_id_vector[1:] - obs_id_vector[:-1]) >= 0\n    if not contiguity_check_array.all():\n        problem_ids = obs_id_vector[np.where(~contiguity_check_array)]\n        msg_1 = \"All rows pertaining to a given choice situation must be \"\n        msg_2 = \"contiguous. \\nRows pertaining to the following observation \"\n        msg_3 = \"id's are not contiguous: \\n{}\"\n        raise ValueError(msg_1 + msg_2 + msg_3.format(problem_ids.tolist()))\n    else:\n        return None", "response": "Ensures that all rows pertaining to a given observation are located in the same design matrix as each other. Raises a helpful ValueError if not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_wide_to_long(wide_data,\n                         ind_vars,\n                         alt_specific_vars,\n                         availability_vars,\n                         obs_id_col,\n                         choice_col,\n                         new_alt_id_name=None):\n    \"\"\"\n    Will convert a cross-sectional dataframe of discrete choice data from wide\n    format to long format.\n\n    Parameters\n    ----------\n    wide_data : pandas dataframe.\n        Contains one row for each observation. Should have the specified\n        `[obs_id_col, choice_col] + availability_vars.values()` columns.\n    ind_vars : list of strings.\n        Each element should be a column heading in `wide_data` that denotes a\n        variable that varies across observations but not across alternatives.\n    alt_specific_vars : dict.\n        Each key should be a string that will be a column heading of the\n        returned, long format dataframe. Each value should be a dictionary\n        where the inner key is the alternative id and the value is the column\n        heading in wide data that specifies the value of the outer key for the\n        associated alternative. The variables denoted by the outer key should\n        vary across individuals and across some or all alternatives.\n    availability_vars : dict.\n        There should be one key value pair for each alternative that is\n        observed in the dataset. Each key should be the alternative id for the\n        alternative, and the value should be the column heading in `wide_data`\n        that denotes (using ones and zeros) whether an alternative is\n        available/unavailable, respectively, for a given observation.\n        Alternative id's, i.e. the keys, must be integers.\n    obs_id_col : str.\n        Denotes the column in `wide_data` that contains the observation ID\n        values for each row.\n    choice_col : str.\n        Denotes the column in `wide_data` that contains a one if the\n        alternative pertaining to the given row was the observed outcome for\n        the observation pertaining to the given row and a zero otherwise.\n    new_alt_id_name : str, optional.\n        If not None, should be a string. This string will be used as the column\n        heading for the alternative id column in the returned 'long' format\n        dataframe. If not passed, this column will be called `'alt_id'`.\n        Default == None.\n\n    Returns\n    -------\n    final_long_df : pandas dataframe.\n        Will contain one row for each available alternative for each\n        observation. Will contain an observation id column of the same name as\n        `obs_id_col`. Will also contain a choice column of the same name as\n        `choice_col`. Will also contain an alternative id column called\n        `alt_id` if `new_alt_id_col == None`, or `new_alt_id` otherwise. Will\n        contain one column per variable in `ind_vars`. Will contain one column\n        per key in `alt_specific_vars`.\n    \"\"\"\n    ##########\n    # Check that all columns of wide_data are being\n    # used in the conversion to long format\n    ##########\n    all_alt_specific_cols = []\n    for var_dict in alt_specific_vars.values():\n        all_alt_specific_cols.extend(var_dict.values())\n\n    vars_accounted_for = set(ind_vars +\n                             # converto list explicitly to support\n                             # both python 2 and 3\n                             list(availability_vars.values()) +\n                             [obs_id_col, choice_col] +\n                             all_alt_specific_cols)\n    num_vars_accounted_for = len(vars_accounted_for)\n\n    ensure_all_columns_are_used(num_vars_accounted_for,\n                                wide_data,\n                                data_title='wide_data')\n\n    ##########\n    # Check that all columns one wishes to use are actually in wide_data\n    ##########\n    ensure_columns_are_in_dataframe(ind_vars,\n                                    wide_data,\n                                    col_title='ind_vars',\n                                    data_title='wide_data')\n\n    ensure_columns_are_in_dataframe(availability_vars.values(),\n                                    wide_data,\n                                    col_title='availability_vars',\n                                    data_title='wide_data')\n\n    for new_column in alt_specific_vars:\n        for alt_id in alt_specific_vars[new_column]:\n            old_column = alt_specific_vars[new_column][alt_id]\n            ensure_columns_are_in_dataframe([old_column],\n                                            wide_data,\n                                            col_title=\"alt_specific_vars\",\n                                            data_title='wide_data')\n\n    ensure_columns_are_in_dataframe([choice_col, obs_id_col],\n                                    wide_data,\n                                    col_title='[choice_col, obs_id_col]',\n                                    data_title='wide_data')\n\n    ##########\n    # Check the integrity of the various columns present in wide_data\n    ##########\n    # Make sure the observation id's are unique (i.e. one per row)\n    ensure_unique_obs_ids_in_wide_data(obs_id_col, wide_data)\n\n    # Make sure there are no blank values in the choice column\n    check_wide_data_for_blank_choices(choice_col, wide_data)\n\n    ##########\n    # Check that the user-provided alternative ids are observed\n    # in the realized choices.\n    ##########\n    ensure_all_wide_alt_ids_are_chosen(choice_col,\n                                       alt_specific_vars,\n                                       availability_vars,\n                                       wide_data)\n\n    ##########\n    # Check that the realized choices are all in the\n    # user-provided alternative ids\n    ##########\n    ensure_chosen_alternatives_are_in_user_alt_ids(choice_col,\n                                                   wide_data,\n                                                   availability_vars)\n\n    ##########\n    # Make sure each observation chose a personally available alternative.\n    ##########\n    ensure_each_wide_obs_chose_an_available_alternative(obs_id_col,\n                                                        choice_col,\n                                                        availability_vars,\n                                                        wide_data)\n\n    ##########\n    # Figure out how many rows/columns should be in the long format dataframe\n    ##########\n    # Note that the number of rows in long format is the\n    # number of available alternatives across all observations\n    sorted_alt_ids = np.sort(wide_data[choice_col].unique())\n    sorted_availability_cols = [availability_vars[x] for x in sorted_alt_ids]\n    num_rows = wide_data[sorted_availability_cols].sum(axis=0).sum()\n\n    #####\n    # Calculate the needed number of colums\n    #####\n    # For each observation, there is at least one column-- the observation id,\n    num_cols = 1\n    # We should also have one alternative id column\n    num_cols += 1\n    # We should also have one column to record the choice of each observation\n    num_cols += 1\n    # We should also have one column for each individual specific variable\n    num_cols += len(ind_vars)\n    # We should also have one column for each alternative specific variable,\n    num_cols += len(alt_specific_vars.keys())\n\n    ##########\n    # Create the columns of the new dataframe\n    ##########\n    #####\n    # Create the observation id column,\n    #####\n    # Determine the various availability values for each observation\n    wide_availability_values = wide_data[list(\n        availability_vars.values())].values\n    new_obs_id_col = (wide_availability_values *\n                      wide_data[obs_id_col].values[:, None]).ravel()\n    # Make sure the observation id column has an integer data type\n    new_obs_id_col = new_obs_id_col.astype(int)\n\n    #####\n    # Create the independent variable columns. Store them in a list.\n    #####\n    new_ind_var_cols = []\n    for var in ind_vars:\n        new_ind_var_cols.append((wide_availability_values *\n                                 wide_data[var].values[:, None]).ravel())\n\n    #####\n    # Create the choice column in the long data format\n    #####\n    wide_choice_data = (wide_data[choice_col].values[:, None] ==\n                        sorted_alt_ids[None, :])\n    new_choice_col = wide_choice_data.ravel()\n    # Make sure the choice column has an integer data type\n    new_choice_col = new_choice_col.astype(int)\n\n    #####\n    # Create the alternative id column\n    #####\n    new_alt_id_col = (wide_availability_values *\n                      sorted_alt_ids[None, :]).ravel().astype(int)\n    # Make sure the alternative id column has an integer data type\n    new_alt_id_col = new_alt_id_col.astype(int)\n\n    #####\n    # Create the alternative specific and subset\n    # alternative specific variable columns\n    #####\n    # For each alternative specific variable, create a wide format array.\n    # Then unravel that array to have the long format column for the\n    # alternative specific variable. Store all the long format columns\n    # in a list\n    new_alt_specific_cols = []\n    for new_col in alt_specific_vars:\n        new_wide_alt_specific_cols = []\n        for alt_id in sorted_alt_ids:\n            # This will extract the correct values for the alternatives over\n            # which the alternative specific variables vary\n            if alt_id in alt_specific_vars[new_col]:\n                rel_wide_column = alt_specific_vars[new_col][alt_id]\n                new_col_vals = wide_data[rel_wide_column].values[:, None]\n                new_wide_alt_specific_cols.append(new_col_vals)\n            # This will create placeholder zeros for the alternatives that\n            # the alternative specific variables do not vary over\n            else:\n                new_wide_alt_specific_cols.append(np.zeros(\n                                                    (wide_data.shape[0], 1)))\n        concatenated_long_column = np.concatenate(new_wide_alt_specific_cols,\n                                                  axis=1).ravel()\n        new_alt_specific_cols.append(concatenated_long_column)\n\n    ##########\n    # Construct the final wide format dataframe to be returned\n    ##########\n    # Identify rows that correspond to unavailable alternatives\n    availability_condition = wide_availability_values.ravel() != 0\n\n    # Figure out the names of all of the columns in the final\n    # dataframe\n    alt_id_column_name = (\"alt_id\" if new_alt_id_name is None\n                          else new_alt_id_name)\n    final_long_columns = ([obs_id_col,\n                           alt_id_column_name,\n                           choice_col] +\n                          ind_vars +\n                          list(alt_specific_vars.keys()))\n\n    # Create a 'record array' of the final dataframe's columns\n    # Note that record arrays are constructed from a list of 1D\n    # arrays hence the array unpacking performed below for\n    # new_ind_var_cols and new_alt_specific_cols\n    all_arrays = ([new_obs_id_col,\n                   new_alt_id_col,\n                   new_choice_col] +\n                  new_ind_var_cols +\n                  new_alt_specific_cols)\n\n    # Be sure to remove rows corresponding to unavailable alternatives\n    # When creating the record array.\n    df_recs = np.rec.fromarrays([all_arrays[pos][availability_condition]\n                                 for pos in range(len(all_arrays))],\n                                names=final_long_columns)\n\n    # Create the final dataframe\n    final_long_df = pd.DataFrame.from_records(df_recs)\n\n    ##########\n    # Make sure one has the correct number of rows and columns in\n    # the final dataframe\n    ##########\n    try:\n        assert final_long_df.shape == (num_rows, num_cols)\n    except AssertionError:\n        msg_1 = \"There is an error with the dataframe that will be returned.\"\n        msg_2 = \"The shape of the dataframe should be {}\".format((num_rows,\n                                                                  num_cols))\n        msg_3 = \"Instead, the returned dataframe will have shape: {}\"\n        total_msg = \"\\n\".join([msg_1, msg_2, msg_3])\n        warnings.warn(total_msg.format(final_long_df.shape))\n\n    # Return the wide format dataframe\n    return final_long_df", "response": "Converts a cross - sectional dataframe of discrete choice data from wide - format to long format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relate_obs_ids_to_chosen_alts(obs_id_array,\n                                  alt_id_array,\n                                  choice_array):\n    \"\"\"\n    Creates a dictionary that relates each unique alternative id to the set of\n    observations ids that chose the given alternative.\n\n    Parameters\n    ----------\n    obs_id_array : 1D ndarray of ints.\n        Should be a long-format array of observation ids. Each element should\n        correspond to the unique id of the unit of observation that corresponds\n        to the given row of the long-format data. Note that each unit of\n        observation may have more than one associated choice situation.\n    alt_id_array : 1D ndarray of ints.\n        Should be a long-format array of alternative ids. Each element should\n        denote the unique id of the alternative that corresponds to the given\n        row of the long format data.\n    choice_array : 1D ndarray of ints.\n        Each element should be either a one or a zero, indicating whether the\n        alternative on the given row of the long format data was chosen or not.\n\n    Returns\n    -------\n    chosen_alts_to_obs_ids : dict.\n        Each key will be a unique value from `alt_id_array`. Each key's value\n        will be a 1D ndarray that contains the sorted, unique observation ids\n        of those observational units that chose the given alternative.\n    \"\"\"\n    # Figure out which units of observation chose each alternative.\n    chosen_alts_to_obs_ids = {}\n\n    for alt_id in np.sort(np.unique(alt_id_array)):\n        # Determine which observations chose the current alternative.\n        selection_condition =\\\n            np.where((alt_id_array == alt_id) & (choice_array == 1))\n\n        # Store the sorted, unique ids that chose the current alternative.\n        chosen_alts_to_obs_ids[alt_id] =\\\n            np.sort(np.unique(obs_id_array[selection_condition]))\n\n    # Return the desired dictionary.\n    return chosen_alts_to_obs_ids", "response": "This function creates a dictionary that relates each unique observation id to the set of observations ids that chose the given alternative."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_num_obs_choosing_each_alternative(obs_per_alt_dict):\n    # Initialize the object that is to be returned.\n    num_obs_per_group = OrderedDict()\n\n    # Determine the number of unique units of observation per group.\n    for alt_id in obs_per_alt_dict:\n        num_obs_per_group[alt_id] = len(obs_per_alt_dict[alt_id])\n\n    # Determine the total number of units of observation that will be chosen\n    # for each bootstrap sample.\n    tot_num_obs = sum([num_obs_per_group[g] for g in num_obs_per_group])\n\n    # Return the desired objects.\n    return num_obs_per_group, tot_num_obs", "response": "Returns the number of units of observation that have chosen the given alternative."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_cross_sectional_bootstrap_samples(obs_id_array,\n                                             alt_id_array,\n                                             choice_array,\n                                             num_samples,\n                                             seed=None):\n    \"\"\"\n    Determines the unique observations that will be present in each bootstrap\n    sample. This function DOES NOT create the new design matrices or a new\n    long-format dataframe for each bootstrap sample. Note that these will be\n    correct bootstrap samples for cross-sectional datasets. This function will\n    not work correctly for panel datasets.\n\n    Parameters\n    ----------\n    obs_id_array : 1D ndarray of ints.\n        Each element should denote a unique observation id for the\n        corresponding row of the long format array.\n    alt_id_array : 1D ndarray of ints.\n        Each element should denote a unique alternative id for the\n        corresponding row of the long format array.\n    choice_array : 1D ndarray of ints.\n        Each element should be a one or a zero. The values should denote a\n        whether or not the corresponding alternative in `alt_id_array` was\n        chosen by the observational unit in the corresponding row of\n        `obs_id_array.`\n    num_samples : int.\n        Denotes the number of bootstrap samples that need to be drawn.\n    seed : non-negative int or None, optional.\n        Denotes the random seed to be used in order to ensure reproducibility\n        of the bootstrap sample generation. Default is None. If None, no seed\n        will be used and the generation of the bootstrap samples will (in\n        general) not be reproducible.\n\n\n    Returns\n    -------\n    ids_per_sample : 2D ndarray.\n        Each row represents a complete bootstrap sample. Each column denotes a\n        selected bootstrap observation that comprises the bootstrap sample. The\n        elements of the array denote the observation ids of the chosen\n        observational units.\n    \"\"\"\n    # Determine the units of observation that chose each alternative.\n    chosen_alts_to_obs_ids =\\\n        relate_obs_ids_to_chosen_alts(obs_id_array, alt_id_array, choice_array)\n\n    # Determine the number of unique units of observation per group and overall\n    num_obs_per_group, tot_num_obs =\\\n        get_num_obs_choosing_each_alternative(chosen_alts_to_obs_ids)\n\n    # Initialize the array that will store the observation ids for each sample\n    ids_per_sample = np.empty((num_samples, tot_num_obs), dtype=float)\n\n    if seed is not None:\n        # Check the validity of the seed argument.\n        if not isinstance(seed, int):\n            msg = \"`boot_seed` MUST be an int.\"\n            raise ValueError(msg)\n\n        # If desiring reproducibility, set the random seed within numpy\n        np.random.seed(seed)\n\n    # Initialize a variable to keep track of what column we're on.\n    col_idx = 0\n    for alt_id in num_obs_per_group:\n        # Get the set of observations that chose the current alternative.\n        relevant_ids = chosen_alts_to_obs_ids[alt_id]\n        # Determine the number of needed resampled ids.\n        resample_size = num_obs_per_group[alt_id]\n        # Resample, with replacement, observations who chose this alternative.\n        current_ids = (np.random.choice(relevant_ids,\n                                        size=resample_size * num_samples,\n                                        replace=True)\n                                .reshape((num_samples, resample_size)))\n        # Determine the last column index to use when storing the resampled ids\n        end_col = col_idx + resample_size\n        # Assign the sampled ids to the correct columns of ids_per_sample\n        ids_per_sample[:, col_idx:end_col] = current_ids\n        # Update the column index\n        col_idx += resample_size\n\n    # Return the resampled observation ids.\n    return ids_per_sample", "response": "This function creates the cross - sectional bootstrap samples for the given observation and alternative ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_bootstrap_id_array(obs_id_per_sample):\n    # Determine the shape of the object to be returned.\n    n_rows, n_cols = obs_id_per_sample.shape\n    # Create the array of bootstrap ids.\n    bootstrap_id_array =\\\n        np.tile(np.arange(n_cols) + 1, n_rows).reshape((n_rows, n_cols))\n    # Return the desired object\n    return bootstrap_id_array", "response": "Creates a 2D array that contains the bootstrap ids for each replication of each observation that is a set of bootstrap samples."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a dictionary that will be used to group the items in a new node by the observation id.", "response": "def create_deepcopied_groupby_dict(orig_df, obs_id_col):\n    \"\"\"\n    Will create a dictionary where each key corresponds to a unique value in\n    `orig_df[obs_id_col]` and each value corresponds to all of the rows of\n    `orig_df` where `orig_df[obs_id_col] == key`.\n\n    Parameters\n    ----------\n    orig_df : pandas DataFrame.\n        Should be long-format dataframe containing the data used to estimate\n        the desired choice model.\n    obs_id_col : str.\n        Should be a column name within `orig_df`. Should denote the original\n        observation id column.\n\n    Returns\n    -------\n    groupby_dict : dict.\n        Each key will be a unique value in `orig_df[obs_id_col]` and each value\n        will be the rows of `orig_df` where `orig_df[obs_id_col] == key`.\n    \"\"\"\n    # Get the observation id values\n    obs_id_vals = orig_df[obs_id_col].values\n    # Get the unique observation ids\n    unique_obs_ids = np.unique(obs_id_vals)\n    # Initialize the dictionary to be returned.\n    groupby_dict = {}\n    # Populate the dictionary with dataframes for each individual.\n    for obs_id in unique_obs_ids:\n        # Filter out only the rows corresponding to the current observation id.\n        desired_rows = obs_id_vals == obs_id\n        # Add the desired dataframe to the dictionary.\n        groupby_dict[obs_id] = orig_df.loc[desired_rows].copy(deep=True)\n\n    # Return the desired object.\n    return groupby_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_column_existence(col_name, df, presence=True):\n    if presence:\n        if col_name not in df.columns:\n            msg = \"Ensure that `{}` is in `df.columns`.\"\n            raise ValueError(msg.format(col_name))\n    else:\n        if col_name in df.columns:\n            msg = \"Ensure that `{}` is not in `df.columns`.\"\n            raise ValueError(msg.format(col_name))\n    return None", "response": "Checks whether or not col_name is in df and raises a helpful error msg\n            if the desired condition is not met."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that all ids in resampled_obs_ids are in orig_obs_id_array. Raises a helpful ValueError if not.", "response": "def ensure_resampled_obs_ids_in_df(resampled_obs_ids, orig_obs_id_array):\n    \"\"\"\n    Checks whether all ids in `resampled_obs_ids` are in `orig_obs_id_array`.\n    Raises a helpful ValueError if not.\n\n    Parameters\n    ----------\n    resampled_obs_ids : 1D ndarray of ints.\n        Should contain the observation ids of the observational units that will\n        be used in the current bootstrap sample.\n    orig_obs_id_array : 1D ndarray of ints.\n        Should countain the observation ids of the observational units in the\n        original dataframe containing the data for this model.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if not np.in1d(resampled_obs_ids, orig_obs_id_array).all():\n        msg =\\\n            \"All values in `resampled_obs_ids` MUST be in `orig_obs_id_array`.\"\n        raise ValueError(msg)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_bootstrap_dataframe(orig_df,\n                               obs_id_col,\n                               resampled_obs_ids_1d,\n                               groupby_dict,\n                               boot_id_col=\"bootstrap_id\"):\n    \"\"\"\n    Will create the altered dataframe of data needed to estimate a choice model\n    with the particular observations that belong to the current bootstrap\n    sample.\n\n    Parameters\n    ----------\n    orig_df : pandas DataFrame.\n        Should be long-format dataframe containing the data used to estimate\n        the desired choice model.\n    obs_id_col : str.\n        Should be a column name within `orig_df`. Should denote the original\n        observation id column.\n    resampled_obs_ids_1d : 1D ndarray of ints.\n        Each value should represent the alternative id of a given bootstrap\n        replicate.\n    groupby_dict : dict.\n        Each key will be a unique value in `orig_df[obs_id_col]` and each value\n        will be the rows of `orig_df` where `orig_df[obs_id_col] == key`.\n    boot_id_col : str, optional.\n        Denotes the new column that will be created to specify the bootstrap\n        observation ids for choice model estimation.\n\n    Returns\n    -------\n    bootstrap_df : pandas Dataframe.\n        Will contain all the same columns as `orig_df` as well as the\n        additional `boot_id_col`. For each value in `resampled_obs_ids_1d`,\n        `bootstrap_df` will contain the long format rows from `orig_df` that\n        have the given observation id.\n    \"\"\"\n    # Check the validity of the passed arguments.\n    check_column_existence(obs_id_col, orig_df, presence=True)\n    check_column_existence(boot_id_col, orig_df, presence=False)\n    # Alias the observation id column\n    obs_id_values = orig_df[obs_id_col].values\n    # Check the validity of the resampled observation ids.\n    ensure_resampled_obs_ids_in_df(resampled_obs_ids_1d, obs_id_values)\n\n    # Initialize a list to store the component dataframes that will be\n    # concatenated to form the final bootstrap_df\n    component_dfs = []\n\n    # Populate component_dfs\n    for boot_id, obs_id in enumerate(resampled_obs_ids_1d):\n        # Extract the dataframe that we desire.\n        extracted_df = groupby_dict[obs_id].copy()\n        # Add the bootstrap id value.\n        extracted_df[boot_id_col] = boot_id + 1\n        # Store the component dataframe\n        component_dfs.append(extracted_df)\n\n    # Create and return the desired dataframe.\n    bootstrap_df = pd.concat(component_dfs, axis=0, ignore_index=True)\n    return bootstrap_df", "response": "Create a dataframe that contains the data needed to estimate a choice model for the current bootstrap language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting all the names to be displayed for the estimated parameters.", "response": "def get_param_names(model_obj):\n    \"\"\"\n    Extracts all the names to be displayed for the estimated parameters.\n\n    Parameters\n    ----------\n    model_obj : an instance of an MNDC object.\n        Should have the following attributes:\n        `['ind_var_names', 'intercept_names', 'shape_names', 'nest_names']`.\n\n    Returns\n    -------\n    all_names : list of strings.\n        There will be one element for each estimated parameter. The order of\n        the parameter names will be\n        `['nest_parameters', 'shape_parameters', 'outside_intercepts',\n          'index_coefficients']`.\n    \"\"\"\n    # Get the index coefficient names\n    all_names = deepcopy(model_obj.ind_var_names)\n    # Add the intercept names if any exist\n    if model_obj.intercept_names is not None:\n        all_names = model_obj.intercept_names + all_names\n    # Add the shape names if any exist\n    if model_obj.shape_names is not None:\n        all_names = model_obj.shape_names + all_names\n    # Add the nest names if any exist\n    if model_obj.nest_names is not None:\n        all_names = model_obj.nest_names + all_names\n    return all_names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the parameter list for use with predict.", "response": "def get_param_list_for_prediction(model_obj, replicates):\n    \"\"\"\n    Create the `param_list` argument for use with `model_obj.predict`.\n\n    Parameters\n    ----------\n    model_obj : an instance of an MNDC object.\n        Should have the following attributes:\n        `['ind_var_names', 'intercept_names', 'shape_names', 'nest_names']`.\n        This model should have already undergone a complete estimation process.\n        I.e. its `fit_mle` method should have been called without\n        `just_point=True`.\n    replicates : 2D ndarray.\n        Should represent the set of parameter values that we now wish to\n        partition for use with the `model_obj.predict` method.\n\n    Returns\n    -------\n    param_list : list.\n        Contains four elements, each being a numpy array. Either all of the\n        arrays should be 1D or all of the arrays should be 2D. If 2D, the\n        arrays should have the same number of columns. Each column being a\n        particular set of parameter values that one wants to predict with.\n        The first element in the list should be the index coefficients. The\n        second element should contain the 'outside' intercept parameters if\n        there are any, or None otherwise. The third element should contain\n        the shape parameters if there are any or None otherwise. The fourth\n        element should contain the nest coefficients if there are any or\n        None otherwise. Default == None.\n    \"\"\"\n    # Check the validity of the passed arguments\n    ensure_samples_is_ndim_ndarray(replicates, ndim=2, name='replicates')\n    # Determine the number of index coefficients, outside intercepts,\n    # shape parameters, and nest parameters\n    num_idx_coefs = len(model_obj.ind_var_names)\n\n    intercept_names = model_obj.intercept_names\n    num_outside_intercepts =\\\n        0 if intercept_names is None else len(intercept_names)\n\n    shape_names = model_obj.shape_names\n    num_shapes = 0 if shape_names is None else len(shape_names)\n\n    nest_names = model_obj.nest_names\n    num_nests = 0 if nest_names is None else len(nest_names)\n\n    parameter_numbers =\\\n        [num_nests, num_shapes, num_outside_intercepts, num_idx_coefs]\n    current_idx = 0\n    param_list = []\n    for param_num in parameter_numbers:\n        if param_num == 0:\n            param_list.insert(0, None)\n            continue\n        upper_idx = current_idx + param_num\n        param_list.insert(0, replicates[:, current_idx:upper_idx].T)\n        current_idx += param_num\n    return param_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the bootstrap replicates for one s given model and dataset.", "response": "def generate_bootstrap_replicates(self,\n                                      num_samples,\n                                      mnl_obj=None,\n                                      mnl_init_vals=None,\n                                      mnl_fit_kwargs=None,\n                                      extract_init_vals=None,\n                                      print_res=False,\n                                      method=\"BFGS\",\n                                      loss_tol=1e-06,\n                                      gradient_tol=1e-06,\n                                      maxiter=1000,\n                                      ridge=None,\n                                      constrained_pos=None,\n                                      boot_seed=None,\n                                      weights=None):\n        \"\"\"\n        Generates the bootstrap replicates for one's given model and dataset.\n\n        Parameters\n        ----------\n        num_samples : positive int.\n            Specifies the number of bootstrap samples that are to be drawn.\n        mnl_obj : an instance of pylogit.MNL or None, optional.\n            Should be the MNL model object that is used to provide starting\n            values for the final model being estimated. If None, then one's\n            final model should be an MNL model. Default == None.\n        mnl_init_vals : 1D ndarray or None, optional.\n            If the model that is being estimated is not an MNL, then\n            `mnl_init_val` should be passed. Should contain the values used to\n            begin the estimation process for the MNL model that is used to\n            provide starting values for our desired model. Default == None.\n        mnl_fit_kwargs : dict or None.\n            If the model that is being estimated is not an MNL, then\n            `mnl_fit_kwargs` should be passed.\n        extract_init_vals : callable or None, optional.\n            Should accept 3 arguments, in the following order. First, it should\n            accept `orig_model_obj`. Second, it should accept a pandas Series\n            of estimated parameters from the MNL model. The Series' index will\n            be the names of the coefficients from `mnl_names`. Thirdly, it\n            should accept an int denoting the number of parameters in the final\n            choice model. The callable should return a 1D ndarray of starting\n            values for the final choice model. Default == None.\n        print_res : bool, optional.\n            Determines whether the timing and initial and final log likelihood\n            results will be printed as they they are determined.\n            Default `== True`.\n        method : str, optional.\n            Should be a valid string for scipy.optimize.minimize. Determines\n            the optimization algorithm that is used for this problem.\n            Default `== 'bfgs'`.\n        loss_tol : float, optional.\n            Determines the tolerance on the difference in objective function\n            values from one iteration to the next that is needed to determine\n            convergence. Default `== 1e-06`.\n        gradient_tol : float, optional.\n            Determines the tolerance on the difference in gradient values from\n            one iteration to the next which is needed to determine convergence.\n            Default `== 1e-06`.\n        maxiter : int, optional.\n            Determines the maximum number of iterations used by the optimizer.\n            Default `== 1000`.\n        ridge : int, float, long, or None, optional.\n            Determines whether or not ridge regression is performed. If a\n            scalar is passed, then that scalar determines the ridge penalty for\n            the optimization. The scalar should be greater than or equal to\n            zero. Default `== None`.\n        constrained_pos : list or None, optional.\n            Denotes the positions of the array of estimated parameters that are\n            not to change from their initial values. If a list is passed, the\n            elements are to be integers where no such integer is greater than\n            `init_vals.size.` Default == None.\n        boot_seed = non-negative int or None, optional.\n            Denotes the random seed to be used when generating the bootstrap\n            samples. If None, the sample generation process will generally be\n            non-reproducible. Default == None.\n        weights : 1D ndarray or None, optional.\n            Allows for the calculation of weighted log-likelihoods. The weights\n            can represent various things. In stratified samples, the weights\n            may be the proportion of the observations in a given strata for a\n            sample in relation to the proportion of observations in that strata\n            in the population. In latent class models, the weights may be the\n            probability of being a particular class.\n\n        Returns\n        -------\n        None. Will store the bootstrap replicates on the\n        `self.bootstrap_replicates` attribute.\n        \"\"\"\n        print(\"Generating Bootstrap Replicates\")\n        print(time.strftime(\"%a %m-%d-%Y %I:%M%p\"))\n        sys.stdout.flush()\n        # Check the passed arguments for validity.\n\n        # Create an array of the observation ids\n        obs_id_array = self.model_obj.data[self.model_obj.obs_id_col].values\n        # Alias the alternative IDs and the Choice Array\n        alt_id_array = self.model_obj.alt_IDs\n        choice_array = self.model_obj.choices\n\n        # Determine how many parameters are being estimated.\n        num_params = self.mle_params.shape[0]\n\n        # Figure out which observations are in each bootstrap sample.\n        obs_id_per_sample =\\\n            bs.create_cross_sectional_bootstrap_samples(obs_id_array,\n                                                        alt_id_array,\n                                                        choice_array,\n                                                        num_samples,\n                                                        seed=boot_seed)\n\n        # Get the dictionary of sub-dataframes for each observation id\n        dfs_by_obs_id =\\\n            bs.create_deepcopied_groupby_dict(self.model_obj.data,\n                                              self.model_obj.obs_id_col)\n\n        # Create a column name for the bootstrap id columns.\n        boot_id_col = \"bootstrap_id\"\n\n        # Initialize an array to store the bootstrapped point estimates.\n        point_estimates = np.empty((num_samples, num_params), dtype=float)\n\n        # Get keyword arguments for final model estimation with new data.\n        fit_kwargs = {\"print_res\": print_res,\n                      \"method\": method,\n                      \"loss_tol\": loss_tol,\n                      \"gradient_tol\": gradient_tol,\n                      \"maxiter\": maxiter,\n                      \"ridge\": ridge,\n                      \"constrained_pos\": constrained_pos,\n                      \"just_point\": True}\n\n        # Get the specification and name dictionary of the MNL model.\n        mnl_spec = None if mnl_obj is None else mnl_obj.specification\n        mnl_names = None if mnl_obj is None else mnl_obj.name_spec\n\n        # Create an iterable for iteration\n        iterable_for_iteration = PROGRESS(xrange(num_samples),\n                                          desc=\"Creating Bootstrap Replicates\",\n                                          total=num_samples)\n\n        # Iterate through the bootstrap samples and perform the MLE\n        for row in iterable_for_iteration:\n            # Get the bootstrapped dataframe\n            bootstrap_df =\\\n                bs.create_bootstrap_dataframe(self.model_obj.data,\n                                              self.model_obj.obs_id_col,\n                                              obs_id_per_sample[row, :],\n                                              dfs_by_obs_id,\n                                              boot_id_col=boot_id_col)\n\n            # Go through the necessary estimation routine to bootstrap the MLE.\n            current_results =\\\n                retrieve_point_est(self.model_obj,\n                                   bootstrap_df,\n                                   boot_id_col,\n                                   num_params,\n                                   mnl_spec,\n                                   mnl_names,\n                                   mnl_init_vals,\n                                   mnl_fit_kwargs,\n                                   extract_init_vals=extract_init_vals,\n                                   **fit_kwargs)\n\n            # Store the bootstrapped point estimate.\n            point_estimates[row] = current_results[\"x\"]\n\n        # Store the point estimates as a pandas dataframe\n        self.bootstrap_replicates =\\\n            pd.DataFrame(point_estimates, columns=self.mle_params.index)\n\n        # Print a 'finished' message for users\n        print(\"Finished Generating Bootstrap Replicates\")\n        print(time.strftime(\"%a %m-%d-%Y %I:%M%p\"))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_jackknife_replicates(self,\n                                      mnl_obj=None,\n                                      mnl_init_vals=None,\n                                      mnl_fit_kwargs=None,\n                                      extract_init_vals=None,\n                                      print_res=False,\n                                      method=\"BFGS\",\n                                      loss_tol=1e-06,\n                                      gradient_tol=1e-06,\n                                      maxiter=1000,\n                                      ridge=None,\n                                      constrained_pos=None):\n        \"\"\"\n        Generates the jackknife replicates for one's given model and dataset.\n\n        Parameters\n        ----------\n        mnl_obj : an instance of pylogit.MNL or None, optional.\n            Should be the MNL model object that is used to provide starting\n            values for the final model being estimated. If None, then one's\n            final model should be an MNL model. Default == None.\n        mnl_init_vals : 1D ndarray or None, optional.\n            If the model that is being estimated is not an MNL, then\n            `mnl_init_val` should be passed. Should contain the values used to\n            begin the estimation process for the MNL model that is used to\n            provide starting values for our desired model. Default == None.\n        mnl_fit_kwargs : dict or None.\n            If the model that is being estimated is not an MNL, then\n            `mnl_fit_kwargs` should be passed.\n        extract_init_vals : callable or None, optional.\n            Should accept 3 arguments, in the following order. First, it should\n            accept `orig_model_obj`. Second, it should accept a pandas Series\n            of estimated parameters from the MNL model. The Series' index will\n            be the names of the coefficients from `mnl_names`. Thirdly, it\n            should accept an int denoting the number of parameters in the final\n            choice model. The callable should return a 1D ndarray of starting\n            values for the final choice model. Default == None.\n        print_res : bool, optional.\n            Determines whether the timing and initial and final log likelihood\n            results will be printed as they they are determined.\n            Default `== True`.\n        method : str, optional.\n            Should be a valid string for scipy.optimize.minimize. Determines\n            the optimization algorithm that is used for this problem.\n            Default `== 'bfgs'`.\n        loss_tol : float, optional.\n            Determines the tolerance on the difference in objective function\n            values from one iteration to the next that is needed to determine\n            convergence. Default `== 1e-06`.\n        gradient_tol : float, optional.\n            Determines the tolerance on the difference in gradient values from\n            one iteration to the next which is needed to determine convergence.\n            Default `== 1e-06`.\n        maxiter : int, optional.\n            Determines the maximum number of iterations used by the optimizer.\n            Default `== 1000`.\n        ridge : int, float, long, or None, optional.\n            Determines whether or not ridge regression is performed. If a\n            scalar is passed, then that scalar determines the ridge penalty for\n            the optimization. The scalar should be greater than or equal to\n            zero. Default `== None`.\n        constrained_pos : list or None, optional.\n            Denotes the positions of the array of estimated parameters that are\n            not to change from their initial values. If a list is passed, the\n            elements are to be integers where no such integer is greater than\n            `init_vals.size.` Default == None.\n\n        Returns\n        -------\n        None. Will store the bootstrap replicates on the\n        `self.bootstrap_replicates` attribute.\n        \"\"\"\n        print(\"Generating Jackknife Replicates\")\n        print(time.strftime(\"%a %m-%d-%Y %I:%M%p\"))\n        sys.stdout.flush()\n        # Take note of the observation id column that is to be used\n        obs_id_col = self.model_obj.obs_id_col\n\n        # Get the array of original observation ids\n        orig_obs_id_array =\\\n            self.model_obj.data[obs_id_col].values\n\n        # Get an array of the unique observation ids.\n        unique_obs_ids = np.sort(np.unique(orig_obs_id_array))\n\n        # Determine how many observations are in one's dataset.\n        num_obs = unique_obs_ids.size\n        # Determine how many parameters are being estimated.\n        num_params = self.mle_params.size\n\n        # Get keyword arguments for final model estimation with new data.\n        fit_kwargs = {\"print_res\": print_res,\n                      \"method\": method,\n                      \"loss_tol\": loss_tol,\n                      \"gradient_tol\": gradient_tol,\n                      \"maxiter\": maxiter,\n                      \"ridge\": ridge,\n                      \"constrained_pos\": constrained_pos,\n                      \"just_point\": True}\n\n        # Get the specification and name dictionary of the MNL model.\n        mnl_spec = None if mnl_obj is None else mnl_obj.specification\n        mnl_names = None if mnl_obj is None else mnl_obj.name_spec\n\n        # Initialize the array of jackknife replicates\n        point_replicates = np.empty((num_obs, num_params), dtype=float)\n\n        # Create an iterable for iteration\n        iterable_for_iteration = PROGRESS(enumerate(unique_obs_ids),\n                                          desc=\"Creating Jackknife Replicates\",\n                                          total=unique_obs_ids.size)\n\n        # Populate the array of jackknife replicates\n        for pos, obs_id in iterable_for_iteration:\n            # Create the dataframe without the current observation\n            new_df = self.model_obj.data.loc[orig_obs_id_array != obs_id]\n            # Get the point estimate for this new dataset\n            current_results =\\\n                retrieve_point_est(self.model_obj,\n                                   new_df,\n                                   obs_id_col,\n                                   num_params,\n                                   mnl_spec,\n                                   mnl_names,\n                                   mnl_init_vals,\n                                   mnl_fit_kwargs,\n                                   extract_init_vals=extract_init_vals,\n                                   **fit_kwargs)\n            # Store the estimated parameters\n            point_replicates[pos] = current_results['x']\n\n        # Store the jackknife replicates as a pandas dataframe\n        self.jackknife_replicates =\\\n            pd.DataFrame(point_replicates, columns=self.mle_params.index)\n        # Print a 'finished' message for users\n        print(\"Finished Generating Jackknife Replicates\")\n        print(time.strftime(\"%a %m-%d-%Y %I:%M%p\"))\n        return None", "response": "This function generates the jackknife replicates for one s given model and dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the log - likelihoods for one s replicates given one s dataset.", "response": "def calc_log_likes_for_replicates(self,\n                                      replicates='bootstrap',\n                                      num_draws=None,\n                                      seed=None):\n        \"\"\"\n        Calculate the log-likelihood value of one's replicates, given one's\n        dataset.\n\n        Parameters\n        ----------\n        replicates : str in {'bootstrap', 'jackknife'}.\n            Denotes which set of replicates should have their log-likelihoods\n            calculated.\n        num_draws : int greater than zero or None, optional.\n            Denotes the number of random draws for mixed logit estimation. If\n            None, then no random draws will be made. Default == None.\n        seed : int greater than zero or None, optional.\n            Denotes the random seed to be used for mixed logit estimation.\n            If None, then no random seed will be set. Default == None.\n\n        Returns\n        -------\n        log_likelihoods : 1D ndarray.\n            Each element stores the log-likelihood of the associated parameter\n            values on the model object's dataset. The log-likelihoods are also\n            stored on the `replicates + '_log_likelihoods'` attribute.\n        \"\"\"\n        # Check the validity of the kwargs\n        ensure_replicates_kwarg_validity(replicates)\n\n        # Get the desired type of replicates\n        replicate_vec = getattr(self, replicates + \"_replicates\").values\n\n        # Determine the choice column\n        choice_col = self.model_obj.choice_col\n\n        # Split the control flow based on whether we're using a Nested Logit\n        current_model_type = self.model_obj.model_type\n        non_2d_predictions =\\\n            [model_type_to_display_name[\"Nested Logit\"],\n             model_type_to_display_name[\"Mixed Logit\"]]\n        if current_model_type not in non_2d_predictions:\n            # Get the param list for this set of replicates\n            param_list =\\\n                get_param_list_for_prediction(self.model_obj, replicate_vec)\n\n            # Get the 'chosen_probs' using the desired set of replicates\n            chosen_probs =\\\n                self.model_obj.predict(self.model_obj.data,\n                                       param_list=param_list,\n                                       return_long_probs=False,\n                                       choice_col=choice_col)\n        else:\n            # Initialize a list of chosen probs\n            chosen_probs_list = []\n\n            # Create an iterable for iteration\n            iterable_for_iteration = PROGRESS(xrange(replicate_vec.shape[0]),\n                                              desc=\"Calculate Gradient Norms\",\n                                              total=replicate_vec.shape[0])\n\n            # Populate the list of chosen probabilities for each vector of\n            # parameter values\n            for idx in iterable_for_iteration:\n                # Get the param list for this set of replicates\n                param_list =\\\n                    get_param_list_for_prediction(self.model_obj,\n                                                  replicate_vec[idx][None, :])\n                # Use 1D parameters in the prediction function\n                param_list =\\\n                    [x.ravel() if x is not None else x for x in param_list]\n\n                # Get the 'chosen_probs' using the desired set of replicates\n                chosen_probs =\\\n                    self.model_obj.predict(self.model_obj.data,\n                                           param_list=param_list,\n                                           return_long_probs=False,\n                                           choice_col=choice_col,\n                                           num_draws=num_draws,\n                                           seed=seed)\n\n                # store those chosen prob_results\n                chosen_probs_list.append(chosen_probs[:, None])\n\n            # Get the final array of chosen probs\n            chosen_probs = np.concatenate(chosen_probs_list, axis=1)\n\n        # Calculate the log_likelihood\n        log_likelihoods = np.log(chosen_probs).sum(axis=0)\n\n        # Store the log-likelihood values\n        attribute_name = replicates + \"_log_likelihoods\"\n        log_like_series = pd.Series(log_likelihoods, name=attribute_name)\n        setattr(self, attribute_name, log_like_series)\n        return log_likelihoods"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the Euclidean - norm of the gradient of one s replicates given the model object s dataset.", "response": "def calc_gradient_norm_for_replicates(self,\n                                          replicates='bootstrap',\n                                          ridge=None,\n                                          constrained_pos=None,\n                                          weights=None):\n        \"\"\"\n        Calculate the Euclidean-norm of the gradient of one's replicates, given\n        one's dataset.\n\n        Parameters\n        ----------\n        replicates : str in {'bootstrap', 'jackknife'}.\n            Denotes which set of replicates should have their log-likelihoods\n            calculated.\n        ridge : float or None, optional.\n            Denotes the ridge penalty used when estimating the replicates, and\n            to be used when calculating the gradient. If None, no ridge penalty\n            is used. Default == None.\n        constrained_pos : list or None, optional.\n            Denotes the positions of the array of estimated parameters that are\n            not to change from their initial values. If a list is passed, the\n            elements are to be integers where no such integer is greater than\n            `self.mle_params` Default == None.\n        weights : 1D ndarray or None, optional.\n            Allows for the calculation of weighted log-likelihoods. The weights\n            can represent various things. In stratified samples, the weights\n            may be the proportion of the observations in a given strata for a\n            sample in relation to the proportion of observations in that strata\n            in the population. In latent class models, the weights may be the\n            probability of being a particular class.\n\n        Returns\n        -------\n        log_likelihoods : 1D ndarray.\n            Each element stores the log-likelihood of the associated parameter\n            values on the model object's dataset. The log-likelihoods are also\n            stored on the `replicates + '_log_likelihoods'` attribute.\n        \"\"\"\n        # Check the validity of the kwargs\n        ensure_replicates_kwarg_validity(replicates)\n        # Create the estimation object\n        estimation_obj =\\\n            create_estimation_obj(self.model_obj,\n                                  self.mle_params.values,\n                                  ridge=ridge,\n                                  constrained_pos=constrained_pos,\n                                  weights=weights)\n        # Prepare the estimation object to calculate the gradients\n        if hasattr(estimation_obj, \"set_derivatives\"):\n            estimation_obj.set_derivatives()\n        # Get the array of parameter replicates\n        replicate_array = getattr(self, replicates + \"_replicates\").values\n        # Determine the number of replicates\n        num_reps = replicate_array.shape[0]\n        # Initialize an empty array to store the gradient norms\n        gradient_norms = np.empty((num_reps,), dtype=float)\n\n        # Create an iterable for iteration\n        iterable_for_iteration = PROGRESS(xrange(num_reps),\n                                          desc=\"Calculating Gradient Norms\",\n                                          total=num_reps)\n\n        # Iterate through the rows of the replicates and calculate and store\n        # the gradient norm for each replicated parameter vector.\n        for row in iterable_for_iteration:\n            current_params = replicate_array[row]\n            gradient = estimation_obj.convenience_calc_gradient(current_params)\n            gradient_norms[row] = np.linalg.norm(gradient)\n        return gradient_norms"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_percentile_interval(self, conf_percentage):\n        # Get the alpha % that corresponds to the given confidence percentage.\n        alpha = bc.get_alpha_from_conf_percentage(conf_percentage)\n        # Create the column names for the dataframe of confidence intervals\n        single_column_names =\\\n            ['{:.3g}%'.format(alpha / 2.0),\n             '{:.3g}%'.format(100 - alpha / 2.0)]\n        # Calculate the desired confidence intervals.\n        conf_intervals =\\\n            bc.calc_percentile_interval(self.bootstrap_replicates.values,\n                                        conf_percentage)\n        # Store the desired confidence intervals\n        self.percentile_interval =\\\n            pd.DataFrame(conf_intervals.T,\n                         index=self.mle_params.index,\n                         columns=single_column_names)\n        return None", "response": "Calculates the percentile bootstrap confidence intervals for one s model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the Bias - Corrected and Accelerated bootstrap confidence intervals for one s model.", "response": "def calc_bca_interval(self, conf_percentage):\n        \"\"\"\n        Calculates Bias-Corrected and Accelerated (BCa) Bootstrap Confidence\n        Intervals for one's model.\n\n        Parameters\n        ----------\n        conf_percentage : scalar in the interval (0.0, 100.0).\n            Denotes the confidence-level for the returned endpoints. For\n            instance, to calculate a 95% confidence interval, pass `95`.\n\n        Returns\n        -------\n        None. Will store the BCa intervals as `self.abc_interval`.\n\n        Notes\n        -----\n        Must have all ready called `self.generate_bootstrap_replicates` and\n        `self.generate_jackknife_replicates`.\n        \"\"\"\n        # Get the alpha % that corresponds to the given confidence percentage.\n        alpha = bc.get_alpha_from_conf_percentage(conf_percentage)\n        # Create the column names for the dataframe of confidence intervals\n        single_column_names =\\\n            ['{:.3g}%'.format(alpha / 2.0),\n             '{:.3g}%'.format(100 - alpha / 2.0)]\n        # Bundle the arguments needed to create the desired confidence interval\n        args = [self.bootstrap_replicates.values,\n                self.jackknife_replicates.values,\n                self.mle_params.values,\n                conf_percentage]\n        # Calculate the BCa confidence intervals.\n        conf_intervals = bc.calc_bca_interval(*args)\n        # Store the BCa confidence intervals.\n        self.bca_interval = pd.DataFrame(conf_intervals.T,\n                                         index=self.mle_params.index,\n                                         columns=single_column_names)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_abc_interval(self,\n                          conf_percentage,\n                          init_vals,\n                          epsilon=0.001,\n                          **fit_kwargs):\n        \"\"\"\n        Calculates Approximate Bootstrap Confidence Intervals for one's model.\n\n        Parameters\n        ----------\n        conf_percentage : scalar in the interval (0.0, 100.0).\n            Denotes the confidence-level for the returned endpoints. For\n            instance, to calculate a 95% confidence interval, pass `95`.\n        init_vals : 1D ndarray.\n            The initial values used to estimate the one's choice model.\n        epsilon : positive float, optional.\n            Should denote the 'very small' value being used to calculate the\n            desired finite difference approximations to the various influence\n            functions. Should be close to zero.\n            Default == sys.float_info.epsilon.\n        fit_kwargs : additional keyword arguments, optional.\n            Should contain any additional kwargs used to alter the default\n            behavior of `model_obj.fit_mle` and thereby enforce conformity with\n            how the MLE was obtained. Will be passed directly to\n            `model_obj.fit_mle`.\n\n        Returns\n        -------\n        None. Will store the ABC intervals as `self.abc_interval`.\n        \"\"\"\n        print(\"Calculating Approximate Bootstrap Confidence (ABC) Intervals\")\n        print(time.strftime(\"%a %m-%d-%Y %I:%M%p\"))\n        sys.stdout.flush()\n        # Get the alpha % that corresponds to the given confidence percentage.\n        alpha = bc.get_alpha_from_conf_percentage(conf_percentage)\n        # Create the column names for the dataframe of confidence intervals\n        single_column_names =\\\n            ['{:.3g}%'.format(alpha / 2.0),\n             '{:.3g}%'.format(100 - alpha / 2.0)]\n        # Calculate the ABC confidence intervals\n        conf_intervals =\\\n            abc.calc_abc_interval(self.model_obj,\n                                  self.mle_params.values,\n                                  init_vals,\n                                  conf_percentage,\n                                  epsilon=epsilon,\n                                  **fit_kwargs)\n        # Store the ABC confidence intervals\n        self.abc_interval = pd.DataFrame(conf_intervals.T,\n                                         index=self.mle_params.index,\n                                         columns=single_column_names)\n        return None", "response": "Calculates the Approximate Bootstrap Confidence Intervals for one s choice model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the confidence intervals for the given set of endpoints.", "response": "def calc_conf_intervals(self,\n                            conf_percentage,\n                            interval_type='all',\n                            init_vals=None,\n                            epsilon=abc.EPSILON,\n                            **fit_kwargs):\n        \"\"\"\n        Calculates percentile, bias-corrected and accelerated, and approximate\n        bootstrap confidence intervals.\n\n        Parameters\n        ----------\n        conf_percentage : scalar in the interval (0.0, 100.0).\n            Denotes the confidence-level for the returned endpoints. For\n            instance, to calculate a 95% confidence interval, pass `95`.\n        interval_type : str in {'all', 'pi', 'bca', 'abc'}, optional.\n            Denotes the type of confidence intervals that should be calculated.\n            'all' results in all types of confidence intervals being\n            calculated. 'pi' means 'percentile intervals', 'bca' means\n            'bias-corrected and accelerated', and 'abc' means 'approximate\n            bootstrap confidence' intervals. Default == 'all'.\n        init_vals : 1D ndarray.\n            The initial values used to estimate the one's choice model.\n        epsilon : positive float, optional.\n            Should denote the 'very small' value being used to calculate the\n            desired finite difference approximations to the various influence\n            functions for the 'abc' intervals. Should be close to zero.\n            Default == sys.float_info.epsilon.\n        fit_kwargs : additional keyword arguments, optional.\n            Should contain any additional kwargs used to alter the default\n            behavior of `model_obj.fit_mle` and thereby enforce conformity with\n            how the MLE was obtained. Will be passed directly to\n            `model_obj.fit_mle` when calculating the 'abc' intervals.\n\n        Returns\n        -------\n        None. Will store the confidence intervals on their respective model\n        objects: `self.percentile_interval`, `self.bca_interval`,\n        `self.abc_interval`, or all of these objects.\n        \"\"\"\n        if interval_type == 'pi':\n            self.calc_percentile_interval(conf_percentage)\n        elif interval_type == 'bca':\n            self.calc_bca_interval(conf_percentage)\n        elif interval_type == 'abc':\n            self.calc_abc_interval(conf_percentage,\n                                   init_vals,\n                                   epsilon=epsilon,\n                                   **fit_kwargs)\n        elif interval_type == 'all':\n            print(\"Calculating Percentile Confidence Intervals\")\n            sys.stdout.flush()\n            self.calc_percentile_interval(conf_percentage)\n\n            print(\"Calculating BCa Confidence Intervals\")\n            sys.stdout.flush()\n            self.calc_bca_interval(conf_percentage)\n\n            # Note we don't print a user message here since that is done in\n            # self.calc_abc_interval().\n            self.calc_abc_interval(conf_percentage,\n                                   init_vals,\n                                   epsilon=epsilon,\n                                   **fit_kwargs)\n            # Get the alpha % for the given confidence percentage.\n            alpha = bc.get_alpha_from_conf_percentage(conf_percentage)\n            # Get lists of the interval type names and the endpoint names\n            interval_type_names = ['percentile_interval',\n                                   'BCa_interval',\n                                   'ABC_interval']\n            endpoint_names = ['{:.3g}%'.format(alpha / 2.0),\n                              '{:.3g}%'.format(100 - alpha / 2.0)]\n            # Create the column names for the dataframe of confidence intervals\n            multi_index_names =\\\n                list(itertools.product(interval_type_names, endpoint_names))\n            df_column_index = pd.MultiIndex.from_tuples(multi_index_names)\n            # Create the dataframe containing all confidence intervals\n            self.all_intervals = pd.concat([self.percentile_interval,\n                                            self.bca_interval,\n                                            self.abc_interval],\n                                           axis=1,\n                                           ignore_index=True)\n            # Store the column names for the combined confidence intervals\n            self.all_intervals.columns = df_column_index\n            self.all_intervals.index = self.mle_params.index\n        else:\n            msg =\\\n                \"interval_type MUST be in `['pi', 'bca', 'abc', 'all']`\"\n            raise ValueError(msg)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_param_vec(param_vec, rows_to_alts, design, return_all_types=False):\n    # Figure out how many parameters are in the index\n    num_index_coefs = design.shape[1]\n\n    # Isolate the initial shape parameters from the betas\n    betas = param_vec[-1 * num_index_coefs:]\n\n    # Get the remaining outside intercepts if there are any\n    remaining_idx = param_vec.shape[0] - num_index_coefs\n    if remaining_idx > 0:\n        intercepts = param_vec[:remaining_idx]\n    else:\n        intercepts = None\n\n    if return_all_types:\n        return None, None, intercepts, betas\n    else:\n        return None, intercepts, betas", "response": "Splits the parameter vector into two lists of n - tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _cloglog_utility_transform(systematic_utilities,\n                               alt_IDs,\n                               rows_to_alts,\n                               shape_params,\n                               intercept_params,\n                               intercept_ref_pos=None,\n                               *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        All elements should be ints, floats, or longs. Should contain the\n        systematic utilities of each observation per available alternative.\n        Note that this vector is formed by the dot product of the design matrix\n        with the vector of utility coefficients.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_alts : 2D scipy sparse matrix.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset. All\n        elements should be zeros or ones.\n    shape_params : None or 1D ndarray.\n        Should be None since the clog-log model has no shape parameters.\n    intercept_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. If J is the\n        total number of possible alternatives for the dataset being modeled,\n        there should be J-1 elements in the array. Use None if no outside\n        intercepts are being estimated.\n    intercept_ref_pos : int, or None, optional.\n        Specifies the index of the alternative, in the ordered array of unique\n        alternatives, that is not having its intercept parameter estimated (in\n        order to ensure identifiability). Should only be None if\n        `intercept_params` is None.\n\n    Returns\n    -------\n    transformations : 2D ndarray.\n        Should have shape `(systematic_utilities.shape[0], 1)`. The returned\n        array contains the transformed utility values for this model. All\n        elements will be ints, longs, or floats.\n    \"\"\"\n    # Calculate the data dependent part of the transformation\n    # Also, along the way, guard against numeric underflow or overflow\n    exp_v = np.exp(systematic_utilities)\n    # exp_v[np.isposinf(exp_v)] = max_comp_value\n\n    exp_exp_v = np.exp(exp_v)\n    # exp_exp_v[np.isposinf(exp_exp_v)] = max_comp_value\n\n    # Calculate the transformed systematic utilities\n    transformations = np.log(exp_exp_v - 1)\n    # Guard against underflow\n    transformations[np.isneginf(transformations)] = -1 * max_comp_value\n    # Guard against overflow when systematic utilities are moderately large\n    too_big_idx = np.where(systematic_utilities >= 3.7)\n    transformations[too_big_idx] = np.exp(systematic_utilities[too_big_idx])\n    # Guard against overflow when systematic utilities are completely too big.\n    inf_idx = np.isposinf(transformations)\n    transformations[inf_idx] = max_comp_value\n\n    # Account for the outside intercept parameters if there are any.\n    if intercept_params is not None and intercept_ref_pos is not None:\n        # Get a list of all the indices (or row indices) corresponding to the\n        # alternatives whose intercept parameters are being estimated.\n        needed_idxs = range(rows_to_alts.shape[1])\n        needed_idxs.remove(intercept_ref_pos)\n\n        if len(intercept_params.shape) > 1 and intercept_params.shape[1] > 1:\n            # Get an array of zeros with shape\n            # (num_possible_alternatives, num_parameter_samples)\n            all_intercepts = np.zeros((rows_to_alts.shape[1],\n                                       intercept_params.shape[1]))\n            # For alternatives having their intercept estimated, replace the\n            # zeros with the current value of the estimated intercepts\n            all_intercepts[needed_idxs, :] = intercept_params\n        else:\n            # Get an array of zeros with shape (num_possible_alternatives,)\n            all_intercepts = np.zeros(rows_to_alts.shape[1])\n            # For alternatives having their intercept estimated, replace the\n            # zeros with the current value of the estimated intercepts\n            all_intercepts[needed_idxs] = intercept_params\n\n        # Add the intercept values to f(x, beta, c)\n        transformations += rows_to_alts.dot(all_intercepts)\n\n    # Be sure to return a 2D array since other functions will be expecting that\n    if len(transformations.shape) == 1:\n        transformations = transformations[:, None]\n\n    return transformations", "response": "This function transforms the given utility values into a single array of systematic utilities and transformations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cloglog_transform_deriv_v(systematic_utilities,\n                               alt_IDs,\n                               rows_to_alts,\n                               shape_params,\n                               output_array=None,\n                               *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        All elements should be ints, floats, or longs. Should contain the\n        systematic utilities of each observation per available alternative.\n        Note that this vector is formed by the dot product of the design matrix\n        with the vector of utility coefficients.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_alts : 2D scipy sparse matrix.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset. All\n        elements should be zeros or ones.\n    shape_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n    output_array : 2D scipy sparse array.\n        The array should be square and it should have\n        `systematic_utilities.shape[0]` rows. It's data is to be replaced with\n        the correct derivatives of the transformation vector with respect to\n        the vector of systematic utilities. This argument is NOT optional.\n\n    Returns\n    -------\n    output_array : 2D scipy sparse array.\n        The shape of the returned array is `(systematic_utilities.shape[0],\n        systematic_utilities.shape[0])`. The returned array specifies the\n        derivative of the transformed utilities with respect to the systematic\n        utilities. All elements are ints, floats, or longs.\n    \"\"\"\n    exp_neg_v = np.exp(-1 * systematic_utilities)\n    exp_v = np.exp(systematic_utilities)\n    denom_part_1 = 1 - np.exp(-1 * exp_v)\n\n    ##########\n    # Guard against numeric overflow and underflow\n    ##########\n    exp_neg_v[np.isposinf(exp_neg_v)] = max_comp_value\n    exp_neg_v[np.where(exp_neg_v == 0)] = min_comp_value\n    # Note that we don't care about the limiting cases of exp_v.\n    # This term can go to positive infinity or zero. If it goes to positive\n    # infinity, then this is okay because denom_part_1 will just go to 1.\n    # If exp_v goes to zero, then denom_part_1 will go to zero. We will simply\n    # cater to that last outcome since we can't divide by zero. The next line\n    # is retained to show what should NOT be done. We will use L'Hopital's rule\n    # after calculating derivs, as should be done.\n    # denom_part_1[np.where(denom_part_1 == 0)] = min_comp_value\n\n    ##########\n    # Calculate the required derivatives and guard against underflow\n    ##########\n    derivs = 1.0 / (denom_part_1 * exp_neg_v)\n    # Note that the limiting value of the expression above, as the systematic\n    # utility goes to negative infinity (i.e. as denom_part_1 goes to zero),\n    # is one. This can be checked using L'Hopital's rule. We will define\n    # infinity as being so negative that `denom_part_1 == 0`\n    derivs[np.where(denom_part_1 == 0)] = 1\n    derivs[np.isposinf(derivs)] = max_comp_value\n\n    # Assign the calculated derivatives to the output array\n    output_array.data = derivs\n\n    # Return the matrix of dh_dv. Note the off-diagonal entries are zero\n    # because each transformation only depends on its value of v and no others\n    return output_array", "response": "This function transforms the derivative of the systematic utilities of each available alternative to the possible alternatives for this dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a function that can be used in the various gradient and hessian calculations to calculate the derivative of the transformation with respect to the outside intercept parameters.", "response": "def create_calc_dh_d_alpha(estimator):\n    \"\"\"\n    Return the function that can be used in the various gradient and hessian\n    calculations to calculate the derivative of the transformation with respect\n    to the outside intercept parameters.\n\n    Parameters\n    ----------\n    estimator : an instance of the estimation.LogitTypeEstimator class.\n        Should contain a `rows_to_alts` attribute that is a 2D scipy sparse\n        matrix that maps the rows of the `design` matrix to the alternatives\n        available in this dataset. Should also contain an `intercept_ref_pos`\n        attribute that is either None or an int. This attribute should denote\n        which intercept is not being estimated (in the case of outside\n        intercept parameters) for identification purposes.\n\n    Returns\n    -------\n    Callable.\n        Will accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, (shape parameters if there are any) and miscellaneous\n        args and kwargs. Should return a 2D array whose elements contain the\n        derivative of the tranformed utility vector with respect to the vector\n        of outside intercepts. The dimensions of the returned vector should\n        be `(design.shape[0], num_alternatives - 1)`.\n    \"\"\"\n    if estimator.intercept_ref_pos is not None:\n        needed_idxs = range(estimator.rows_to_alts.shape[1])\n        needed_idxs.remove(estimator.intercept_ref_pos)\n        dh_d_alpha = (estimator.rows_to_alts\n                               .copy()\n                               .transpose()[needed_idxs, :]\n                               .transpose())\n    else:\n        dh_d_alpha = None\n    # Create a function that will take in the pre-formed matrix, replace its\n    # data in-place with the new data, and return the correct dh_dalpha on each\n    # iteration of the minimizer\n    calc_dh_d_alpha = partial(_cloglog_transform_deriv_alpha,\n                              output_array=dh_d_alpha)\n\n    return calc_dh_d_alpha"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the chi - squared values for each choice situation in the unique language dataset.", "response": "def calc_individual_chi_squares(residuals,\n                                long_probabilities,\n                                rows_to_obs):\n    \"\"\"\n    Calculates individual chi-squared values for each choice situation in the\n    dataset.\n\n    Parameters\n    ----------\n    residuals : 1D ndarray.\n        The choice vector minus the predicted probability of each alternative\n        for each observation.\n    long_probabilities : 1D ndarray.\n        The probability of each alternative being chosen in each choice\n        situation.\n    rows_to_obs : 2D scipy sparse array.\n        Should map each row of the long format dataferame to the unique\n        observations in the dataset.\n\n    Returns\n    -------\n    ind_chi_squareds : 1D ndarray.\n        Will have as many elements as there are columns in `rows_to_obs`. Each\n        element will contain the pearson chi-squared value for the given choice\n        situation.\n    \"\"\"\n    chi_squared_terms = np.square(residuals) / long_probabilities\n    return rows_to_obs.T.dot(chi_squared_terms)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating McFadden s rho - squared and rho - bar squared for a given model.", "response": "def calc_rho_and_rho_bar_squared(final_log_likelihood,\n                                 null_log_likelihood,\n                                 num_est_parameters):\n    \"\"\"\n    Calculates McFadden's rho-squared and rho-bar squared for the given model.\n\n    Parameters\n    ----------\n    final_log_likelihood : float.\n        The final log-likelihood of the model whose rho-squared and rho-bar\n        squared are being calculated for.\n    null_log_likelihood : float.\n        The log-likelihood of the model in question, when all parameters are\n        zero or their 'base' values.\n    num_est_parameters : int.\n        The number of parameters estimated in this model.\n\n    Returns\n    -------\n    `(rho_squared, rho_bar_squared)` : tuple of floats.\n        The rho-squared and rho-bar-squared for the model.\n    \"\"\"\n    rho_squared = 1.0 - final_log_likelihood / null_log_likelihood\n    rho_bar_squared = 1.0 - ((final_log_likelihood - num_est_parameters) /\n                             null_log_likelihood)\n\n    return rho_squared, rho_bar_squared"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_and_store_post_estimation_results(results_dict,\n                                           estimator):\n    \"\"\"\n    Calculates and stores post-estimation results that require the use of the\n    systematic utility transformation functions or the various derivative\n    functions. Note that this function is only valid for logit-type models.\n\n    Parameters\n    ----------\n    results_dict : dict.\n        This dictionary should be the dictionary returned from\n        scipy.optimize.minimize. In particular, it should have the following\n        keys: `[\"fun\", \"x\", \"log_likelihood_null\"]`.\n    estimator : an instance of the EstimationObj class.\n        Should contain the following attributes or methods:\n\n          - convenience_split_params\n          - convenience_calc_probs\n          - convenience_calc_gradient\n          - convenience_calc_hessian\n          - convenience_calc_fisher_approx\n          - choice_vector\n          - rows_to_obs\n\n    Returns\n    -------\n    results_dict : dict.\n        The following keys will have been entered into `results_dict`:\n\n          - final_log_likelihood\n          - utility_coefs\n          - intercept_params\n          - shape_params\n          - nest_params\n          - chosen_probs\n          - long_probs\n          - residuals\n          - ind_chi_squareds\n          - rho_squared\n          - rho_bar_squared\n          - final_gradient\n          - final_hessian\n          - fisher_info\n    \"\"\"\n    # Store the final log-likelihood\n    final_log_likelihood = -1 * results_dict[\"fun\"]\n    results_dict[\"final_log_likelihood\"] = final_log_likelihood\n\n    # Get the final array of estimated parameters\n    final_params = results_dict[\"x\"]\n\n    # Add the estimated parameters to the results dictionary\n    split_res = estimator.convenience_split_params(final_params,\n                                                   return_all_types=True)\n    results_dict[\"nest_params\"] = split_res[0]\n    results_dict[\"shape_params\"] = split_res[1]\n    results_dict[\"intercept_params\"] = split_res[2]\n    results_dict[\"utility_coefs\"] = split_res[3]\n\n    # Get the probability of the chosen alternative and long_form probabilities\n    chosen_probs, long_probs = estimator.convenience_calc_probs(final_params)\n    results_dict[\"chosen_probs\"] = chosen_probs\n    results_dict[\"long_probs\"] = long_probs\n\n    #####\n    # Calculate the residuals and individual chi-square values\n    #####\n    # Calculate the residual vector\n    if len(long_probs.shape) == 1:\n        residuals = estimator.choice_vector - long_probs\n    else:\n        residuals = estimator.choice_vector[:, None] - long_probs\n    results_dict[\"residuals\"] = residuals\n\n    # Calculate the observation specific chi-squared components\n    args = [residuals, long_probs, estimator.rows_to_obs]\n    results_dict[\"ind_chi_squareds\"] = calc_individual_chi_squares(*args)\n\n    # Calculate and store the rho-squared and rho-bar-squared\n    log_likelihood_null = results_dict[\"log_likelihood_null\"]\n    rho_results = calc_rho_and_rho_bar_squared(final_log_likelihood,\n                                               log_likelihood_null,\n                                               final_params.shape[0])\n    results_dict[\"rho_squared\"] = rho_results[0]\n    results_dict[\"rho_bar_squared\"] = rho_results[1]\n\n    #####\n    # Calculate the gradient, hessian, and BHHH approximation to the fisher\n    # info matrix\n    #####\n    results_dict[\"final_gradient\"] =\\\n        estimator.convenience_calc_gradient(final_params)\n    results_dict[\"final_hessian\"] =\\\n        estimator.convenience_calc_hessian(final_params)\n    results_dict[\"fisher_info\"] =\\\n        estimator.convenience_calc_fisher_approx(final_params)\n\n    # Store the constrained positions that was used in this estimation process\n    results_dict[\"constrained_pos\"] = estimator.constrained_pos\n\n    return results_dict", "response": "Calculates and stores the post - estimation results that require the use of the systematic utility transformation functions or the various derivative functions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nestimate the given choice model that is defined by `estimator`. Parameters ---------- init_vals : 1D ndarray. Should contain the initial values to start the optimization process with. estimator : an instance of the EstimationObj class. method : str, optional. Should be a valid string for scipy.optimize.minimize. Determines the optimization algorithm that is used for this problem. Default `== 'bfgs'`. loss_tol : float, optional. Determines the tolerance on the difference in objective function values from one iteration to the next that is needed to determine convergence. Default `== 1e-06`. gradient_tol : float, optional. Determines the tolerance on the difference in gradient values from one iteration to the next which is needed to determine convergence. Default `== 1e-06`. maxiter : int, optional. Determines the maximum number of iterations used by the optimizer. Default `== 1000`. print_res : bool, optional. Determines whether the timing and initial and final log likelihood results will be printed as they they are determined. Default `== True`. use_hessian : bool, optional. Determines whether the `calc_neg_hessian` method of the `estimator` object will be used as the hessian function during the estimation. This kwarg is used since some models (such as the Mixed Logit and Nested Logit) use a rather crude (i.e. the BHHH) approximation to the Fisher Information Matrix, and users may prefer to not use this approximation for the hessian during estimation. just_point : bool, optional. Determines whether or not calculations that are non-critical for obtaining the maximum likelihood point estimate will be performed. Default == False. Return ------ results : dict. The dictionary of estimation results that is returned by scipy.optimize.minimize. It will also have (at minimum) the following keys: - \"log-likelihood_null\" - \"final_log_likelihood\" - \"utility_coefs\" - \"intercept_params\" - \"shape_params\" - \"nest_params\" - \"chosen_probs\" - \"long_probs\" - \"residuals\" - \"ind_chi_squareds\" - \"rho_squared\" - \"rho_bar_squared\" - \"final_gradient\" - \"final_hessian\" - \"fisher_info\"", "response": "def estimate(init_values,\n             estimator,\n             method,\n             loss_tol,\n             gradient_tol,\n             maxiter,\n             print_results,\n             use_hessian=True,\n             just_point=False,\n             **kwargs):\n    \"\"\"\n    Estimate the given choice model that is defined by `estimator`.\n\n    Parameters\n    ----------\n    init_vals : 1D ndarray.\n        Should contain the initial values to start the optimization process\n        with.\n    estimator : an instance of the EstimationObj class.\n    method : str, optional.\n        Should be a valid string for scipy.optimize.minimize. Determines\n        the optimization algorithm that is used for this problem.\n        Default `== 'bfgs'`.\n    loss_tol : float, optional.\n        Determines the tolerance on the difference in objective function\n        values from one iteration to the next that is needed to determine\n        convergence. Default `== 1e-06`.\n    gradient_tol : float, optional.\n        Determines the tolerance on the difference in gradient values from\n        one iteration to the next which is needed to determine convergence.\n        Default `== 1e-06`.\n    maxiter : int, optional.\n        Determines the maximum number of iterations used by the optimizer.\n        Default `== 1000`.\n    print_res : bool, optional.\n        Determines whether the timing and initial and final log likelihood\n        results will be printed as they they are determined.\n        Default `== True`.\n    use_hessian : bool, optional.\n        Determines whether the `calc_neg_hessian` method of the `estimator`\n        object will be used as the hessian function during the estimation. This\n        kwarg is used since some models (such as the Mixed Logit and Nested\n        Logit) use a rather crude (i.e. the BHHH) approximation to the Fisher\n        Information Matrix, and users may prefer to not use this approximation\n        for the hessian during estimation.\n    just_point : bool, optional.\n        Determines whether or not calculations that are non-critical for\n        obtaining the maximum likelihood point estimate will be performed.\n        Default == False.\n\n    Return\n    ------\n    results : dict.\n        The dictionary of estimation results that is returned by\n        scipy.optimize.minimize. It will also have (at minimum) the following\n        keys:\n          - \"log-likelihood_null\"\n          - \"final_log_likelihood\"\n          - \"utility_coefs\"\n          - \"intercept_params\"\n          - \"shape_params\"\n          - \"nest_params\"\n          - \"chosen_probs\"\n          - \"long_probs\"\n          - \"residuals\"\n          - \"ind_chi_squareds\"\n          - \"rho_squared\"\n          - \"rho_bar_squared\"\n          - \"final_gradient\"\n          - \"final_hessian\"\n          - \"fisher_info\"\n    \"\"\"\n    if not just_point:\n        # Perform preliminary calculations\n        log_likelihood_at_zero =\\\n            estimator.convenience_calc_log_likelihood(estimator.zero_vector)\n\n        initial_log_likelihood =\\\n            estimator.convenience_calc_log_likelihood(init_values)\n\n        if print_results:\n            # Print the log-likelihood at zero\n            null_msg = \"Log-likelihood at zero: {:,.4f}\"\n            print(null_msg.format(log_likelihood_at_zero))\n\n            # Print the log-likelihood at the starting values\n            init_msg = \"Initial Log-likelihood: {:,.4f}\"\n            print(init_msg.format(initial_log_likelihood))\n            sys.stdout.flush()\n\n    # Get the hessian fucntion for this estimation process\n    hess_func = estimator.calc_neg_hessian if use_hessian else None\n\n    # Estimate the actual parameters of the model\n    start_time = time.time()\n\n    results = minimize(estimator.calc_neg_log_likelihood_and_neg_gradient,\n                       init_values,\n                       method=method,\n                       jac=True,\n                       hess=hess_func,\n                       tol=loss_tol,\n                       options={'gtol': gradient_tol,\n                                \"maxiter\": maxiter},\n                       **kwargs)\n\n    if not just_point:\n        if print_results:\n            # Stop timing the estimation process and report the timing results\n            end_time = time.time()\n            elapsed_sec = (end_time - start_time)\n            elapsed_min = elapsed_sec / 60.0\n            if elapsed_min > 1.0:\n                msg = \"Estimation Time for Point Estimation: {:.2f} minutes.\"\n                print(msg.format(elapsed_min))\n            else:\n                msg = \"Estimation Time for Point Estimation: {:.2f} seconds.\"\n                print(msg.format(elapsed_sec))\n            print(\"Final log-likelihood: {:,.4f}\".format(-1 * results[\"fun\"]))\n            sys.stdout.flush()\n\n        # Store the log-likelihood at zero\n        results[\"log_likelihood_null\"] = log_likelihood_at_zero\n\n        # Calculate and store the post-estimation results\n        results = calc_and_store_post_estimation_results(results, estimator)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convenience_split_params(self, params, return_all_types=False):\n        return self.split_params(params,\n                                 self.rows_to_alts,\n                                 self.design,\n                                 return_all_types=return_all_types)", "response": "Splits parameter vector into shape intercept and index parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating and returns the negative of the log - likelihood and the negative of the gradient.", "response": "def calc_neg_log_likelihood_and_neg_gradient(self, params):\n        \"\"\"\n        Calculates and returns the negative of the log-likelihood and the\n        negative of the gradient. This function is used as the objective\n        function in scipy.optimize.minimize.\n        \"\"\"\n        neg_log_likelihood = -1 * self.convenience_calc_log_likelihood(params)\n        neg_gradient = -1 * self.convenience_calc_gradient(params)\n\n        if self.constrained_pos is not None:\n            neg_gradient[self.constrained_pos] = 0\n\n        return neg_log_likelihood, neg_gradient"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the probabilities of the chosen alternative and the long format probabilities for this model and dataset.", "response": "def convenience_calc_probs(self, params):\n        \"\"\"\n        Calculates the probabilities of the chosen alternative, and the long\n        format probabilities for this model and dataset.\n        \"\"\"\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        prob_args = [betas,\n                     self.design,\n                     self.alt_id_vector,\n                     self.rows_to_obs,\n                     self.rows_to_alts,\n                     self.utility_transform]\n\n        prob_kwargs = {\"intercept_params\": intercepts,\n                       \"shape_params\": shapes,\n                       \"chosen_row_to_obs\": self.chosen_row_to_obs,\n                       \"return_long_probs\": True}\n        prob_results = cc.calc_probabilities(*prob_args, **prob_kwargs)\n\n        return prob_results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the log - likelihood for this model and dataset.", "response": "def convenience_calc_log_likelihood(self, params):\n        \"\"\"\n        Calculates the log-likelihood for this model and dataset.\n        \"\"\"\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        args = [betas,\n                self.design,\n                self.alt_id_vector,\n                self.rows_to_obs,\n                self.rows_to_alts,\n                self.choice_vector,\n                self.utility_transform]\n\n        kwargs = {\"intercept_params\": intercepts,\n                  \"shape_params\": shapes,\n                  \"ridge\": self.ridge,\n                  \"weights\": self.weights}\n        log_likelihood = cc.calc_log_likelihood(*args, **kwargs)\n\n        return log_likelihood"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convenience_calc_hessian(self, params):\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        args = [betas,\n                self.design,\n                self.alt_id_vector,\n                self.rows_to_obs,\n                self.rows_to_alts,\n                self.utility_transform,\n                self.calc_dh_d_shape,\n                self.calc_dh_dv,\n                self.calc_dh_d_alpha,\n                self.block_matrix_idxs,\n                intercepts,\n                shapes,\n                self.ridge,\n                self.weights]\n\n        return cc.calc_hessian(*args)", "response": "Calculates the hessian of the log - likelihood for this model / dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the BHHH approximation of the Fisher Information Matrix for the given parameters.", "response": "def convenience_calc_fisher_approx(self, params):\n        \"\"\"\n        Calculates the BHHH approximation of the Fisher Information Matrix for\n        this model / dataset.\n        \"\"\"\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        args = [betas,\n                self.design,\n                self.alt_id_vector,\n                self.rows_to_obs,\n                self.rows_to_alts,\n                self.choice_vector,\n                self.utility_transform,\n                self.calc_dh_d_shape,\n                self.calc_dh_dv,\n                self.calc_dh_d_alpha,\n                intercepts,\n                shapes,\n                self.ridge,\n                self.weights]\n\n        return cc.calc_fisher_info_matrix(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming the systematic utilities to a 2D array.", "response": "def _mnl_utility_transform(systematic_utilities, *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        Should contain the systematic utilities for each each available\n        alternative for each observation.\n\n    Returns\n    -------\n    `systematic_utilities[:, None]`\n    \"\"\"\n    # Be sure to return a 2D array since other functions will be expecting this\n    if len(systematic_utilities.shape) == 1:\n        systematic_utilities = systematic_utilities[:, np.newaxis]\n\n    return systematic_utilities"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_length_of_initial_values(self, init_values):\n        # Calculate the expected number of index parameters\n        num_index_coefs = self.design.shape[1]\n\n        if init_values.shape[0] != num_index_coefs:\n            msg_1 = \"The initial values are of the wrong dimension.\"\n            msg_2 = \"It should be of dimension {}\"\n            msg_3 = \"But instead it has dimension {}\"\n            raise ValueError(msg_1 +\n                             msg_2.format(num_index_coefs) +\n                             msg_3.format(init_values.shape[0]))\n\n        return None", "response": "Checks that the initial values of the record store are of the correct length. Raises a helpful ValueError if otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit the MLE algorithm for the given set of parameters.", "response": "def fit_mle(self,\n                init_vals,\n                print_res=True,\n                method=\"BFGS\",\n                loss_tol=1e-06,\n                gradient_tol=1e-06,\n                maxiter=1000,\n                ridge=None,\n                constrained_pos=None,\n                just_point=False,\n                **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        init_vals : 1D ndarray.\n            The initial values to start the optimization process with. There\n            should be one value for each utility coefficient being estimated.\n        print_res : bool, optional.\n            Determines whether the timing and initial and final log likelihood\n            results will be printed as they they are determined.\n        method : str, optional.\n            Should be a valid string that can be passed to\n            scipy.optimize.minimize. Determines the optimization algorithm that\n            is used for this problem. If 'em' is passed, a custom coded EM\n            algorithm will be used. Default `== 'newton-cg'`.\n        loss_tol : float, optional.\n            Determines the tolerance on the difference in objective function\n            values from one iteration to the next that is needed to determine\n            convergence. Default `== 1e-06`.\n        gradient_tol : float, optional.\n            Determines the tolerance on the difference in gradient values from\n            one iteration to the next which is needed to determine convergence.\n        ridge : int, float, long, or None, optional.\n            Determines whether or not ridge regression is performed. If a\n            scalar is passed, then that scalar determines the ridge penalty for\n            the optimization. Default `== None`.\n        constrained_pos : list or None, optional.\n            Denotes the positions of the array of estimated parameters that are\n            not to change from their initial values. If a list is passed, the\n            elements are to be integers where no such integer is greater than\n            `init_vals.size.` Default == None.\n        just_point : bool, optional.\n            Determines whether (True) or not (False) calculations that are non-\n            critical for obtaining the maximum likelihood point estimate will\n            be performed. If True, this function will return the results\n            dictionary from scipy.optimize. Default == False.\n\n        Returns\n        -------\n        None or dict.\n            If `just_point` is False, None is returned and the estimation\n            results are saved to the model instance. If `just_point` is True,\n            then the results dictionary from scipy.optimize() is returned.\n        \"\"\"\n        # Check integrity of passed arguments\n        kwargs_to_be_ignored = [\"init_shapes\", \"init_intercepts\", \"init_coefs\"]\n        if any([x in kwargs for x in kwargs_to_be_ignored]):\n            msg = \"MNL model does not use of any of the following kwargs:\\n{}\"\n            msg_2 = \"Remove such kwargs and pass a single init_vals argument\"\n            raise ValueError(msg.format(kwargs_to_be_ignored) + msg_2)\n\n        if ridge is not None:\n            warnings.warn(_ridge_warning_msg)\n\n        # Store the optimization method\n        self.optimization_method = method\n\n        # Store the ridge parameter\n        self.ridge_param = ridge\n\n        # Construct the mappings from alternatives to observations and from\n        # chosen alternatives to observations\n        mapping_res = self.get_mappings_for_fit()\n\n        # Create the estimation object\n        zero_vector = np.zeros(init_vals.shape)\n        mnl_estimator = MNLEstimator(self,\n                                     mapping_res,\n                                     ridge,\n                                     zero_vector,\n                                     split_param_vec,\n                                     constrained_pos=constrained_pos)\n        # Set the derivative functions for estimation\n        mnl_estimator.set_derivatives()\n\n        # Perform one final check on the length of the initial values\n        mnl_estimator.check_length_of_initial_values(init_vals)\n\n        # Get the estimation results\n        estimation_res = estimate(init_vals,\n                                  mnl_estimator,\n                                  method,\n                                  loss_tol,\n                                  gradient_tol,\n                                  maxiter,\n                                  print_res,\n                                  just_point=just_point)\n\n        if not just_point:\n            # Store the estimation results\n            self.store_fit_results(estimation_res)\n\n            return None\n        else:\n            return estimation_res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_normal_draws(num_mixers,\n                     num_draws,\n                     num_vars,\n                     seed=None):\n    \"\"\"\n    Parameters\n    ----------\n    num_mixers : int.\n        Should be greater than zero. Denotes the number of observations for\n        which we are making draws from a normal distribution for. I.e. the\n        number of observations with randomly distributed coefficients.\n    num_draws : int.\n        Should be greater than zero. Denotes the number of draws that are to be\n        made from each normal distribution.\n    num_vars : int.\n        Should be greater than zero. Denotes the number of variables for which\n        we need to take draws from the normal distribution.\n    seed : int or None, optional.\n        If an int is passed, it should be greater than zero. Denotes the value\n        to be used in seeding the random generator used to generate the draws\n        from the normal distribution. Default == None.\n\n    Returns\n    -------\n    all_draws : list of 2D ndarrays.\n        The list will have num_vars elements. Each element will be a num_obs by\n        num_draws numpy array of draws from a normal distribution with mean\n        zero and standard deviation of one.\n    \"\"\"\n    # Check the validity of the input arguments\n    assert all([isinstance(x, int) for x in [num_mixers, num_draws, num_vars]])\n    assert all([x > 0 for x in [num_mixers, num_draws, num_vars]])\n    if seed is not None:\n        assert isinstance(seed, int) and seed > 0\n\n    normal_dist = scipy.stats.norm(loc=0.0, scale=1.0)\n    all_draws = []\n    if seed:\n        np.random.seed(seed)\n    for i in xrange(num_vars):\n        all_draws.append(normal_dist.rvs(size=(num_mixers, num_draws)))\n    return all_draws", "response": "Returns a list of draws from a normal distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_expanded_design_for_mixing(design,\n                                      draw_list,\n                                      mixing_pos,\n                                      rows_to_mixers):\n    \"\"\"\n    Parameters\n    ----------\n    design : 2D ndarray.\n        All elements should be ints, floats, or longs. Each row corresponds to\n        an available alternative for a given individual. There should be one\n        column per index coefficient being estimated.\n    draw_list : list of 2D ndarrays.\n        All numpy arrays should have the same number of columns (`num_draws`)\n        and the same number of rows (`num_mixers`). All elements of the numpy\n        arrays should be ints, floats, or longs. Should have as many elements\n        as there are lements in `mixing_pos`.\n    mixing_pos : list of ints.\n        Each element should denote a column in design whose associated index\n        coefficient is being treated as a random variable.\n    rows_to_mixers : 2D scipy sparse array.\n        All elements should be zeros and ones. Will map the rows of the design\n        matrix to the particular units that the mixing is being performed over.\n        Note that in the case of panel data, this matrix will be different from\n        `rows_to_obs`.\n\n    Returns\n    -------\n    design_3d : 3D numpy array.\n        Each slice of the third dimension will contain a copy of the design\n        matrix corresponding to a given draw of the random variables being\n        mixed over.\n    \"\"\"\n    if len(mixing_pos) != len(draw_list):\n        msg = \"mixing_pos == {}\".format(mixing_pos)\n        msg_2 = \"len(draw_list) == {}\".format(len(draw_list))\n        raise ValueError(msg + \"\\n\" + msg_2)\n\n    # Determine the number of draws being used. Note the next line assumes an\n    # equal number of draws from each random coefficient's mixing distribution.\n    num_draws = draw_list[0].shape[1]\n    orig_num_vars = design.shape[1]\n\n    # Initialize the expanded design matrix that replicates the columns of the\n    # variables that are being mixed over.\n    arrays_for_mixing = design[:, mixing_pos]\n    expanded_design = np.concatenate((design, arrays_for_mixing),\n                                     axis=1).copy()\n    design_3d = np.repeat(expanded_design[:, None, :],\n                          repeats=num_draws,\n                          axis=1)\n\n    # Multiply the columns that are being mixed over by their appropriate\n    # draws from the normal distribution\n    for pos, idx in enumerate(mixing_pos):\n        rel_draws = draw_list[pos]\n        # Note that rel_long_draws will be a dense, 2D numpy array of shape\n        # (num_rows, num_draws).\n        rel_long_draws = rows_to_mixers.dot(rel_draws)\n        # Create the actual column in design 3d that should be used.\n        # It should be the multiplication of the draws random variable and the\n        # independent variable associated with the param that is being mixed.\n        # NOTE THE IMPLICIT ASSUMPTION THAT ONLY INDEX COEFFICIENTS ARE MIXED.\n        # Also, the final axis is selected on because the final axis sepecifies\n        # the particular variable being multiplied by the draws. We select with\n        # orig_num_vars + pos since the variables being mixed over were added,\n        # in order so we simply need to start at the first position after all\n        # the original variables (i.e. at orig_num_vars) and iterate.\n        design_3d[:, :, orig_num_vars + pos] *= rel_long_draws\n\n    return design_3d", "response": "Create an expanded design matrix that replicates the columns of the design matrix that are being mixed over."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the probability of making the observed sequence of choices for the given choice set.", "response": "def calc_choice_sequence_probs(prob_array,\n                               choice_vec,\n                               rows_to_mixers,\n                               return_type=None):\n    \"\"\"\n    Parameters\n    ----------\n    prob_array : 2D ndarray.\n        All elements should be ints, floats, or longs. All elements should be\n        between zero and one (exclusive). Each element should represent the\n        probability of the corresponding alternative being chosen by the\n        corresponding individual during the given choice situation, given the\n        particular draw of coefficients being considered. There should be one\n        column for each draw of the coefficients.\n    choice_vec : 1D ndarray.\n        All elements should be zeros or ones. Should denote the rows that were\n        chosen by the individuals corresponding to those rows.\n    rows_to_mixers : 2D scipy sparse array.\n        All elements should be zeros and ones. Will map the rows of the design\n        matrix to the particular units that the mixing is being performed over.\n        Note that in the case of panel data, this matrix will be different from\n        `rows_to_obs`.\n    return_type : `'all'` or None, optional.\n        If `'all'` is passed, then a tuple will be returned. The first element\n        will be a 1D numpy array of shape `(num_mixing_units,)`. Each value\n        will be the average probability of predicting the associated mixing\n        unit's probability of making the observed sequence of choices. The\n        second element of the tuple will be a 2D numpy array with shape\n        `(num_mixing_units, num_draws)`, where\n        `num_draws == prob_array.shape[1]`. Each value will be the probability\n        of predicting the associated mixing unit's probability of making the\n        observed sequence of choices, given the associated draw of the mixing\n        distribution for the given individual. If None, only the first\n        element of the tuple described above will be returned. Default == None.\n\n    Returns\n    -------\n    See `return_type` kwarg.\n    \"\"\"\n    if return_type not in [None, 'all']:\n        raise ValueError(\"return_type must be None or 'all'.\")\n\n    log_chosen_prob_array = choice_vec[:, None] * np.log(prob_array)\n    # Create a 2D array with shape (num_mixing_units, num_random_draws)\n    # Each element will be the log of the probability of the sequence of\n    # choices, given the random draw of the coefficients\n    expanded_log_sequence_probs = rows_to_mixers.T.dot(log_chosen_prob_array)\n    # Calculate the probability of the sequence of choices for each mixing\n    # unit, given the random draw of the coefficients\n    expanded_sequence_probs = np.exp(expanded_log_sequence_probs)\n    # Guard against underflow since none of the probabilities are actually zero\n    zero_idx = np.where(expanded_sequence_probs == 0)\n    expanded_sequence_probs[zero_idx] = min_comp_value\n    # Calculate the simulated probability of the sequence of choices of each\n    # mixing unit\n    sequence_probs = expanded_sequence_probs.mean(axis=1)\n\n    if return_type is None:\n        return sequence_probs\n    elif return_type == 'all':\n        return sequence_probs, expanded_sequence_probs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the mixed log likelihood of a given set of entries.", "response": "def calc_mixed_log_likelihood(params,\n                              design_3d,\n                              alt_IDs,\n                              rows_to_obs,\n                              rows_to_alts,\n                              rows_to_mixers,\n                              choice_vector,\n                              utility_transform,\n                              ridge=None,\n                              weights=None):\n    \"\"\"\n    Parameters\n    ----------\n    params : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated (i.e. num_features  +\n        num_coefs_being_mixed).\n    design_3d : 3D ndarray.\n        All elements should be ints, floats, or longs. Should have one row per\n        observation per available alternative. The second axis should have as\n        many elements as there are draws from the mixing distributions of the\n        coefficients. The last axis should have one element per index\n        coefficient being estimated.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D scipy sparse array.\n        All elements should be zeros and ones. There should be one row per\n        observation per available alternative and one column per observation.\n        This matrix maps the rows of the design matrix to the unique\n        observations (on the columns).\n    rows_to_alts : 2D scipy sparse array.\n        All elements should be zeros and ones. There should be one row per\n        observation per available alternative and one column per possible\n        alternative. This matrix maps the rows of the design matrix to the\n        possible alternatives for this dataset.\n    rows_to_mixers : 2D scipy sparse array.\n        All elements should be zeros and ones. Will map the rows of the design\n        matrix to the particular units that the mixing is being performed over.\n        Note that in the case of panel data, this matrix will be different from\n        `rows_to_obs`.\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    utility_transform :  callable.\n        Should accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 2D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated and the\n        given draw of the random coefficients. There should be one column for\n        each draw of the random coefficients. There should have one row per\n        individual per choice situation per available alternative.\n    ridge : scalar or None, optional.\n        Determines whether or not ridge regression is performed. If a scalar is\n        passed, then that scalar determines the ridge penalty for the\n        optimization. Default = None.\n    weights : 1D ndarray or None.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    log_likelihood: float.\n        The log-likelihood of the mixed logit model evaluated at the passed\n        values of `params`.\n    \"\"\"\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = np.ones(design_3d.shape[0])\n    weights_per_obs =\\\n        np.max(rows_to_mixers.toarray() * weights[:, None], axis=0)\n\n    # Calculate the regular probability array. Note the implicit assumption\n    # that params == index coefficients.\n    prob_array = general_calc_probabilities(params,\n                                            design_3d,\n                                            alt_IDs,\n                                            rows_to_obs,\n                                            rows_to_alts,\n                                            utility_transform,\n                                            return_long_probs=True)\n\n    # Calculate the simulated probability of correctly predicting each persons\n    # sequence of choices. Note that this function implicitly assumes that the\n    # mixing unit is the individual\n    simulated_sequence_probs = calc_choice_sequence_probs(prob_array,\n                                                          choice_vector,\n                                                          rows_to_mixers)\n\n    # Calculate the log-likelihood of the dataset\n    log_likelihood = weights_per_obs.dot(np.log(simulated_sequence_probs))\n\n    # Adujust for the presence of a ridge estimator. Again, note that we are\n    # implicitly assuming that the only model being mixed is the MNL model,\n    # such that params == index coefficients.\n    if ridge is None:\n        return log_likelihood\n    else:\n        return log_likelihood - ridge * np.square(params).sum()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the mixed logit gradient for a given set of entries.", "response": "def calc_mixed_logit_gradient(params,\n                              design_3d,\n                              alt_IDs,\n                              rows_to_obs,\n                              rows_to_alts,\n                              rows_to_mixers,\n                              choice_vector,\n                              utility_transform,\n                              ridge=None,\n                              weights=None):\n    \"\"\"\n    Parameters\n    ----------\n    params : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated\n        (i.e. num_features  + num_coefs_being_mixed).\n    design_3d : 3D ndarray.\n        All elements should be ints, floats, or longs. Should have one row per\n        observation per available alternative. The second axis should have as\n        many elements as there are draws from the mixing distributions of the\n        coefficients. The last axis should have one element per index\n        coefficient being estimated.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D scipy sparse array.\n        All elements should be zeros and ones. Should have one row per\n        observation per available alternative and one column per observation.\n        This matrix maps the rows of the design matrix to the unique\n        observations (on the columns).\n    rows_to_alts : 2D scipy sparse array.\n        All elements should be zeros and ones. Should have one row per\n        observation per available alternative and one column per possible\n        alternative. This matrix maps the rows of the design matrix to the\n        possible alternatives for this dataset.\n    rows_to_mixers : 2D scipy sparse array.\n        All elements should be zeros and ones. Will map the rows of the design\n        matrix to the particular units that the mixing is being performed over.\n        Note that in the case of panel data, this matrix will be different from\n        `rows_to_obs`.\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    utility_transform :  callable.\n        Should accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 2D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated and the\n        given draw of the random coefficients. There should be one column for\n        each draw of the random coefficients. There should have one row per\n        individual per choice situation per available alternative.\n    ridge : int, float, long, or None, optional.\n        Determines whether or not ridge regression is performed. If a float is\n        passed, then that float determines the ridge penalty for the\n        optimization. Default = None.\n    weights : 1D ndarray or None.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    gradient : ndarray of shape (design_3d.shape[2],).\n        The returned array is the gradient of the log-likelihood of the mixed\n        MNL model with respect to `params`.\n    \"\"\"\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = np.ones(design_3d.shape[0])\n\n    # Calculate the regular probability array. Note the implicit assumption\n    # that params == index coefficients.\n    prob_array = general_calc_probabilities(params,\n                                            design_3d,\n                                            alt_IDs,\n                                            rows_to_obs,\n                                            rows_to_alts,\n                                            utility_transform,\n                                            return_long_probs=True)\n\n    # Calculate the simulated probability of correctly predicting each persons\n    # sequence of choices. Note that this function implicitly assumes that the\n    # mixing unit is the individual\n    prob_results = calc_choice_sequence_probs(prob_array,\n                                              choice_vector,\n                                              rows_to_mixers,\n                                              return_type=\"all\")\n    # Calculate the sequence probabilities given random draws\n    # and calculate the overal simulated probabilities\n    sequence_prob_array = prob_results[1]\n    simulated_probs = prob_results[0]\n\n    # Convert the various probabilties to long format\n    long_sequence_prob_array = rows_to_mixers.dot(sequence_prob_array)\n    long_simulated_probs = rows_to_mixers.dot(simulated_probs)\n    # Scale sequence probabilites given random draws by simulated probabilities\n    scaled_sequence_probs = (long_sequence_prob_array /\n                             long_simulated_probs[:, None])\n    # Calculate the scaled error. Will have shape == (num_rows, num_draws)\n    scaled_error = ((choice_vector[:, None] - prob_array) *\n                    scaled_sequence_probs)\n\n    # Calculate the gradient. Note that the lines below assume that we are\n    # taking the gradient of an MNL model. Should refactor to make use of the\n    # built in gradient function for logit-type models. Should also refactor\n    # the gradient function for logit-type models to be able to handle 2D\n    # systematic utility arrays.\n    gradient = (scaled_error[:, :, None] *\n                design_3d *\n                weights[:, None, None]).sum(axis=0)\n\n    gradient = gradient.mean(axis=0)\n\n    # Account for the ridge parameter if an L2 penalization is being performed\n    if ridge is not None:\n        gradient -= 2 * ridge * params\n\n    return gradient.ravel()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_neg_log_likelihood_and_neg_gradient(beta,\n                                             design_3d,\n                                             alt_IDs,\n                                             rows_to_obs,\n                                             rows_to_alts,\n                                             rows_to_mixers,\n                                             choice_vector,\n                                             utility_transform,\n                                             constrained_pos,\n                                             ridge=None,\n                                             weights=None,\n                                             *args):\n    \"\"\"\n    Parameters\n    ----------\n    beta : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated (i.e. num_features  +\n        num_coefs_being_mixed).\n    design_3d : 3D ndarray.\n        All elements should be ints, floats, or longs. Should have one row per\n        observation per available alternative. The second axis should have as\n        many elements as there are draws from the mixing distributions of the\n        coefficients. The last axis should have one element per index\n        coefficient being estimated.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D scipy sparse array.\n        All elements should be zeros and ones. Should have one row per\n        observation per available alternative and one column per observation.\n        This matrix maps the rows of the design matrix to the unique\n        observations (on the columns).\n    rows_to_alts : 2D scipy sparse array.\n        All elements should be zeros and ones. Should have one row per\n        observation per available alternative and one column per possible\n        alternative. This matrix maps the rows of the design matrix to the\n        possible alternatives for this dataset.\n    rows_to_mixers : 2D scipy sparse array.\n        All elements should be zeros and ones. Will map the rows of the design\n        matrix to the particular units that the mixing is being performed over.\n        Note that in the case of panel data, this matrix will be different from\n        `rows_to_obs`.\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    utility_transform :  callable.\n        Should accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 2D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated and the\n        given draw of the random coefficients. There should be one column for\n        each draw of the random coefficients. There should have one row per\n        individual per choice situation per available alternative.\n    constrained_pos : list of ints, or None, optional.\n        Each int denotes a position in the array of estimated parameters that\n        are not to change from their initial values. None of the integers\n        should be greater than `beta.size`.\n    ridge : int, float, long, or None, optional.\n        Determines whether or not ridge regression is performed. If a float is\n        passed, then that float determines the ridge penalty for the\n        optimization. Default = None.\n    weights : 1D ndarray or None.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    tuple. (`neg_log_likelihood`, `neg_beta_gradient_vec`).\n        The first element is a float. The second element is a 1D numpy array of\n        shape (design.shape[1],). The first element is the negative\n        log-likelihood of this model evaluated at the passed values of beta.\n        The second element is the gradient of the negative log- likelihood with\n        respect to the vector of utility coefficients.\n    \"\"\"\n    neg_log_likelihood = -1 * calc_mixed_log_likelihood(beta,\n                                                        design_3d,\n                                                        alt_IDs,\n                                                        rows_to_obs,\n                                                        rows_to_alts,\n                                                        rows_to_mixers,\n                                                        choice_vector,\n                                                        utility_transform,\n                                                        ridge=ridge,\n                                                        weights=weights)\n\n    neg_beta_gradient_vec = -1 * calc_mixed_logit_gradient(beta,\n                                                           design_3d,\n                                                           alt_IDs,\n                                                           rows_to_obs,\n                                                           rows_to_alts,\n                                                           rows_to_mixers,\n                                                           choice_vector,\n                                                           utility_transform,\n                                                           ridge=ridge,\n                                                           weights=weights)\n\n    if constrained_pos is not None:\n        neg_beta_gradient_vec[constrained_pos] = 0\n\n    return neg_log_likelihood, neg_beta_gradient_vec", "response": "Calculates the negative log likelihood and negative gradient of the log likelihood of a given set of unique entries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_bhhh_hessian_approximation_mixed_logit(params,\n                                                design_3d,\n                                                alt_IDs,\n                                                rows_to_obs,\n                                                rows_to_alts,\n                                                rows_to_mixers,\n                                                choice_vector,\n                                                utility_transform,\n                                                ridge=None,\n                                                weights=None):\n    \"\"\"\n    Parameters\n    ----------\n    params : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated (i.e. num_features  +\n        num_coefs_being_mixed).\n    design_3d : 3D ndarray.\n        All elements should be ints, floats, or longs. Should have one row per\n        observation per available alternative. The second axis should have as\n        many elements as there are draws from the mixing distributions of the\n        coefficients. The last axis should have one element per index\n        coefficient being estimated.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D scipy sparse array.\n        All elements should be zeros and ones. Should have one row per\n        observation per available alternative and one column per observation.\n        This matrix maps the rows of the design matrix to the unique\n        observations (on the columns).\n    rows_to_alts : 2D scipy sparse array.\n        All elements should be zeros and ones. Should have one row per\n        observation per available alternative and one column per possible\n        alternative. This matrix maps the rows of the design matrix to the\n        possible alternatives for this dataset.\n    rows_to_mixers : 2D scipy sparse array.\n        All elements should be zeros and ones. Will map the rows of the design\n        matrix to the particular units that the mixing is being performed over.\n        Note that in the case of panel data, this matrix will be different from\n        `rows_to_obs`.\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    utility_transform :  callable.\n        Should accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 2D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated and the\n        given draw of the random coefficients. There should be one column for\n        each draw of the random coefficients. There should have one row per\n        individual per choice situation per available alternative.\n    ridge : int, float, long, or None, optional.\n        Determines whether or not ridge regression is performed. If a float is\n        passed, then that float determines the ridge penalty for the\n        optimization. Default = None.\n    weights : 1D ndarray or None, optional.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class. Default == None.\n\n    Returns\n    -------\n    bhhh_matrix : 2D ndarray of shape `(design.shape[1], design.shape[1])`.\n        The returned array is the BHHH approximation of the Fisher Information\n        Matrix. I.e it is the negative of the sum of the outer product of\n        each individual's gradient with itself.\n    \"\"\"\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = np.ones(design_3d.shape[0])\n    weights_per_obs =\\\n        np.max(rows_to_mixers.toarray() * weights[:, None], axis=0)\n    # Calculate the regular probability array. Note the implicit assumption\n    # that params == index coefficients.\n    prob_array = general_calc_probabilities(params,\n                                            design_3d,\n                                            alt_IDs,\n                                            rows_to_obs,\n                                            rows_to_alts,\n                                            utility_transform,\n                                            return_long_probs=True)\n\n    # Calculate the simulated probability of correctly predicting each persons\n    # sequence of choices. Note that this function implicitly assumes that the\n    # mixing unit is the individual\n    prob_results = calc_choice_sequence_probs(prob_array,\n                                              choice_vector,\n                                              rows_to_mixers,\n                                              return_type=\"all\")\n    # Calculate the sequence probabilities given random draws\n    # and calculate the overal simulated probabilities\n    sequence_prob_array = prob_results[1]\n    simulated_probs = prob_results[0]\n\n    # Convert the various probabilties to long format\n    long_sequence_prob_array = rows_to_mixers.dot(sequence_prob_array)\n    long_simulated_probs = rows_to_mixers.dot(simulated_probs)\n    # Scale sequence probabilites given random draws by simulated probabilities\n    scaled_sequence_probs = (long_sequence_prob_array /\n                             long_simulated_probs[:, None])\n    # Calculate the scaled error. Will have shape == (num_rows, num_draws)\n    scaled_error = ((choice_vector[:, None] - prob_array) *\n                    scaled_sequence_probs)\n\n    # Calculate the gradient. Note that the lines below assume that we are\n    # taking the gradient of an MNL model. Should refactor to make use of the\n    # built in gradient function for logit-type models. Should also refactor\n    # the gradient function for logit-type models to be able to handle 2D\n    # systematic utility arrays. `gradient` will have shape\n    # (design_3d.shape[0], design_3d.shape[2])\n    gradient = (scaled_error[:, :, None] * design_3d).mean(axis=1)\n\n    gradient_per_obs = rows_to_mixers.T.dot(gradient)\n\n    bhhh_matrix =\\\n        gradient_per_obs.T.dot(weights_per_obs[:, None] * gradient_per_obs)\n\n    if ridge is not None:\n        bhhh_matrix -= 2 * ridge\n\n    # Note the \"-1\" is because we are approximating the Fisher information\n    # matrix which has a negative one in the front of it?\n    return -1 * bhhh_matrix", "response": "Calculates the Bhhh - Hessian approximation for mixed logit."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_conf_percentage_validity(conf_percentage):\n    msg = \"conf_percentage MUST be a number between 0.0 and 100.\"\n    condition_1 = isinstance(conf_percentage, Number)\n    if not condition_1:\n        raise ValueError(msg)\n    else:\n        condition_2 = 0 < conf_percentage < 100\n        if not condition_2:\n            raise ValueError(msg)\n    return None", "response": "Ensures that conf_percentage is in range 0. 0 and 100. Raises a helpful ValueError otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_samples_is_ndim_ndarray(samples, name='bootstrap', ndim=2):\n    assert isinstance(ndim, int)\n    assert isinstance(name, str)\n    if not isinstance(samples, np.ndarray) or not (samples.ndim == ndim):\n        sample_name = name + \"_samples\"\n        msg = \"`{}` MUST be a {}D ndarray.\".format(sample_name, ndim)\n        raise ValueError(msg)\n    return None", "response": "Ensures that samples is an ndim - dimensional numpy array. Raises a helpful\n    ValueError if otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that wide_weights is a 1D or 2D ndarray. Raises a helpful ValueError if otherwise.", "response": "def ensure_wide_weights_is_1D_or_2D_ndarray(wide_weights):\n    \"\"\"\n    Ensures that `wide_weights` is a 1D or 2D ndarray. Raises a helpful\n    ValueError if otherwise.\n    \"\"\"\n    if not isinstance(wide_weights, np.ndarray):\n        msg = \"wide_weights MUST be a ndarray.\"\n        raise ValueError(msg)\n    ndim = wide_weights.ndim\n    if not 0 < ndim < 3:\n        msg = \"wide_weights MUST be a 1D or 2D ndarray.\"\n        raise ValueError(msg)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_validity_of_long_form_args(model_obj, wide_weights, rows_to_obs):\n    # Ensure model_obj has the necessary method for create_long_form_weights\n    ensure_model_obj_has_mapping_constructor(model_obj)\n    # Ensure wide_weights is a 1D or 2D ndarray.\n    ensure_wide_weights_is_1D_or_2D_ndarray(wide_weights)\n    # Ensure rows_to_obs is a scipy sparse matrix\n    ensure_rows_to_obs_validity(rows_to_obs)\n    return None", "response": "Ensures that the args to create_long_form_weights are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a 2D array of weights for the long - format alternative.", "response": "def create_long_form_weights(model_obj, wide_weights, rows_to_obs=None):\n    \"\"\"\n    Converts an array of weights with one element per observation (wide-format)\n    to an array of weights with one element per observation per available\n    alternative (long-format).\n\n    Parameters\n    ----------\n    model_obj : an instance or sublcass of the MNDC class.\n        Should be the model object that corresponds to the model we are\n        constructing the bootstrap confidence intervals for.\n    wide_weights : 1D or 2D ndarray.\n        Should contain one element or one column per observation in\n        `model_obj.data`, depending on whether `wide_weights` is 1D or 2D\n        respectively. These elements should be the weights for optimizing the\n        model's objective function for estimation.\n    rows_to_obs : 2D scipy sparse array.\n        A mapping matrix of zeros and ones, were `rows_to_obs[i, j]` is one if\n        row `i` of the long-format data belongs to observation `j` and zero\n        otherwise.\n\n    Returns\n    -------\n    long_weights : 1D or 2D ndarray.\n        Should contain one element or one column per observation in\n        `model_obj.data`, depending on whether `wide_weights` is 1D or 2D\n        respectively. These elements should be the weights from `wide_weights`,\n        simply mapping each observation's weight to the corresponding row in\n        the long-format data.\n    \"\"\"\n    # Ensure argument validity\n    check_validity_of_long_form_args(model_obj, wide_weights, rows_to_obs)\n    # Get a rows_to_obs mapping matrix.\n    if rows_to_obs is None:\n        rows_to_obs = model_obj.get_mappings_for_fit()['rows_to_obs']\n    # Create a 2D version of\n    wide_weights_2d =\\\n        wide_weights if wide_weights.ndim == 2 else wide_weights[:, None]\n    long_weights = rows_to_obs.dot(wide_weights_2d)\n    if wide_weights.ndim == 1:\n        long_weights = long_weights.sum(axis=1)\n    return long_weights"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_finite_diff_terms_for_abc(model_obj,\n                                   mle_params,\n                                   init_vals,\n                                   epsilon,\n                                   **fit_kwargs):\n    \"\"\"\n    Calculates the terms needed for the finite difference approximations of\n    the empirical influence and second order empirical influence functions.\n\n    Parameters\n    ----------\n    model_obj : an instance or sublcass of the MNDC class.\n        Should be the model object that corresponds to the model we are\n        constructing the bootstrap confidence intervals for.\n    mle_params : 1D ndarray.\n        Should contain the desired model's maximum likelihood point estimate.\n    init_vals : 1D ndarray.\n        The initial values used to estimate the desired choice model.\n    epsilon : positive float.\n        Should denote the 'very small' value being used to calculate the\n        desired finite difference approximations to the various influence\n        functions. Should be 'close' to zero.\n    fit_kwargs : additional keyword arguments, optional.\n        Should contain any additional kwargs used to alter the default behavior\n        of `model_obj.fit_mle` and thereby enforce conformity with how the MLE\n        was obtained. Will be passed directly to `model_obj.fit_mle`.\n\n    Returns\n    -------\n    term_plus : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the finite difference term that comes from adding a small value\n        to the observation corresponding to that elements respective row.\n    term_minus : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the finite difference term that comes from subtracting a small\n        value to the observation corresponding to that elements respective row.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6, Equations 22.32 and 22.36.\n\n    Notes\n    -----\n    The returned, symbolic value for `term_minus` does not explicitly appear in\n    Equations 22.32 or 22.36. However, it is used to compute a midpoint / slope\n    approximation to the finite difference derivative used to define the\n    empirical influence function.\n    \"\"\"\n    # Determine the number of observations in this dataset.\n    num_obs = model_obj.data[model_obj.obs_id_col].unique().size\n    # Determine the initial weights per observation.\n    init_weights_wide = np.ones(num_obs, dtype=float) / num_obs\n    # Initialize wide weights for elements of the second order influence array.\n    init_wide_weights_plus = (1 - epsilon) * init_weights_wide\n    init_wide_weights_minus = (1 + epsilon) * init_weights_wide\n    # Initialize the second order influence array\n    term_plus = np.empty((num_obs, init_vals.shape[0]), dtype=float)\n    term_minus = np.empty((num_obs, init_vals.shape[0]), dtype=float)\n    # Get the rows_to_obs mapping matrix for this model.\n    rows_to_obs = model_obj.get_mappings_for_fit()['rows_to_obs']\n    # Extract the initial weights from the fit kwargs\n    new_fit_kwargs = deepcopy(fit_kwargs)\n    if fit_kwargs is not None and 'weights' in fit_kwargs:\n        orig_weights = fit_kwargs['weights']\n        del new_fit_kwargs['weights']\n    else:\n        orig_weights = 1\n    # Make sure we're just getting the point estimate\n    new_fit_kwargs['just_point'] = True\n\n    # Populate the second order influence array\n    for obs in xrange(num_obs):\n        # Note we create the long weights in a for-loop to avoid creating a\n        # num_obs by num_obs matrix, which may be a problem for large datasets\n        # Get the wide format weights for this observation\n        current_wide_weights_plus = init_wide_weights_plus.copy()\n        current_wide_weights_plus[obs] += epsilon\n\n        current_wide_weights_minus = init_wide_weights_minus.copy()\n        current_wide_weights_minus[obs] -= epsilon\n        # Get the long format weights for this observation\n        long_weights_plus =\\\n            (create_long_form_weights(model_obj, current_wide_weights_plus,\n                                      rows_to_obs=rows_to_obs) * orig_weights)\n        long_weights_minus =\\\n            (create_long_form_weights(model_obj,\n                                      current_wide_weights_minus,\n                                      rows_to_obs=rows_to_obs) * orig_weights)\n        # Get the needed influence estimates.\n        term_plus[obs] = model_obj.fit_mle(init_vals,\n                                           weights=long_weights_plus,\n                                           **new_fit_kwargs)['x']\n        term_minus[obs] = model_obj.fit_mle(init_vals,\n                                            weights=long_weights_minus,\n                                            **new_fit_kwargs)['x']\n    return term_plus, term_minus", "response": "Calculates the finite difference approximations of the terms needed for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the empirical influence array needed to compute the approximate boostrap confidence intervals.", "response": "def calc_empirical_influence_abc(term_plus,\n                                 term_minus,\n                                 epsilon):\n    \"\"\"\n    Calculates the finite difference, midpoint / slope approximation to the\n    empirical influence array needed to compute the approximate boostrap\n    confidence (ABC) intervals.\n\n    Parameters\n    ----------\n    term_plus : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the finite difference term that comes from adding a small value\n        to the observation corresponding to that elements respective row.\n    term_minus : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the finite difference term that comes from subtracting a small\n        value to the observation corresponding to that elements respective row.\n    epsilon : positive float.\n        Should denote the 'very small' value being used to calculate the\n        desired finite difference approximations to the various influence\n        functions. Should be 'close' to zero.\n\n    Returns\n    -------\n    empirical_influence : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the empirical influence of the associated observation on the\n        associated parameter.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6, Equation 22.32.\n\n    Notes\n    -----\n    This function is based off of the code in Efron's original Bootstrap\n    library, written in S-plus. It is a finite difference, midpoint or slope\n    approximation of Equation 22.32.\n    \"\"\"\n    # Calculate the denominator of each row of the second order influence array\n    denominator = 2 * epsilon\n    # Calculate the empirical influence array.\n    empirical_influence = np.zeros(term_plus.shape)\n    diff_idx = ~np.isclose(term_plus, term_minus, atol=1e-12, rtol=0)\n    if diff_idx.any():\n        empirical_influence[diff_idx] =\\\n            (term_plus[diff_idx] - term_minus[diff_idx]) / denominator\n    return empirical_influence"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the 2nd order approximation of an analytical approximation to the 2nd order approximation of an analytical approximation to the 2nd order approximation.", "response": "def calc_2nd_order_influence_abc(mle_params,\n                                 term_plus,\n                                 term_minus,\n                                 epsilon):\n    \"\"\"\n    Calculates either a 'positive' finite difference approximation or an\n    approximation of a 'positive' finite difference approximation to the the\n    2nd order empirical influence array needed to compute the approximate\n    boostrap confidence (ABC) intervals. See the 'Notes' section for more\n    information on the ambiguous function description.\n\n    Parameters\n    ----------\n    mle_params : 1D ndarray.\n        Should contain the desired model's maximum likelihood point estimate.\n    term_plus : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the finite difference term that comes from adding a small value\n        to the observation corresponding to that elements respective row.\n    term_minus : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the finite difference term that comes from subtracting a small\n        value to the observation corresponding to that elements respective row.\n    epsilon : positive float.\n        Should denote the 'very small' value being used to calculate the\n        desired finite difference approximations to the various influence\n        functions. Should be 'close' to zero.\n\n    Returns\n    -------\n    second_order_influence : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the second order empirical influence of the associated\n        observation on the associated parameter.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6, Equation 22.36.\n\n    Notes\n    -----\n    This function is based on the code in Efron's original Bootstrap library\n    written in S-plus. It is not equivalent to the 'positive' finite difference\n    approximation to Equation 22.36, where epsilon is set to a small float. The\n    reason for this discrepancy is becaue `term_minus` is not equal to the\n    third term in the numerator in Equation 22.36. That term uses\n    `(1 - epsilon)P^0` whereas `term_minus` uses `(1 + epsilon)P^0`. At the\n    limit, both of these terms would be `P^0` and therefore equal. I think\n    Efron's original code was making the assumption that the two terms are\n    approximately equal to conserve computational resources. Either that or\n    Equation 22.36, as printed, is incorrect because its third term really\n    should be `(1 + epsilon)P^0`.\n    \"\"\"\n    # Calculate the denominator of each row of the second order influence array\n    denominator = epsilon**2\n    # Initialize the second term used to calculate each row in the second order\n    # influence array\n    term_2 = np.broadcast_to(2 * mle_params, term_plus.shape)\n    # Calculate the second order empirical influence array.\n    second_order_influence = np.zeros(term_plus.shape, dtype=float)\n    # Only perform the calculations if the values in the component terms are\n    # sufficiently different. This is to prevent floating point errors. atol is\n    # simply set to a 'reasonably small' number.\n    diff_idx = ~np.isclose(term_plus + term_minus, term_2, atol=1e-12, rtol=0)\n    if diff_idx.any():\n        second_order_influence[diff_idx] =\\\n            ((term_plus[diff_idx] - term_2[diff_idx] + term_minus[diff_idx]) /\n             denominator)\n    return second_order_influence"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_influence_arrays_for_abc(model_obj,\n                                  mle_est,\n                                  init_values,\n                                  epsilon,\n                                  **fit_kwargs):\n    \"\"\"\n    Calculates the empirical influence array and the 2nd order empirical\n    influence array needed to compute the approximate boostrap confidence (ABC)\n    intervals.\n\n    Parameters\n    ----------\n    model_obj : an instance or sublcass of the MNDC class.\n        Should be the model object that corresponds to the model we are\n        constructing the bootstrap confidence intervals for.\n    mle_est : 1D ndarray.\n        Should contain the desired model's maximum likelihood point estimate.\n    init_vals : 1D ndarray.\n        The initial values used to estimate the desired choice model.\n    epsilon : positive float.\n        Should denote the 'very small' value being used to calculate the\n        desired finite difference approximations to the various influence\n        functions. Should be 'close' to zero.\n    fit_kwargs : additional keyword arguments, optional.\n        Should contain any additional kwargs used to alter the default behavior\n        of `model_obj.fit_mle` and thereby enforce conformity with how the MLE\n        was obtained. Will be passed directly to `model_obj.fit_mle`.\n\n    Returns\n    -------\n    empirical_influence : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the empirical influence of the associated observation on the\n        associated parameter.\n    second_order_influence : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the second order empirical influence of the associated\n        observation on the associated parameter.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6, Equations 22.32 and 22.36.\n    \"\"\"\n    # Calculate the two arrays of finite difference terms needed for the\n    # various influence arrays that we want to calculate.\n    term_plus, term_minus = calc_finite_diff_terms_for_abc(model_obj,\n                                                           mle_est,\n                                                           init_values,\n                                                           epsilon,\n                                                           **fit_kwargs)\n    # Calculate the empirical influence array.\n    empirical_influence =\\\n        calc_empirical_influence_abc(term_plus, term_minus, epsilon)\n    # Calculate the second order influence array.\n    second_order_influence =\\\n        calc_2nd_order_influence_abc(mle_est, term_plus, term_minus, epsilon)\n    # Return the desired terms\n    return empirical_influence, second_order_influence", "response": "Calculates the empirical influence arrays needed to compute the approximate boostrap confidence intervals for the given MNDC class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_std_error_abc(empirical_influence):\n    num_obs = empirical_influence.shape[0]\n    std_error = ((empirical_influence**2).sum(axis=0))**0.5 / num_obs\n    return std_error", "response": "Calculates the standard error of the MLE estimates for use in calculating\n    the approximate bootstrap confidence intervals."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_acceleration_abc(empirical_influence):\n    influence_cubed = empirical_influence**3\n    influence_squared = empirical_influence**2\n    numerator = influence_cubed.sum(axis=0)\n    denominator = 6 * (influence_squared.sum(axis=0))**1.5\n    acceleration = numerator / denominator\n    return acceleration", "response": "Calculates the acceleration constant for the approximate bootstrap based on the empirical influence of the associated observation on the ABC."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_bias_abc(second_order_influence):\n    num_obs = second_order_influence.shape[0]\n    constant = 2.0 * num_obs**2\n    bias = second_order_influence.sum(axis=0) / constant\n    return bias", "response": "Calculates the approximate bias of the MLE estimates for use in calculating\n    the approximate bootstrap confidence intervals."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_quadratic_coef_abc(model_object,\n                            mle_params,\n                            init_vals,\n                            empirical_influence,\n                            std_error,\n                            epsilon,\n                            **fit_kwargs):\n    \"\"\"\n    Calculates the quadratic coefficient needed to compute the approximate\n    boostrap confidence (ABC) intervals.\n\n    Parameters\n    ----------\n    model_object : an instance or sublcass of the MNDC class.\n        Should be the model object that corresponds to the model we are\n        constructing the bootstrap confidence intervals for.\n    mle_params : 1D ndarray.\n        Should contain the desired model's maximum likelihood point estimate.\n    init_vals : 1D ndarray.\n        The initial values used to estimate the desired choice model.\n    empirical_influence : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the empirical influence of the associated observation on the\n        associated parameter.\n    std_error : 1D ndarray.\n        Contains the standard error of the MLE estimates for use in the ABC\n        confidence intervals.\n    epsilon : positive float.\n        Should denote the 'very small' value being used to calculate the\n        desired finite difference approximations to the various influence\n        functions. Should be 'close' to zero.\n    fit_kwargs : additional keyword arguments, optional.\n        Should contain any additional kwargs used to alter the default behavior\n        of `model_obj.fit_mle` and thereby enforce conformity with how the MLE\n        was obtained. Will be passed directly to `model_obj.fit_mle`.\n    Returns\n    -------\n    quadratic_coef : 1D ndarray.\n        Contains a measure of nonlinearity of the MLE estimation function as\n        one moves in the 'least favorable direction.'\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6, Equation 22.37.\n    \"\"\"\n    # Determine the number of observations in this dataset.\n    num_obs = float(empirical_influence.shape[0])\n    # Calculate the standardized version of the empirical influence values\n    # assuming mean zero. This is the mean of empirical influence values\n    # (sum over n) divided by the standard error of the mean of the empiricial\n    # influence values, i.e. the square root of [n^2 * variance] = n * std_err\n    # where std_err is the standard error of the sampling distribution of the\n    # influence values.\n    standardized_influence =\\\n        empirical_influence / (num_obs**2 * std_error[None, :])\n    # Determine the initial weights per observation.\n    init_weights_wide = (np.ones(int(num_obs), dtype=float) / num_obs)[:, None]\n    # Create the wide weights for the various terms of the quadratic_coef\n    term_1_wide_weights =\\\n        (1 - epsilon) * init_weights_wide + epsilon * standardized_influence\n    term_3_wide_weights =\\\n        (1 - epsilon) * init_weights_wide - epsilon * standardized_influence\n    # Get the rows_to_obs mapping matrix for this model.\n    rows_to_obs = model_object.get_mappings_for_fit()['rows_to_obs']\n    # Initialize the terms for the quadratic coefficients.\n    expected_term_shape = (init_vals.shape[0], init_vals.shape[0])\n    term_1_array = np.empty(expected_term_shape, dtype=float)\n    term_3_array = np.empty(expected_term_shape, dtype=float)\n    # Extract the initial weights from the fit kwargs\n    new_fit_kwargs = deepcopy(fit_kwargs)\n    if fit_kwargs is not None and 'weights' in fit_kwargs:\n        orig_weights = fit_kwargs['weights']\n        del new_fit_kwargs['weights']\n    else:\n        orig_weights = 1\n    # Make sure we're just getting the point estimate\n    new_fit_kwargs['just_point'] = True\n    # Calculate the various terms of the quadratic_coef\n    for param_id in xrange(expected_term_shape[0]):\n        # Get a 'long-format' array of the weights per observation\n        term_1_long_weights =\\\n            (create_long_form_weights(model_object,\n                                      term_1_wide_weights[:, param_id],\n                                      rows_to_obs=rows_to_obs) * orig_weights)\n        term_3_long_weights =\\\n            (create_long_form_weights(model_object,\n                                      term_3_wide_weights[:, param_id],\n                                      rows_to_obs=rows_to_obs) * orig_weights)\n        # Populate the given row of the term_1 and term_3 arrays.\n        term_1_array[param_id] =\\\n            model_object.fit_mle(init_vals,\n                                 weights=term_1_long_weights,\n                                 **new_fit_kwargs)['x']\n        term_3_array[param_id] =\\\n            model_object.fit_mle(init_vals,\n                                 weights=term_3_long_weights,\n                                 **new_fit_kwargs)['x']\n    # Extract the desired terms from their 2d arrays\n    term_1 = np.diag(term_1_array)\n    term_3 = np.diag(term_3_array)\n    term_2 = 2 * mle_params\n    # Calculate the quadratic coefficient\n    quadratic_coef = np.zeros(term_1.shape, dtype=float)\n    denominator = epsilon**2\n    # Only perform the calculations if the values in the component terms are\n    # sufficiently different. This is to prevent floating point errors. Note\n    # that atol is simply set to a 'reasonably small' number.\n    diff_idx = ~np.isclose(term_1 + term_3, term_2, atol=1e-10, rtol=0)\n    if diff_idx.any():\n        quadratic_coef[diff_idx] =\\\n            ((term_1[diff_idx] - term_2[diff_idx] + term_3[diff_idx]) /\n             denominator)\n    return quadratic_coef", "response": "Calculates the quadratic coefficient needed to compute the approximate MNDC class for the given model object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_bias_correction_abc(acceleration, total_curvature):\n    inner_arg = 2 * norm.cdf(acceleration) * norm.cdf(-1 * total_curvature)\n    bias_correction = norm.ppf(inner_arg)\n    return bias_correction", "response": "Calculates the bias correction constant for the approximate bootstrap\n    confidence intervals."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_endpoint_from_percentile_abc(model_obj,\n                                      init_vals,\n                                      percentile,\n                                      bias_correction,\n                                      acceleration,\n                                      std_error,\n                                      empirical_influence,\n                                      **fit_kwargs):\n    \"\"\"\n    Calculates the endpoint of the 1-tailed, (percentile)% confidence interval.\n    Note this interval spans from negative infinity to the calculated endpoint.\n\n    Parameters\n    ----------\n    model_obj : an instance or sublcass of the MNDC class.\n        Should be the model object that corresponds to the model we are\n        constructing the bootstrap confidence intervals for.\n    init_vals : 1D ndarray.\n        The initial values used to estimate the desired choice model.\n    percentile : scalar in (0.0, 100.0).\n        Denotes the percentile of the standard normal distribution at which\n        we'd like to evaluate the inverse cumulative distribution function and\n        then convert this standardized value back to our approximate bootstrap\n        distribution.\n    bias_correction : 1D ndarray of scalars.\n        Contains the computed bias correction for the MLE estimates that the\n        ABC interval is being computed for.\n    acceleration : 1D ndarray of scalars.\n        Should contain the ABC intervals' estimated acceleration constants.\n    std_error : 1D ndarray.\n        Contains the standard error of the MLE estimates for use in the ABC\n        confidence intervals.\n    empirical_influence : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the empirical influence of the associated observation on the\n        associated parameter.\n    fit_kwargs : additional keyword arguments, optional.\n        Should contain any additional kwargs used to alter the default behavior\n        of `model_obj.fit_mle` and thereby enforce conformity with how the MLE\n        was obtained. Will be passed directly to `model_obj.fit_mle`.\n\n    Returns\n    -------\n    endpoint : 1D ndarray.\n        Contains the endpoint from our approximate bootstrap distribution's\n        1-tailed, upper `percentile` confidence interval.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6, Equation 22.33.\n    \"\"\"\n    # Get the bias corrected standard normal value for the relevant percentile.\n    # We multiply percentile by 0.01 because the ppf function requires decimals\n    # in [0.0, 1.0].\n    bias_corrected_z = bias_correction + norm.ppf(percentile * 0.01)\n    # Calculate the multiplier for the least favorable direction\n    # (i.e. the empirical influence function).\n    lam = bias_corrected_z / (1 - acceleration * bias_corrected_z)**2\n    multiplier = lam / std_error\n    # Calculate the initial weights\n    num_obs = empirical_influence.shape[0]\n    init_weights_wide = np.ones(num_obs, dtype=float)[:, None] / num_obs\n    # Get the necessary weight adjustment term for calculating the endpoint.\n    weight_adjustment_wide = (multiplier[None, :] * empirical_influence)\n    wide_weights_all_params = init_weights_wide + weight_adjustment_wide\n    # Extract the initial weights from the fit kwargs\n    new_fit_kwargs = deepcopy(fit_kwargs)\n    if fit_kwargs is not None and 'weights' in fit_kwargs:\n        orig_weights = fit_kwargs['weights']\n        del new_fit_kwargs['weights']\n    else:\n        orig_weights = np.ones(model_obj.data.shape[0], dtype=float)\n    # Make sure we're just getting the point estimate\n    new_fit_kwargs['just_point'] = True\n    # Get a long format version of the weights needed to compute the endpoints\n    long_weights_all_params =\\\n        (create_long_form_weights(model_obj, wide_weights_all_params) *\n         orig_weights[:, None])\n    # Initialize the array to store the desired enpoints\n    num_params = init_vals.shape[0]\n    endpoint = np.empty(num_params, dtype=float)\n    # Populate the endpoint array\n    for param_id in xrange(num_params):\n        current_weights = long_weights_all_params[:, param_id]\n        current_estimate = model_obj.fit_mle(init_vals,\n                                             weights=current_weights,\n                                             **new_fit_kwargs)['x']\n        endpoint[param_id] = current_estimate[param_id]\n    return endpoint", "response": "Calculates the endpoint of the MNDC class given a 1 - tailed percentile."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef efron_endpoints_for_abc_confidence_interval(conf_percentage,\n                                                model_obj,\n                                                init_vals,\n                                                bias_correction,\n                                                acceleration,\n                                                std_error,\n                                                empirical_influence,\n                                                **fit_kwargs):\n    \"\"\"\n    Calculates the endpoints of the equal-tailed, `conf_percentage`%\n    approximate bootstrap confidence (ABC) interval.\n\n    Parameters\n    ----------\n    conf_percentage : scalar in the interval (0.0, 100.0).\n        Denotes the confidence-level for the returned endpoints. For instance,\n        to calculate a 95% confidence interval, pass `95`.\n    model_obj : an instance or sublcass of the MNDC class.\n        Should be the model object that corresponds to the model we are\n        constructing the bootstrap confidence intervals for.\n    init_vals : 1D ndarray.\n        The initial values used to estimate the desired choice model.\n    bias_correction : 1D ndarray of scalars.\n        Contains the computed bias correction for the MLE estimates that the\n        ABC interval is being computed for.\n    acceleration : 1D ndarray of scalars.\n        Should contain the ABC intervals' estimated acceleration constants.\n    std_error : 1D ndarray.\n        Contains the standard error of the MLE estimates for use in the ABC\n        confidence intervals.\n    empirical_influence : 2D ndarray.\n        Should have one row for each observation. Should have one column for\n        each parameter in the parameter vector being estimated. Elements should\n        denote the empirical influence of the associated observation on the\n        associated parameter.\n    fit_kwargs : additional keyword arguments, optional.\n        Should contain any additional kwargs used to alter the default behavior\n        of `model_obj.fit_mle` and thereby enforce conformity with how the MLE\n        was obtained. Will be passed directly to `model_obj.fit_mle`.\n\n    Returns\n    -------\n    lower_endpoint, upper_endpoint : 1D ndarray.\n        Contains the lower or upper endpoint, respectively, from our\n        `conf_percentage`% ABC interval.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6.\n\n    Notes\n    -----\n    This function does not directly implement Equation 22.33. Instead, it\n    implements Efron's endpoint calculations from 'abcnon.R' in the 'bootstrap'\n    library in R. It is not clear where these calculations come from, and\n    if/how these calculations are equivalent to Equation 22.33.\n    \"\"\"\n    # Calculate the percentiles for the lower and upper endpoints\n    alpha_percent = get_alpha_from_conf_percentage(conf_percentage)\n    # Note that we divide by 100 because scipy.stats.norm.ppf only accepts\n    # floats between 0.0 and 1.0\n    lower_percentile = alpha_percent / 2.0\n    upper_percentile = 100 - lower_percentile\n    # Calculate the lower endpoint\n    lower_endpoint = efron_endpoint_from_percentile_abc(model_obj,\n                                                        init_vals,\n                                                        lower_percentile,\n                                                        bias_correction,\n                                                        acceleration,\n                                                        std_error,\n                                                        empirical_influence,\n                                                        **fit_kwargs)\n    # Calculate the upper endpoint\n    upper_endpoint = efron_endpoint_from_percentile_abc(model_obj,\n                                                        init_vals,\n                                                        upper_percentile,\n                                                        bias_correction,\n                                                        acceleration,\n                                                        std_error,\n                                                        empirical_influence,\n                                                        **fit_kwargs)\n    return lower_endpoint, upper_endpoint", "response": "Returns an array of endpoints that are used to estimate the Efron endpoints for the given confidence - level of the given ABC interval."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the approximate bootstrap confidence intervals for a given set of parameters.", "response": "def calc_abc_interval(model_obj,\n                      mle_params,\n                      init_vals,\n                      conf_percentage,\n                      epsilon=0.001,\n                      **fit_kwargs):\n    \"\"\"\n    Calculate 'approximate bootstrap confidence' intervals.\n\n    Parameters\n    ----------\n    model_obj : an instance or sublcass of the MNDC class.\n        Should be the model object that corresponds to the model we are\n        constructing the bootstrap confidence intervals for.\n    mle_params : 1D ndarray.\n        Should contain the desired model's maximum likelihood point estimate.\n    init_vals : 1D ndarray.\n        The initial values used to estimate the desired choice model.\n    conf_percentage : scalar in the interval (0.0, 100.0).\n        Denotes the confidence-level of the returned confidence interval. For\n        instance, to calculate a 95% confidence interval, pass `95`.\n    epsilon : positive float, optional.\n        Should denote the 'very small' value being used to calculate the\n        desired finite difference approximations to the various influence\n        functions. Should be close to zero. Default == sys.float_info.epsilon.\n    fit_kwargs : additional keyword arguments, optional.\n        Should contain any additional kwargs used to alter the default behavior\n        of `model_obj.fit_mle` and thereby enforce conformity with how the MLE\n        was obtained. Will be passed directly to `model_obj.fit_mle`.\n\n    Returns\n    -------\n    conf_intervals : 2D ndarray.\n        The shape of the returned array will be `(2, samples.shape[1])`. The\n        first row will correspond to the lower value in the confidence\n        interval. The second row will correspond to the upper value in the\n        confidence interval. There will be one column for each element of the\n        parameter vector being estimated.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 22.6.\n    DiCiccio, Thomas J., and Bradley Efron. \"Bootstrap confidence intervals.\"\n        Statistical science (1996): 189-212.\n    \"\"\"\n    # Check validity of arguments\n    check_conf_percentage_validity(conf_percentage)\n    # Calculate the empirical influence component and second order empirical\n    # influence component for each observation\n    empirical_influence, second_order_influence =\\\n        calc_influence_arrays_for_abc(model_obj,\n                                      mle_params,\n                                      init_vals,\n                                      epsilon,\n                                      **fit_kwargs)\n    # Calculate the acceleration constant for the ABC interval.\n    acceleration = calc_acceleration_abc(empirical_influence)\n    # Use the delta method to calculate the standard error of the MLE parameter\n    # estimate of the model using the original data.\n    std_error = calc_std_error_abc(empirical_influence)\n    # Approximate the bias of the MLE parameter estimates.\n    bias = calc_bias_abc(second_order_influence)\n\n    # Calculate the quadratic coefficient. Note we are using the 'efron'\n    # version of the desired function because the direct implementation of the\n    # formulas in the textbook don't return the correct results. The 'efron'\n    # versions re-implement the calculations from 'abcnon.R' in Efron's\n    # 'bootstrap' library in R.\n\n    # quadratic_coef = calc_quadratic_coef_abc(model_obj,\n    #                                          mle_params,\n    #                                          init_vals,\n    #                                          empirical_influence,\n    #                                          std_error,\n    #                                          epsilon,\n    #                                          **fit_kwargs)\n    quadratic_coef = efron_quadratic_coef_abc(model_obj,\n                                              mle_params,\n                                              init_vals,\n                                              empirical_influence,\n                                              std_error,\n                                              epsilon,\n                                              **fit_kwargs)\n\n    # Calculate the total curvature of the level surface of the weight vector,\n    # where the set of weights in the surface are those where the weighted MLE\n    # equals the original (i.e. the equal-weighted) MLE.\n    total_curvature = calc_total_curvature_abc(bias, std_error, quadratic_coef)\n    # Calculate the bias correction constant.\n    bias_correction = calc_bias_correction_abc(acceleration, total_curvature)\n\n    # Calculate the lower limit of the conf_percentage confidence intervals\n    # Note we are using the 'efron' version of the desired function because the\n    # direct implementation of the formulas in the textbook don't return the\n    # correct results. The 'efron' versions re-implement the calculations from\n    # 'abcnon.R' in Efron's 'bootstrap' library in R.\n\n    # lower_endpoint, upper_endpoint =\\\n    #     calc_endpoints_for_abc_confidence_interval(conf_percentage,\n    #                                                model_obj,\n    #                                                init_vals,\n    #                                                bias_correction,\n    #                                                acceleration,\n    #                                                std_error,\n    #                                                empirical_influence,\n    #                                                **fit_kwargs)\n    lower_endpoint, upper_endpoint =\\\n        efron_endpoints_for_abc_confidence_interval(conf_percentage,\n                                                    model_obj,\n                                                    init_vals,\n                                                    bias_correction,\n                                                    acceleration,\n                                                    std_error,\n                                                    empirical_influence,\n                                                    **fit_kwargs)\n    # Combine the enpoints into a single ndarray.\n    conf_intervals = combine_conf_endpoints(lower_endpoint, upper_endpoint)\n    return conf_intervals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate nested probabilities for the given nesting coefficients index_coefs design and rows_to_obs.", "response": "def calc_nested_probs(nest_coefs,\n                      index_coefs,\n                      design,\n                      rows_to_obs,\n                      rows_to_nests,\n                      chosen_row_to_obs=None,\n                      return_type=\"long_probs\",\n                      *args,\n                      **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    nest_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each nesting coefficient being estimated. If 2D, should\n        have 1 column for each set of nesting coefficients being used to\n        predict the probabilities of each alternative being chosen. There\n        should be one row per nesting coefficient. Elements denote the inverse\n        of the scale coefficients for each of the lower level nests.\n    index_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each utility coefficient being estimated (i.e.\n        num_features). If 2D, should have 1 column for each set of coefficients\n        being used to predict the probabilities of each alternative being\n        chosen. There should be one row per index coefficient.\n    design : 2D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs.\n    rows_to_obs : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_nests : 2D scipy sparse array.\n        There should with one row per observation per available alternative and\n        one column per nest. This matrix maps the rows of the design matrix to\n        the unique nests (on the columns).\n    chosen_row_to_obs : 2D scipy sparse array, or None, optional.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix indicates, for each observation\n        (on the columns), which rows of the design matrix were the realized\n        outcome. If an array is passed then an array of shape\n        (num_observations,) can be returned and each element will be the\n        probability of the realized outcome of the given observation.\n        Default == None.\n    return_type : str, optional.\n        Indicates what object(s) are to be returned from the function. Valid\n        values are: `['long_probs', 'chosen_probs', 'long_and_chosen_probs',\n        'all_prob_dict']`. If `long_probs`, the long format probabilities (a 1D\n        numpy array with one element per observation per available alternative)\n        will be returned. If `chosen_probs`, a 1D numpy array with one element\n        per observation will be returned, where the values are the\n        probabilities of the chosen alternative for the given observation. If\n        `long_and_chosen_probs`, a tuple of chosen_probs and long_probs will be\n        returned. If `all_prob_dict`, a dictionary will be returned. The values\n        will all be 1D numpy arrays of probabilities dictated by the value's\n        corresponding key. The keys will be `long_probs`, `nest_choice_probs`,\n        `prob_given_nest`, and `chosen_probs`. If chosen_row_to_obs is None,\n        then `chosen_probs` will be None. If `chosen_row_to_obs` is passed,\n        then `chosen_probs` will be a 1D array as described above.\n        `nest_choice_probs` is of the same shape as `rows_to_nests` and it\n        denotes the probability of each individual choosing each of the\n        possible nests. `prob_given_nest` is of the same shape as `long_probs`\n        and it denotes the probability of the individual associated with a\n        given row choosing the alternative associated with that row, given that\n        the individual chooses the nest that contains the given alternative.\n        Default == `long_probs`.\n\n    Returns\n    -------\n    See above for documentation of the `return_type` kwarg.\n    \"\"\"\n    # Check for 2D index coefficients or nesting coefficients\n    try:\n        assert len(index_coefs.shape) <= 2\n        assert (len(index_coefs.shape) == 1) or (index_coefs.shape[1] == 1)\n        assert len(nest_coefs.shape) <= 2\n        assert (len(nest_coefs.shape) == 1) or (nest_coefs.shape[1] == 1)\n    except AssertionError:\n        msg = \"Support for 2D index_coefs or nest_coefs not yet implemented.\"\n        raise NotImplementedError(msg)\n\n    # Check for kwarg validity\n    valid_return_types = ['long_probs',\n                          'chosen_probs',\n                          'long_and_chosen_probs',\n                          'all_prob_dict']\n    if return_type not in valid_return_types:\n        msg = \"return_type must be one of the following values: \"\n        raise ValueError(msg + str(valid_return_types))\n\n    chosen_probs_needed = ['chosen_probs', 'long_and_chosen_probs']\n    if chosen_row_to_obs is None and return_type in chosen_probs_needed:\n        msg = \"chosen_row_to_obs is None AND return_type in {}.\"\n        raise ValueError(msg.format(chosen_probs_needed) +\n                         \"\\nThis is invalid.\")\n\n    # Calculate the index for each alternative for each individual, V = X*beta\n    index_vals = design.dot(index_coefs)\n\n    # Get the long format nest parameters for each row of the design matrix\n    long_nest_coefs = rows_to_nests.dot(nest_coefs)\n\n    # Calculate the scaled index values (index / nest_param = V / lambda)\n    scaled_index = index_vals / long_nest_coefs\n\n    # Guard against overflow\n    pos_inf_idx = np.isposinf(scaled_index)\n    neg_inf_idx = np.isneginf(scaled_index)\n    scaled_index[pos_inf_idx] = max_comp_value\n    scaled_index[neg_inf_idx] = -1 * max_comp_value\n\n    # Calculate the e^(scaled-index) = exp(V / lambda)\n    exp_scaled_index = np.exp(scaled_index)\n\n    # Guard against overflow\n    inf_idx = np.isposinf(exp_scaled_index)\n    exp_scaled_index[inf_idx] = max_comp_value\n    # Guard against underflow. Note that I'm not sure this is the best place or\n    # best way to perform such guarding. If all of an observations indices\n    # suffer underflow, then we'll have 0 / 0 when calculating the\n    # probabilities and I should use L'Hopital's rule to get the correct\n    # probability. However, replacing underflowed values here may result in\n    # incorrectly assigning probabilities of either zero for all alternatives\n    # or 1 / num_alternatives for all alternatives.\n    zero_idx = (exp_scaled_index == 0)\n    exp_scaled_index[zero_idx] = min_comp_value\n\n    # Calculate the log-sum for each nest, for each observation. Note that the\n    # \"*\" is used to compute the dot product between the mapping matrix which\n    # is a scipy.sparse matrix and the second term which is a scipy sparse\n    # matrix. Note the dimensions of ind_log_sums_per_nest are (obs, nests).\n    # Calculates sum _{j \\in C_m} exp(V_{ij} / \\lambda_m) for each nest m.\n    ind_exp_sums_per_nest = (rows_to_obs.T *\n                             rows_to_nests.multiply(exp_scaled_index[:, None]))\n    # Ensure that ind_exp_sums_per_nest is an ndarray\n    if isinstance(ind_exp_sums_per_nest, np.matrixlib.defmatrix.matrix):\n        ind_exp_sums_per_nest = np.asarray(ind_exp_sums_per_nest)\n    elif issparse(ind_exp_sums_per_nest):\n        ind_exp_sums_per_nest = ind_exp_sums_per_nest.toarray()\n    # Guard against overflow\n    inf_idx = np.isposinf(ind_exp_sums_per_nest)\n    ind_exp_sums_per_nest[inf_idx] = max_comp_value\n\n    # Get the long-format representation of ind_log_sums_per_nest. Each row\n    # will have two columns, one for each nest. The entries of the matrix will\n    # be the log-sum for each nest, for the individual associated with the\n    # given row. The \"*\" is used to perform the dot product since rows_to_obs\n    # is a sparse matrix & ind_exp_sums_per_nest is a dense numpy matrix.\n    long_exp_sums_per_nest = rows_to_obs.dot(ind_exp_sums_per_nest)\n    if isinstance(long_exp_sums_per_nest, np.matrixlib.defmatrix.matrix):\n        long_exp_sums_per_nest = np.asarray(long_exp_sums_per_nest)\n\n    # Get the relevant log-sum for each row of the long-format data\n    # Note the .A converts the numpy matrix into a numpy array\n    # This is sum _{j \\in C_m} exp(V_{ij} / \\lambda_m) for the nest\n    # belonging to each row\n    long_exp_sums = (rows_to_nests.multiply(long_exp_sums_per_nest)\n                                  .sum(axis=1)\n                                  .A).ravel()\n\n    # Get the denominators for each individual\n    ind_denom = (np.power(ind_exp_sums_per_nest,\n                          nest_coefs[None, :])\n                   .sum(axis=1))\n    # Guard against overflow and underflow\n    inf_idx = np.isposinf(ind_denom)\n    ind_denom[inf_idx] = max_comp_value\n\n    zero_idx = (ind_denom == 0)\n    ind_denom[zero_idx] = min_comp_value\n\n    # Get the long format denominators.\n    long_denom = rows_to_obs.dot(ind_denom)\n    # Ensure that long_denom is 1D.\n    long_denom.ravel()\n\n    # Get the long format numerators\n    long_numerators = (exp_scaled_index *\n                       np.power(long_exp_sums,\n                                (long_nest_coefs - 1)))\n    # Guard agains overflow and underflow\n    inf_idx = np.isposinf(long_numerators)\n    long_numerators[inf_idx] = max_comp_value\n\n    zero_idx = (long_numerators == 0)\n    long_numerators[zero_idx] = min_comp_value\n\n    # Calculate and return the long-format probabilities\n    long_probs = (long_numerators / long_denom).ravel()\n    # Guard against underflow\n    long_probs[np.where(long_probs == 0)] = min_comp_value\n\n    # If desired, isolate the probabilities of the chosen alternatives\n    if chosen_row_to_obs is None:\n        chosen_probs = None\n    else:\n        # chosen_probs will be of shape (num_observations,)\n        chosen_probs = (chosen_row_to_obs.transpose()\n                                         .dot(long_probs))\n        chosen_probs = np.asarray(chosen_probs).ravel()\n\n    # Return the long form and chosen probabilities if desired\n    if return_type == 'long_and_chosen_probs':\n        return chosen_probs, long_probs\n    # If working with predictions, return just the long form probabilities\n    elif return_type == 'long_probs':\n        return long_probs\n    # If estimating the model and storing fitted probabilities or testing the\n    # model on data for which we know the chosen alternative, just return the\n    # chosen probabilities.\n    elif return_type == 'chosen_probs':\n        return chosen_probs\n    # If we want all the factors of the probability (e.g. as when calculating\n    # the gradient)\n    elif return_type == 'all_prob_dict':\n        # Create the dictionary of the various probabilities to be returned\n        prob_dict = {}\n        prob_dict[\"long_probs\"] = long_probs\n        prob_dict[\"chosen_probs\"] = chosen_probs\n\n        # Calculate the 'prob_given_nest' array\n        prob_given_nest = exp_scaled_index / long_exp_sums\n        # Guard against underflow\n        zero_idx = (prob_given_nest == 0)\n        prob_given_nest[zero_idx] = min_comp_value\n\n        # Calculate the 'nest_choice_probs'. Note ind_denom is a matrix with\n        # shape (num_obs, 1) so no need to explicitly broadcast\n        nest_choice_probs = (np.power(ind_exp_sums_per_nest,\n                                      nest_coefs[None, :]) /\n                             ind_denom[:, None])\n        # Guard against underflow\n        zero_idx = (nest_choice_probs == 0)\n        nest_choice_probs[zero_idx] = min_comp_value\n        # Return dictionary.\n        # Note the \".A\" converts the numpy matrix into a numpy array\n        prob_dict[\"prob_given_nest\"] = prob_given_nest\n        prob_dict[\"nest_choice_probs\"] = nest_choice_probs\n        prob_dict[\"ind_sums_per_nest\"] = ind_exp_sums_per_nest\n\n        return prob_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_nested_log_likelihood(nest_coefs,\n                               index_coefs,\n                               design,\n                               rows_to_obs,\n                               rows_to_nests,\n                               choice_vector,\n                               ridge=None,\n                               weights=None,\n                               *args,\n                               **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    nest_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each nesting coefficient being estimated. If 2D, should\n        have 1 column for each set of nesting coefficients being used to\n        predict the probabilities of each alternative being chosen. There\n        should be one row per nesting coefficient. Elements denote the inverse\n        of the scale coefficients for each of the lower level nests.\n    index_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each utility coefficient being estimated\n        (i.e. num_features). If 2D, should have 1 column for each set of\n        coefficients being used to predict the probabilities of choosing each\n        alternative. There should be one row per index coefficient.\n    design : 2D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs.\n    rows_to_obs : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_nests : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per nest. This matrix maps the rows of the design matrix to\n        the unique nests (on the columns).\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    ridge : int, float, long, or None, optional.\n        Determines whether or not ridge regression is performed. If an int,\n        float or long is passed, then that scalar determines the ridge penalty\n        for the optimization. Default = None.\n    weights : 1D ndarray or None, optional.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    log_likelihood : float.\n        The log likelihood of the nested logit model. Includes ridge penalty if\n        a penalized regression is being performed.\n    \"\"\"\n    # Calculate the probability of each individual choosing each available\n    # alternative for that individual.\n    long_probs = calc_nested_probs(nest_coefs,\n                                   index_coefs,\n                                   design,\n                                   rows_to_obs,\n                                   rows_to_nests,\n                                   return_type='long_probs')\n\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = 1\n\n    # Calculate the log likelihood\n    log_likelihood = choice_vector.dot(weights * np.log(long_probs))\n\n    if ridge is None:\n        return log_likelihood\n    else:\n        # Note that the 1.0 is used since the 'null' nest coefficient is equal\n        # to 1.0.\n        params = np.concatenate(((nest_coefs - 1.0), index_coefs), axis=0)\n\n        return log_likelihood - ridge * np.square(params).sum()", "response": "Calculates the nested log likelihood of the given nests."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prep_vectors_for_gradient(nest_coefs,\n                              index_coefs,\n                              design,\n                              choice_vec,\n                              rows_to_obs,\n                              rows_to_nests,\n                              *args,\n                              **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    nest_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each nesting coefficient being estimated. If 2D, should\n        have 1 column for each set of nesting coefficients being used to\n        predict the probabilities of each alternative being chosen. There\n        should be one row per nesting coefficient. Elements denote the inverse\n        of the scale coefficients for each of the lower level nests. Note, this\n        is NOT THE LOGIT of the inverse of the scale coefficients.\n    index_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each utility coefficient being estimated\n        (i.e. num_features). If 2D, should have 1 column for each set of\n        coefficients being used to predict the probabilities of choosing each\n        alternative. There should be one row per index coefficient.\n    design : 2D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated.\n        All elements should be ints, floats, or longs.\n    choice_vec : 1D ndarray.\n        All elements should by ints, floats, or longs. Each element represents\n        whether the individual associated with the given row chose the\n        alternative associated with the given row. Should have the same number\n        of rows as `design`.\n    rows_to_obs : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_nests : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per nest. This matrix maps the rows of the design matrix to\n        the unique nests (on the columns).\n\n    Returns\n    -------\n    desired_arrays : dict.\n        Will contain the arrays necessary for calculating the gradient of the\n        nested logit log-likelihood. The keys will be:\n            `[\"long_nest_params\", \"scaled_y\", \"long_chosen_nest\",\n              \"obs_to_chosen_nests\", \"p_tilde_given_nest\", \"long_probs\",\n              \"prob_given_nest\", \"nest_choice_probs\", \"ind_sums_per_nest\"]`\n    \"\"\"\n    # Create the \"long_nest_parameters\" which is an array with one element per\n    # alternative per observation, where each element is the nest parameter for\n    # the alternative corresponding to the given row\n    long_nest_params = (rows_to_nests.multiply(nest_coefs[None, :])\n                                     .sum(axis=1)\n                                     .A\n                                     .ravel())\n\n    # Calculate y-tilde\n    scaled_y = choice_vec / long_nest_params\n    # Guard against overflow\n    inf_index = np.isinf(scaled_y)\n    scaled_y[inf_index] = max_comp_value\n\n    # Determine which nest was chosen by each row's individual.\n    # Resulting matrix has shape (num_rows, num_nests)\n    obs_to_chosen_nests = (rows_to_obs.T *\n                           rows_to_nests.multiply(choice_vec[:, None])).A\n    row_to_chosen_nest = rows_to_obs * obs_to_chosen_nests\n    # Determine whether the given row is part of the nest that was chosen\n    long_chosen_nest = (rows_to_nests.multiply(row_to_chosen_nest)\n                                     .sum(axis=1)\n                                     .A\n                                     .ravel())\n\n    # Get the various probabilities\n    prob_dict = calc_nested_probs(nest_coefs,\n                                  index_coefs,\n                                  design,\n                                  rows_to_obs,\n                                  rows_to_nests,\n                                  return_type='all_prob_dict')\n\n    # Calculate p_tilde_ij_given_nest\n    p_tilde_row_given_nest = (prob_dict[\"prob_given_nest\"] *\n                              long_chosen_nest /\n                              long_nest_params)\n    # Guard against overflow\n    inf_index = np.isinf(p_tilde_row_given_nest)\n    p_tilde_row_given_nest[inf_index] = max_comp_value\n\n    # Return all the desired matrices and arrays\n    desired_arrays = {}\n    desired_arrays[\"long_nest_params\"] = long_nest_params.ravel()\n    desired_arrays[\"scaled_y\"] = scaled_y.ravel()\n    desired_arrays[\"long_chosen_nest\"] = long_chosen_nest\n    desired_arrays[\"obs_to_chosen_nests\"] = obs_to_chosen_nests\n    desired_arrays[\"p_tilde_given_nest\"] = p_tilde_row_given_nest\n    desired_arrays[\"long_probs\"] = prob_dict[\"long_probs\"]\n    desired_arrays[\"prob_given_nest\"] = prob_dict[\"prob_given_nest\"]\n    desired_arrays[\"nest_choice_probs\"] = prob_dict[\"nest_choice_probs\"]\n    desired_arrays[\"ind_sums_per_nest\"] = prob_dict[\"ind_sums_per_nest\"]\n\n    return desired_arrays", "response": "This function takes the coefficients of the nests and returns the vector vectors that can be used to predict the probabilities of each alternative in the specified nests."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef naturalize_nest_coefs(nest_coef_estimates):\n    # Calculate the exponential term of the\n    # logistic transformation\n    exp_term = np.exp(-1 * nest_coef_estimates)\n\n    # Guard against_overflow\n    inf_idx = np.isinf(exp_term)\n    exp_term[inf_idx] = max_comp_value\n\n    # Calculate the 'natural' nest coefficients\n    nest_coefs = 1.0 / (1.0 + exp_term)\n\n    # Guard against underflow\n    zero_idx = (nest_coefs == 0)\n    nest_coefs[zero_idx] = min_comp_value\n\n    return nest_coefs", "response": "This function calculates the natural nest coefficients for a logit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the gradient of the nested nests for the given entry point.", "response": "def calc_nested_gradient(orig_nest_coefs,\n                         index_coefs,\n                         design,\n                         choice_vec,\n                         rows_to_obs,\n                         rows_to_nests,\n                         ridge=None,\n                         weights=None,\n                         use_jacobian=True,\n                         *args,\n                         **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    orig_nest_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each nesting coefficient being estimated. If 2D, should\n        have 1 column for each set of nesting coefficients being used to\n        predict the probabilities of each alternative being chosen. There\n        should be one row per nesting coefficient. Elements denote the logit of\n        the inverse of the scale coefficients for each lower level nests.\n    index_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each utility coefficient being estimated\n        (i.e. num_features). If 2D, should have 1 column for each set of\n        coefficients being used to predict the probabilities of choosing each\n        alternative. There should be one row per index coefficient.\n    design : 2D ndarray.\n       There should be one row per observation per available alternative. There\n       should be one column per utility coefficient being estimated. All\n       elements should be ints, floats, or longs.\n    choice_vec : 1D ndarray.\n        All elements should by ints, floats, or longs. Each element represents\n        whether the individual associated with the given row chose the\n        alternative associated with the given row.\n    rows_to_obs : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_nests : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per nest. This matrix maps the rows of the design matrix to\n        the unique nests (on the columns).\n    ridge : int, float, long, or None, optional.\n        Determines whether or not ridge regression is performed. If an int,\n        float or long is passed, then that scalar determines the ridge penalty\n        for the optimization. Default `== None`.\n    weights : 1D ndarray or None.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n    use_jacobian : bool, optional.\n        Determines whether or not the jacobian will be used when calculating\n        the gradient. When performing model estimation, `use_jacobian` should\n        be `True` if the values being estimated are actually the logit of the\n        nest coefficients. Default `== True`.\n\n    Returns\n    -------\n    gradient : 1D numpy array.\n       The gradient of the log-likelihood with respect to the given nest\n       coefficients and index coefficients.\n    \"\"\"\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = np.ones(design.shape[0])\n    weights_per_obs = np.max(rows_to_obs.toarray() * weights[:, None], axis=0)\n\n    # Transform the nest coefficients into their \"always positive\" versions\n    nest_coefs = naturalize_nest_coefs(orig_nest_coefs)\n\n    # Get the vectors and matrices needed to calculate the gradient\n    vector_dict = prep_vectors_for_gradient(nest_coefs,\n                                            index_coefs,\n                                            design,\n                                            choice_vec,\n                                            rows_to_obs,\n                                            rows_to_nests)\n\n    # Calculate the index for each alternative for each person\n    sys_utility = design.dot(index_coefs)\n\n    # Calculate w_ij\n    long_w = sys_utility / vector_dict[\"long_nest_params\"]\n    # Guard against overflow\n    inf_index = np.isposinf(long_w)\n    long_w[inf_index] = max_comp_value\n\n    ##########\n    # Calculate d_log_likelihood_d_nest_params\n    ##########\n    # Calculate the term that onlny depends on nest level values\n    log_exp_sums = np.log(vector_dict[\"ind_sums_per_nest\"])\n    # Guard against overflow\n    log_exp_sums[np.isneginf(log_exp_sums)] = -1 * max_comp_value\n\n    # Calculate the first term of the derivative of the log-liikelihood\n    # with respect to the nest parameters\n    nest_gradient_term_1 = ((vector_dict[\"obs_to_chosen_nests\"] -\n                             vector_dict[\"nest_choice_probs\"]) *\n                            log_exp_sums *\n                            weights_per_obs[:, None]).sum(axis=0)\n\n    # Calculate the second term of the derivative of the log-liikelihood\n    # with respect to the nest parameters\n    half_deriv = ((vector_dict[\"long_probs\"] -\n                   vector_dict[\"long_chosen_nest\"] *\n                   vector_dict[\"prob_given_nest\"]) *\n                  long_w *\n                  weights)\n    nest_gradient_term_2 = (rows_to_nests.transpose()\n                                         .dot(half_deriv)[:, None]).ravel()\n\n    # Calculate the third term of the derivative of the log-likelihood\n    # with respect to the nest parameters\n    nest_gradient_term_3a = (choice_vec -\n                             vector_dict[\"long_chosen_nest\"] *\n                             vector_dict[\"prob_given_nest\"])\n    nest_gradient_term_3b = ((-1 * nest_gradient_term_3a * long_w * weights) /\n                             vector_dict[\"long_nest_params\"])\n    # Guard against overflow\n    inf_idx = np.isposinf(nest_gradient_term_3b)\n    nest_gradient_term_3b[inf_idx] = max_comp_value\n\n    neg_inf_idx = np.isneginf(nest_gradient_term_3b)\n    nest_gradient_term_3b[neg_inf_idx] = -1 * max_comp_value\n\n    # Get the nest-wide version of this piece of the gradient\n    nest_gradient_term_3 = (rows_to_nests.transpose()\n                                         .dot(nest_gradient_term_3b)).ravel()\n\n    # Combine the two terms. Note the \"nest_coefs * (1 - nest_coefs)\" is due to\n    # the fact that we're estimating the logit of the nest coefficients instead\n    # of the nest coefficient itself. We therefore need to multiply by\n    # d_nest_coef_d_estimated_variable to get the correct gradient.\n    # d_nest_coef_d_estimated_variable == nest_coefs * (1 - nest_coefs).\n    if use_jacobian:\n        jacobian = nest_coefs * (1.0 - nest_coefs)\n    else:\n        jacobian = 1\n    nest_gradient = ((nest_gradient_term_1 +\n                      nest_gradient_term_2 +\n                      nest_gradient_term_3) *\n                     jacobian)[None, :]\n\n    ##########\n    # Calculate d_loglikelihood_d_beta\n    ##########\n    beta_gradient_term_1 = ((vector_dict[\"scaled_y\"] -\n                             vector_dict[\"p_tilde_given_nest\"] +\n                             vector_dict[\"p_tilde_given_nest\"] *\n                             vector_dict[\"long_nest_params\"] -\n                             vector_dict[\"long_probs\"]) *\n                            weights)[None, :]\n    #####\n    # Calculate the derivative with respect to beta\n    #####\n    beta_gradient = beta_gradient_term_1.dot(design)\n\n    #####\n    # Combine the gradient pieces and account for ridge parameter\n    #####\n    gradient = np.concatenate((nest_gradient, beta_gradient), axis=1).ravel()\n\n    if ridge is not None:\n        # Note that the 20 is used in place of 'infinity' since I would really\n        # like to specify the expected value of the nest coefficient to 1, but\n        # that would make the logit of the nest parameter infinity. Instead I\n        # use 20 as a close enough value-- (1 + exp(-20))**-1 is approx. 1.\n        params = np.concatenate(((20 - orig_nest_coefs), index_coefs), axis=0)\n\n        gradient -= 2 * ridge * params\n\n    return gradient"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the Bhhh - Hessian approximation for a given set of nests.", "response": "def calc_bhhh_hessian_approximation(orig_nest_coefs,\n                                    index_coefs,\n                                    design,\n                                    choice_vec,\n                                    rows_to_obs,\n                                    rows_to_nests,\n                                    ridge=None,\n                                    weights=None,\n                                    use_jacobian=True,\n                                    *args,\n                                    **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    orig_nest_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each nesting coefficient being estimated. If 2D, should\n        have 1 column for each set of nesting coefficients being used to\n        predict the probabilities of each alternative being chosen. There\n        should be one row per nesting coefficient. Elements denote the inverse\n        of the scale coefficients for each of the lower level nests.\n    index_coefs : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each utility coefficient being estimated\n        (i.e. num_features). If 2D, should have 1 column for each set of\n        coefficients being used to predict the probabilities of choosing each\n        alternative. There should be one row per index coefficient.\n    design : 2D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs.\n    choice_vec : 1D ndarray.\n        All elements should by ints, floats, or longs. Each element represents\n        whether the individual associated with the given row chose the\n        alternative associated with the given row.\n    rows_to_obs : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_nests : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per nest. This matrix maps the rows of the design matrix to\n        the unique nests (on the columns).\n    ridge : int, float, long, or None, optional.\n        Determines whether or not ridge regression is performed. If an int,\n        float or long is passed, then that scalar determines the ridge penalty\n        for the optimization. Default == None. Note that if this parameter is\n        passed, the values of the BHHH matrix MAY BE INCORRECT since it is not\n        100% clear how penalization affects the information matrix.\n    use_jacobian : bool, optional.\n        Determines whether or not the jacobian will be used when calculating\n        the gradient. When performing model estimation, `use_jacobian` should\n        be `True` if the values that are actually being estimated are the\n        logit of the nest coefficients. Default `== False`.\n\n    Returns\n    -------\n    bhhh_matrix : 2D ndarray.\n       The negative of the sum of the outer products of the gradient of the\n       log-likelihood function for each observation.\n    \"\"\"\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = np.ones(design.shape[0])\n    weights_per_obs = np.max(rows_to_obs.toarray() * weights[:, None], axis=0)\n\n    # Transform the nest coefficients into their \"always positive\" versions\n    nest_coefs = naturalize_nest_coefs(orig_nest_coefs)\n\n    # Get the vectors and matrices needed to calculate the gradient\n    vector_dict = prep_vectors_for_gradient(nest_coefs,\n                                            index_coefs,\n                                            design,\n                                            choice_vec,\n                                            rows_to_obs,\n                                            rows_to_nests)\n\n    # Calculate the index for each alternative for each person\n    sys_utility = design.dot(index_coefs)\n\n    # Calculate w_ij\n    long_w = sys_utility / vector_dict[\"long_nest_params\"]\n    # Guard against overflow\n    inf_index = np.isposinf(long_w)\n    long_w[inf_index] = max_comp_value\n\n    ##########\n    # Calculate d_log_likelihood_d_nest_params\n    ##########\n    # Calculate the term that only depends on nest level values\n    log_exp_sums = np.log(vector_dict[\"ind_sums_per_nest\"])\n    # Guard against overflow\n    log_exp_sums[np.isneginf(log_exp_sums)] = -1 * max_comp_value\n\n    # Calculate the first term of the derivative of the log-liikelihood\n    # with respect to the nest parameters. Note we do not sum this object since\n    # we want the values at the 'individual' level, which they already are.\n    nest_gradient_term_1 = ((vector_dict[\"obs_to_chosen_nests\"] -\n                             vector_dict[\"nest_choice_probs\"]) *\n                            log_exp_sums)\n\n    # Calculate the second term of the derivative of the log-liikelihood\n    # with respect to the nest parameters\n    half_deriv = ((vector_dict[\"long_probs\"] -\n                   vector_dict[\"long_chosen_nest\"] *\n                   vector_dict[\"prob_given_nest\"]) *\n                  long_w)[:, None]\n    # \"Spread out\" the second term across the appropriate nests\n    spread_half_deriv = rows_to_nests.multiply(half_deriv)\n    # Aggregate the spread out half-derivatives to the individual level\n    # This object should have shape (num_obs, num_nests)\n    nest_gradient_term_2 = rows_to_obs.transpose().dot(spread_half_deriv).A\n\n    # Calculate the third term of the derivative of the log-likelihood\n    # with respect to the nest parameters\n    nest_gradient_term_3a = (choice_vec -\n                             vector_dict[\"long_chosen_nest\"] *\n                             vector_dict[\"prob_given_nest\"])\n\n    nest_gradient_term_3b = ((-1 * nest_gradient_term_3a * long_w) /\n                             vector_dict[\"long_nest_params\"])\n\n    # Guard against overflow\n    inf_idx = np.isposinf(nest_gradient_term_3b)\n    nest_gradient_term_3b[inf_idx] = max_comp_value\n\n    neg_inf_idx = np.isneginf(nest_gradient_term_3b)\n    nest_gradient_term_3b[neg_inf_idx] = -1 * max_comp_value\n\n    # Get the nest-wide version of this piece of the gradient\n    spread_out_term_3b = rows_to_nests.multiply(nest_gradient_term_3b[:, None])\n    nest_gradient_term_3 = rows_to_obs.transpose().dot(spread_out_term_3b).A\n\n    # Combine the terms. Note the \"nest_coefs * (1 - nest_coefs)\" is due to the\n    # fact that we're estimating the logit of the nest coefficients instead of\n    # the nest coefficient itself. We therefore need to multiply by\n    # d_nest_coef_d_estimated_variable to get the correct gradient.\n    # d_nest_coef_d_estimated_variable == nest_coefs * (1 - nest_coefs).\n    # As with the various nest_gradient_terms, the nest_gradient should be of\n    # shape (num_obs, num_nests)\n    if use_jacobian:\n        jacobian = (nest_coefs * (1.0 - nest_coefs))[None, :]\n    else:\n        jacobian = 1\n\n    nest_gradient = ((nest_gradient_term_1 +\n                      nest_gradient_term_2 +\n                      nest_gradient_term_3) *\n                     jacobian)\n\n    ##########\n    # Calculate d_loglikelihood_d_beta\n    ##########\n    beta_gradient_term_1 = (vector_dict[\"scaled_y\"] -\n                            vector_dict[\"p_tilde_given_nest\"] +\n                            vector_dict[\"p_tilde_given_nest\"] *\n                            vector_dict[\"long_nest_params\"] -\n                            vector_dict[\"long_probs\"])[:, None]\n    #####\n    # Calculate the derivative with respect to beta\n    #####\n    beta_gradient = rows_to_obs.T.dot(beta_gradient_term_1 * design)\n\n    #####\n    # Combine the gradient pieces\n    #####\n    gradient_matrix = np.concatenate((nest_gradient, beta_gradient), axis=1)\n\n    #####\n    # Compute and return the outer product of each row of the gradient\n    # with itself. Then sum these individual matrices together. The line below\n    # does the same computation just with less memory and time.\n    bhhh_matrix =\\\n        gradient_matrix.T.dot(weights_per_obs[:, None] * gradient_matrix)\n\n    if ridge is not None:\n        # The rational behind adding 2 * ridge is that the information\n        # matrix should approximate the hessian and in the hessian we subtract\n        # 2 * ridge at the end. We add 2 * ridge here, since we will multiply\n        # by negative one afterwards. I don't know if this is the correct way\n        # to calculate the Fisher Information Matrix in ridge regression\n        # models.\n        bhhh_matrix += 2 * ridge\n\n    # Note the \"-1\" is because we are approximating the Fisher information\n    # matrix which has a negative one in the front of it?\n    # Note that if we were using the bhhh_matrix to calculate the robust\n    # covariance matrix, then we would not multiply by negative one here.\n    return -1 * bhhh_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _uneven_utility_transform(systematic_utilities,\n                              alt_IDs,\n                              rows_to_alts,\n                              shape_params,\n                              intercept_params,\n                              intercept_ref_pos=None,\n                              *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        All elements should be ints, floats, or longs. Should contain the\n        systematic utilities of each observation per available alternative.\n        Note that this vector is formed by the dot product of the design matrix\n        with the vector of utility coefficients.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_alts : 2D scipy sparse matrix.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset. All\n        elements should be zeros or ones.\n    shape_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n    intercept_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. If J is the\n        total number of possible alternatives for the dataset being modeled,\n        there should be J-1 elements in the array.\n    intercept_ref_pos : int, or None, optional.\n        Specifies the index of the alternative, in the ordered array of unique\n        alternatives, that is not having its intercept parameter estimated (in\n        order to ensure identifiability). Should only be None if\n        `intercept_params` is None.\n\n    Returns\n    -------\n    transformed_utilities : 2D ndarray.\n        Should have shape `(systematic_utilities.shape[0], 1)`. The returned\n        array contains the transformed utility values for this model. All\n        elements will be ints, longs, or floats.\n    \"\"\"\n    # Convert the shape parameters back into their 'natural parametrization'\n    natural_shapes = np.exp(shape_params)\n    natural_shapes[np.isposinf(natural_shapes)] = max_comp_value\n    # Figure out what shape values correspond to each row of the\n    # systematic utilities\n    long_natural_shapes = rows_to_alts.dot(natural_shapes)\n\n    # Get the exponentiated neative utilities\n    exp_neg_utilities = np.exp(-1 * systematic_utilities)\n\n    # Get the log of 1 + exponentiated negative utilities\n    log_1_plus_exp_neg_utilitiles = np.log1p(exp_neg_utilities)\n\n    # Guard against overflow. Underflow not a problem since we add one to a\n    # near zero number and log of one will evaluate to zero\n    inf_idx = np.isinf(log_1_plus_exp_neg_utilitiles)\n    log_1_plus_exp_neg_utilitiles[inf_idx] = -1 * systematic_utilities[inf_idx]\n\n    # Get the exponentiated (negative utilities times the shape parameter)\n    exp_neg_shape_utilities = np.exp(-1 *\n                                     long_natural_shapes *\n                                     systematic_utilities)\n\n    # Get the log of 1 + exponentiated (negative utiltiies times the shape)\n    log_1_plus_exp_neg_shape_utilities = np.log1p(exp_neg_shape_utilities)\n\n    ##########\n    # Guard against overflow\n    ##########\n    # Check for any values which have gone off to positive infinity\n    inf_idx = np.isinf(log_1_plus_exp_neg_shape_utilities)\n    # Replace those values with an approximation of the true values by ignoring\n    # the \"1.\" The idea is that 1 + infinity ~ infinity so the effect of the +1\n    # on the log is minimal.\n    if np.any(inf_idx):\n        log_1_plus_exp_neg_shape_utilities[inf_idx] =\\\n              -1 * long_natural_shapes[inf_idx] * systematic_utilities[inf_idx]\n\n    # Calculate the transformed utility values\n    transformed_utilities = (systematic_utilities +\n                             log_1_plus_exp_neg_utilitiles -\n                             log_1_plus_exp_neg_shape_utilities)\n    # Perform a final guard against numbers that are too large to deal with\n    transformed_utilities[np.isposinf(transformed_utilities)] = max_comp_value\n    transformed_utilities[np.isneginf(transformed_utilities)] = -max_comp_value\n    transformed_utilities[np.isneginf(systematic_utilities)] = -max_comp_value\n\n    # Account for the outside intercept parameters if there are any.\n    if intercept_params is not None and intercept_ref_pos is not None:\n        # Get a list of all the indices (or row indices) corresponding to the\n        # alternatives whose intercept parameters are being estimated.\n        needed_idxs = range(rows_to_alts.shape[1])\n        needed_idxs.remove(intercept_ref_pos)\n\n        if len(intercept_params.shape) > 1 and intercept_params.shape[1] > 1:\n            # Get an array of zeros with shape\n            # (num_possible_alternatives, num_parameter_samples)\n            all_intercepts = np.zeros((rows_to_alts.shape[1],\n                                       intercept_params.shape[1]))\n            # For alternatives having their intercept estimated, replace the\n            # zeros with the current value of the estimated intercepts\n            all_intercepts[needed_idxs, :] = intercept_params\n        else:\n            # Get an array of zeros with shape (num_possible_alternatives,)\n            all_intercepts = np.zeros(rows_to_alts.shape[1])\n            # For alternatives having their intercept estimated, replace the\n            # zeros with the current value of the estimated intercepts\n            all_intercepts[needed_idxs] = intercept_params\n\n        # Add the intercept values to f(x, beta, c)\n        transformed_utilities += rows_to_alts.dot(all_intercepts)\n\n    # Be sure to return a 2D array since other functions will be expecting that\n    if len(transformed_utilities.shape) == 1:\n        transformed_utilities = transformed_utilities[:, np.newaxis]\n\n    return transformed_utilities", "response": "This function transforms the systematic utilities of each available alternative into a new array of utility values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _uneven_transform_deriv_v(systematic_utilities,\n                              alt_IDs,\n                              rows_to_alts,\n                              shape_params,\n                              output_array=None,\n                              *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        All elements should be ints, floats, or longs. Should contain the\n        systematic utilities of each observation per available alternative.\n        Note that this vector is formed by the dot product of the design matrix\n        with the vector of utility coefficients.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_alts : 2D scipy sparse matrix.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset. All\n        elements should be zeros or ones.\n    shape_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n    output_array : 2D scipy sparse array.\n        The array should be square and it should have\n        `systematic_utilities.shape[0]` rows. It's data is to be replaced with\n        the correct derivatives of the transformation vector with respect to\n        the vector of systematic utilities. This argument is NOT optional.\n\n    Returns\n    -------\n    output_array : 2D scipy sparse array.\n        The shape of the returned array is `(systematic_utilities.shape[0],\n        systematic_utilities.shape[0])`. The returned array specifies the\n        derivative of the transformed utilities with respect to the systematic\n        utilities. All elements are ints, floats, or longs.\n    \"\"\"\n    # Convert the shape parameters back into their 'natural parametrization'\n    natural_shapes = np.exp(shape_params)\n    natural_shapes[np.isposinf(natural_shapes)] = max_comp_value\n    # Figure out what shape values correspond to each row of the\n    # systematic utilities\n    long_shapes = rows_to_alts.dot(natural_shapes)\n\n    # Get the exponentiated neative utilities\n    exp_neg_utilities = np.exp(-1 * systematic_utilities)\n\n    # Get the exponentiated (utilities times the shape parameter)\n    exp_shape_utilities = np.exp(long_shapes * systematic_utilities)\n\n    # Calculate the derivative of h_ij with respect to v_ij\n    # Note that the derivative of h_ij with respect to any other systematic\n    # utility is zero.\n    derivs = (1.0 / (1.0 + exp_neg_utilities) +\n              long_shapes / (1.0 + exp_shape_utilities))\n\n    output_array.data = derivs\n\n    # Return the matrix of dh_dv. Note the off-diagonal entries are zero\n    # because each transformation only depends on its value of v and no others\n    return output_array", "response": "This function transforms the derivative of the systematic utilities of each available alternative to the given rows."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _uneven_transform_deriv_shape(systematic_utilities,\n                                  alt_IDs,\n                                  rows_to_alts,\n                                  shape_params,\n                                  output_array=None,\n                                  *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        All elements should be ints, floats, or longs. Should contain the\n        systematic utilities of each observation per available alternative.\n        Note that this vector is formed by the dot product of the design matrix\n        with the vector of utility coefficients.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_alts : 2D scipy sparse matrix.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset. All\n        elements should be zeros or ones.\n    shape_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n    output_array : 2D scipy sparse array.\n        The array should have shape `(systematic_utilities.shape[0],\n        shape_params.shape[0])`. It's data is to be replaced with the correct\n        derivatives of the transformation vector with respect to the vector of\n        shape parameters. This argument is NOT optional.\n\n    Returns\n    -------\n    output_array : 2D scipy sparse array.\n        The shape of the returned array is `(systematic_utilities.shape[0],\n        shape_params.shape[0])`. The returned array specifies the derivative of\n        the transformed utilities with respect to the shape parameters. All\n        elements are ints, floats, or longs.\n    \"\"\"\n    # Convert the shape parameters back into their 'natural parametrization'\n    natural_shapes = np.exp(shape_params)\n    natural_shapes[np.isposinf(natural_shapes)] = max_comp_value\n    # Figure out what shape values correspond to each row of the\n    # systematic utilities\n    long_shapes = rows_to_alts.dot(natural_shapes)\n\n    # Get the exponentiated (utilities times the shape parameter)\n    exp_shape_utilities = np.exp(long_shapes * systematic_utilities)\n\n    # Calculate the derivative of h_ij with respect to shape_j.\n    derivs = (systematic_utilities / (1.0 + exp_shape_utilities))\n    # Guard against overflow. Only for cases of systematic_utilities becomming\n    # huge. It is unlikely this safeguard will be needed.\n    derivs[np.isposinf(systematic_utilities)] = 0\n    # Guard against underflow from v --> -inf.\n    huge_index = np.isneginf(systematic_utilities)\n    derivs[huge_index] = -max_comp_value\n\n    # Return the matrix of dh_dshapes. Note the matrix should be of dimension\n    # (systematic_utilities.shape[0], shape_params.shape[0])\n    # Note that the \"* long_shapes\" accounts for the fact that the derivative\n    # of the natural shape parameters with resepect to the actual shape\n    # parameters being estimated is simply\n    # exp(actual shape parameters) = natural shape parameters. The\n    # multiplication comes from the chain rule.\n    output_array.data = derivs * long_shapes\n    return output_array", "response": "This function transforms the derivative of the systematic utilities with respect to the shape parameters of the model being used."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_calc_dh_dv(estimator):\n    dh_dv = diags(np.ones(estimator.design.shape[0]), 0, format='csr')\n    # Create a function that will take in the pre-formed matrix, replace its\n    # data in-place with the new data, and return the correct dh_dv on each\n    # iteration of the minimizer\n    calc_dh_dv = partial(_uneven_transform_deriv_v, output_array=dh_dv)\n    return calc_dh_dv", "response": "Create a function that can be used in the various gradient and hessian\n    calculations to calculate the derivative of the transformation with respect\n    to the index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_calc_dh_d_shape(estimator):\n    dh_d_shape = estimator.rows_to_alts.copy()\n    # Create a function that will take in the pre-formed matrix, replace its\n    # data in-place with the new data, and return the correct dh_dshape on each\n    # iteration of the minimizer\n    calc_dh_d_shape = partial(_uneven_transform_deriv_shape,\n                              output_array=dh_d_shape)\n    return calc_dh_d_shape", "response": "Create a function that can be used in the various gradient and hessian\n    calculations to calculate the derivative of the transformation with respect to the vector\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the initial values of the logit model are of the correct length. Raises a helpful ValueError if otherwise.", "response": "def check_length_of_initial_values(self, init_values):\n        \"\"\"\n        Ensures that `init_values` is of the correct length. Raises a helpful\n        ValueError if otherwise.\n\n        Parameters\n        ----------\n        init_values : 1D ndarray.\n            The initial values to start the optimizatin process with. There\n            should be one value for each index coefficient, outside intercept\n            parameter, and shape parameter being estimated.\n\n        Returns\n        -------\n        None.\n        \"\"\"\n        # Calculate the expected number of shape and index parameters\n        # Note the uneven logit model has one shape parameter per alternative.\n        num_alts = self.rows_to_alts.shape[1]\n        num_index_coefs = self.design.shape[1]\n\n        if self.intercept_ref_pos is not None:\n            assumed_param_dimensions = num_index_coefs + 2 * num_alts - 1\n        else:\n            assumed_param_dimensions = num_index_coefs + num_alts\n\n        if init_values.shape[0] != assumed_param_dimensions:\n            msg_1 = \"The initial values are of the wrong dimension.\"\n            msg_2 = \"It should be of dimension {}\"\n            msg_3 = \"But instead it has dimension {}\"\n            raise ValueError(msg_1 +\n                             msg_2.format(assumed_param_dimensions) +\n                             msg_3.format(init_values.shape[0]))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms a systematic utilities array into a 2D array.", "response": "def mnl_utility_transform(sys_utility_array, *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    sys_utility_array : ndarray.\n        Should have 1D or 2D. Should have been created by the dot product of a\n        design matrix and an array of index coefficients.\n\n    Returns\n    -------\n        systematic_utilities : 2D ndarray.\n            The input systematic utilities. If `sys_utility_array` is 2D, then\n            `sys_utility_array` is returned. Else, returns\n            `sys_utility_array[:, None]`.\n    \"\"\"\n    # Return a 2D array of systematic utility values\n    if len(sys_utility_array.shape) == 1:\n        systematic_utilities = sys_utility_array[:, np.newaxis]\n    else:\n        systematic_utilities = sys_utility_array\n\n    return systematic_utilities"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_length_of_init_values(design_3d, init_values):\n    if init_values.shape[0] != design_3d.shape[2]:\n        msg_1 = \"The initial values are of the wrong dimension. \"\n        msg_2 = \"They should be of dimension {}\".format(design_3d.shape[2])\n        raise ValueError(msg_1 + msg_2)\n\n    return None", "response": "Checks that the initial values of the current language are of the correct length given the design matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstore particular items in the results dictionary that are unique to mixed logit-type models. In particular, this function calculates and adds `sequence_probs` and `expanded_sequence_probs` to the results dictionary. The `constrained_pos` object is also stored to the results_dict. Parameters ---------- estimator : an instance of the MixedEstimator class. Should contain a `choice_vector` attribute that is a 1D ndarray representing the choices made for this model's dataset. Should also contain a `rows_to_mixers` attribute that maps each row of the long format data to a unit of observation that the mixing is being performed over. results_dict : dict. This dictionary should be the dictionary returned from scipy.optimize.minimize. In particular, it should have the following `long_probs` key. Returns ------- results_dict.", "response": "def add_mixl_specific_results_to_estimation_res(estimator, results_dict):\n    \"\"\"\n    Stores particular items in the results dictionary that are unique to mixed\n    logit-type models. In particular, this function calculates and adds\n    `sequence_probs` and `expanded_sequence_probs` to the results dictionary.\n    The `constrained_pos` object is also stored to the results_dict.\n\n    Parameters\n    ----------\n    estimator : an instance of the MixedEstimator class.\n        Should contain a `choice_vector` attribute that is a 1D ndarray\n        representing the choices made for this model's dataset. Should also\n        contain a `rows_to_mixers` attribute that maps each row of the long\n        format data to a unit of observation that the mixing is being performed\n        over.\n    results_dict : dict.\n        This dictionary should be the dictionary returned from\n        scipy.optimize.minimize. In particular, it should have the following\n        `long_probs` key.\n\n    Returns\n    -------\n    results_dict.\n    \"\"\"\n    # Get the probability of each sequence of choices, given the draws\n    prob_res = mlc.calc_choice_sequence_probs(results_dict[\"long_probs\"],\n                                              estimator.choice_vector,\n                                              estimator.rows_to_mixers,\n                                              return_type='all')\n    # Add the various items to the results_dict.\n    results_dict[\"simulated_sequence_probs\"] = prob_res[0]\n    results_dict[\"expanded_sequence_probs\"] = prob_res[1]\n\n    return results_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits parameter vector into shape intercept and index parameters.", "response": "def convenience_split_params(self, params, return_all_types=False):\n        \"\"\"\n        Splits parameter vector into shape, intercept, and index parameters.\n\n        Parameters\n        ----------\n        params : 1D ndarray.\n            The array of parameters being estimated or used in calculations.\n        return_all_types : bool, optional.\n            Determines whether or not a tuple of 4 elements will be returned\n            (with one element for the nest, shape, intercept, and index\n            parameters for this model). If False, a tuple of 3 elements will\n            be returned with one element for the shape, intercept, and index\n            parameters.\n\n        Returns\n        -------\n        tuple. Will have 4 or 3 elements based on `return_all_types`.\n        \"\"\"\n        return self.split_params(params,\n                                 return_all_types=return_all_types)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convenience_calc_probs(self, params):\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        prob_args = (betas,\n                     self.design_3d,\n                     self.alt_id_vector,\n                     self.rows_to_obs,\n                     self.rows_to_alts,\n                     self.utility_transform)\n        prob_kwargs = {\"chosen_row_to_obs\": self.chosen_row_to_obs,\n                       \"return_long_probs\": True}\n        probability_results = general_calc_probabilities(*prob_args,\n                                                         **prob_kwargs)\n\n        return probability_results", "response": "Calculates the probabilities of the chosen alternative and the long\n        format probabilities for this model and dataset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the log - likelihood for this model and dataset.", "response": "def convenience_calc_log_likelihood(self, params):\n        \"\"\"\n        Calculates the log-likelihood for this model and dataset.\n        \"\"\"\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        args = [betas,\n                self.design_3d,\n                self.alt_id_vector,\n                self.rows_to_obs,\n                self.rows_to_alts,\n                self.rows_to_mixers,\n                self.choice_vector,\n                self.utility_transform]\n\n        kwargs = {\"ridge\": self.ridge, \"weights\": self.weights}\n        log_likelihood = general_log_likelihood(*args, **kwargs)\n\n        return log_likelihood"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convenience_calc_gradient(self, params):\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        args = [betas,\n                self.design_3d,\n                self.alt_id_vector,\n                self.rows_to_obs,\n                self.rows_to_alts,\n                self.rows_to_mixers,\n                self.choice_vector,\n                self.utility_transform]\n\n        return general_gradient(*args, ridge=self.ridge, weights=self.weights)", "response": "Calculates the gradient of the log - likelihood for this model / dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convenience_calc_hessian(self, params):\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        args = [betas,\n                self.design_3d,\n                self.alt_id_vector,\n                self.rows_to_obs,\n                self.rows_to_alts,\n                self.rows_to_mixers,\n                self.choice_vector,\n                self.utility_transform]\n\n        approx_hess =\\\n            general_bhhh(*args, ridge=self.ridge, weights=self.weights)\n\n        # Account for the constrained position when presenting the results of\n        # the approximate hessian.\n        if self.constrained_pos is not None:\n            for idx_val in self.constrained_pos:\n                approx_hess[idx_val, :] = 0\n                approx_hess[:, idx_val] = 0\n                approx_hess[idx_val, idx_val] = -1\n\n        return approx_hess", "response": "Calculates the hessian of the log - likelihood for this model / dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the BHHH approximation of the Fisher Information Matrix for the given set of parameters.", "response": "def convenience_calc_fisher_approx(self, params):\n        \"\"\"\n        Calculates the BHHH approximation of the Fisher Information Matrix for\n        this model / dataset. Note that this function name is INCORRECT with\n        regard to the actual actions performed. The Mixed Logit model uses a\n        placeholder for the BHHH approximation of the Fisher Information Matrix\n        because the BHHH approximation is already being used to approximate the\n        hessian.\n\n        This placeholder allows calculation of a value for the 'robust'\n        standard errors, even though such a value is not useful since it is not\n        correct...\n        \"\"\"\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        placeholder_bhhh = np.diag(-1 * np.ones(betas.shape[0]))\n\n        return placeholder_bhhh"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfitting the MLE algorithm to obtain the log likelihood of the wealternate data from the initial values of the initial values of the normal distribution.", "response": "def fit_mle(self,\n                init_vals,\n                num_draws,\n                seed=None,\n                constrained_pos=None,\n                print_res=True,\n                method=\"BFGS\",\n                loss_tol=1e-06,\n                gradient_tol=1e-06,\n                maxiter=1000,\n                ridge=None,\n                just_point=False,\n                **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        init_vals : 1D ndarray.\n            Should contain the initial values to start the optimization process\n            with. There should be one value for each utility coefficient and\n            shape parameter being estimated.\n        num_draws : int.\n            Should be greater than zero. Denotes the number of draws that we\n            are making from each normal distribution.\n        seed : int or None, optional.\n            If an int is passed, it should be greater than zero. Denotes the\n            value to be used in seeding the random generator used to generate\n            the draws from the normal distribution. Default == None.\n        constrained_pos : list or None, optional.\n            Denotes the positions of the array of estimated parameters that are\n            not to change from their initial values. If a list is passed, the\n            elements are to be integers where no such integer is greater than\n            `init_values.size.` Default == None.\n        print_res : bool, optional.\n            Determines whether the timing and initial and final log likelihood\n            results will be printed as they they are determined.\n        method : str, optional.\n            Should be a valid string which can be passed to\n            scipy.optimize.minimize. Determines the optimization algorithm\n            that is used for this problem.\n        loss_tol : float, optional.\n            Determines the tolerance on the difference in objective function\n            values from one iteration to the next which is needed to determine\n            convergence. Default = 1e-06.\n        gradient_tol : float, optional.\n            Determines the tolerance on the difference in gradient values from\n            one iteration to the next which is needed to determine convergence.\n            Default = 1e-06.\n        maxiter : int, optional.\n            Denotes the maximum number of iterations of the algorithm specified\n            by `method` that will be used to estimate the parameters of the\n            given model. Default == 1000.\n        ridge : int, float, long, or None, optional.\n            Determines whether or not ridge regression is performed. If a float\n            is passed, then that float determines the ridge penalty for the\n            optimization. Default = None.\n        just_point : bool, optional.\n            Determines whether (True) or not (False) calculations that are non-\n            critical for obtaining the maximum likelihood point estimate will\n            be performed. If True, this function will return the results\n            dictionary from scipy.optimize. Default == False.\n\n        Returns\n        -------\n        None. Estimation results are saved to the model instance.\n        \"\"\"\n        # Check integrity of passed arguments\n        kwargs_to_be_ignored = [\"init_shapes\", \"init_intercepts\", \"init_coefs\"]\n        if any([x in kwargs for x in kwargs_to_be_ignored]):\n            msg = \"MNL model does not use of any of the following kwargs:\\n{}\"\n            msg_2 = \"Remove such kwargs and pass a single init_vals argument\"\n            raise ValueError(msg.format(kwargs_to_be_ignored) + msg_2)\n\n        # Store the optimization method\n        self.optimization_method = method\n\n        # Store the ridge parameter\n        self.ridge_param = ridge\n\n        if ridge is not None:\n            warnings.warn(_ridge_warning_msg)\n\n        # Construct the mappings from alternatives to observations and from\n        # chosen alternatives to observations\n        mapping_res = self.get_mappings_for_fit()\n        rows_to_mixers = mapping_res[\"rows_to_mixers\"]\n\n        # Get the draws for each random coefficient\n        num_mixing_units = rows_to_mixers.shape[1]\n        draw_list = mlc.get_normal_draws(num_mixing_units,\n                                         num_draws,\n                                         len(self.mixing_pos),\n                                         seed=seed)\n\n        # Create the 3D design matrix\n        self.design_3d = mlc.create_expanded_design_for_mixing(self.design,\n                                                               draw_list,\n                                                               self.mixing_pos,\n                                                               rows_to_mixers)\n\n        # Create the estimation object\n        zero_vector = np.zeros(init_vals.shape)\n        mixl_estimator = MixedEstimator(self,\n                                        mapping_res,\n                                        ridge,\n                                        zero_vector,\n                                        split_param_vec,\n                                        constrained_pos=constrained_pos)\n\n        # Perform one final check on the length of the initial values\n        mixl_estimator.check_length_of_initial_values(init_vals)\n\n        # Get the estimation results\n        estimation_res = estimate(init_vals,\n                                  mixl_estimator,\n                                  method,\n                                  loss_tol,\n                                  gradient_tol,\n                                  maxiter,\n                                  print_res,\n                                  use_hessian=True,\n                                  just_point=just_point)\n\n        if not just_point:\n            # Store the mixed logit specific estimation results\n            args = [mixl_estimator, estimation_res]\n            estimation_res = add_mixl_specific_results_to_estimation_res(*args)\n\n            # Store the estimation results\n            self.store_fit_results(estimation_res)\n\n            return None\n        else:\n            return estimation_res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters the past mappings.", "response": "def __filter_past_mappings(self,\n                               past_mappings,\n                               long_inclusion_array):\n        \"\"\"\n        Parameters\n        ----------\n        past_mappings : dict.\n            All elements should be None or compressed sparse row matrices from\n            scipy.sparse. The following keys should be in past_mappings:\n\n            - \"rows_to_obs\",\n            - \"rows_to_alts\",\n            - \"chosen_rows_to_obs\",\n            - \"rows_to_nests\",\n            - \"rows_to_mixers\"\n\n            The values that are not None should be 'mapping' matrices that\n            denote which rows of the past long-format design matrix belong to\n            which unique object such as unique observations, unique\n            alternatives, unique nests, unique 'mixing' units etc.\n        long_inclusion_array : 1D ndarray.\n            Should denote, via a `1`, the rows of the past mapping matrices\n            that are to be included in the filtered mapping matrices.\n\n        Returns\n        -------\n        new_mappings : dict.\n            The returned dictionary will be the same as `past_mappings` except\n            that all the mapping matrices will have been filtered according to\n            `long_inclusion_array`.\n        \"\"\"\n        new_mappings = {}\n        for key in past_mappings:\n            if past_mappings[key] is None:\n                new_mappings[key] = None\n            else:\n                mask_array = long_inclusion_array[:, None]\n                orig_map = past_mappings[key]\n                # Initialize the resultant array that is desired\n                new_map = orig_map.multiply(np.tile(mask_array,\n                                                    (1, orig_map.shape[1]))).A\n                # Perform the desired filtering\n                current_filter = (new_map.sum(axis=1) != 0)\n                if current_filter.shape[0] > 0:\n                    current_filter = current_filter.ravel()\n                    new_map = new_map[current_filter, :]\n                # Do the second filtering\n                current_filter = (new_map.sum(axis=0) != 0)\n                if current_filter.shape[0] > 0:\n                    current_filter = current_filter.ravel()\n                    new_map = new_map[:, current_filter]\n\n                new_mappings[key] = csr_matrix(new_map)\n\n        return new_mappings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _convert_eta_to_c(eta, ref_position):\n    # Exponentiate eta\n    exp_eta = np.exp(eta)\n\n    # Guard against overflow\n    exp_eta[np.isposinf(exp_eta)] = max_comp_value\n    # Guard against underflow\n    exp_eta[exp_eta == 0] = min_comp_value\n\n    # Calculate the denominator in a logistic transformation\n    # Note the +1 is for the reference alternative which has been\n    # constrained so that its corresponding eta = 0 and exp(0) = 1\n    denom = exp_eta.sum(axis=0) + 1\n\n    # Get a list of all the indices (or row indices) corresponding to the\n    # alternatives whose shape parameters are being estimated.\n    replace_list = list(range(eta.shape[0] + 1))\n    replace_list.remove(ref_position)\n\n    # Initialize an array for the vector of shape parameters, c\n    if len(eta.shape) > 1 and eta.shape[1] > 1:\n        # Get an array of zeros with shape\n        # (num_possible_alternatives, num_parameter_samples). This is used when\n        # working with samples from a Bayesian posterior distribution\n        c_vector = np.zeros((eta.shape[0] + 1,\n                             eta.shape[1]))\n\n        # Calculate the natural shape parameters\n        c_vector[replace_list, :] = exp_eta / denom\n        c_vector[ref_position, :] = 1.0 / denom\n    else:\n        # Get an array of zeros with shape (num_possible_alternatives,)\n        c_vector = np.zeros(eta.shape[0] + 1)\n\n        # Calculate the natural shape parameters\n        c_vector[replace_list] = exp_eta / denom\n        c_vector[ref_position] = 1.0 / denom\n\n    return c_vector", "response": "Converts the natural log of the array eta to the c vector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calc_deriv_c_with_respect_to_eta(natural_shapes,\n                                      ref_position,\n                                      output_array=None):\n    \"\"\"\n    Parameters\n    ----------\n    natural_shapes : 1D ndarray.\n        Should have one element per available alternative in the dataset whose\n        choice situations are being modeled. Should have at least\n        `ref_position` elements in it.\n    ref_position : int.\n        Specifies the position in the array of natural shape parameters that\n        should be equal to 1 - the sum of the other elements. Specifies the\n        alternative in the ordered array of unique alternatives that is not\n        having its shape parameter estimated (in order to ensure\n        identifiability).\n    output_array : 2D ndarray.\n        This array is to have its data overwritten with the correct derivatives\n        of the natural shape parameters with respect to transformed shape\n        parameters. Should have shape ==\n        `(natural_shapes.shape[0], natural_shapes.shape[0] - 1)`.\n\n    Returns\n    -------\n    output_array : 2D ndarray.\n        Has shape == (natural_shapes.shape[0], natural_shapes.shape[0] - 1).\n        Will contain the derivative of the shape parameters, with\n        respect to the underlying 'transformed' shape parameters.\n    \"\"\"\n    # Generate a list of the indices which indicate the columns to be\n    # selected from a 2D numpy array of\n    # np.diag(natural_shapes) - np.outer(natural_shapes, natural_shapes)\n    columns_to_be_kept = range(natural_shapes.shape[0])\n    columns_to_be_kept.remove(ref_position)\n\n    # Calculate and store the derivative of the natural shape parameters\n    # with respect to the reduced shape parameters.\n    output_array[:, :] = (np.diag(natural_shapes) -\n                          np.outer(natural_shapes,\n                                   natural_shapes))[:, columns_to_be_kept]\n\n    return output_array", "response": "Calculates the derivative of the natural shape parameters with respect to eta."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _asym_transform_deriv_v(systematic_utilities,\n                            alt_IDs,\n                            rows_to_alts,\n                            eta,\n                            ref_position=None,\n                            output_array=None,\n                            *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        Contains the systematic utilities for each each available alternative\n        for each observation. All elements should be ints, floats, or longs.\n    alt_IDs : 1D ndarray.\n       All elements should be ints. There should be one row per obervation per\n       available alternative for the given observation. Elements denote the\n       alternative corresponding to the given row of the design matrix.\n    rows_to_alts : 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset.\n    eta : 1D ndarray.\n        Each element should be an int, float, or long. There should be one\n        value per transformed shape parameter. Note that if there are J\n        possible alternatives in the dataset, then there should be J - 1\n        elements in `eta`.\n    ref_position : int.\n        Specifies the position in the array of natural shape parameters that\n        should be equal to 1 - the sum of the other elements. Specifies the\n        alternative in the ordered array of unique alternatives that is not\n        having its shape parameter estimated (to ensure identifiability).\n    output_array : 2D scipy sparse matrix.\n        This matrix's data is to be replaced with the correct derivatives of\n        the transformation vector with respect to the vector of systematic\n        utilities.\n\n    Returns\n    -------\n    output_array : 2D scipy sparse matrix.\n        Will be a square matrix with `systematic_utilities.shape[0]` rows and\n        columns. `output_array` specifies the derivative of the transformed\n        utilities with respect to the index, V.\n    \"\"\"\n    ##########\n    # Convert the reduced shape parameters to the natural shape parameters\n    ##########\n    natural_shape_params = _convert_eta_to_c(eta, ref_position)\n\n    ##########\n    # Calculate the derivative of the transformed utilities with respect to\n    # the systematic utilities\n    ##########\n    # Create a vector which contains the appropriate shape for each row in the\n    # design matrix\n    long_shapes = rows_to_alts.dot(natural_shape_params)\n\n    # Determine how many alternatives there are\n    num_alts = rows_to_alts.shape[1]\n\n    # Get the natural log of the long_shapes\n    log_long_shapes = np.log(long_shapes)\n    # Guard against underflow, aka long_shapes too close to zero.\n    # I assume this should never happen because convert_eta_to_c never outputs\n    # zeros, by design.\n    log_long_shapes[np.isneginf(log_long_shapes)] = -1 * max_comp_value\n\n    # Get the natural log of (1 - long_shapes) / (num_alts - 1)\n    log_1_sub_long_shapes = np.log((1 - long_shapes) /\n                                   (num_alts - 1))\n    # Guard against underflow, aka 1 - long_shapes too close to zero.\n    small_idx = np.isneginf(log_1_sub_long_shapes)\n    log_1_sub_long_shapes[small_idx] = -1 * max_comp_value\n\n    # Calculate the derivative of h_ij with respect to v_ij\n    # Note that the derivative of h_ij with respect to any other systematic\n    # utility is zero.\n    derivs = -1 * ((systematic_utilities >= 0).astype(int) *\n                   log_long_shapes +\n                   (systematic_utilities < 0).astype(int) *\n                   log_1_sub_long_shapes)\n\n    output_array.data = derivs\n\n    # Return the matrix of dh_dv. Note the off-diagonal entries are zero\n    # because each transformation only depends on its value of v and no others\n    return output_array", "response": "This function transforms the derivative of the systematic utilities with respect to the index of the other elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a function that can be used in the various gradient and hessian calculations to calculate the derivative of the transformation with respect to the index.", "response": "def create_calc_dh_dv(estimator):\n    \"\"\"\n    Return the function that can be used in the various gradient and hessian\n    calculations to calculate the derivative of the transformation with respect\n    to the index.\n\n    Parameters\n    ----------\n    estimator : an instance of the estimation.LogitTypeEstimator class.\n        Should contain a `design` attribute that is a 2D ndarray representing\n        the design matrix for this model and dataset.\n\n    Returns\n    -------\n    Callable.\n        Will accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, (shape parameters if there are any) and miscellaneous\n        args and kwargs. Should return a 2D array whose elements contain the\n        derivative of the tranformed utility vector with respect to the vector\n        of systematic utilities. The dimensions of the returned vector should\n        be `(design.shape[0], design.shape[0])`.\n    \"\"\"\n    dh_dv = diags(np.ones(estimator.design.shape[0]), 0, format='csr')\n    # Create a function that will take in the pre-formed matrix, replace its\n    # data in-place with the new data, and return the correct dh_dv on each\n    # iteration of the minimizer\n    calc_dh_dv = partial(_asym_transform_deriv_v,\n                         ref_position=estimator.shape_ref_pos,\n                         output_array=dh_dv)\n    return calc_dh_dv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_calc_dh_d_shape(estimator):\n    num_alts = estimator.rows_to_alts.shape[1]\n    pre_dc_d_eta = np.zeros((num_alts, num_alts - 1), dtype=float)\n\n    pre_dh_dc = estimator.rows_to_alts.copy()\n\n    pre_dh_d_eta = np.matrix(np.zeros((estimator.design.shape[0],\n                                       num_alts - 1), dtype=float))\n\n    easy_calc_dc_d_eta = partial(_calc_deriv_c_with_respect_to_eta,\n                                 output_array=pre_dc_d_eta)\n\n    # Create a function that will take in the pre-formed matrix, replace its\n    # data in-place with the new data, and return the correct dh_dshape on each\n    # iteration of the minimizer\n    calc_dh_d_eta = partial(_asym_transform_deriv_shape,\n                            ref_position=estimator.shape_ref_pos,\n                            dh_dc_array=pre_dh_dc,\n                            fill_dc_d_eta=easy_calc_dc_d_eta,\n                            output_array=pre_dh_d_eta)\n    return calc_dh_d_eta", "response": "Create a function that can be used in various gradient and hessian functions to calculate the derivative of the transformation with respect to the shape parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _scobit_utility_transform(systematic_utilities,\n                              alt_IDs,\n                              rows_to_alts,\n                              shape_params,\n                              intercept_params,\n                              intercept_ref_pos=None,\n                              *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    systematic_utilities : 1D ndarray.\n        All elements should be ints, floats, or longs. Should contain the\n        systematic utilities of each observation per available alternative.\n        Note that this vector is formed by the dot product of the design matrix\n        with the vector of utility coefficients.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_alts : 2D scipy sparse matrix.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset. All\n        elements should be zeros or ones.\n    shape_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n    intercept_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. If J is the\n        total number of possible alternatives for the dataset being modeled,\n        there should be J-1 elements in the array.\n    intercept_ref_pos : int, or None, optional.\n        Specifies the index of the alternative, in the ordered array of unique\n        alternatives, that is not having its intercept parameter estimated (in\n        order to ensure identifiability). Should only be None if\n        `intercept_params` is None.\n\n    Returns\n    -------\n    transformations : 2D ndarray.\n        Should have shape `(systematic_utilities.shape[0], 1)`. The returned\n        array contains the transformed utility values for this model. All\n        elements should be ints, floats, or longs.\n    \"\"\"\n    # Figure out what indices are to be filled in\n    if intercept_ref_pos is not None and intercept_params is not None:\n        needed_idxs = range(intercept_params.shape[0] + 1)\n        needed_idxs.remove(intercept_ref_pos)\n\n        if len(intercept_params.shape) > 1 and intercept_params.shape[1] > 1:\n            # Get an array of zeros with shape\n            # (num_possible_alternatives, num_parameter_samples)\n            all_intercepts = np.zeros((rows_to_alts.shape[1],\n                                       intercept_params.shape[1]))\n            # For alternatives having their intercept estimated, replace the\n            # zeros with the current value of the estimated intercepts\n            all_intercepts[needed_idxs, :] = intercept_params\n        else:\n            # Get an array of zeros with shape (num_possible_alternatives,)\n            all_intercepts = np.zeros(rows_to_alts.shape[1])\n            # For alternatives having their intercept estimated, replace the\n            # zeros with the current value of the estimated intercepts\n            all_intercepts[needed_idxs] = intercept_params\n    else:\n        # Create a full set of intercept parameters including the intercept\n        # constrained to zero\n        all_intercepts = np.zeros(rows_to_alts.shape[1])\n\n    # Figure out what intercept values correspond to each row of the\n    # systematic utilities\n    long_intercepts = rows_to_alts.dot(all_intercepts)\n\n    # Convert the shape parameters back into their 'natural parametrization'\n    natural_shapes = np.exp(shape_params)\n    natural_shapes[np.isposinf(natural_shapes)] = max_comp_value\n    # Figure out what shape values correspond to each row of the\n    # systematic utilities\n    long_natural_shapes = rows_to_alts.dot(natural_shapes)\n\n    # Calculate the data dependent part of the transformation\n    # Also, along the way, guard against numeric underflow or overflow\n    exp_neg_v = np.exp(-1 * systematic_utilities)\n    exp_neg_v[np.isposinf(exp_neg_v)] = max_comp_value\n\n    powered_term = np.power(1 + exp_neg_v, long_natural_shapes)\n    powered_term[np.isposinf(powered_term)] = max_comp_value\n\n    term_2 = np.log(powered_term - 1)\n    # Guard against overvlow\n    too_big_idx = np.isposinf(powered_term)\n    term_2[too_big_idx] = (-1 * long_natural_shapes[too_big_idx] *\n                           systematic_utilities[too_big_idx])\n\n    transformations = long_intercepts - term_2\n    # Guard against overflow\n    transformations[np.isposinf(transformations)] = max_comp_value\n    transformations[np.isneginf(transformations)] = -1 * max_comp_value\n\n    # Be sure to return a 2D array since other functions will be expecting that\n    if len(transformations.shape) == 1:\n        transformations = transformations[:, np.newaxis]\n\n    return transformations", "response": "This function transforms the systematic utilities matrix into a single array of transformations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit_mle(self,\n                init_vals,\n                init_shapes=None,\n                init_intercepts=None,\n                init_coefs=None,\n                print_res=True,\n                method=\"BFGS\",\n                loss_tol=1e-06,\n                gradient_tol=1e-06,\n                maxiter=1000,\n                ridge=None,\n                constrained_pos=None,\n                just_point=False,\n                **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        init_vals : 1D ndarray.\n            The initial values to start the optimization process with. There\n            should be one value for each index coefficient and shape\n            parameter being estimated. Shape parameters should come before\n            intercept parameters, which should come before index coefficients.\n            One can also pass None, and instead pass `init_shapes`, optionally\n            `init_intercepts` if `\"intercept\"` is not in the utility\n            specification, and `init_coefs`.\n        init_shapes : 1D ndarray or None, optional.\n            The initial values of the shape parameters. All elements should be\n            ints, floats, or longs. There should be one parameter per possible\n            alternative id in the dataset. This keyword argument will be\n            ignored if `init_vals` is not None. Default == None.\n        init_intercepts : 1D ndarray or None, optional.\n            The initial values of the intercept parameters. There should be one\n            parameter per possible alternative id in the dataset, minus one.\n            The passed values for this argument will be ignored if `init_vals`\n            is not None. This keyword argument should only be used if\n            `\"intercept\"` is not in the utility specification. Default == None.\n        init_coefs : 1D ndarray or None, optional.\n            The initial values of the index coefficients. There should be one\n            coefficient per index variable. The passed values for this argument\n            will be ignored if `init_vals` is not None. Default == None.\n        print_res : bool, optional.\n            Determines whether the timing and initial and final log likelihood\n            results will be printed as they they are determined.\n            Default `== True`.\n        method : str, optional.\n            Should be a valid string for scipy.optimize.minimize. Determines\n            the optimization algorithm that is used for this problem.\n            Default `== 'bfgs'`.\n        loss_tol : float, optional.\n            Determines the tolerance on the difference in objective function\n            values from one iteration to the next that is needed to determine\n            convergence. Default `== 1e-06`.\n        gradient_tol : float, optional.\n            Determines the tolerance on the difference in gradient values from\n            one iteration to the next which is needed to determine convergence.\n            Default `== 1e-06`.\n        maxiter : int, optional.\n            Determines the maximum number of iterations used by the optimizer.\n            Default `== 1000`.\n        ridge : int, float, long, or None, optional.\n            Determines whether or not ridge regression is performed. If a\n            scalar is passed, then that scalar determines the ridge penalty for\n            the optimization. The scalar should be greater than or equal to\n            zero. Default `== None`.\n        constrained_pos : list or None, optional.\n            Denotes the positions of the array of estimated parameters that are\n            not to change from their initial values. If a list is passed, the\n            elements are to be integers where no such integer is greater than\n            `init_vals.size.` Default == None.\n        just_point : bool, optional.\n            Determines whether (True) or not (False) calculations that are non-\n            critical for obtaining the maximum likelihood point estimate will\n            be performed. If True, this function will return the results\n            dictionary from scipy.optimize. Default == False.\n\n        Returns\n        -------\n        None. Estimation results are saved to the model instance.\n        \"\"\"\n        # Store the optimization method\n        self.optimization_method = method\n\n        # Store the ridge parameter\n        self.ridge_param = ridge\n        if ridge is not None:\n            warnings.warn(_ridge_warning_msg)\n\n        # Construct the mappings from alternatives to observations and from\n        # chosen alternatives to observations\n        mapping_res = self.get_mappings_for_fit()\n        rows_to_alts = mapping_res[\"rows_to_alts\"]\n\n        # Create init_vals from init_coefs, init_intercepts, and init_shapes if\n        # those arguments are passed to the function and init_vals is None.\n        if init_vals is None and all([x is not None for x in [init_shapes,\n                                                              init_coefs]]):\n            ##########\n            # Check the integrity of the parameter kwargs\n            ##########\n            num_alternatives = rows_to_alts.shape[1]\n            try:\n                assert init_shapes.shape[0] == num_alternatives\n            except AssertionError:\n                msg = \"init_shapes is of length {} but should be of length {}\"\n                raise ValueError(msg.format(init_shapes.shape,\n                                            num_alternatives))\n\n            try:\n                assert init_coefs.shape[0] == self.design.shape[1]\n            except AssertionError:\n                msg = \"init_coefs has length {} but should have length {}\"\n                raise ValueError(msg.format(init_coefs.shape,\n                                            self.design.shape[1]))\n\n            try:\n                if init_intercepts is not None:\n                    assert init_intercepts.shape[0] == (num_alternatives - 1)\n            except AssertionError:\n                msg = \"init_intercepts has length {} but should have length {}\"\n                raise ValueError(msg.format(init_intercepts.shape,\n                                            num_alternatives - 1))\n\n            # The code block below will limit users to only having 'inside'\n            # OR 'outside' intercept parameters but not both.\n            # try:\n            #     condition_1 = \"intercept\" not in self.specification\n            #     condition_2 = init_intercepts is None\n            #     assert condition_1 or condition_2\n            # except AssertionError as e:\n            #     msg = \"init_intercepts should only be used if 'intercept' is\"\n            #     msg_2 = \" not in one's index specification.\"\n            #     msg_3 = \"Either make init_intercepts = None or remove \"\n            #     msg_4 = \"'intercept' from the specification.\"\n            #     print(msg + msg_2 )\n            #     print(msg_3 + msg_4)\n            #     raise e\n\n            if init_intercepts is not None:\n                init_vals = np.concatenate((init_shapes,\n                                            init_intercepts,\n                                            init_coefs), axis=0)\n            else:\n                init_vals = np.concatenate((init_shapes,\n                                            init_coefs), axis=0)\n        elif init_vals is None:\n            msg = \"If init_vals is None, then users must pass both init_coefs \"\n            msg_2 = \"and init_shapes.\"\n            raise ValueError(msg + msg_2)\n\n        # Create the estimation object\n        zero_vector = np.zeros(init_vals.shape)\n        scobit_estimator = ScobitEstimator(self,\n                                           mapping_res,\n                                           ridge,\n                                           zero_vector,\n                                           split_param_vec,\n                                           constrained_pos=constrained_pos)\n        # Set the derivative functions for estimation\n        scobit_estimator.set_derivatives()\n\n        # Perform one final check on the length of the initial values\n        scobit_estimator.check_length_of_initial_values(init_vals)\n\n        # Get the estimation results\n        estimation_res = estimate(init_vals,\n                                  scobit_estimator,\n                                  method,\n                                  loss_tol,\n                                  gradient_tol,\n                                  maxiter,\n                                  print_res,\n                                  just_point=just_point)\n\n        if not just_point:\n            # Store the estimation results\n            self.store_fit_results(estimation_res)\n\n            return None\n        else:\n            return estimation_res", "response": "Fits the MLE model to the specified set of possible alternative ids."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef identify_degenerate_nests(nest_spec):\n    degenerate_positions = []\n    for pos, key in enumerate(nest_spec):\n        if len(nest_spec[key]) == 1:\n            degenerate_positions.append(pos)\n    return degenerate_positions", "response": "Identify the nests within nest_spec that are degenerate i. e. those nests with only a single alternative within the nest."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that the initial values of the object contains the correct length.", "response": "def check_length_of_initial_values(self, init_values):\n        \"\"\"\n        Ensures that the initial values are of the correct length.\n        \"\"\"\n        # Figure out how many shape parameters we should have and how many\n        # index coefficients we should have\n        num_nests = self.rows_to_nests.shape[1]\n        num_index_coefs = self.design.shape[1]\n\n        assumed_param_dimensions = num_index_coefs + num_nests\n        if init_values.shape[0] != assumed_param_dimensions:\n            msg = \"The initial values are of the wrong dimension\"\n            msg_1 = \"It should be of dimension {}\"\n            msg_2 = \"But instead it has dimension {}\"\n            raise ValueError(msg +\n                             msg_1.format(assumed_param_dimensions) +\n                             msg_2.format(init_values.shape[0]))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convenience_split_params(self, params, return_all_types=False):\n        return split_param_vec(params,\n                               self.rows_to_nests,\n                               return_all_types=return_all_types)", "response": "This function splits the parameter vector into nest parameters and index parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convenience_calc_probs(self, params):\n        orig_nest_coefs, betas = self.convenience_split_params(params)\n        natural_nest_coefs = nc.naturalize_nest_coefs(orig_nest_coefs)\n\n        args = [natural_nest_coefs,\n                betas,\n                self.design,\n                self.rows_to_obs,\n                self.rows_to_nests]\n        kwargs = {\"chosen_row_to_obs\": self.chosen_row_to_obs,\n                  \"return_type\": \"long_and_chosen_probs\"}\n\n        probability_results = general_calc_probabilities(*args, **kwargs)\n\n        return probability_results", "response": "Calculates the probabilities of the chosen alternative and the long\n        format probabilities for this model and dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convenience_calc_log_likelihood(self, params):\n        orig_nest_coefs, betas = self.convenience_split_params(params)\n        natural_nest_coefs = nc.naturalize_nest_coefs(orig_nest_coefs)\n\n        args = [natural_nest_coefs,\n                betas,\n                self.design,\n                self.rows_to_obs,\n                self.rows_to_nests,\n                self.choice_vector]\n        kwargs = {\"ridge\": self.ridge, \"weights\": self.weights}\n\n        log_likelihood = general_log_likelihood(*args, **kwargs)\n\n        return log_likelihood", "response": "Calculates the log - likelihood for this model and dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convenience_calc_gradient(self, params):\n        orig_nest_coefs, betas = self.convenience_split_params(params)\n\n        args = [orig_nest_coefs,\n                betas,\n                self.design,\n                self.choice_vector,\n                self.rows_to_obs,\n                self.rows_to_nests]\n\n        return general_gradient(*args, ridge=self.ridge, weights=self.weights)", "response": "Calculates the gradient of the log - likelihood for this model / dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convenience_calc_hessian(self, params):\n        orig_nest_coefs, betas = self.convenience_split_params(params)\n\n        args = [orig_nest_coefs,\n                betas,\n                self.design,\n                self.choice_vector,\n                self.rows_to_obs,\n                self.rows_to_nests]\n\n        approx_hess =\\\n            bhhh_approx(*args, ridge=self.ridge, weights=self.weights)\n\n        # Account for the contrained position when presenting the results of\n        # the approximate hessian.\n        if self.constrained_pos is not None:\n            for idx_val in self.constrained_pos:\n                approx_hess[idx_val, :] = 0\n                approx_hess[:, idx_val] = 0\n                approx_hess[idx_val, idx_val] = -1\n\n        return approx_hess", "response": "Calculates the hessian of the log - likelihood for this model / dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the BHHH approximation of the Fisher Information Matrix for this model / dataset. Note that this function name is INCORRECT with regard to the actual actions performed. The Nested Logit model uses a placeholder for the BHHH approximation of the Fisher Information Matrix because the BHHH approximation is already being used to approximate the hessian. This placeholder allows calculation of a value for the 'robust' standard errors, even though such a value is not useful since it is not correct...", "response": "def convenience_calc_fisher_approx(self, params):\n        \"\"\"\n        Calculates the BHHH approximation of the Fisher Information Matrix for\n        this model / dataset. Note that this function name is INCORRECT with\n        regard to the actual actions performed. The Nested Logit model uses a\n        placeholder for the BHHH approximation of the Fisher Information Matrix\n        because the BHHH approximation is already being used to approximate the\n        hessian.\n\n        This placeholder allows calculation of a value for the 'robust'\n        standard errors, even though such a value is not useful since it is not\n        correct...\n        \"\"\"\n        placeholder_bhhh = np.diag(-1 * np.ones(params.shape[0]))\n\n        return placeholder_bhhh"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit_mle(self,\n                init_vals,\n                constrained_pos=None,\n                print_res=True,\n                method=\"BFGS\",\n                loss_tol=1e-06,\n                gradient_tol=1e-06,\n                maxiter=1000,\n                ridge=None,\n                just_point=False,\n                **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        init_vals : 1D ndarray.\n            Should containn the initial values to start the optimization\n            process with. There should be one value for each nest parameter\n            and utility coefficient. Nest parameters not being estimated\n            should still be included. Handle these parameters using the\n            `constrained_pos` kwarg.\n        constrained_pos : list, or None, optional.\n            Denotes the positions of the array of estimated parameters that are\n            not to change from their initial values. If a list is passed, the\n            elements are to be integers where no such integer is greater than\n            `init_values.size`.\n        print_res : bool, optional.\n            Determines whether the timing and initial and final log likelihood\n            results will be printed as they they are determined.\n        method : str, optional.\n            Should be a valid string which can be passed to\n            `scipy.optimize.minimize`. Determines the optimization algorithm\n            which is used for this problem.\n        loss_tol : float, optional.\n            Determines the tolerance on the difference in objective function\n            values from one iteration to the next which is needed to determine\n            convergence. Default `== 1e-06`.\n        gradient_tol : float, optional.\n            Determines the tolerance on the difference in gradient values from\n            one iteration to the next which is needed to determine convergence.\n            Default `== 1e-06`.\n        ridge : int, float, long, or None, optional.\n            Determines whether ridge regression is performed. If a scalar is\n            passed, then that scalar determines the ridge penalty for the\n            optimization. Default `== None`.\n        just_point : bool, optional.\n            Determines whether (True) or not (False) calculations that are non-\n            critical for obtaining the maximum likelihood point estimate will\n            be performed. If True, this function will return the results\n            dictionary from scipy.optimize. Default == False.\n\n        Returns\n        -------\n        None. Estimation results are saved to the model instance.\n        \"\"\"\n        # Check integrity of passed arguments\n        kwargs_to_be_ignored = [\"init_shapes\", \"init_intercepts\", \"init_coefs\"]\n        if any([x in kwargs for x in kwargs_to_be_ignored]):\n            msg = \"Nested Logit model does not use the following kwargs:\\n{}\"\n            msg_2 = \"Remove such kwargs and pass a single init_vals argument\"\n            raise ValueError(msg.format(kwargs_to_be_ignored) + msg_2)\n\n        # Store the optimization method\n        self.optimization_method = method\n\n        # Store the ridge parameter\n        self.ridge_param = ridge\n\n        if ridge is not None:\n            warnings.warn(_ridge_warning_msg)\n\n        # Construct the mappings from alternatives to observations and from\n        # chosen alternatives to observations\n        mapping_res = self.get_mappings_for_fit()\n\n        # Determine the degenerate nests whose nesting parameters are to be\n        # constrained to one. Note the following functions assume that the nest\n        # parameters are placed before the index coefficients.\n        fixed_params = identify_degenerate_nests(self.nest_spec)\n\n        # Include the user specified parameters that are to be constrained to\n        # their initial values\n        if constrained_pos is not None:\n            fixed_params.extend(constrained_pos)\n        final_constrained_pos = sorted(list(set(fixed_params)))\n\n        # Create the estimation object\n        zero_vector = np.zeros(init_vals.shape)\n        estimator_args = [self,\n                          mapping_res,\n                          ridge,\n                          zero_vector,\n                          split_param_vec]\n        estimator_kwargs = {\"constrained_pos\": final_constrained_pos}\n        nested_estimator = NestedEstimator(*estimator_args,\n                                           **estimator_kwargs)\n\n        # Perform one final check on the length of the initial values\n        nested_estimator.check_length_of_initial_values(init_vals)\n\n        # Get the estimation results\n        estimation_res = estimate(init_vals,\n                                  nested_estimator,\n                                  method,\n                                  loss_tol,\n                                  gradient_tol,\n                                  maxiter,\n                                  print_res,\n                                  use_hessian=True,\n                                  just_point=just_point)\n\n        if not just_point:\n            # Store the estimation results\n            self.store_fit_results(estimation_res)\n\n            return None\n        else:\n            return estimation_res", "response": "Fits the MLE algorithm for the specified set of nest parameter values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the probabilities of the given systematic modules.", "response": "def calc_probabilities(beta,\n                       design,\n                       alt_IDs,\n                       rows_to_obs,\n                       rows_to_alts,\n                       utility_transform,\n                       intercept_params=None,\n                       shape_params=None,\n                       chosen_row_to_obs=None,\n                       return_long_probs=False):\n    \"\"\"\n    Parameters\n    ----------\n    beta : 1D or 2D ndarray.\n        All elements should by ints, floats, or longs. If 1D, should have 1\n        element for each utility coefficient being estimated (i.e.\n        num_features). If 2D, should have 1 column for each set of coefficients\n        being used to predict the probabilities of each alternative being\n        chosen. There should be one row per index coefficient.\n    design : 2D or 3D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs. If `len(design.shape) == 3`,\n        then beta MUST be 1D.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_alts : 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset.\n    utility_transform : callable.\n        Should accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 1D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated.\n    intercept_params : 1D ndarray, or None, optional.\n        If an array, each element should be an int, float, or long. For\n        identifiability, there should be J- 1 elements where J is the total\n        number of observed alternatives for this dataset. Default == None.\n    shape_params : 1D ndarray, or None, optional.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n        Default == None.\n    chosen_row_to_obs :  2D scipy sparse array, or None, optional.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix indicates, for each observation\n        (on the columns), which rows of the design matrix were the realized\n        outcome. If an array is passed then an array of shape\n        (num_observations,) will be returned and each element will be the\n        probability of the realized outcome of the given observation.\n        Default == None.\n    return_long_probs :  bool, optional.\n        Indicates whether or not the long format probabilites (a 1D numpy array\n        with one element per observation per available alternative) should be\n        returned. Default == False.\n\n    Returns\n    -------\n    numpy array or tuple of two numpy arrays.\n        If `chosen_row_to_obs` is passed AND `return_long_probs is True`, then\n        the tuple `(chosen_probs, long_probs)` is returned. If\n        `return_long_probs is True` and `chosen_row_to_obs is None`, then\n        `long_probs` is returned. If `chosen_row_to_obs` is passed and\n        `return_long_probs is False` then `chosen_probs` is returned.\n\n        `chosen_probs` is a 1D numpy array of shape (num_observations,). Each\n        element is the probability of the corresponding observation being\n        associated with its realized outcome.\n\n        `long_probs` is a 1D numpy array with one element per observation per\n        available alternative for that observation. Each element is the\n        probability of the corresponding observation being associated with that\n        rows corresponding alternative.\n\n        If `beta` is a 2D array, `chosen_probs` and `long_probs` will also be\n        2D arrays, with as many columns as there are sets of parameters being\n        used to calculate probabilities with.\n\n        It is NOT valid to have `chosen_row_to_obs == None` and\n        `return_long_probs == False`.\n    \"\"\"\n    # Check argument validity\n    if (len(beta.shape) >= 2) and (len(design.shape) >= 3):\n        msg_1 = \"Cannot calculate probabilities with both 3D design matrix AND\"\n        msg_2 = \" 2D coefficient array.\"\n        raise ValueError(msg_1 + msg_2)\n    if chosen_row_to_obs is None and return_long_probs is False:\n        msg = \"chosen_row_to_obs is None AND return_long_probs is False\"\n        raise ValueError(msg)\n\n    # Calculate the systematic utility for each alternative for each individual\n    sys_utilities = design.dot(beta)\n\n    # Calculate the probability from the transformed utilities\n    # The transformed utilities will be of shape (num_rows, 1)\n    transformed_utilities = utility_transform(sys_utilities,\n                                              alt_IDs,\n                                              rows_to_alts,\n                                              shape_params,\n                                              intercept_params)\n\n    # The following commands are to guard against numeric under/over-flow\n    too_small_idx = transformed_utilities < min_exponent_val\n    too_large_idx = transformed_utilities > max_exponent_val\n\n    transformed_utilities[too_small_idx] = min_exponent_val\n    transformed_utilities[too_large_idx] = max_exponent_val\n\n    # Exponentiate the transformed utilities\n    long_exponentials = np.exp(transformed_utilities)\n\n    # long_probs will be of shape (num_rows,) Each element will provide the\n    # probability of the observation associated with that row having the\n    # alternative associated with that row as the observation's outcome\n    individual_denominators = np.asarray(rows_to_obs.transpose().dot(\n                                                    long_exponentials))\n    long_denominators = np.asarray(rows_to_obs.dot(individual_denominators))\n    if len(long_exponentials.shape) > 1 and long_exponentials.shape[1] > 1:\n        long_probs = (long_exponentials / long_denominators)\n    else:\n        long_probs = (long_exponentials / long_denominators).ravel()\n\n    # Guard against underflow\n    long_probs[long_probs == 0] = min_comp_value\n\n    if chosen_row_to_obs is None:\n        chosen_probs = None\n    else:\n        # chosen_probs will be of shape (num_observations,)\n        chosen_exponentials = np.asarray(\n                         chosen_row_to_obs.transpose().dot(long_exponentials))\n        if len(long_exponentials.shape) > 1 and long_exponentials.shape[1] > 1:\n            chosen_probs = chosen_exponentials / individual_denominators\n        else:\n            chosen_probs = (chosen_exponentials /\n                            individual_denominators).ravel()\n\n    # Return the long form and chosen probabilities if desired\n    if return_long_probs and chosen_probs is not None:\n        return chosen_probs, long_probs\n    # If working with predictions, return just the long form probabilities\n    elif return_long_probs and chosen_probs is None:\n        return long_probs\n    # If estimating the model and storing fitted probabilities or testing the\n    # model on data for which we know the chosen alternative, just return the\n    # chosen probabilities.\n    elif chosen_probs is not None:\n        return chosen_probs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the log likelihood of a given set of systematic related classes.", "response": "def calc_log_likelihood(beta,\n                        design,\n                        alt_IDs,\n                        rows_to_obs,\n                        rows_to_alts,\n                        choice_vector,\n                        utility_transform,\n                        intercept_params=None,\n                        shape_params=None,\n                        ridge=None,\n                        weights=None):\n    \"\"\"\n    Parameters\n    ----------\n    beta : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated (i.e. num_features).\n    design : 2D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_alts : 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset.\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    utility_transform:  callable.\n        Should accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 1D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated.\n    intercept_params:   1D ndarray, or None, optional.\n        If an array, each element should be an int, float, or long. For\n        identifiability, there should be J- 1 elements where J is the total\n        number of observed alternatives for this dataset. Default == None.\n    shape_params : 1D ndarray, or None, optional.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n        Default == None.\n    ridge : int, float, long, or None, optional.\n        Determines whether or not ridge regression is performed. If an int,\n        float or long is passed, then that scalar determines the ridge penalty\n        for the optimization. Default = None.\n    weights : 1D ndarray or None, optional.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    log_likelihood : float. The log likelihood of the multinomial choice model.\n    \"\"\"\n    # Calculate the probability of each individual choosing each available\n    # alternative for that individual.\n    long_probs = calc_probabilities(beta,\n                                    design,\n                                    alt_IDs,\n                                    rows_to_obs,\n                                    rows_to_alts,\n                                    utility_transform,\n                                    intercept_params=intercept_params,\n                                    shape_params=shape_params,\n                                    return_long_probs=True)\n\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = 1\n\n    # Calculate the log likelihood\n    log_likelihood = choice_vector.dot(weights * np.log(long_probs))\n\n    if ridge is None:\n        return log_likelihood\n    else:\n        param_list = [x for x in [shape_params, intercept_params, beta]\n                      if x is not None]\n        if len(param_list) > 1:\n            params = np.concatenate(param_list, axis=0)\n        else:\n            params = param_list[0]\n        return log_likelihood - ridge * np.square(params).sum()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the gradient of a systematic language entry.", "response": "def calc_gradient(beta,\n                  design,\n                  alt_IDs,\n                  rows_to_obs,\n                  rows_to_alts,\n                  choice_vector,\n                  utility_transform,\n                  transform_first_deriv_c,\n                  transform_first_deriv_v,\n                  transform_deriv_alpha,\n                  intercept_params,\n                  shape_params,\n                  ridge,\n                  weights):\n    \"\"\"\n    Parameters\n    ----------\n    beta : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated (i.e. num_features).\n    design : 2D ndarray.\n        Tjere should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D scipy sparse array.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_alts : 2D scipy sparse array\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset.\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    utility_transform : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 1D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated.\n    transform_first_deriv_c : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, the `rows_to_alts` array, (shape parameters if there\n        are any) and miscellaneous args and kwargs. Should return a 2D matrix\n        or sparse array whose elements contain the derivative of the tranformed\n        utility vector with respect to the vector of shape parameters. The\n        dimensions of the returned vector should be\n        `(design.shape[0], num_alternatives)`. If there are no shape parameters\n        then the callable should return None.\n    transform_first_deriv_v : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, (shape parameters if there are any) and miscellaneous\n        args and kwargs. Should return a 2D array whose elements contain the\n        derivative of the tranformed utility vector with respect to the vector\n        of systematic utilities. The dimensions of the returned vector should\n        be `(design.shape[0], design.shape[0])`.\n    transform_deriv_alpha : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, the `rows_to_alts` array, (intercept parameters if\n        there are any) and miscellaneous args and kwargs. Should return a 2D\n        array whose elements contain the derivative of the tranformed utility\n        vector with respect to the vector of shape parameters. The dimensions\n        of the returned vector should be\n        `(design.shape[0], num_alternatives - 1)`. If there are no intercept\n        parameters, the callable should return None.\n    intercept_params : 1D numpy array or None.\n        If an array, each element should be an int, float, or long. For\n        identifiability, there should be J- 1 elements where J is the total\n        number of observed alternatives for this dataset. Default == None.\n    shape_params : 1D ndarray or None.\n       If an array, each element should be an int, float, or long. There should\n       be one value per shape parameter of the model being used.\n       Default == None.\n    ridge : int, float, long, or None.\n        Determines whether or not ridge regression is performed. If an int,\n        float or long is passed, then that scalar determines the ridge penalty\n        for the optimization. Default = None.\n    weights : 1D ndarray or None.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    gradient : 1D ndarray.\n       It's shape is (beta.shape[0], ). It is the second derivative of the log-\n       likelihood with respect to beta.\n    \"\"\"\n    # Calculate the systematic utility for each alternative for each individual\n    sys_utilities = design.dot(beta)\n\n    # Calculate the probability of each individual choosing each available\n    # alternative for that individual.\n    long_probs = calc_probabilities(beta,\n                                    design,\n                                    alt_IDs,\n                                    rows_to_obs,\n                                    rows_to_alts,\n                                    utility_transform,\n                                    intercept_params=intercept_params,\n                                    shape_params=shape_params,\n                                    return_long_probs=True)\n\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = 1\n\n    ##########\n    # Get the required matrices\n    ##########\n    # Differentiate the transformed utilities with respect to the shape params\n    # Note that dh_dc should be a sparse array\n    dh_dc = transform_first_deriv_c(sys_utilities, alt_IDs,\n                                    rows_to_alts, shape_params)\n    # Differentiate the transformed utilities by the intercept params\n    # Note that dh_d_alpha should be a sparse array\n    dh_d_alpha = transform_deriv_alpha(sys_utilities, alt_IDs,\n                                       rows_to_alts, intercept_params)\n    # Differentiate the transformed utilities with respect to the systematic\n    # utilities. Note that dh_dv should be a sparse matrix\n    dh_dv = transform_first_deriv_v(sys_utilities, alt_IDs,\n                                    rows_to_alts, shape_params)\n    # Differentiate the transformed utilities with respect to the utility\n    # coefficients. Note that dh_db should be a dense **matrix**, not a dense\n    # 2D array. This is because the dot product of a 2D scipy sparse array and\n    # a 2D dense numpy array yields a 2D dense numpy matrix\n    dh_db = dh_dv.dot(design)\n    # Differentiate the log likelihood w/ respect to the transformed utilities\n    # Note that d_ll_dh will be a dense 2D numpy array.\n    d_ll_dh = np.multiply(weights, choice_vector - long_probs)[np.newaxis, :]\n\n    # Calculate the gradient of the log-likelihood with respect to the betas\n    d_ll_d_beta = d_ll_dh.dot(dh_db)\n\n    ##########\n    # Form and return the gradient\n    ##########\n    if shape_params is not None and intercept_params is not None:\n        # Note that we use d_ll_dh * dh_dc and d_ll_dh * dh_d_alpha because\n        # that is how one computes the dot product between a dense 2D numpy\n        # array and a 2D sparse matrix. This is due to numpy ndarrays and\n        # scipy sparse matrices not playing nicely together. However, numpy\n        # ndarrays and numpy matrices can be dot producted together,\n        # hence d_ll_dh.dot(dh_db).\n\n        # Note that the 'np.asarray' is because dll_dh * dh_dc will be a row\n        # matrix, but we want a 1D numpy array.\n        gradient = np.concatenate((np.asarray(d_ll_dh * hstack((dh_dc,\n                                                                dh_d_alpha),\n                                                               format='csr')),\n                                   d_ll_d_beta), axis=1).ravel()\n        params = np.concatenate((shape_params, intercept_params, beta),\n                                axis=0)\n\n    elif shape_params is not None and intercept_params is None:\n        # Note that we use d_ll_dh * dh_dc because that is how one computes\n        # the dot product between a dense 2D numpy array and a 2D sparse matrix\n        # This is due to numpy ndarrays and scipy sparse matrices not playing\n        # nicely together. However, numpy ndarrays and numpy matrices can be\n        # dot producted together, hence d_ll_dh.dot(dh_db).\n\n        # Note that the 'np.asarray' is because dll_dh * dh_dc will be a row\n        # matrix, but we want a 1D numpy array.\n        gradient = np.concatenate((np.asarray(d_ll_dh * dh_dc), d_ll_d_beta),\n                                  axis=1).ravel()\n        params = np.concatenate((shape_params, beta), axis=0)\n\n    elif shape_params is None and intercept_params is not None:\n        # Note that we use d_ll_dh * dh_d_alpha because that's how one computes\n        # the dot product between a dense 2D numpy array and a 2D sparse matrix\n        # This is due to numpy ndarrays and scipy sparse matrices not playing\n        # nicely together. However, numpy ndarrays and numpy matrices can be\n        # dot producted together, hence d_ll_dh.dot(dh_db).\n\n        # Note 'np.asarray' is used because dll_dh * dh_d_alpha will be a row\n        # matrix, but we want a 1D numpy array.\n        gradient = np.concatenate((np.asarray(d_ll_dh * dh_d_alpha),\n                                   d_ll_d_beta), axis=1).ravel()\n        params = np.concatenate((intercept_params, beta), axis=0)\n\n    else:\n        gradient = d_ll_d_beta.ravel()\n        params = beta\n\n    if ridge is not None:\n        gradient -= 2 * ridge * params\n\n    return gradient"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_matrix_block_indices(row_to_obs):\n    # Initialize the list of index arrays to be returned\n    output_indices = []\n    # Determine the number of observations in the dataset\n    num_obs = row_to_obs.shape[1]\n    # Get the indices of the non-zero elements and their values\n    row_indices, col_indices, values = scipy.sparse.find(row_to_obs)\n    # Iterate over each observation, i.e. each column in row_to_obs, and\n    # determine which rows belong to that observation (i.e. the rows with ones\n    # in them).\n    for col in xrange(num_obs):\n        # Store the array of row indices belonging to the current observation\n        output_indices.append(row_indices[np.where(col_indices == col)])\n\n    return output_indices", "response": "Create the matrix of indices belonging to the current observation and the next available alternative."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates a robust outer product of two vectors.", "response": "def robust_outer_product(vec_1, vec_2):\n    \"\"\"\n    Calculates a 'robust' outer product of two vectors that may or may not\n    contain very small values.\n\n    Parameters\n    ----------\n    vec_1 : 1D ndarray\n    vec_2 : 1D ndarray\n\n    Returns\n    -------\n    outer_prod : 2D ndarray. The outer product of vec_1 and vec_2\n    \"\"\"\n    mantissa_1, exponents_1 = np.frexp(vec_1)\n    mantissa_2, exponents_2 = np.frexp(vec_2)\n    new_mantissas = mantissa_1[None, :] * mantissa_2[:, None]\n    new_exponents = exponents_1[None, :] + exponents_2[:, None]\n    return new_mantissas * np.exp2(new_exponents)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the matrix blocks that are used to generate the tree structure for the given long_probs and matrix_block_indices.", "response": "def create_matrix_blocks(long_probs, matrix_block_indices):\n    \"\"\"\n    Parameters\n    ----------\n    long_probs : 1D ndarray.\n        There should be one row per observation per available alternative. The\n        elements of the array will indicate the probability of the alternative\n        being the outcome associated with the corresponding observation.\n    matrix_block_indices : list of arrays.\n        There will be one array per observation. The arrays will note which\n        rows correspond to which observations.\n\n    Returns\n    -------\n    output_matrices : list of matrices.\n        Each matrix will contain the derivative of P_i with respect to H_i, and\n        there will be one matrix for each observations i. P_i is the array of\n        probabilities of each observation being associated with its available\n        alternatives. H_i is the array of transformed index values for each\n        alternative that is available to observation i.\n    \"\"\"\n    # Initialize the list of matrices that is to be returned.\n    output_matrices = []\n    # Iterate over each observation, i.e. over each list of rows that\n    # corresponds to each observation.\n    for indices in matrix_block_indices:\n        # Isolate P_i, the vector of probabilities of each alternative that\n        # is associated with the current observation\n        current_probs = long_probs[indices]\n        # Get the outer product of the current probabilities\n        # probability_outer_product = np.outer(current_probs, current_probs)\n        probability_outer_product = robust_outer_product(current_probs,\n                                                         current_probs)\n\n        # Create the desired dP_i/dh_i matrix\n        dP_i_dh_i = np.diag(current_probs) - probability_outer_product\n        # Ensure that the diagonal is positive and non-zero, since it must be.\n        diag_idx = np.diag_indices_from(dP_i_dh_i)\n        diag_values = dP_i_dh_i[diag_idx].copy()\n        diag_values[diag_values == 0] = min_comp_value\n        dP_i_dh_i[diag_idx] = diag_values\n        # Guard against underflow on the off-diagonals\n        underflow_idxs = np.where(dP_i_dh_i == 0)\n        for i in xrange(underflow_idxs[0].size):\n            row_idx, col_idx = underflow_idxs[0][i], underflow_idxs[1][i]\n            if row_idx != col_idx:\n                # Since this type of underflow essentially comes from\n                # multiplying two very small numbers, just set the overall\n                # result to a small number\n                dP_i_dh_i[row_idx,\n                          col_idx] = -1 * min_comp_value\n        # Store the desired dP_i/dh_i matrix\n        output_matrices.append(dP_i_dh_i)\n\n    return output_matrices"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_hessian(beta,\n                 design,\n                 alt_IDs,\n                 rows_to_obs,\n                 rows_to_alts,\n                 utility_transform,\n                 transform_first_deriv_c,\n                 transform_first_deriv_v,\n                 transform_deriv_alpha,\n                 block_matrix_idxs,\n                 intercept_params,\n                 shape_params,\n                 ridge,\n                 weights):\n    \"\"\"\n    Parameters\n    ----------\n    beta : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated (i.e. num_features).\n    design : 2D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_alts: 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset.\n    utility_transform : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a 1D\n        array whose elements contain the appropriately transformed systematic\n        utility values, based on the current model being evaluated.\n    transform_first_deriv_c : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, the `rows_to_alts` array, (shape parameters if there\n        are any) and miscellaneous args and kwargs. Should return a 2D array\n        whose elements contain the derivative of the tranformed utilities with\n        respect to the vector of shape parameters. The dimensions of the\n        returned vector should be `(design.shape[0], num_alternatives)`.\n    transform_first_deriv_v : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, (shape parameters if there are any) and miscellaneous\n        args and kwargs. Should return a 2D array whose elements contain the\n        derivative of the tranformed utility vector with respect to the vector\n        of systematic utilities. The dimensions of the returned vector should\n        be `(design.shape[0], design.shape[0])`.\n    transform_deriv_alpha : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, the rows_to_alts array, (intercept parameters if\n        there are any) and miscellaneous args and kwargs. Should return a 2D\n        array whose elements contain the derivative of the tranformed utilities\n        with respect to the vector of shape parameters. The dimensions of the\n        returned vector should be `(design.shape[0], num_alternatives - 1)`. If\n        `intercept_params == None`, the callable should return None.\n    block_matrix_idxs : list of arrays.\n        There will be one array per observation. The arrays will note which\n        rows correspond to which observations.\n    intercept_params : 1D ndarray.\n        Each element should be an int, float, or long. For identifiability,\n        there should be J- 1 elements where J is the total number of observed\n        alternatives in the dataset.\n    shape_params: None or 1D ndarray.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n        Default == None.\n    ridge : int, float, long, or None.\n        Determines whether or not ridge regression is performed. If an int,\n        float or long is passed, then that scalar determines the ridge penalty\n        for the optimization. Default = None.\n    weights : 1D ndarray or None.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    hess : 2D ndarray.\n        It's shape is `(beta.shape[0], beta.shape[0])`. It is the second\n        derivative of the log likelihood with respect to beta.\n    \"\"\"\n    # Calculate the systematic utility for each alternative for each individual\n    sys_utilities = design.dot(beta)\n\n    # Calculate the probability of each individual choosing each available\n    # alternative for that individual.\n    long_probs = calc_probabilities(beta,\n                                    design,\n                                    alt_IDs,\n                                    rows_to_obs,\n                                    rows_to_alts,\n                                    utility_transform,\n                                    intercept_params=intercept_params,\n                                    shape_params=shape_params,\n                                    return_long_probs=True)\n\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = np.ones(design.shape[0])\n\n    ##########\n    # Get the required matrices\n    ##########\n    # Differentiate the transformed utilities with respect to the shape params\n    # Note that dh_dc will be a 2D scipy sparse matrix\n    dh_dc = transform_first_deriv_c(sys_utilities, alt_IDs,\n                                    rows_to_alts, shape_params)\n    # Differentiate the transformed utilities with respect to the systematic\n    # utilities. Note that dh_dv will be a 2D scipy sparse matrix.\n    dh_dv = transform_first_deriv_v(sys_utilities, alt_IDs,\n                                    rows_to_alts, shape_params)\n    # Differentiate the transformed utilities by the intercept params\n    # Note that dh_d_alpha should be a sparse array\n    dh_d_alpha = transform_deriv_alpha(sys_utilities, alt_IDs,\n                                       rows_to_alts, intercept_params)\n    # Differentiate the transformed utilities with respect to the utility\n    # coefficients. Note that dh_db will be a 2D dense numpy matrix\n    dh_db = dh_dv.dot(design)\n\n    # Differentiate the probabilities with respect to the transformed utilities\n    # Note that dp_dh will be a 2D dense numpy array\n    block_matrices = create_matrix_blocks(long_probs, block_matrix_idxs)\n    dp_dh = block_diag(*block_matrices) * weights[None, :]\n\n    ##########\n    # Calculate the second and mixed partial derivatives within the hessian\n    ##########\n    # Calculate the second derivative of the log-likelihood with repect to the\n    # utility coefficients. Should have shape (design.shape[0],\n    # design.shape[0]). Since dp_dh is a 2D dense numpy array and dh_db is a\n    # 2D dense numpy matrix, using the .dot() syntax should work to compute\n    # the dot product.\n    d2_ll_db2 = -1 * dh_db.T.dot(dp_dh.dot(dh_db))\n\n    ##########\n    # Form and return the hessian\n    ##########\n    if shape_params is not None and intercept_params is not None:\n        # Calculate the second derivative of the log-likelihood with respect\n        # to the shape parameters. Should have shape (shape_params.shape[0],\n        # shape_params.shape[0]). Note that since dp_dh is a 2D dense numpy\n        # array and dh_dc is a sparse matrix or dense numpy matrix, to\n        # compute the dot product we need to use the * operator\n        d2_ll_dc2 = -1 * dh_dc.T.dot(dp_dh * dh_dc)\n\n        # Calculate the second derivative of the log-likelihood with respect\n        # to the intercept parameters. Should have shape (J - 1, J - 1) where\n        # J is the total number of observed alternatives for this dataset.\n        # Note that since dp_dh is a 2D dense numpy array and dh_d_alpha is a\n        # sparse matrix or dense numpy matrix, to compute the dot product\n        # we need to use the * operator\n        d2_ll_d_alpha2 = -1 * dh_d_alpha.T.dot(dp_dh * dh_d_alpha)\n\n        # Calculate the mixed second derivative of the log-likelihood with\n        # respect to the intercept and shape parameters. Should have shape\n        # (dh_d_alpha.shape[1], dh_dc.shape[1]). Note that since dp_dh is a 2D\n        # dense numpy array and dh_dc is a sparse matrix or dense numpy\n        # matrix, to compute the dot product we need to use the * operator\n        d2_ll_dc_d_alpha = -1 * dh_d_alpha.T.dot(dp_dh * dh_dc)\n\n        # Calculate the mixed partial derivative of the log-likelihood with\n        # respect to the utility coefficients and then with respect to the\n        # shape parameters. Should have shape (design.shape[0],\n        # shape_params.shape[0]). Note that since dp_dh is a 2D dense numpy\n        # array and dh_dc is a sparse matrix or dense numpy matrix, to\n        # compute the dot product we need to use the * operator\n        d2_ll_dc_db = -1 * dh_db.T.dot(dp_dh * dh_dc)\n\n        # Calculate the mixed partial derivative of the log-likelihood with\n        # respect to the utility coefficients and then with respect to the\n        # intercept parameters. Should have shape (design.shape[0],\n        # intercept_params.shape[0]). Note that since dp_dh is a 2D dense numpy\n        # array and dh_d_alpha is a sparse matrix or dense numpy matrix, to\n        # compute the dot product we need to use the * operator\n        d2_ll_d_alpha_db = -1 * dh_db.T.dot(dp_dh * dh_d_alpha)\n\n        # Form the 3 by 3 partitioned hessian of 2nd derivatives\n        top_row = np.concatenate((d2_ll_dc2,\n                                  d2_ll_dc_d_alpha.T,\n                                  d2_ll_dc_db.T), axis=1)\n        middle_row = np.concatenate((d2_ll_dc_d_alpha,\n                                     d2_ll_d_alpha2,\n                                     d2_ll_d_alpha_db.T), axis=1)\n        last_row = np.concatenate((d2_ll_dc_db,\n                                   d2_ll_d_alpha_db,\n                                   d2_ll_db2), axis=1)\n        hess = np.concatenate((top_row,\n                               middle_row,\n                               last_row), axis=0)\n\n    elif shape_params is not None and intercept_params is None:\n        # Calculate the second derivative of the log-likelihood with respect\n        # to the shape parameters. Should have shape (shape_params.shape[0],\n        # shape_params.shape[0]). Note that since dp_dh is a 2D dense numpy\n        # array and dh_dc is a sparse matrix or dense numpy matrix, to\n        # compute the dot product we need to use the * operator\n        d2_ll_dc2 = -1 * dh_dc.T.dot(dp_dh * dh_dc)\n\n        # Calculate the mixed partial derivative of the log-likelihood with\n        # respect to the utility coefficients and then with respect to the\n        # shape parameters. Should have shape (design.shape[0],\n        # shape_params.shape[0]). Note that since dp_dh is a 2D dense numpy\n        # array and dh_dc is a sparse matrix or dense numpy matrix, to\n        # compute the dot product we need to use the * operator\n        d2_ll_dc_db = -1 * dh_db.T.dot(dp_dh * dh_dc)\n\n        hess = np.concatenate((np.concatenate((d2_ll_dc2,\n                                               d2_ll_dc_db.T), axis=1),\n                               np.concatenate((d2_ll_dc_db,\n                                               d2_ll_db2), axis=1)), axis=0)\n\n    elif shape_params is None and intercept_params is not None:\n        # Calculate the second derivative of the log-likelihood with respect\n        # to the intercept parameters. Should have shape (J - 1, J - 1) where\n        # J is the total number of observed alternatives for this dataset.\n        # Note that since dp_dh is a 2D dense numpy array and dh_d_alpha is a\n        # sparse matrix or dense numpy matrix, to compute the dot product\n        # we need to use the * operator\n        d2_ll_d_alpha2 = -1 * dh_d_alpha.T.dot(dp_dh * dh_d_alpha)\n\n        # Calculate the mixed partial derivative of the log-likelihood with\n        # respect to the utility coefficients and then with respect to the\n        # intercept parameters. Should have shape (design.shape[0],\n        # intercept_params.shape[0]). Note that since dp_dh is a 2D dense numpy\n        # array and dh_d_alpha is a sparse matrix or dense numpy matrix, to\n        # compute the dot product we need to use the * operator\n        d2_ll_d_alpha_db = -1 * dh_db.T.dot(dp_dh * dh_d_alpha)\n\n        hess = np.concatenate((np.concatenate((d2_ll_d_alpha2,\n                                               d2_ll_d_alpha_db.T), axis=1),\n                               np.concatenate((d2_ll_d_alpha_db,\n                                               d2_ll_db2), axis=1)), axis=0)\n    else:\n        hess = d2_ll_db2\n\n    if ridge is not None:\n        hess -= 2 * ridge\n\n    # Make sure we are returning standard numpy arrays\n    if isinstance(hess, np.matrixlib.defmatrix.matrix):\n        hess = np.asarray(hess)\n\n    return hess", "response": "Calculates the Hessian of the systematic utility values for the given language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the Fisher Info matrix for a given set of entries.", "response": "def calc_fisher_info_matrix(beta,\n                            design,\n                            alt_IDs,\n                            rows_to_obs,\n                            rows_to_alts,\n                            choice_vector,\n                            utility_transform,\n                            transform_first_deriv_c,\n                            transform_first_deriv_v,\n                            transform_deriv_alpha,\n                            intercept_params,\n                            shape_params,\n                            ridge,\n                            weights):\n    \"\"\"\n    Parameters\n    ----------\n    beta : 1D ndarray.\n        All elements should by ints, floats, or longs. Should have 1 element\n        for each utility coefficient being estimated (i.e. num_features).\n    design : 2D ndarray.\n        There should be one row per observation per available alternative.\n        There should be one column per utility coefficient being estimated. All\n        elements should be ints, floats, or longs.\n    alt_IDs : 1D ndarray.\n        All elements should be ints. There should be one row per obervation per\n        available alternative for the given observation. Elements denote the\n        alternative corresponding to the given row of the design matrix.\n    rows_to_obs : 2D ndarray.\n        There should be one row per observation per available alternative and\n        one column per observation. This matrix maps the rows of the design\n        matrix to the unique observations (on the columns).\n    rows_to_alts : 2D ndarray\n        There should be one row per observation per available alternative and\n        one column per possible alternative. This matrix maps the rows of the\n        design matrix to the possible alternatives for this dataset.\n    choice_vector : 1D ndarray.\n        All elements should be either ones or zeros. There should be one row\n        per observation per available alternative for the given observation.\n        Elements denote the alternative which is chosen by the given\n        observation with a 1 and a zero otherwise.\n    utility_transform : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, and miscellaneous args and kwargs. Should return a\n        1D array whose elements contain the appropriately transformed\n        systematic utility values, based on the current model being evaluated.\n    transform_first_deriv_c : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n        alternative IDs, the `rows_to_alts` array, (shape parameters if there\n        are any) and miscellaneous args and kwargs. Should return a 2D array\n        whose elements contain the derivative of the tranformed utilities with\n        respect to the vector of shape parameters. The dimensions of the\n        returned vector should be `(design.shape[0], num_alternatives)`.\n    transform_first_deriv_v : callable.\n        Must accept a 1D array of systematic utility values, a 1D array of\n         alternative IDs, (shape parameters if there are any) and miscellaneous\n         args and kwargs. Should return a 2D array whose elements contain the\n         derivative of the utility tranformation vector with respect to the\n         vector of systematic utilities. The dimensions of the returned vector\n         should be `(design.shape[0],  design.shape[0])`.\n    shape_params : None or 1D ndarray.\n        If an array, each element should be an int, float, or long. There\n        should be one value per shape parameter of the model being used.\n        Default == None.\n    ridge : int, float, long, or None.\n        Determines whether or not ridge regression is performed. If an int,\n        float or long is passed, then that scalar determines the ridge penalty\n        for the optimization. Default = None.\n    weights : 1D ndarray or None.\n        Allows for the calculation of weighted log-likelihoods. The weights can\n        represent various things. In stratified samples, the weights may be\n        the proportion of the observations in a given strata for a sample in\n        relation to the proportion of observations in that strata in the\n        population. In latent class models, the weights may be the probability\n        of being a particular class.\n\n    Returns\n    -------\n    fisher_matrix : 2D numpy array.\n        It will be a square matrix, with one row and one column for each shape,\n        intercept, and index coefficient. Contains the BHHH approximation to\n        the Fisher Information matrix of the log likelihood.\n    \"\"\"\n    # Calculate the systematic utility for each alternative for each individual\n    sys_utilities = design.dot(beta)\n\n    # Calculate the probability that the individual associated with a given row\n    # chooses the alternative associated with a given row.\n    long_probs = calc_probabilities(beta,\n                                    design,\n                                    alt_IDs,\n                                    rows_to_obs,\n                                    rows_to_alts,\n                                    utility_transform,\n                                    intercept_params=intercept_params,\n                                    shape_params=shape_params,\n                                    return_long_probs=True)\n\n    # Calculate the weights for the sample\n    if weights is None:\n        weights = np.ones(design.shape[0])\n    weights_per_obs =\\\n        np.max(rows_to_obs.toarray() * weights[:, None], axis=0)\n\n    ##########\n    # Get the required matrices\n    ##########\n    # Differentiate the transformed utilities with respect to the shape params\n    dh_dc = transform_first_deriv_c(sys_utilities, alt_IDs,\n                                    rows_to_alts, shape_params)\n    # Differentiate the transformed utilities with respect to the systematic\n    # utilities\n    dh_dv = transform_first_deriv_v(sys_utilities, alt_IDs,\n                                    rows_to_alts, shape_params)\n    # Differentiate the transformed utilities by the intercept params\n    # Note that dh_d_alpha should be a sparse array\n    dh_d_alpha = transform_deriv_alpha(sys_utilities, alt_IDs,\n                                       rows_to_alts, intercept_params)\n    # Differentiate the transformed utilities with respect to the utility\n    # coefficients. This should be a dense numpy array.\n    dh_db = np.asarray(dh_dv.dot(design))\n    # Differentiate the log likelihood w/ respect to the transformed utilities\n    d_ll_dh = (choice_vector - long_probs)[np.newaxis, :]\n\n    ##########\n    # Create the matrix where each row represents the gradient of a particular\n    # observations log-likelihood with respect to the shape parameters and\n    # beta, depending on whether there are shape parameters being estimated\n    ##########\n    if shape_params is not None and intercept_params is not None:\n        if isinstance(dh_dc, np.matrixlib.defmatrix.matrix):\n            # Note that the '.A' transforms the matrix into a numpy ndarray\n            gradient_vec = d_ll_dh.T * np.concatenate((dh_dc.A,\n                                                       dh_d_alpha.toarray(),\n                                                       dh_db), axis=1)\n        else:\n            gradient_vec = d_ll_dh.T * np.concatenate((dh_dc.toarray(),\n                                                       dh_d_alpha.toarray(),\n                                                       dh_db), axis=1)\n    elif shape_params is not None and intercept_params is None:\n        if isinstance(dh_dc, np.matrixlib.defmatrix.matrix):\n            # Note that the '.A' transforms the matrix into a numpy ndarray\n            gradient_vec = d_ll_dh.T * np.concatenate((dh_dc.A, dh_db), axis=1)\n        else:\n            gradient_vec = d_ll_dh.T * np.concatenate((dh_dc.toarray(),\n                                                       dh_db), axis=1)\n    elif shape_params is None and intercept_params is not None:\n        # Note '.to_array()' is used because dh_d_alpha will be a sparse\n        # matrix, but we want a 2D numpy array.\n        gradient_vec = d_ll_dh.T * np.concatenate((dh_d_alpha.toarray(),\n                                                   dh_db), axis=1)\n    else:\n        gradient_vec = d_ll_dh.T * dh_db\n\n    # Make sure that we calculate the gradient PER OBSERVATION\n    # and then take the outer products of those gradients.\n    # Note that this is different than taking the outer products of the\n    # gradient of the log-likelihood per available alternative per observation\n    gradient_vec = rows_to_obs.T.dot(gradient_vec)\n\n    # Compute and return the outer product of each row of the gradient\n    # with itself. Then sum these individual matrices together. The line below\n    # does the same computation just with less memory and time.\n    fisher_matrix =\\\n        gradient_vec.T.dot(np.multiply(weights_per_obs[:, None], gradient_vec))\n\n    if ridge is not None:\n        # The rational behind adding 2 * ridge is that the fisher information\n        # matrix should approximate the hessian and in the hessian we add\n        # 2 * ridge at the end. I don't know if this is the correct way to\n        # calculate the Fisher Information in ridge regression models.\n        fisher_matrix -= 2 * ridge\n\n    return fisher_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the asymptotic covariance matrix for the given n - tuple of base - classes.", "response": "def calc_asymptotic_covariance(hessian, fisher_info_matrix):\n    \"\"\"\n    Parameters\n    ----------\n    hessian : 2D ndarray.\n        It should have shape `(num_vars, num_vars)`. It is the matrix of second\n        derivatives of the total loss across the dataset, with respect to each\n        pair of coefficients being estimated.\n    fisher_info_matrix : 2D ndarray.\n        It should have a shape of `(num_vars, num_vars)`.  It is the\n        approximation of the negative of the expected hessian formed by taking\n        the outer product of (each observation's gradient of the loss function)\n        with itself, and then summing across all observations.\n\n    Returns\n    -------\n    huber_white_matrix : 2D ndarray.\n        Will have shape `(num_vars, num_vars)`. The entries in the returned\n        matrix are calculated by the following formula:\n        `hess_inverse * fisher_info_matrix * hess_inverse`.\n    \"\"\"\n    # Calculate the inverse of the hessian\n    hess_inv = scipy.linalg.inv(hessian)\n\n    return np.dot(hess_inv, np.dot(fisher_info_matrix, hess_inv))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_percentile_interval(bootstrap_replicates, conf_percentage):\n    # Check validity of arguments\n    check_conf_percentage_validity(conf_percentage)\n    ensure_samples_is_ndim_ndarray(bootstrap_replicates, ndim=2)\n    # Get the alpha * 100% value\n    alpha = get_alpha_from_conf_percentage(conf_percentage)\n    # Get the lower and upper percentiles that demarcate the desired interval.\n    lower_percent = alpha / 2.0\n    upper_percent = 100.0 - lower_percent\n    # Calculate the lower and upper endpoints of the confidence intervals.\n    # Note that the particular choices of interpolation methods are made in\n    # order to produce conservatively wide confidence intervals and ensure that\n    # all returned endpoints in the confidence intervals are actually observed\n    # in the bootstrap distribution. This is in accordance with the spirit of\n    # Efron and Tibshirani (1994).\n    lower_endpoint = np.percentile(bootstrap_replicates,\n                                   lower_percent,\n                                   interpolation='lower',\n                                   axis=0)\n    upper_endpoint = np.percentile(bootstrap_replicates,\n                                   upper_percent,\n                                   interpolation='higher',\n                                   axis=0)\n    # Combine the enpoints into a single ndarray.\n    conf_intervals = combine_conf_endpoints(lower_endpoint, upper_endpoint)\n    return conf_intervals", "response": "Calculates the bootstrap confidence intervals for a given set of bootstrap distributions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the bias correction factor for the Bias Corrected and Accelerated bootstrap confidence intervals.", "response": "def calc_bias_correction_bca(bootstrap_replicates, mle_estimate):\n    \"\"\"\n    Calculate the bias correction for the Bias Corrected and Accelerated (BCa)\n    bootstrap confidence intervals.\n\n    Parameters\n    ----------\n    bootstrap_replicates : 2D ndarray.\n        Each row should correspond to a different bootstrap parameter sample.\n        Each column should correspond to an element of the parameter vector\n        being estimated.\n    mle_estimate : 1D ndarray.\n        The original dataset's maximum likelihood point estimate. Should have\n        one elements for each component of the estimated parameter vector.\n\n    Returns\n    -------\n    bias_correction : 1D ndarray.\n        There will be one element for each element in `mle_estimate`. Elements\n        denote the bias correction factors for each component of the parameter\n        vector.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 14.3, Equation 14.14.\n    \"\"\"\n    numerator = (bootstrap_replicates < mle_estimate[None, :]).sum(axis=0)\n    denominator = float(bootstrap_replicates.shape[0])\n    bias_correction = norm.ppf(numerator / denominator)\n    return bias_correction"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_acceleration_bca(jackknife_replicates):\n    # Get the mean of the bootstrapped statistics.\n    jackknife_mean = jackknife_replicates.mean(axis=0)[None, :]\n    # Calculate the differences between the mean of the bootstrapped statistics\n    differences = jackknife_mean - jackknife_replicates\n    numerator = (differences**3).sum(axis=0)\n    denominator = 6 * ((differences**2).sum(axis=0))**1.5\n    # guard against division by zero. Note that this guard shouldn't distort\n    # the computational results since the numerator should be zero whenever the\n    # denominator is zero.\n    zero_denom = np.where(denominator == 0)\n    denominator[zero_denom] = MIN_COMP_VALUE\n    # Compute the acceleration.\n    acceleration = numerator / denominator\n    return acceleration", "response": "Calculates the acceleration constant for the Bias Corrected and Accelerated jackknife bootstrap confidence intervals."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the lower percentile of the Bias Corrected and Accelerated bootstrap confidence intervals.", "response": "def calc_lower_bca_percentile(alpha_percent, bias_correction, acceleration):\n    \"\"\"\n    Calculate the lower values of the Bias Corrected and Accelerated (BCa)\n    bootstrap confidence intervals.\n\n    Parameters\n    ----------\n    alpha_percent : float in (0.0, 100.0).\n        `100 - confidence_percentage`, where `confidence_percentage` is the\n        confidence level (such as 95%), expressed as a percent.\n    bias_correction : 1D ndarray.\n        There will be one element for each element in `mle_estimate`. Elements\n        denote the bias correction factors for each component of the parameter\n        vector.\n    acceleration : 1D ndarray.\n        There will be one element for each element in `mle_estimate`. Elements\n        denote the acceleration factors for each component of the parameter\n        vector.\n\n    Returns\n    -------\n    lower_percentile : 1D ndarray.\n        There will be one element for each element in `mle_estimate`. Elements\n        denote the smaller values in the confidence interval for each component\n        of the parameter vector.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 14.3, Equation 14.10.\n\n    Notes\n    -----\n    The `alpha` used in this function is different from the `alpha` used in\n    Efron and Tibshirani (1994). The `alpha` used in this function must be\n    converted to a decimal (by dividing by 100) and then divided by 2 (to\n    account for the equal-tailed nature of the confidence interval) in order to\n    be made equivalent to the `alpha` in Efron and Tibshirani (1994).\n    \"\"\"\n    z_lower = norm.ppf(alpha_percent / (100.0 * 2))\n    numerator = bias_correction + z_lower\n    denominator = 1 - acceleration * numerator\n    lower_percentile =\\\n        norm.cdf(bias_correction + numerator / denominator) * 100\n    return lower_percentile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_upper_bca_percentile(alpha_percent, bias_correction, acceleration):\n    z_upper = norm.ppf(1 - alpha_percent / (100.0 * 2))\n    numerator = bias_correction + z_upper\n    denominator = 1 - acceleration * numerator\n    upper_percentile =\\\n        norm.cdf(bias_correction + numerator / denominator) * 100\n    return upper_percentile", "response": "Calculates the upper percentile of the Bias Corrected and Accelerated bootstrap confidence intervals."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the bias - corrected and accelerated bootstrap confidence intervals.", "response": "def calc_bca_interval(bootstrap_replicates,\n                      jackknife_replicates,\n                      mle_params,\n                      conf_percentage):\n    \"\"\"\n    Calculate 'bias-corrected and accelerated' bootstrap confidence intervals.\n\n    Parameters\n    ----------\n    bootstrap_replicates : 2D ndarray.\n        Each row should correspond to a different bootstrap parameter sample.\n        Each column should correspond to an element of the parameter vector\n        being estimated.\n    jackknife_replicates : 2D ndarray.\n        Each row should correspond to a different jackknife parameter sample,\n        formed by deleting a particular observation and then re-estimating the\n        desired model. Each column should correspond to an element of the\n        parameter vector being estimated.\n    mle_params : 1D ndarray.\n        The original dataset's maximum likelihood point estimate. Should have\n        the same number of elements as `samples.shape[1]`.\n    conf_percentage : scalar in the interval (0.0, 100.0).\n        Denotes the confidence-level of the returned confidence interval. For\n        instance, to calculate a 95% confidence interval, pass `95`.\n\n    Returns\n    -------\n    conf_intervals : 2D ndarray.\n        The shape of the returned array will be `(2, samples.shape[1])`. The\n        first row will correspond to the lower value in the confidence\n        interval. The second row will correspond to the upper value in the\n        confidence interval. There will be one column for each element of the\n        parameter vector being estimated.\n\n    References\n    ----------\n    Efron, Bradley, and Robert J. Tibshirani. An Introduction to the Bootstrap.\n        CRC press, 1994. Section 14.3.\n    DiCiccio, Thomas J., and Bradley Efron. \"Bootstrap confidence intervals.\"\n        Statistical science (1996): 189-212.\n    \"\"\"\n    # Check validity of arguments\n    check_conf_percentage_validity(conf_percentage)\n    ensure_samples_is_ndim_ndarray(bootstrap_replicates, ndim=2)\n    ensure_samples_is_ndim_ndarray(jackknife_replicates,\n                                   name='jackknife', ndim=2)\n    # Calculate the alpha * 100% value\n    alpha_percent = get_alpha_from_conf_percentage(conf_percentage)\n    # Estimate the bias correction for the bootstrap samples\n    bias_correction =\\\n        calc_bias_correction_bca(bootstrap_replicates, mle_params)\n    # Estimate the acceleration\n    acceleration = calc_acceleration_bca(jackknife_replicates)\n    # Get the lower and upper percent value for the raw bootstrap samples.\n    lower_percents =\\\n        calc_lower_bca_percentile(alpha_percent, bias_correction, acceleration)\n    upper_percents =\\\n        calc_upper_bca_percentile(alpha_percent, bias_correction, acceleration)\n    # Get the lower and upper endpoints for the desired confidence intervals.\n    lower_endpoints = np.diag(np.percentile(bootstrap_replicates,\n                                            lower_percents,\n                                            interpolation='lower',\n                                            axis=0))\n    upper_endpoints = np.diag(np.percentile(bootstrap_replicates,\n                                            upper_percents,\n                                            interpolation='higher',\n                                            axis=0))\n    # Combine the enpoints into a single ndarray.\n    conf_intervals = combine_conf_endpoints(lower_endpoints, upper_endpoints)\n    return conf_intervals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract the default initial values for the MNL class based on the MNL model that is closest to the desired model.", "response": "def extract_default_init_vals(orig_model_obj, mnl_point_series, num_params):\n    \"\"\"\n    Get the default initial values for the desired model type, based on the\n    point estimate of the MNL model that is 'closest' to the desired model.\n\n    Parameters\n    ----------\n    orig_model_obj : an instance or sublcass of the MNDC class.\n        Should correspond to the actual model that we want to bootstrap.\n    mnl_point_series : pandas Series.\n        Should denote the point estimate from the MNL model that is 'closest'\n        to the desired model.\n    num_params : int.\n        Should denote the number of parameters being estimated (including any\n        parameters that are being constrained during estimation).\n\n    Returns\n    -------\n    init_vals : 1D ndarray of initial values for the MLE of the desired model.\n    \"\"\"\n    # Initialize the initial values\n    init_vals = np.zeros(num_params, dtype=float)\n    # Figure out which values in mnl_point_series are the index coefficients\n    no_outside_intercepts = orig_model_obj.intercept_names is None\n    if no_outside_intercepts:\n        init_index_coefs = mnl_point_series.values\n        init_intercepts = None\n    else:\n        init_index_coefs =\\\n            mnl_point_series.loc[orig_model_obj.ind_var_names].values\n        init_intercepts =\\\n            mnl_point_series.loc[orig_model_obj.intercept_names].values\n\n    # Add any mixing variables to the index coefficients.\n    if orig_model_obj.mixing_vars is not None:\n        num_mixing_vars = len(orig_model_obj.mixing_vars)\n        init_index_coefs = np.concatenate([init_index_coefs,\n                                           np.zeros(num_mixing_vars)],\n                                          axis=0)\n\n    # Account for the special transformation of the index coefficients that is\n    # needed for the asymmetric logit model.\n    if orig_model_obj.model_type == model_type_to_display_name[\"Asym\"]:\n        multiplier = np.log(len(np.unique(orig_model_obj.alt_IDs)))\n        # Cast the initial index coefficients to a float dtype to ensure\n        # successful broadcasting\n        init_index_coefs = init_index_coefs.astype(float)\n        # Adjust the scale of the index coefficients for the asymmetric logit.\n        init_index_coefs /= multiplier\n\n    # Combine the initial interept values with the initial index coefficients\n    if init_intercepts is not None:\n        init_index_coefs =\\\n            np.concatenate([init_intercepts, init_index_coefs], axis=0)\n\n    # Add index coefficients (and mixing variables) to the total initial array\n    num_index = init_index_coefs.shape[0]\n    init_vals[-1 * num_index:] = init_index_coefs\n\n    # Note that the initial values for the transformed nest coefficients and\n    # the shape parameters is zero so we don't have to change anything\n    return init_vals"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the internal abbreviation for the given model object.", "response": "def get_model_abbrev(model_obj):\n    \"\"\"\n    Extract the string used to specify the model type of this model object in\n    `pylogit.create_chohice_model`.\n\n    Parameters\n    ----------\n    model_obj : An MNDC_Model instance.\n\n    Returns\n    -------\n    str. The internal abbreviation used for the particular type of MNDC_Model.\n    \"\"\"\n    # Get the 'display name' for our model.\n    model_type = model_obj.model_type\n    # Find the model abbreviation for this model's display name.\n    for key in model_type_to_display_name:\n        if model_type_to_display_name[key] == model_type:\n            return key\n    # If none of the strings in model_type_to_display_name matches our model\n    # object, then raise an error.\n    msg = \"Model object has an unknown or incorrect model type.\"\n    raise ValueError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of keyword arguments needed to create a new MNDC_Model object.", "response": "def get_model_creation_kwargs(model_obj):\n    \"\"\"\n    Get a dictionary of the keyword arguments needed to create the passed model\n    object using `pylogit.create_choice_model`.\n\n    Parameters\n    ----------\n    model_obj : An MNDC_Model instance.\n\n    Returns\n    -------\n    model_kwargs : dict.\n        Contains the keyword arguments and the required values that are needed\n        to initialize a replica of `model_obj`.\n    \"\"\"\n    # Extract the model abbreviation for this model\n    model_abbrev = get_model_abbrev(model_obj)\n\n    # Create a dictionary to store the keyword arguments needed to Initialize\n    # the new model object.d\n    model_kwargs = {\"model_type\": model_abbrev,\n                    \"names\": model_obj.name_spec,\n                    \"intercept_names\": model_obj.intercept_names,\n                    \"intercept_ref_pos\": model_obj.intercept_ref_position,\n                    \"shape_names\": model_obj.shape_names,\n                    \"shape_ref_pos\": model_obj.shape_ref_position,\n                    \"nest_spec\": model_obj.nest_spec,\n                    \"mixing_vars\": model_obj.mixing_vars,\n                    \"mixing_id_col\": model_obj.mixing_id_col}\n\n    return model_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mnl_point_est(orig_model_obj,\n                      new_df,\n                      boot_id_col,\n                      num_params,\n                      mnl_spec,\n                      mnl_names,\n                      mnl_init_vals,\n                      mnl_fit_kwargs):\n    \"\"\"\n    Calculates the MLE for the desired MNL model.\n\n    Parameters\n    ----------\n    orig_model_obj : An MNDC_Model instance.\n        The object corresponding to the desired model being bootstrapped.\n    new_df : pandas DataFrame.\n        The pandas dataframe containing the data to be used to estimate the\n        MLE of the MNL model for the current bootstrap sample.\n    boot_id_col : str.\n        Denotes the new column that specifies the bootstrap observation ids for\n        choice model estimation.\n    num_params : non-negative int.\n        The number of parameters in the MLE of the `orig_model_obj`.\n    mnl_spec : OrderedDict or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_spec` should be an\n        OrderedDict that contains the specification dictionary used to estimate\n        the MNL model that will provide starting values for the final estimated\n        model. If `orig_model_obj` is a MNL model, then `mnl_spec` may be None.\n    mnl_names : OrderedDict or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_names` should be an\n        OrderedDict that contains the name dictionary used to initialize the\n        MNL model that will provide starting values for the final estimated\n        model. If `orig_model_obj` is a MNL, then `mnl_names` may be None.\n    mnl_init_vals : 1D ndarray or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_init_vals` should be\n        a 1D ndarray. `mnl_init_vals` should denote the initial values used to\n        estimate the MNL model that provides starting values for the final\n        desired model. If `orig_model_obj` is a MNL model, then `mnl_init_vals`\n        may be None.\n    mnl_fit_kwargs : dict or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_fit_kwargs` should be\n        a dict. `mnl_fit_kwargs` should denote the keyword arguments used when\n        calling the `fit_mle` function of the MNL model that will provide\n        starting values to the desired choice model. If `orig_model_obj` is a\n        MNL model, then `mnl_fit_kwargs` may be None.\n\n    Returns\n    -------\n    mnl_point : dict.\n        The dictionary returned by `scipy.optimize` after estimating the\n        desired MNL model.\n    mnl_obj : An MNL model instance.\n        The model object used to estimate the desired MNL model.\n    \"\"\"\n    # Get specification and name dictionaries for the mnl model, for the case\n    # where the model being bootstrapped is an MNL model. In this case, the\n    # the mnl_spec and the mnl_names that are passed to the function are\n    # expected to be None.\n    if orig_model_obj.model_type == model_type_to_display_name[\"MNL\"]:\n        mnl_spec = orig_model_obj.specification\n        mnl_names = orig_model_obj.name_spec\n        if mnl_init_vals is None:\n            mnl_init_vals = np.zeros(num_params)\n        if mnl_fit_kwargs is None:\n            mnl_fit_kwargs = {}\n\n    # Alter the mnl_fit_kwargs to ensure that we only perform point estimation\n    mnl_fit_kwargs[\"just_point\"] = True\n    # Use BFGS by default to estimate the MNL since it works well for the MNL.\n    if \"method\" not in mnl_fit_kwargs:\n        mnl_fit_kwargs[\"method\"] = \"BFGS\"\n\n    # Initialize the mnl model object for the given bootstrap sample.\n    mnl_obj = pl.create_choice_model(data=new_df,\n                                     alt_id_col=orig_model_obj.alt_id_col,\n                                     obs_id_col=boot_id_col,\n                                     choice_col=orig_model_obj.choice_col,\n                                     specification=mnl_spec,\n                                     model_type=\"MNL\",\n                                     names=mnl_names)\n\n    # Get the MNL point estimate for the parameters of this bootstrap sample.\n    mnl_point = mnl_obj.fit_mle(mnl_init_vals, **mnl_fit_kwargs)\n    return mnl_point, mnl_obj", "response": "Calculates the MLE for the current bootstrap sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retrieve_point_est(orig_model_obj,\n                       new_df,\n                       new_id_col,\n                       num_params,\n                       mnl_spec,\n                       mnl_names,\n                       mnl_init_vals,\n                       mnl_fit_kwargs,\n                       extract_init_vals=None,\n                       **fit_kwargs):\n    \"\"\"\n    Calculates the MLE for the desired MNL model.\n\n    Parameters\n    ----------\n    orig_model_obj : An MNDC_Model instance.\n        The object corresponding to the desired model being bootstrapped.\n    new_df : pandas DataFrame.\n        The pandas dataframe containing the data to be used to estimate the\n        MLE of the MNL model for the current bootstrap sample.\n    new_id_col : str.\n        Denotes the new column that specifies the bootstrap observation ids for\n        choice model estimation.\n    num_params : non-negative int.\n        The number of parameters in the MLE of the `orig_model_obj`.\n    mnl_spec : OrderedDict or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_spec` should be an\n        OrderedDict that contains the specification dictionary used to estimate\n        the MNL model that will provide starting values for the final estimated\n        model. If `orig_model_obj` is a MNL model, then `mnl_spec` may be None.\n    mnl_names : OrderedDict or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_names` should be an\n        OrderedDict that contains the name dictionary used to initialize the\n        MNL model that will provide starting values for the final estimated\n        model. If `orig_model_obj` is a MNL, then `mnl_names` may be None.\n    mnl_init_vals : 1D ndarray or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_init_vals` should be\n        a 1D ndarray. `mnl_init_vals` should denote the initial values used to\n        estimate the MNL model that provides starting values for the final\n        desired model. If `orig_model_obj` is a MNL model, then `mnl_init_vals`\n        may be None.\n    mnl_fit_kwargs : dict or None.\n        If `orig_model_obj` is not a MNL model, then `mnl_fit_kwargs` should be\n        a dict. `mnl_fit_kwargs` should denote the keyword arguments used when\n        calling the `fit_mle` function of the MNL model that will provide\n        starting values to the desired choice model. If `orig_model_obj` is a\n        MNL model, then `mnl_fit_kwargs` may be None.\n    extract_init_vals : callable or None, optional.\n        Should accept 3 arguments, in the following order. First, it should\n        accept `orig_model_obj`. Second, it should accept a pandas Series of\n        the estimated parameters from the MNL model. The index of the Series\n        will be the names of the coefficients from `mnl_names`. Thirdly, it\n        should accept an int denoting the number of parameters in the desired\n        choice model. The callable should return a 1D ndarray of starting\n        values for the desired choice model. Default == None.\n    fit_kwargs : dict.\n        Denotes the keyword arguments to be used when estimating the desired\n        choice model using the current bootstrap sample (`new_df`). All such\n        kwargs will be directly passed to the `fit_mle` method of the desired\n        model object.\n\n    Returns\n    -------\n    final_point : dict.\n        The dictionary returned by `scipy.optimize` after estimating the\n        desired choice model.\n    \"\"\"\n    # Get the MNL point estimate for the parameters of this bootstrap sample.\n    mnl_point, mnl_obj = get_mnl_point_est(orig_model_obj,\n                                           new_df,\n                                           new_id_col,\n                                           num_params,\n                                           mnl_spec,\n                                           mnl_names,\n                                           mnl_init_vals,\n                                           mnl_fit_kwargs)\n    mnl_point_series = pd.Series(mnl_point[\"x\"], index=mnl_obj.ind_var_names)\n\n    # Denote the MNL point estimate as our final point estimate if the final\n    # model we're interested in is an MNL.\n    if orig_model_obj.model_type == model_type_to_display_name[\"MNL\"]:\n        final_point = mnl_point\n    else:\n        # Determine the function to be used when extracting the initial values\n        # for the final model from the MNL MLE point estimate.\n        if extract_init_vals is None:\n            extraction_func = extract_default_init_vals\n        else:\n            extraction_func = extract_init_vals\n\n        # Extract the initial values\n        default_init_vals =\\\n            extraction_func(orig_model_obj, mnl_point_series, num_params)\n\n        # Get the keyword arguments needed to initialize the new model object.\n        model_kwargs = get_model_creation_kwargs(orig_model_obj)\n\n        # Create a new model object\n        new_obj =\\\n            pl.create_choice_model(data=new_df,\n                                   alt_id_col=orig_model_obj.alt_id_col,\n                                   obs_id_col=new_id_col,\n                                   choice_col=orig_model_obj.choice_col,\n                                   specification=orig_model_obj.specification,\n                                   **model_kwargs)\n\n        # Be sure to add 'just_point' to perform pure point estimation.\n        if 'just_point' not in fit_kwargs:\n            fit_kwargs['just_point'] = True\n\n        # Fit the model with new data, and return the point estimate dict.\n        final_point = new_obj.fit_mle(default_init_vals, **fit_kwargs)\n\n    return final_point", "response": "This function calculates the MLE for the current bootstrap sample."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_valid_model_type(specified_type, model_type_list):\n    if specified_type not in model_type_list:\n        msg_1 = \"The specified model_type was not valid.\"\n        msg_2 = \"Valid model-types are {}\".format(model_type_list)\n        msg_3 = \"The passed model-type was: {}\".format(specified_type)\n        total_msg = \"\\n\".join([msg_1, msg_2, msg_3])\n        raise ValueError(total_msg)\n    return None", "response": "Checks to make sure that specified_type is in model_type_list and raises a helpful error if this is not the case."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a choice model for the given data.", "response": "def create_choice_model(data,\n                        alt_id_col,\n                        obs_id_col,\n                        choice_col,\n                        specification,\n                        model_type,\n                        intercept_ref_pos=None,\n                        shape_ref_pos=None,\n                        names=None,\n                        intercept_names=None,\n                        shape_names=None,\n                        nest_spec=None,\n                        mixing_id_col=None,\n                        mixing_vars=None):\n    \"\"\"\n    Parameters\n    ----------\n    data : string or pandas dataframe.\n        If `data` is a string, it should be an absolute or relative path to\n        a CSV file containing the long format data for this choice model.\n        Note long format has one row per available alternative for each\n        observation. If `data` is a pandas dataframe, `data` should already\n        be in long format.\n    alt_id_col : string.\n        Should denote the column in data that contains the alternative\n        identifiers for each row.\n    obs_id_col : string.\n        Should denote the column in data that contains the observation\n        identifiers for each row.\n    choice_col : string.\n        Should denote the column in data which contains the ones and zeros\n        that denote whether or not the given row corresponds to the chosen\n        alternative for the given individual.\n    specification : OrderedDict.\n        Keys are a proper subset of the columns in `long_form_df`. Values are\n        either a list or a single string, `all_diff` or `all_same`. If a list,\n        the elements should be:\n            1) single objects that are within the alternative ID column of\n               `long_form_df`\n            2) lists of objects that are within the alternative ID column of\n               `long_form_df`. For each single object in the list, a unique\n                column will be created (i.e. there will be a unique\n                coefficient for that variable in the corresponding utility\n                equation of the corresponding alternative). For lists within\n                the `specification_dict` values, a single column will be\n                created for all the alternatives within iterable (i.e. there\n                will be one common coefficient for the variables in the\n                iterable).\n    model_type : string.\n        Denotes the model type of the choice_model being instantiated.\n        Should be one of the following values:\n\n            - \"MNL\"\n            - \"Asym\"\n            - \"Cloglog\"\n            - \"Scobit\"\n            - \"Uneven\"\n            - \"Nested Logit\"\n            - \"Mixed Logit\"\n    intercept_ref_pos : int, optional.\n        Valid only when the intercepts being estimated are not part of the\n        index. Specifies the alternative in the ordered array of unique\n        alternative ids whose intercept or alternative-specific constant is\n        not estimated, to ensure model identifiability. Default == None.\n    shape_ref_pos : int, optional.\n        Specifies the alternative in the ordered array of unique\n        alternative ids whose shape parameter is not estimated, to ensure\n        model identifiability. Default == None.\n    names : OrderedDict or None, optional.\n        Should have the same keys as `specification`. For each key:\n\n            - if the corresponding value in `specification` is\n              \"all_same\", then there should be a single string as the value\n              in names.\n            - if the corresponding value in `specification` is \"all_diff\",\n              then there should be a list of strings as the value in names.\n              There should be one string in the value in names for each\n              possible alternative.\n            - if the corresponding value in `specification` is a list, then\n              there should be a list of strings as the value in names.\n              There should be one string the value in names per item in the\n              value in `specification`.\n        Default == None.\n    intercept_names : list of strings or None, optional.\n        If a list is passed, then the list should have the same number of\n        elements as there are possible alternatives in data, minus 1. Each\n        element of the list should be the name of the corresponding\n        alternative's intercept term, in sorted order of the possible\n        alternative IDs. If None is passed, the resulting names that are\n        shown in the estimation results will be\n        [\"Outside_ASC_{}\".format(x) for x in shape_names]. Default = None.\n    shape_names : list of strings or None, optional.\n        If a list is passed, then the list should have the same number of\n        elements as there are possible alternative IDs in data. Each\n        element of the list should be a string denoting the name of the\n        corresponding alternative, in sorted order of the possible\n        alternative IDs. The resulting names which are shown in the\n        estimation results will be\n        [\"shape_{}\".format(x) for x in shape_names]. Default = None.\n    nest_spec : OrderedDict or None, optional.\n        Keys are strings that define the name of the nests. Values are\n        lists of alternative ids, denoting which alternatives belong to\n        which nests. Each alternative id  only be associated with a single\n        nest! Default == None.\n    mixing_id_col : str, or None, optional.\n        Should be a column heading in `data`. Should denote the column in\n        `data` which contains the identifiers of the units of observation\n        over which the coefficients of the model are thought to be randomly\n        distributed. If `model_type == \"Mixed Logit\"`, then `mixing_id_col`\n        must be passed. Default == None.\n    mixing_vars : list, or None, optional.\n        All elements of the list should be strings. Each string should be\n        present in the values of `names.values()` and they're associated\n        variables should only be index variables (i.e. part of the design\n        matrix). If `model_type == \"Mixed Logit\"`, then `mixing_vars` must\n        be passed. Default == None.\n\n    Returns\n    -------\n    model_obj : instantiation of the Choice Model Class corresponding\n        to the model type passed as the function argument. The returned\n        object will have been instantiated with the arguments passed to\n        this function.\n    \"\"\"\n    # Make sure the model type is valid\n    ensure_valid_model_type(model_type, valid_model_types)\n\n    # Carry out the appropriate instantiation process for the chosen\n    # choice model\n    model_kwargs = {\"intercept_ref_pos\": intercept_ref_pos,\n                    \"shape_ref_pos\": shape_ref_pos,\n                    \"names\": names,\n                    \"intercept_names\": intercept_names,\n                    \"shape_names\": shape_names,\n                    \"nest_spec\": nest_spec,\n                    \"mixing_id_col\": mixing_id_col,\n                    \"mixing_vars\": mixing_vars}\n    return model_type_to_class[model_type](data,\n                                           alt_id_col,\n                                           obs_id_col,\n                                           choice_col,\n                                           specification,\n                                           **model_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ensure_valid_nums_in_specification_cols(specification, dataframe):\n    problem_cols = []\n    for col in specification:\n        # The condition below checks for values that are not floats or integers\n        # This will catch values that are strings.\n        if dataframe[col].dtype.kind not in ['f', 'i', 'u']:\n            problem_cols.append(col)\n        # The condition below checks for positive or negative inifinity values.\n        elif np.isinf(dataframe[col]).any():\n            problem_cols.append(col)\n        # This condition will check for NaN values.\n        elif np.isnan(dataframe[col]).any():\n            problem_cols.append(col)\n\n    if problem_cols != []:\n        msg = \"The following columns contain either +/- inifinity values, \"\n        msg_2 = \"NaN values, or values that are not real numbers \"\n        msg_3 = \"(e.g. strings):\\n{}\"\n        total_msg = msg + msg_2 + msg_3\n        raise ValueError(total_msg.format(problem_cols))\n\n    return None", "response": "Checks whether each column in specification contains numeric data and raises a helpful ValueError if not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that ref_position is a valid integer and that num_alts is a valid number of alternatives.", "response": "def ensure_ref_position_is_valid(ref_position, num_alts, param_title):\n    \"\"\"\n    Ensures that `ref_position` is None or an integer that is in the interval\n    `[0, num_alts - 1]`. If None, ensures that intercepts are not the\n    parameters being estimated. Raises a helpful ValueError if otherwise.\n\n    Parameters\n    ----------\n    ref_position : int.\n        An integer denoting the position in an array of parameters that will\n        be constrained for identification purposes.\n    num_alts : int.\n        An integer denoting the total number of alternatives in one's universal\n        choice set.\n    param_title : {'intercept_names', 'shape_names'}.\n        String denoting the name of the parameters that are being estimated,\n        with a constraint for identification. E.g. 'intercept_names'.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    assert param_title in ['intercept_names', 'shape_names']\n\n    try:\n        assert ref_position is None or isinstance(ref_position, int)\n    except AssertionError:\n        msg = \"ref_position for {} must be an int or None.\"\n        raise TypeError(msg.format(param_title))\n\n    if param_title == \"intercept_names\":\n        try:\n            assert ref_position is not None\n        except AssertionError:\n            raise ValueError(\"At least one intercept should be constrained.\")\n\n    try:\n        if ref_position is not None:\n            assert ref_position >= 0 and ref_position <= num_alts - 1\n    except AssertionError:\n        msg = \"ref_position must be between 0 and num_alts - 1.\"\n        raise ValueError(msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that the length of the parameter names is equal to the number of alternatives in the universal choice set.", "response": "def check_length_of_shape_or_intercept_names(name_list,\n                                             num_alts,\n                                             constrained_param,\n                                             list_title):\n    \"\"\"\n    Ensures that the length of the parameter names matches the number of\n    parameters that will be estimated. Will raise a ValueError otherwise.\n\n    Parameters\n    ----------\n    name_list : list of strings.\n        Each element should be the name of a parameter that is to be estimated.\n    num_alts : int.\n        Should be the total number of alternatives in the universal choice set\n        for this dataset.\n    constrainted_param : {0, 1, True, False}\n        Indicates whether (1 or True) or not (0 or False) one of the type of\n        parameters being estimated will be constrained. For instance,\n        constraining one of the intercepts.\n    list_title : str.\n        Should specify the type of parameters whose names are being checked.\n        Examples include 'intercept_params' or 'shape_params'.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if len(name_list) != (num_alts - constrained_param):\n        msg_1 = \"{} is of the wrong length:\".format(list_title)\n        msg_2 = \"len({}) == {}\".format(list_title, len(name_list))\n        correct_length = num_alts - constrained_param\n        msg_3 = \"The correct length is: {}\".format(correct_length)\n        total_msg = \"\\n\".join([msg_1, msg_2, msg_3])\n        raise ValueError(total_msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_type_of_nest_spec_keys_and_values(nest_spec):\n    try:\n        assert all([isinstance(k, str) for k in nest_spec])\n        assert all([isinstance(nest_spec[k], list) for k in nest_spec])\n    except AssertionError:\n        msg = \"All nest_spec keys/values must be strings/lists.\"\n        raise TypeError(msg)\n\n    return None", "response": "Checks that the keys and values of nest_spec are strings and lists. Raises a helpful ValueError if they are not."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_for_empty_nests_in_nest_spec(nest_spec):\n    empty_nests = []\n    for k in nest_spec:\n        if len(nest_spec[k]) == 0:\n            empty_nests.append(k)\n    if empty_nests != []:\n        msg = \"The following nests are INCORRECTLY empty: {}\"\n        raise ValueError(msg.format(empty_nests))\n\n    return None", "response": "Checks that the nests in nest_spec are not empty lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ensure_alt_ids_in_nest_spec_are_ints(nest_spec, list_elements):\n    try:\n        assert all([isinstance(x, int) for x in list_elements])\n    except AssertionError:\n        msg = \"All elements of the nest_spec values should be integers\"\n        raise ValueError(msg)\n\n    return None", "response": "Ensures that the alternative id s in nest_spec are integers. Raises a helpful ValueError if they are not."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that the alternative id s in nest_spec are only associated with a single nest. Raises a helpful ValueError if they are not.", "response": "def ensure_alt_ids_are_only_in_one_nest(nest_spec, list_elements):\n    \"\"\"\n    Ensures that the alternative id's in `nest_spec` are only associated with\n    a single nest. Raises a helpful ValueError if they are not.\n\n    Parameters\n    ----------\n    nest_spec : OrderedDict, or None, optional.\n        Keys are strings that define the name of the nests. Values are lists of\n        alternative ids, denoting which alternatives belong to which nests.\n        Each alternative id must only be associated with a single nest!\n        Default == None.\n    list_elements : list of ints.\n        Each element should correspond to one of the alternatives identified as\n        belonging to a nest.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    try:\n        assert len(set(list_elements)) == len(list_elements)\n    except AssertionError:\n        msg = \"Each alternative id should only be in a single nest.\"\n        raise ValueError(msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_all_alt_ids_have_a_nest(nest_spec, list_elements, all_ids):\n    unaccounted_alt_ids = []\n    for alt_id in all_ids:\n        if alt_id not in list_elements:\n            unaccounted_alt_ids.append(alt_id)\n    if unaccounted_alt_ids != []:\n        msg = \"Associate the following alternative ids with a nest: {}\"\n        raise ValueError(msg.format(unaccounted_alt_ids))\n\n    return None", "response": "Ensures that the alternative ids in nest_spec are all associated with a single nest. Raises a helpful ValueError if they are not."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that the alternative id s in nest_spec are all in the universal choice set. Raises a helpful ValueError if they are not.", "response": "def ensure_nest_alts_are_valid_alts(nest_spec, list_elements, all_ids):\n    \"\"\"\n    Ensures that the alternative id's in `nest_spec` are all in the universal\n    choice set for this dataset. Raises a helpful ValueError if they are not.\n\n    Parameters\n    ----------\n    nest_spec : OrderedDict, or None, optional.\n        Keys are strings that define the name of the nests. Values are lists of\n        alternative ids, denoting which alternatives belong to which nests.\n        Each alternative id must only be associated with a single nest!\n        Default == None.\n    list_elements : list of ints.\n        Each element should correspond to one of the alternatives identified as\n        belonging to a nest.\n    all_ids : list of ints.\n        Each element should correspond to one of the alternatives that is\n        present in the universal choice set for this model.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    invalid_alt_ids = []\n    for x in list_elements:\n        if x not in all_ids:\n            invalid_alt_ids.append(x)\n    if invalid_alt_ids != []:\n        msg = \"The following elements are not in df[alt_id_col]: {}\"\n        raise ValueError(msg.format(invalid_alt_ids))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_num_rows_of_parameter_array(param_array, correct_num_rows, title):\n    if param_array.shape[0] != correct_num_rows:\n        msg = \"{}.shape[0] should equal {}, but it does not\"\n        raise ValueError(msg.format(title, correct_num_rows))\n\n    return None", "response": "Checks that the shape of the param_array is equal to the correct_num_rows. Raises a helpful ValueError if otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that the param_list is a list with the expected length. Raises a helpful ValueError if this is not the case.", "response": "def check_type_and_size_of_param_list(param_list, expected_length):\n    \"\"\"\n    Ensure that param_list is a list with the expected length. Raises a helpful\n    ValueError if this is not the case.\n    \"\"\"\n    try:\n        assert isinstance(param_list, list)\n        assert len(param_list) == expected_length\n    except AssertionError:\n        msg = \"param_list must be a list containing {} elements.\"\n        raise ValueError(msg.format(expected_length))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that all elements of param_list are numpy arrays or None. Raises a helpful ValueError if otherwise.", "response": "def check_type_of_param_list_elements(param_list):\n    \"\"\"\n    Ensures that all elements of param_list are ndarrays or None. Raises a\n    helpful ValueError if otherwise.\n    \"\"\"\n    try:\n        assert isinstance(param_list[0], np.ndarray)\n        assert all([(x is None or isinstance(x, np.ndarray))\n                    for x in param_list])\n    except AssertionError:\n        msg = \"param_list[0] must be a numpy array.\"\n        msg_2 = \"All other elements must be numpy arrays or None.\"\n        total_msg = msg + \"\\n\" + msg_2\n        raise TypeError(total_msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that each array in param_list has the same number of columns. Raises a helpful ValueError if otherwise.", "response": "def check_num_columns_in_param_list_arrays(param_list):\n    \"\"\"\n    Ensure that each array in param_list, that is not None, has the same number\n    of columns. Raises a helpful ValueError if otherwise.\n\n    Parameters\n    ----------\n    param_list : list of ndarrays or None.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    try:\n        num_columns = param_list[0].shape[1]\n        assert all([x is None or (x.shape[1] == num_columns)\n                    for x in param_list])\n    except AssertionError:\n        msg = \"param_list arrays should have equal number of columns.\"\n        raise ValueError(msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that all arrays in param_list have the same number of dimensions. Raises a helpful ValueError if otherwise.", "response": "def check_dimensional_equality_of_param_list_arrays(param_list):\n    \"\"\"\n    Ensures that all arrays in param_list have the same dimension, and that\n    this dimension is either 1 or 2 (i.e. all arrays are 1D arrays or all\n    arrays are 2D arrays.) Raises a helpful ValueError if otherwise.\n\n    Parameters\n    ----------\n    param_list : list of ndarrays or None.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    try:\n        num_dimensions = len(param_list[0].shape)\n        assert num_dimensions in [1, 2]\n        assert all([(x is None or (len(x.shape) == num_dimensions))\n                    for x in param_list])\n    except AssertionError:\n        msg = \"Each array in param_list should be 1D or 2D.\"\n        msg_2 = \"And all arrays should have the same number of dimensions.\"\n        total_msg = msg + \"\\n\" + msg_2\n        raise ValueError(total_msg)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that all of the mixing_vars in ind_var_names are present in name_dict. Raises a helpful ValueError if otherwise.", "response": "def ensure_all_mixing_vars_are_in_the_name_dict(mixing_vars,\n                                                name_dict,\n                                                ind_var_names):\n    \"\"\"\n    Ensures that all of the variables listed in `mixing_vars` are present in\n    `ind_var_names`. Raises a helpful ValueError if otherwise.\n\n    Parameters\n    ----------\n    mixing_vars : list of strings, or None.\n        Each string denotes a parameter to be treated as a random variable.\n    name_dict : OrderedDict or None.\n        Contains the specification relating column headers in one's data (i.e.\n        the keys of the OrderedDict) to the index coefficients to be estimated\n        based on this data (i.e. the values of each key).\n    ind_var_names : list of strings.\n        Each string denotes an index coefficient (i.e. a beta) to be estimated.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if mixing_vars is None:\n        return None\n\n    # Determine the strings in mixing_vars that are missing from ind_var_names\n    problem_names = [variable_name for variable_name in mixing_vars\n                     if variable_name not in ind_var_names]\n\n    # Create error messages for the case where we have a name dictionary and\n    # the case where we do not have a name dictionary.\n    msg_0 = \"The following parameter names were not in the values of the \"\n    msg_1 = \"passed name dictionary: \\n{}\"\n    msg_with_name_dict = msg_0 + msg_1.format(problem_names)\n\n    msg_2 = \"The following paramter names did not match any of the default \"\n    msg_3 = \"names generated for the parameters to be estimated: \\n{}\"\n    msg_4 = \"The default names that were generated were: \\n{}\"\n    msg_without_name_dict = (msg_2 +\n                             msg_3.format(problem_names) +\n                             msg_4.format(ind_var_names))\n\n    # Raise a helpful ValueError if any mixing_vars were missing from\n    # ind_var_names\n    if problem_names != []:\n        if name_dict:\n            raise ValueError(msg_with_name_dict)\n        else:\n            raise ValueError(msg_without_name_dict)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nensure that all of the available alternatives in the given dataframe are chosen at the given alternative ID. Raises a ValueError if they are not.", "response": "def ensure_all_alternatives_are_chosen(alt_id_col, choice_col, dataframe):\n    \"\"\"\n    Ensures that all of the available alternatives in the dataset are chosen at\n    least once (for model identification). Raises a ValueError otherwise.\n\n    Parameters\n    ----------\n    alt_id_col : str.\n        Should denote the column in `dataframe` that contains the alternative\n        identifiers for each row.\n    choice_col : str.\n        Should denote the column in `dataframe` that contains the ones and\n        zeros that denote whether or not the given row corresponds to the\n        chosen alternative for the given individual.\n    dataframe : pandas dataframe.\n        Should contain the data being used to estimate the model, as well as\n        the headers denoted by `alt_id_col` and `choice_col`.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    all_ids = set(dataframe[alt_id_col].unique())\n    chosen_ids = set(dataframe.loc[dataframe[choice_col] == 1,\n                                   alt_id_col].unique())\n    non_chosen_ids = all_ids.difference(chosen_ids)\n    if len(non_chosen_ids) != 0:\n        msg = (\"The following alternative ID's were not chosen in any choice \"\n               \"situation: \\n{}\")\n        raise ValueError(msg.format(non_chosen_ids))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_aic(model_object):\n    assert isinstance(model_object.params, pd.Series)\n    assert isinstance(model_object.log_likelihood, Number)\n\n    return -2 * model_object.log_likelihood + 2 * model_object.params.size", "response": "Compute the Akaike Information Criteria for an estimated model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the Bayesian Information Criteria for an estimated MNDC_Model instance.", "response": "def compute_bic(model_object):\n    \"\"\"\n    Compute the Bayesian Information Criteria for an estimated model.\n\n    Parameters\n    ----------\n    model_object : an MNDC_Model (multinomial discrete choice model) instance.\n        The model should have already been estimated.\n        `model_object.log_likelihood` and `model_object.nobs` should be a\n        number, and `model_object.params` should be a pandas Series.\n\n    Returns\n    -------\n    bic : float.\n        The BIC for the estimated model.\n\n    Notes\n    -----\n    bic = -2 * log_likelihood + log(num_observations) * num_parameters\n\n    The original BIC was introduced as (-1 / 2) times the formula above.\n    However, for model comparison purposes, it does not matter if the\n    goodness-of-fit measure is multiplied by a constant across all models being\n    compared. Moreover, the formula used above allows for a common scale\n    between measures such as the AIC, BIC, DIC, etc.\n\n    References\n    ----------\n    Schwarz, G. (1978), 'Estimating the dimension of a model', The Annals of\n        Statistics 6, 2: 461\u2013464.\n    \"\"\"\n    assert isinstance(model_object.params, pd.Series)\n    assert isinstance(model_object.log_likelihood, Number)\n    assert isinstance(model_object.nobs, Number)\n\n    log_likelihood = model_object.log_likelihood\n    num_obs = model_object.nobs\n    num_params = model_object.params.size\n\n    return -2 * log_likelihood + np.log(num_obs) * num_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_mappings_for_fit(self, dense=False):\n        return create_long_form_mappings(self.data,\n                                         self.obs_id_col,\n                                         self.alt_id_col,\n                                         choice_col=self.choice_col,\n                                         nest_spec=self.nest_spec,\n                                         mix_id_col=self.mixing_id_col,\n                                         dense=dense)", "response": "Returns a dictionary mapping the observations to the alternatives that are associated with the given node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the basic estimation results from the given dictionary and stores them on the object self.", "response": "def _store_basic_estimation_results(self, results_dict):\n        \"\"\"\n        Extracts the basic estimation results (i.e. those that need no further\n        calculation or logic applied to them) and stores them on the model\n        object.\n\n        Parameters\n         ----------\n        results_dict : dict.\n            The estimation result dictionary that is output from\n            scipy.optimize.minimize. In addition to the standard keys which are\n            included, it should also contain the following keys:\n            `[\"final_log_likelihood\", \"chosen_probs\", \"long_probs\",\n              \"residuals\", \"ind_chi_squareds\", \"sucess\", \"message\",\n              \"rho_squared\", \"rho_bar_squared\", \"log_likelihood_null\"]`\n\n        Returns\n        -------\n        None.\n        \"\"\"\n        # Store the log-likelilhood, fitted probabilities, residuals, and\n        # individual chi-square statistics\n        self.log_likelihood = results_dict[\"final_log_likelihood\"]\n        self.fitted_probs = results_dict[\"chosen_probs\"]\n        self.long_fitted_probs = results_dict[\"long_probs\"]\n        self.long_residuals = results_dict[\"residuals\"]\n        self.ind_chi_squareds = results_dict[\"ind_chi_squareds\"]\n        self.chi_square = self.ind_chi_squareds.sum()\n\n        # Store the 'estimation success' of the optimization\n        self.estimation_success = results_dict[\"success\"]\n        self.estimation_message = results_dict[\"message\"]\n\n        # Store the summary measures of the model fit\n        self.rho_squared = results_dict[\"rho_squared\"]\n        self.rho_bar_squared = results_dict[\"rho_bar_squared\"]\n\n        # Store the initial and null log-likelihoods\n        self.null_log_likelihood = results_dict[\"log_likelihood_null\"]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the dataframe that displays the estimation results and store it on the model instance.", "response": "def _create_results_summary(self):\n        \"\"\"\n        Create the dataframe that displays the estimation results, and store\n        it on the model instance.\n\n        Returns\n        -------\n        None.\n        \"\"\"\n        # Make sure we have all attributes needed to create the results summary\n        needed_attributes = [\"params\",\n                             \"standard_errors\",\n                             \"tvalues\",\n                             \"pvalues\",\n                             \"robust_std_errs\",\n                             \"robust_t_stats\",\n                             \"robust_p_vals\"]\n        try:\n            assert all([hasattr(self, attr) for attr in needed_attributes])\n            assert all([isinstance(getattr(self, attr), pd.Series)\n                        for attr in needed_attributes])\n        except AssertionError:\n            msg = \"Call this function only after setting/calculating all other\"\n            msg_2 = \" estimation results attributes\"\n            raise NotImplementedError(msg + msg_2)\n\n        self.summary = pd.concat((self.params,\n                                  self.standard_errors,\n                                  self.tvalues,\n                                  self.pvalues,\n                                  self.robust_std_errs,\n                                  self.robust_t_stats,\n                                  self.robust_p_vals), axis=1)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _record_values_for_fit_summary_and_statsmodels(self):\n        # Make sure we have all attributes needed to create the results summary\n        needed_attributes = [\"fitted_probs\",\n                             \"params\",\n                             \"log_likelihood\",\n                             \"standard_errors\"]\n        try:\n            assert all([hasattr(self, attr) for attr in needed_attributes])\n            assert all([getattr(self, attr) is not None\n                        for attr in needed_attributes])\n        except AssertionError:\n            msg = \"Call this function only after setting/calculating all other\"\n            msg_2 = \" estimation results attributes\"\n            raise NotImplementedError(msg + msg_2)\n\n        # Record the number of observations\n        self.nobs = self.fitted_probs.shape[0]\n        # This is the number of estimated parameters\n        self.df_model = self.params.shape[0]\n        # The number of observations minus the number of estimated parameters\n        self.df_resid = self.nobs - self.df_model\n        # This is just the log-likelihood. The opaque name is used for\n        # conformance with statsmodels\n        self.llf = self.log_likelihood\n        # This is just a repeat of the standard errors\n        self.bse = self.standard_errors\n        # These are the penalized measures of fit used for model comparison\n        self.aic = compute_aic(self)\n        self.bic = compute_bic(self)\n\n        return None", "response": "Record the values needed to create the summary and statsmodels estimation results table for the given model instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and store a pandas series that will display to users the fit_summary attribute of the object.", "response": "def _create_fit_summary(self):\n        \"\"\"\n        Create and store a pandas series that will display to users the\n        various statistics/values that indicate how well the estimated model\n        fit the given dataset.\n\n        Returns\n        -------\n        None.\n        \"\"\"\n        # Make sure we have all attributes needed to create the results summary\n        needed_attributes = [\"df_model\",\n                             \"nobs\",\n                             \"null_log_likelihood\",\n                             \"log_likelihood\",\n                             \"rho_squared\",\n                             \"rho_bar_squared\",\n                             \"estimation_message\"]\n        try:\n            assert all([hasattr(self, attr) for attr in needed_attributes])\n            assert all([getattr(self, attr) is not None\n                        for attr in needed_attributes])\n        except AssertionError:\n            msg = \"Call this function only after setting/calculating all other\"\n            msg_2 = \" estimation results attributes\"\n            raise NotImplementedError(msg + msg_2)\n\n        self.fit_summary = pd.Series([self.df_model,\n                                      self.nobs,\n                                      self.null_log_likelihood,\n                                      self.log_likelihood,\n                                      self.rho_squared,\n                                      self.rho_bar_squared,\n                                      self.estimation_message],\n                                     index=[\"Number of Parameters\",\n                                            \"Number of Observations\",\n                                            \"Null Log-Likelihood\",\n                                            \"Fitted Log-Likelihood\",\n                                            \"Rho-Squared\",\n                                            \"Rho-Bar-Squared\",\n                                            \"Estimation Message\"])\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring the estimation results that relate to statistical inference on the model instance.", "response": "def _store_inferential_results(self,\n                                   value_array,\n                                   index_names,\n                                   attribute_name,\n                                   series_name=None,\n                                   column_names=None):\n        \"\"\"\n        Store the estimation results that relate to statistical inference, such\n        as parameter estimates, standard errors, p-values, etc.\n\n        Parameters\n        ----------\n        value_array : 1D or 2D ndarray.\n            Contains the values that are to be stored on the model instance.\n        index_names : list of strings.\n            Contains the names that are to be displayed on the 'rows' for each\n            value being stored. There should be one element for each value of\n            `value_array.`\n        series_name : string or None, optional.\n            The name of the pandas series being created for `value_array.` This\n            kwarg should be None when `value_array` is a 1D ndarray.\n        attribute_name : string.\n            The attribute name that will be exposed on the model instance and\n            related to the passed `value_array.`\n        column_names : list of strings, or None, optional.\n            Same as `index_names` except that it pertains to the columns of a\n            2D ndarray. When `value_array` is a 2D ndarray, There should be one\n            element for each column of `value_array.` This kwarg should be None\n            otherwise.\n\n        Returns\n        -------\n        None. Stores a pandas series or dataframe on the model instance.\n        \"\"\"\n        if len(value_array.shape) == 1:\n            assert series_name is not None\n            new_attribute_value = pd.Series(value_array,\n                                            index=index_names,\n                                            name=series_name)\n        elif len(value_array.shape) == 2:\n            assert column_names is not None\n            new_attribute_value = pd.DataFrame(value_array,\n                                               index=index_names,\n                                               columns=column_names)\n\n        setattr(self, attribute_name, new_attribute_value)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _store_generic_inference_results(self,\n                                         results_dict,\n                                         all_params,\n                                         all_names):\n        \"\"\"\n        Store the model inference values that are common to all choice models.\n        This includes things like index coefficients, gradients, hessians,\n        asymptotic covariance matrices, t-values, p-values, and robust versions\n        of these values.\n\n        Parameters\n        ----------\n        results_dict : dict.\n            The estimation result dictionary that is output from\n            scipy.optimize.minimize. In addition to the standard keys which are\n            included, it should also contain the following keys:\n           `[\"utility_coefs\", \"final_gradient\", \"final_hessian\",\n             \"fisher_info\"]`.\n            The \"final_gradient\", \"final_hessian\", and \"fisher_info\" values\n            should be the gradient, hessian, and Fisher-Information Matrix of\n            the log likelihood, evaluated at the final parameter vector.\n        all_params : list of 1D ndarrays.\n            Should contain the various types of parameters that were actually\n            estimated.\n        all_names : list of strings.\n            Should contain names of each estimated parameter.\n\n        Returns\n        -------\n        None. Stores all results on the model instance.\n        \"\"\"\n        # Store the utility coefficients\n        self._store_inferential_results(results_dict[\"utility_coefs\"],\n                                        index_names=self.ind_var_names,\n                                        attribute_name=\"coefs\",\n                                        series_name=\"coefficients\")\n\n        # Store the gradient\n        self._store_inferential_results(results_dict[\"final_gradient\"],\n                                        index_names=all_names,\n                                        attribute_name=\"gradient\",\n                                        series_name=\"gradient\")\n\n        # Store the hessian\n        self._store_inferential_results(results_dict[\"final_hessian\"],\n                                        index_names=all_names,\n                                        attribute_name=\"hessian\",\n                                        column_names=all_names)\n\n        # Store the variance-covariance matrix\n        self._store_inferential_results(-1 * scipy.linalg.inv(self.hessian),\n                                        index_names=all_names,\n                                        attribute_name=\"cov\",\n                                        column_names=all_names)\n\n        # Store ALL of the estimated parameters\n        self._store_inferential_results(np.concatenate(all_params, axis=0),\n                                        index_names=all_names,\n                                        attribute_name=\"params\",\n                                        series_name=\"parameters\")\n\n        # Store the standard errors\n        self._store_inferential_results(np.sqrt(np.diag(self.cov)),\n                                        index_names=all_names,\n                                        attribute_name=\"standard_errors\",\n                                        series_name=\"std_err\")\n\n        # Store the t-stats of the estimated parameters\n        self.tvalues = self.params / self.standard_errors\n        self.tvalues.name = \"t_stats\"\n\n        # Store the p-values\n        p_vals = 2 * scipy.stats.norm.sf(np.abs(self.tvalues))\n        self._store_inferential_results(p_vals,\n                                        index_names=all_names,\n                                        attribute_name=\"pvalues\",\n                                        series_name=\"p_values\")\n\n        # Store the fischer information matrix of estimated coefficients\n        self._store_inferential_results(results_dict[\"fisher_info\"],\n                                        index_names=all_names,\n                                        attribute_name=\"fisher_information\",\n                                        column_names=all_names)\n\n        # Store the 'robust' variance-covariance matrix\n        robust_covariance = calc_asymptotic_covariance(self.hessian,\n                                                       self.fisher_information)\n        self._store_inferential_results(robust_covariance,\n                                        index_names=all_names,\n                                        attribute_name=\"robust_cov\",\n                                        column_names=all_names)\n\n        # Store the 'robust' standard errors\n        self._store_inferential_results(np.sqrt(np.diag(self.robust_cov)),\n                                        index_names=all_names,\n                                        attribute_name=\"robust_std_errs\",\n                                        series_name=\"robust_std_err\")\n\n        # Store the 'robust' t-stats of the estimated coefficients\n        self.robust_t_stats = self.params / self.robust_std_errs\n        self.robust_t_stats.name = \"robust_t_stats\"\n\n        # Store the 'robust' p-values\n        one_sided_p_vals = scipy.stats.norm.sf(np.abs(self.robust_t_stats))\n        self._store_inferential_results(2 * one_sided_p_vals,\n                                        index_names=all_names,\n                                        attribute_name=\"robust_p_vals\",\n                                        series_name=\"robust_p_values\")\n\n        return None", "response": "Stores the generic inference results for the current choice model instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _store_optional_parameters(self,\n                                   optional_params,\n                                   name_list_attr,\n                                   default_name_str,\n                                   all_names,\n                                   all_params,\n                                   param_attr_name,\n                                   series_name):\n        \"\"\"\n        Extract the optional parameters from the `results_dict`, save them\n        to the model object, and update the list of all parameters and all\n        parameter names.\n\n        Parameters\n        ----------\n        optional_params : 1D ndarray.\n            The optional parameters whose values and names should be stored.\n        name_list_attr : str.\n            The attribute name on the model object where the names of the\n            optional estimated parameters will be stored (if they exist).\n        default_name_str : str.\n            The name string that will be used to create generic names for the\n            estimated parameters, in the event that the estimated parameters\n            do not have names that were specified by the user. Should contain\n            empty curly braces for use with python string formatting.\n        all_names : list of strings.\n            The current list of the names of the estimated parameters. The\n            names of these optional parameters will be added to the beginning\n            of this list.\n        all_params : list of 1D ndarrays.\n            Each array is a set of estimated parameters. The current optional\n            parameters will be added to the beginning of this list.\n        param_attr_name : str.\n            The attribute name that will be used to store the optional\n            parameter values on the model object.\n        series_name : str.\n            The string that will be used as the name of the series that\n            contains the optional parameters.\n\n        Returns\n        -------\n        (all_names, all_params) : tuple.\n        \"\"\"\n        # Identify the number of optional parameters\n        num_elements = optional_params.shape[0]\n\n        # Get the names of the optional parameters\n        parameter_names = getattr(self, name_list_attr)\n        if parameter_names is None:\n            parameter_names = [default_name_str.format(x) for x in\n                               range(1, num_elements + 1)]\n\n        # Store the names of the optional parameters in all_names\n        all_names = list(parameter_names) + list(all_names)\n        # Store the values of the optional parameters in all_params\n        all_params.insert(0, optional_params)\n\n        # Store the optional parameters on the model object\n        self._store_inferential_results(optional_params,\n                                        index_names=parameter_names,\n                                        attribute_name=param_attr_name,\n                                        series_name=series_name)\n        return all_names, all_params", "response": "Internal method that stores the optional parameters in the results_dict attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _adjust_inferential_results_for_parameter_constraints(self,\n                                                              constraints):\n        \"\"\"\n        Ensure that parameters that were constrained during estimation do not\n        have any values showed for inferential results. After all, no inference\n        was performed.\n\n        Parameters\n        ----------\n        constraints : list of ints, or None.\n            If list, should contain the positions in the array of all estimated\n            parameters that were constrained to their initial values.\n\n        Returns\n        -------\n        None.\n        \"\"\"\n        if constraints is not None:\n            # Ensure the model object has inferential results\n            inferential_attributes = [\"standard_errors\",\n                                      \"tvalues\",\n                                      \"pvalues\",\n                                      \"robust_std_errs\",\n                                      \"robust_t_stats\",\n                                      \"robust_p_vals\"]\n            assert all([hasattr(self, x) for x in inferential_attributes])\n            assert hasattr(self, \"params\")\n\n            all_names = self.params.index.tolist()\n\n            for series in [getattr(self, x) for x in inferential_attributes]:\n                for pos in constraints:\n                    series.loc[all_names[pos]] = np.nan\n\n        return None", "response": "Adjusts the inferential results for the specified set of parameters that were constrained during estimation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_result_dict_for_needed_keys(self, results_dict):\n        missing_cols = [x for x in needed_result_keys if x not in results_dict]\n        if missing_cols != []:\n            msg = \"The following keys are missing from results_dict\\n{}\"\n            raise ValueError(msg.format(missing_cols))\n        return None", "response": "Ensures that the results_dict has the needed keys to store all the\n            estimation results. Raise a helpful ValueError otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_mixing_variable_names_to_individual_vars(self):\n        assert isinstance(self.ind_var_names, list)\n        # Note that if one estimates a mixed logit model, then the mixing\n        # variables will be added to individual vars. And if one estimates\n        # the model again (perhaps from different starting values), then\n        # an error will be raised when creating the coefs series because we\n        # will have added the mixing variables twice. The condition below\n        # should prevent this error.\n        already_included = any([\"Sigma \" in x for x in self.ind_var_names])\n\n        if self.mixing_vars is not None and not already_included:\n            new_ind_var_names = [\"Sigma \" + x for x in self.mixing_vars]\n            self.ind_var_names += new_ind_var_names\n        return None", "response": "Add the mixing variable names to the list of individual variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef store_fit_results(self, results_dict):\n        # Check to make sure the results_dict has all the needed keys.\n        self._check_result_dict_for_needed_keys(results_dict)\n\n        # Store the basic estimation results that simply need to be transferred\n        # from the results_dict to the model instance.\n        self._store_basic_estimation_results(results_dict)\n\n        # Account for attributes from the mixed logit model.\n        if not hasattr(self, \"design_3d\"):\n            self.design_3d = None\n\n        # Initialize the lists of all parameter names and all parameter values\n        # Note we add the new mixing variables to the list of index\n        # coefficients after estimation so that we can correctly create the\n        # design matrix during the estimation proces. The create_design_3d\n        # function relies on the original list of independent variable names.\n        self._add_mixing_variable_names_to_individual_vars()\n        all_names = deepcopy(self.ind_var_names)\n        all_params = [deepcopy(results_dict[\"utility_coefs\"])]\n\n        ##########\n        # Figure out whether this model had nest, shape, or intercept\n        # parameters and store each of these appropriately\n        ##########\n        if results_dict[\"intercept_params\"] is not None:\n            storage_args = [results_dict[\"intercept_params\"],\n                            \"intercept_names\",\n                            \"Outside_ASC_{}\",\n                            all_names,\n                            all_params,\n                            \"intercepts\",\n                            \"intercept_parameters\"]\n            storage_results = self._store_optional_parameters(*storage_args)\n            all_names, all_params = storage_results\n        else:\n            self.intercepts = None\n\n        if results_dict[\"shape_params\"] is not None:\n            storage_args = [results_dict[\"shape_params\"],\n                            \"shape_names\",\n                            \"Shape_{}\",\n                            all_names,\n                            all_params,\n                            \"shapes\",\n                            \"shape_parameters\"]\n            storage_results = self._store_optional_parameters(*storage_args)\n            all_names, all_params = storage_results\n        else:\n            self.shapes = None\n\n        if results_dict[\"nest_params\"] is not None:\n            storage_args = [results_dict[\"nest_params\"],\n                            \"nest_names\",\n                            \"Nest_Param_{}\",\n                            all_names,\n                            all_params,\n                            \"nests\",\n                            \"nest_parameters\"]\n            storage_results = self._store_optional_parameters(*storage_args)\n            all_names, all_params = storage_results\n        else:\n            self.nests = None\n\n        # Store the model results and values needed for model inference\n        self._store_generic_inference_results(results_dict,\n                                              all_params,\n                                              all_names)\n\n        # Adjust the inferential results to account for parameters that were\n        # not actually estimated, i.e. parameters that were constrained.\n        constraints = results_dict[\"constrained_pos\"]\n        self._adjust_inferential_results_for_parameter_constraints(constraints)\n\n        # Store a summary dataframe of the estimation results\n        self._create_results_summary()\n\n        # Record values for the fit_summary and statsmodels table\n        self._record_values_for_fit_summary_and_statsmodels()\n\n        # Store a \"Fit Summary\"\n        self._create_fit_summary()\n\n        return None", "response": "Stores the results of the fit of the logit model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the measures of fit and the estimation results for the model.", "response": "def print_summaries(self):\n        \"\"\"\n        Returns None. Will print the measures of fit and the estimation results\n        for the  model.\n        \"\"\"\n        if hasattr(self, \"fit_summary\") and hasattr(self, \"summary\"):\n            print(\"\\n\")\n            print(self.fit_summary)\n            print(\"=\" * 30)\n            print(self.summary)\n\n        else:\n            msg = \"This {} object has not yet been estimated so there \"\n            msg_2 = \"are no estimation summaries to print.\"\n            raise NotImplementedError(msg.format(self.model_type) + msg_2)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef conf_int(self, alpha=0.05, coefs=None, return_df=False):\n\n        # Get the critical z-value for alpha / 2\n        z_critical = scipy.stats.norm.ppf(1.0 - alpha / 2.0,\n                                          loc=0, scale=1)\n\n        # Calculate the lower and upper values for the confidence interval.\n        lower = self.params - z_critical * self.standard_errors\n        upper = self.params + z_critical * self.standard_errors\n        # Name the series of lower / upper values for the confidence interval.\n        lower.name = \"lower\"\n        upper.name = \"upper\"\n\n        # Combine the various series.\n        combined = pd.concat((lower, upper), axis=1)\n\n        # Subset the combined dataframe if need be.\n        if coefs is not None:\n            combined = combined.loc[coefs, :]\n\n        # Return the desired object, whether dataframe or array\n        if return_df:\n            return combined\n        else:\n            return combined.values", "response": "This function calculates the confidence interval of the one - term log entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_statsmodels_summary(self,\n                                title=None,\n                                alpha=.05):\n        \"\"\"\n        Parameters\n        ----------\n        title : str, or None, optional.\n            Will be the title of the returned summary. If None, the default\n            title is used.\n        alpha : float, optional.\n            Should be between 0.0 and 1.0. Determines the width of the\n            displayed, (1 - alpha)% confidence interval.\n\n        Returns\n        -------\n        statsmodels.summary object or None.\n        \"\"\"\n        try:\n            # Get the statsmodels Summary class\n            from statsmodels.iolib.summary import Summary\n        except ImportError:\n            print(\"statsmodels not installed. Resorting to standard summary\")\n            return self.print_summaries()\n\n        if not hasattr(self, \"estimation_success\"):\n            msg = \"Must estimate a model before a summary can be returned.\"\n            raise NotImplementedError(msg)\n\n        # Get an instantiation of the Summary class.\n        smry = Summary()\n\n        # Get the yname and yname_list.\n        # Note I'm not really sure what the yname_list is.\n        new_yname, new_yname_list = self.choice_col, None\n\n        # Get the model name\n        model_name = self.model_type\n\n        ##########\n        # Note the following commands are basically directly from\n        # statsmodels.discrete.discrete_model\n        ##########\n        top_left = [('Dep. Variable:', None),\n                    ('Model:', [model_name]),\n                    ('Method:', ['MLE']),\n                    ('Date:', None),\n                    ('Time:', None),\n                    ('AIC:', [\"{:,.3f}\".format(self.aic)]),\n                    ('BIC:', [\"{:,.3f}\".format(self.bic)])\n                    ]\n\n        top_right = [('No. Observations:', [\"{:,}\".format(self.nobs)]),\n                     ('Df Residuals:', [\"{:,}\".format(self.df_resid)]),\n                     ('Df Model:', [\"{:,}\".format(self.df_model)]),\n                     ('Pseudo R-squ.:',\n                      [\"{:.3f}\".format(self.rho_squared)]),\n                     ('Pseudo R-bar-squ.:',\n                      [\"{:.3f}\".format(self.rho_bar_squared)]),\n                     ('Log-Likelihood:', [\"{:,.3f}\".format(self.llf)]),\n                     ('LL-Null:',\n                      [\"{:,.3f}\".format(self.null_log_likelihood)]),\n                     ]\n\n        if title is None:\n            title = model_name + ' ' + \"Regression Results\"\n\n        xnames = self.params.index.tolist()\n\n        # for top of table\n        smry.add_table_2cols(self,\n                             gleft=top_left,\n                             gright=top_right,  # [],\n                             yname=new_yname,\n                             xname=xnames,\n                             title=title)\n        # for parameters, etc\n        smry.add_table_params(self,\n                              yname=[new_yname_list],\n                              xname=xnames,\n                              alpha=alpha,\n                              use_t=False)\n        return smry", "response": "Returns a new instance of statsmodels. summary. Summary object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the given parameter list is well - formed and that all of the elements in the parameter list are in the correct format.", "response": "def check_param_list_validity(self, param_list):\n        \"\"\"\n        Parameters\n        ----------\n        param_list : list.\n            Contains four elements, each being a numpy array. Either all of the\n            arrays should be 1D or all of the arrays should be 2D. If 2D, the\n            arrays should have the same number of columns. Each column being a\n            particular set of parameter values that one wants to predict with.\n            The first element in the list should be the index coefficients. The\n            second element should contain the 'outside' intercept parameters if\n            there are any, or None otherwise. The third element should contain\n            the shape parameters if there are any or None otherwise. The fourth\n            element should contain the nest coefficients if there are any or\n            None otherwise. Default == None.\n\n        Returns\n        -------\n        None. Will check whether `param_list` and its elements meet all\n        requirements specified above and required for correct calculation of\n        the probabilities to be predicted.\n        \"\"\"\n        if param_list is None:\n            return None\n\n        # Make sure there are four elements in param_list\n        check_type_and_size_of_param_list(param_list, 4)\n\n        # Make sure each element in the list is a numpy array or is None\n        check_type_of_param_list_elements(param_list)\n\n        # Make sure each array in param_list has the same number of dimensions\n        check_dimensional_equality_of_param_list_arrays(param_list)\n\n        # If using 2D arrays, ensure each array has the same number of columns.\n        if len(param_list[0].shape) == 2:\n            check_num_columns_in_param_list_arrays(param_list)\n\n        # Make sure each array has the correct number of elements\n        num_index_coefs = len(self.ind_var_names)\n\n        check_num_rows_of_parameter_array(param_list[0],\n                                          num_index_coefs,\n                                          'param_list[0]')\n\n        if param_list[1] is not None:\n            num_intercepts = (0 if self.intercept_names is None else\n                              len(self.intercept_names))\n\n            check_num_rows_of_parameter_array(param_list[1],\n                                              num_intercepts,\n                                              'param_list[1]')\n\n        if param_list[2] is not None:\n            num_shapes = (0 if self.shape_names is None else\n                          len(self.shape_names))\n\n            check_num_rows_of_parameter_array(param_list[2],\n                                              num_shapes,\n                                              'param_list[2]')\n\n        if param_list[3] is not None:\n            num_nests = (0 if self.nest_names is None else\n                         len(self.nest_names))\n\n            check_num_rows_of_parameter_array(param_list[3],\n                                              num_nests,\n                                              'param_list[3]')\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_pickle(self, filepath):\n        if not isinstance(filepath, str):\n            raise ValueError(\"filepath must be a string.\")\n\n        if not filepath.endswith(\".pkl\"):\n            filepath = filepath + \".pkl\"\n\n        with open(filepath, \"wb\") as f:\n            pickle.dump(self, f)\n\n        print(\"Model saved to {}\".format(filepath))\n\n        return None", "response": "Saves the current state of the object to a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecorating a method as handling prefix tokens of the given kinds", "response": "def prefix(*kinds):\n    \"\"\"Decorate a method as handling prefix tokens of the given kinds\"\"\"\n    def wrap(fn):\n        try:\n            fn.prefix_kinds.extend(kinds)\n        except AttributeError:\n            fn.prefix_kinds = list(kinds)\n        return fn\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecorates a method as handling infix tokens of the given kinds", "response": "def infix(*kinds):\n    \"\"\"Decorate a method as handling infix tokens of the given kinds\"\"\"\n    def wrap(fn):\n        try:\n            fn.infix_kinds.extend(kinds)\n        except AttributeError:\n            fn.infix_kinds = list(kinds)\n        return fn\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef attempt(self, *kinds):\n        if self._error:\n            raise self._error\n        token = self.next_token\n        if not token:\n            return None\n        if kinds and token.kind not in kinds:\n            return None\n        self._advance()\n        return token", "response": "Try to get the next token if it matches one of the kinds given. If no kinds are given return None."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the next token raising an exception if it doesn t match one of the given kinds.", "response": "def require(self, *kinds):\n        \"\"\"Get the next token, raising an exception if it doesn't match one of\n        the given kinds, or the input ends. If no kinds are given, returns the\n        next token of any kind.\"\"\"\n        token = self.attempt()\n        if not token:\n            raise SyntaxError('Unexpected end of input')\n        if kinds and token.kind not in kinds:\n            raise SyntaxError.unexpected(token, kinds)\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a mutable local symbol table with the seeded local symbols.", "response": "def local_symbol_table(imports=None, symbols=()):\n    \"\"\"Constructs a local symbol table.\n\n    Args:\n        imports (Optional[SymbolTable]): Shared symbol tables to import.\n        symbols (Optional[Iterable[Unicode]]): Initial local symbols to add.\n\n    Returns:\n        SymbolTable: A mutable local symbol table with the seeded local symbols.\n    \"\"\"\n    return SymbolTable(\n        table_type=LOCAL_TABLE_TYPE,\n        symbols=symbols,\n        imports=imports\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a shared symbol table.", "response": "def shared_symbol_table(name, version, symbols, imports=None):\n    \"\"\"Constructs a shared symbol table.\n\n    Args:\n        name (unicode): The name of the shared symbol table.\n        version (int): The version of the shared symbol table.\n        symbols (Iterable[unicode]): The symbols to associate with the table.\n        imports (Optional[Iterable[SymbolTable]): The shared symbol tables to inject into this one.\n\n    Returns:\n        SymbolTable: The constructed table.\n    \"\"\"\n    return SymbolTable(\n        table_type=SHARED_TABLE_TYPE,\n        symbols=symbols,\n        name=name,\n        version=version,\n        imports=imports\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef placeholder_symbol_table(name, version, max_id):\n    if version <= 0:\n        raise ValueError('Version must be grater than or equal to 1: %s' % version)\n    if max_id < 0:\n        raise ValueError('Max ID must be zero or positive: %s' % max_id)\n\n    return SymbolTable(\n        table_type=SHARED_TABLE_TYPE,\n        symbols=repeat(None, max_id),\n        name=name,\n        version=version,\n        is_substitute=True\n    )", "response": "Constructs a shared symbol table that consists symbols that all have no known text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef substitute_symbol_table(table, version, max_id):\n    if not table.table_type.is_shared:\n        raise ValueError('Symbol table to substitute from must be a shared table')\n    if version <= 0:\n        raise ValueError('Version must be grater than or equal to 1: %s' % version)\n    if max_id < 0:\n        raise ValueError('Max ID must be zero or positive: %s' % max_id)\n\n    # TODO Recycle the symbol tokens from the source table into the substitute.\n    if max_id <= table.max_id:\n        symbols = (token.text for token in islice(table, max_id))\n    else:\n        symbols = chain(\n            (token.text for token in table),\n            repeat(None, max_id - table.max_id)\n        )\n\n    return SymbolTable(\n        table_type=SHARED_TABLE_TYPE,\n        symbols=symbols,\n        name=table.name,\n        version=version,\n        is_substitute=True\n    )", "response": "Substitute a given shared symbol table for another version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __add(self, token):\n        self.__symbols.append(token)\n        text = token.text\n        if text is not None and text not in self.__mapping:\n            self.__mapping[text] = token", "response": "Unconditionally adds a token to the table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a token normalizing the SID and import reference to this table.", "response": "def __add_shared(self, original_token):\n        \"\"\"Adds a token, normalizing the SID and import reference to this table.\"\"\"\n        sid = self.__new_sid()\n        token = SymbolToken(original_token.text, sid, self.__import_location(sid))\n        self.__add(token)\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a token normalizing only the SID", "response": "def __add_import(self, original_token):\n        \"\"\"Adds a token, normalizing only the SID\"\"\"\n        sid = self.__new_sid()\n        token = SymbolToken(original_token.text, sid, original_token.location)\n        self.__add(token)\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the given Unicode text as a locally defined symbol.", "response": "def __add_text(self, text):\n        \"\"\"Adds the given Unicode text as a locally defined symbol.\"\"\"\n        if text is not None and not isinstance(text, six.text_type):\n            raise TypeError('Local symbol definition must be a Unicode sequence or None: %r' % text)\n        sid = self.__new_sid()\n        location = None\n        if self.table_type.is_shared:\n            location = self.__import_location(sid)\n        token = SymbolToken(text, sid, location)\n        self.__add(token)\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninterns the given Unicode sequence into the symbol table.", "response": "def intern(self, text):\n        \"\"\"Interns the given Unicode sequence into the symbol table.\n\n        Note:\n            This operation is only valid on local symbol tables.\n\n        Args:\n            text (unicode): The target to intern.\n\n        Returns:\n            SymbolToken: The mapped symbol token which may already exist in the table.\n        \"\"\"\n        if self.table_type.is_shared:\n            raise TypeError('Cannot intern on shared symbol table')\n        if not isinstance(text, six.text_type):\n            raise TypeError('Cannot intern non-Unicode sequence into symbol table: %r' % text)\n\n        token = self.get(text)\n        if token is None:\n            token = self.__add_text(text)\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a token by text or local ID.", "response": "def get(self, key, default=None):\n        \"\"\"Returns a token by text or local ID, with a default.\n\n        A given text image may be associated with more than one symbol ID.  This will return the first definition.\n\n        Note:\n            User defined symbol IDs are always one-based.  Symbol zero is a special symbol that\n            always has no text.\n\n        Args:\n            key (unicode | int):  The key to lookup.\n            default(Optional[SymbolToken]): The default to return if the key is not found\n\n        Returns:\n            SymbolToken: The token associated with the key or the default if it doesn't exist.\n        \"\"\"\n        if isinstance(key, six.text_type):\n            return self.__mapping.get(key, None)\n        if not isinstance(key, int):\n            raise TypeError('Key must be int or Unicode sequence.')\n\n        # TODO determine if $0 should be returned for all symbol tables.\n        if key == 0:\n            return SYMBOL_ZERO_TOKEN\n\n        # Translate one-based SID to zero-based intern table\n        index = key - 1\n        if index < 0 or key > len(self):\n            return default\n        return self.__symbols[index]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(self, table):\n        if table.table_type.is_system:\n            raise ValueError('Cannot add system table to catalog')\n        if not table.table_type.is_shared:\n            raise ValueError('Cannot add local table to catalog')\n        if table.is_substitute:\n            raise ValueError('Cannot add substitute table to catalog')\n\n        versions = self.__tables.get(table.name)\n        if versions is None:\n            versions = {}\n            self.__tables[table.name] = versions\n        versions[table.version] = table", "response": "Adds a shared table to the catalog."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve the symbol table for a given name and version.", "response": "def resolve(self, name, version, max_id):\n        \"\"\"Resolves the table for a given name and version.\n\n        Args:\n            name (unicode): The name of the table to resolve.\n            version (int): The version of the table to resolve.\n            max_id (Optional[int]): The maximum ID of the table requested.\n                May be ``None`` in which case an exact match on ``name`` and ``version``\n                is required.\n\n        Returns:\n            SymbolTable: The *closest* matching symbol table.  This is either an exact match,\n            a placeholder, or a derived substitute depending on what tables are registered.\n        \"\"\"\n        if not isinstance(name, six.text_type):\n            raise TypeError('Name must be a Unicode sequence: %r' % name)\n        if not isinstance(version, int):\n            raise TypeError('Version must be an int: %r' % version)\n        if version <= 0:\n            raise ValueError('Version must be positive: %s' % version)\n        if max_id is not None and max_id < 0:\n            raise ValueError('Max ID must be zero or positive: %s' % max_id)\n\n        versions = self.__tables.get(name)\n        if versions is None:\n            if max_id is None:\n                raise CannotSubstituteTable(\n                    'Found no table for %s, but no max_id' % name\n                )\n            return placeholder_symbol_table(name, version, max_id)\n\n        table = versions.get(version)\n        if table is None:\n            # TODO Replace the keys map with a search tree based dictionary.\n            keys = list(versions)\n            keys.sort()\n            table = versions[keys[-1]]\n\n        if table.version == version and (max_id is None or table.max_id == max_id):\n            return table\n\n        if max_id is None:\n            raise CannotSubstituteTable(\n                'Found match for %s, but not version %d, and no max_id' % (name, version)\n            )\n\n        return substitute_symbol_table(table, version, max_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a node to the tree that represents the start of a container.", "response": "def start_container(self):\n        \"\"\"Add a node to the tree that represents the start of a container.\n\n        Until end_container is called, any nodes added through add_scalar_value\n        or start_container will be children of this new node.\n        \"\"\"\n        self.__container_lengths.append(self.current_container_length)\n        self.current_container_length = 0\n        new_container_node = _Node()\n        self.__container_node.add_child(new_container_node)\n        self.__container_nodes.append(self.__container_node)\n        self.__container_node = new_container_node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef end_container(self, header_buf):\n        if not self.__container_nodes:\n            raise ValueError(\"Attempted to end container with none active.\")\n        # Header needs to be the first node visited on this subtree.\n        self.__container_node.add_leaf(_Node(header_buf))\n        self.__container_node = self.__container_nodes.pop()\n        parent_container_length = self.__container_lengths.pop()\n        self.current_container_length = \\\n            parent_container_length + self.current_container_length + len(header_buf)", "response": "Add a node containing the container s header to the current subtree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a node to the tree containing a scalar value.", "response": "def add_scalar_value(self, value_buf):\n        \"\"\"Add a node to the tree containing a scalar value.\n\n        Args:\n            value_buf (bytearray): bytearray containing the scalar value.\n        \"\"\"\n        self.__container_node.add_child(_Node(value_buf))\n        self.current_container_length += len(value_buf)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwalking the BufferTree and reset it when finished.", "response": "def drain(self):\n        \"\"\"Walk the BufferTree and reset it when finished.\n\n        Yields:\n            any: The current node's value.\n        \"\"\"\n        if self.__container_nodes:\n            raise ValueError(\"Attempted to drain without ending all containers.\")\n        for buf in self.__depth_traverse(self.__root):\n            if buf is not None:\n                yield buf\n        self.__reset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests two objects for equivalence under the Ion data model.", "response": "def ion_equals(a, b, timestamps_instants_only=False):\n    \"\"\"Tests two objects for equivalence under the Ion data model.\n\n    There are three important cases:\n        * When neither operand specifies its `ion_type` or `annotations`, this method will only return True when the\n          values of both operands are equivalent under the Ion data model.\n        * When only one of the operands specifies its `ion_type` and `annotations`, this method will only return True\n          when that operand has no annotations and has a value equivalent to the other operand under the Ion data model.\n        * When both operands specify `ion_type` and `annotations`, this method will only return True when the ion_type\n          and annotations of both are the same and their values are equivalent under the Ion data model.\n\n    Note that the order of the operands does not matter.\n\n    Args:\n        a (object): The first operand.\n        b (object): The second operand.\n        timestamps_instants_only (Optional[bool]): False if timestamp objects (datetime and its subclasses) should be\n            compared according to the Ion data model (where the instant, precision, and offset must be equal); True\n            if these objects should be considered equivalent if they simply represent the same instant.\n    \"\"\"\n    if timestamps_instants_only:\n        return _ion_equals_timestamps_instants(a, b)\n    return _ion_equals_timestamps_data_model(a, b)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomparing a and b according to the description of the ion_equals method.", "response": "def _ion_equals(a, b, timestamp_comparison_func, recursive_comparison_func):\n    \"\"\"Compares a and b according to the description of the ion_equals method.\"\"\"\n    for a, b in ((a, b), (b, a)):  # Ensures that operand order does not matter.\n        if isinstance(a, _IonNature):\n            if isinstance(b, _IonNature):\n                # Both operands have _IonNature. Their IonTypes and annotations must be equivalent.\n                eq = a.ion_type is b.ion_type and _annotations_eq(a, b)\n            else:\n                # Only one operand has _IonNature. It cannot be equivalent to the other operand if it has annotations.\n                eq = not a.ion_annotations\n            if eq:\n                if isinstance(a, IonPyList):\n                    return _sequences_eq(a, b, recursive_comparison_func)\n                elif isinstance(a, IonPyDict):\n                    return _structs_eq(a, b, recursive_comparison_func)\n                elif isinstance(a, IonPyTimestamp):\n                    return timestamp_comparison_func(a, b)\n                elif isinstance(a, IonPyNull):\n                    return isinstance(b, IonPyNull) or (b is None and a.ion_type is IonType.NULL)\n                elif isinstance(a, IonPySymbol) or (isinstance(a, IonPyText) and a.ion_type is IonType.SYMBOL):\n                    return _symbols_eq(a, b)\n                elif isinstance(a, IonPyDecimal):\n                    return _decimals_eq(a, b)\n                elif isinstance(a, IonPyFloat):\n                    return _floats_eq(a, b)\n                else:\n                    return a == b\n            return False\n    # Reaching this point means that neither operand has _IonNature.\n    for a, b in ((a, b), (b, a)):  # Ensures that operand order does not matter.\n        if isinstance(a, list):\n            return _sequences_eq(a, b, recursive_comparison_func)\n        elif isinstance(a, dict):\n            return _structs_eq(a, b, recursive_comparison_func)\n        elif isinstance(a, datetime):\n            return timestamp_comparison_func(a, b)\n        elif isinstance(a, SymbolToken):\n            return _symbols_eq(a, b)\n        elif isinstance(a, Decimal):\n            return _decimals_eq(a, b)\n        elif isinstance(a, float):\n            return _floats_eq(a, b)\n    return a == b"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _timestamps_eq(a, b):\n    assert isinstance(a, datetime)\n    if not isinstance(b, datetime):\n        return False\n    # Local offsets must be equivalent.\n    if (a.tzinfo is None) ^ (b.tzinfo is None):\n        return False\n    if a.utcoffset() != b.utcoffset():\n        return False\n    for a, b in ((a, b), (b, a)):\n        if isinstance(a, Timestamp):\n            if isinstance(b, Timestamp):\n                # Both operands declare their precisions. They are only equivalent if their precisions are the same.\n                if a.precision is b.precision and a.fractional_precision is b.fractional_precision:\n                    break\n                return False\n            elif a.precision is not TimestampPrecision.SECOND or a.fractional_precision != MICROSECOND_PRECISION:\n                # Only one of the operands declares its precision. It is only equivalent to the other (a naive datetime)\n                # if it has full microseconds precision.\n                return False\n    return a == b", "response": "Compares two Timestamp operands for equivalence under the Ion data model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares two timestamp operands for point - in - time equivalence only.", "response": "def _timestamp_instants_eq(a, b):\n    \"\"\"Compares two timestamp operands for point-in-time equivalence only.\"\"\"\n    assert isinstance(a, datetime)\n    if not isinstance(b, datetime):\n        return False\n    # datetime's __eq__ can't compare a None offset and a non-None offset. For these equivalence semantics, a None\n    # offset (unknown local offset) is treated equivalently to a +00:00.\n    if a.tzinfo is None:\n        a = a.replace(tzinfo=OffsetTZInfo())\n    if b.tzinfo is None:\n        b = b.replace(tzinfo=OffsetTZInfo())\n    # datetime's __eq__ implementation compares instants; offsets and precision need not be equal.\n    return a == b"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a VarInt or VarUInt field from a file - like object.", "response": "def _parse_var_int_components(buf, signed):\n    \"\"\"Parses a ``VarInt`` or ``VarUInt`` field from a file-like object.\"\"\"\n    value = 0\n    sign = 1\n    while True:\n        ch = buf.read(1)\n        if ch == '':\n            raise IonException('Variable integer under-run')\n        octet = ord(ch)\n        if signed:\n            if octet & _VAR_INT_SIGN_MASK:\n                sign = -1\n            value = octet & _VAR_INT_SIGN_VALUE_MASK\n            signed = False\n        else:\n            value <<= _VAR_INT_VALUE_BITS\n            value |= octet & _VAR_INT_VALUE_MASK\n\n        if octet & _VAR_INT_SIGNAL_MASK:\n            break\n    return sign, value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_signed_int_components(buf):\n    sign_bit = 0\n    value = 0\n\n    first = True\n    while True:\n        ch = buf.read(1)\n        if ch == b'':\n            break\n        octet = ord(ch)\n        if first:\n            if octet & _SIGNED_INT_SIGN_MASK:\n                sign_bit = 1\n            value = octet & _SIGNED_INT_SIGN_VALUE_MASK\n            first = False\n        else:\n            value <<= 8\n            value |= octet\n\n    return sign_bit, value", "response": "Parses the remainder of a file - like object as a signed magnitude value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_decimal(buf):\n    exponent = _parse_var_int(buf, signed=True)\n    sign_bit, coefficient = _parse_signed_int_components(buf)\n\n    if coefficient == 0:\n        # Handle the zero cases--especially negative zero\n        value = Decimal((sign_bit, (0,), exponent))\n    else:\n        coefficient *= sign_bit and -1 or 1\n        value = Decimal(coefficient).scaleb(exponent)\n\n    return value", "response": "Parses the remainder of a file - like object as a decimal."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the given bytes data as a list of SymbolToken s.", "response": "def _parse_sid_iter(data):\n    \"\"\"Parses the given :class:`bytes` data as a list of :class:`SymbolToken`\"\"\"\n    limit = len(data)\n    buf = BytesIO(data)\n    while buf.tell() < limit:\n        sid = _parse_var_int(buf, signed=False)\n        yield SymbolToken(None, sid)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_delegate_handler(delegate):\n    @coroutine\n    def handler(*args):\n        yield\n        yield delegate.send(Transition(args, delegate))\n\n    return handler", "response": "Creates a handler function that returns a co - routine that can yield once with the given arguments to the given co - routine."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a co-routine for retrieving data up to a requested size. Args: length (int): The minimum length requested. whence (Coroutine): The co-routine to return to after the data is satisfied. ctx (_HandlerContext): The context for the read. skip (Optional[bool]): Whether the requested number of bytes should be skipped. stream_event (Optional[IonEvent]): The stream event to return if no bytes are read or available.", "response": "def _read_data_handler(length, whence, ctx, skip=False, stream_event=ION_STREAM_INCOMPLETE_EVENT):\n    \"\"\"Creates a co-routine for retrieving data up to a requested size.\n\n    Args:\n        length (int): The minimum length requested.\n        whence (Coroutine): The co-routine to return to after the data is satisfied.\n        ctx (_HandlerContext): The context for the read.\n        skip (Optional[bool]): Whether the requested number of bytes should be skipped.\n        stream_event (Optional[IonEvent]): The stream event to return if no bytes are read or\n            available.\n    \"\"\"\n    trans = None\n    queue = ctx.queue\n\n    if length > ctx.remaining:\n        raise IonException('Length overrun: %d bytes, %d remaining' % (length, ctx.remaining))\n\n    # Make sure to check the queue first.\n    queue_len = len(queue)\n    if queue_len > 0:\n        # Any data available means we can only be incomplete.\n        stream_event = ION_STREAM_INCOMPLETE_EVENT\n    length -= queue_len\n\n    if skip:\n        # For skipping we need to consume any remnant in the buffer queue.\n        if length >= 0:\n            queue.skip(queue_len)\n        else:\n            queue.skip(queue_len + length)\n\n    while True:\n        data_event, self = (yield trans)\n        if data_event is not None and data_event.data is not None:\n            data = data_event.data\n            data_len = len(data)\n            if data_len > 0:\n                # We got something so we can only be incomplete.\n                stream_event = ION_STREAM_INCOMPLETE_EVENT\n            length -= data_len\n            if not skip:\n                queue.extend(data)\n            else:\n                pos_adjustment = data_len\n                if length < 0:\n                    pos_adjustment += length\n                    # More data than we need to skip, so make sure to accumulate that remnant.\n                    queue.extend(data[length:])\n                queue.position += pos_adjustment\n        if length <= 0:\n            # We got all the data we need, go back immediately\n            yield Transition(None, whence)\n\n        trans = Transition(stream_event, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles scalars and thunks.", "response": "def _length_scalar_handler(scalar_factory, ion_type, length, ctx):\n    \"\"\"Handles scalars, ``scalar_factory`` is a function that returns a value or thunk.\"\"\"\n    _, self = yield\n    if length == 0:\n        data = b''\n    else:\n        yield ctx.read_data_transition(length, self)\n        data = ctx.queue.read(length)\n\n    scalar = scalar_factory(data)\n    event_cls = IonEvent\n    if callable(scalar):\n        # TODO Wrap the exception to get context position.\n        event_cls = IonThunkEvent\n    yield ctx.event_transition(event_cls, IonEventType.SCALAR, ion_type, scalar)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling annotations. ``ion_type`` is ignored.", "response": "def _annotation_handler(ion_type, length, ctx):\n    \"\"\"Handles annotations.  ``ion_type`` is ignored.\"\"\"\n    _, self = yield\n    self_handler = _create_delegate_handler(self)\n\n    if ctx.annotations is not None:\n        raise IonException('Annotation cannot be nested in annotations')\n\n    # We have to replace our context for annotations specifically to encapsulate the limit\n    ctx = ctx.derive_container_context(length, add_depth=0)\n    # Immediately read the length field and the annotations\n    (ann_length, _), _ = yield ctx.immediate_transition(\n        _var_uint_field_handler(self_handler, ctx)\n    )\n\n    if ann_length < 1:\n        raise IonException('Invalid annotation length subfield; annotation wrapper must have at least one annotation.')\n\n    # Read/parse the annotations.\n    yield ctx.read_data_transition(ann_length, self)\n    ann_data = ctx.queue.read(ann_length)\n    annotations = tuple(_parse_sid_iter(ann_data))\n\n    if ctx.limit - ctx.queue.position < 1:\n        # There is no space left for the 'value' subfield, which is required.\n        raise IonException('Incorrect annotation wrapper length.')\n\n    # Go parse the start of the value but go back to the real parent container.\n    yield ctx.immediate_transition(\n        _start_type_handler(ctx.field_name, ctx.whence, ctx, annotations=annotations)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ordered_struct_start_handler(handler, ctx):\n    _, self = yield\n    self_handler = _create_delegate_handler(self)\n    (length, _), _ = yield ctx.immediate_transition(\n        _var_uint_field_handler(self_handler, ctx)\n    )\n    if length < 2:\n        # A valid field name/value pair is at least two octets: one for the field name SID and one for the value.\n        raise IonException('Ordered structs (type ID 0xD1) must have at least one field name/value pair.')\n    yield ctx.immediate_transition(handler(length, ctx))", "response": "Handles the special case of ordered structs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle container start events.", "response": "def _container_start_handler(ion_type, length, ctx):\n    \"\"\"Handles container delegation.\"\"\"\n    _, self = yield\n\n    container_ctx = ctx.derive_container_context(length)\n    if ctx.annotations and ctx.limit != container_ctx.limit:\n        # 'ctx' is the annotation wrapper context. `container_ctx` represents the wrapper's 'value' subfield. Their\n        # limits must match.\n        raise IonException('Incorrect annotation wrapper length.')\n    delegate = _container_handler(ion_type, container_ctx)\n\n    # We start the container, and transition to the new container processor.\n    yield ctx.event_transition(\n        IonEvent, IonEventType.CONTAINER_START, ion_type, value=None, whence=delegate\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a generator that yields the content of a container.", "response": "def _container_handler(ion_type, ctx):\n    \"\"\"Handler for the body of a container (or the top-level stream).\n\n    Args:\n        ion_type (Optional[IonType]): The type of the container or ``None`` for the top-level.\n        ctx (_HandlerContext): The context for the container.\n    \"\"\"\n    transition = None\n    first = True\n    at_top = ctx.depth == 0\n    while True:\n        data_event, self = (yield transition)\n        if data_event is not None and data_event.type is ReadEventType.SKIP:\n            yield ctx.read_data_transition(ctx.remaining, self, skip=True)\n\n        if ctx.queue.position == ctx.limit:\n            # We are at the end of the container.\n            # Yield the close event and go to enclosing container.\n            yield Transition(\n                IonEvent(IonEventType.CONTAINER_END, ion_type, depth=ctx.depth-1),\n                ctx.whence\n            )\n\n        if ion_type is IonType.STRUCT:\n            # Read the field name.\n            self_handler = _create_delegate_handler(self)\n            (field_sid, _), _ = yield ctx.immediate_transition(\n                _var_uint_field_handler(self_handler, ctx)\n            )\n            field_name = SymbolToken(None, field_sid)\n        else:\n            field_name = None\n\n        expects_ivm = first and at_top\n        transition = ctx.immediate_transition(\n            _start_type_handler(field_name, self, ctx, expects_ivm, at_top=at_top)\n        )\n        first = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bind_length_handlers(tids, user_handler, lns):\n    for tid in tids:\n        for ln in lns:\n            type_octet = _gen_type_octet(tid, ln)\n            ion_type = _TID_VALUE_TYPE_TABLE[tid]\n            if ln == 1 and ion_type is IonType.STRUCT:\n                handler = partial(_ordered_struct_start_handler, partial(user_handler, ion_type))\n            elif ln < _LENGTH_FIELD_FOLLOWS:\n                # Directly partially bind length.\n                handler = partial(user_handler, ion_type, ln)\n            else:\n                # Delegate to length field parsing first.\n                handler = partial(_var_uint_field_handler, partial(user_handler, ion_type))\n            _HANDLER_DISPATCH_TABLE[type_octet] = handler", "response": "Binds a set of handlers to the given factory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbind a set of scalar handlers for an inclusive range of low - nibble values.", "response": "def _bind_length_scalar_handlers(tids, scalar_factory, lns=_NON_ZERO_LENGTH_LNS):\n    \"\"\"Binds a set of scalar handlers for an inclusive range of low-nibble values.\n\n    Args:\n        tids (Sequence[int]): The Type IDs to bind to.\n        scalar_factory (Callable): The factory for the scalar parsing function.\n            This function can itself return a function representing a thunk to defer the\n            scalar parsing or a direct value.\n        lns (Sequence[int]): The low-nibble lengths to bind to.\n    \"\"\"\n    handler = partial(_length_scalar_handler, scalar_factory)\n    return _bind_length_handlers(tids, handler, lns)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a raw binary reader co - routine.", "response": "def raw_reader(queue=None):\n    \"\"\"Returns a raw binary reader co-routine.\n\n    Args:\n        queue (Optional[BufferQueue]): The buffer read data for parsing, if ``None`` a\n            new one will be created.\n\n    Yields:\n        IonEvent: parse events, will have an event type of ``INCOMPLETE`` if data is needed\n            in the middle of a value or ``STREAM_END`` if there is no data **and** the parser\n            is not in the middle of parsing a value.\n\n            Receives :class:`DataEvent`, with :class:`ReadEventType` of ``NEXT`` or ``SKIP``\n            to iterate over values, or ``DATA`` if the last event was a ``INCOMPLETE``\n            or ``STREAM_END`` event type.\n\n            ``SKIP`` is only allowed within a container. A reader is *in* a container\n            when the ``CONTAINER_START`` event type is encountered and *not in* a container\n            when the ``CONTAINER_END`` event type for that container is encountered.\n    \"\"\"\n    if queue is None:\n        queue = BufferQueue()\n    ctx = _HandlerContext(\n        position=0,\n        limit=None,\n        queue=queue,\n        field_name=None,\n        annotations=None,\n        depth=0,\n        whence=None\n    )\n\n    return reader_trampoline(_container_handler(None, ctx))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine how many bytes are remaining in the current context.", "response": "def remaining(self):\n        \"\"\"Determines how many bytes are remaining in the current context.\"\"\"\n        if self.depth == 0:\n            return _STREAM_REMAINING\n        return self.limit - self.queue.position"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an immediate event_transition to read a specified number of bytes.", "response": "def read_data_transition(self, length, whence=None,\n                             skip=False, stream_event=ION_STREAM_INCOMPLETE_EVENT):\n        \"\"\"Returns an immediate event_transition to read a specified number of bytes.\"\"\"\n        if whence is None:\n            whence = self.whence\n\n        return Transition(\n            None, _read_data_handler(length, whence, self, skip, stream_event)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an ion event event_transition that yields to another co - routine.", "response": "def event_transition(self, event_cls, event_type,\n                         ion_type=None, value=None, annotations=None, depth=None, whence=None):\n        \"\"\"Returns an ion event event_transition that yields to another co-routine.\n\n        If ``annotations`` is not specified, then the ``annotations`` are the annotations of this\n        context.\n        If ``depth`` is not specified, then the ``depth`` is depth of this context.\n        If ``whence`` is not specified, then ``whence`` is the whence of this context.\n        \"\"\"\n        if annotations is None:\n            annotations = self.annotations\n        if annotations is None:\n            annotations = ()\n        if not (event_type is IonEventType.CONTAINER_START) and \\\n                annotations and (self.limit - self.queue.position) != 0:\n            # This value is contained in an annotation wrapper, from which its limit was inherited. It must have\n            # reached, but not surpassed, that limit.\n            raise IonException('Incorrect annotation wrapper length.')\n\n        if depth is None:\n            depth = self.depth\n\n        if whence is None:\n            whence = self.whence\n\n        return Transition(\n            event_cls(event_type, ion_type, value, self.field_name, annotations, depth),\n            whence\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize obj as an Ion - formatted stream to fp.", "response": "def dump(obj, fp, imports=None, binary=True, sequence_as_stream=False, skipkeys=False, ensure_ascii=True,\n         check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None,\n         use_decimal=True, namedtuple_as_object=True, tuple_as_array=True, bigint_as_string=False, sort_keys=False,\n         item_sort_key=None, for_json=None, ignore_nan=False, int_as_string_bitcount=None, iterable_as_array=False,\n         **kw):\n    \"\"\"Serialize ``obj`` as an Ion-formatted stream to ``fp`` (a file-like object), using the following conversion\n    table::\n        +-------------------+-------------------+\n        |  Python           |       Ion         |\n        |-------------------+-------------------|\n        | None              |    null.null      |\n        |-------------------+-------------------|\n        | IonPyNull(<type>) |    null.<type>    |\n        |-------------------+-------------------|\n        | True, False,      |                   |\n        | IonPyInt(BOOL),   |     bool          |\n        | IonPyBool,        |                   |\n        |-------------------+-------------------|\n        | int (Python 2, 3) |                   |\n        | long (Python 2),  |      int          |\n        | IonPyInt(INT)     |                   |\n        |-------------------+-------------------|\n        | float, IonPyFloat |     float         |\n        |-------------------+-------------------|\n        | Decimal,          |                   |\n        | IonPyDecimal      |     decimal       |\n        |-------------------+-------------------|\n        | datetime,         |                   |\n        | Timestamp,        |    timestamp      |\n        | IonPyTimestamp    |                   |\n        |-------------------+-------------------|\n        | SymbolToken,      |                   |\n        | IonPySymbol,      |     symbol        |\n        | IonPyText(SYMBOL) |                   |\n        |-------------------+-------------------|\n        | str (Python 3),   |                   |\n        | unicode (Python2),|     string        |\n        | IonPyText(STRING) |                   |\n        |-------------------+-------------------|\n        | IonPyBytes(CLOB)  |     clob          |\n        |-------------------+-------------------|\n        | str (Python 2),   |                   |\n        | bytes (Python 3)  |     blob          |\n        | IonPyBytes(BLOB)  |                   |\n        |-------------------+-------------------|\n        | list, tuple,      |                   |\n        | IonPyList(LIST)   |     list          |\n        |-------------------+-------------------|\n        | IonPyList(SEXP)   |     sexp          |\n        |-------------------+-------------------|\n        | dict, namedtuple, |                   |\n        | IonPyDict         |     struct        |\n        +-------------------+-------------------+\n\n    Args:\n        obj (Any): A python object to serialize according to the above table. Any Python object which is neither an\n            instance of nor inherits from one of the types in the above table will raise TypeError.\n        fp (BaseIO): A file-like object.\n        imports (Optional[Sequence[SymbolTable]]): A sequence of shared symbol tables to be used by by the writer.\n        binary (Optional[True|False]): When True, outputs binary Ion. When false, outputs text Ion.\n        sequence_as_stream (Optional[True|False]): When True, if ``obj`` is a sequence, it will be treated as a stream\n            of top-level Ion values (i.e. the resulting Ion data will begin with ``obj``'s first element).\n            Default: False.\n        skipkeys: NOT IMPLEMENTED\n        ensure_ascii: NOT IMPLEMENTED\n        check_circular: NOT IMPLEMENTED\n        allow_nan: NOT IMPLEMENTED\n        cls: NOT IMPLEMENTED\n        indent (Str): If binary is False and indent is a string, then members of containers will be pretty-printed with\n            a newline followed by that string repeated for each level of nesting. None (the default) selects the most\n            compact representation without any newlines. Example: to indent with four spaces per level of nesting,\n            use ``'    '``.\n        separators: NOT IMPLEMENTED\n        encoding: NOT IMPLEMENTED\n        default: NOT IMPLEMENTED\n        use_decimal: NOT IMPLEMENTED\n        namedtuple_as_object: NOT IMPLEMENTED\n        tuple_as_array: NOT IMPLEMENTED\n        bigint_as_string: NOT IMPLEMENTED\n        sort_keys: NOT IMPLEMENTED\n        item_sort_key: NOT IMPLEMENTED\n        for_json: NOT IMPLEMENTED\n        ignore_nan: NOT IMPLEMENTED\n        int_as_string_bitcount: NOT IMPLEMENTED\n        iterable_as_array: NOT IMPLEMENTED\n        **kw: NOT IMPLEMENTED\n\n    \"\"\"\n\n    raw_writer = binary_writer(imports) if binary else text_writer(indent=indent)\n    writer = blocking_writer(raw_writer, fp)\n    writer.send(ION_VERSION_MARKER_EVENT)  # The IVM is emitted automatically in binary; it's optional in text.\n    if sequence_as_stream and isinstance(obj, (list, tuple)):\n        # Treat this top-level sequence as a stream; serialize its elements as top-level values, but don't serialize the\n        # sequence itself.\n        for top_level in obj:\n            _dump(top_level, writer)\n    else:\n        _dump(obj, writer)\n    writer.send(ION_STREAM_END_EVENT)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dumps(obj, imports=None, binary=True, sequence_as_stream=False, skipkeys=False, ensure_ascii=True, check_circular=True,\n          allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, use_decimal=True,\n          namedtuple_as_object=True, tuple_as_array=True, bigint_as_string=False, sort_keys=False, item_sort_key=None,\n          for_json=None, ignore_nan=False, int_as_string_bitcount=None, iterable_as_array=False, **kw):\n    \"\"\"Serialize ``obj`` as Python ``string`` or ``bytes`` object, using the conversion table used by ``dump`` (above).\n\n    Args:\n        obj (Any): A python object to serialize according to the above table. Any Python object which is neither an\n            instance of nor inherits from one of the types in the above table will raise TypeError.\n        imports (Optional[Sequence[SymbolTable]]): A sequence of shared symbol tables to be used by by the writer.\n        binary (Optional[True|False]): When True, outputs binary Ion. When false, outputs text Ion.\n        sequence_as_stream (Optional[True|False]): When True, if ``obj`` is a sequence, it will be treated as a stream\n            of top-level Ion values (i.e. the resulting Ion data will begin with ``obj``'s first element).\n            Default: False.\n        skipkeys: NOT IMPLEMENTED\n        ensure_ascii: NOT IMPLEMENTED\n        check_circular: NOT IMPLEMENTED\n        allow_nan: NOT IMPLEMENTED\n        cls: NOT IMPLEMENTED\n        indent (Str): If binary is False and indent is a string, then members of containers will be pretty-printed with\n            a newline followed by that string repeated for each level of nesting. None (the default) selects the most\n            compact representation without any newlines. Example: to indent with four spaces per level of nesting,\n            use ``'    '``.\n        separators: NOT IMPLEMENTED\n        encoding: NOT IMPLEMENTED\n        default: NOT IMPLEMENTED\n        use_decimal: NOT IMPLEMENTED\n        namedtuple_as_object: NOT IMPLEMENTED\n        tuple_as_array: NOT IMPLEMENTED\n        bigint_as_string: NOT IMPLEMENTED\n        sort_keys: NOT IMPLEMENTED\n        item_sort_key: NOT IMPLEMENTED\n        for_json: NOT IMPLEMENTED\n        ignore_nan: NOT IMPLEMENTED\n        int_as_string_bitcount: NOT IMPLEMENTED\n        iterable_as_array: NOT IMPLEMENTED\n        **kw: NOT IMPLEMENTED\n\n    Returns:\n        Union[str|bytes]: The string or binary representation of the data.  if ``binary=True``, this will be a\n            ``bytes`` object, otherwise this will be a ``str`` object (or ``unicode`` in the case of Python 2.x)\n    \"\"\"\n    ion_buffer = six.BytesIO()\n\n    dump(obj, ion_buffer, sequence_as_stream=sequence_as_stream, binary=binary, skipkeys=skipkeys, ensure_ascii=ensure_ascii, check_circular=check_circular,\n         allow_nan=allow_nan, cls=cls, indent=indent, separators=separators, encoding=encoding, default=default,\n         use_decimal=use_decimal, namedtuple_as_object=namedtuple_as_object, tuple_as_array=tuple_as_array,\n         bigint_as_string=bigint_as_string, sort_keys=sort_keys, item_sort_key=item_sort_key, for_json=for_json,\n         ignore_nan=ignore_nan, int_as_string_bitcount=int_as_string_bitcount, iterable_as_array=iterable_as_array)\n\n    ret_val = ion_buffer.getvalue()\n    ion_buffer.close()\n    if not binary:\n        ret_val = ret_val.decode('utf-8')\n    return ret_val", "response": "Serialize obj to a string or bytes object using the conversion table used by the writer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(ion_str, catalog=None, single_value=True, encoding='utf-8', cls=None, object_hook=None, parse_float=None,\n          parse_int=None, parse_constant=None, object_pairs_hook=None, use_decimal=None, **kw):\n    \"\"\"Deserialize ``ion_str``, which is a string representation of an Ion object, to a Python object using the\n    conversion table used by load (above).\n\n    Args:\n        fp (str): A string representation of Ion data.\n        catalog (Optional[SymbolTableCatalog]): The catalog to use for resolving symbol table imports.\n        single_value (Optional[True|False]): When True, the data in ``ion_str`` is interpreted as a single Ion value,\n            and will be returned without an enclosing container. If True and there are multiple top-level values in\n            the Ion stream, IonException will be raised. NOTE: this means that when data is dumped using\n            ``sequence_as_stream=True``, it must be loaded using ``single_value=False``. Default: True.\n        encoding: NOT IMPLEMENTED\n        cls: NOT IMPLEMENTED\n        object_hook: NOT IMPLEMENTED\n        parse_float: NOT IMPLEMENTED\n        parse_int: NOT IMPLEMENTED\n        parse_constant: NOT IMPLEMENTED\n        object_pairs_hook: NOT IMPLEMENTED\n        use_decimal: NOT IMPLEMENTED\n        **kw: NOT IMPLEMENTED\n\n    Returns (Any):\n        if single_value is True:\n            A Python object representing a single Ion value.\n        else:\n            A sequence of Python objects representing a stream of Ion values.\n    \"\"\"\n\n    if isinstance(ion_str, six.binary_type):\n        ion_buffer = BytesIO(ion_str)\n    elif isinstance(ion_str, six.text_type):\n        ion_buffer = six.StringIO(ion_str)\n    else:\n        raise TypeError('Unsupported text: %r' % ion_str)\n\n    return load(ion_buffer, catalog=catalog, single_value=single_value, encoding=encoding, cls=cls,\n                object_hook=object_hook, parse_float=parse_float, parse_int=parse_int, parse_constant=parse_constant,\n                object_pairs_hook=object_pairs_hook, use_decimal=use_decimal)", "response": "Deserialize an Ion string into a Python object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the unicode character representing the given code point.", "response": "def _narrow_unichr(code_point):\n    \"\"\"Retrieves the unicode character representing any given code point, in a way that won't break on narrow builds.\n\n    This is necessary because the built-in unichr function will fail for ordinals above 0xFFFF on narrow builds (UCS2);\n    ordinals above 0xFFFF would require recalculating and combining surrogate pairs. This avoids that by retrieving the\n    unicode character that was initially read.\n\n    Args:\n        code_point (int|CodePoint): An int or a subclass of int that contains the unicode character representing its\n            code point in an attribute named 'char'.\n    \"\"\"\n    try:\n        if len(code_point.char) > 1:\n            return code_point.char\n    except AttributeError:\n        pass\n    return six.unichr(code_point)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprovide a co - routine trampoline for a given logical version of a state machine.", "response": "def reader_trampoline(start, allow_flush=False):\n    \"\"\"Provides the co-routine trampoline for a reader state machine.\n\n    The given co-routine is a state machine that yields :class:`Transition` and takes\n    a Transition of :class:`amazon.ion.core.DataEvent` and the co-routine itself.\n\n    A reader must start with a ``ReadEventType.NEXT`` event to prime the parser.  In many cases\n    this will lead to an ``IonEventType.INCOMPLETE`` being yielded, but not always\n    (consider a reader over an in-memory data structure).\n\n    Notes:\n        A reader delimits its incomplete parse points with ``IonEventType.INCOMPLETE``.\n        Readers also delimit complete parse points with ``IonEventType.STREAM_END``;\n        this is similar to the ``INCOMPLETE`` case except that it denotes that a logical\n        termination of data is *allowed*. When these event are received, the only valid\n        input event type is a ``ReadEventType.DATA``.\n\n        Generally, ``ReadEventType.NEXT`` is used to get the next parse event, but\n        ``ReadEventType.SKIP`` can be used to skip over the current container.\n\n        An internal state machine co-routine can delimit a state change without yielding\n        to the caller by yielding ``None`` event, this will cause the trampoline to invoke\n        the transition delegate, immediately.\n    Args:\n        start: The reader co-routine to initially delegate to.\n        allow_flush(Optional[bool]): True if this reader supports receiving ``NEXT`` after\n            yielding ``INCOMPLETE`` to trigger an attempt to flush pending parse events,\n            otherwise False.\n\n    Yields:\n        amazon.ion.core.IonEvent: the result of parsing.\n\n        Receives :class:`DataEvent` to parse into :class:`amazon.ion.core.IonEvent`.\n    \"\"\"\n    data_event = yield\n    if data_event is None or data_event.type is not ReadEventType.NEXT:\n        raise TypeError('Reader must be started with NEXT')\n    trans = Transition(None, start)\n    while True:\n        trans = trans.delegate.send(Transition(data_event, trans.delegate))\n        data_event = None\n        if trans.event is not None:\n            # Only yield if there is an event.\n            data_event = (yield trans.event)\n            if trans.event.event_type.is_stream_signal:\n                if data_event.type is not ReadEventType.DATA:\n                    if not allow_flush or not (trans.event.event_type is IonEventType.INCOMPLETE and\n                                               data_event.type is ReadEventType.NEXT):\n                        raise TypeError('Reader expected data: %r' % (data_event,))\n            else:\n                if data_event.type is ReadEventType.DATA:\n                    raise TypeError('Reader did not expect data')\n            if data_event.type is ReadEventType.DATA and len(data_event.data) == 0:\n                raise ValueError('Empty data not allowed')\n            if trans.event.depth == 0 \\\n                    and trans.event.event_type is not IonEventType.CONTAINER_START \\\n                    and data_event.type is ReadEventType.SKIP:\n                raise TypeError('Cannot skip at the top-level')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide an implementation of using the reader co - routine with a file - like object.", "response": "def blocking_reader(reader, input, buffer_size=_DEFAULT_BUFFER_SIZE):\n    \"\"\"Provides an implementation of using the reader co-routine with a file-like object.\n\n    Args:\n        reader(Coroutine): A reader co-routine.\n        input(BaseIO): The file-like object to read from.\n        buffer_size(Optional[int]): The optional buffer size to use.\n    \"\"\"\n    ion_event = None\n    while True:\n        read_event = (yield ion_event)\n        ion_event = reader.send(read_event)\n        while ion_event is not None and ion_event.event_type.is_stream_signal:\n            data = input.read(buffer_size)\n            if len(data) == 0:\n                # End of file.\n                if ion_event.event_type is IonEventType.INCOMPLETE:\n                    ion_event = reader.send(NEXT_EVENT)\n                    continue\n                else:\n                    yield ION_STREAM_END_EVENT\n                    return\n            ion_event = reader.send(read_data_event(data))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, length, skip=False):\n        if length > self.__size:\n            raise IndexError(\n                'Cannot pop %d bytes, %d bytes in buffer queue' % (length, self.__size))\n        self.position += length\n        self.__size -= length\n        segments = self.__segments\n        offset = self.__offset\n\n        data = self.__data_cls()\n        while length > 0:\n            segment = segments[0]\n            segment_off = offset\n            segment_len = len(segment)\n            segment_rem = segment_len - segment_off\n            segment_read_len = min(segment_rem, length)\n\n            if segment_off == 0 and segment_read_len == segment_rem:\n                # consume an entire segment\n                if skip:\n                    segment_slice = self.__element_type()\n                else:\n                    segment_slice = segment\n            else:\n                # Consume a part of the segment.\n                if skip:\n                    segment_slice = self.__element_type()\n                else:\n                    segment_slice = segment[segment_off:segment_off + segment_read_len]\n                offset = 0\n            segment_off += segment_read_len\n            if segment_off == segment_len:\n                segments.popleft()\n                self.__offset = 0\n            else:\n                self.__offset = segment_off\n\n            if length <= segment_rem and len(data) == 0:\n                return segment_slice\n            data.extend(segment_slice)\n            length -= segment_read_len\n        if self.is_unicode:\n            return data.as_text()\n        else:\n            return data", "response": "Consumes the first length bytes from the accumulator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef skip(self, length):\n        if length >= self.__size:\n            skip_amount = self.__size\n            rem = length - skip_amount\n            self.__segments.clear()\n            self.__offset = 0\n            self.__size = 0\n            self.position += skip_amount\n        else:\n            rem = 0\n            self.read(length, skip=True)\n        return rem", "response": "Removes length bytes and returns the number still required to skip"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef managed_reader(reader, catalog=None):\n    if catalog is None:\n        catalog = SymbolTableCatalog()\n\n    ctx = _ManagedContext(catalog)\n    symbol_trans = Transition(None, None)\n    ion_event = None\n    while True:\n        if symbol_trans.delegate is not None \\\n                and ion_event is not None \\\n                and not ion_event.event_type.is_stream_signal:\n            # We have a symbol processor active, do not yield to user.\n            delegate = symbol_trans.delegate\n            symbol_trans = delegate.send(Transition(ion_event, delegate))\n            if symbol_trans.delegate is None:\n                # When the symbol processor terminates, the event is the context\n                # and there is no delegate.\n                ctx = symbol_trans.event\n                data_event = NEXT_EVENT\n            else:\n                data_event = symbol_trans.event\n        else:\n            data_event = None\n\n            if ion_event is not None:\n                event_type = ion_event.event_type\n                ion_type = ion_event.ion_type\n                depth = ion_event.depth\n\n                # System values only happen at the top-level\n                if depth == 0:\n                    if event_type is IonEventType.VERSION_MARKER:\n                        if ion_event != ION_VERSION_MARKER_EVENT:\n                            raise IonException('Invalid IVM: %s' % (ion_event,))\n\n                        # Reset and swallow IVM\n                        ctx = _ManagedContext(ctx.catalog)\n                        data_event = NEXT_EVENT\n\n                    elif ion_type is IonType.SYMBOL \\\n                            and len(ion_event.annotations) == 0 \\\n                            and ion_event.value is not None \\\n                            and ctx.resolve(ion_event.value).text == TEXT_ION_1_0:\n                        assert symbol_trans.delegate is None\n\n                        # A faux IVM is a NOP\n                        data_event = NEXT_EVENT\n\n                    elif event_type is IonEventType.CONTAINER_START \\\n                            and ion_type is IonType.STRUCT \\\n                            and ctx.has_symbol_table_annotation(ion_event.annotations):\n                        assert symbol_trans.delegate is None\n\n                        # Activate a new symbol processor.\n                        delegate = _local_symbol_table_handler(ctx)\n                        symbol_trans = Transition(None, delegate)\n                        data_event = NEXT_EVENT\n\n            if data_event is None:\n                # No system processing or we have to get data, yield control.\n                if ion_event is not None:\n                    ion_event = _managed_thunk_event(ctx, ion_event)\n                data_event = yield ion_event\n\n        ion_event = reader.send(data_event)", "response": "A non - blocking reader that yields events from the underlying reader."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nresolve the given token against the current table.", "response": "def resolve(self, token):\n        \"\"\"Attempts to resolve the :class:`SymbolToken` against the current table.\n\n        If the ``text`` is not None, the token is returned, otherwise, a token\n        in the table is attempted to be retrieved.  If not token is found, then\n        this method will raise.\n        \"\"\"\n        if token.text is not None:\n            return token\n        resolved_token = self.symbol_table.get(token.sid, None)\n        if resolved_token is None:\n            raise IonException('Out of range SID: %d' % token.sid)\n        return resolved_token"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nraise an IonException upon encountering the given illegal character in the given context.", "response": "def _illegal_character(c, ctx, message=''):\n    \"\"\"Raises an IonException upon encountering the given illegal character in the given context.\n\n    Args:\n        c (int|None): Ordinal of the illegal character.\n        ctx (_HandlerContext):  Context in which the illegal character was encountered.\n        message (Optional[str]): Additional information, as necessary.\n\n    \"\"\"\n    container_type = ctx.container.ion_type is None and 'top-level' or ctx.container.ion_type.name\n    value_type = ctx.ion_type is None and 'unknown' or ctx.ion_type.name\n    if c is None:\n        header = 'Illegal token'\n    else:\n        c = 'EOF' if BufferQueue.is_eof(c) else _chr(c)\n        header = 'Illegal character %s' % (c,)\n    raise IonException('%s at position %d in %s value contained in %s. %s Pending value: %s'\n                       % (header, ctx.queue.position, value_type, container_type, message, ctx.value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps the given dictionary such that the given fallback function will be called when a nonexistent key is accessed.", "response": "def _defaultdict(dct, fallback=_illegal_character):\n    \"\"\"Wraps the given dictionary such that the given fallback function will be called when a nonexistent key is\n    accessed.\n    \"\"\"\n    out = defaultdict(lambda: fallback)\n    for k, v in six.iteritems(dct):\n        out[k] = v\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges a sequence of dictionaries and tuples into a single dictionary.", "response": "def _merge_mappings(*args):\n    \"\"\"Merges a sequence of dictionaries and/or tuples into a single dictionary.\n\n    If a given argument is a tuple, it must have two elements, the first of which is a sequence of keys and the second\n    of which is a single value, which will be mapped to from each of the keys in the sequence.\n    \"\"\"\n    dct = {}\n    for arg in args:\n        if isinstance(arg, dict):\n            merge = arg\n        else:\n            assert isinstance(arg, tuple)\n            keys, value = arg\n            merge = dict(zip(keys, [value]*len(keys)))\n        dct.update(merge)\n    return dct"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the input to a SymbolToken suitable for being emitted as part of an IonEvent.", "response": "def _as_symbol(value, is_symbol_value=True):\n    \"\"\"Converts the input to a :class:`SymbolToken` suitable for being emitted as part of a :class:`IonEvent`.\n\n    If the input has an `as_symbol` method (e.g. :class:`CodePointArray`), it will be converted using that method.\n    Otherwise, it must already be a `SymbolToken`. In this case, there is nothing to do unless the input token is not a\n    symbol value and it is an :class:`_IVMToken`. This requires the `_IVMToken` to be converted to a regular\n    `SymbolToken`.\n    \"\"\"\n    try:\n        return value.as_symbol()\n    except AttributeError:\n        assert isinstance(value, SymbolToken)\n    if not is_symbol_value:\n        try:\n            # This converts _IVMTokens to regular SymbolTokens when the _IVMToken cannot represent an IVM (i.e.\n            # it is a field name or annotation).\n            return value.regular_token()\n        except AttributeError:\n            pass\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _number_negative_start_handler(c, ctx):\n    assert c == _MINUS\n    assert len(ctx.value) == 0\n    ctx.set_ion_type(IonType.INT)\n    ctx.value.append(c)\n    c, _ = yield\n    yield ctx.immediate_transition(_NEGATIVE_TABLE[c](c, ctx))", "response": "Handles numeric values that start with a negative sign."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle numeric values that start with zero or negative zero. Branches to delegate co - routines according to _ZERO_START_TABLE.", "response": "def _number_zero_start_handler(c, ctx):\n    \"\"\"Handles numeric values that start with zero or negative zero. Branches to delegate co-routines according to\n    _ZERO_START_TABLE.\n    \"\"\"\n    assert c == _ZERO\n    assert len(ctx.value) == 0 or (len(ctx.value) == 1 and ctx.value[0] == _MINUS)\n    ctx.set_ion_type(IonType.INT)\n    ctx.value.append(c)\n    c, _ = yield\n    if _ends_value(c):\n        trans = ctx.event_transition(IonThunkEvent, IonEventType.SCALAR, ctx.ion_type, _parse_decimal_int(ctx.value))\n        if c == _SLASH:\n            trans = ctx.immediate_transition(_number_slash_end_handler(c, ctx, trans))\n        yield trans\n    yield ctx.immediate_transition(_ZERO_START_TABLE[c](c, ctx))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling numeric values that start with digits 1 - 9 and start with digits 1 - 9 and end with digits 1 - 9.", "response": "def _number_or_timestamp_handler(c, ctx):\n    \"\"\"Handles numeric values that start with digits 1-9. May terminate a value, in which case that value is an\n    int. If it does not terminate a value, it branches to delegate co-routines according to _NUMBER_OR_TIMESTAMP_TABLE.\n    \"\"\"\n    assert c in _DIGITS\n    ctx.set_ion_type(IonType.INT)  # If this is the last digit read, this value is an Int.\n    val = ctx.value\n    val.append(c)\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    while True:\n        if _ends_value(c):\n            trans = ctx.event_transition(IonThunkEvent, IonEventType.SCALAR,\n                                         ctx.ion_type, _parse_decimal_int(ctx.value))\n            if c == _SLASH:\n                trans = ctx.immediate_transition(_number_slash_end_handler(c, ctx, trans))\n        else:\n            if c not in _DIGITS:\n                trans = ctx.immediate_transition(_NUMBER_OR_TIMESTAMP_TABLE[c](c, ctx))\n            else:\n                val.append(c)\n        c, _ = yield trans"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _number_slash_end_handler(c, ctx, event):\n    assert c == _SLASH\n    c, self = yield\n    next_ctx = ctx.derive_child_context(ctx.whence)\n    comment = _comment_handler(_SLASH, next_ctx, next_ctx.whence)\n    comment.send((c, comment))\n    # If the previous line returns without error, it's a valid comment and the number may be emitted.\n    yield _CompositeTransition(event, ctx, comment, next_ctx, initialize_handler=False)", "response": "Handles numeric values that end in a forward slash."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _numeric_handler_factory(charset, transition, assertion, illegal_before_underscore, parse_func,\n                             illegal_at_end=(None,), ion_type=None, append_first_if_not=None, first_char=None):\n    \"\"\"Generates a handler co-routine which tokenizes a numeric component (a token or sub-token).\n\n    Args:\n        charset (sequence): Set of ordinals of legal characters for this numeric component.\n        transition (callable): Called upon termination of this component (i.e. when a character not in ``charset`` is\n            found). Accepts the previous character ordinal, the current character ordinal, the current context, and the\n            previous transition. Returns a Transition if the component ends legally; otherwise, raises an error.\n        assertion (callable): Accepts the first character's ordinal and the current context. Returns True if this is\n            a legal start to the component.\n        illegal_before_underscore (sequence): Set of ordinals of illegal characters to precede an underscore for this\n            component.\n        parse_func (callable): Called upon ending the numeric value. Accepts the current token value and returns a\n            thunk that lazily parses the token.\n        illegal_at_end (Optional[sequence]): Set of ordinals of characters that may not legally end the value.\n        ion_type (Optional[IonType]): The type of the value if it were to end on this component.\n        append_first_if_not (Optional[int]): The ordinal of a character that should not be appended to the token if\n            it occurs first in this component (e.g. an underscore in many cases).\n        first_char (Optional[int]): The ordinal of the character that should be appended instead of the character that\n            occurs first in this component. This is useful for preparing the token for parsing in the case where a\n            particular character is peculiar to the Ion format (e.g. 'd' to denote the exponent of a decimal value\n            should be replaced with 'e' for compatibility with python's Decimal type).\n    \"\"\"\n    @coroutine\n    def numeric_handler(c, ctx):\n        assert assertion(c, ctx)\n        if ion_type is not None:\n            ctx.set_ion_type(ion_type)\n        val = ctx.value\n        if c != append_first_if_not:\n            first = c if first_char is None else first_char\n            val.append(first)\n        prev = c\n        c, self = yield\n        trans = ctx.immediate_transition(self)\n        while True:\n            if _ends_value(c):\n                if prev == _UNDERSCORE or prev in illegal_at_end:\n                    _illegal_character(c, ctx, '%s at end of number.' % (_chr(prev),))\n                trans = ctx.event_transition(IonThunkEvent, IonEventType.SCALAR, ctx.ion_type, parse_func(ctx.value))\n                if c == _SLASH:\n                    trans = ctx.immediate_transition(_number_slash_end_handler(c, ctx, trans))\n            else:\n                if c == _UNDERSCORE:\n                    if prev == _UNDERSCORE or prev in illegal_before_underscore:\n                        _illegal_character(c, ctx, 'Underscore after %s.' % (_chr(prev),))\n                else:\n                    if c not in charset:\n                        trans = transition(prev, c, ctx, trans)\n                    else:\n                        val.append(c)\n            prev = c\n            c, _ = yield trans\n    return numeric_handler", "response": "Generates a handler co - routine which tokenizes a numeric value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a handler co - routine which tokenizes an exponent of a numeric value.", "response": "def _exponent_handler_factory(ion_type, exp_chars, parse_func, first_char=None):\n    \"\"\"Generates a handler co-routine which tokenizes an numeric exponent.\n\n    Args:\n        ion_type (IonType): The type of the value with this exponent.\n        exp_chars (sequence): The set of ordinals of the legal exponent characters for this component.\n        parse_func (callable): Called upon ending the numeric value. Accepts the current token value and returns a\n            thunk that lazily parses the token.\n        first_char (Optional[int]): The ordinal of the character that should be appended instead of the character that\n            occurs first in this component. This is useful for preparing the token for parsing in the case where a\n            particular character is peculiar to the Ion format (e.g. 'd' to denote the exponent of a decimal value\n            should be replaced with 'e' for compatibility with python's Decimal type).\n    \"\"\"\n    def transition(prev, c, ctx, trans):\n        if c in _SIGN and prev in exp_chars:\n            ctx.value.append(c)\n        else:\n            _illegal_character(c, ctx)\n        return trans\n    illegal = exp_chars + _SIGN\n    return _numeric_handler_factory(_DIGITS, transition, lambda c, ctx: c in exp_chars, illegal, parse_func,\n                                    illegal_at_end=illegal, ion_type=ion_type, first_char=first_char)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _coefficient_handler_factory(trans_table, parse_func, assertion=lambda c, ctx: True,\n                                 ion_type=None, append_first_if_not=None):\n    \"\"\"Generates a handler co-routine which tokenizes a numeric coefficient.\n\n    Args:\n        trans_table (dict): lookup table for the handler for the next component of this numeric token, given the\n            ordinal of the first character in that component.\n        parse_func (callable): Called upon ending the numeric value. Accepts the current token value and returns a\n            thunk that lazily parses the token.\n        assertion (callable): Accepts the first character's ordinal and the current context. Returns True if this is\n            a legal start to the component.\n        ion_type (Optional[IonType]): The type of the value if it were to end on this coefficient.\n        append_first_if_not (Optional[int]): The ordinal of a character that should not be appended to the token if\n            it occurs first in this component (e.g. an underscore in many cases).\n    \"\"\"\n    def transition(prev, c, ctx, trans):\n        if prev == _UNDERSCORE:\n            _illegal_character(c, ctx, 'Underscore before %s.' % (_chr(c),))\n        return ctx.immediate_transition(trans_table[c](c, ctx))\n    return _numeric_handler_factory(_DIGITS, transition, assertion, (_DOT,), parse_func,\n                                    ion_type=ion_type, append_first_if_not=append_first_if_not)", "response": "Generates a handler co - routine which tokenizes a numeric coefficient."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a handler co - routine which tokenizes an integer of a particular radix.", "response": "def _radix_int_handler_factory(radix_indicators, charset, parse_func):\n    \"\"\"Generates a handler co-routine which tokenizes a integer of a particular radix.\n\n    Args:\n        radix_indicators (sequence): The set of ordinals of characters that indicate the radix of this int.\n        charset (sequence): Set of ordinals of legal characters for this radix.\n        parse_func (callable): Called upon ending the numeric value. Accepts the current token value and returns a\n            thunk that lazily parses the token.\n    \"\"\"\n    def assertion(c, ctx):\n        return c in radix_indicators and \\\n               ((len(ctx.value) == 1 and ctx.value[0] == _ZERO) or\n                (len(ctx.value) == 2 and ctx.value[0] == _MINUS and ctx.value[1] == _ZERO)) and \\\n               ctx.ion_type == IonType.INT\n    return _numeric_handler_factory(charset, lambda prev, c, ctx, trans: _illegal_character(c, ctx),\n                                    assertion, radix_indicators, parse_func, illegal_at_end=radix_indicators)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _timestamp_zero_start_handler(c, ctx):\n    val = ctx.value\n    ctx.set_ion_type(IonType.TIMESTAMP)\n    if val[0] == _MINUS:\n        _illegal_character(c, ctx, 'Negative year not allowed.')\n    val.append(c)\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    while True:\n        if c in _TIMESTAMP_YEAR_DELIMITERS:\n            trans = ctx.immediate_transition(_timestamp_handler(c, ctx))\n        elif c in _DIGITS:\n            val.append(c)\n        else:\n            _illegal_character(c, ctx)\n        c, _ = yield trans", "response": "Handles numeric values that start with a zero followed by another digit."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_timestamp(tokens):\n    def parse():\n        precision = TimestampPrecision.YEAR\n        off_hour = tokens[_TimestampState.OFF_HOUR]\n        off_minutes = tokens[_TimestampState.OFF_MINUTE]\n        microsecond = None\n        fraction_digits = None\n        if off_hour is not None:\n            assert off_minutes is not None\n            off_sign = -1 if _MINUS in off_hour else 1\n            off_hour = int(off_hour)\n            off_minutes = int(off_minutes) * off_sign\n            if off_sign == -1 and off_hour == 0 and off_minutes == 0:\n                # -00:00 (unknown UTC offset) is a naive datetime.\n                off_hour = None\n                off_minutes = None\n        else:\n            assert off_minutes is None\n\n        year = tokens[_TimestampState.YEAR]\n        assert year is not None\n        year = int(year)\n\n        month = tokens[_TimestampState.MONTH]\n        if month is None:\n            month = 1\n        else:\n            month = int(month)\n            precision = TimestampPrecision.MONTH\n\n        day = tokens[_TimestampState.DAY]\n        if day is None:\n            day = 1\n        else:\n            day = int(day)\n            precision = TimestampPrecision.DAY\n\n        hour = tokens[_TimestampState.HOUR]\n        minute = tokens[_TimestampState.MINUTE]\n        if hour is None:\n            assert minute is None\n            hour = 0\n            minute = 0\n        else:\n            assert minute is not None\n            hour = int(hour)\n            minute = int(minute)\n            precision = TimestampPrecision.MINUTE\n\n        second = tokens[_TimestampState.SECOND]\n        if second is None:\n            second = 0\n        else:\n            second = int(second)\n            precision = TimestampPrecision.SECOND\n\n            fraction = tokens[_TimestampState.FRACTIONAL]\n            if fraction is not None:\n                fraction_digits = len(fraction)\n                if fraction_digits > MICROSECOND_PRECISION:\n                    for digit in fraction[MICROSECOND_PRECISION:]:\n                        if digit != _ZERO:\n                            raise ValueError('Only six significant digits supported in timestamp fractional. Found %s.'\n                                             % (fraction,))\n                    fraction_digits = MICROSECOND_PRECISION\n                    fraction = fraction[0:MICROSECOND_PRECISION]\n                else:\n                    fraction.extend(_ZEROS[MICROSECOND_PRECISION - fraction_digits])\n                microsecond = int(fraction)\n        return timestamp(\n            year, month, day,\n            hour, minute, second, microsecond,\n            off_hour, off_minutes,\n            precision=precision, fractional_precision=fraction_digits\n        )\n    return parse", "response": "Parses the given _TimestampTokens and returns a Timestamp object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _timestamp_handler(c, ctx):\n    assert c in _TIMESTAMP_YEAR_DELIMITERS\n    ctx.set_ion_type(IonType.TIMESTAMP)\n    if len(ctx.value) != 4:\n        _illegal_character(c, ctx, 'Timestamp year is %d digits; expected 4.' % (len(ctx.value),))\n    prev = c\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    state = _TimestampState.YEAR\n    nxt = _DIGITS\n    tokens = _TimestampTokens(ctx.value)\n    val = None\n    can_terminate = False\n    if prev == _T:\n        nxt += _VALUE_TERMINATORS\n        can_terminate = True\n    while True:\n        is_eof = can_terminate and BufferQueue.is_eof(c)\n        if c not in nxt and not is_eof:\n            _illegal_character(c, ctx, 'Expected %r in state %r.' % ([_chr(x) for x in nxt], state))\n        if c in _VALUE_TERMINATORS or is_eof:\n            if not can_terminate:\n                _illegal_character(c, ctx, 'Unexpected termination of timestamp.')\n            trans = ctx.event_transition(IonThunkEvent, IonEventType.SCALAR, ctx.ion_type, _parse_timestamp(tokens))\n            if c == _SLASH:\n                trans = ctx.immediate_transition(_number_slash_end_handler(c, ctx, trans))\n        else:\n            can_terminate = False\n            if c == _Z:\n                # Z implies UTC, i.e. +00:00 local offset.\n                tokens.transition(_TimestampState.OFF_HOUR).append(_ZERO)\n                tokens.transition(_TimestampState.OFF_MINUTE).append(_ZERO)\n                nxt = _VALUE_TERMINATORS\n                can_terminate = True\n            elif c == _T:\n                nxt = _VALUE_TERMINATORS + _DIGITS\n                can_terminate = True\n            elif c in _TIMESTAMP_DELIMITERS:\n                nxt = _DIGITS\n            elif c in _DIGITS:\n                if prev == _PLUS or (state > _TimestampState.MONTH and prev == _HYPHEN):\n                    state = _TimestampState.OFF_HOUR\n                    val = tokens.transition(state)\n                    if prev == _HYPHEN:\n                        val.append(prev)\n                elif prev in (_TIMESTAMP_DELIMITERS + (_T,)):\n                    state = _TimestampState[state + 1]\n                    val = tokens.transition(state)\n                    if state == _TimestampState.FRACTIONAL:\n                        nxt = _DIGITS + _TIMESTAMP_OFFSET_INDICATORS\n                elif prev in _DIGITS:\n                    if state == _TimestampState.MONTH:\n                        nxt = _TIMESTAMP_YEAR_DELIMITERS\n                    elif state == _TimestampState.DAY:\n                        nxt = (_T,) + _VALUE_TERMINATORS\n                        can_terminate = True\n                    elif state == _TimestampState.HOUR:\n                        nxt = (_COLON,)\n                    elif state == _TimestampState.MINUTE:\n                        nxt = _TIMESTAMP_OFFSET_INDICATORS + (_COLON,)\n                    elif state == _TimestampState.SECOND:\n                        nxt = _TIMESTAMP_OFFSET_INDICATORS + (_DOT,)\n                    elif state == _TimestampState.FRACTIONAL:\n                        nxt = _DIGITS + _TIMESTAMP_OFFSET_INDICATORS\n                    elif state == _TimestampState.OFF_HOUR:\n                        nxt = (_COLON,)\n                    elif state == _TimestampState.OFF_MINUTE:\n                        nxt = _VALUE_TERMINATORS\n                        can_terminate = True\n                    else:\n                        raise ValueError('Unknown timestamp state %r.' % (state,))\n                else:\n                    # Reaching this branch would be indicative of a programming error within this state machine.\n                    raise ValueError('Digit following %s in timestamp state %r.' % (_chr(prev), state))\n                val.append(c)\n        prev = c\n        c, _ = yield trans", "response": "Handles timestamp values. Entered after the year component has been completed ; tokenizes the remaining\n    components and yields a new tree of IonTypes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle comments. Upon completion of the comment immediately transitions back to whence.", "response": "def _comment_handler(c, ctx, whence):\n    \"\"\"Handles comments. Upon completion of the comment, immediately transitions back to `whence`.\"\"\"\n    assert c == _SLASH\n    c, self = yield\n    if c == _SLASH:\n        ctx.set_line_comment()\n        block_comment = False\n    elif c == _ASTERISK:\n        if ctx.line_comment:\n            # This happens when a block comment immediately follows a line comment.\n            ctx.set_line_comment(False)\n        block_comment = True\n    else:\n        _illegal_character(c, ctx, 'Illegal character sequence \"/%s\".' % (_chr(c),))\n    done = False\n    prev = None\n    trans = ctx.immediate_transition(self)\n    while not done:\n        c, _ = yield trans\n        if block_comment:\n            if prev == _ASTERISK and c == _SLASH:\n                done = True\n            prev = c\n        else:\n            if c in _NEWLINES or BufferQueue.is_eof(c):\n                done = True\n    yield ctx.set_self_delimiting(True).immediate_transition(whence)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sexp_slash_handler(c, ctx, whence=None, pending_event=None):\n    assert c == _SLASH\n    if whence is None:\n        whence = ctx.whence\n    c, self = yield\n    ctx.queue.unread(c)\n    if c == _ASTERISK or c == _SLASH:\n        yield ctx.immediate_transition(_comment_handler(_SLASH, ctx, whence))\n    else:\n        if pending_event is not None:\n            # Since this is the start of a new value and not a comment, the pending event must be emitted.\n            assert pending_event.event is not None\n            yield _CompositeTransition(pending_event, ctx, partial(_operator_symbol_handler, _SLASH))\n        yield ctx.immediate_transition(_operator_symbol_handler(_SLASH, ctx))", "response": "Handles the special case of a forward - slash within an s - expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles triple - quoted strings. Remains active until a value other than a long string is encountered.", "response": "def _long_string_handler(c, ctx, is_field_name=False):\n    \"\"\"Handles triple-quoted strings. Remains active until a value other than a long string is encountered.\"\"\"\n    assert c == _SINGLE_QUOTE\n    is_clob = ctx.ion_type is IonType.CLOB\n    max_char = _MAX_CLOB_CHAR if is_clob else _MAX_TEXT_CHAR\n    assert not (is_clob and is_field_name)\n    if not is_clob and not is_field_name:\n        ctx.set_ion_type(IonType.STRING)\n    assert not ctx.value\n    ctx.set_unicode(quoted_text=True)\n    val = ctx.value\n    if is_field_name:\n        assert not val\n        ctx.set_pending_symbol()\n        val = ctx.pending_symbol\n    quotes = 0\n    in_data = True\n    c, self = yield\n    here = ctx.immediate_transition(self)\n    trans = here\n    while True:\n        if c == _SINGLE_QUOTE and not _is_escaped(c):\n            quotes += 1\n            if quotes == 3:\n                in_data = not in_data\n                ctx.set_quoted_text(in_data)\n                quotes = 0\n        else:\n            if in_data:\n                _validate_long_string_text(c, ctx, max_char)\n                # Any quotes found in the meantime are part of the data\n                val.extend(_SINGLE_QUOTES[quotes])\n                if not _is_escaped_newline(c):\n                    val.append(c)\n                quotes = 0\n            else:\n                if quotes > 0:\n                    assert quotes < 3\n                    if is_field_name or is_clob:\n                        # There are at least two values here, which is illegal for field names or within clobs.\n                        _illegal_character(c, ctx, 'Malformed triple-quoted text: %s' % (val,))\n                    else:\n                        # This string value is followed by a quoted symbol.\n                        if ctx.container.is_delimited:\n                            _illegal_character(c, ctx, 'Delimiter %s not found after value.'\n                                               % (_chr(ctx.container.delimiter[0]),))\n                        trans = ctx.event_transition(IonEvent, IonEventType.SCALAR, ctx.ion_type, ctx.value.as_text())\n                        if quotes == 1:\n                            if BufferQueue.is_eof(c):\n                                _illegal_character(c, ctx, \"Unexpected EOF.\")\n                            # c was read as a single byte. Re-read it as a code point.\n                            ctx.queue.unread(c)\n                            ctx.set_quoted_text(True)\n                            c, _ = yield ctx.immediate_transition(self)\n                            trans = _CompositeTransition(\n                                trans,\n                                ctx,\n                                partial(_quoted_symbol_handler, c, is_field_name=False),\n                            )\n                        else:  # quotes == 2\n                            trans = _CompositeTransition(trans, ctx, None, ctx.set_empty_symbol())\n                elif c not in _WHITESPACE:\n                    if is_clob:\n                        trans = ctx.immediate_transition(_clob_end_handler(c, ctx))\n                    elif c == _SLASH:\n                        if ctx.container.ion_type is IonType.SEXP:\n                            pending = ctx.event_transition(IonEvent, IonEventType.SCALAR,\n                                                           ctx.ion_type, ctx.value.as_text())\n                            trans = ctx.immediate_transition(_sexp_slash_handler(c, ctx, self, pending))\n                        else:\n                            trans = ctx.immediate_transition(_comment_handler(c, ctx, self))\n                    elif is_field_name:\n                        if c != _COLON:\n                            _illegal_character(c, ctx, 'Illegal character after field name %s.' % (val,))\n                        trans = ctx.immediate_transition(ctx.whence)\n                    else:\n                        trans = ctx.event_transition(IonEvent, IonEventType.SCALAR, ctx.ion_type, ctx.value.as_text())\n        c, _ = yield trans\n        ctx.set_self_delimiting(False)  # If comments separated long string components, this would have been set.\n        trans = here"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle typed null values. Entered once null has been found.", "response": "def _typed_null_handler(c, ctx):\n    \"\"\"Handles typed null values. Entered once `null.` has been found.\"\"\"\n    assert c == _DOT\n    c, self = yield\n    nxt = _NULL_STARTS\n    i = 0\n    length = None\n    done = False\n    trans = ctx.immediate_transition(self)\n    while True:\n        if done:\n            if _ends_value(c) or (ctx.container.ion_type is IonType.SEXP and c in _OPERATORS):\n                trans = ctx.event_transition(IonEvent, IonEventType.SCALAR, nxt.ion_type, None)\n            else:\n                _illegal_character(c, ctx, 'Illegal null type.')\n        elif length is None:\n            if c not in nxt:\n                _illegal_character(c, ctx, 'Illegal null type.')\n            nxt = nxt[c]\n            if isinstance(nxt, _NullSequence):\n                length = len(nxt.sequence)\n        else:\n            if c != nxt[i]:\n                _illegal_character(c, ctx, 'Illegal null type.')\n            i += 1\n            done = i == length\n        c, _ = yield trans"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the start of an unquoted text token.", "response": "def _symbol_or_keyword_handler(c, ctx, is_field_name=False):\n    \"\"\"Handles the start of an unquoted text token.\n\n    This may be an operator (if in an s-expression), an identifier symbol, or a keyword.\n    \"\"\"\n    in_sexp = ctx.container.ion_type is IonType.SEXP\n    if c not in _IDENTIFIER_STARTS:\n        if in_sexp and c in _OPERATORS:\n            c_next, _ = yield\n            ctx.queue.unread(c_next)\n            yield ctx.immediate_transition(_operator_symbol_handler(c, ctx))\n        _illegal_character(c, ctx)\n    assert not ctx.value\n    ctx.set_unicode().set_ion_type(IonType.SYMBOL)\n    val = ctx.value\n    val.append(c)\n    maybe_null = c == _N_LOWER\n    maybe_nan = maybe_null\n    maybe_true = c == _T_LOWER\n    maybe_false = c == _F_LOWER\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    keyword_trans = None\n    match_index = 0\n    while True:\n        def check_keyword(name, keyword_sequence, ion_type, value, match_transition=lambda: None):\n            maybe_keyword = True\n            transition = None\n            if match_index < len(keyword_sequence):\n                maybe_keyword = c == keyword_sequence[match_index]\n            else:\n                transition = match_transition()\n                if transition is not None:\n                    pass\n                elif _ends_value(c):\n                    if is_field_name:\n                        _illegal_character(c, ctx, '%s keyword as field name not allowed.' % (name,))\n                    transition = ctx.event_transition(IonEvent, IonEventType.SCALAR, ion_type, value)\n                elif c == _COLON:\n                    message = ''\n                    if is_field_name:\n                        message = '%s keyword as field name not allowed.' % (name,)\n                    _illegal_character(c, ctx, message)\n                elif in_sexp and c in _OPERATORS:\n                    transition = ctx.event_transition(IonEvent, IonEventType.SCALAR, ion_type, value)\n                else:\n                    maybe_keyword = False\n            return maybe_keyword, transition\n        if maybe_null:\n            def check_null_dot():\n                transition = None\n                found = c == _DOT\n                if found:\n                    if is_field_name:\n                        _illegal_character(c, ctx, \"Illegal character in field name.\")\n                    transition = ctx.immediate_transition(_typed_null_handler(c, ctx))\n                return transition\n            maybe_null, keyword_trans = check_keyword('null', _NULL_SUFFIX.sequence,\n                                                      IonType.NULL, None, check_null_dot)\n        if maybe_nan:\n            maybe_nan, keyword_trans = check_keyword('nan', _NAN_SUFFIX, IonType.FLOAT, _NAN)\n        elif maybe_true:\n            maybe_true, keyword_trans = check_keyword('true', _TRUE_SUFFIX, IonType.BOOL, True)\n        elif maybe_false:\n            maybe_false, keyword_trans = check_keyword('false', _FALSE_SUFFIX, IonType.BOOL, False)\n        if maybe_null or maybe_nan or maybe_true or maybe_false:\n            if keyword_trans is not None:\n                trans = keyword_trans\n            else:\n                val.append(c)\n                match_index += 1\n        else:\n            if c in _SYMBOL_TOKEN_TERMINATORS:\n                # This might be an annotation or a field name\n                ctx.set_pending_symbol(val)\n                trans = ctx.immediate_transition(ctx.whence)\n            elif _ends_value(c) or (in_sexp and c in _OPERATORS):\n                trans = ctx.event_transition(IonEvent, IonEventType.SCALAR, IonType.SYMBOL, val.as_symbol())\n            else:\n                trans = ctx.immediate_transition(_unquoted_symbol_handler(c, ctx, is_field_name=is_field_name))\n        c, _ = yield trans"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _inf_or_operator_handler_factory(c_start, is_delegate=True):\n    @coroutine\n    def inf_or_operator_handler(c, ctx):\n        next_ctx = None\n        if not is_delegate:\n            ctx.value.append(c_start)\n            c, self = yield\n        else:\n            assert ctx.value[0] == c_start\n            assert c not in _DIGITS\n            ctx.queue.unread(c)\n            next_ctx = ctx\n            _, self = yield\n            assert c == _\n        maybe_inf = True\n        ctx.set_ion_type(IonType.FLOAT)\n        match_index = 0\n        trans = ctx.immediate_transition(self)\n        while True:\n            if maybe_inf:\n                if match_index < len(_INF_SUFFIX):\n                    maybe_inf = c == _INF_SUFFIX[match_index]\n                else:\n                    if _ends_value(c) or (ctx.container.ion_type is IonType.SEXP and c in _OPERATORS):\n                        yield ctx.event_transition(\n                            IonEvent, IonEventType.SCALAR, IonType.FLOAT, c_start == _MINUS and _NEG_INF or _POS_INF\n                        )\n                    else:\n                        maybe_inf = False\n            if maybe_inf:\n                match_index += 1\n            else:\n                ctx.set_unicode()\n                if match_index > 0:\n                    next_ctx = ctx.derive_child_context(ctx.whence)\n                    for ch in _INF_SUFFIX[0:match_index]:\n                        next_ctx.value.append(ch)\n                break\n            c, self = yield trans\n        if ctx.container is not _C_SEXP:\n            _illegal_character(c, next_ctx is None and ctx or next_ctx,\n                               'Illegal character following %s.' % (_chr(c_start),))\n        if match_index == 0:\n            if c in _OPERATORS:\n                yield ctx.immediate_transition(_operator_symbol_handler(c, ctx))\n            yield ctx.event_transition(IonEvent, IonEventType.SCALAR, IonType.SYMBOL, ctx.value.as_symbol())\n        yield _CompositeTransition(\n            ctx.event_transition(IonEvent, IonEventType.SCALAR, IonType.SYMBOL, ctx.value.as_symbol()),\n            ctx,\n            partial(_unquoted_symbol_handler, c),\n            next_ctx\n        )\n    return inf_or_operator_handler", "response": "Generates a co - routine that yields inf or - inf."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _operator_symbol_handler(c, ctx):\n    assert c in _OPERATORS\n    ctx.set_unicode()\n    val = ctx.value\n    val.append(c)\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    while c in _OPERATORS:\n        val.append(c)\n        c, _ = yield trans\n    yield ctx.event_transition(IonEvent, IonEventType.SCALAR, IonType.SYMBOL, val.as_symbol())", "response": "Handles operator symbol values within s - expressions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a transition which ends the current symbol token.", "response": "def _symbol_token_end(c, ctx, is_field_name, value=None):\n    \"\"\"Returns a transition which ends the current symbol token.\"\"\"\n    if value is None:\n        value = ctx.value\n    if is_field_name or c in _SYMBOL_TOKEN_TERMINATORS or ctx.quoted_text:\n        # This might be an annotation or a field name. Mark it as self-delimiting because a symbol token termination\n        # character has been found.\n        ctx.set_self_delimiting(ctx.quoted_text).set_pending_symbol(value).set_quoted_text(False)\n        trans = ctx.immediate_transition(ctx.whence)\n    else:\n        trans = ctx.event_transition(IonEvent, IonEventType.SCALAR, IonType.SYMBOL, _as_symbol(value))\n    return trans"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles unquoted symbol tokens.", "response": "def _unquoted_symbol_handler(c, ctx, is_field_name=False):\n    \"\"\"Handles identifier symbol tokens. If in an s-expression, these may be followed without whitespace by\n    operators.\n    \"\"\"\n    in_sexp = ctx.container.ion_type is IonType.SEXP\n    ctx.set_unicode()\n    if c not in _IDENTIFIER_CHARACTERS:\n        if in_sexp and c in _OPERATORS:\n            c_next, _ = yield\n            ctx.queue.unread(c_next)\n            assert ctx.value\n            yield _CompositeTransition(\n                ctx.event_transition(IonEvent, IonEventType.SCALAR, IonType.SYMBOL, ctx.value.as_symbol()),\n                ctx,\n                partial(_operator_symbol_handler, c)\n            )\n        _illegal_character(c, ctx.set_ion_type(IonType.SYMBOL))\n    val = ctx.value\n    val.append(c)\n    prev = c\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    while True:\n        if c not in _WHITESPACE:\n            if prev in _WHITESPACE or _ends_value(c) or c == _COLON or (in_sexp and c in _OPERATORS):\n                break\n            if c not in _IDENTIFIER_CHARACTERS:\n                _illegal_character(c, ctx.set_ion_type(IonType.SYMBOL))\n            val.append(c)\n        prev = c\n        c, _ = yield trans\n    yield _symbol_token_end(c, ctx, is_field_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles symbol tokens that begin with a dollar sign or a regular unquoted symbol.", "response": "def _symbol_identifier_or_unquoted_symbol_handler(c, ctx, is_field_name=False):\n    \"\"\"Handles symbol tokens that begin with a dollar sign. These may end up being system symbols ($ion_*), symbol\n    identifiers ('$' DIGITS+), or regular unquoted symbols.\n    \"\"\"\n    assert c == _DOLLAR_SIGN\n    in_sexp = ctx.container.ion_type is IonType.SEXP\n    ctx.set_unicode().set_ion_type(IonType.SYMBOL)\n    val = ctx.value\n    val.append(c)\n    prev = c\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    maybe_ivm = ctx.depth == 0 and not is_field_name and not ctx.annotations\n    complete_ivm = False\n    maybe_symbol_identifier = True\n    match_index = 1\n    ivm_post_underscore = False\n    while True:\n        if c not in _WHITESPACE:\n            if prev in _WHITESPACE or _ends_value(c) or c == _COLON or (in_sexp and c in _OPERATORS):\n                break\n            maybe_symbol_identifier = maybe_symbol_identifier and c in _DIGITS\n            if maybe_ivm:\n                if match_index == len(_IVM_PREFIX):\n                    if c in _DIGITS:\n                        if ivm_post_underscore:\n                            complete_ivm = True\n                    elif c == _UNDERSCORE and not ivm_post_underscore:\n                        ivm_post_underscore = True\n                    else:\n                        maybe_ivm = False\n                        complete_ivm = False\n                else:\n                    maybe_ivm = c == _IVM_PREFIX[match_index]\n            if maybe_ivm:\n                if match_index < len(_IVM_PREFIX):\n                    match_index += 1\n            elif not maybe_symbol_identifier:\n                yield ctx.immediate_transition(_unquoted_symbol_handler(c, ctx, is_field_name))\n            val.append(c)\n        elif match_index < len(_IVM_PREFIX):\n            maybe_ivm = False\n        prev = c\n        c, _ = yield trans\n    if len(val) == 1:\n        assert val[0] == _chr(_DOLLAR_SIGN)\n    elif maybe_symbol_identifier:\n        assert not maybe_ivm\n        sid = int(val[1:])\n        val = SymbolToken(None, sid)\n    elif complete_ivm:\n        val = _IVMToken(*val.as_symbol())\n    yield _symbol_token_end(c, ctx, is_field_name, value=val)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _quoted_text_handler_factory(delimiter, assertion, before, after, append_first=True,\n                                 on_close=lambda ctx: None):\n    \"\"\"Generates handlers for quoted text tokens (either short strings or quoted symbols).\n\n    Args:\n        delimiter (int): Ordinal of the quoted text's delimiter.\n        assertion (callable): Accepts the first character's ordinal, returning True if that character is a legal\n            beginning to the token.\n        before (callable): Called upon initialization. Accepts the first character's ordinal, the current context, True\n            if the token is a field name, and True if the token is a clob; returns the token's current value and True\n            if ``on_close`` should be called upon termination of the token.\n        after (callable): Called after termination of the token. Accepts the final character's ordinal, the current\n            context, and True if the token is a field name; returns a Transition.\n        append_first (Optional[bool]): True if the first character the coroutine receives is part of the text data, and\n            should therefore be appended to the value; otherwise, False (in which case, the first character must be\n            the delimiter).\n        on_close (Optional[callable]): Called upon termination of the token (before ``after``), if ``before`` indicated\n            that ``on_close`` should be called. Accepts the current context and returns a Transition. This is useful\n            for yielding a different kind of Transition based on initialization parameters given to ``before`` (e.g.\n            string vs. clob).\n    \"\"\"\n    @coroutine\n    def quoted_text_handler(c, ctx, is_field_name=False):\n        assert assertion(c)\n\n        def append():\n            if not _is_escaped_newline(c):\n                val.append(c)\n        is_clob = ctx.ion_type is IonType.CLOB\n        max_char = _MAX_CLOB_CHAR if is_clob else _MAX_TEXT_CHAR\n        ctx.set_unicode(quoted_text=True)\n        val, event_on_close = before(c, ctx, is_field_name, is_clob)\n        if append_first:\n            append()\n        c, self = yield\n        trans = ctx.immediate_transition(self)\n        done = False\n        while not done:\n            if c == delimiter and not _is_escaped(c):\n                done = True\n                if event_on_close:\n                    trans = on_close(ctx)\n                else:\n                    break\n            else:\n                _validate_short_quoted_text(c, ctx, max_char)\n                append()\n            c, _ = yield trans\n        yield after(c, ctx, is_field_name)\n    return quoted_text_handler", "response": "Generates a handler function for a quoted text token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _short_string_handler_factory():\n    def before(c, ctx, is_field_name, is_clob):\n        assert not (is_clob and is_field_name)\n        is_string = not is_clob and not is_field_name\n        if is_string:\n            ctx.set_ion_type(IonType.STRING)\n        val = ctx.value\n        if is_field_name:\n            assert not val\n            ctx.set_pending_symbol()\n            val = ctx.pending_symbol\n        return val, is_string\n\n    def on_close(ctx):\n        ctx.set_self_delimiting(True)\n        return ctx.event_transition(IonEvent, IonEventType.SCALAR, ctx.ion_type, ctx.value.as_text())\n\n    def after(c, ctx, is_field_name):\n        ctx.set_quoted_text(False).set_self_delimiting(True)\n        return ctx.immediate_transition(\n            ctx.whence if is_field_name else _clob_end_handler(c, ctx),\n        )\n\n    return _quoted_text_handler_factory(_DOUBLE_QUOTE, lambda c: c == _DOUBLE_QUOTE, before, after, append_first=False,\n                                        on_close=on_close)", "response": "Generates the short string handler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _quoted_symbol_handler_factory():\n    def before(c, ctx, is_field_name, is_clob):\n        assert not is_clob\n        _validate_short_quoted_text(c, ctx, _MAX_TEXT_CHAR)\n        return ctx.value, False\n\n    return _quoted_text_handler_factory(\n        _SINGLE_QUOTE,\n        lambda c: (c != _SINGLE_QUOTE or _is_escaped(c)),\n        before,\n        _symbol_token_end,\n    )", "response": "Generates the handler for the quoted symbol ( single quoted )."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a handler function that returns when a single quote is found.", "response": "def _single_quote_handler_factory(on_single_quote, on_other):\n    \"\"\"Generates handlers used for classifying tokens that begin with one or more single quotes.\n\n    Args:\n        on_single_quote (callable): Called when another single quote is found. Accepts the current character's ordinal,\n            the current context, and True if the token is a field name; returns a Transition.\n        on_other (callable): Called when any character other than a single quote is found.  Accepts the current\n            character's ordinal, the current context, and True if the token is a field name; returns a Transition.\n    \"\"\"\n    @coroutine\n    def single_quote_handler(c, ctx, is_field_name=False):\n        assert c == _SINGLE_QUOTE\n        c, self = yield\n        if c == _SINGLE_QUOTE and not _is_escaped(c):\n            yield on_single_quote(c, ctx, is_field_name)\n        else:\n            ctx.set_unicode(quoted_text=True)\n            yield on_other(c, ctx, is_field_name)\n    return single_quote_handler"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _struct_or_lob_handler(c, ctx):\n    assert c == _OPEN_BRACE\n    c, self = yield\n    yield ctx.immediate_transition(_STRUCT_OR_LOB_TABLE[c](c, ctx))", "response": "Handles tokens that begin with an open brace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _lob_start_handler(c, ctx):\n    assert c == _OPEN_BRACE\n    c, self = yield\n    trans = ctx.immediate_transition(self)\n    quotes = 0\n    while True:\n        if c in _WHITESPACE:\n            if quotes > 0:\n                _illegal_character(c, ctx)\n        elif c == _DOUBLE_QUOTE:\n            if quotes > 0:\n                _illegal_character(c, ctx)\n            ctx.set_ion_type(IonType.CLOB).set_unicode(quoted_text=True)\n            yield ctx.immediate_transition(_short_string_handler(c, ctx))\n        elif c == _SINGLE_QUOTE:\n            if not quotes:\n                ctx.set_ion_type(IonType.CLOB).set_unicode(quoted_text=True)\n            quotes += 1\n            if quotes == 3:\n                yield ctx.immediate_transition(_long_string_handler(c, ctx))\n        else:\n            yield ctx.immediate_transition(_blob_end_handler(c, ctx))\n        c, _ = yield trans", "response": "Handles tokens that begin with two open braces."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a handler for the end of a lob or clob.", "response": "def _lob_end_handler_factory(ion_type, action, validate=lambda c, ctx, action_res: None):\n    \"\"\"Generates handlers for the end of blob or clob values.\n\n    Args:\n        ion_type (IonType): The type of this lob (either blob or clob).\n        action (callable): Called for each non-whitespace, non-closing brace character encountered before the end of\n            the lob. Accepts the current character's ordinal, the current context, the previous character's ordinal,\n            the result of the previous call to ``action`` (if any), and True if this is the first call to ``action``.\n            Returns any state that will be needed by subsequent calls to ``action``. For blobs, this should validate\n            the character is valid base64; for clobs, this should ensure there are no illegal characters (e.g. comments)\n            between the end of the data and the end of the clob.\n        validate (Optional[callable]): Called once the second closing brace has been found. Accepts the current\n            character's ordinal, the current context, and the result of the last call to ``action``; raises an error\n            if this is not a valid lob value.\n    \"\"\"\n    assert ion_type is IonType.BLOB or ion_type is IonType.CLOB\n\n    @coroutine\n    def lob_end_handler(c, ctx):\n        val = ctx.value\n        prev = c\n        action_res = None\n        if c != _CLOSE_BRACE and c not in _WHITESPACE:\n            action_res = action(c, ctx, prev, action_res, True)\n        c, self = yield\n        trans = ctx.immediate_transition(self)\n        while True:\n            if c in _WHITESPACE:\n                if prev == _CLOSE_BRACE:\n                    _illegal_character(c, ctx.set_ion_type(ion_type), 'Expected }.')\n            elif c == _CLOSE_BRACE:\n                if prev == _CLOSE_BRACE:\n                    validate(c, ctx, action_res)\n                    break\n            else:\n                action_res = action(c, ctx, prev, action_res, False)\n            prev = c\n            c, _ = yield trans\n        ctx.set_self_delimiting(True)  # Lob values are self-delimiting (they are terminated by '}}').\n        yield ctx.event_transition(IonThunkEvent, IonEventType.SCALAR, ion_type, _parse_lob(ion_type, val))\n    return lob_end_handler"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _blob_end_handler_factory():\n    def expand_res(res):\n        if res is None:\n            return 0, 0\n        return res\n\n    def action(c, ctx, prev, res, is_first):\n        num_digits, num_pads = expand_res(res)\n        if c in _BASE64_DIGITS:\n            if prev == _CLOSE_BRACE or prev == _BASE64_PAD:\n                _illegal_character(c, ctx.set_ion_type(IonType.BLOB))\n            num_digits += 1\n        elif c == _BASE64_PAD:\n            if prev == _CLOSE_BRACE:\n                _illegal_character(c, ctx.set_ion_type(IonType.BLOB))\n            num_pads += 1\n        else:\n            _illegal_character(c, ctx.set_ion_type(IonType.BLOB))\n        ctx.value.append(c)\n        return num_digits, num_pads\n\n    def validate(c, ctx, res):\n        num_digits, num_pads = expand_res(res)\n        if num_pads > 3 or (num_digits + num_pads) % 4 != 0:\n            _illegal_character(c, ctx, 'Incorrect number of pad characters (%d) for a blob of %d base-64 digits.'\n                               % (num_pads, num_digits))\n\n    return _lob_end_handler_factory(IonType.BLOB, action, validate)", "response": "Generates the handler for the end of a blob value. This includes the base - 64 digits and two closing braces."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the handler for the end of a clob value.", "response": "def _clob_end_handler_factory():\n    \"\"\"Generates the handler for the end of a clob value. This includes anything from the data's closing quote through\n    the second closing brace.\n    \"\"\"\n    def action(c, ctx, prev, res, is_first):\n        if is_first and ctx.is_self_delimiting and c == _DOUBLE_QUOTE:\n            assert c is prev\n            return res\n        _illegal_character(c, ctx)\n\n    return _lob_end_handler_factory(IonType.CLOB, action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _container_start_handler_factory(ion_type, before_yield=lambda c, ctx: None):\n    assert ion_type.is_container\n\n    @coroutine\n    def container_start_handler(c, ctx):\n        before_yield(c, ctx)\n        yield\n        yield ctx.event_transition(IonEvent, IonEventType.CONTAINER_START, ion_type, value=None)\n    return container_start_handler", "response": "Generates a handler for tokens that begin with container start characters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a co-routine for retrieving data up to a requested size. Args: whence (Coroutine): The co-routine to return to after the data is satisfied. ctx (_HandlerContext): The context for the read. complete (True|False): True if STREAM_END should be emitted if no bytes are read or available; False if INCOMPLETE should be emitted in that case. can_flush (True|False): True if NEXT may be requested after INCOMPLETE is emitted as a result of this data request.", "response": "def _read_data_handler(whence, ctx, complete, can_flush):\n    \"\"\"Creates a co-routine for retrieving data up to a requested size.\n\n    Args:\n        whence (Coroutine): The co-routine to return to after the data is satisfied.\n        ctx (_HandlerContext): The context for the read.\n        complete (True|False): True if STREAM_END should be emitted if no bytes are read or\n            available; False if INCOMPLETE should be emitted in that case.\n        can_flush (True|False): True if NEXT may be requested after INCOMPLETE is emitted as a result of this data\n            request.\n    \"\"\"\n    trans = None\n    queue = ctx.queue\n\n    while True:\n        data_event, self = (yield trans)\n        if data_event is not None:\n            if data_event.data is not None:\n                data = data_event.data\n                data_len = len(data)\n                if data_len > 0:\n                    queue.extend(data)\n                    yield Transition(None, whence)\n            elif data_event.type is ReadEventType.NEXT:\n                queue.mark_eof()\n                if not can_flush:\n                    _illegal_character(queue.read_byte(), ctx, \"Unexpected EOF.\")\n                yield Transition(None, whence)\n        trans = Transition(complete and ION_STREAM_END_EVENT or ION_STREAM_INCOMPLETE_EVENT, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _container_handler(c, ctx):\n    _, self = (yield None)\n    queue = ctx.queue\n    child_context = None\n    is_field_name = ctx.ion_type is IonType.STRUCT\n    delimiter_required = False\n    complete = ctx.depth == 0\n    can_flush = False\n\n    def has_pending_symbol():\n        return child_context and child_context.pending_symbol is not None\n\n    def symbol_value_event():\n        return child_context.event_transition(\n            IonEvent, IonEventType.SCALAR, IonType.SYMBOL, _as_symbol(child_context.pending_symbol))\n\n    def pending_symbol_value():\n        if has_pending_symbol():\n            assert not child_context.value\n            if ctx.ion_type is IonType.STRUCT and child_context.field_name is None:\n                _illegal_character(c, ctx,\n                                   'Encountered STRUCT value %s without field name.' % (child_context.pending_symbol,))\n            return symbol_value_event()\n        return None\n\n    def is_value_decorated():\n        return child_context is not None and (child_context.annotations or child_context.field_name is not None)\n\n    def _can_flush():\n        return child_context is not None and \\\n               child_context.depth == 0 and \\\n               (\n                   (\n                       child_context.ion_type is not None and\n                       (\n                           child_context.ion_type.is_numeric or\n                           (child_context.ion_type.is_text and not ctx.quoted_text and not is_field_name)\n                       )\n                   ) or\n                   (\n                       child_context.line_comment and\n                       not is_value_decorated()\n                   )\n               )\n\n    while True:\n        # Loop over all values in this container.\n        if c in ctx.container.end or c in ctx.container.delimiter or BufferQueue.is_eof(c):\n            symbol_event = pending_symbol_value()\n            if symbol_event is not None:\n                yield symbol_event\n                child_context = None\n                delimiter_required = ctx.container.is_delimited\n            if c in ctx.container.end:\n                if not delimiter_required and is_value_decorated():\n                    _illegal_character(c, child_context,\n                                       'Dangling field name (%s) and/or annotation(s) (%r) at end of container.'\n                                       % (child_context.field_name, child_context.annotations))\n                # Yield the close event and go to enclosing container. This coroutine instance will never resume.\n                yield Transition(\n                    IonEvent(IonEventType.CONTAINER_END, ctx.ion_type, depth=ctx.depth-1),\n                    ctx.whence\n                )\n                raise ValueError('Resumed a finished container handler.')\n            elif c in ctx.container.delimiter:\n                if not delimiter_required:\n                    _illegal_character(c, ctx.derive_child_context(None),\n                                       'Encountered delimiter %s without preceding value.'\n                                       % (_chr(ctx.container.delimiter[0]),))\n                is_field_name = ctx.ion_type is IonType.STRUCT\n                delimiter_required = False\n                c = None\n            else:\n                assert BufferQueue.is_eof(c)\n                assert len(queue) == 0\n                yield ctx.read_data_event(self, complete=True)\n                c = None\n        if c is not None and c not in _WHITESPACE:\n            can_flush = False\n            if c == _SLASH:\n                if child_context is None:\n                    # This is the start of a new child value (or, if this is a comment, a new value will start after the\n                    # comment ends).\n                    child_context = ctx.derive_child_context(self)\n                if ctx.ion_type is IonType.SEXP:\n                    handler = _sexp_slash_handler(c, child_context, pending_event=pending_symbol_value())\n                else:\n                    handler = _comment_handler(c, child_context, self)\n            elif delimiter_required:\n                # This is not the delimiter, or whitespace, or the start of a comment. Throw.\n                _illegal_character(c, ctx.derive_child_context(None), 'Delimiter %s not found after value.'\n                                   % (_chr(ctx.container.delimiter[0]),))\n            elif has_pending_symbol():\n                # A character besides whitespace, comments, and delimiters has been found, and there is a pending\n                # symbol. That pending symbol is either an annotation, a field name, or a symbol value.\n                if c == _COLON:\n                    if is_field_name:\n                        is_field_name = False\n                        child_context.set_field_name()\n                        c = None\n                    else:\n                        assert not ctx.quoted_text\n                        if len(queue) == 0:\n                            yield ctx.read_data_event(self)\n                        c = queue.read_byte()\n                        if c == _COLON:\n                            child_context.set_annotation()\n                            c = None  # forces another character to be read safely\n                        else:\n                            # Colon that doesn't indicate a field name or annotation.\n                            _illegal_character(c, child_context)\n                else:\n                    if is_field_name:\n                        _illegal_character(c, child_context, 'Illegal character after field name %s.'\n                                           % child_context.pending_symbol)\n                    # It's a symbol value delimited by something other than a comma (i.e. whitespace or comment)\n                    yield symbol_value_event()\n                    child_context = None\n                    delimiter_required = ctx.container.is_delimited\n                continue\n            else:\n                if not is_value_decorated():\n                    # This is the start of a new child value.\n                    child_context = ctx.derive_child_context(self)\n                if is_field_name:\n                    handler = _FIELD_NAME_START_TABLE[c](c, child_context)\n                else:\n                    handler = _VALUE_START_TABLE[c](c, child_context)  # Initialize the new handler\n                    can_flush = _IMMEDIATE_FLUSH_TABLE[c]\n            container_start = c == _OPEN_BRACKET or \\\n                              c == _OPEN_PAREN  # _OPEN_BRACE might start a lob; that is handled elsewhere.\n            quoted_start = c == _DOUBLE_QUOTE or c == _SINGLE_QUOTE\n            while True:\n                # Loop over all characters in the current token. A token is either a non-symbol value or a pending\n                # symbol, which may end up being a field name, annotation, or symbol value.\n                if container_start:\n                    c = None\n                    container_start = False\n                else:\n                    if child_context.quoted_text or quoted_start:\n                        quoted_start = False\n                        yield child_context.next_code_point(self)\n                        c = child_context.code_point\n                    else:\n                        if len(queue) == 0:\n                            yield ctx.read_data_event(self, can_flush=can_flush)\n                        c = queue.read_byte()\n                trans = handler.send((c, handler))\n                if trans.event is not None:\n                    is_self_delimiting = False\n                    if child_context.is_composite:\n                        # This is a composite transition, i.e. it is an event transition followed by an immediate\n                        # transition to the handler coroutine for the next token.\n                        next_transition = trans.next_transition\n                        child_context = trans.next_context\n                        assert next_transition is None or next_transition.event is None\n                    else:\n                        next_transition = None\n                        is_self_delimiting = child_context.is_self_delimiting\n                        child_context = None\n                    # This child value is finished. c is now the first character in the next value or sequence.\n                    # Hence, a new character should not be read; it should be provided to the handler for the next\n                    # child context.\n                    yield trans\n                    event_ion_type = trans.event.ion_type  # None in the case of IVM event.\n                    is_container = event_ion_type is not None and event_ion_type.is_container and \\\n                        trans.event.event_type is not IonEventType.SCALAR\n                    if is_container:\n                        assert next_transition is None\n                        yield Transition(\n                            None,\n                            _container_handler(c, ctx.derive_container_context(trans.event.ion_type, self))\n                        )\n                    complete = ctx.depth == 0\n                    can_flush = False\n                    if is_container or is_self_delimiting:\n                        # The end of the value has been reached, and c needs to be updated\n                        assert not ctx.quoted_text\n                        if len(queue) == 0:\n                            yield ctx.read_data_event(self, complete, can_flush)\n                        c = queue.read_byte()\n                    delimiter_required = ctx.container.is_delimited\n                    if next_transition is None:\n                        break\n                    else:\n                        trans = next_transition\n                elif self is trans.delegate:\n                    child_context.set_ion_type(None)  # The next token will determine the type.\n                    complete = False\n                    can_flush = _can_flush()\n                    if is_field_name:\n                        assert not can_flush\n                        if c == _COLON or not child_context.is_self_delimiting:\n                            break\n                    elif has_pending_symbol():\n                        can_flush = ctx.depth == 0\n                        if not child_context.is_self_delimiting or child_context.line_comment:\n                            break\n                    elif child_context.is_self_delimiting:\n                        # This is the end of a comment. If this is at the top level and is un-annotated,\n                        # it may end the stream.\n                        complete = ctx.depth == 0 and not is_value_decorated()\n                    # This happens at the end of a comment within this container, or when a symbol token has been\n                    # found. In both cases, an event should not be emitted. Read the next character and continue.\n                    if len(queue) == 0:\n                        yield ctx.read_data_event(self, complete, can_flush)\n                    c = queue.read_byte()\n                    break\n                # This is an immediate transition to a handler (may be the same one) for the current token.\n                can_flush = _can_flush()\n                handler = trans.delegate\n        else:\n            assert not ctx.quoted_text\n            if len(queue) == 0:\n                yield ctx.read_data_event(self, complete, can_flush)\n            c = queue.read_byte()", "response": "Coroutine for container values. Delegates to other coroutines to tokenize all child values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _skip_trampoline(handler):\n    data_event, self = (yield None)\n    delegate = handler\n    event = None\n    depth = 0\n    while True:\n        def pass_through():\n            _trans = delegate.send(Transition(data_event, delegate))\n            return _trans, _trans.delegate, _trans.event\n\n        if data_event is not None and data_event.type is ReadEventType.SKIP:\n            while True:\n                trans, delegate, event = pass_through()\n                if event is not None:\n                    if event.event_type is IonEventType.CONTAINER_END and event.depth <= depth:\n                        break\n                if event is None or event.event_type is IonEventType.INCOMPLETE:\n                    data_event, _ = yield Transition(event, self)\n        else:\n            trans, delegate, event = pass_through()\n            if event is not None and (event.event_type is IonEventType.CONTAINER_START or\n                                      event.event_type is IonEventType.CONTAINER_END):\n                depth = event.depth\n        data_event, _ = yield Transition(event, self)", "response": "Intercepts events from container handlers emitting them only if they should not be skipped."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the next code point from within a quoted string or symbol.", "response": "def _next_code_point_handler(whence, ctx):\n    \"\"\"Retrieves the next code point from within a quoted string or symbol.\"\"\"\n    data_event, self = yield\n    queue = ctx.queue\n    unicode_escapes_allowed = ctx.ion_type is not IonType.CLOB\n    escaped_newline = False\n    escape_sequence = b''\n    low_surrogate_required = False\n    while True:\n        if len(queue) == 0:\n            yield ctx.read_data_event(self)\n        queue_iter = iter(queue)\n        code_point_generator = _next_code_point_iter(queue, queue_iter)\n        code_point = next(code_point_generator)\n        if code_point == _BACKSLASH:\n            escape_sequence += six.int2byte(_BACKSLASH)\n            num_digits = None\n            while True:\n                if len(queue) == 0:\n                    yield ctx.read_data_event(self)\n                code_point = next(queue_iter)\n                if six.indexbytes(escape_sequence, -1) == _BACKSLASH:\n                    if code_point == _ord(b'u') and unicode_escapes_allowed:\n                        # 4-digit unicode escapes, plus '\\u' for each surrogate\n                        num_digits = 12 if low_surrogate_required else 6\n                        low_surrogate_required = False\n                    elif low_surrogate_required:\n                        _illegal_character(code_point, ctx,\n                                           'Unpaired high surrogate escape sequence %s.' % (escape_sequence,))\n                    elif code_point == _ord(b'x'):\n                        num_digits = 4  # 2-digit hex escapes\n                    elif code_point == _ord(b'U') and unicode_escapes_allowed:\n                        num_digits = 10  # 8-digit unicode escapes\n                    elif code_point in _COMMON_ESCAPES:\n                        if code_point == _SLASH or code_point == _QUESTION_MARK:\n                            escape_sequence = b''  # Drop the \\. Python does not recognize these as escapes.\n                        escape_sequence += six.int2byte(code_point)\n                        break\n                    elif code_point in _NEWLINES:\n                        escaped_newline = True\n                        break\n                    else:\n                        # This is a backslash followed by an invalid escape character. This is illegal.\n                        _illegal_character(code_point, ctx, 'Invalid escape sequence \\\\%s.' % (_chr(code_point),))\n                    escape_sequence += six.int2byte(code_point)\n                else:\n                    if code_point not in _HEX_DIGITS:\n                        _illegal_character(code_point, ctx,\n                                           'Non-hex character %s found in unicode escape.' % (_chr(code_point),))\n                    escape_sequence += six.int2byte(code_point)\n                    if len(escape_sequence) == num_digits:\n                        break\n            if not escaped_newline:\n                decoded_escape_sequence = escape_sequence.decode('unicode-escape')\n                cp_iter = _next_code_point_iter(decoded_escape_sequence, iter(decoded_escape_sequence), to_int=ord)\n                code_point = next(cp_iter)\n                if code_point is None:\n                    # This is a high surrogate. Restart the loop to gather the low surrogate.\n                    low_surrogate_required = True\n                    continue\n                code_point = CodePoint(code_point)\n                code_point.char = decoded_escape_sequence\n                code_point.is_escaped = True\n                ctx.set_code_point(code_point)\n                yield Transition(None, whence)\n        elif low_surrogate_required:\n            _illegal_character(code_point, ctx, 'Unpaired high surrogate escape sequence %s.' % (escape_sequence,))\n        if code_point == _CARRIAGE_RETURN:\n            # Normalize all newlines (\\r, \\n, and \\r\\n) to \\n .\n            if len(queue) == 0:\n                yield ctx.read_data_event(self)\n            code_point = next(queue_iter)\n            if code_point != _NEWLINE:\n                queue.unread(code_point)\n                code_point = _NEWLINE\n        while code_point is None:\n            yield ctx.read_data_event(self)\n            code_point = next(code_point_generator)\n        if escaped_newline:\n            code_point = CodePoint(code_point)\n            code_point.char = _ESCAPED_NEWLINE\n            code_point.is_escaped = True\n        ctx.set_code_point(code_point)\n        yield Transition(None, whence)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reader(queue=None, is_unicode=False):\n    if queue is None:\n        queue = BufferQueue(is_unicode)\n    ctx = _HandlerContext(\n        container=_C_TOP_LEVEL,\n        queue=queue,\n        field_name=None,\n        annotations=None,\n        depth=0,\n        whence=None,\n        value=None,\n        ion_type=None,  # Top level\n        pending_symbol=None\n    )\n    return reader_trampoline(_skip_trampoline(_container_handler(None, ctx)), allow_flush=True)", "response": "Returns a raw binary reader co - routine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an ion event event_transition that yields to another co - routine.", "response": "def event_transition(self, event_cls, event_type, ion_type, value):\n        \"\"\"Returns an ion event event_transition that yields to another co-routine.\"\"\"\n        annotations = self.annotations or ()\n        depth = self.depth\n        whence = self.whence\n\n        if ion_type is IonType.SYMBOL:\n            if not annotations and depth == 0 and isinstance(value, _IVMToken):\n                event = value.ivm_event()\n                if event is None:\n                    _illegal_character(None, self, 'Illegal IVM: %s.' % (value.text,))\n                return Transition(event, whence)\n            assert not isinstance(value, _IVMToken)\n\n        return Transition(\n            event_cls(event_type, ion_type, value, self.field_name, annotations, depth),\n            whence\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_data_event(self, whence, complete=False, can_flush=False):\n        return Transition(None, _read_data_handler(whence, self, complete, can_flush))", "response": "Creates a transition to a co - routine for retrieving data as bytes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the context s value to a sequence of unicode code points indicating whether the text is quoted.", "response": "def set_unicode(self, quoted_text=False):\n        \"\"\"Converts the context's ``value`` to a sequence of unicode code points for holding text tokens, indicating\n        whether the text is quoted.\n        \"\"\"\n        if isinstance(self.value, CodePointArray):\n            assert self.quoted_text == quoted_text\n            return self\n        self.value = CodePointArray(self.value)\n        self.quoted_text = quoted_text\n        self.line_comment = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the context s quoted_text flag. Useful when entering and exiting quoted text tokens.", "response": "def set_quoted_text(self, quoted_text):\n        \"\"\"Sets the context's ``quoted_text`` flag. Useful when entering and exiting quoted text tokens.\"\"\"\n        self.quoted_text = quoted_text\n        self.line_comment = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef derive_container_context(self, ion_type, whence):\n        if ion_type is IonType.STRUCT:\n            container = _C_STRUCT\n        elif ion_type is IonType.LIST:\n            container = _C_LIST\n        elif ion_type is IonType.SEXP:\n            container = _C_SEXP\n        else:\n            raise TypeError('Cannot derive container context for non-container type %s.' % (ion_type.name,))\n        return _HandlerContext(\n            container=container,\n            queue=self.queue,\n            field_name=self.field_name,\n            annotations=self.annotations,\n            depth=self.depth + 1,\n            whence=whence,\n            value=None,  # containers don't have a value\n            ion_type=ion_type,\n            pending_symbol=None\n        )", "response": "Derives a container context as a child of the current context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_empty_symbol(self):\n        self.field_name = None\n        self.annotations = None\n        self.ion_type = None\n        self.set_pending_symbol(CodePointArray())\n        return self", "response": "Resets the context to the empty symbol."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef derive_child_context(self, whence):\n        return _HandlerContext(\n            container=self.container,\n            queue=self.queue,\n            field_name=None,\n            annotations=None,\n            depth=self.depth,\n            whence=whence,\n            value=bytearray(),  # children start without a value\n            ion_type=None,\n            pending_symbol=None\n        )", "response": "Derives a scalar context as a child of the current context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the context to the given IonType.", "response": "def set_ion_type(self, ion_type):\n        \"\"\"Sets context to the given IonType.\"\"\"\n        if ion_type is self.ion_type:\n            return self\n        self.ion_type = ion_type\n        self.line_comment = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend the context s pending_symbol to its annotations sequence.", "response": "def set_annotation(self):\n        \"\"\"Appends the context's ``pending_symbol`` to its ``annotations`` sequence.\"\"\"\n        assert self.pending_symbol is not None\n        assert not self.value\n        annotations = (_as_symbol(self.pending_symbol, is_symbol_value=False),)  # pending_symbol becomes an annotation\n        self.annotations = annotations if not self.annotations else self.annotations + annotations\n        self.ion_type = None\n        self.pending_symbol = None  # reset pending symbol\n        self.quoted_text = False\n        self.line_comment = False\n        self.is_self_delimiting = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_field_name(self):\n        assert self.pending_symbol is not None\n        assert not self.value\n        self.field_name = _as_symbol(self.pending_symbol, is_symbol_value=False)  # pending_symbol becomes field name\n        self.pending_symbol = None  # reset pending symbol\n        self.quoted_text = False\n        self.line_comment = False\n        self.is_self_delimiting = False\n        return self", "response": "Sets the context s pending_symbol as its field_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_pending_symbol(self, pending_symbol=None):\n        if pending_symbol is None:\n            pending_symbol = CodePointArray()\n        self.value = bytearray()  # reset value\n        self.pending_symbol = pending_symbol\n        self.line_comment = False\n        return self", "response": "Sets the context s pending_symbol with the given unicode sequence and resets the context s value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_base(buf, value, bits_per_octet, end_bit=0, sign_bit=0, is_signed=False):\n    if value == 0:\n        buf.append(sign_bit | end_bit)\n        return 1\n    num_bits = bit_length(value)\n    num_octets = num_bits // bits_per_octet\n    # 'remainder' is the number of value bits in the first octet.\n    remainder = num_bits % bits_per_octet\n    if remainder != 0 or is_signed:\n        # If signed, the first octet has one fewer bit available, requiring another octet.\n        num_octets += 1\n    else:\n        # This ensures that unsigned values that fit exactly are not shifted too far.\n        remainder = bits_per_octet\n    for i in range(num_octets):\n        octet = 0\n        if i == 0:\n            octet |= sign_bit\n        if i == num_octets - 1:\n            octet |= end_bit\n        # 'remainder' is used for alignment such that only the first octet\n        # may contain insignificant zeros.\n        octet |= ((value >> (num_bits - (remainder + bits_per_octet * i))) & _OCTET_MASKS[bits_per_octet])\n        buf.append(octet)\n    return num_octets", "response": "Write a field to the buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs a type that can be extended to create immutable value types.", "response": "def record(*fields):\n    \"\"\"Constructs a type that can be extended to create immutable, value types.\n\n    Examples:\n        A typical declaration looks like::\n\n            class MyRecord(record('a', ('b', 1))):\n                pass\n\n        The above would make a sub-class of ``collections.namedtuple`` that was named ``MyRecord`` with\n        a constructor that had the ``b`` field set to 1 by default.\n\n    Note:\n        This uses meta-class machinery to rewrite the inheritance hierarchy.\n        This is done in order to make sure that the underlying ``namedtuple`` instance is\n        bound to the right type name and to make sure that the synthetic class that is generated\n        to enable this machinery is not enabled for sub-classes of a user's record class.\n\n    Args:\n        fields (list[str | (str, any)]): A sequence of str or pairs that\n    \"\"\"\n    @six.add_metaclass(_RecordMetaClass)\n    class RecordType(object):\n        _record_sentinel = True\n        _record_fields = fields\n\n    return RecordType"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef coroutine(func):\n    def wrapper(*args, **kwargs):\n        gen = func(*args, **kwargs)\n        val = next(gen)\n        if val != None:\n            raise TypeError('Unexpected value from start of coroutine')\n        return gen\n    wrapper.__name__ = func.__name__\n    wrapper.__doc__ = func.__doc__\n    return wrapper", "response": "A function that wraps a PEP - 342 enhanced generator in a way that avoids boilerplate of the priming call to next."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unicode_iter(val):\n    val_iter = iter(val)\n    while True:\n        try:\n            code_point = next(_next_code_point(val, val_iter, to_int=ord))\n        except StopIteration:\n            return\n        if code_point is None:\n            raise ValueError('Unpaired high surrogate at end of Unicode sequence: %r' % val)\n        yield code_point", "response": "Provides an iterator over the code points of the given Unicode sequence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide the next code point in the given Unicode sequence.", "response": "def _next_code_point(val, val_iter, yield_char=False, to_int=lambda x: x):\n    \"\"\"Provides the next *code point* in the given Unicode sequence.\n\n    This generator function yields complete character code points, never incomplete surrogates. When a low surrogate is\n    found without following a high surrogate, this function raises ``ValueError`` for having encountered an unpaired\n    low surrogate. When the provided iterator ends on a high surrogate, this function yields ``None``. This is the\n    **only** case in which this function yields ``None``. When this occurs, the user may append additional data to the\n    input unicode sequence and resume iterating through another ``next`` on this generator. When this function receives\n    ``next`` after yielding ``None``, it *reinitializes the unicode iterator*. This means that this feature can only\n    be used for values that contain an ``__iter__`` implementation that remains at the current position in the data\n    when called (e.g. :class:`BufferQueue`). At this point, there are only two possible outcomes:\n        * If next code point is a valid low surrogate, this function yields the combined code point represented by the\n          surrogate pair.\n        * Otherwise, this function raises ``ValueError`` for having encountered an unpaired high surrogate.\n\n    Args:\n        val (unicode|BufferQueue): A unicode sequence or unicode BufferQueue over which to iterate.\n        val_iter (Iterator[unicode|BufferQueue]): The unicode sequence iterator over ``val`` from which to generate the\n            next integer code point in the range ``0x0`` to ``0x10FFFF``.\n        yield_char (Optional[bool]): If True **and** the character code point resulted from a surrogate pair, this\n            function will yield a :class:`CodePoint` representing the character code point and containing the original\n            unicode character. This is useful when the original unicode character will be needed again because UCS2\n            Python builds will error when trying to convert code points greater than 0xFFFF back into their\n            unicode character representations. This avoids requiring the user to mathematically re-derive the\n            surrogate pair in order to successfully convert the code point back to a unicode character.\n        to_int (Optional[callable]): A function to call on each element of val_iter to convert that element to an int.\n    \"\"\"\n    try:\n        high = next(val_iter)\n    except StopIteration:\n        return\n    low = None\n    code_point = to_int(high)\n    if _LOW_SURROGATE_START <= code_point <= _LOW_SURROGATE_END:\n        raise ValueError('Unpaired low surrogate in Unicode sequence: %d' % code_point)\n    elif _HIGH_SURROGATE_START <= code_point <= _HIGH_SURROGATE_END:\n        def combine_surrogates():\n            low_surrogate = next(val_iter)\n            low_code_point = to_int(low_surrogate)\n            if low_code_point < _LOW_SURROGATE_START or low_code_point > _LOW_SURROGATE_END:\n                raise ValueError('Unpaired high surrogate: %d' % code_point)\n            # Decode the surrogates\n            real_code_point = _NON_BMP_OFFSET\n            real_code_point += (code_point - _HIGH_SURROGATE_START) << 10\n            real_code_point += (low_code_point - _LOW_SURROGATE_START)\n            return real_code_point, low_surrogate\n        try:\n            code_point, low = combine_surrogates()\n        except StopIteration:\n            yield None\n            val_iter = iter(val)  # More data has appeared in val.\n            code_point, low = combine_surrogates()\n    if yield_char and low is not None:\n        out = CodePoint(code_point)\n        if isinstance(val, six.text_type):\n            # Iterating over a text type returns text types.\n            out.char = high + low\n        else:\n            out.char = six.unichr(high) + six.unichr(low)\n    else:\n        out = code_point\n    yield out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timestamp(year, month=1, day=1,\n              hour=0, minute=0, second=0, microsecond=None,\n              off_hours=None, off_minutes=None,\n              precision=None, fractional_precision=None):\n    \"\"\"Shorthand for the :class:`Timestamp` constructor.\n\n    Specifically, converts ``off_hours`` and ``off_minutes`` parameters to a suitable\n    :class:`OffsetTZInfo` instance.\n    \"\"\"\n    delta = None\n    if off_hours is not None:\n        if off_hours < -23 or off_hours > 23:\n            raise ValueError('Hour offset %d is out of required range -23..23.' % (off_hours,))\n        delta = timedelta(hours=off_hours)\n    if off_minutes is not None:\n        if off_minutes < -59 or off_minutes > 59:\n            raise ValueError('Minute offset %d is out of required range -59..59.' % (off_minutes,))\n        minutes_delta = timedelta(minutes=off_minutes)\n        if delta is None:\n            delta = minutes_delta\n        else:\n            delta += minutes_delta\n\n    tz = None\n    if delta is not None:\n        tz = OffsetTZInfo(delta)\n\n    if microsecond is not None:\n        if fractional_precision is None:\n            fractional_precision = MICROSECOND_PRECISION\n    else:\n        microsecond = 0\n        if fractional_precision is not None:\n            raise ValueError('Cannot have fractional precision without a fractional component.')\n\n    return Timestamp(\n        year, month, day,\n        hour, minute, second, microsecond,\n        tz, precision=precision, fractional_precision=fractional_precision\n    )", "response": "Returns a new Timestamp instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef derive_field_name(self, field_name):\n        cls = type(self)\n        # We use ordinals to avoid thunk materialization.\n        return cls(\n            self[0],\n            self[1],\n            self[2],\n            field_name,\n            self[4],\n            self[5]\n        )", "response": "Derives a new IonEvent from this one setting the field_name attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef derive_annotations(self, annotations):\n        cls = type(self)\n        # We use ordinals to avoid thunk materialization.\n        return cls(\n            self[0],\n            self[1],\n            self[2],\n            self[3],\n            annotations,\n            self[5]\n        )", "response": "Derives a new IonEvent from this one setting the annotations attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nderives a new event from this one setting the value attribute.", "response": "def derive_value(self, value):\n        \"\"\"Derives a new event from this one setting the ``value`` attribute.\n\n        Args:\n            value: (any):\n                The value associated with the derived event.\n\n        Returns:\n            IonEvent: The newly generated non-thunk event.\n        \"\"\"\n        return IonEvent(\n            self.event_type,\n            self.ion_type,\n            value,\n            self.field_name,\n            self.annotations,\n            self.depth\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nderive a new event from this one setting the depth attribute.", "response": "def derive_depth(self, depth):\n        \"\"\"Derives a new event from this one setting the ``depth`` attribute.\n\n        Args:\n            depth: (int):\n                The annotations associated with the derived event.\n\n        Returns:\n            IonEvent: The newly generated event.\n        \"\"\"\n        cls = type(self)\n        # We use ordinals to avoid thunk materialization.\n        return cls(\n            self[0],\n            self[1],\n            self[2],\n            self[3],\n            self[4],\n            depth\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a Timestamp from UTC fields adjusted to the local offset if given.", "response": "def adjust_from_utc_fields(*args, **kwargs):\n        \"\"\"Constructs a timestamp from UTC fields adjusted to the local offset if given.\"\"\"\n        raw_ts = Timestamp(*args, **kwargs)\n        offset = raw_ts.utcoffset()\n        if offset is None or offset == timedelta():\n            return raw_ts\n\n        # XXX This returns a datetime, not a Timestamp (which has our precision if defined)\n        adjusted = raw_ts + offset\n        if raw_ts.precision is None:\n            # No precision means we can just return a regular datetime\n            return adjusted\n\n        return Timestamp(\n            adjusted.year,\n            adjusted.month,\n            adjusted.day,\n            adjusted.hour,\n            adjusted.minute,\n            adjusted.second,\n            adjusted.microsecond,\n            raw_ts.tzinfo,\n            precision=raw_ts.precision,\n            fractional_precision=raw_ts.fractional_precision\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _serialize_scalar_from_string_representation_factory(type_name, types, str_func=str):\n    def serialize(ion_event):\n        value = ion_event.value\n        validate_scalar_value(value, types)\n        return six.b(str_func(value))\n    serialize.__name__ = '_serialize_' + type_name\n    return serialize", "response": "Builds a function that leverage Python str or similar functionality."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that serializes container start and end.", "response": "def _serialize_container_factory(suffix, container_map):\n    \"\"\"Returns a function that serializes container start/end.\n\n    Args:\n        suffix (str): The suffix to name the function with.\n        container_map (Dictionary[core.IonType, bytes]): The\n\n    Returns:\n        function: The closure for serialization.\n    \"\"\"\n    def serialize(ion_event):\n        if not ion_event.ion_type.is_container:\n            raise TypeError('Expected container type')\n        return container_map[ion_event.ion_type]\n    serialize.__name__ = '_serialize_container_' + suffix\n    return serialize"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a raw text writer co - routine.", "response": "def raw_writer(indent=None):\n    \"\"\"Returns a raw text writer co-routine.\n\n    Yields:\n        DataEvent: serialization events to write out\n\n        Receives :class:`amazon.ion.core.IonEvent` or ``None`` when the co-routine yields\n        ``HAS_PENDING`` :class:`WriteEventType` events.\n    \"\"\"\n\n    is_whitespace_str = isinstance(indent, str) and re.search(r'\\A\\s*\\Z', indent, re.M) is not None\n    if not (indent is None or is_whitespace_str):\n        raise ValueError('The indent parameter must either be None or a string containing only whitespace')\n\n    indent_bytes = six.b(indent) if isinstance(indent, str) else indent\n\n    return writer_trampoline(_raw_writer_coroutine(indent=indent_bytes))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef writer_trampoline(start):\n    trans = Transition(None, start)\n    while True:\n        ion_event = (yield trans.event)\n        if trans.event is None:\n            if ion_event is None:\n                raise TypeError('Cannot start Writer with no event')\n        else:\n            if trans.event.type is WriteEventType.HAS_PENDING and ion_event is not None:\n                raise TypeError('Writer expected to receive no event: %r' % (ion_event,))\n            if trans.event.type is not WriteEventType.HAS_PENDING and ion_event is None:\n                raise TypeError('Writer expected to receive event')\n            if ion_event is not None and ion_event.event_type is IonEventType.INCOMPLETE:\n                raise TypeError('Writer cannot receive INCOMPLETE event')\n        trans = trans.delegate.send(Transition(ion_event, trans.delegate))", "response": "Provides the co - routine trampoline for a given co - routine."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndrains the writer of its pending write events.", "response": "def _drain(writer, ion_event):\n    \"\"\"Drain the writer of its pending write events.\n\n    Args:\n        writer (Coroutine): A writer co-routine.\n        ion_event (amazon.ion.core.IonEvent): The first event to apply to the writer.\n\n    Yields:\n        DataEvent: Yields each pending data event.\n    \"\"\"\n    result_event = _WRITE_EVENT_HAS_PENDING_EMPTY\n    while result_event.type is WriteEventType.HAS_PENDING:\n        result_event = writer.send(ion_event)\n        ion_event = None\n        yield result_event"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blocking_writer(writer, output):\n    result_type = None\n    while True:\n        ion_event = (yield result_type)\n        for result_event in _drain(writer, ion_event):\n            output.write(result_event.data)\n        result_type = result_event.type", "response": "Provides an implementation of using a writer co - routine with a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy this instance. Its IonEvent (if any) is not preserved. Keeping this protected until/unless we decide there's use for it publicly.", "response": "def _copy(self):\n        \"\"\"Copies this instance. Its IonEvent (if any) is not preserved.\n\n        Keeping this protected until/unless we decide there's use for it publicly.\n        \"\"\"\n        args, kwargs = self._to_constructor_args(self)\n        value = self.__class__(*args, **kwargs)\n        value.ion_event = None\n        value.ion_type = self.ion_type\n        value.ion_annotations = self.ion_annotations\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a native extension from the properties of an event.", "response": "def from_event(cls, ion_event):\n        \"\"\"Constructs the given native extension from the properties of an event.\n\n        Args:\n            ion_event (IonEvent): The event to construct the native value from.\n        \"\"\"\n        if ion_event.value is not None:\n            args, kwargs = cls._to_constructor_args(ion_event.value)\n        else:\n            # if value is None (i.e. this is a container event), args must be empty or initialization of the\n            # underlying container will fail.\n            args, kwargs = (), {}\n        value = cls(*args, **kwargs)\n        value.ion_event = ion_event\n        value.ion_type = ion_event.ion_type\n        value.ion_annotations = ion_event.annotations\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a value as a copy with an associated Ion type and annotations.", "response": "def from_value(cls, ion_type, value, annotations=()):\n        \"\"\"Constructs a value as a copy with an associated Ion type and annotations.\n\n        Args:\n            ion_type (IonType): The associated Ion type.\n            value (Any): The value to construct from, generally of type ``cls``.\n            annotations (Sequence[unicode]):  The sequence Unicode strings decorating this value.\n        \"\"\"\n        if value is None:\n            value = IonPyNull()\n        else:\n            args, kwargs = cls._to_constructor_args(value)\n            value = cls(*args, **kwargs)\n        value.ion_event = None\n        value.ion_type = ion_type\n        value.ion_annotations = annotations\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_event(self, event_type, field_name=None, depth=None):\n        if self.ion_event is None:\n            value = self\n            if isinstance(self, IonPyNull):\n                value = None\n            self.ion_event = IonEvent(event_type, ion_type=self.ion_type, value=value, field_name=field_name,\n                                      annotations=self.ion_annotations, depth=depth)\n        return self.ion_event", "response": "Constructs an IonEvent from this _IonNature value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bytes_to_long(s):\n    if isinstance(s, int):\n        # On Python 2, indexing into a bytearray returns a byte string; on Python 3, an int.\n        return s\n    acc = 0\n    if USING_PYTHON2:\n        acc = long(acc)  # noqa\n    unpack = struct.unpack\n    length = len(s)\n    if length % 4:\n        extra = (4 - length % 4)\n        s = b'\\000' * extra + s\n        length = length + extra\n    for i in range(0, length, 4):\n        acc = (acc << 32) + unpack(b'>I', s[i:i + 4])[0]\n    return acc", "response": "Convert a byte string to a long integer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstring long_to_bytes - Convert a long integer to a byte string.", "response": "def long_to_bytes(n, blocksize=0):\n    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n    Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front of the\n    byte string with binary zeros so that the length is a multiple of\n    blocksize.\n    \"\"\"\n    # after much testing, this algorithm was deemed to be the fastest\n    s = b''\n    if USING_PYTHON2:\n        n = long(n)  # noqa\n    pack = struct.pack\n    while n > 0:\n        s = pack(b'>I', n & 0xffffffff) + s\n        n = n >> 32\n    # strip off leading zeros\n    for i in range(len(s)):\n        if s[i] != b'\\000'[0]:\n            break\n    else:\n        # only happens when n == 0\n        s = b'\\000'\n        i = 0\n    s = s[i:]\n    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n    # de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef raw_p_sha1(secret, seed, sizes=()):\n    full_size = 0\n    for size in sizes:\n        full_size += size\n\n    result = b''\n    accum = seed\n    while len(result) < full_size:\n        accum = hmac_sha1(secret, accum)\n        result += hmac_sha1(secret, accum + seed)\n\n    parts = []\n    for size in sizes:\n        parts.append(result[:size])\n        result = result[size:]\n    return tuple(parts)", "response": "Derive one or more keys from secret and seed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies a list of X509 certs.", "response": "def verify_x509_cert_chain(cert_chain, ca_pem_file=None, ca_path=None):\n    \"\"\"\n    Look at certs in the cert chain and add them to the store one by one.\n    Return the cert at the end of the chain. That is the cert to be used by the caller for verifying.\n    From https://www.w3.org/TR/xmldsig-core2/#sec-X509Data:\n    \"All certificates appearing in an X509Data element must relate to the validation key by either containing it\n    or being part of a certification chain that terminates in a certificate containing the validation key.\n    No ordering is implied by the above constraints\"\n    \"\"\"\n    from OpenSSL import SSL\n    context = SSL.Context(SSL.TLSv1_METHOD)\n    if ca_pem_file is None and ca_path is None:\n        import certifi\n        ca_pem_file = certifi.where()\n    context.load_verify_locations(ensure_bytes(ca_pem_file, none_ok=True), capath=ca_path)\n    store = context.get_cert_store()\n    certs = list(reversed(cert_chain))\n    end_of_chain, last_error = None, None\n    while len(certs) > 0:\n        for cert in certs:\n            try:\n                end_of_chain = _add_cert_to_store(store, cert)\n                certs.remove(cert)\n                break\n            except RedundantCert:\n                certs.remove(cert)\n                if end_of_chain is None:\n                    end_of_chain = cert\n                break\n            except Exception as e:\n                last_error = e\n        else:\n            raise last_error\n    return end_of_chain"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _remove_sig(signature, idempotent=False):\n    try:\n        signaturep = next(signature.iterancestors())\n    except StopIteration:\n        if idempotent:\n            return\n        raise ValueError(\"Can't remove the root signature node\")\n    if signature.tail is not None:\n        try:\n            signatures = next(signature.itersiblings(preceding=True))\n        except StopIteration:\n            if signaturep.text is not None:\n                signaturep.text = signaturep.text + signature.tail\n            else:\n                signaturep.text = signature.tail\n        else:\n            if signatures.tail is not None:\n                signatures.tail = signatures.tail + signature.tail\n            else:\n                signatures.tail = signature.tail\n    signaturep.remove(signature)", "response": "Removes the signature node from the root signature node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsigns the data and return the root element of the XML tree.", "response": "def sign(self, data, key=None, passphrase=None, cert=None, reference_uri=None, key_name=None, key_info=None,\n             id_attribute=None):\n        \"\"\"\n        Sign the data and return the root element of the resulting XML tree.\n\n        :param data: Data to sign\n        :type data: String, file-like object, or XML ElementTree Element API compatible object\n        :param key:\n            Key to be used for signing. When signing with a certificate or RSA/DSA/ECDSA key, this can be a string\n            containing a PEM-formatted key, or a :py:class:`cryptography.hazmat.primitives.interfaces.RSAPublicKey`,\n            :py:class:`cryptography.hazmat.primitives.interfaces.DSAPublicKey`, or\n            :py:class:`cryptography.hazmat.primitives.interfaces.EllipticCurvePublicKey` object. When signing with a\n            HMAC, this should be a string containing the shared secret.\n        :type key:\n            string, :py:class:`cryptography.hazmat.primitives.interfaces.RSAPublicKey`,\n            :py:class:`cryptography.hazmat.primitives.interfaces.DSAPublicKey`, or\n            :py:class:`cryptography.hazmat.primitives.interfaces.EllipticCurvePublicKey` object\n        :param passphrase: Passphrase to use to decrypt the key, if any.\n        :type passphrase: string\n        :param cert:\n            X.509 certificate to use for signing. This should be a string containing a PEM-formatted certificate, or an\n            array of strings or OpenSSL.crypto.X509 objects containing the certificate and a chain of intermediate\n            certificates.\n        :type cert: string, array of strings, or array of OpenSSL.crypto.X509 objects\n        :param reference_uri:\n            Custom reference URI or list of reference URIs to incorporate into the signature. When ``method`` is set to\n            ``detached`` or ``enveloped``, reference URIs are set to this value and only the referenced elements are\n            signed.\n        :type reference_uri: string or list\n        :param key_name: Add a KeyName element in the KeyInfo element that may be used by the signer to communicate a\n            key identifier to the recipient. Typically, KeyName contains an identifier related to the key pair used to\n            sign the message.\n        :type key_name: string\n        :param key_info: A custom KeyInfo element to insert in the signature. Use this to supply\n            ``<wsse:SecurityTokenReference>`` or other custom key references.\n        :type key_info: :py:class:`lxml.etree.Element`\n        :param id_attribute:\n            Name of the attribute whose value ``URI`` refers to. By default, SignXML will search for \"Id\", then \"ID\".\n        :type id_attribute: string\n\n        :returns:\n            A :py:class:`lxml.etree.Element` object representing the root of the XML tree containing the signature and\n            the payload data.\n\n        To specify the location of an enveloped signature within **data**, insert a\n        ``<ds:Signature Id=\"placeholder\"></ds:Signature>`` element in **data** (where\n        \"ds\" is the \"http://www.w3.org/2000/09/xmldsig#\" namespace). This element will\n        be replaced by the generated signature, and excised when generating the digest.\n        \"\"\"\n        if id_attribute is not None:\n            self.id_attributes = (id_attribute, )\n\n        if isinstance(cert, (str, bytes)):\n            cert_chain = list(iterate_pem(cert))\n        else:\n            cert_chain = cert\n\n        if isinstance(reference_uri, (str, bytes)):\n            reference_uris = [reference_uri]\n        else:\n            reference_uris = reference_uri\n\n        sig_root, doc_root, c14n_inputs, reference_uris = self._unpack(data, reference_uris)\n        signed_info_element, signature_value_element = self._build_sig(sig_root, reference_uris, c14n_inputs)\n\n        if key is None:\n            raise InvalidInput('Parameter \"key\" is required')\n\n        signed_info_c14n = self._c14n(signed_info_element, algorithm=self.c14n_alg)\n        if self.sign_alg.startswith(\"hmac-\"):\n            from cryptography.hazmat.primitives.hmac import HMAC\n            signer = HMAC(key=key,\n                          algorithm=self._get_hmac_digest_method_by_tag(self.sign_alg),\n                          backend=default_backend())\n            signer.update(signed_info_c14n)\n            signature_value_element.text = ensure_str(b64encode(signer.finalize()))\n            sig_root.append(signature_value_element)\n        elif any(self.sign_alg.startswith(i) for i in [\"dsa-\", \"rsa-\", \"ecdsa-\"]):\n            if isinstance(key, (str, bytes)):\n                from cryptography.hazmat.primitives.serialization import load_pem_private_key\n                key = load_pem_private_key(key, password=passphrase, backend=default_backend())\n\n            hash_alg = self._get_signature_digest_method_by_tag(self.sign_alg)\n            if self.sign_alg.startswith(\"dsa-\"):\n                signature = key.sign(signed_info_c14n, algorithm=hash_alg)\n            elif self.sign_alg.startswith(\"ecdsa-\"):\n                signature = key.sign(signed_info_c14n, signature_algorithm=ec.ECDSA(algorithm=hash_alg))\n            elif self.sign_alg.startswith(\"rsa-\"):\n                signature = key.sign(signed_info_c14n, padding=PKCS1v15(), algorithm=hash_alg)\n            else:\n                raise NotImplementedError()\n            if self.sign_alg.startswith(\"dsa-\"):\n                # Note: The output of the DSA signer is a DER-encoded ASN.1 sequence of two DER integers.\n                from asn1crypto.algos import DSASignature\n                decoded_signature = DSASignature.load(signature).native\n                r = decoded_signature['r']\n                s = decoded_signature['s']\n                signature = long_to_bytes(r).rjust(32, b\"\\0\") + long_to_bytes(s).rjust(32, b\"\\0\")\n\n            signature_value_element.text = ensure_str(b64encode(signature))\n\n            if key_info is None:\n                key_info = SubElement(sig_root, ds_tag(\"KeyInfo\"))\n                if key_name is not None:\n                    keyname = SubElement(key_info, ds_tag(\"KeyName\"))\n                    keyname.text = key_name\n\n                if cert_chain is None:\n                    self._serialize_key_value(key, key_info)\n                else:\n                    x509_data = SubElement(key_info, ds_tag(\"X509Data\"))\n                    for cert in cert_chain:\n                        x509_certificate = SubElement(x509_data, ds_tag(\"X509Certificate\"))\n                        if isinstance(cert, (str, bytes)):\n                            x509_certificate.text = strip_pem_header(cert)\n                        else:\n                            from OpenSSL.crypto import dump_certificate, FILETYPE_PEM\n                            x509_certificate.text = strip_pem_header(dump_certificate(FILETYPE_PEM, cert))\n            else:\n                sig_root.append(key_info)\n        else:\n            raise NotImplementedError()\n\n        if self.method == methods.enveloping:\n            for c14n_input in c14n_inputs:\n                doc_root.append(c14n_input)\n        return doc_root if self.method == methods.enveloped else sig_root"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify(self, data, require_x509=True, x509_cert=None, cert_subject_name=None, ca_pem_file=None, ca_path=None,\n               hmac_key=None, validate_schema=True, parser=None, uri_resolver=None, id_attribute=None,\n               expect_references=1):\n        \"\"\"\n        Verify the XML signature supplied in the data and return the XML node signed by the signature, or raise an\n        exception if the signature is not valid. By default, this requires the signature to be generated using a valid\n        X.509 certificate. To enable other means of signature validation, set the **require_x509** argument to `False`.\n\n        .. admonition:: See what is signed\n\n         It is important to understand and follow the best practice rule of \"See what is signed\" when verifying XML\n         signatures. The gist of this rule is: if your application neglects to verify that the information it trusts is\n         what was actually signed, the attacker can supply a valid signature but point you to malicious data that wasn't\n         signed by that signature.\n\n         In SignXML, you can ensure that the information signed is what you expect to be signed by only trusting the\n         data returned by the ``verify()`` method. The return value is the XML node or string that was signed. Also,\n         depending on the signature settings used, comments in the XML data may not be subject to signing, so may need\n         to be untrusted.\n\n         **Recommended reading:** http://www.w3.org/TR/xmldsig-bestpractices/#practices-applications\n\n        .. admonition:: Establish trust\n\n         If you do not supply any keyword arguments to ``verify()``, the default behavior is to trust **any** valid XML\n         signature generated using a valid X.509 certificate trusted by your system's CA store. This means anyone can\n         get an SSL certificate and generate a signature that you will trust. To establish trust in the signer, use the\n         ``x509_cert`` argument to specify a certificate that was pre-shared out-of-band (e.g. via SAML metadata, as\n         shown in :ref:`Verifying SAML assertions <verifying-saml-assertions>`), or ``cert_subject_name`` to specify a\n         subject name that must be in the signing X.509 certificate given by the signature (verified as if it were a\n         domain name), or ``ca_pem_file``/``ca_path`` to give a custom CA.\n\n        :param data: Signature data to verify\n        :type data: String, file-like object, or XML ElementTree Element API compatible object\n        :param require_x509:\n            If ``True``, a valid X.509 certificate-based signature with an established chain of trust is required to\n            pass validation. If ``False``, other types of valid signatures (e.g. HMAC or RSA public key) are accepted.\n        :type require_x509: boolean\n        :param x509_cert:\n            A trusted external X.509 certificate, given as a PEM-formatted string or OpenSSL.crypto.X509 object, to use\n            for verification. Overrides any X.509 certificate information supplied by the signature. If left set to\n            ``None``, requires that the signature supply a valid X.509 certificate chain that validates against the\n            known certificate authorities. Implies **require_x509=True**.\n        :type x509_cert: string or OpenSSL.crypto.X509\n        :param ca_pem_file:\n            Filename of a PEM file containing certificate authority information to use when verifying certificate-based\n            signatures.\n        :type ca_pem_file: string or bytes\n        :param ca_path:\n            Path to a directory containing PEM-formatted certificate authority files to use when verifying\n            certificate-based signatures. If neither **ca_pem_file** nor **ca_path** is given, the Mozilla CA bundle\n            provided by :py:mod:`certifi` will be loaded.\n        :type ca_path: string\n        :param cert_subject_name:\n            Subject Common Name to check the signing X.509 certificate against. Implies **require_x509=True**.\n        :type cert_subject_name: string\n        :param hmac_key: If using HMAC, a string containing the shared secret.\n        :type hmac_key: string\n        :param validate_schema: Whether to validate **data** against the XML Signature schema.\n        :type validate_schema: boolean\n        :param parser: Custom XML parser instance to use when parsing **data**.\n        :type parser: :py:class:`lxml.etree.XMLParser` compatible parser\n        :param uri_resolver: Function to use to resolve reference URIs that don't start with \"#\".\n        :type uri_resolver: callable\n        :param id_attribute:\n            Name of the attribute whose value ``URI`` refers to. By default, SignXML will search for \"Id\", then \"ID\".\n        :type id_attribute: string\n        :param expect_references:\n            Number of references to expect in the signature. If this is not 1, an array of VerifyResults is returned.\n            If set to a non-integer, any number of references is accepted (otherwise a mismatch raises an error).\n        :type expect_references: int or boolean\n\n        :raises: :py:class:`cryptography.exceptions.InvalidSignature`\n\n        :returns: VerifyResult object with the signed data, signed xml and signature xml\n        :rtype: VerifyResult\n\n        \"\"\"\n        self.hmac_key = hmac_key\n        self.require_x509 = require_x509\n        self.x509_cert = x509_cert\n        self._parser = parser\n\n        if x509_cert:\n            self.require_x509 = True\n\n        if id_attribute is not None:\n            self.id_attributes = (id_attribute, )\n\n        root = self.get_root(data)\n        if root.tag == ds_tag(\"Signature\"):\n            signature_ref = root\n        else:\n            signature_ref = self._find(root, \"Signature\", anywhere=True)\n\n        # HACK: deep copy won't keep root's namespaces\n        signature = fromstring(etree.tostring(signature_ref), parser=parser)\n\n        if validate_schema:\n            self.schema().assertValid(signature)\n\n        signed_info = self._find(signature, \"SignedInfo\")\n        c14n_method = self._find(signed_info, \"CanonicalizationMethod\")\n        c14n_algorithm = c14n_method.get(\"Algorithm\")\n        signature_method = self._find(signed_info, \"SignatureMethod\")\n        signature_value = self._find(signature, \"SignatureValue\")\n        signature_alg = signature_method.get(\"Algorithm\")\n        raw_signature = b64decode(signature_value.text)\n        x509_data = signature.find(\"ds:KeyInfo/ds:X509Data\", namespaces=namespaces)\n        signed_info_c14n = self._c14n(signed_info, algorithm=c14n_algorithm)\n\n        if x509_data is not None or self.require_x509:\n            from OpenSSL.crypto import load_certificate, X509, FILETYPE_PEM, verify, Error as OpenSSLCryptoError\n\n            if self.x509_cert is None:\n                if x509_data is None:\n                    raise InvalidInput(\"Expected a X.509 certificate based signature\")\n                certs = [cert.text for cert in self._findall(x509_data, \"X509Certificate\")]\n                if not certs:\n                    msg = \"Expected to find an X509Certificate element in the signature\"\n                    msg += \" (X509SubjectName, X509SKI are not supported)\"\n                    raise InvalidInput(msg)\n                cert_chain = [load_certificate(FILETYPE_PEM, add_pem_header(cert)) for cert in certs]\n                signing_cert = verify_x509_cert_chain(cert_chain, ca_pem_file=ca_pem_file, ca_path=ca_path)\n            elif isinstance(self.x509_cert, X509):\n                signing_cert = self.x509_cert\n            else:\n                signing_cert = load_certificate(FILETYPE_PEM, add_pem_header(self.x509_cert))\n\n            if cert_subject_name and signing_cert.get_subject().commonName != cert_subject_name:\n                raise InvalidSignature(\"Certificate subject common name mismatch\")\n\n            signature_digest_method = self._get_signature_digest_method(signature_alg).name\n            try:\n                verify(signing_cert, raw_signature, signed_info_c14n, signature_digest_method)\n            except OpenSSLCryptoError as e:\n                try:\n                    lib, func, reason = e.args[0][0]\n                except Exception:\n                    reason = e\n                raise InvalidSignature(\"Signature verification failed: {}\".format(reason))\n            # TODO: CN verification goes here\n            # TODO: require one of the following to be set: either x509_cert or (ca_pem_file or ca_path) or common_name\n            # Use ssl.match_hostname or code from it to perform match\n        elif \"hmac-sha\" in signature_alg:\n            if self.hmac_key is None:\n                raise InvalidInput('Parameter \"hmac_key\" is required when verifying a HMAC signature')\n\n            from cryptography.hazmat.primitives.hmac import HMAC\n            signer = HMAC(key=ensure_bytes(self.hmac_key),\n                          algorithm=self._get_hmac_digest_method(signature_alg),\n                          backend=default_backend())\n            signer.update(signed_info_c14n)\n            if raw_signature != signer.finalize():\n                raise InvalidSignature(\"Signature mismatch (HMAC)\")\n        else:\n            key_value = signature.find(\"ds:KeyInfo/ds:KeyValue\", namespaces=namespaces)\n            if key_value is None:\n                raise InvalidInput(\"Expected to find either KeyValue or X509Data XML element in KeyInfo\")\n\n            self._verify_signature_with_pubkey(signed_info_c14n, raw_signature, key_value, signature_alg)\n\n        verify_results = []\n        for reference in self._findall(signed_info, \"Reference\"):\n            transforms = self._find(reference, \"Transforms\", require=False)\n            digest_algorithm = self._find(reference, \"DigestMethod\").get(\"Algorithm\")\n            digest_value = self._find(reference, \"DigestValue\")\n            payload = self._resolve_reference(root, reference, uri_resolver=uri_resolver)\n            payload_c14n = self._apply_transforms(payload, transforms, signature_ref, c14n_algorithm)\n            if digest_value.text != self._get_digest(payload_c14n, self._get_digest_method(digest_algorithm)):\n                raise InvalidDigest(\"Digest mismatch for reference {}\".format(len(verify_results)))\n\n            # We return the signed XML (and only that) to ensure no access to unsigned data happens\n            try:\n                payload_c14n_xml = fromstring(payload_c14n)\n            except etree.XMLSyntaxError:\n                payload_c14n_xml = None\n            verify_results.append(VerifyResult(payload_c14n, payload_c14n_xml, signature))\n\n        if type(expect_references) is int and len(verify_results) != expect_references:\n            msg = \"Expected to find {} references, but found {}\"\n            raise InvalidSignature(msg.format(expect_references, len(verify_results)))\n\n        return verify_results if expect_references > 1 else verify_results[0]", "response": "Verify the XML signature supplied in the data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authorize(self, scope=None, redirect_uri=None, state=None):\n        _logger.debug(\"Called authorize()\")\n        params = {'client_id': self.client_id}\n        if scope:\n            params['scope'] = scope\n        if redirect_uri:\n            params['redirect_uri'] = redirect_uri\n        if state:\n            params['state'] = state\n\n        url = self.auth_url + 'authorize?' + urlencode(params)\n        _logger.debug(\"Redirecting to %s\", url)\n        return redirect(url)", "response": "Redirect to GitHub and request access to a user s data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authorized_handler(self, f):\n        @wraps(f)\n        def decorated(*args, **kwargs):\n            if 'code' in request.args:\n                data = self._handle_response()\n            else:\n                data = self._handle_invalid_response()\n            return f(*((data,) + args), **kwargs)\n        return decorated", "response": "Decorator for the route that is used as the callback for authorizing\n        with GitHub."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle the response from GitHub to GitHub.", "response": "def _handle_response(self):\n        \"\"\"\n        Handles response after the redirect to GitHub. This response\n        determines if the user has allowed the this application access. If we\n        were then we send a POST request for the access_key used to\n        authenticate requests to GitHub.\n\n        \"\"\"\n        _logger.debug(\"Handling response from GitHub\")\n        params = {\n            'code': request.args.get('code'),\n            'client_id': self.client_id,\n            'client_secret': self.client_secret\n        }\n        url = self.auth_url + 'access_token'\n        _logger.debug(\"POSTing to %s\", url)\n        _logger.debug(params)\n        response = self.session.post(url, data=params)\n        data = parse_qs(response.content)\n        _logger.debug(\"response.content = %s\", data)\n        for k, v in data.items():\n            if len(v) == 1:\n                data[k] = v[0]\n        token = data.get(b'access_token', None)\n        if token is not None:\n            token = token.decode('ascii')\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a HTTP request and returns the response object.", "response": "def raw_request(self, method, resource, access_token=None, **kwargs):\n        \"\"\"\n        Makes a HTTP request and returns the raw\n        :class:`~requests.Response` object.\n\n        \"\"\"\n        headers = self._pop_headers(kwargs)\n        headers['Authorization'] = self._get_authorization_header(access_token)\n        url = self._get_resource_url(resource)\n        return self.session.request(method, url, allow_redirects=True, headers=headers, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request(self, method, resource, all_pages=False, **kwargs):\n        response = self.raw_request(method, resource, **kwargs)\n\n        if not is_valid_response(response):\n            raise GitHubError(response)\n\n        if is_json_response(response):\n            result = response.json()\n            while all_pages and response.links.get('next'):\n                url = response.links['next']['url']\n                response = self.raw_request(method, url, **kwargs)\n                if not is_valid_response(response) or \\\n                        not is_json_response(response):\n                    raise GitHubError(response)\n                body = response.json()\n                if isinstance(body, list):\n                    result += body\n                elif isinstance(body, dict) and 'items' in body:\n                    result['items'] += body['items']\n                else:\n                    raise GitHubError(response)\n            return result\n        else:\n            return response", "response": "Makes a request to the given endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, resource, params=None, **kwargs):\n        return self.request('GET', resource, params=params, **kwargs)", "response": "Shortcut for request ( GET resource params = None kwargs = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_lazy(rlp, sedes=None, **sedes_kwargs):\n    item, end = consume_item_lazy(rlp, 0)\n    if end != len(rlp):\n        raise DecodingError('RLP length prefix announced wrong length', rlp)\n    if isinstance(item, LazyList):\n        item.sedes = sedes\n        item.sedes_kwargs = sedes_kwargs\n        return item\n    elif sedes:\n        return sedes.deserialize(item, **sedes_kwargs)\n    else:\n        return item", "response": "Decodes an RLP encoded object in a lazy fashion."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading an item from an RLP string lazily.", "response": "def consume_item_lazy(rlp, start):\n    \"\"\"Read an item from an RLP string lazily.\n\n    If the length prefix announces a string, the string is read; if it\n    announces a list, a :class:`LazyList` is created.\n\n    :param rlp: the rlp string to read from\n    :param start: the position at which to start reading\n    :returns: a tuple ``(item, end)`` where ``item`` is the read string or a\n              :class:`LazyList` and ``end`` is the position of the first\n              unprocessed byte.\n    \"\"\"\n    p, t, l, s = consume_length_prefix(rlp, start)\n    if t is bytes:\n        item, _, end = consume_payload(rlp, p, s, bytes, l)\n        return item, end\n    else:\n        assert t is list\n        return LazyList(rlp, s, s + l), s + l"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a specific element from an rlp encoded nested list.", "response": "def peek(rlp, index, sedes=None):\n    \"\"\"Get a specific element from an rlp encoded nested list.\n\n    This function uses :func:`rlp.decode_lazy` and, thus, decodes only the\n    necessary parts of the string.\n\n    Usage example::\n\n        >>> import rlp\n        >>> rlpdata = rlp.encode([1, 2, [3, [4, 5]]])\n        >>> rlp.peek(rlpdata, 0, rlp.sedes.big_endian_int)\n        1\n        >>> rlp.peek(rlpdata, [2, 0], rlp.sedes.big_endian_int)\n        3\n\n    :param rlp: the rlp string\n    :param index: the index of the element to peek at (can be a list for\n                  nested data)\n    :param sedes: a sedes used to deserialize the peeked at object, or `None`\n                  if no deserialization should be performed\n    :raises: :exc:`IndexError` if `index` is invalid (out of range or too many\n             levels)\n    \"\"\"\n    ll = decode_lazy(rlp)\n    if not isinstance(index, Iterable):\n        index = [index]\n    for i in index:\n        if isinstance(ll, Atomic):\n            raise IndexError('Too many indices given')\n        ll = ll[i]\n    if sedes:\n        return sedes.deserialize(ll)\n    else:\n        return ll"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a sedes for text data with exactly l encoded characters.", "response": "def fixed_length(cls, l, allow_empty=False):\n        \"\"\"Create a sedes for text data with exactly `l` encoded characters.\"\"\"\n        return cls(l, l, allow_empty=allow_empty)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _eq(left, right):\n    if isinstance(left, (tuple, list)) and isinstance(right, (tuple, list)):\n        return len(left) == len(right) and all(_eq(*pair) for pair in zip(left, right))\n    else:\n        return left == right", "response": "Equality comparison that allows for equality between tuple and list types\n    with equivalent elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if obj is a sequence but not a string or bytes.", "response": "def is_sequence(obj):\n    \"\"\"Check if `obj` is a sequence, but not a string or bytes.\"\"\"\n    return isinstance(obj, Sequence) and not (\n        isinstance(obj, str) or BinaryClass.is_valid_type(obj))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(obj, sedes=None, infer_serializer=True, cache=True):\n    if isinstance(obj, Serializable):\n        cached_rlp = obj._cached_rlp\n        if sedes is None and cached_rlp:\n            return cached_rlp\n        else:\n            really_cache = (\n                cache and\n                sedes is None\n            )\n    else:\n        really_cache = False\n\n    if sedes:\n        item = sedes.serialize(obj)\n    elif infer_serializer:\n        item = infer_sedes(obj).serialize(obj)\n    else:\n        item = obj\n\n    result = encode_raw(item)\n    if really_cache:\n        obj._cached_rlp = result\n    return result", "response": "Encode a Python object in RLP format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_raw(item):\n    if isinstance(item, Atomic):\n        if len(item) == 1 and item[0] < 128:\n            return item\n        payload = item\n        prefix_offset = 128  # string\n    elif not isinstance(item, str) and isinstance(item, collections.Sequence):\n        payload = b''.join(encode_raw(x) for x in item)\n        prefix_offset = 192  # list\n    else:\n        msg = 'Cannot encode object of type {0}'.format(type(item).__name__)\n        raise EncodingError(msg, item)\n\n    try:\n        prefix = length_prefix(len(payload), prefix_offset)\n    except ValueError:\n        raise EncodingError('Item too big to encode', item)\n\n    return prefix + payload", "response": "RLP encode a nested sequence of ) : class : Atomic s."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing the prefix to lists or strings denoting their length.", "response": "def length_prefix(length, offset):\n    \"\"\"Construct the prefix to lists or strings denoting their length.\n\n    :param length: the length of the item in bytes\n    :param offset: ``0x80`` when encoding raw bytes, ``0xc0`` when encoding a\n                   list\n    \"\"\"\n    if length < 56:\n        return ALL_BYTES[offset + length]\n    elif length < LONG_LENGTH:\n        length_string = int_to_big_endian(length)\n        return ALL_BYTES[offset + 56 - 1 + len(length_string)] + length_string\n    else:\n        raise ValueError('Length greater than 256**8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a length prefix from an RLP string.", "response": "def consume_length_prefix(rlp, start):\n    \"\"\"Read a length prefix from an RLP string.\n\n    :param rlp: the rlp byte string to read from\n    :param start: the position at which to start reading\n    :returns: a tuple ``(prefix, type, length, end)``, where ``type`` is either ``str``\n              or ``list`` depending on the type of the following payload,\n              ``length`` is the length of the payload in bytes, and ``end`` is\n              the position of the first payload byte in the rlp string\n    \"\"\"\n    b0 = rlp[start]\n    if b0 < 128:  # single byte\n        return (b'', bytes, 1, start)\n    elif b0 < SHORT_STRING:  # short string\n        if b0 - 128 == 1 and rlp[start + 1] < 128:\n            raise DecodingError('Encoded as short string although single byte was possible', rlp)\n        return (rlp[start:start + 1], bytes, b0 - 128, start + 1)\n    elif b0 < 192:  # long string\n        ll = b0 - 183  # - (128 + 56 - 1)\n        if rlp[start + 1:start + 2] == b'\\x00':\n            raise DecodingError('Length starts with zero bytes', rlp)\n        len_prefix = rlp[start + 1:start + 1 + ll]\n        l = big_endian_to_int(len_prefix)  # noqa: E741\n        if l < 56:\n            raise DecodingError('Long string prefix used for short string', rlp)\n        return (rlp[start:start + 1] + len_prefix, bytes, l, start + 1 + ll)\n    elif b0 < 192 + 56:  # short list\n        return (rlp[start:start + 1], list, b0 - 192, start + 1)\n    else:  # long list\n        ll = b0 - 192 - 56 + 1\n        if rlp[start + 1:start + 2] == b'\\x00':\n            raise DecodingError('Length starts with zero bytes', rlp)\n        len_prefix = rlp[start + 1:start + 1 + ll]\n        l = big_endian_to_int(len_prefix)  # noqa: E741\n        if l < 56:\n            raise DecodingError('Long list prefix used for short list', rlp)\n        return (rlp[start:start + 1] + len_prefix, list, l, start + 1 + ll)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef consume_payload(rlp, prefix, start, type_, length):\n    if type_ is bytes:\n        item = rlp[start: start + length]\n        return (item, [prefix + item], start + length)\n    elif type_ is list:\n        items = []\n        per_item_rlp = []\n        list_rlp = prefix\n        next_item_start = start\n        end = next_item_start + length\n        while next_item_start < end:\n            p, t, l, s = consume_length_prefix(rlp, next_item_start)\n            item, item_rlp, next_item_start = consume_payload(rlp, p, s, t, l)\n            per_item_rlp.append(item_rlp)\n            # When the item returned above is a single element, item_rlp will also contain a\n            # single element, but when it's a list, the first element will be the RLP of the\n            # whole List, which is what we want here.\n            list_rlp += item_rlp[0]\n            items.append(item)\n        per_item_rlp.insert(0, list_rlp)\n        if next_item_start > end:\n            raise DecodingError('List length prefix announced a too small '\n                                'length', rlp)\n        return (items, per_item_rlp, next_item_start)\n    else:\n        raise TypeError('Type must be either list or bytes')", "response": "Read the payload of an item from an RLP string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consume_item(rlp, start):\n    p, t, l, s = consume_length_prefix(rlp, start)\n    return consume_payload(rlp, p, s, t, l)", "response": "Read an item from an RLP string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode(rlp, sedes=None, strict=True, recursive_cache=False, **kwargs):\n    if not is_bytes(rlp):\n        raise DecodingError('Can only decode RLP bytes, got type %s' % type(rlp).__name__, rlp)\n    try:\n        item, per_item_rlp, end = consume_item(rlp, 0)\n    except IndexError:\n        raise DecodingError('RLP string too short', rlp)\n    if end != len(rlp) and strict:\n        msg = 'RLP string ends with {} superfluous bytes'.format(len(rlp) - end)\n        raise DecodingError(msg, rlp)\n    if sedes:\n        obj = sedes.deserialize(item, **kwargs)\n        if is_sequence(obj) or hasattr(obj, '_cached_rlp'):\n            _apply_rlp_cache(obj, per_item_rlp, recursive_cache)\n        return obj\n    else:\n        return item", "response": "Decodes an RLP encoded object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninferring the sedes object for a given Python object.", "response": "def infer_sedes(obj):\n    \"\"\"Try to find a sedes objects suitable for a given Python object.\n\n    The sedes objects considered are `obj`'s class, `big_endian_int` and\n    `binary`. If `obj` is a sequence, a :class:`rlp.sedes.List` will be\n    constructed recursively.\n\n    :param obj: the python object for which to find a sedes object\n    :raises: :exc:`TypeError` if no appropriate sedes could be found\n    \"\"\"\n    if is_sedes(obj.__class__):\n        return obj.__class__\n    elif not isinstance(obj, bool) and isinstance(obj, int) and obj >= 0:\n        return big_endian_int\n    elif BinaryClass.is_valid_type(obj):\n        return binary\n    elif not isinstance(obj, str) and isinstance(obj, collections.Sequence):\n        return List(map(infer_sedes, obj))\n    elif isinstance(obj, bool):\n        return boolean\n    elif isinstance(obj, str):\n        return text\n    msg = 'Did not find sedes handling type {}'.format(type(obj).__name__)\n    raise TypeError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef destinations(self, cluster='main'):\n        if not self.config.has_section(cluster):\n            raise SystemExit(\"Cluster '%s' not defined in %s\"\n                             % (cluster, self.config_file))\n        destinations = self.config.get(cluster, 'destinations')\n        return destinations.replace(' ', '').split(',')", "response": "Return a list of destinations for a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the replication factor for a cluster as an integer.", "response": "def replication_factor(self, cluster='main'):\n        \"\"\"Return the replication factor for a cluster as an integer.\"\"\"\n        if not self.config.has_section(cluster):\n            raise SystemExit(\"Cluster '%s' not defined in %s\"\n                             % (cluster, self.config_file))\n        return int(self.config.get(cluster, 'replication_factor'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ssh_user(self, cluster='main'):\n        if not self.config.has_section(cluster):\n            raise SystemExit(\"Cluster '%s' not defined in %s\"\n                             % (cluster, self.config_file))\n        try:\n            return self.config.get(cluster, 'ssh_user')\n        except NoOptionError:\n            return pwd.getpwuid(os.getuid()).pw_name", "response": "Return the ssh user for a cluster or current user if undefined."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlocks whisper files during carbon - sync.", "response": "def whisper_lock_writes(self, cluster='main'):\n        \"\"\"Lock whisper files during carbon-sync.\"\"\"\n        if not self.config.has_section(cluster):\n            raise SystemExit(\"Cluster '%s' not defined in %s\"\n                             % (cluster, self.config_file))\n        try:\n            return bool(self.config.get(cluster, 'whisper_lock_writes'))\n        except NoOptionError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hashing_type(self, cluster='main'):\n        if not self.config.has_section(cluster):\n            raise SystemExit(\"Cluster '%s' not defined in %s\"\n                             % (cluster, self.config_file))\n        hashing_type = 'carbon_ch'\n        try:\n            return self.config.get(cluster, 'hashing_type')\n        except NoOptionError:\n            return hashing_type", "response": "Get the hashing type of a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfilling the gaps in dst using data from src.", "response": "def fill_archives(src, dst, startFrom, endAt=0, overwrite=False,\n                  lock_writes=False):\n    \"\"\"\n    Fills gaps in dst using data from src.\n\n    src is the path as a string\n    dst is the path as a string\n    startFrom is the latest timestamp (archives are read backward)\n    endAt is the earliest timestamp (archives are read backward).\n          if absent, we take the earliest timestamp in the archive\n    overwrite will write all non null points from src dst.\n    lock using whisper lock if true\n    \"\"\"\n    if lock_writes is False:\n        whisper.LOCK = False\n    elif whisper.CAN_LOCK and lock_writes is True:\n        whisper.LOCK = True\n\n    header = whisper.info(dst)\n    archives = header['archives']\n    archives = sorted(archives, key=lambda t: t['retention'])\n\n    for archive in archives:\n        fromTime = max(endAt, time.time() - archive['retention'])\n        if fromTime >= startFrom:\n            continue\n\n        (timeInfo, values) = whisper.fetch(dst, fromTime, untilTime=startFrom)\n        (start, end, step) = timeInfo\n        gapstart = None\n        for value in values:\n            has_value = bool(value and not overwrite)\n            if not has_value and not gapstart:\n                gapstart = start\n            elif has_value and gapstart:\n                if (start - gapstart) >= archive['secondsPerPoint']:\n                    fill(src, dst, gapstart - step, start)\n                gapstart = None\n            start += step\n        # fill if this gap continues to the end\n        if gapstart:\n            fill(src, dst, gapstart - step, end - step)\n\n        # The next archive only needs to be filled up to the latest point\n        # in time we updated.\n        startFrom = fromTime"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data(path, hours, offset=0):\n    now = time.time()\n    end = now - _to_sec(offset)  # Will default to now\n    start = end - _to_sec(hours)\n    _data = whisper.fetch(path, start, end)\n    return all(x is None for x in _data[-1])", "response": "Returns True if the metric at path has any whisper data newer than hours."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stat(path, hours, offset=None):\n    return os.stat(path).st_mtime < (time.time() - _to_sec(hours))", "response": "Check if a metric file at path has been modified since hours ago."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef short_path(path, cwd=None):\n    if not isinstance(path, str):\n        return path\n    if cwd is None:\n        cwd = os.getcwd()\n    abspath = os.path.abspath(path)\n    relpath = os.path.relpath(path, cwd)\n    if len(abspath) <= len(relpath):\n        return abspath\n    return relpath", "response": "Return relative or absolute path name whichever is shortest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a copy of the __all__ dict with irrelevant items removed.", "response": "def get_all_dict(module):\n    \"\"\"Return a copy of the __all__ dict with irrelevant items removed.\"\"\"\n    if hasattr(module, \"__all__\"):\n        all_dict = copy.deepcopy(module.__all__)\n    else:\n        all_dict = copy.deepcopy(dir(module))\n        all_dict = [name for name in all_dict\n                    if not name.startswith(\"_\")]\n    for name in ['absolute_import', 'division', 'print_function']:\n        try:\n            all_dict.remove(name)\n        except ValueError:\n            pass\n\n    # Modules are almost always private; real submodules need a separate\n    # run of refguide_check.\n    all_dict = [name for name in all_dict\n                if not inspect.ismodule(getattr(module, name, None))]\n\n    deprecated = []\n    not_deprecated = []\n    for name in all_dict:\n        f = getattr(module, name, None)\n        if callable(f) and is_deprecated(f):\n            deprecated.append(name)\n        else:\n            not_deprecated.append(name)\n\n    others = set(dir(module)).difference(set(deprecated)).difference(set(not_deprecated))\n\n    return not_deprecated, deprecated, others"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare(all_dict, others, names, module_name):\n    only_all = set()\n    for name in all_dict:\n        if name not in names:\n            only_all.add(name)\n\n    only_ref = set()\n    missing = set()\n    for name in names:\n        if name not in all_dict:\n            for pat in REFGUIDE_ALL_SKIPLIST:\n                if re.match(pat, module_name + '.' + name):\n                    if name not in others:\n                        missing.add(name)\n                    break\n            else:\n                only_ref.add(name)\n\n    return only_all, only_ref, missing", "response": "Return sets of objects only in __all__ refguide or completely missing."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck reStructuredText formatting of docstrings of modules.", "response": "def check_rest(module, names, dots=True):\n    \"\"\"\n    Check reStructuredText formatting of docstrings\n\n    Returns: [(name, success_flag, output), ...]\n    \"\"\"\n\n    try:\n        skip_types = (dict, str, unicode, float, int)\n    except NameError:\n        # python 3\n        skip_types = (dict, str, float, int)\n\n    results = []\n\n    if module.__name__[6:] not in OTHER_MODULE_DOCS:\n        results += [(module.__name__,) +\n                    validate_rst_syntax(inspect.getdoc(module),\n                                        module.__name__, dots=dots)]\n\n    for name in names:\n        full_name = module.__name__ + '.' + name\n        obj = getattr(module, name, None)\n\n        if obj is None:\n            results.append((full_name, False, \"%s has no docstring\" % (full_name,)))\n            continue\n        elif isinstance(obj, skip_types):\n            continue\n\n        if inspect.ismodule(obj):\n            text = inspect.getdoc(obj)\n        else:\n            try:\n                text = str(get_doc_object(obj))\n            except:\n                import traceback\n                results.append((full_name, False,\n                                \"Error in docstring format!\\n\" +\n                                traceback.format_exc()))\n                continue\n\n        m = re.search(\"([\\x00-\\x09\\x0b-\\x1f])\", text)\n        if m:\n            msg = (\"Docstring contains a non-printable character %r! \"\n                   \"Maybe forgot r\\\"\\\"\\\"?\" % (m.group(1),))\n            results.append((full_name, False, msg))\n            continue\n\n        try:\n            src_file = short_path(inspect.getsourcefile(obj))\n        except TypeError:\n            src_file = None\n\n        if src_file:\n            file_full_name = src_file + ':' + full_name\n        else:\n            file_full_name = full_name\n\n        results.append((full_name,) +\n                       validate_rst_syntax(text, file_full_name, dots=dots))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the header of the current instance of the class.", "response": "def update_header(self):\n        \"\"\"\n        Updates header to edffile struct\n        \"\"\"\n        set_technician(self.handle, du(self.technician))\n        set_recording_additional(self.handle, du(self.recording_additional))\n        set_patientname(self.handle, du(self.patient_name))\n        set_patientcode(self.handle, du(self.patient_code))\n        set_patient_additional(self.handle, du(self.patient_additional))\n        set_equipment(self.handle, du(self.equipment))\n        set_admincode(self.handle, du(self.admincode))\n        if isinstance(self.gender, int):\n            set_gender(self.handle, self.gender)\n        elif self.gender == \"Male\":\n            set_gender(self.handle, 0)\n        elif self.gender == \"Female\":\n            set_gender(self.handle, 1)\n\n        set_datarecord_duration(self.handle, self.duration)\n        set_number_of_annotation_signals(self.handle, self.number_of_annotations)\n        set_startdatetime(self.handle, self.recording_start_time.year, self.recording_start_time.month,\n                          self.recording_start_time.day, self.recording_start_time.hour,\n                          self.recording_start_time.minute, self.recording_start_time.second)\n        if isstr(self.birthdate):\n            if self.birthdate != '':\n                birthday = datetime.strptime(self.birthdate, '%d %b %Y').date()\n                set_birthdate(self.handle, birthday.year, birthday.month, birthday.day)\n        else:\n            set_birthdate(self.handle, self.birthdate.year, self.birthdate.month, self.birthdate.day)\n        for i in np.arange(self.n_channels):\n            set_samplefrequency(self.handle, i, self.channels[i]['sample_rate'])\n            set_physical_maximum(self.handle, i, self.channels[i]['physical_max'])\n            set_physical_minimum(self.handle, i, self.channels[i]['physical_min'])\n            set_digital_maximum(self.handle, i, self.channels[i]['digital_max'])\n            set_digital_minimum(self.handle, i, self.channels[i]['digital_min'])\n            set_label(self.handle, i, du(self.channels[i]['label']))\n            set_physical_dimension(self.handle, i, du(self.channels[i]['dimension']))\n            set_transducer(self.handle, i, du(self.channels[i]['transducer']))\n            set_prefilter(self.handle, i, du(self.channels[i]['prefilter']))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the file header", "response": "def setHeader(self, fileHeader):\n        \"\"\"\n        Sets the file header\n        \"\"\"\n        self.technician = fileHeader[\"technician\"]\n        self.recording_additional = fileHeader[\"recording_additional\"]\n        self.patient_name = fileHeader[\"patientname\"]\n        self.patient_additional = fileHeader[\"patient_additional\"]\n        self.patient_code = fileHeader[\"patientcode\"]\n        self.equipment = fileHeader[\"equipment\"]\n        self.admincode = fileHeader[\"admincode\"]\n        self.gender = fileHeader[\"gender\"]\n        self.recording_start_time = fileHeader[\"startdate\"]\n        self.birthdate = fileHeader[\"birthdate\"]\n        self.update_header()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setSignalHeader(self, edfsignal, channel_info):\n        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal] = channel_info\n        self.update_header()", "response": "Sets the parameter for the signal edfsignal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the parameter for all signals", "response": "def setSignalHeaders(self, signalHeaders):\n        \"\"\"\n        Sets the parameter for all signals\n\n        Parameters\n        ----------\n        signalHeaders : array_like\n            containing dict with\n                'label' : str\n                          channel label (string, <= 16 characters, must be unique)\n                'dimension' : str\n                          physical dimension (e.g., mV) (string, <= 8 characters)\n                'sample_rate' : int\n                          sample frequency in hertz\n                'physical_max' : float\n                          maximum physical value\n                'physical_min' : float\n                         minimum physical value\n                'digital_max' : int\n                         maximum digital value (-2**15 <= x < 2**15)\n                'digital_min' : int\n                         minimum digital value (-2**15 <= x < 2**15)\n        \"\"\"\n        for edfsignal in np.arange(self.n_channels):\n            self.channels[edfsignal] = signalHeaders[edfsignal]\n        self.update_header()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_number_of_annotation_signals(self, number_of_annotations):\n        number_of_annotations = max((min((int(number_of_annotations), 64)), 1))\n        self.number_of_annotations = number_of_annotations\n        self.update_header()", "response": "This function is used to set the number of annotation signals for the record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setStartdatetime(self, recording_start_time):\n        if isinstance(recording_start_time,datetime):\n            self.recording_start_time = recording_start_time\n        else:\n            self.recording_start_time = datetime.strptime(recording_start_time,\"%d %b %Y %H:%M:%S\")\n        self.update_header()", "response": "Sets the recording start time of the recording class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the samplefrequency of the signal edfsignal.", "response": "def setSamplefrequency(self, edfsignal, samplefrequency):\n        \"\"\"\n        Sets the samplefrequency of signal edfsignal.\n\n        Notes\n        -----\n        This function is required for every signal and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['sample_rate'] = samplefrequency\n        self.update_header()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the physical_maximum of the signal edfsignal.", "response": "def setPhysicalMaximum(self, edfsignal, physical_maximum):\n        \"\"\"\n        Sets the physical_maximum of signal edfsignal.\n\n        Parameters\n        ----------\n        edfsignal: int\n            signal number\n        physical_maximum: float\n            Sets the physical maximum\n\n        Notes\n        -----\n        This function is required for every signal and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['physical_max'] = physical_maximum\n        self.update_header()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setPhysicalMinimum(self, edfsignal, physical_minimum):\n        if (edfsignal < 0 or edfsignal > self.n_channels):\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['physical_min'] = physical_minimum\n        self.update_header()", "response": "Sets the physical minimum of the signal edfsignal."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setDigitalMaximum(self, edfsignal, digital_maximum):\n        if (edfsignal < 0 or edfsignal > self.n_channels):\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['digital_max'] = digital_maximum\n        self.update_header()", "response": "Sets the digital maximum value for the specified signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the digital minimum value of a signal.", "response": "def setDigitalMinimum(self, edfsignal, digital_minimum):\n        \"\"\"\n        Sets the minimum digital value of signal edfsignal.\n        Usually, the value -32768 is used for EDF+ and -8388608 for BDF+. Usually this will be (-(digital_maximum + 1)).\n\n        Parameters\n        ----------\n        edfsignal : int\n            signal number\n        digital_minimum : int\n            Sets the minimum digital value\n\n        Notes\n        -----\n        This function is optional and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n        if (edfsignal < 0 or edfsignal > self.n_channels):\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['digital_min'] = digital_minimum\n        self.update_header()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the label of the given signal.", "response": "def setLabel(self, edfsignal, label):\n        \"\"\"\n        Sets the label (name) of signal edfsignal (\"FP1\", \"SaO2\", etc.).\n\n        Parameters\n        ----------\n        edfsignal : int\n            signal number on which the label should be changed\n        label : str\n            signal label\n\n        Notes\n        -----\n        This function is recommended for every signal and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n        if (edfsignal < 0 or edfsignal > self.n_channels):\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['label'] = label\n        self.update_header()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setPhysicalDimension(self, edfsignal, physical_dimension):\n        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['dimension'] = physical_dimension\n        self.update_header()", "response": "Sets the physical dimension of the signal edfsignal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the transducer of the channel with the given signal edfsignal.", "response": "def setTransducer(self, edfsignal, transducer):\n        \"\"\"\n        Sets the transducer of signal edfsignal\n\n        :param edfsignal: int\n        :param transducer: str\n\n        Notes\n        -----\n        This function is optional for every signal and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n        if (edfsignal < 0 or edfsignal > self.n_channels):\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['transducer'] = transducer\n        self.update_header()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setPrefilter(self, edfsignal, prefilter):\n        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['prefilter'] = prefilter\n        self.update_header()", "response": "Sets the prefilter of the given signal edfsignal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef writeSamples(self, data_list, digital = False):\n\n\n        if (len(data_list) != len(self.channels)):\n            raise WrongInputSize(len(data_list))\n            \n        if digital:\n            if any([not np.issubdtype(a.dtype, np.integer) for a in data_list]):\n                raise TypeError('Digital = True requires all signals in int')\n\n\n        ind = []\n        notAtEnd = True\n        for i in np.arange(len(data_list)):\n            ind.append(0)\n\n        sampleLength = 0\n        sampleRates = np.zeros(len(data_list), dtype=np.int)\n        for i in np.arange(len(data_list)):\n            sampleRates[i] = self.channels[i]['sample_rate']\n            if (np.size(data_list[i]) < ind[i] + self.channels[i]['sample_rate']):\n                notAtEnd = False\n            sampleLength += self.channels[i]['sample_rate']\n\n        dataOfOneSecond = np.array([], dtype=np.int if digital else None)\n\n        while notAtEnd:\n            # dataOfOneSecondInd = 0\n            del dataOfOneSecond\n            dataOfOneSecond = np.array([], dtype=np.int if digital else None)\n            for i in np.arange(len(data_list)):\n                # dataOfOneSecond[dataOfOneSecondInd:dataOfOneSecondInd+self.channels[i]['sample_rate']] = data_list[i].ravel()[int(ind[i]):int(ind[i]+self.channels[i]['sample_rate'])]\n                dataOfOneSecond = np.append(dataOfOneSecond,data_list[i].ravel()[int(ind[i]):int(ind[i]+sampleRates[i])])\n                # self.writePhysicalSamples(data_list[i].ravel()[int(ind[i]):int(ind[i]+self.channels[i]['sample_rate'])])\n                ind[i] += sampleRates[i]\n                # dataOfOneSecondInd += sampleRates[i]\n            if digital:\n                self.blockWriteDigitalSamples(dataOfOneSecond)   \n            else:\n                self.blockWritePhysicalSamples(dataOfOneSecond)\n                \n            for i in np.arange(len(data_list)):\n                if (np.size(data_list[i]) < ind[i] + sampleRates[i]):\n                    notAtEnd = False\n\n        # dataOfOneSecondInd = 0\n        for i in np.arange(len(data_list)):\n            lastSamples = np.zeros(sampleRates[i], dtype=np.int if digital else None)\n            lastSampleInd = int(np.max(data_list[i].shape) - ind[i])\n            lastSampleInd = int(np.min((lastSampleInd,sampleRates[i])))\n            if lastSampleInd > 0:\n                lastSamples[:lastSampleInd] = data_list[i].ravel()[-lastSampleInd:]\n                # dataOfOneSecond[dataOfOneSecondInd:dataOfOneSecondInd+self.channels[i]['sample_rate']] = lastSamples\n                # dataOfOneSecondInd += self.channels[i]['sample_rate']\n                if digital:\n                    self.writeDigitalSamples(lastSamples)   \n                else:\n                    self.writePhysicalSamples(lastSamples)", "response": "Writes samples from a list of data to the bdf file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef writeAnnotation(self, onset_in_seconds, duration_in_seconds, description, str_format='utf-8'):\n        if str_format == 'utf-8':\n            if duration_in_seconds >= 0:\n                return write_annotation_utf8(self.handle, np.round(onset_in_seconds*10000).astype(int), np.round(duration_in_seconds*10000).astype(int), du(description))\n            else:\n                return write_annotation_utf8(self.handle, np.round(onset_in_seconds*10000).astype(int), -1, du(description))\n        else:\n            if duration_in_seconds >= 0:\n                return write_annotation_latin1(self.handle, np.round(onset_in_seconds*10000).astype(int), np.round(duration_in_seconds*10000).astype(int), u(description).encode('latin1'))\n            else:\n                return write_annotation_latin1(self.handle, np.round(onset_in_seconds*10000).astype(int), -1, u(description).encode('latin1'))", "response": "Writes an annotation to the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readAnnotations(self):\n        annot = self.read_annotation()\n        annot = np.array(annot)\n        if (annot.shape[0] == 0):\n            return np.array([]), np.array([]), np.array([])\n        ann_time = self._get_float(annot[:, 0])\n        ann_text = annot[:, 2]\n        ann_text_out = [\"\" for x in range(len(annot[:, 1]))]\n        for i in np.arange(len(annot[:, 1])):\n            ann_text_out[i] = self._convert_string(ann_text[i])\n            if annot[i, 1] == '':\n                annot[i, 1] = '-1'\n        ann_duration = self._get_float(annot[:, 1])\n        return ann_time/10000000, ann_duration, np.array(ann_text_out)", "response": "Reads the annotations from a edf - file and returns the time duration and text of the annotations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the file header as a dict", "response": "def getHeader(self):\n        \"\"\"\n        Returns the file header as dict\n\n        Parameters\n        ----------\n        None\n        \"\"\"\n        return {\"technician\": self.getTechnician(), \"recording_additional\": self.getRecordingAdditional(),\n                \"patientname\": self.getPatientName(), \"patient_additional\": self.getPatientAdditional(),\n                \"patientcode\": self.getPatientCode(), \"equipment\": self.getEquipment(),\n                \"admincode\": self.getAdmincode(), \"gender\": self.getGender(), \"startdate\": self.getStartdatetime(),\n                \"birthdate\": self.getBirthdate()}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the header of one signal as a dict", "response": "def getSignalHeader(self, chn):\n        \"\"\"\n        Returns the  header of one signal as  dicts\n\n        Parameters\n        ----------\n        None\n        \"\"\"\n        return {'label': self.getLabel(chn),\n                'dimension': self.getPhysicalDimension(chn),\n                                 'sample_rate': self.getSampleFrequency(chn),\n                'physical_max':self.getPhysicalMaximum(chn),\n                'physical_min': self.getPhysicalMinimum(chn),\n                'digital_max': self.getDigitalMaximum(chn),\n                'digital_min': self.getDigitalMinimum(chn),\n                'prefilter':self.getPrefilter(chn),\n                'transducer': self.getTransducer(chn)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the header of all signals in the file as array of dicts", "response": "def getSignalHeaders(self):\n        \"\"\"\n        Returns the  header of all signals as array of dicts\n\n        Parameters\n        ----------\n        None\n        \"\"\"\n        signalHeader = []\n        for chn in np.arange(self.signals_in_file):\n            signalHeader.append(self.getSignalHeader(chn))\n        return signalHeader"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getStartdatetime(self):\n        return datetime(self.startdate_year, self.startdate_month, self.startdate_day,\n                                 self.starttime_hour, self.starttime_minute, self.starttime_second)", "response": "Returns the date and starttime as datetime object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getBirthdate(self, string=True):\n\n        if string:\n            return self._convert_string(self.birthdate.rstrip())\n        else:\n            return datetime.strptime(self._convert_string(self.birthdate.rstrip()), \"%d %b %Y\")", "response": "Returns the birthdate as string object"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a numpy array of samplefrequencies of all signals in the file.", "response": "def getSampleFrequencies(self):\n        \"\"\"\n        Returns  samplefrequencies of all signals.\n\n        Parameters\n        ----------\n        None\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> all(f.getSampleFrequencies()==200.0)\n        True\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        return np.array([round(self.samplefrequency(chn))\n                         for chn in np.arange(self.signals_in_file)])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the samplefrequency of signal edfsignal.", "response": "def getSampleFrequency(self,chn):\n        \"\"\"\n        Returns the samplefrequency of signal edfsignal.\n\n        Parameters\n        ----------\n        chn : int\n            channel number\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> f.getSampleFrequency(0)==200.0\n        True\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        if 0 <= chn < self.signals_in_file:\n            return round(self.samplefrequency(chn))\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all the signal labels in the file.", "response": "def getSignalLabels(self):\n        \"\"\"\n        Returns all labels (name) (\"FP1\", \"SaO2\", etc.).\n\n        Parameters\n        ----------\n        None\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> f.getSignalLabels()==['squarewave', 'ramp', 'pulse', 'noise', 'sine 1 Hz', 'sine 8 Hz', 'sine 8.1777 Hz', 'sine 8.5 Hz', 'sine 15 Hz', 'sine 17 Hz', 'sine 50 Hz']\n        True\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        return [self._convert_string(self.signal_label(chn).strip())\n                for chn in np.arange(self.signals_in_file)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the label of the signal chn.", "response": "def getLabel(self,chn):\n        \"\"\"\n        Returns the label (name) of signal chn (\"FP1\", \"SaO2\", etc.).\n\n        Parameters\n        ----------\n        chn : int\n            channel number\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> f.getLabel(0)=='squarewave'\n        True\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        if 0 <= chn < self.signals_in_file:\n            return self._convert_string(self.signal_label(chn).rstrip())\n        else:\n            return self._convert_string('')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getPrefilter(self,chn):\n        if 0 <= chn < self.signals_in_file:\n            return self._convert_string(self.prefilter(chn).rstrip())\n        else:\n            return self._convert_string('')", "response": "Returns the prefilter of signal chn."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getPhysicalMaximum(self,chn=None):\n        if chn is not None:\n            if 0 <= chn < self.signals_in_file:\n                return self.physical_max(chn)\n            else:\n                return 0\n        else:\n            physMax = np.zeros(self.signals_in_file)\n            for i in np.arange(self.signals_in_file):\n                physMax[i] = self.physical_max(i)\n            return physMax", "response": "Returns the maximum physical value of signal edfsignal."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getPhysicalMinimum(self,chn=None):\n        if chn is not None:\n            if 0 <= chn < self.signals_in_file:\n                return self.physical_min(chn)\n            else:\n                return 0\n        else:\n            physMin = np.zeros(self.signals_in_file)\n            for i in np.arange(self.signals_in_file):\n                physMin[i] = self.physical_min(i)\n            return physMin", "response": "Returns the minimum physical value of signal edfsignal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the maximum digital value of signal edfsignal.", "response": "def getDigitalMaximum(self, chn=None):\n        \"\"\"\n        Returns the maximum digital value of signal edfsignal.\n\n        Parameters\n        ----------\n        chn : int\n            channel number\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> f.getDigitalMaximum(0)\n        32767\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        if chn is not None:\n            if 0 <= chn < self.signals_in_file:\n                return self.digital_max(chn)\n            else:\n                return 0\n        else:\n            digMax = np.zeros(self.signals_in_file)\n            for i in np.arange(self.signals_in_file):\n                digMax[i] = self.digital_max(i)\n            return digMax"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the minimum digital value of signal edfsignal.", "response": "def getDigitalMinimum(self, chn=None):\n        \"\"\"\n        Returns the minimum digital value of signal edfsignal.\n\n        Parameters\n        ----------\n        chn : int\n            channel number\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> f.getDigitalMinimum(0)\n        -32768\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        if chn is not None:\n            if 0 <= chn < self.signals_in_file:\n                return self.digital_min(chn)\n            else:\n                return 0\n        else:\n            digMin = np.zeros(self.signals_in_file)\n            for i in np.arange(self.signals_in_file):\n                digMin[i] = self.digital_min(i)\n            return digMin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the transducer of signal chn.", "response": "def getTransducer(self, chn):\n        \"\"\"\n        Returns the transducer of signal chn (\"AgAgCl cup electrodes\", etc.).\n\n        Parameters\n        ----------\n        chn : int\n            channel number\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> f.getTransducer(0)==''\n        True\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        if 0 <= chn < self.signals_in_file:\n            return self._convert_string(self.transducer(chn).rstrip())\n        else:\n            return self._convert_string('')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the physical dimension of signal edfsignal.", "response": "def getPhysicalDimension(self, chn):\n        \"\"\"\n        Returns the physical dimension of signal edfsignal (\"uV\", \"BPM\", \"mA\", \"Degr.\", etc.)\n\n        Parameters\n        ----------\n        chn : int\n            channel number\n\n        Examples\n        --------\n        >>> import pyedflib\n        >>> f = pyedflib.data.test_generator()\n        >>> f.getPhysicalDimension(0)=='uV'\n        True\n        >>> f._close()\n        >>> del f\n\n        \"\"\"\n        if 0 <= chn < self.signals_in_file:\n            return self._convert_string(self.physical_dimension(chn).rstrip())\n        else:\n            return self._convert_string('')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef readSignal(self, chn, start=0, n=None):\n        if start < 0:\n            return np.array([])\n        if n is not None and n < 0:\n            return np.array([])\n        nsamples = self.getNSamples()\n        if chn < len(nsamples):\n            if n is None:\n                n = nsamples[chn]\n            elif n > nsamples[chn]:\n                return np.array([])\n            x = np.zeros(n, dtype=np.float64)\n            self.readsignal(chn, start, n, x)\n            return x\n        else:\n            return np.array([])", "response": "Reads the physical data of a signal chn."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning information about the opened EDF file containing the signal labels sample frequencies and number of samples.", "response": "def file_info_long(self):\n        \"\"\"\n        Returns information about the opened EDF/BDF file\n        \"\"\"\n        self.file_info()\n        for ii in np.arange(self.signals_in_file):\n            print(\"label:\", self.getSignalLabels()[ii], \"fs:\",\n                  self.getSampleFrequencies()[ii], \"nsamples\",\n                  self.getNSamples()[ii])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a stack of traces in a single time series.", "response": "def stackplot_t(tarray, seconds=None, start_time=None, ylabels=None):\n    \"\"\"\n    will plot a stack of traces one above the other assuming\n    tarray.shape =  numSamples, numRows\n    \"\"\"\n    data = tarray\n    numSamples, numRows = tarray.shape\n# data = np.random.randn(numSamples,numRows) # test data\n# data.shape = numSamples, numRows\n    if seconds:\n        t = seconds * np.arange(numSamples, dtype=float)/numSamples\n# import pdb\n# pdb.set_trace()\n        if start_time:\n            t = t+start_time\n            xlm = (start_time, start_time+seconds)\n        else:\n            xlm = (0,seconds)\n\n    else:\n        t = np.arange(numSamples, dtype=float)\n        xlm = (0,numSamples)\n\n    ticklocs = []\n    ax = plt.subplot(111)\n    plt.xlim(*xlm)\n    # xticks(np.linspace(xlm, 10))\n    dmin = data.min()\n    dmax = data.max()\n    dr = (dmax - dmin)*0.7  # Crowd them a bit.\n    y0 = dmin\n    y1 = (numRows-1) * dr + dmax\n    plt.ylim(y0, y1)\n\n    segs = []\n    for i in range(numRows):\n        segs.append(np.hstack((t[:,np.newaxis], data[:,i,np.newaxis])))\n        # print \"segs[-1].shape:\", segs[-1].shape\n        ticklocs.append(i*dr)\n\n    offsets = np.zeros((numRows,2), dtype=float)\n    offsets[:,1] = ticklocs\n\n    lines = LineCollection(segs, offsets=offsets,\n                           transOffset=None,\n                           )\n\n    ax.add_collection(lines)\n\n    # set the yticks to use axes coords on the y axis\n    ax.set_yticks(ticklocs)\n    # ax.set_yticklabels(['PG3', 'PG5', 'PG7', 'PG9'])\n    # if not plt.ylabels:\n    plt.ylabels = [\"%d\" % ii for ii in range(numRows)]\n    ax.set_yticklabels(ylabels)\n\n    plt.xlabel('time (s)')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare parser for the prepare subcommand", "response": "def subcmd_prepare_parser(subcmd):\n    \"\"\" prepare subcommand \"\"\"\n    subcmd.add_argument(\n        '--provider',\n        action='store',\n        dest='provider',\n        help=u'Targeted cluster type',\n        choices=['openshift', 'kubernetes'],\n        default='openshift'\n    )\n\n    subcmd.add_argument(\n        '--dockerfile',\n        '-f',\n        action='store',\n        dest='dockerfile',\n        help=u'Name of Dockerfile to build with'\n    )\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subcmd_remove_parser(subcmd):\n    subcmd.add_argument(\n        '--broker',\n        action='store',\n        dest='broker',\n        help=u'Route to the Ansible Service Broker'\n    )\n    subcmd.add_argument(\n        '--local', '-l',\n        action='store_true',\n        dest='local',\n        help=u'Remove image from internal OpenShift registry',\n        default=False\n    )\n    subcmd.add_argument(\n        '--all',\n        action='store_true',\n        dest='all',\n        help=u'Remove all stored APBs',\n        default=False\n    )\n    subcmd.add_argument(\n        '--id',\n        action='store',\n        dest='id',\n        help=u'ID of APB to remove'\n    )\n    subcmd.add_argument(\n        '--secure',\n        action='store_true',\n        dest='verify',\n        help=u'Verify SSL connection to Ansible Service Broker',\n        default=False\n    )\n    subcmd.add_argument(\n        '--ca-path',\n        action='store',\n        dest='cert',\n        help=u'CA cert to use for verifying SSL connection to Ansible Service Broker',\n        default=None\n    )\n    subcmd.add_argument(\n        '--username',\n        '-u',\n        action='store',\n        default=None,\n        dest='basic_auth_username',\n        help=u'Specify the basic auth username to be used'\n    )\n    subcmd.add_argument(\n        '--password',\n        '-p',\n        action='store',\n        default=None,\n        dest='basic_auth_password',\n        help=u'Specify the basic auth password to be used'\n    )\n    subcmd.add_argument(\n        '--no-relist',\n        action='store_true',\n        dest='no_relist',\n        help=u'Do not relist the catalog after pushing an apb to the broker',\n        default=False\n    )\n    subcmd.add_argument(\n        '--broker-name',\n        action='store',\n        dest='broker_name',\n        help=u'Name of the ServiceBroker k8s resource',\n        default=u'ansible-service-broker'\n    )\n    return", "response": "add subcommand to remove an apb from the catalog"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_path(start, goal, neighbors_fnct, reversePath=False, heuristic_cost_estimate_fnct=lambda a, b: Infinite, distance_between_fnct=lambda a, b: 1.0, is_goal_reached_fnct=lambda a, b: a == b):\n    class FindPath(AStar):\n\n        def heuristic_cost_estimate(self, current, goal):\n            return heuristic_cost_estimate_fnct(current, goal)\n\n        def distance_between(self, n1, n2):\n            return distance_between_fnct(n1, n2)\n\n        def neighbors(self, node):\n            return neighbors_fnct(node)\n\n        def is_goal_reached(self, current, goal):\n            return is_goal_reached_fnct(current, goal)\n    return FindPath().astar(start, goal, reversePath)", "response": "A non - class version of the path finding algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preset(name):\n    def decorator(func):\n        registry.register_preset(func, name)\n        return func\n    return decorator", "response": "Decorator to register a custom preset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(name, type=None, context=None, position=None):\n    def decorator(func):\n        registry.register_check(func, name, type, context, position)\n        return func\n    return decorator", "response": "Decorator to register a check on the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(source, **options):\n    source, options, inspector_settings = _parse_arguments(source, **options)\n\n    # Validate\n    inspector = Inspector(**inspector_settings)\n    report = inspector.inspect(source, **options)\n\n    return report", "response": "Validates a source file and returns a report."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_datapackage(resource_paths):\n    dp = datapackage.Package({\n        'name': 'change-me',\n        'schema': 'tabular-data-package',\n    })\n\n    for path in resource_paths:\n        dp.infer(path)\n\n    return dp", "response": "Create tabular data package with resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init(paths, output, **kwargs):\n    dp = goodtables.init_datapackage(paths)\n\n    click.secho(\n        json_module.dumps(dp.descriptor, indent=4),\n        file=output\n    )\n\n    exit(dp.valid)", "response": "Init data package from list of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clean_empty(d):\n    if not isinstance(d, (dict, list)):\n        return d\n    if isinstance(d, list):\n        return [v for v in (_clean_empty(v) for v in d) if v is not None]\n    return {\n        k: v for k, v in\n        ((k, _clean_empty(v)) for k, v in d.items())\n        if v is not None\n    }", "response": "Remove None values from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inspect(self, source, preset=None, **options):\n\n        # Start timer\n        start = datetime.datetime.now()\n\n        # Prepare preset\n        preset = self.__get_source_preset(source, preset)\n        if preset == 'nested':\n            options['presets'] = self.__presets\n            for s in source:\n                if s.get('preset') is None:\n                    s['preset'] = self.__get_source_preset(s['source'])\n\n        # Prepare tables\n        preset_func = self.__get_preset(preset)['func']\n        warnings, tables = preset_func(source, **options)\n        if len(tables) > self.__table_limit:\n            warnings.append(\n                'Dataset inspection has reached %s table(s) limit' %\n                (self.__table_limit))\n            tables = tables[:self.__table_limit]\n\n        # Collect table reports\n        table_reports = []\n        if tables:\n            tasks = []\n            pool = ThreadPool(processes=len(tables))\n            try:\n                for table in tables:\n                    tasks.append(pool.apply_async(self.__inspect_table, (table,)))\n                for task in tasks:\n                    table_warnings, table_report = task.get()\n                    warnings.extend(table_warnings)\n                    table_reports.append(table_report)\n            finally:\n                pool.terminate()\n\n        # Stop timer\n        stop = datetime.datetime.now()\n\n        # Compose report\n        report = {\n            'time': round((stop - start).total_seconds(), 3),\n            'valid': all(item['valid'] for item in table_reports),\n            'error-count': sum(len(item['errors']) for item in table_reports),\n            'table-count': len(tables),\n            'tables': table_reports,\n            'warnings': warnings,\n            'preset': preset,\n        }\n\n        return report", "response": "Inspects the source and returns a dictionary of the results."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate list of cells from headers fields and values.", "response": "def create_cells(headers, schema_fields, values=None, row_number=None):\n    \"\"\"Create list of cells from headers, fields and values.\n\n    Args:\n        headers (List[str]): The headers values.\n        schema_fields (List[tableschema.field.Field]): The tableschema\n            fields.\n        values (List[Any], optional): The cells values. If not specified,\n            the created cells will have the same values as their\n            corresponding headers. This is useful for specifying headers\n            cells.\n            If the list has any `None` values, as is the case on empty\n            cells, the resulting Cell will have an empty string value. If\n            the `values` list has a different length than the `headers`,\n            the resulting Cell will have value `None`.\n        row_number (int, optional): The row number.\n\n    Returns:\n        List[dict]: List of cells.\n    \"\"\"\n    fillvalue = '_fillvalue'\n    is_header_row = (values is None)\n    cells = []\n\n    iterator = zip_longest(headers, schema_fields, values or [], fillvalue=fillvalue)\n    for column_number, (header, field, value) in enumerate(iterator, start=1):\n        if header == fillvalue:\n            header = None\n        elif is_header_row:\n            value = header\n        if field == fillvalue:\n            field = None\n        if value == fillvalue:\n            value = None\n        elif value is None:\n            value = ''\n\n        cell = create_cell(header, value, field, column_number, row_number)\n        cells.append(cell)\n\n    return cells"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __impl_read_chain(self, start, read_sector_f, read_fat_f):\n        sector = start\n        check = [ sector ] # keep a list of sectors we've already read\n        buffer = StringIO()\n        while sector != ENDOFCHAIN:\n            buffer.write(read_sector_f(sector))\n            next = read_fat_f(sector)\n            if next in check:\n                logging.error('infinite loop detected at {0} to {1} starting at {2}'.format(\n                    sector, next, sector_start))\n                return buffer.getvalue()\n            check.append(next)\n            sector = next\n        return buffer.getvalue()", "response": "Returns the entire contents of a chain starting at the given sector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to ubisoft automatically called when needed", "response": "def connect(self):\n        \"\"\"|coro|\n\n        Connect to ubisoft, automatically called when needed\"\"\"\n        if time.time() < self._login_cooldown:\n            raise FailedToConnect(\"login on cooldown\")\n\n        resp = yield from self.session.post(\"https://connect.ubi.com/ubiservices/v2/profiles/sessions\", headers = {\n            \"Content-Type\": \"application/json\",\n            \"Ubi-AppId\": self.appid,\n            \"Authorization\": \"Basic \" + self.token\n        }, data=json.dumps({\"rememberMe\": True}))\n\n        data = yield from resp.json()\n\n        if \"ticket\" in data:\n            self.key = data.get(\"ticket\")\n            self.sessionid = data.get(\"sessionId\")\n            self.uncertain_spaceid = data.get(\"spaceId\")\n        else:\n            raise FailedToConnect"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_players(self, name=None, platform=None, uid=None):\n\n        if name is None and uid is None:\n            raise TypeError(\"name and uid are both None, exactly one must be given\")\n\n        if name is not None and uid is not None:\n            raise TypeError(\"cannot search by uid and name at the same time, please give one or the other\")\n\n        if platform is None:\n            raise TypeError(\"platform cannot be None\")\n\n        if \"platform\" not in self.cache: self.cache[platform] = {}\n\n        if name:\n            cache_key = \"NAME:%s\" % name\n        else:\n            cache_key = \"UID:%s\" % uid\n\n        if cache_key in self.cache[platform]:\n            if self.cachetime > 0 and self.cache[platform][cache_key][0] < time.time():\n                del self.cache[platform][cache_key]\n            else:\n                return self.cache[platform][cache_key][1]\n\n        if name:\n            data = yield from self.get(\"https://public-ubiservices.ubi.com/v2/profiles?nameOnPlatform=%s&platformType=%s\" % (parse.quote(name), parse.quote(platform)))\n        else:\n            data = yield from self.get(\"https://public-ubiservices.ubi.com/v2/users/%s/profiles?platformType=%s\" % (uid, parse.quote(platform)))\n\n        if \"profiles\" in data:\n            results = [Player(self, x) for x in data[\"profiles\"] if x.get(\"platformType\", \"\") == platform]\n            if len(results) == 0: raise InvalidRequest(\"No results\")\n            if self.cachetime != 0:\n                self.cache[platform][cache_key] = [time.time() + self.cachetime, results]\n            return results\n        else:\n            raise InvalidRequest(\"Missing key profiles in returned JSON object %s\" % str(data))", "response": "get a list of players matching the term on that platform and uid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_player(self, name=None, platform=None, uid=None):\n\n        results = yield from self.get_players(name=name, platform=platform, uid=uid)\n        return results[0]", "response": "|coro| Returns the first player in the set of available players."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a list of information about operators - their badge unique statistic etc.", "response": "def get_operator_definitions(self):\n        \"\"\"|coro|\n\n        Retrieves a list of information about operators - their badge, unique statistic, etc.\n\n        Returns\n        -------\n        dict\n            operators\"\"\"\n        if self._op_definitions is not None:\n            return self._op_definitions\n\n        resp = yield from self.session.get(\"https://game-rainbow6.ubi.com/assets/data/operators.24b865895.json\")\n\n        data = yield from resp.json()\n        self._op_definitions = data\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the operator index from the operator definitions dict", "response": "def get_operator_index(self, name):\n        \"\"\"|coro|\n\n        Gets the operators index from the operator definitions dict\n\n        Returns\n        -------\n        str\n            the operator index\"\"\"\n        opdefs = yield from self.get_operator_definitions()\n\n        name = name.lower()\n        if name not in opdefs:\n            return None\n\n        return opdefs[name][\"index\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_operator_statistic(self, name):\n        opdefs = yield from self.get_operator_definitions()\n\n        name = name.lower()\n        if name not in opdefs:\n            return None\n\n        # some operators (e.g. Kaid and Nomad) don't have a unique statistic sectoin for some reason...\n        if \"uniqueStatistic\" not in opdefs[name] or \"pvp\" not in opdefs[name][\"uniqueStatistic\"]:\n            return None\n\n        return opdefs[name][\"uniqueStatistic\"][\"pvp\"][\"statisticId\"]", "response": "Get the operator unique statistic from the operator definitions dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_operator_badge(self, name):\n        opdefs = yield from self.get_operator_definitions()\n\n        name = name.lower()\n        if name not in opdefs:\n            return None\n\n        badge = opdefs[name][\"badge\"]\n\n        if not badge.startswith(\"http\"):\n            badge = \"https://game-rainbow6.ubi.com/\" + badge\n\n        return badge", "response": "Get the operator badge URL"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the list of api definitions from Ubisoft", "response": "def get_definitions(self):\n        \"\"\"|coro|\n\n        Retrieves the list of api definitions, downloading it from Ubisoft if it hasn't been fetched all ready\n        Primarily for internal use, but could contain useful information.\n\n        Returns\n        -------\n        dict\n            definitions\"\"\"\n        if self._definitions is not None:\n            return self._definitions\n\n        resp = yield from self.session.get(\"https://ubistatic-a.akamaihd.net/0058/prod/assets/data/statistics.definitions.eb165e13.json\")\n\n        data = yield from resp.json()\n        self._definitions = data\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the object s location index for the given key", "response": "def get_object_index(self, key):\n        \"\"\"|coro|\n\n        Mainly for internal use with get_operator,\n        returns the \"location\" index for the key in the definitions\n\n        Returns\n        -------\n        str\n            the object's location index\"\"\"\n        defns = yield from self.get_definitions()\n\n        for x in defns:\n            if key in x and \"objectIndex\" in defns[x]:\n                return defns[x][\"objectIndex\"]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the charm URL for the bracket this rank is in Returns ------- charm URL for the bracket this rank is in", "response": "def get_charm_url(self):\n        \"\"\"Get charm URL for the bracket this rank is in\n\n        Returns\n        -------\n        :class:`str`\n            the URL for the charm\n\n        \"\"\"\n        if self.rank_id <= 4: return self.RANK_CHARMS[0]\n        if self.rank_id <= 8: return self.RANK_CHARMS[1]\n        if self.rank_id <= 12: return self.RANK_CHARMS[2]\n        if self.rank_id <= 16: return self.RANK_CHARMS[3]\n        if self.rank_id <= 19: return self.RANK_CHARMS[4]\n        return self.RANK_CHARMS[5]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the players XP and level", "response": "def load_level(self):\n        \"\"\"|coro|\n\n        Load the players XP and level\"\"\"\n        data = yield from self.auth.get(\"https://public-ubiservices.ubi.com/v1/spaces/%s/sandboxes/%s/r6playerprofile/playerprofile/progressions?profile_ids=%s\" % (self.spaceid, self.platform_url, self.id))\n\n        if \"player_profiles\" in data and len(data[\"player_profiles\"]) > 0:\n            self.xp = data[\"player_profiles\"][0].get(\"xp\", 0)\n            self.level = data[\"player_profiles\"][0].get(\"level\", 0)\n        else:\n            raise InvalidRequest(\"Missing key player_profiles in returned JSON object %s\" % str(data))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_rank(self, region, season=-1):\n        data = yield from self.auth.get(\"https://public-ubiservices.ubi.com/v1/spaces/%s/sandboxes/%s/r6karma/players?board_id=pvp_ranked&profile_ids=%s&region_id=%s&season_id=%s\" % (self.spaceid, self.platform_url, self.id, region, season))\n\n        if \"players\" in data and self.id in data[\"players\"]:\n            regionkey = \"%s:%s\" % (region, season)\n            self.ranks[regionkey] = Rank(data[\"players\"][self.id])\n            return self.ranks[regionkey]\n        else:\n            raise InvalidRequest(\"Missing players key in returned JSON object %s\" % str(data))", "response": "Load the players rank for this region and season."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_rank(self, region, season=-1):\n        cache_key = \"%s:%s\" % (region, season)\n        if cache_key in self.ranks:\n            return self.ranks[cache_key]\n\n        result = yield from self.load_rank(region, season)\n        return result", "response": "Returns the rank for the given region and season"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_all_operators(self):\n        statistics = \"operatorpvp_kills,operatorpvp_death,operatorpvp_roundwon,operatorpvp_roundlost,operatorpvp_meleekills,operatorpvp_totalxp,operatorpvp_headshot,operatorpvp_timeplayed,operatorpvp_dbno\"\n\n        for operator in OperatorStatisticNames:\n            operator_key = yield from self.auth.get_operator_statistic(operator)\n            if operator_key:\n                statistics += \",\" + operator_key\n\n        data = yield from self.auth.get(\"https://public-ubiservices.ubi.com/v1/spaces/%s/sandboxes/%s/playerstats2/statistics?populations=%s&statistics=%s\" % (self.spaceid, self.platform_url, self.id, statistics))\n\n        if \"results\" not in data or not self.id in data[\"results\"]:\n            raise InvalidRequest(\"Missing results key in returned JSON object %s\" % str(data))\n\n        data = data[\"results\"][self.id]\n\n        for operator in OperatorStatisticNames:\n            location = yield from self.auth.get_operator_index(operator.lower())\n            op_data = {x.split(\":\")[0].split(\"_\")[1]: data[x] for x in data if x is not None and location in x}\n            operator_key = yield from self.auth.get_operator_statistic(operator)\n            if operator_key:\n                op_data[\"__statistic_name\"] = operator_key.split(\"_\")[1]\n\n            self.operators[operator.lower()] = Operator(operator.lower(), op_data)\n\n        return self.operators", "response": "Load the player stats for all operators. Returns a dictionary of all operators found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all operators in the database", "response": "def get_all_operators(self):\n        \"\"\"|coro|\n\n        Checks the player stats for all operators, loading them all again if any aren't found\n        This is significantly more efficient than calling get_operator for every operator name.\n\n        Returns\n        -------\n        dict[:class:`Operator`]\n            the dictionary of all operators found\"\"\"\n        if len(self.operators) >= len(OperatorStatisticNames):\n            return self.operators\n\n        result = yield from self.load_all_operators()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the operator from the API and store it in self. operators.", "response": "def load_operator(self, operator):\n        \"\"\"|coro|\n\n        Loads the players stats for the operator\n\n        Parameters\n        ----------\n        operator : str\n            the name of the operator\n\n        Returns\n        -------\n        :class:`Operator`\n            the operator object found\"\"\"\n        location = yield from self.auth.get_operator_index(operator)\n        if location is None:\n            raise ValueError(\"invalid operator %s\" % operator)\n\n        operator_key = yield from self.auth.get_operator_statistic(operator)\n        if operator_key is not None:\n            operator_key = \",\" + operator_key\n        else:\n            operator_key = \"\"\n\n        data = yield from self.auth.get(\"https://public-ubiservices.ubi.com/v1/spaces/%s/sandboxes/%s/playerstats2/statistics?populations=%s&statistics=operatorpvp_kills,operatorpvp_death,operatorpvp_roundwon,operatorpvp_roundlost,operatorpvp_meleekills,operatorpvp_totalxp,operatorpvp_headshot,operatorpvp_timeplayed,operatorpvp_dbno%s\" % (self.spaceid, self.platform_url, self.id, operator_key))\n\n        if not \"results\" in data or not self.id in data[\"results\"]:\n            raise InvalidRequest(\"Missing results key in returned JSON object %s\" % str(data))\n\n        data = data[\"results\"][self.id]\n\n        data = {x.split(\":\")[0].split(\"_\")[1]: data[x] for x in data if x is not None and location in x}\n\n        if operator_key:\n            data[\"__statistic_name\"] = operator_key.split(\"_\")[1]\n\n        #if len(data) < 5:\n        #    raise InvalidRequest(\"invalid number of results for operator in JSON object %s\" % data)\n\n        oper = Operator(operator, data)\n        self.operators[operator] = oper\n        return oper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_operator(self, operator):\n        if operator in self.operators:\n            return self.operators[operator]\n\n        result = yield from self.load_operator(operator)\n        return result", "response": "Get the operator object for the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads the players weapons stats Returns ------- list of all the weapons", "response": "def load_weapons(self):\n        \"\"\"|coro|\n\n        Load the players weapon stats\n\n        Returns\n        -------\n        list[:class:`Weapon`]\n            list of all the weapon objects found\"\"\"\n        data = yield from self.auth.get(\"https://public-ubiservices.ubi.com/v1/spaces/%s/sandboxes/%s/playerstats2/statistics?populations=%s&statistics=weapontypepvp_kills,weapontypepvp_headshot,weapontypepvp_bulletfired,weapontypepvp_bullethit\" % (self.spaceid, self.platform_url, self.id))\n\n        if not \"results\" in data or not self.id in data[\"results\"]:\n            raise InvalidRequest(\"Missing key results in returned JSON object %s\" % str(data))\n\n        data = data[\"results\"][self.id]\n        self.weapons = [Weapon(i) for i in range(7)]\n\n        for x in data:\n            spl = x.split(\":\")\n            category = spl[0].split(\"_\")[1]\n            try:\n                weapontype = int(spl[1]) - 1\n                weapon = self.weapons[weapontype]\n                if category == \"kills\": weapon.kills = data[x]\n                elif category == \"headshot\": weapon.headshots = data[x]\n                elif category == \"bulletfired\": weapon.shots = data[x]\n                elif category == \"bullethit\": weapon.hits = data[x]\n            except (ValueError, TypeError, IndexError):\n                pass\n\n        return self.weapons"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_gamemodes(self):\n\n        stats = yield from self._fetch_statistics(\"secureareapvp_matchwon\", \"secureareapvp_matchlost\", \"secureareapvp_matchplayed\",\n                                                  \"secureareapvp_bestscore\", \"rescuehostagepvp_matchwon\", \"rescuehostagepvp_matchlost\",\n                                                  \"rescuehostagepvp_matchplayed\", \"rescuehostagepvp_bestscore\", \"plantbombpvp_matchwon\",\n                                                  \"plantbombpvp_matchlost\", \"plantbombpvp_matchplayed\", \"plantbombpvp_bestscore\",\n                                                  \"generalpvp_servershacked\", \"generalpvp_serverdefender\", \"generalpvp_serveraggression\",\n                                                  \"generalpvp_hostagerescue\", \"generalpvp_hostagedefense\")\n\n        self.gamemodes = {x: Gamemode(x) for x in GamemodeNames}\n        for name in self.gamemodes:\n            statname, gamemode = name + \"pvp_\", self.gamemodes[name]\n\n            gamemode.best_score = stats.get(statname + \"bestscore\", 0)\n            gamemode.lost = stats.get(statname + \"matchlost\", 0)\n            gamemode.won = stats.get(statname + \"matchwon\", 0)\n            gamemode.played = stats.get(statname + \"matchplayed\", 0)\n\n            if name == \"securearea\":\n                gamemode.areas_secured = stats.get(\"generalpvp_servershacked\", 0)\n                gamemode.areas_defended = stats.get(\"generalpvp_serverdefender\", 0)\n                gamemode.areas_contested = stats.get(\"generalpvp_serveraggression\", 0)\n            elif name == \"rescuehostage\":\n                gamemode.hostages_rescued = stats.get(\"generalpvp_hostagerescue\", 0)\n                gamemode.hostages_defended = stats.get(\"generalpvp_hostagedefense\", 0)\n\n\n\n        return self.gamemodes", "response": "Load the gamemodes of the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_general(self):\n\n        stats = yield from self._fetch_statistics(\"generalpvp_timeplayed\", \"generalpvp_matchplayed\", \"generalpvp_matchwon\",\n                                                  \"generalpvp_matchlost\", \"generalpvp_kills\", \"generalpvp_death\",\n                                                  \"generalpvp_bullethit\", \"generalpvp_bulletfired\", \"generalpvp_killassists\",\n                                                  \"generalpvp_revive\", \"generalpvp_headshot\", \"generalpvp_penetrationkills\",\n                                                  \"generalpvp_meleekills\", \"generalpvp_dbnoassists\", \"generalpvp_suicide\",\n                                                  \"generalpvp_barricadedeployed\", \"generalpvp_reinforcementdeploy\", \"generalpvp_totalxp\",\n                                                  \"generalpvp_rappelbreach\", \"generalpvp_distancetravelled\", \"generalpvp_revivedenied\",\n                                                  \"generalpvp_dbno\", \"generalpvp_gadgetdestroy\", \"generalpvp_blindkills\")\n\n        statname = \"generalpvp_\"\n        self.deaths = stats.get(statname + \"death\", 0)\n        self.penetration_kills = stats.get(statname + \"penetrationkills\", 0)\n        self.matches_won = stats.get(statname + \"matchwon\", 0)\n        self.bullets_hit = stats.get(statname + \"bullethit\", 0)\n        self.melee_kills = stats.get(statname + \"meleekills\", 0)\n        self.bullets_fired = stats.get(statname + \"bulletfired\", 0)\n        self.matches_played = stats.get(statname + \"matchplayed\", 0)\n        self.kill_assists = stats.get(statname + \"killassists\", 0)\n        self.time_played = stats.get(statname + \"timeplayed\", 0)\n        self.revives = stats.get(statname + \"revive\", 0)\n        self.kills = stats.get(statname + \"kills\", 0)\n        self.headshots = stats.get(statname + \"headshot\", 0)\n        self.matches_lost = stats.get(statname + \"matchlost\", 0)\n        self.dbno_assists = stats.get(statname + \"dbnoassists\", 0)\n        self.suicides = stats.get(statname + \"suicide\", 0)\n        self.barricades_deployed = stats.get(statname + \"barricadedeployed\", 0)\n        self.reinforcements_deployed = stats.get(statname + \"reinforcementdeploy\", 0)\n        self.total_xp = stats.get(statname + \"totalxp\", 0)\n        self.rappel_breaches = stats.get(statname + \"rappelbreach\", 0)\n        self.distance_travelled = stats.get(statname + \"distancetravelled\", 0)\n        self.revives_denied = stats.get(statname + \"revivedenied\", 0)\n        self.dbnos = stats.get(statname + \"dbno\", 0)\n        self.gadgets_destroyed = stats.get(statname + \"gadgetdestroy\", 0)\n        self.blind_kills = stats.get(statname + \"blindkills\")", "response": "Load the general stats for the current Player."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_queues(self):\n\n        stats = yield from self._fetch_statistics(\"casualpvp_matchwon\", \"casualpvp_matchlost\", \"casualpvp_timeplayed\",\n                                                  \"casualpvp_matchplayed\", \"casualpvp_kills\", \"casualpvp_death\",\n                                                  \"rankedpvp_matchwon\", \"rankedpvp_matchlost\", \"rankedpvp_timeplayed\",\n                                                  \"rankedpvp_matchplayed\", \"rankedpvp_kills\", \"rankedpvp_death\")\n\n        self.ranked = GameQueue(\"ranked\")\n        self.casual = GameQueue(\"casual\")\n\n        for gq in (self.ranked, self.casual):\n            statname = gq.name + \"pvp_\"\n\n            gq.won = stats.get(statname + \"matchwon\", 0)\n            gq.lost = stats.get(statname + \"matchlost\", 0)\n            gq.time_played = stats.get(statname + \"timeplayed\", 0)\n            gq.played = stats.get(statname + \"matchplayed\", 0)\n            gq.kills = stats.get(statname + \"kills\", 0)\n            gq.deaths = stats.get(statname + \"death\", 0)", "response": "Load the players game queues."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_terrohunt(self):\n        stats = yield from self._fetch_statistics(\"generalpve_dbnoassists\", \"generalpve_death\", \"generalpve_revive\",\n                                                  \"generalpve_matchwon\", \"generalpve_suicide\", \"generalpve_servershacked\",\n                                                  \"generalpve_serverdefender\", \"generalpve_barricadedeployed\", \"generalpve_reinforcementdeploy\",\n                                                  \"generalpve_kills\", \"generalpve_hostagedefense\", \"generalpve_bulletfired\",\n                                                  \"generalpve_matchlost\", \"generalpve_killassists\", \"generalpve_totalxp\",\n                                                  \"generalpve_hostagerescue\", \"generalpve_penetrationkills\", \"generalpve_meleekills\",\n                                                  \"generalpve_rappelbreach\", \"generalpve_distancetravelled\", \"generalpve_matchplayed\",\n                                                  \"generalpve_serveraggression\", \"generalpve_timeplayed\", \"generalpve_revivedenied\",\n                                                  \"generalpve_dbno\", \"generalpve_bullethit\", \"generalpve_blindkills\", \"generalpve_headshot\",\n                                                  \"generalpve_gadgetdestroy\", \"generalpve_accuracy\")\n\n        self.terrorist_hunt = GameQueue(\"terrohunt\")\n\n        statname = \"generalpve_\"\n        self.terrorist_hunt.deaths = stats.get(statname + \"death\", 0)\n        self.terrorist_hunt.penetration_kills = stats.get(statname + \"penetrationkills\", 0)\n        self.terrorist_hunt.matches_won = stats.get(statname + \"matchwon\", 0)\n        self.terrorist_hunt.bullets_hit = stats.get(statname + \"bullethit\", 0)\n        self.terrorist_hunt.melee_kills = stats.get(statname + \"meleekills\", 0)\n        self.terrorist_hunt.bullets_fired = stats.get(statname + \"bulletfired\", 0)\n        self.terrorist_hunt.matches_played = stats.get(statname + \"matchplayed\", 0)\n        self.terrorist_hunt.kill_assists = stats.get(statname + \"killassists\", 0)\n        self.terrorist_hunt.time_played = stats.get(statname + \"timeplayed\", 0)\n        self.terrorist_hunt.revives = stats.get(statname + \"revive\", 0)\n        self.terrorist_hunt.kills = stats.get(statname + \"kills\", 0)\n        self.terrorist_hunt.headshots = stats.get(statname + \"headshot\", 0)\n        self.terrorist_hunt.matches_lost = stats.get(statname + \"matchlost\", 0)\n        self.terrorist_hunt.dbno_assists = stats.get(statname + \"dbnoassists\", 0)\n        self.terrorist_hunt.suicides = stats.get(statname + \"suicide\", 0)\n        self.terrorist_hunt.barricades_deployed = stats.get(statname + \"barricadedeployed\", 0)\n        self.terrorist_hunt.reinforcements_deployed = stats.get(statname + \"reinforcementdeploy\", 0)\n        self.terrorist_hunt.total_xp = stats.get(statname + \"totalxp\", 0)\n        self.terrorist_hunt.rappel_breaches = stats.get(statname + \"rappelbreach\", 0)\n        self.terrorist_hunt.distance_travelled = stats.get(statname + \"distancetravelled\", 0)\n        self.terrorist_hunt.revives_denied = stats.get(statname + \"revivedenied\", 0)\n        self.terrorist_hunt.dbnos = stats.get(statname + \"dbno\", 0)\n        self.terrorist_hunt.gadgets_destroyed = stats.get(statname + \"gadgetdestroy\", 0)\n        self.terrorist_hunt.areas_secured = stats.get(statname + \"servershacked\", 0)\n        self.terrorist_hunt.areas_defended = stats.get(statname + \"serverdefender\", 0)\n        self.terrorist_hunt.areas_contested = stats.get(statname + \"serveraggression\", 0)\n        self.terrorist_hunt.hostages_rescued = stats.get(statname + \"hostagerescue\", 0)\n        self.terrorist_hunt.hostages_defended = stats.get(statname + \"hostagedefense\", 0)\n        self.terrorist_hunt.blind_kills = stats.get(statname + \"blindkills\", 0)\n\n        return self.terrorist_hunt", "response": "Load the player s general stats for the terrorist hunt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self, method, *args, **kargs):\n        result = None\n\n        '''\n            max 10 rechecks\n        '''\n        for i in range(0, 10):\n            try:\n\n                method_map = {\n                    'get_lead_by_id': self.get_lead_by_id,\n                    'get_multiple_leads_by_filter_type': self.get_multiple_leads_by_filter_type,\n                    'get_multiple_leads_by_list_id': self.get_multiple_leads_by_list_id,\n                    'get_multiple_leads_by_list_id_yield': self.get_multiple_leads_by_list_id_yield,\n                    'get_multiple_leads_by_program_id': self.get_multiple_leads_by_program_id,\n                    'get_multiple_leads_by_program_id_yield': self.get_multiple_leads_by_program_id_yield,\n                    'change_lead_program_status': self.change_lead_program_status,\n                    'create_update_leads': self.create_update_leads,\n                    'associate_lead': self.associate_lead,\n                    'push_lead': self.push_lead,\n                    'merge_lead': self.merge_lead,\n                    'get_lead_partitions': self.get_lead_partitions,\n                    'create_list': self.create_list,\n                    'update_list': self.update_list,\n                    'delete_list': self.delete_list,\n                    'get_list_by_id': self.get_list_by_id,\n                    'get_list_by_name': self.get_list_by_name,\n                    'get_multiple_lists': self.get_multiple_lists,\n                    'browse_lists': self.browse_lists,\n                    'add_leads_to_list': self.add_leads_to_list,\n                    'remove_leads_from_list': self.remove_leads_from_list,\n                    'member_of_list': self.member_of_list,\n                    'get_campaign_by_id': self.get_campaign_by_id,\n                    'get_multiple_campaigns': self.get_multiple_campaigns,\n                    'schedule_campaign': self.schedule_campaign,\n                    'request_campaign': self.request_campaign,\n                    'import_lead': self.import_lead,\n                    'get_import_lead_status': self.get_import_lead_status,\n                    'get_import_failure_file': self.get_import_failure_file,\n                    'get_import_warning_file': self.get_import_warning_file,\n                    'describe': self.describe,\n                    'get_activity_types': self.get_activity_types,\n                    'get_paging_token': self.get_paging_token,\n                    'get_lead_activities': self.get_lead_activities,\n                    'get_lead_activities_yield': self.get_lead_activities_yield,\n                    'get_lead_changes': self.get_lead_changes,\n                    'get_lead_changes_yield': self.get_lead_changes_yield,\n                    'add_custom_activities': self.add_custom_activities,\n                    'get_daily_usage': self.get_daily_usage,\n                    'get_last_7_days_usage': self.get_last_7_days_usage,\n                    'get_daily_errors': self.get_daily_errors,\n                    'get_last_7_days_errors': self.get_last_7_days_errors,\n                    'delete_lead': self.delete_lead,\n                    'get_deleted_leads': self.get_deleted_leads,\n                    'update_leads_partition': self.update_leads_partition,\n                    'create_folder': self.create_folder,\n                    'get_folder_by_id': self.get_folder_by_id,\n                    'get_folder_by_name': self.get_folder_by_name,\n                    'get_folder_contents': self.get_folder_contents,\n                    'update_folder': self.update_folder,\n                    'delete_folder': self.delete_folder,\n                    'browse_folders': self.browse_folders,\n                    'create_token': self.create_token,\n                    'get_tokens': self.get_tokens,\n                    'delete_tokens': self.delete_tokens,\n                    'create_email_template': self.create_email_template,\n                    'get_email_template_by_id': self.get_email_template_by_id,\n                    'get_email_template_by_name': self.get_email_template_by_name,\n                    'update_email_template': self.update_email_template,\n                    'delete_email_template': self.delete_email_template,\n                    'get_email_templates': self.get_email_templates,\n                    'get_email_templates_yield': self.get_email_templates_yield,\n                    'get_email_template_content': self.get_email_template_content,\n                    'update_email_template_content': self.update_email_template_content,\n                    'approve_email_template': self.approve_email_template,\n                    'unapprove_email_template': self.unapprove_email_template,\n                    'discard_email_template_draft': self.discard_email_template_draft,\n                    'clone_email_template': self.clone_email_template,\n                    'create_email': self.create_email,\n                    'get_email_by_id': self.get_email_by_id,\n                    'get_email_by_name': self.get_email_by_name,\n                    'delete_email': self.delete_email,\n                    'update_email': self.update_email,\n                    'get_emails': self.get_emails,\n                    'get_emails_yield': self.get_emails_yield,\n                    'get_email_content': self.get_email_content,\n                    'update_email_content': self.update_email_content,\n                    'update_email_content_in_editable_section': self.update_email_content_in_editable_section,\n                    'get_email_dynamic_content': self.get_email_dynamic_content,\n                    'update_email_dynamic_content': self.update_email_dynamic_content,\n                    'approve_email': self.approve_email,\n                    'unapprove_email': self.unapprove_email,\n                    'discard_email_draft': self.discard_email_draft,\n                    'clone_email': self.clone_email,\n                    'send_sample_email': self.send_sample_email,\n                    'get_email_full_content': self.get_email_full_content,\n                    'create_landing_page': self.create_landing_page,\n                    'get_landing_page_by_id': self.get_landing_page_by_id,\n                    'get_landing_page_by_name': self.get_landing_page_by_name,\n                    'delete_landing_page': self.delete_landing_page,\n                    'update_landing_page': self.update_landing_page,\n                    'get_landing_pages': self.get_landing_pages,\n                    'get_landing_pages_yield': self.get_landing_pages_yield,\n                    'get_landing_page_content': self.get_landing_page_content,\n                    'create_landing_page_content_section': self.create_landing_page_content_section,\n                    'update_landing_page_content_section': self.update_landing_page_content_section,\n                    'delete_landing_page_content_section': self.delete_landing_page_content_section,\n                    'get_landing_page_dynamic_content': self.get_landing_page_dynamic_content,\n                    'update_landing_page_dynamic_content': self.update_landing_page_dynamic_content,\n                    'approve_landing_page': self.approve_landing_page,\n                    'unapprove_landing_page': self.unapprove_landing_page,\n                    'discard_landing_page_draft': self.discard_landing_page_draft,\n                    'clone_landing_page': self.clone_landing_page,\n                    'create_form': self.create_form,\n                    'get_form_by_id': self.get_form_by_id,\n                    'get_form_by_name': self.get_form_by_name,\n                    'delete_form': self.delete_form,\n                    'update_form': self.update_form,\n                    'get_forms': self.get_forms,\n                    'get_forms_yield': self.get_forms_yield,\n                    'get_form_fields': self.get_form_fields,\n                    'create_form_field': self.create_form_field,\n                    'update_form_field': self.update_form_field,\n                    'delete_form_field': self.delete_form_field,\n                    'approve_form': self.approve_form,\n                    'unapprove_form': self.unapprove_form,\n                    'discard_form_draft': self.discard_form_draft,\n                    'clone_form': self.clone_form,\n                    'create_file': self.create_file,\n                    'get_file_by_id': self.get_file_by_id,\n                    'get_file_by_name': self.get_file_by_name,\n                    'list_files': self.list_files,\n                    'get_files_yield': self.get_files_yield,\n                    'update_file_content': self.update_file_content,\n                    'create_snippet': self.create_snippet,\n                    'get_snippet_by_id': self.get_snippet_by_id,\n                    'delete_snippet': self.delete_snippet,\n                    'update_snippet': self.update_snippet,\n                    'get_snippets': self.get_snippets,\n                    'get_snippets_yield': self.get_snippets_yield,\n                    'get_snippet_content': self.get_snippet_content,\n                    'update_snippet_content': self.update_snippet_content,\n                    'approve_snippet': self.approve_snippet,\n                    'unapprove_snippet': self.unapprove_snippet,\n                    'discard_snippet_draft': self.discard_snippet_draft,\n                    'clone_snippet': self.clone_snippet,\n                    'update_snippet_dynamic_content': self.update_snippet_dynamic_content,\n                    'get_snippet_dynamic_content': self.get_snippet_dynamic_content,\n                    'get_segmentations': self.get_segmentations,\n                    'get_segments': self.get_segments,\n                    'create_landing_page_template': self.create_landing_page_template,\n                    'get_landing_page_template_by_id': self.get_landing_page_template_by_id,\n                    'get_landing_page_template_by_name': self.get_landing_page_template_by_name,\n                    'get_landing_page_templates': self.get_landing_page_templates,\n                    'get_landing_page_templates_yield': self.get_landing_page_templates_yield,\n                    'get_landing_page_template_content': self.get_landing_page_template_content,\n                    'update_landing_page_template_content': self.update_landing_page_template_content,\n                    'update_landing_page_template': self.update_landing_page_template,\n                    'delete_landing_page_template': self.delete_landing_page_template,\n                    'approve_landing_page_template': self.approve_landing_page_template,\n                    'unapprove_landing_page_template': self.unapprove_landing_page_template,\n                    'discard_landing_page_template_draft': self.discard_landing_page_template_draft,\n                    'clone_landing_page_template': self.clone_landing_page_template,\n                    'create_program': self.create_program,\n                    'get_program_by_id': self.get_program_by_id,\n                    'get_program_by_name': self.get_program_by_name,\n                    'get_program_by_tag_type': self.get_program_by_tag_type,\n                    'update_program': self.update_program,\n                    'delete_program': self.delete_program,\n                    'browse_programs': self.browse_programs,\n                    'get_programs_yield': self.get_programs_yield,\n                    'clone_program': self.clone_program,\n                    'approve_program': self.approve_program,\n                    'unapprove_program': self.unapprove_program,\n                    'get_channels': self.get_channels,\n                    'get_channel_by_name': self.get_channel_by_name,\n                    'get_tags': self.get_tags,\n                    'get_tag_by_name': self.get_tag_by_name,\n                    'get_list_of_custom_objects': self.get_list_of_custom_objects,\n                    'describe_custom_object': self.describe_custom_object,\n                    'create_update_custom_objects': self.create_update_custom_objects,\n                    'delete_custom_objects': self.delete_custom_objects,\n                    'get_custom_objects': self.get_custom_objects,\n                    'describe_opportunity': self.describe_opportunity,\n                    'create_update_opportunities': self.create_update_opportunities,\n                    'delete_opportunities': self.delete_opportunities,\n                    'get_opportunities': self.get_opportunities,\n                    'describe_opportunity_role': self.describe_opportunity_role,\n                    'create_update_opportunities_roles': self.create_update_opportunities_roles,\n                    'delete_opportunity_roles': self.delete_opportunity_roles,\n                    'get_opportunity_roles': self.get_opportunity_roles,\n                    'describe_company': self.describe_company,\n                    'create_update_companies': self.create_update_companies,\n                    'delete_companies': self.delete_companies,\n                    'get_companies': self.get_companies,\n                    'describe_sales_person': self.describe_sales_person,\n                    'create_update_sales_persons': self.create_update_sales_persons,\n                    'delete_sales_persons': self.delete_sales_persons,\n                    'get_sales_persons': self.get_sales_persons,\n                    'get_custom_activity_types': self.get_custom_activity_types,\n                    'describe_custom_activity_type': self.describe_custom_activity_type,\n                    'create_custom_activity_type': self.create_custom_activity_type,\n                    'update_custom_activity_type': self.update_custom_activity_type,\n                    'approve_custom_activity_type': self.approve_custom_activity_type,\n                    'create_custom_activity_type_attribute': self.create_custom_activity_type_attribute,\n                    'discard_custom_activity_type_draft': self.discard_custom_activity_type_draft,\n                    'delete_custom_activity_type': self.delete_custom_activity_type,\n                    'update_custom_activity_type_attribute': self.update_custom_activity_type_attribute,\n                    'delete_custom_activity_type_attribute': self.delete_custom_activity_type_attribute,\n                    'get_leads_export_jobs_list': self.get_leads_export_jobs_list,\n                    'get_activities_export_jobs_list': self.get_activities_export_jobs_list,\n                    'create_leads_export_job': self.create_leads_export_job,\n                    'create_activities_export_job': self.create_activities_export_job,\n                    'enqueue_leads_export_job': self.enqueue_leads_export_job,\n                    'enqueue_activities_export_job': self.enqueue_activities_export_job,\n                    'cancel_leads_export_job': self.cancel_leads_export_job,\n                    'cancel_activities_export_job': self.cancel_activities_export_job,\n                    'get_leads_export_job_status': self.get_leads_export_job_status,\n                    'get_activities_export_job_status': self.get_activities_export_job_status,\n                    'get_leads_export_job_file': self.get_leads_export_job_file,\n                    'get_activities_export_job_file': self.get_activities_export_job_file\n                }\n                result = method_map[method](*args, **kargs)\n            except MarketoException as e:\n                '''\n                601 -> auth token not valid\n                602 -> auth token expired\n                '''\n                if e.code in ['601', '602']:\n                    self.authenticate()\n                    continue\n                else:\n                    raise Exception({'message': e.message, 'code': e.code})\n            break\n        return result", "response": "Execute the method in the base class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the libdmtx shared library. Returns the object that is used to store the results in the global list LIBDMTX and EXTERNAL_DEPENDENCIES.", "response": "def load_libdmtx():\n    \"\"\"Loads the libdmtx shared library.\n\n    Populates the globals LIBDMTX and EXTERNAL_DEPENDENCIES.\n    \"\"\"\n    global LIBDMTX\n    global EXTERNAL_DEPENDENCIES\n    if not LIBDMTX:\n        LIBDMTX = dmtx_library.load()\n        EXTERNAL_DEPENDENCIES = [LIBDMTX]\n\n    return LIBDMTX"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a foreign function exported by libdmtx.", "response": "def libdmtx_function(fname, restype, *args):\n    \"\"\"Returns a foreign function exported by `libdmtx`.\n\n    Args:\n        fname (:obj:`str`): Name of the exported function as string.\n        restype (:obj:): Return type - one of the `ctypes` primitive C data\n        types.\n        *args: Arguments - a sequence of `ctypes` primitive C data types.\n\n    Returns:\n        cddl.CFunctionType: A wrapper around the function.\n    \"\"\"\n    prototype = CFUNCTYPE(restype, *args)\n    return prototype((fname, load_libdmtx()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the libdmtx shared library.", "response": "def load():\n    \"\"\"Loads the libdmtx shared library.\n    \"\"\"\n    if 'Windows' == platform.system():\n        # Possible scenarios here\n        #   1. Run from source, DLLs are in pylibdmtx directory\n        #       cdll.LoadLibrary() imports DLLs in repo root directory\n        #   2. Wheel install into CPython installation\n        #       cdll.LoadLibrary() imports DLLs in package directory\n        #   3. Wheel install into virtualenv\n        #       cdll.LoadLibrary() imports DLLs in package directory\n        #   4. Frozen\n        #       cdll.LoadLibrary() imports DLLs alongside executable\n\n        fname = _windows_fname()\n        try:\n            libdmtx = cdll.LoadLibrary(fname)\n        except OSError:\n            libdmtx = cdll.LoadLibrary(\n                str(Path(__file__).parent.joinpath(fname))\n            )\n    else:\n        # Assume a shared library on the path\n        path = find_library('dmtx')\n        if not path:\n            raise ImportError('Unable to find dmtx shared library')\n        libdmtx = cdll.LoadLibrary(path)\n\n    return libdmtx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _region(decoder, timeout):\n    region = dmtxRegionFindNext(decoder, timeout)\n    try:\n        yield region\n    finally:\n        if region:\n            dmtxRegionDestroy(byref(region))", "response": "A context manager for dmtxRegionFindNext and dmtxRegionDestroy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decoded_matrix_region(decoder, region, corrections):\n    message = dmtxDecodeMatrixRegion(decoder, region, corrections)\n    try:\n        yield message\n    finally:\n        if message:\n            dmtxMessageDestroy(byref(message))", "response": "A context manager for the DmtxMessage created and destoyed by dmtxDecodeMatrixRegion and dmtxMessageDestroy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decode_region(decoder, region, corrections, shrink):\n    with _decoded_matrix_region(decoder, region, corrections) as msg:\n        if msg:\n            # Coordinates\n            p00 = DmtxVector2()\n            p11 = DmtxVector2(1.0, 1.0)\n            dmtxMatrix3VMultiplyBy(\n                p00,\n                region.contents.fit2raw\n            )\n            dmtxMatrix3VMultiplyBy(p11, region.contents.fit2raw)\n            x0 = int((shrink * p00.X) + 0.5)\n            y0 = int((shrink * p00.Y) + 0.5)\n            x1 = int((shrink * p11.X) + 0.5)\n            y1 = int((shrink * p11.Y) + 0.5)\n            return Decoded(\n                string_at(msg.contents.output),\n                Rect(x0, y0, x1 - x0, y1 - y0)\n            )\n        else:\n            return None", "response": "Decodes and returns the value in a region."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the image data in the order they appear in the base image.", "response": "def _pixel_data(image):\n    \"\"\"Returns (pixels, width, height, bpp)\n\n    Returns:\n        :obj: `tuple` (pixels, width, height, bpp)\n    \"\"\"\n    # Test for PIL.Image and numpy.ndarray without requiring that cv2 or PIL\n    # are installed.\n    if 'PIL.' in str(type(image)):\n        pixels = image.tobytes()\n        width, height = image.size\n    elif 'numpy.ndarray' in str(type(image)):\n        if 'uint8' != str(image.dtype):\n            image = image.astype('uint8')\n        try:\n            pixels = image.tobytes()\n        except AttributeError:\n            # `numpy.ndarray.tobytes()` introduced in `numpy` 1.9.0 - use the\n            # older `tostring` method.\n            pixels = image.tostring()\n        height, width = image.shape[:2]\n    else:\n        # image should be a tuple (pixels, width, height)\n        pixels, width, height = image\n\n        # Check dimensions\n        if 0 != len(pixels) % (width * height):\n            raise PyLibDMTXError(\n                (\n                    'Inconsistent dimensions: image data of {0} bytes is not '\n                    'divisible by (width x height = {1})'\n                ).format(len(pixels), (width * height))\n            )\n\n    # Compute bits-per-pixel\n    bpp = 8 * len(pixels) // (width * height)\n    if bpp not in _PACK_ORDER:\n        raise PyLibDMTXError(\n            'Unsupported bits-per-pixel: [{0}] Should be one of {1}'.format(\n                bpp, sorted(_PACK_ORDER.keys())\n            )\n        )\n\n    return pixels, width, height, bpp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode(image, timeout=None, gap_size=None, shrink=1, shape=None,\n           deviation=None, threshold=None, min_edge=None, max_edge=None,\n           corrections=None, max_count=None):\n    \"\"\"Decodes datamatrix barcodes in `image`.\n\n    Args:\n        image: `numpy.ndarray`, `PIL.Image` or tuple (pixels, width, height)\n        timeout (int): milliseconds\n        gap_size (int):\n        shrink (int):\n        shape (int):\n        deviation (int):\n        threshold (int):\n        min_edge (int):\n        max_edge (int):\n        corrections (int):\n        max_count (int): stop after reading this many barcodes. `None` to read\n            as many as possible.\n\n    Returns:\n        :obj:`list` of :obj:`Decoded`: The values decoded from barcodes.\n    \"\"\"\n    dmtx_timeout = None\n    if timeout:\n        now = dmtxTimeNow()\n        dmtx_timeout = dmtxTimeAdd(now, timeout)\n\n    if max_count is not None and max_count < 1:\n        raise ValueError('Invalid max_count [{0}]'.format(max_count))\n\n    pixels, width, height, bpp = _pixel_data(image)\n\n    results = []\n    with _image(\n        cast(pixels, c_ubyte_p), width, height, _PACK_ORDER[bpp]\n    ) as img:\n        with _decoder(img, shrink) as decoder:\n            properties = [\n                (DmtxProperty.DmtxPropScanGap, gap_size),\n                (DmtxProperty.DmtxPropSymbolSize, shape),\n                (DmtxProperty.DmtxPropSquareDevn, deviation),\n                (DmtxProperty.DmtxPropEdgeThresh, threshold),\n                (DmtxProperty.DmtxPropEdgeMin, min_edge),\n                (DmtxProperty.DmtxPropEdgeMax, max_edge)\n            ]\n\n            # Set only those properties with a non-None value\n            for prop, value in ((p, v) for p, v in properties if v is not None):\n                dmtxDecodeSetProp(decoder, prop, value)\n\n            if not corrections:\n                corrections = DmtxUndefined\n\n            while True:\n                with _region(decoder, dmtx_timeout) as region:\n                    # Finished file or ran out of time before finding another\n                    # region\n                    if not region:\n                        break\n                    else:\n                        # Decoded\n                        res = _decode_region(\n                            decoder, region, corrections, shrink\n                        )\n                        if res:\n                            results.append(res)\n\n                            # Stop if we've reached maximum count\n                            if max_count and len(results) == max_count:\n                                break\n\n    return results", "response": "Decodes datamatrix barcodes in image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(data, scheme=None, size=None):\n\n    size = size if size else 'ShapeAuto'\n    size_name = '{0}{1}'.format(ENCODING_SIZE_PREFIX, size)\n    if not hasattr(DmtxSymbolSize, size_name):\n        raise PyLibDMTXError(\n            'Invalid size [{0}]: should be one of {1}'.format(\n                size, ENCODING_SIZE_NAMES\n            )\n        )\n    size = getattr(DmtxSymbolSize, size_name)\n\n    scheme = scheme if scheme else 'Ascii'\n    scheme_name = '{0}{1}'.format(\n        ENCODING_SCHEME_PREFIX, scheme.capitalize()\n    )\n    if not hasattr(DmtxScheme, scheme_name):\n        raise PyLibDMTXError(\n            'Invalid scheme [{0}]: should be one of {1}'.format(\n                scheme, ENCODING_SCHEME_NAMES\n            )\n        )\n    scheme = getattr(DmtxScheme, scheme_name)\n\n    with _encoder() as encoder:\n        dmtxEncodeSetProp(encoder, DmtxProperty.DmtxPropScheme, scheme)\n        dmtxEncodeSetProp(encoder, DmtxProperty.DmtxPropSizeRequest, size)\n\n        if dmtxEncodeDataMatrix(encoder, len(data), cast(data, c_ubyte_p)) == 0:\n            raise PyLibDMTXError(\n                'Could not encode data, possibly because the image is not '\n                'large enough to contain the data'\n            )\n\n        w, h, bpp = map(\n            partial(dmtxImageGetProp, encoder[0].image),\n            (\n                DmtxProperty.DmtxPropWidth, DmtxProperty.DmtxPropHeight,\n                DmtxProperty.DmtxPropBitsPerPixel\n            )\n        )\n        size = w * h * bpp // 8\n        pixels = cast(\n            encoder[0].image[0].pxl, ctypes.POINTER(ctypes.c_ubyte * size)\n        )\n        return Encoded(\n            width=w, height=h, bpp=bpp, pixels=ctypes.string_at(pixels, size)\n        )", "response": "Encodes data in a DataMatrix image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the Excel and draw the wulf net and Plot points job done~", "response": "def lines(self, Width=1, Color='k'):\n        '''\n        read the Excel, then draw the wulf net and Plot points, job done~\n        '''\n        self.axes.clear()\n\n        # self.axes.set_xlim(-90, 450)\n        self.axes.set_ylim(0, 90)\n\n        titles = list('NWSE')\n\n        titles = ['N', '330', '300', 'W', '240', '210', 'S', '150', '120', 'E', '60', '30']\n        self.n = len(titles)\n        self.angles = np.arange(90, 90 + 360, 360.0 / self.n)\n\n        self.angles = np.array([90., 120., 150., 180., 210., 240., 270., 300., 330.,\n                                360., 30., 60.])\n        self.axes.set_thetagrids(self.angles, labels=titles, fontsize=14)\n\n        raw = self._df\n\n        Data = []\n        Labels = []\n\n\n        if (int(self.type_slider.value()) == 0):\n            list1 = [self.eqan(x) for x in range(15, 90, 15)]\n        else:\n            list1 = [self.eqar(x) for x in range(15, 90, 15)]\n\n        list2 = [str(x) for x in range(15, 90, 15)]\n        self.axes.set_rgrids(list1, list2)\n\n        for i in range(len(raw)):\n            Data.append([raw.at[i, 'Dip'], raw.at[i, 'Dip-Angle'], raw.at[i, 'Color'],\n                         raw.at[i, 'Width'], raw.at[i, 'Alpha'], raw.at[i, 'Label']])\n            Dip = raw.at[i, 'Dip']\n            Dip_Angle = raw.at[i, 'Dip-Angle']\n\n            Label = raw.at[i, 'Label']\n            if (Label not in Labels):\n                Labels.append(Label)\n            else:\n                Label = ''\n\n            Width = 1\n            Color = 'red'\n            Alpha = 0.8\n            Marker = 'o'\n            Size = 50\n\n            Setting = [Width, Color, Alpha, Marker, Size]\n\n            Width = raw.at[i, 'Width']\n            Color = raw.at[i, 'Color']\n            Alpha = raw.at[i, 'Alpha']\n            Marker = raw.at[i, 'Marker']\n            Size = raw.at[i, 'Size']\n\n            if (Color not in Setting or Color != ''):\n                Width = raw.at[i, 'Width']\n                Color = raw.at[i, 'Color']\n                Alpha = raw.at[i, 'Alpha']\n                Marker = raw.at[i, 'Marker']\n                Size = raw.at[i, 'Size']\n\n                Setting = [Width, Color, Alpha, Marker, Size]\n            r = np.arange(Dip - 90, Dip + 91, 1)\n            BearR = [np.radians(-A + 90) for A in r]\n\n            if (int(self.type_slider.value()) == 0):\n                Line = (self.eqan(self.getangular(Dip_Angle, Dip, r)))\n            else:\n                Line = (self.eqar(self.getangular(Dip_Angle, Dip, r)))\n\n            self.axes.plot(BearR, Line, color=Color, linewidth=Width, alpha=Alpha, label=Label)\n\n        # self.axes.thetagrids(range(360 + 90, 0 + 90, -30), [str(x) for x in range(0, 360, 30)])\n\n        if (self.legend_cb.isChecked()):\n            self.axes.legend(bbox_to_anchor=(1.5, 1), loc=2, borderaxespad=0, prop=fontprop)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_edge(edges, edge_points, coords, i, j):\n    if (i, j) in edges or (j, i) in edges:\n        # already added\n        return( edges.add((i, j)), edge_points.append(coords[[i, j]]))", "response": "Add a line between the i - th and j - th points"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the alpha shape of a set of points.", "response": "def alpha_shape(points, alpha):\n    \"\"\"\n    Compute the alpha shape (concave hull) of a set\n    of points.\n    @param points: Iterable container of points.\n    @param alpha: alpha value to influence the\n        gooeyness of the border. Smaller numbers\n        don't fall inward as much as larger numbers.\n        Too large, and you lose everything!\n    \"\"\"\n    if len(points) < 4:\n        # When you have a triangle, there is no sense\n        # in computing an alpha shape.\n        return geometry.MultiPoint(list(points)).convex_hull\n\n    #coords = np.array([point.coords[0] for point in points])\n\n    coords = np.array(points)\n\n    print(coords)\n\n    tri = Delaunay(coords)\n    edges = set()\n    edge_points = []\n    # loop over triangles:\n    # ia, ib, ic = indices of corner points of the\n    # triangle\n    for ia, ib, ic in tri.vertices:\n        pa = coords[ia]\n        pb = coords[ib]\n        pc = coords[ic]\n        # Lengths of sides of triangle\n        a = math.sqrt((pa[0]-pb[0])**2 + (pa[1]-pb[1])**2)\n        b = math.sqrt((pb[0]-pc[0])**2 + (pb[1]-pc[1])**2)\n        c = math.sqrt((pc[0]-pa[0])**2 + (pc[1]-pa[1])**2)\n        # Semiperimeter of triangle\n        s = (a + b + c)/2.0\n        # Area of triangle by Heron's formula\n        area = math.sqrt(s*(s-a)*(s-b)*(s-c))\n        circum_r = a*b*c/(4.0*area)\n        # Here's the radius filter.\n        #print circum_r\n        if circum_r < 1.0/alpha:\n            add_edge(edges, edge_points, coords, ia, ib)\n            add_edge(edges, edge_points, coords, ib, ic)\n            add_edge(edges, edge_points, coords, ic, ia)\n    m = geometry.MultiLineString(edge_points)\n    triangles = list(polygonize(m))\n    return (cascaded_union(triangles), edge_points)\n\n    print (cascaded_union(triangles), edge_points)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Distance_Calculation(self):\n\n        print(self.whole_labels)\n        distance_result={}\n\n        #distance_result[self.whole_labels[i]] = []\n\n        print(distance_result)\n\n        for i in range(len(self.whole_labels)):\n            #print(self.whole_labels[i], self.fa_result[self.result_to_fit.index == self.whole_labels[i]][0])\n            print( self.whole_labels[i], len(self.fa_result[self.result_to_fit.index == self.whole_labels[i]]))\n\n            pass\n\n\n        '''\n        for i in range(len(self.whole_labels)):\n            for j in range(len(self.whole_labels)):\n                if i ==j:\n                    pass\n                else:\n                    distance_result[self.whole_labels[i] + ' to ' + self.whole_labels[j]] = []\n\n                    self.fa_result[self.result_to_fit.index == self.whole_labels[i]]\n\n                    self.fa_result[self.result_to_fit.index == self.whole_labels[j]]\n\n                    for m in range(len(self.fa_result[self.result_to_fit.index == self.whole_labels[i]])):\n                        for n in range(len(self.fa_result[self.result_to_fit.index == self.whole_labels[j]])):\n                            pass\n\n                            self.fa_result[self.result_to_fit.index == self.whole_labels[i]][m]\n\n                            #tmp_dist= self.Hsim_Distance(self.fa_result[self.result_to_fit.index == self.whole_labels[i]][m],self.fa_result[self.result_to_fit.index == self.whole_labels[j]][n])\n                            #print(tmp_dist)\n                            #distance_result[self.whole_labels[i] + ' to ' + self.whole_labels[j]].append(tmp_dist)\n            pass\n        \n        '''\n\n\n        #print(self.fa_result)\n\n        try:\n            self.fa_data_to_test[self.data_to_test_to_fit.index == self.whole_labels[0], 0]\n        except Exception as e:\n            pass", "response": "Calculates the distance between two sets of elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the main frame for the current locale.", "response": "def create_main_frame(self):\n        self.main_frame = QWidget()\n\n        #self.main_frame.setFixedSize(self.width(), self.width())\n\n        self.dpi = 128\n        self.ShapeGroups =200\n        self.view = gl.GLViewWidget()\n\n        #self.view = pg.PlotWidget()\n\n        #self.view.setFixedSize(self.width(),self.height())\n\n        self.view.setFixedSize(self.width(), self.width())\n\n        self.view.setParent(self.main_frame)\n\n\n        # Other GUI controls\n        self.save_button = QPushButton('&Save')\n        self.save_button.clicked.connect(self.saveImgFile)\n\n        self.draw_button = QPushButton('&Reset')\n        self.draw_button.clicked.connect(self.Reset)\n\n        self.load_button = QPushButton('&Load')\n        #self.load_button.clicked.connect(self.Load)\n\n\n        self.fit_cb= QCheckBox('&PolyFit')\n        self.fit_cb.setChecked(False)\n        self.fit_cb.stateChanged.connect(self.Magic)  # int\n\n        self.fit_label = QLabel('Exp')\n        self.fit_seter = QLineEdit(self)\n        self.fit_seter.textChanged[str].connect(self.FitChanged)\n\n\n        self.shape_cb= QCheckBox('&Shape')\n        self.shape_cb.setChecked(False)\n        self.shape_cb.stateChanged.connect(self.Magic)  # int\n\n\n        self.Normalize_cb = QCheckBox('&Normalize')\n        self.Normalize_cb.setChecked(False)\n        self.Normalize_cb.stateChanged.connect(self.Magic)  # int\n\n        self.norm_slider_label = QLabel('Standard:' + self.NameChosen)\n        self.norm_slider = QSlider(Qt.Horizontal)\n        self.norm_slider.setRange(0, 4)\n        self.norm_slider.setValue(0)\n        self.norm_slider.setTracking(True)\n        self.norm_slider.setTickPosition(QSlider.TicksBothSides)\n        self.norm_slider.valueChanged.connect(self.Magic)  # int\n\n        self.x_element = QSlider(Qt.Horizontal)\n        self.x_element.setRange(0, len(self.items) - 1)\n        self.x_element.setValue(0)\n        self.x_element.setTracking(True)\n        self.x_element.setTickPosition(QSlider.TicksBothSides)\n        self.x_element.valueChanged.connect(self.Magic)  # int\n\n        self.x_element_label = QLabel('X')\n\n        self.logx_cb = QCheckBox('&Log')\n        self.logx_cb.setChecked(False)\n        self.logx_cb.stateChanged.connect(self.Magic)  # int\n\n        self.y_element = QSlider(Qt.Horizontal)\n        self.y_element.setRange(0, len(self.items) - 1)\n        self.y_element.setValue(1)\n        self.y_element.setTracking(True)\n        self.y_element.setTickPosition(QSlider.TicksBothSides)\n        self.y_element.valueChanged.connect(self.Magic)  # int\n\n        self.y_element_label = QLabel('Y')\n\n        self.logy_cb = QCheckBox('&Log')\n        self.logy_cb.setChecked(False)\n        self.logy_cb.stateChanged.connect(self.Magic)  # int\n\n        self.z_element = QSlider(Qt.Horizontal)\n        self.z_element.setRange(0, len(self.items) - 1)\n        self.z_element.setValue(2)\n        self.z_element.setTracking(True)\n        self.z_element.setTickPosition(QSlider.TicksBothSides)\n        self.z_element.valueChanged.connect(self.Magic)  # int\n\n        self.z_element_label = QLabel('Z')\n\n        self.logz_cb = QCheckBox('&Log')\n        self.logz_cb.setChecked(False)\n        self.logz_cb.stateChanged.connect(self.Magic)  # int\n\n\n\n\n        self.xlim_seter_left_label = QLabel('Xleft')\n        self.xlim_seter_left = QLineEdit(self)\n        self.xlim_seter_left.textChanged[str].connect(self.XleftChanged)\n\n        self.xlim_seter_right_label = QLabel('Xright')\n        self.xlim_seter_right = QLineEdit(self)\n        self.xlim_seter_right.textChanged[str].connect(self.XrightChanged)\n\n\n        self.ylim_seter_down_label = QLabel('Ydown')\n        self.ylim_seter_down = QLineEdit(self)\n        self.ylim_seter_down.textChanged[str].connect(self.YdownChanged)\n\n\n        self.ylim_seter_up_label = QLabel('Yup')\n        self.ylim_seter_up = QLineEdit(self)\n        self.ylim_seter_up.textChanged[str].connect(self.YupChanged)\n\n\n\n        self.hbox0 = QHBoxLayout()\n        self.hbox1 = QHBoxLayout()\n        self.hbox2 = QHBoxLayout()\n        self.hbox3 = QHBoxLayout()\n        self.hbox4 = QHBoxLayout()\n        self.hbox5 = QHBoxLayout()\n        self.hbox6 = QHBoxLayout()\n        self.hbox7 = QHBoxLayout()\n\n\n\n        '''\n        for w in [self.fit_cb,self.fit_label, self.fit_seter,self.xlim_seter_left_label,self.xlim_seter_left,self.xlim_seter_right_label,self.xlim_seter_right,self.ylim_seter_down_label,self.ylim_seter_down,self.ylim_seter_up_label,self.ylim_seter_up,self.shape_cb]:\n            self.hbox0.addWidget(w)\n            self.hbox0.setAlignment(w, Qt.AlignVCenter)\n        '''\n\n        for w in [self.view]:\n            self.hbox0.addWidget(w)\n            self.hbox0.setAlignment(w, Qt.AlignVCenter)\n\n\n        for w in [self.Normalize_cb, self.norm_slider_label, self.norm_slider]:\n            self.hbox1.addWidget(w)\n            self.hbox1.setAlignment(w, Qt.AlignVCenter)\n\n        for w in [self.logx_cb, self.x_element_label, self.x_element]:\n            self.hbox2.addWidget(w)\n            self.hbox2.setAlignment(w, Qt.AlignVCenter)\n\n        for w in [self.logy_cb, self.y_element_label, self.y_element]:\n            self.hbox3.addWidget(w)\n            self.hbox3.setAlignment(w, Qt.AlignVCenter)\n\n        for w in [self.logz_cb, self.z_element_label, self.z_element]:\n            self.hbox4.addWidget(w)\n            self.hbox4.setAlignment(w, Qt.AlignVCenter)\n\n\n\n        self.vbox = QVBoxLayout()\n        #self.vbox.addWidget(self.view)\n        self.vbox.addLayout(self.hbox0)\n        self.vbox.addLayout(self.hbox1)\n        self.vbox.addLayout(self.hbox2)\n        self.vbox.addLayout(self.hbox3)\n        self.vbox.addLayout(self.hbox4)\n\n        self.main_frame.setLayout(self.vbox)\n        self.setCentralWidget(self.main_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the main frame and the figure and the navigation toolbar", "response": "def create_main_frame(self):\n\n\n\n        self.resize(800, 800)\n\n        self.main_frame = QWidget()\n        self.dpi = 128\n        self.fig = Figure((8.0, 8.0), dpi=self.dpi)\n\n        self.fig.subplots_adjust(hspace=0.5, wspace=0.5, left=0.13, bottom=0.2, right=0.7, top=0.9)\n\n        self.canvas = FigureCanvas(self.fig)\n        self.canvas.setParent(self.main_frame)\n        self.axes = self.fig.add_subplot(111)\n        # self.axes.hold(False)\n\n        # Create the navigation toolbar, tied to the canvas\n        self.mpl_toolbar = NavigationToolbar(self.canvas, self.main_frame)\n\n        # Other GUI controls\n\n        self.load_data_button = QPushButton('&Add Data to Compare')\n        self.load_data_button.clicked.connect(self.loadDataToTest)\n\n        self.save_plot_button = QPushButton('&Save IMG')\n        self.save_plot_button .clicked.connect(self.saveImgFile)\n\n        self.stat_button = QPushButton('&Show Stat')\n        self.stat_button.clicked.connect(self.Stat)\n\n\n        self.load_img_button = QPushButton('&Load Basemap')\n        self.load_img_button.clicked.connect(self.Load)\n\n        self.unload_img_button = QPushButton('&Unload Basemap')\n        self.unload_img_button.clicked.connect(self.Unload)\n\n\n        self.legend_cb = QCheckBox('&Legend')\n        self.legend_cb.setChecked(True)\n        self.legend_cb.stateChanged.connect(self.Magic)  # int\n\n\n\n        self.show_load_data_cb = QCheckBox('&Show Loaded Data')\n        self.show_load_data_cb.setChecked(True)\n        self.show_load_data_cb.stateChanged.connect(self.Magic)  # int\n\n\n        self.show_data_index_cb = QCheckBox('&Show Data Index')\n        self.show_data_index_cb.setChecked(False)\n        self.show_data_index_cb.stateChanged.connect(self.Magic)  # int\n\n\n        self.hyperplane_cb= QCheckBox('&Hyperplane')\n        self.hyperplane_cb.setChecked(False)\n        self.hyperplane_cb.stateChanged.connect(self.Magic)  # int\n\n\n        self.fit_cb= QCheckBox('&PolyFit')\n        self.fit_cb.setChecked(False)\n        self.fit_cb.stateChanged.connect(self.Magic)  # int\n\n\n\n\n        self.fit_seter = QLineEdit(self)\n        self.fit_seter.textChanged[str].connect(self.FitChanged)\n\n\n\n        self.fit_slider_label = QLabel('y= f(x) EXP')\n        self.fit_slider = QSlider(Qt.Vertical)\n        self.fit_slider.setRange(0, 1)\n        self.fit_slider.setValue(0)\n        self.fit_slider.setTracking(True)\n        self.fit_slider.setTickPosition(QSlider.TicksBothSides)\n        self.fit_slider.valueChanged.connect(self.Magic)  # int\n\n\n        self.shape_cb= QCheckBox('&Shape')\n        self.shape_cb.setChecked(False)\n        self.shape_cb.stateChanged.connect(self.Magic)  # int\n\n        #self.shape_label = QLabel('Step')\n        #self.shape_seter = QLineEdit(self)\n        #self.shape_seter.textChanged[str].connect(self.ShapeChanged)\n\n\n\n        self.norm_cb = QCheckBox('&Norm')\n        self.norm_cb.setChecked(False)\n        self.norm_cb.stateChanged.connect(self.Magic)  # int\n\n        self.standard_slider = QSlider(Qt.Horizontal)\n        self.standard_slider.setRange(0, len(self.StandardsName))\n\n        if len(self._given_Standard) > 0:\n            self.standard_slider.setValue(len(self.StandardsName))\n            self.right_label = QLabel(\"Self Defined Standard\")\n\n        else:\n            self.standard_slider.setValue(0)\n            self.right_label = QLabel(self.StandardsName[int(self.standard_slider.value())])\n\n\n        self.standard_slider.setTracking(True)\n        self.standard_slider.setTickPosition(QSlider.TicksBothSides)\n        self.standard_slider.valueChanged.connect(self.Magic)  # int\n        self.left_label= QLabel('Standard' )\n\n\n\n        self.x_element = QSlider(Qt.Horizontal)\n        self.x_element.setRange(0, len(self.items) - 1)\n        self.x_element.setValue(0)\n        self.x_element.setTracking(True)\n        self.x_element.setTickPosition(QSlider.TicksBothSides)\n        self.x_element.valueChanged.connect(self.ValueChooser)  # int\n\n\n\n        self.x_seter = QLineEdit(self)\n        self.x_seter.textChanged[str].connect(self.LabelSeter)\n\n        #self.x_calculator = QLineEdit(self)\n\n\n\n        self.logx_cb = QCheckBox('&Log')\n        self.logx_cb.setChecked(False)\n        self.logx_cb.stateChanged.connect(self.Magic)  # int\n\n        self.y_element = QSlider(Qt.Horizontal)\n        self.y_element.setRange(0, len(self.items) - 1)\n        self.y_element.setValue(1)\n        self.y_element.setTracking(True)\n        self.y_element.setTickPosition(QSlider.TicksBothSides)\n        self.y_element.valueChanged.connect(self.ValueChooser)  # int\n\n\n        self.y_seter = QLineEdit(self)\n        self.y_seter.textChanged[str].connect(self.LabelSeter)\n\n        #self.y_calculator = QLineEdit(self)\n\n\n        self.logy_cb = QCheckBox('&Log')\n        self.logy_cb.setChecked(False)\n        self.logy_cb.stateChanged.connect(self.Magic)  # int\n\n\n\n\n        self.hyperplane_cb= QCheckBox('&Hyperplane')\n        self.hyperplane_cb.setChecked(False)\n        self.hyperplane_cb.stateChanged.connect(self.Magic)  # int\n\n\n        self.save_predict_button_selected = QPushButton('&Predict Selected')\n        self.save_predict_button_selected.clicked.connect(self.showPredictResultSelected)\n\n        self.save_predict_button = QPushButton('&Predict All')\n        self.save_predict_button.clicked.connect(self.showPredictResult)\n\n        self.load_data_button = QPushButton('&Add Data to Compare')\n        self.load_data_button.clicked.connect(self.loadDataToTest)\n\n        self.width_size_seter_label = QLabel('SVG Width')\n        self.width_size_seter = QLineEdit(self)\n\n        self.width_size_seter.textChanged[str].connect(self.WChanged)\n\n        self.height_size_seter_label = QLabel('SVG Height')\n        self.height_size_seter = QLineEdit(self)\n\n        self.height_size_seter.textChanged[str].connect(self.HChanged)\n\n        self.Left_size_seter_label = QLabel('PNG Left')\n        self.Left_size_seter = QLineEdit(self)\n\n        self.Left_size_seter.textChanged[str].connect(self.LeftChanged)\n\n        self.Right_size_seter_label = QLabel('PNG Right')\n        self.Right_size_seter = QLineEdit(self)\n\n        self.Right_size_seter.textChanged[str].connect(self.RightChanged)\n\n        self.Up_size_seter_label = QLabel('PNG Top')\n        self.Up_size_seter = QLineEdit(self)\n\n        self.Up_size_seter.textChanged[str].connect(self.UpChanged)\n\n        self.Down_size_seter_label = QLabel('PNG Bottom')\n        self.Down_size_seter = QLineEdit(self)\n\n        self.Down_size_seter.textChanged[str].connect(self.DownChanged)\n\n        #\n        # Layout with box sizers\n        #\n        self.hbox = QHBoxLayout()\n        self.hbox0 = QHBoxLayout()\n        self.hbox1 = QHBoxLayout()\n        self.hbox2 = QHBoxLayout()\n        self.hbox3 = QHBoxLayout()\n        self.hbox4 = QHBoxLayout()\n        self.hbox5 = QHBoxLayout()\n\n        w=self.width()\n        h=self.height()\n\n        #self.load_data_button.setFixedWidth(w/4)\n\n\n        self.kernel_select = QSlider(Qt.Horizontal)\n        self.kernel_select.setRange(0, len(self.kernel_list)-1)\n        self.kernel_select.setValue(0)\n        self.kernel_select.setTracking(True)\n        self.kernel_select.setTickPosition(QSlider.TicksBothSides)\n        self.kernel_select.valueChanged.connect(self.Magic)  # int\n        self.kernel_select_label = QLabel('Kernel')\n\n        for w in [self.save_plot_button ,self.stat_button,self.load_data_button,self.save_predict_button,self.save_predict_button_selected]:\n            self.hbox.addWidget(w)\n            self.hbox.setAlignment(w, Qt.AlignVCenter)\n\n\n        for w in [self.legend_cb,self.show_load_data_cb,self.show_data_index_cb, self.norm_cb,self.shape_cb,self.hyperplane_cb,self.kernel_select_label,self.kernel_select]:\n            self.hbox0.addWidget(w)\n            self.hbox0.setAlignment(w, Qt.AlignVCenter)\n\n\n        for w in [self.left_label, self.standard_slider,self.right_label,self.fit_cb,self.fit_slider,self.fit_slider_label ,self.fit_seter]:\n            self.hbox1.addWidget(w)\n            self.hbox1.setAlignment(w, Qt.AlignVCenter)\n\n\n        for w in [self.logx_cb,self.x_seter, self.x_element]:\n            self.hbox2.addWidget(w)\n            self.hbox2.setAlignment(w, Qt.AlignVCenter)\n\n        for w in [self.logy_cb,self.y_seter, self.y_element]:\n            self.hbox3.addWidget(w)\n            self.hbox3.setAlignment(w, Qt.AlignVCenter)\n\n\n        for w in [self.load_img_button, self.width_size_seter_label, self.width_size_seter, self.height_size_seter_label,\n                  self.height_size_seter]:\n            self.hbox4.addWidget(w)\n            self.hbox4.setAlignment(w, Qt.AlignLeft)\n\n\n\n        for w in [self.unload_img_button,self.Left_size_seter_label, self.Left_size_seter,\n                  self.Right_size_seter_label,  self.Right_size_seter,self.Down_size_seter_label, self.Down_size_seter,\n                  self.Up_size_seter_label ,self.Up_size_seter]:\n            self.hbox5.addWidget(w)\n            self.hbox5.setAlignment(w, Qt.AlignLeft)\n\n\n\n        self.vbox = QVBoxLayout()\n        self.vbox.addWidget(self.mpl_toolbar)\n        self.vbox.addWidget(self.canvas)\n        self.vbox.addLayout(self.hbox)\n        self.vbox.addLayout(self.hbox0)\n        self.vbox.addLayout(self.hbox1)\n        self.vbox.addLayout(self.hbox2)\n        self.vbox.addLayout(self.hbox3)\n        self.vbox.addLayout(self.hbox4)\n        self.vbox.addLayout(self.hbox5)\n\n        self.textbox = GrowingTextEdit(self)\n\n        self.vbox.addWidget(self.textbox)\n\n\n\n\n\n        self.main_frame.setLayout(self.vbox)\n        self.setCentralWidget(self.main_frame)\n\n        w=self.width()\n        h=self.height()\n\n        self.x_seter.setFixedWidth(w/10)\n        self.y_seter.setFixedWidth(w/10)\n\n        '''\n        self.save_plot_button.setFixedWidth(w/10)\n        self.stat_button.setFixedWidth(w/10)\n        self.load_data_button.setFixedWidth(w/4)\n        self.save_predict_button_selected.setFixedWidth(w/4)\n        self.save_predict_button.setFixedWidth(w/4)\n        '''\n\n        self.standard_slider.setFixedWidth(w/5)\n\n        self.right_label.setFixedWidth(w/5)\n\n        self.fit_seter.setFixedWidth(w/20)\n\n        self.load_img_button.setFixedWidth(w/5)\n        self.unload_img_button.setFixedWidth(w/5)\n\n        self.width_size_seter_label.setFixedWidth(w/10)\n        self.height_size_seter_label.setFixedWidth(w/10)\n\n        self.width_size_seter.setMinimumWidth(w/20)\n        self.height_size_seter.setMinimumWidth(w/20)\n\n        self.Right_size_seter_label.setFixedWidth(w/10)\n        self.Left_size_seter_label.setFixedWidth(w/10)\n        self.Up_size_seter_label.setFixedWidth(w/10)\n        self.Down_size_seter_label.setFixedWidth(w/10)\n\n        self.Right_size_seter.setFixedWidth(w/20)\n        self.Left_size_seter.setFixedWidth(w/20)\n        self.Up_size_seter.setFixedWidth(w/20)\n        self.Down_size_seter.setFixedWidth(w/20)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the main frame and the figure and the navigation toolbar.", "response": "def create_main_frame(self):\n        self.resize(800, 1000)\n        self.main_frame = QWidget()\n        self.dpi = 128\n        self.fig = Figure((12, 11), dpi=self.dpi)\n\n        self.fig.subplots_adjust(hspace=0.5, wspace=0.5, left=0.1, bottom=0.2, right=0.7, top=0.9)\n\n        # 8 * np.sqrt(3)\n        self.canvas = FigureCanvas(self.fig)\n        self.canvas.setParent(self.main_frame)\n\n        self.axes = self.fig.add_subplot(111)\n        self.axes.axis('off')\n        self.axes.set_xlim(-10, 110)\n        self.axes.set_ylim(-105 * np.sqrt(3) / 2, 105 * np.sqrt(3) / 2)\n\n        self.canvas = FigureCanvas(self.fig)\n        self.canvas.setParent(self.main_frame)\n        self.axes = self.fig.add_subplot(111)\n        # Create the navigation toolbar, tied to the canvas\n        self.mpl_toolbar = NavigationToolbar(self.canvas, self.main_frame)\n\n        # Other GUI controls\n        self.save_button = QPushButton('&Save')\n        self.save_button.clicked.connect(self.saveImgFile)\n\n        #self.result_button = QPushButton('&Result')\n        #self.result_button.clicked.connect(self.Explain)\n\n        self.legend_cb = QCheckBox('&Legend')\n        self.legend_cb.setChecked(True)\n        self.legend_cb.stateChanged.connect(self.QAPF)  # int\n\n        self.slider_left_label = QLabel('Plutonic')\n        self.slider_right_label = QLabel('Volcanic')\n\n\n        self.slider = QSlider(Qt.Horizontal)\n        self.slider.setRange(0, 1)\n        self.slider.setValue(0)\n        self.slider.setTracking(True)\n        self.slider.setTickPosition(QSlider.TicksBothSides)\n        self.slider.valueChanged.connect(self.QAPF)  # int\n\n        '''\n        self.Tag_cb = QCheckBox('&Plutonic')\n        self.Tag_cb.setChecked(True)\n        self.Tag_cb.stateChanged.connect(self.QAPF)  # int\n        if (self.Tag_cb.isChecked()):\n            self.Tag_cb.setText('&Plutonic')\n        else:\n            self.Tag_cb.setText('&Volcanic')\n        '''\n\n\n        self.detail_cb = QCheckBox('&Detail')\n        self.detail_cb.setChecked(True)\n        self.detail_cb.stateChanged.connect(self.QAPF)  # int\n\n        #\n        # Layout with box sizers\n        #\n        self.hbox = QHBoxLayout()\n\n        for w in [self.save_button,  self.detail_cb, self.legend_cb,self.slider_left_label,self.slider,self.slider_right_label]:\n            self.hbox.addWidget(w)\n            self.hbox.setAlignment(w, Qt.AlignVCenter)\n\n        self.vbox = QVBoxLayout()\n        self.vbox.addWidget(self.mpl_toolbar)\n        self.vbox.addWidget(self.canvas)\n        self.vbox.addLayout(self.hbox)\n\n\n\n        self.textbox = GrowingTextEdit(self)\n\n        self.vbox.addWidget(self.textbox)\n\n        self.main_frame.setLayout(self.vbox)\n        self.setCentralWidget(self.main_frame)\n\n\n\n        w=self.width()\n        h=self.height()\n\n        #setFixedWidth(w/10)\n\n        self.slider.setMinimumWidth(w/10)\n        self.slider_left_label.setMinimumWidth(w/10)\n        self.slider_right_label.setMinimumWidth(w/10)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef TriToBin(self, x, y, z):\n\n        '''\n        Turn an x-y-z triangular coord to an a-b coord.\n        if z is negative, calc with its abs then return (a, -b).\n        :param x,y,z: the three numbers of the triangular coord\n        :type x,y,z: float or double are both OK, just numbers\n        :return:  the corresponding a-b coord\n        :rtype:   a tuple consist of a and b\n        '''\n\n        if (z >= 0):\n            if (x + y + z == 0):\n                return (0, 0)\n            else:\n                Sum = x + y + z\n                X = 100.0 * x / Sum\n                Y = 100.0 * y / Sum\n                Z = 100.0 * z / Sum\n                if (X + Y != 0):\n                    a = Z / 2.0 + (100.0 - Z) * Y / (Y + X)\n                else:\n                    a = Z / 2.0\n                b = Z / 2.0 * (np.sqrt(3))\n                return (a, b)\n        else:\n            z = abs(z)\n            if (x + y + z == 0):\n                return (0, 0)\n            else:\n                Sum = x + y + z\n                X = 100.0 * x / Sum\n                Y = 100.0 * y / Sum\n                Z = 100.0 * z / Sum\n                if (X + Y != 0):\n                    a = Z / 2.0 + (100.0 - Z) * Y / (Y + X)\n                else:\n                    a = Z / 2.0\n                b = Z / 2.0 * (np.sqrt(3))\n                return (a, -b)", "response": "This function turns an x - y - z triangular coord to an a - b coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn an a - b coordinate to an x - y - z triangular coordinate.", "response": "def BinToTri(self, a, b):\n\n        '''\n        Turn an a-b coord to an x-y-z triangular coord .\n        if z is negative, calc with its abs then return (a, -b).\n        :param a,b: the numbers of the a-b coord\n        :type a,b: float or double are both OK, just numbers\n        :return:  the corresponding x-y-z triangular coord\n        :rtype:   a tuple consist of x,y,z\n        '''\n\n        if (b >= 0):\n            y = a - b / np.sqrt(3)\n            z = b * 2 / np.sqrt(3)\n            x = 100 - (a + b / np.sqrt(3))\n            return (x, y, z)\n        else:\n            y = a + b / np.sqrt(3)\n            z = b * 2 / np.sqrt(3)\n            x = 100 - (a - b / np.sqrt(3))\n            return (x, y, z)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Cross(self, A=[(0, 0), (10, 10)], B=[(0, 10), (100, 0)]):\n\n        '''\n        Return the crosspoint of two line A and B.\n        :param A: first line\n        :type A: a list consist of two tuples, beginning and end point of the line\n        :param B: second line\n        :type B: a list consist of two tuples, beginning and end point of the line\n        :return: the crosspoint of A and B\n        :rtype: a list consist of two numbers, the x-y of the crosspoint\n        '''\n\n        x0, y0 = A[0]\n        x1, y1 = A[1]\n        x2, y2 = B[0]\n        x3, y3 = B[1]\n\n        b1 = (y1 - y0) / (x1 - x0)\n        b2 = (y3 - y2) / (x3 - x2)\n        c1 = y0 - b1 * x0\n        c2 = y2 - b2 * x2\n\n        x = (c2 - c1) / (b1 - b2)\n        y = b1 * x + c1\n\n        return ([x, y])", "response": "Return the crosspoint of two line A and B."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef TriCross(self, A=[(100, 0, 0), (0, 50, 60)], B=[(50, 50, 0), (0, 0, 100)]):\n\n        '''\n        Return the crosspoint of two line A and B in triangular coord.\n        :param A: first line\n        :type A: a list consist of two tuples, beginning and end point of the line\n        :param B: second line\n        :type B: a list consist of two tuples, beginning and end point of the line\n        :return:  the crosspoint of A and B\n        :rtype:   a list consist of three numbers, the x-y-z of the triangular coord\n        '''\n\n        x0, y0 = self.TriToBin(A[0][0], A[0][1], A[0][2])\n        x1, y1 = self.TriToBin(A[1][0], A[1][1], A[1][2])\n        x2, y2 = self.TriToBin(B[0][0], B[0][1], B[0][2])\n        x3, y3 = self.TriToBin(B[1][0], B[1][1], B[1][2])\n\n        b1 = (y1 - y0) / (x1 - x0)\n        b2 = (y3 - y2) / (x3 - x2)\n        c1 = y0 - b1 * x0\n        c2 = y2 - b2 * x2\n\n        x = (c2 - c1) / (b1 - b2)\n        y = b1 * x + c1\n\n        result = self.BinToTri(x, y)\n        return (result)", "response": "TriCross returns the crosspoint of two line A and B in triangular coord."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Fill(self, P=[(100, 0), (85, 15), (0, 3)], Color='blue', Alpha=0.3):\n\n        '''\n        Fill a region in planimetric rectangular coord.\n        :param P: the peak points of the region in planimetric rectangular coord\n        :type P: a list consist of at least three tuples, which are the points in planimetric rectangular coord\n        :param Color: the color used to fill the region\n        :type Color: a string; b: blue, g: green, r: red, c: cyan, m: magenta, y: yellow, k: black, w: white\n        :param Alpha: the transparency used to fill the region\n        :type Alpha: a float number from 0 to 1, higher darker, lower more transparent\n        '''\n        a = []\n        b = []\n\n        for i in P:\n            a.append(i[0])\n            b.append(i[1])\n\n        return (a, b)", "response": "Fill a region in planimetric rectangular coord."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfill a region in triangular coord.", "response": "def TriFill(self, P=[(100, 0, 0), (85, 15, 0), (0, 3, 97)], Color='blue', Alpha=0.3):\n\n        '''\n         Fill a region in triangular coord.\n        :param P: the peak points of the region in triangular coord\n        :type P: a list consist of at least three tuples, which are the points in triangular coord\n        :param Color: the color used to fill the region\n        :type Color: a string; b: blue, g: green, r: red, c: cyan, m: magenta, y: yellow, k: black, w: white\n        :param Alpha: the transparency used to fill the region\n        :type Alpha: a float number from 0 to 1, higher darker, lower more transparent\n        '''\n\n        a = []\n        b = []\n\n        for i in P:\n            a.append(self.TriToBin(i[0], i[1], i[2])[0])\n            b.append(self.TriToBin(i[0], i[1], i[2])[1])\n\n        return (a, b)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting the points in the line with given option", "response": "def sequence(self):\n        '''\n        sort the points in the line with given option\n        '''\n        if (len(self.Points[0]) == 2):\n            if (self.Sort == 'X' or self.Sort == 'x'):\n                self.Points.sort(key=lambda x: x[0])\n                self.order(self.Points)\n\n            elif (self.Sort == 'Y' or self.Sort == 'y'):\n                self.Points.sort(key=lambda x: x[1])\n                self.order(self.Points)\n            else:\n                self.order(self.Points)\n\n        if (len(self.Points[0]) == 3):\n            if (self.Sort == 'X' or self.Sort == 'x'):\n                self.Points.sort(key=lambda x: x[0])\n                self.order(self.Points)\n\n            elif (self.Sort == 'Y' or self.Sort == 'y'):\n                self.Points.sort(key=lambda x: x[1])\n                self.order(self.Points)\n            elif (self.Sort == 'Z' or self.Sort == 'Z'):\n                self.Points.sort(key=lambda x: x[2])\n                self.order(self.Points)\n            else:\n                self.order(self.Points)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resizeEvent(self, evt=None):\n\n        w = self.width()\n        h = self.height()\n        '''\n        if h<=360:\n            h=360\n            self.resize(w,h)\n        if w<=640:\n            w = 640\n            self.resize(w, h)\n        '''\n\n        step = (w * 94 / 100) / 5\n        foot = h * 3 / 48", "response": "Resize the image to match the size of the image."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef K2OSiO2(self, Left=35, Right=79, X0=30, X1=90, X_Gap=7, Base=0,\n            Top=19, Y0=1, Y1=19, Y_Gap=19, FontSize=12, xlabel=r'$SiO_2 wt\\%$', ylabel=r'$K_2O wt\\%$', width=12,\n            height=12, dpi=300):\n        self.setWindowTitle('K2OSiO2  diagram ')\n        self.axes.clear()\n        #self.axes.axis('off')\n        \n        \n        self.axes.set_xlabel(self.xlabel)\n        self.axes.set_ylabel(self.ylabel)\n        self.axes.spines['right'].set_color('none')\n        self.axes.spines['top'].set_color('none')\n\n        '''\n        self.axes.set_xticks([30,40,50,60,70,80,90])\n        self.axes.set_xticklabels([30,40,50,60,70,80,90])\n\n        self.axes.set_yticks([0, 5, 10, 15, 20])\n        self.axes.set_yticklabels([0, 5, 10, 15, 20])\n\n        self.axes.set_ylim(bottom=0)\n\n        '''\n\n\n\n        all_labels=[]\n        all_colors=[]\n        all_markers=[]\n        all_alpha=[]\n\n        for i in range(len(self._df)):\n            target = self._df.at[i, 'Label']\n            color = self._df.at[i, 'Color']\n            marker = self._df.at[i, 'Marker']\n            alpha = self._df.at[i, 'Alpha']\n\n            if target not in self.SVM_labels:\n                self.SVM_labels.append(target)\n            if target not in all_labels:\n                all_labels.append(target)\n                all_colors.append(color)\n                all_markers.append(marker)\n                all_alpha.append(alpha)\n\n        self.whole_labels = all_labels\n\n\n        PointLabels = []\n        PointColors = []\n        x = []\n        y = []\n\n\n        title = 'K2O-SiO2diagram'\n        self.setWindowTitle(title)\n        self.textbox.setText(self.reference)\n\n\n        k_1=(2.9-1.2)/(68-48)\n        y_1= 1.2+ (85-48)*k_1\n        y_0= 1.2+ (45-48)*k_1\n        self.DrawLine([(45, y_0),(48, 1.2), (68,2.9),(85,y_1)])\n\n        k_2=(1.2-0.3)/(68-48)\n        y_2= 0.3+ (85-48)*k_2\n        y_3= 0.3+ (45-48)*k_2\n        self.DrawLine([(45, y_3),(48, 0.3), (68, 1.2),(85,y_2)])\n        Labels=['High K','Medium K','Low K']\n        Locations=[(80,5),(80,3),(80,1)]\n        X_offset, Y_offset=0,0\n\n        for k in range(len(Labels)):\n            self.axes.annotate(Labels[k], Locations[k], xycoords='data', xytext=(X_offset, Y_offset),\n                               textcoords='offset points',\n                               fontsize=9, color='grey', alpha=0.8)\n\n        self.Check()\n\n        if self.OutPutCheck==True:\n            pass\n\n        if (self._changed):\n            df =  self.CleanDataFile(self._df)\n\n            for i in range(len(df)):\n                TmpLabel = ''\n                if (df.at[i, 'Label'] in PointLabels or df.at[i, 'Label'] == ''):\n                    TmpLabel = ''\n                else:\n                    PointLabels.append(df.at[i, 'Label'])\n                    TmpLabel = df.at[i, 'Label']\n\n\n                TmpColor = ''\n                if (df.at[i, 'Color'] in PointColors or df.at[i, 'Color'] == ''):\n                    TmpColor = ''\n                else:\n                    PointColors.append(df.at[i, 'Color'])\n                    TmpColor = df.at[i, 'Color']\n\n\n\n                x.append(df.at[i, 'SiO2'])\n                y.append(df.at[i, 'K2O'])\n                Size = df.at[i, 'Size']\n                Color = df.at[i, 'Color']\n\n                # print(Color, df.at[i, 'SiO2'], (df.at[i, 'Na2O'] + df.at[i, 'K2O']))\n\n                Alpha = df.at[i, 'Alpha']\n                Marker = df.at[i, 'Marker']\n                Label = df.at[i, 'Label']\n\n                xtest=df.at[i, 'SiO2']\n                ytest=df.at[i, 'K2O']\n\n                for j in self.ItemNames:\n                    if self.SelectDic[j].contains_point([xtest,ytest]):\n\n                        self.LabelList.append(Label)\n                        self.TypeList.append(j)\n\n                        break\n                    pass\n\n\n                self.axes.scatter(df.at[i, 'SiO2'], df.at[i, 'K2O'], marker=df.at[i, 'Marker'],\n                  s=df.at[i, 'Size'], color=df.at[i, 'Color'], alpha=df.at[i, 'Alpha'], label=TmpLabel)\n\n\n\n\n            XtoFit = {}\n            YtoFit = {}\n\n            SVM_X=[]\n            SVM_Y=[]\n\n            for i in  PointLabels:\n                XtoFit[i]=[]\n                YtoFit[i]=[]\n\n\n            for i in range(len(df)):\n                Alpha = df.at[i, 'Alpha']\n                Marker = df.at[i, 'Marker']\n                Label = df.at[i, 'Label']\n\n                xtest=df.at[i, 'SiO2']\n                ytest=df.at[i, 'K2O']\n\n                XtoFit[Label].append(xtest)\n                YtoFit[Label].append(ytest)\n\n                SVM_X.append(xtest)\n                SVM_Y.append(ytest)\n\n            if (self.shape_cb.isChecked()):\n                for i in PointLabels:\n\n                    if XtoFit[i] != YtoFit[i]:\n                        xmin, xmax = min(XtoFit[i]), max(XtoFit[i])\n                        ymin, ymax = min(YtoFit[i]), max(YtoFit[i])\n\n                        DensityColorMap = 'Greys'\n                        DensityAlpha = 0.1\n\n                        DensityLineColor = PointColors[PointLabels.index(i)]\n                        DensityLineAlpha = 0.3\n\n                        # Peform the kernel density estimate\n                        xx, yy = np.mgrid[xmin:xmax:200j, ymin:ymax:200j]\n                        # print(self.ShapeGroups)\n                        # command='''xx, yy = np.mgrid[xmin:xmax:'''+str(self.ShapeGroups)+ '''j, ymin:ymax:''' +str(self.ShapeGroups)+'''j]'''\n                        # exec(command)\n                        # print(xx, yy)\n                        positions = np.vstack([xx.ravel(), yy.ravel()])\n                        values = np.vstack([XtoFit[i], YtoFit[i]])\n                        kernelstatus = True\n                        try:\n                            st.gaussian_kde(values)\n                        except Exception as e:\n                            self.ErrorEvent(text=repr(e))\n                            kernelstatus = False\n                        if kernelstatus == True:\n                            kernel = st.gaussian_kde(values)\n                            f = np.reshape(kernel(positions).T, xx.shape)\n                            # Contourf plot\n                            cfset = self.axes.contourf(xx, yy, f, cmap=DensityColorMap, alpha=DensityAlpha)\n                            ## Or kernel density estimate plot instead of the contourf plot\n                            # self.axes.imshow(np.rot90(f), cmap='Blues', extent=[xmin, xmax, ymin, ymax])\n                            # Contour plot\n                            cset = self.axes.contour(xx, yy, f, colors=DensityLineColor, alpha=DensityLineAlpha)\n                            # Label plot\n                            #self.axes.clabel(cset, inline=1, fontsize=10)\n\n\n\n\n            if (len(self.data_to_test) > 0):\n\n                contained = True\n                missing = 'Miss setting infor:'\n\n                for i in ['Label', 'Color', 'Marker', 'Alpha']:\n                    if i not in self.data_to_test.columns.values.tolist():\n                        contained = False\n                        missing = missing + '\\n' + i\n\n                if contained == True:\n                    for i in self.data_to_test.columns.values.tolist():\n                        if i not in self._df.columns.values.tolist():\n                            self.data_to_test = self.data_to_test.drop(columns=i)\n\n                    # print(self.data_to_test)\n\n                    test_labels = []\n                    test_colors = []\n                    test_markers = []\n                    test_alpha = []\n\n                    for i in range(len(self.data_to_test)):\n\n                        # print(self.data_to_test.at[i, 'Label'])\n                        target = self.data_to_test.at[i, 'Label']\n                        color = self.data_to_test.at[i, 'Color']\n                        marker = self.data_to_test.at[i, 'Marker']\n                        alpha = self.data_to_test.at[i, 'Alpha']\n\n                        if target not in test_labels and target not in all_labels:\n                            test_labels.append(target)\n                            test_colors.append(color)\n                            test_markers.append(marker)\n                            test_alpha.append(alpha)\n\n                    self.whole_labels = self.whole_labels + test_labels\n\n\n                    self.load_settings_backup = self.data_to_test\n                    Load_ItemsToTest = ['Label', 'Number', 'Tag', 'Name', 'Author', 'DataType', 'Marker', 'Color',\n                                        'Size',\n                                        'Alpha',\n                                        'Style', 'Width']\n                    for i in self.data_to_test.columns.values.tolist():\n                        if i not in Load_ItemsToTest:\n                            self.load_settings_backup = self.load_settings_backup.drop(i, 1)\n\n                    print(self.load_settings_backup, self.data_to_test)\n\n                    print(self.load_settings_backup.shape, self.data_to_test.shape)\n\n\n                    try:\n\n                        for i in range(len(self.data_to_test)):\n\n                            target = self.data_to_test.at[i, 'Label']\n                            if target not in all_labels:\n                                all_labels.append(target)\n                                tmp_label = self.data_to_test.at[i, 'Label']\n                            else:\n                                tmp_label=''\n\n                            x_load_test = self.data_to_test.at[i, 'SiO2']\n                            y_load_test = self.data_to_test.at[i, 'K2O']\n\n                            for j in self.ItemNames:\n                                if self.SelectDic[j].contains_point([x_load_test, y_load_test]):\n                                    self.LabelList.append(self.data_to_test.at[i, 'Label'])\n                                    self.TypeList.append(j)\n                                    break\n                                pass\n\n                            if (self.show_load_data_cb.isChecked()):\n\n                                self.axes.scatter(self.data_to_test.at[i, 'SiO2'],self.data_to_test.at[i, 'K2O'],\n                                                  marker=self.data_to_test.at[i, 'Marker'],\n                                                  s=self.data_to_test.at[i, 'Size'],\n                                                  color=self.data_to_test.at[i, 'Color'],\n                                                  alpha=self.data_to_test.at[i, 'Alpha'],\n                                                  label=tmp_label)\n\n\n\n\n                    except Exception as e:\n                        self.ErrorEvent(text=repr(e))\n\n\n            if (self.legend_cb.isChecked()):\n                self.axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, prop=fontprop)\n\n\n            self.All_X=SVM_X\n            self.All_Y=SVM_Y\n\n            if (self.hyperplane_cb.isChecked()):\n                clf = svm.SVC(C=1.0, kernel='linear',probability= True)\n\n                svm_x= SVM_X\n                svm_y= SVM_Y\n\n                print(len(svm_x),len(svm_y),len(df.index))\n\n                xx, yy = np.meshgrid(np.arange(min(svm_x), max(svm_x), np.ptp(svm_x) / 500),\n                                     np.arange(min(svm_y), max(svm_y), np.ptp(svm_y) / 500))\n\n                le = LabelEncoder()\n                le.fit(self._df.Label)\n                class_label = le.transform(self._df.Label)\n                svm_train= pd.concat([pd.DataFrame(svm_x),pd.DataFrame(svm_y)], axis=1)\n                svm_train=svm_train.values\n                clf.fit(svm_train,class_label)\n                Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                self.axes.contourf(xx, yy, Z, cmap='hot', alpha=0.2)\n\n\n            if (self.show_data_index_cb.isChecked()):\n\n                if 'Index' in self._df.columns.values:\n\n                    for i in range(len(self._df)):\n                        self.axes.annotate(self._df.at[i, 'Index'],\n                                           xy=(self.All_X[i],\n                                               self.All_Y[i]),\n                                           color=self._df.at[i, 'Color'],\n                                           alpha=self._df.at[i, 'Alpha'])\n                else:\n                    for i in range(len(self._df)):\n                        self.axes.annotate('No' + str(i + 1),\n                                           xy=(self.All_X[i],\n                                               self.All_Y[i]),\n                                           color=self._df.at[i, 'Color'],\n                                           alpha=self._df.at[i, 'Alpha'])\n            self.canvas.draw()\n\n\n\n\n\n        self.OutPutTitle='K2OSiO2'\n\n        self.OutPutData = pd.DataFrame(\n            {'Label': self.LabelList,\n             'RockType': self.TypeList\n             })\n\n        self.OutPutFig=self.fig", "response": "self.axes.set_xticks([30,40,50,60,70,80,90])\n        self.axes.set_xticklabels([30,40,50,60,70,80,90])\n\n        self.axes.set_yticks([0, 5, 10, 15, 20])\n        self.axes.set_yticklabels([0, 5, 10, 15, 20])\n\n        self.axes.set_ylim(bottom=0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef REE(self, Left=0, Right=16, X0=1, X1=15, X_Gap=15, Base=-1,\n            Top=6, Y0=-1,\n            Y1=3, Y_Gap=5, FontSize=12,\n            xLabel=r'$REE-Standardlized-Pattern$', yLabel='', width=12, height=12, dpi=300):\n\n        self.axes.clear()\n        self.axes.spines['right'].set_color('none')\n        self.axes.spines['top'].set_color('none')\n        self.WholeData = []\n        self.OutPutData = pd.DataFrame()\n        self.LabelList=[]\n        self.algebraDeltaEuList=[]\n        self.geometricDeltaEuList=[]\n        self.LaPrDeltaCeList=[]\n        self.LaNdDeltaCeList=[]\n        self.LaSmList=[]\n        self.LaYbList=[]\n        self.GdYbList=[]\n        self.LREEList=[]\n        self.MREEList=[]\n        self.HREEList=[]\n        self.ALLREEList=[]\n        self.BinHREEList=[]\n        self.BinLREEList=[]\n        self.L_HREEList=[]\n\n        self.AllAlpha=[]\n        self.AllWidth = []\n        self.AllSize = []\n\n        #raw = self._df\n\n\n        raw = self.CleanDataFile(self._df)\n\n        self.FontSize = FontSize\n\n        PointLabels = []\n        k = 0\n\n\n        slider_value=int(self.standard_slider.value())\n\n        item_value=int(self.item_slider.value())\n\n\n\n\n        if slider_value < len(self.StandardsName):\n            standardnamechosen = self.StandardsName[slider_value]\n            standardchosen = self.Standards[standardnamechosen]\n            self.textbox.setText(self.reference+\"\\nStandard Chosen: \"+self.StandardsName[slider_value])\n\n            right_label_text=self.StandardsName[slider_value]\n\n        elif len(self._given_Standard)<=0:\n            standardnamechosen = self.StandardsName[slider_value-1]\n            standardchosen = self.Standards[standardnamechosen]\n            self.textbox.setText(self.reference+\"\\nStandard Chosen: \"+self.StandardsName[slider_value-1])\n\n            right_label_text = self.StandardsName[slider_value-1]\n\n        else:\n            standardchosen = self._given_Standard\n            self.textbox.setText(self.reference + \"\\n You are using Self Defined Standard\")\n            right_label_text = \"Self Defined Standard\"\n\n        for i in range(len(raw)):\n            # raw.at[i, 'DataType'] == 'User' or raw.at[i, 'DataType'] == 'user' or raw.at[i, 'DataType'] == 'USER'\n\n            TmpLabel = ''\n            LinesX = []\n            LinesY = []\n            TmpEu = raw.at[i, 'Eu'] / standardchosen['Eu']\n            TmpSm = raw.at[i, 'Sm'] / standardchosen['Sm']\n            TmpGd = raw.at[i, 'Gd'] / standardchosen['Gd']\n            TmpCe = raw.at[i, 'Ce'] / standardchosen['Ce']\n            TmpLa = raw.at[i, 'La'] / standardchosen['La']\n            TmpPr = raw.at[i, 'Pr'] / standardchosen['Pr']\n            TmpNd = raw.at[i, 'Nd'] / standardchosen['Nd']\n            TmpYb = raw.at[i, 'Yb'] / standardchosen['Yb']\n\n            algebraEu = 2*TmpEu/(TmpSm+TmpGd)\n            geometricEu = TmpEu/np.power((TmpSm*TmpGd),0.5)\n\n            firstCe=2*TmpCe/(TmpLa+TmpPr)\n            secondCe=3*TmpCe/(2*TmpLa+TmpNd)\n\n            LaYb=TmpLa/TmpYb\n            LaSm=TmpLa/TmpSm\n            GdYb=TmpGd/TmpYb\n\n\n            tmpLREEResult = 0\n            tmpMREEResult = 0\n            tmpHREEResult = 0\n            tmpWholeResult = 0\n            tmpBinLREE=0\n            tmpBinHREE=0\n\n            for j in self.Element:\n\n                if j in self.LREE:\n                    tmpLREEResult += raw.at[i, j]\n                elif j in self.MREE:\n                    tmpMREEResult += raw.at[i, j]\n                elif j in self.HREE:\n                    tmpHREEResult += raw.at[i, j]\n\n                if j in self.BinLREE:\n                    tmpBinLREE += raw.at[i, j]\n                elif j in self.BinHREE:\n                    tmpBinHREE += raw.at[i, j]\n\n                tmpWholeResult+= raw.at[i, j]\n\n\n\n            self.LabelList.append(raw.at[i, 'Label'])\n            self.algebraDeltaEuList.append( algebraEu )\n            self.geometricDeltaEuList.append( geometricEu )\n            self.LaPrDeltaCeList.append(firstCe)\n            self.LaNdDeltaCeList.append(secondCe)\n            self.LaSmList.append(LaSm)\n            self.LaYbList.append(LaYb)\n            self.GdYbList.append(GdYb)\n            self.LREEList.append( tmpLREEResult )\n            self.MREEList.append( tmpMREEResult )\n            self.HREEList.append( tmpHREEResult )\n            self.ALLREEList.append( tmpWholeResult )\n            self.BinHREEList.append(tmpBinHREE)\n            self.BinLREEList.append(tmpBinLREE)\n            self.L_HREEList.append(tmpBinLREE/tmpBinHREE)\n\n            '''\n            for i in self.data_to_norm.columns.values.tolist():\n                if i not in self.Element:\n                    self.data_to_norm = self.data_to_norm.drop(i, 1)            \n            '''\n\n            Y_bottom=0\n            Y_top=0\n\n            for j in range(len(self.Element)):\n                tmp = raw.at[i, self.Element[j]] / standardchosen[self.Element[j]]\n                self.data_to_norm.at[i, self.Element[j]] = tmp\n                tmpflag = 1\n                a = 0\n                try:\n                    a = math.log(tmp, 10)\n                except(ValueError):\n                    tmpflag = 0\n                    pass\n                if (tmpflag == 1):\n                    if Y_bottom >a:Y_bottom =a\n                    if Y_top<a:Y_top=a\n\n            self.axes.set_ylim(Y_bottom, Y_top+1)\n\n            if item_value == 0:\n                self.item_left_label.setText('Show All')\n                for j in range(len(self.Element)):\n                    tmp = raw.at[i, self.Element[j]] / standardchosen[self.Element[j]]\n                    self.data_to_norm.at[i, self.Element[j]] = tmp\n                    tmpflag = 1\n                    a = 0\n                    try:\n                        a = math.log(tmp, 10)\n                    except(ValueError):\n                        tmpflag = 0\n                        pass\n                    if (tmpflag == 1):\n                        if (raw.at[i, 'Label'] in PointLabels or raw.at[i, 'Label'] == ''):\n                            self.WholeData.append(math.log(tmp, 10))\n                            TmpLabel = ''\n                        else:\n                            PointLabels.append(raw.at[i, 'Label'])\n                            TmpLabel = raw.at[i, 'Label']\n\n                        LinesY.append(a)\n                        LinesX.append(j + 1)\n                        self.axes.scatter(j + 1, math.log(tmp, 10), marker=raw.at[i, 'Marker'],\n                                          s=raw.at[i, 'Size'], color=raw.at[i, 'Color'], alpha=raw.at[i, 'Alpha'],\n                                          label=TmpLabel)\n                self.axes.plot(LinesX, LinesY, color=raw.at[i, 'Color'], linewidth=raw.at[i, 'Width'],\n                               linestyle=raw.at[i, 'Style'], alpha=raw.at[i, 'Alpha'])\n\n                if (self.show_data_index_cb.isChecked()):\n\n                    if 'Index' in self._df_back.columns.values:\n                        self.axes.annotate(self._df_back.at[i, 'Index'],\n                                           xy=(LinesX[-1], LinesY[-1]),\n                                           color=self._df.at[i, 'Color'],\n                                           alpha=self._df.at[i, 'Alpha'])\n                    else:\n                        self.axes.annotate('No' + str(i + 1),\n                                           xy=(LinesX[-1],\n                                               LinesY[-1]),\n                                           color=self._df.at[i, 'Color'],\n                                           alpha=self._df.at[i, 'Alpha'])\n\n\n\n\n\n            else:\n                self.item_left_label.setText(self.AllLabel[item_value-1])\n\n                for j in range(len(self.Element)):\n                    tmp = raw.at[i, self.Element[j]] / standardchosen[self.Element[j]]\n                    self.data_to_norm.at[i, self.Element[j]] = tmp\n                    tmpflag = 1\n                    a = 0\n                    try:\n                        a = math.log(tmp, 10)\n                    except(ValueError):\n                        tmpflag = 0\n                        pass\n                    if (tmpflag == 1):\n                        if (raw.at[i, 'Label'] in PointLabels or raw.at[i, 'Label'] == ''):\n                            TmpLabel = ''\n                        else:\n                            PointLabels.append(raw.at[i, 'Label'])\n                            TmpLabel = raw.at[i, 'Label']\n                        alpha= raw.at[i, 'Alpha']\n                        linewidth= raw.at[i, 'Width']\n                        pointsize= raw.at[i, 'Size']\n\n                        if raw.at[i, 'Label'] == self.AllLabel[item_value - 1]:\n                            LinesY.append(a)\n                            LinesX.append(j + 1)\n                            self.WholeData.append(math.log(tmp, 10))\n                            self.axes.scatter(j + 1, math.log(tmp, 10), marker=raw.at[i, 'Marker'],\n                                              s=pointsize, color=raw.at[i, 'Color'], alpha=alpha,\n                                              label=TmpLabel)\n                        self.axes.plot(LinesX, LinesY, color=raw.at[i, 'Color'], linewidth=linewidth,linestyle=raw.at[i, 'Style'], alpha=alpha)\n\n                print(LinesX,LinesY)\n\n                if (self.show_data_index_cb.isChecked()):\n\n                    if len(LinesX) > 0 and len(LinesY) > 0:\n                        if 'Index' in self._df_back.columns.values:\n                            self.axes.annotate(self._df_back.at[i, 'Index'],\n                                               xy=(LinesX[-1], LinesY[-1]),\n                                               color=self._df.at[i, 'Color'],\n                                               alpha=self._df.at[i, 'Alpha'])\n                        else:\n                            self.axes.annotate('No' + str(i + 1),\n                                               xy=(LinesX[-1], LinesY[-1]),\n                                               color=self._df.at[i, 'Color'],\n                                               alpha=self._df.at[i, 'Alpha'])\n\n\n        Tale = 0\n        Head = 0\n\n        if (len(self.WholeData) > 0):\n            Tale = min(self.WholeData)\n            Head = max(self.WholeData)+0.5\n\n        Location = round(Tale - (Head - Tale) / 5)\n\n        count = round((Head - Tale) / 5 * 7) + 1\n\n\n\n        if (self.legend_cb.isChecked()):\n            self.axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, prop=fontprop)\n\n\n\n\n        self.standard_right_label.setText(right_label_text)\n\n\n        self.yticks = [Location+i for i in range(count)]\n        self.yticklabels = [str(np.power(10.0, (Location + i))) for i in range(count)]\n\n        self.axes.set_yticks(self.yticks)\n        self.axes.set_yticklabels(self.yticklabels, fontsize=6)\n\n\n        #self.axes.set_yscale('log')\n\n        self.axes.set_xticks(self.xticks)\n        self.axes.set_xticklabels(self.xticklabels, rotation=-45, fontsize=6)\n\n\n\n\n\n\n        self.canvas.draw()\n\n\n\n\n        self.OutPutTitle='REE'\n\n        #print(len(self.algebraDeltaEuList))\n\n        self.OutPutData = pd.DataFrame(\n            {'Label': self.LabelList,\n             'Eu/Eu*(algebra)': self.algebraDeltaEuList,\n             'Eu/Eu*(square)': self.geometricDeltaEuList,\n\n             'Ce/Ce*(LaPr)': self.LaPrDeltaCeList,\n             'Ce/Ce*(LaNd)': self.LaNdDeltaCeList,\n\n             '(La/Sm)N':self.LaSmList,\n             '(La/Yb)N':self.LaYbList,\n             '(Gd/Yb)N':self.GdYbList,\n\n             'trisection LREE': self.LREEList,\n             'trisection MREE': self.MREEList,\n             'trisection HREE': self.HREEList,\n             'bisection LREE': self.BinLREEList,\n             'bisection HREE': self.BinHREEList,\n             'LREE/HREE':self.L_HREEList,\n\n             'ALLREE': self.ALLREEList\n             })\n\n        #print('middle ',len(self.OutPutData['Eu/Eu*(algebra)']))\n\n\n        '''\n        self.LaPrDeltaCeList.append(firstCe)\n        self.LaNdDeltaCeList.append(secondCe)\n        '''\n\n        self.OutPutFig=self.fig", "response": "This method is used to display a REE image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_main_frame(self):\n        self.resize(800, 600)\n        self.main_frame = QWidget()\n        self.dpi = 128\n\n\n        self.save_button = QPushButton('&Save Result')\n        self.save_button.clicked.connect(self.saveResult)\n\n\n\n        self.qapf_button = QPushButton('&QAPF')\n        self.qapf_button.clicked.connect(self.QAPF)\n\n\n        '''\n        self.tableView = CustomQTableView(self.main_frame)\n        self.tableView.setObjectName('tableView')\n        self.tableView.setSortingEnabled(True)\n\n        '''\n\n\n        self.tableViewMole = CustomQTableView(self.main_frame)\n        self.tableViewMole.setObjectName('tableViewMole')\n        self.tableViewMole.setSortingEnabled(True)\n\n        self.tableViewWeight = CustomQTableView(self.main_frame)\n        self.tableViewWeight.setObjectName('tableViewWeight')\n        self.tableViewWeight.setSortingEnabled(True)\n\n        self.tableViewVolume = CustomQTableView(self.main_frame)\n        self.tableViewVolume.setObjectName('tableViewVolume')\n        self.tableViewVolume.setSortingEnabled(True)\n\n        self.tableViewCalced = CustomQTableView(self.main_frame)\n        self.tableViewCalced.setObjectName('tableViewCalced')\n        self.tableViewCalced.setSortingEnabled(True)\n        #\n        # Layout with box sizers\n        #\n\n        self.hbox = QHBoxLayout()\n\n        for w in [self.qapf_button]:\n            self.hbox.addWidget(w)\n            self.hbox.setAlignment(w, Qt.AlignVCenter)\n\n        self.vbox = QVBoxLayout()\n\n        self.vbox.addWidget(self.tableViewMole)\n        self.vbox.addWidget(self.tableViewWeight)\n        self.vbox.addWidget(self.tableViewVolume)\n        self.vbox.addWidget(self.tableViewCalced)\n        self.vbox.addWidget(self.save_button)\n\n        self.vbox.addLayout(self.hbox)\n\n        self.main_frame.setLayout(self.vbox)\n        self.setCentralWidget(self.main_frame)", "response": "Create the main frame for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef singleCalc(self,\n                   m={'Al2O3': 13.01, 'Alpha': 0.6, 'Ba': 188.0, 'Be': 0.85, 'CaO': 8.35, 'Ce': 28.2, 'Co': 45.2,\n                      'Cr': 117.0, 'Cs': 0.83, 'Cu': 53.5, 'Dy': 5.58, 'Er': 2.96, 'Eu': 1.79, 'Fe2O3': 14.47,\n                      'FeO': 5.51, 'Ga': 19.4, 'Gd': 5.24, 'Hf': 3.38, 'Ho': 1.1, 'K2O': 0.72, 'LOI': 5.05,\n                      'La': 11.4, 'Label': 'ZhangSH2016', 'Li': 15.0, 'Lu': 0.39, 'Mg#': 41.9, 'MgO': 5.26,\n                      'MnO': 0.21, 'Na2O': 1.88, 'Nb': 12.6, 'Nd': 18.4, 'Ni': 69.4, 'P2O5': 0.23, 'Pb': 3.17,\n                      'Pr': 3.95, 'Rb': 18.4, 'Sc': 37.4, 'SiO2': 48.17, 'Size': 10, 'Sm': 5.08, 'Sr': 357,\n                      'Ta': 0.77, 'Tb': 0.88, 'Th': 1.85, 'TiO2': 2.56, 'Tl': 0.06, 'Tm': 0.44, 'Total': 99.91,\n                      'U': 0.41, 'V': 368.0, 'Y': 29.7, 'Yb': 2.68, 'Zn': 100.0, 'Zr': 130.0, }):\n\n        DataResult={}\n        DataWeight={}\n        DataVolume={}\n        DataCalced={}\n\n\n        DataResult.update({'Label': m['Label']})\n        DataWeight.update({'Label': m['Label']})\n        DataVolume.update({'Label': m['Label']})\n        DataCalced.update({'Label': m['Label']})\n        DataResult.update({'Width': m['Width']})\n        DataWeight.update({'Width': m['Width']})\n        DataVolume.update({'Width': m['Width']})\n        DataCalced.update({'Width': m['Width']})\n        DataResult.update({'Style': m['Style']})\n        DataWeight.update({'Style': m['Style']})\n        DataVolume.update({'Style': m['Style']})\n        DataCalced.update({'Style': m['Style']})\n        DataResult.update({'Alpha': m['Alpha']})\n        DataWeight.update({'Alpha': m['Alpha']})\n        DataVolume.update({'Alpha': m['Alpha']})\n        DataCalced.update({'Alpha': m['Alpha']})\n        DataResult.update({'Size': m['Size']})\n        DataWeight.update({'Size': m['Size']})\n        DataVolume.update({'Size': m['Size']})\n        DataCalced.update({'Size': m['Size']})\n        DataResult.update({'Color': m['Color']})\n        DataWeight.update({'Color': m['Color']})\n        DataVolume.update({'Color': m['Color']})\n        DataCalced.update({'Color': m['Color']})\n        DataResult.update({'Marker': m['Marker']})\n        DataWeight.update({'Marker': m['Marker']})\n        DataVolume.update({'Marker': m['Marker']})\n        DataCalced.update({'Marker': m['Marker']})\n\n        WholeMass = 0\n        EachMole = {}\n\n        for j in self.Elements:\n            '''\n            Get the Whole Mole of the dataset\n            '''\n\n            try:\n                T_TMP = m[j]\n            except(KeyError):\n                T_TMP = 0\n\n            if j == 'Sr':\n                TMP = T_TMP / (87.62 / 103.619 * 10000)\n            elif j == 'Ba':\n                TMP = T_TMP / (137.327 / 153.326 * 10000)\n            elif j == 'Ni':\n                TMP = T_TMP / (58.6934 / 74.69239999999999 * 10000)\n            elif j == 'Cr':\n                TMP = T_TMP / ((2 * 51.9961) / 151.98919999999998 * 10000)\n            elif j == 'Zr':\n                # Zr Multi 2 here\n                TMP = T_TMP / ((2 * 91.224) / 123.22200000000001 * 10000)\n            else:\n                TMP = T_TMP\n            V = TMP\n            try:\n                WholeMass += float(V)\n            except ValueError:\n                pass\n\n\n\n        WeightCorrectionFactor = (100 / WholeMass)\n\n        for j in self.Elements:\n            '''\n            Get the Mole percentage of each element\n            '''\n\n            try:\n                T_TMP = m[j]\n            except(KeyError):\n                T_TMP = 0\n\n            if j == 'Sr':\n                TMP = T_TMP / (87.62 / 103.619 * 10000)\n\n            elif j == 'Ba':\n                TMP = T_TMP / (137.327 / 153.326 * 10000)\n\n            elif j == 'Ni':\n                TMP = T_TMP / (58.6934 / 74.69239999999999 * 10000)\n\n            elif j == 'Cr':\n                TMP = T_TMP / ((2 * 51.9961) / 151.98919999999998 * 10000)\n\n            elif j == 'Zr':\n                # Zr not Multiple by 2 Here\n                TMP = T_TMP / ((91.224) / 123.22200000000001 * 10000)\n\n            else:\n                TMP = T_TMP\n\n            try:\n                M = TMP / self.BaseMass[j] * WeightCorrectionFactor\n            except TypeError:\n                pass\n\n            # M= TMP/NewMass(j) * WeightCorrectionFactor\n\n            EachMole.update({j: M})\n        # self.DataMole.append(EachMole)\n\n        DataCalculating = EachMole\n\n        Fe3 = DataCalculating['Fe2O3']\n        Fe2 = DataCalculating['FeO']\n        Mg = DataCalculating['MgO']\n        Ca = DataCalculating['CaO']\n        Na = DataCalculating['Na2O']\n\n        try:\n            DataCalced.update({'Fe3+/(Total Fe) in rock (Mole)': 100 * Fe3 * 2 / (Fe3 * 2 + Fe2)})\n        except(ZeroDivisionError):\n            DataCalced.update({'Fe3+/(Total Fe) in rock (Mole)': 0})\n            pass\n\n        try:\n            DataCalced.update({'Mg/(Mg+Total Fe) in rock (Mole)': 100 * Mg / (Mg + Fe3 * 2 + Fe2)})\n        except(ZeroDivisionError):\n            DataCalced.update({'Mg/(Mg+Total Fe) in rock (Mole)': 0})\n            pass\n\n        try:\n            DataCalced.update({'Mg/(Mg+Fe2+) in rock (Mole)': 100 * Mg / (Mg + Fe2)})\n        except(ZeroDivisionError):\n            DataCalced.update({'Mg/(Mg+Fe2+) in rock (Mole)': 0})\n            pass\n\n        try:\n            DataCalced.update({'Ca/(Ca+Na) in rock (Mole)': 100 * Ca / (Ca + Na * 2)})\n        except(ZeroDivisionError):\n            DataCalced.update({'Ca/(Ca+Na) in rock (Mole)': 0})\n            pass\n\n        DataCalculating['CaO'] += DataCalculating['Sr']\n        DataCalculating['Sr'] = 0\n\n        DataCalculating['K2O'] += 2 * DataCalculating['Ba']\n        DataCalculating['Ba'] = 0\n\n        try:\n            if DataCalculating['CaO'] >= 10 / 3 * DataCalculating['P2O5']:\n                DataCalculating['CaO'] -= 10 / 3 * DataCalculating['P2O5']\n            else:\n                DataCalculating['CaO'] = 0\n        except(ZeroDivisionError):\n            pass\n\n        DataCalculating['P2O5'] = DataCalculating['P2O5'] / 1.5\n\n        Apatite = DataCalculating['P2O5']\n\n        # IF(S19>=T15,S19-T15,0)\n\n        if DataCalculating['F'] >= DataCalculating['P2O5']:\n            DataCalculating['F'] -= DataCalculating['P2O5']\n        else:\n            DataCalculating['F'] = 0\n\n        if DataCalculating['F'] >= DataCalculating['P2O5']:\n            DataCalculating['F'] -= DataCalculating['P2O5']\n        else:\n            DataCalculating['F'] = 0\n\n        if DataCalculating['Na2O'] >= DataCalculating['Cl']:\n            DataCalculating['Na2O'] -= DataCalculating['Cl']\n        else:\n            DataCalculating['Na2O'] = 0\n\n        Halite = DataCalculating['Cl']\n\n        # IF(U12>=(U19/2),U12-(U19/2),0)\n        if DataCalculating['CaO'] >= 0.5 * DataCalculating['F']:\n            DataCalculating['CaO'] -= 0.5 * DataCalculating['F']\n        else:\n            DataCalculating['CaO'] = 0\n\n        DataCalculating['F'] *= 0.5\n\n        Fluorite = DataCalculating['F']\n\n        # =IF(V17>0,IF(V13>=V17,'Thenardite',IF(V13>0,'Both','Anhydrite')),'None')\n        AorT = 0\n        if DataCalculating['SO3'] <= 0:\n            AorT = 'None'\n        else:\n            if DataCalculating['Na2O'] >= DataCalculating['SO3']:\n                AorT = 'Thenardite'\n            else:\n                if DataCalculating['Na2O'] > 0:\n                    AorT = 'Both'\n                else:\n                    AorT = 'Anhydrite'\n\n        # =IF(W26='Anhydrite',V17,IF(W26='Both',V12,0))\n        # =IF(W26='Thenardite',V17,IF(W26='Both',V17-W17,0))\n\n        if AorT == 'Anhydrite':\n            DataCalculating['Sr'] = 0\n        elif AorT == 'Thenardite':\n            DataCalculating['Sr'] = DataCalculating['SO3']\n            DataCalculating['SO3'] = 0\n        elif AorT == 'Both':\n            DataCalculating['Sr'] = DataCalculating['SO3'] - DataCalculating['CaO']\n            DataCalculating['SO3'] = DataCalculating['CaO']\n        else:\n            DataCalculating['SO3'] = 0\n            DataCalculating['Sr'] = 0\n\n        DataCalculating['CaO'] -= DataCalculating['SO3']\n\n        DataCalculating['Na2O'] -= DataCalculating['Sr']\n\n        Anhydrite = DataCalculating['SO3']\n        Thenardite = DataCalculating['Sr']\n\n        Pyrite = 0.5 * DataCalculating['S']\n\n        # =IF(W9>=(W18*0.5),W9-(W18*0.5),0)\n\n        if DataCalculating['FeO'] >= DataCalculating['S'] * 0.5:\n            DataCalculating['FeO'] -= DataCalculating['S'] * 0.5\n        else:\n            DataCalculating['FeO'] = 0\n\n        # =IF(X24>0,IF(X9>=X24,'Chromite',IF(X9>0,'Both','Magnesiochromite')),'None')\n\n        if DataCalculating['Cr'] > 0:\n            if DataCalculating['FeO'] >= DataCalculating['Cr']:\n                CorM = 'Chromite'\n            elif DataCalculating['FeO'] > 0:\n                CorM = 'Both'\n            else:\n                CorM = 'Magnesiochromite'\n        else:\n            CorM = 'None'\n\n        # =IF(Y26='Chromite',X24,IF(Y26='Both',X9,0))\n        # =IF(Y26='Magnesiochromite',X24,IF(Y26='Both',X24-Y24,0))\n\n        if CorM == 'Chromite':\n            DataCalculating['Cr'] = DataCalculating['Cr']\n            DataCalculating['Ni'] = 0\n\n        elif CorM == 'Magnesiochromite':\n            DataCalculating['Ni'] = DataCalculating['Cr']\n            DataCalculating['Cr'] = 0\n\n        elif CorM == 'Both':\n            DataCalculating['Ni'] = DataCalculating['Cr'] - DataCalculating['FeO']\n            DataCalculating['Cr'] = DataCalculating['FeO']\n\n        else:\n            DataCalculating['Cr'] = 0\n            DataCalculating['Ni'] = 0\n\n        DataCalculating['MgO'] -= DataCalculating['Ni']\n\n        Magnesiochromite = DataCalculating['Ni']\n        Chromite = DataCalculating['Cr']\n\n        # =IF(X9>=Y24,X9-Y24,0)\n\n        if DataCalculating['FeO'] >= DataCalculating['Cr']:\n            DataCalculating['FeO'] -= DataCalculating['Cr']\n        else:\n            DataCalculating['FeO'] = 0\n\n        # =IF(Y6>0,IF(Y9>=Y6,'Ilmenite',IF(Y9>0,'Both','Sphene')),'None')\n\n        if DataCalculating['TiO2'] < 0:\n            IorS = 'None'\n        else:\n            if DataCalculating['FeO'] >= DataCalculating['TiO2']:\n                IorS = 'Ilmenite'\n            else:\n                if DataCalculating['FeO'] > 0:\n                    IorS = 'Both'\n                else:\n                    IorS = 'Sphene'\n\n        # =IF(Z26='Ilmenite',Y6,IF(Z26='Both',Y9,0))\n        # =IF(Z26='Sphene',Y6,IF(Z26='Both',Y6-Z6,0))\n\n        if IorS == 'Ilmenite':\n            DataCalculating['TiO2'] = DataCalculating['TiO2']\n            DataCalculating['MnO'] = 0\n\n        elif IorS == 'Sphene':\n            DataCalculating['MnO'] = DataCalculating['TiO2']\n            DataCalculating['TiO2'] = 0\n\n        elif IorS == 'Both':\n\n            DataCalculating['MnO'] = DataCalculating['TiO2'] - DataCalculating['FeO']\n            DataCalculating['TiO2'] = DataCalculating['FeO']\n\n        else:\n            DataCalculating['TiO2'] = 0\n            DataCalculating['MnO'] = 0\n\n        DataCalculating['FeO'] -= DataCalculating['TiO2']\n\n        Ilmenite = DataCalculating['TiO2']\n\n        # =IF(Z16>0,IF(Z12>=Z16,'Calcite',IF(Z12>0,'Both','Na2CO3')),'None')\n\n        if DataCalculating['CO2'] <= 0:\n            CorN = 'None'\n        else:\n            if DataCalculating['CaO'] >= DataCalculating['CO2']:\n                CorN = 'Calcite'\n            else:\n                if DataCalculating['CaO'] > 0:\n                    CorN = 'Both'\n                else:\n                    CorN = 'Na2CO3'\n\n        # =IF(AA26='Calcite',Z16,IF(AA26='Both',Z12,0))\n\n        # =IF(AA26='Na2CO3',Z16,IF(AA26='Both',Z16-AA16,0))\n\n        if CorN == 'None':\n            DataCalculating['CO2'] = 0\n            DataCalculating['SO3'] = 0\n\n        elif CorN == 'Calcite':\n            DataCalculating['CO2'] = DataCalculating['CO2']\n            DataCalculating['SO3'] = 0\n\n        elif CorN == 'Na2CO3':\n            DataCalculating['SO3'] = DataCalculating['SO3']\n            DataCalculating['CO2'] = 0\n\n        elif CorN == 'Both':\n            DataCalculating['SO3'] = DataCalculating['CO2'] - DataCalculating['CaO']\n            DataCalculating['CO2'] = DataCalculating['CaO']\n\n        DataCalculating['CaO'] -= DataCalculating['CO2']\n\n        Calcite = DataCalculating['CO2']\n\n        Na2CO3 = DataCalculating['SO3']\n\n        # =IF(AA17>Z13,0,Z13-AA17)\n        if DataCalculating['SO3'] > DataCalculating['Na2O']:\n            DataCalculating['Na2O'] = 0\n        else:\n            DataCalculating['Na2O'] -= DataCalculating['SO3']\n\n        DataCalculating['SiO2'] -= DataCalculating['Zr']\n        Zircon = DataCalculating['Zr']\n\n        # =IF(AB14>0,IF(AB7>=AB14,'Orthoclase',IF(AB7>0,'Both','K2SiO3')),'None')\n\n        if DataCalculating['K2O'] <= 0:\n            OorK = 'None'\n        else:\n            if DataCalculating['Al2O3'] >= DataCalculating['K2O']:\n                OorK = 'Orthoclase'\n            else:\n                if DataCalculating['Al2O3'] > 0:\n                    OorK = 'Both'\n                else:\n                    OorK = 'K2SiO3'\n\n        # =IF(AC26='Orthoclase',AB14,IF(AC26='Both',AB7,0))\n        # =IF(AC26='K2SiO3',AB14,IF(AC26='Both',AB14-AB7,0))\n\n        if OorK == 'None':\n            DataCalculating['K2O'] = 0\n            DataCalculating['P2O5'] = 0\n\n\n        elif OorK == 'Orthoclase':\n            DataCalculating['K2O'] = DataCalculating['K2O']\n            DataCalculating['P2O5'] = 0\n\n\n        elif OorK == 'K2SiO3':\n            DataCalculating['P2O5'] = DataCalculating['K2O']\n            DataCalculating['K2O'] = 0\n\n\n\n        elif OorK == 'Both':\n\n            DataCalculating['P2O5'] = DataCalculating['K2O'] - DataCalculating['Al2O3']\n            DataCalculating['K2O'] = DataCalculating['Al2O3']\n\n        DataCalculating['Al2O3'] -= DataCalculating['K2O']\n\n        # =IF(AC13>0,IF(AC7>=AC13,'Albite',IF(AC7>0,'Both','Na2SiO3')),'None')\n\n        if DataCalculating['Na2O'] <= 0:\n            AorN = 'None'\n        else:\n            if DataCalculating['Al2O3'] >= DataCalculating['Na2O']:\n                AorN = 'Albite'\n            else:\n                if DataCalculating['Al2O3'] > 0:\n                    AorN = 'Both'\n                else:\n                    AorN = 'Na2SiO3'\n\n        # =IF(AND(AC7>=AC13,AC7>0),AC7-AC13,0)\n\n        if DataCalculating['Al2O3'] >= DataCalculating['Na2O'] and DataCalculating['Al2O3'] > 0:\n            DataCalculating['Al2O3'] -= DataCalculating['Na2O']\n        else:\n            DataCalculating['Al2O3'] = 0\n\n        # =IF(AD26='Albite',AC13,IF(AD26='Both',AC7,0))\n        # =IF(AD26='Na2SiO3',AC13,IF(AD26='Both',AC13-AD13,0))\n\n        if AorN == 'Albite':\n            DataCalculating['Cl'] = 0\n\n        elif AorN == 'Both':\n            DataCalculating['Cl'] = DataCalculating['Na2O'] - DataCalculating['Al2O3']\n            DataCalculating['Na2O'] = DataCalculating['Al2O3']\n\n        elif AorN == 'Na2SiO3':\n            DataCalculating['Cl'] = DataCalculating['Na2O']\n            DataCalculating['Na2O'] = 0\n\n        elif AorN == 'None':\n            DataCalculating['Na2O'] = 0\n            DataCalculating['Cl'] = 0\n\n        # =IF(AD7>0,IF(AD12>0,'Anorthite','None'),'None')\n\n        '''\n        Seem like should be =IF(AD7>0,IF(AD12>AD7,'Anorthite','Corundum'),'None')\n\n        If Al2O3 is left after alloting orthoclase and albite, then:\n        Anorthite = Al2O3, CaO = CaO - Al2O3, SiO2 = SiO2 - 2 Al2O3, Al2O3 = 0\n        If Al2O3 exceeds CaO in the preceding calculation, then:\n        Anorthite = CaO, Al2O3 = Al2O3 - CaO, SiO2 = SiO2 - 2 CaO\n        Corundum = Al2O3, CaO =0, Al2O3 = 0\n\n\n            if DataCalculating['Al2O3']<=0:\n                AorC='None'\n            else:\n                if DataCalculating['CaO']>DataCalculating['Al2O3']:\n                    AorC= 'Anorthite'\n                else:\n                    Aorc='Corundum'\n\n        '''\n\n        if DataCalculating['Al2O3'] <= 0:\n            AorC = 'None'\n        else:\n            if DataCalculating['CaO'] > 0:\n                AorC = 'Anorthite'\n            else:\n                Aorc = 'None'\n\n        # =IF(AE26='Anorthite',IF(AD12>AD7,0,AD7-AD12),AD7)\n\n        # =IF(AE26='Anorthite',IF(AD7>AD12,0,AD12-AD7),AD12)\n\n        # =IF(AE26='Anorthite',IF(AD7>AD12,AD12,AD7),0)\n\n        if AorC == 'Anorthite':\n            if DataCalculating['Al2O3'] >= DataCalculating['CaO']:\n                DataCalculating['Sr'] = DataCalculating['CaO']\n                DataCalculating['Al2O3'] -= DataCalculating['CaO']\n                DataCalculating['CaO'] = 0\n\n            else:\n                DataCalculating['Sr'] = DataCalculating['Al2O3']\n                DataCalculating['CaO'] -= DataCalculating['Al2O3']\n                DataCalculating['Al2O3'] = 0\n\n        else:\n            DataCalculating['Sr'] = 0\n\n        Corundum = DataCalculating['Al2O3']\n        Anorthite = DataCalculating['Sr']\n\n        # =IF(AE10>0,IF(AE12>=AE10,'Sphene',IF(AE12>0,'Both','Rutile')),'None')\n\n        if DataCalculating['MnO'] <= 0:\n            SorR = 'None'\n        else:\n            if DataCalculating['CaO'] >= DataCalculating['MnO']:\n                SorR = 'Sphene'\n            elif DataCalculating['CaO'] > 0:\n                SorR = 'Both'\n            else:\n                SorR = 'Rutile'\n\n        # =IF(AF26='Sphene',AE10,IF(AF26='Both',AE12,0))\n\n        # =IF(AF26='Rutile',AE10,IF(AF26='Both',AE10-AE12,0))\n\n        if SorR == 'Sphene':\n            DataCalculating['MnO'] = DataCalculating['MnO']\n            DataCalculating['S'] = 0\n\n        elif SorR == 'Rutile':\n            DataCalculating['S'] = DataCalculating['MnO']\n            DataCalculating['MnO'] = 0\n\n\n        elif SorR == 'Both':\n            DataCalculating['S'] = DataCalculating['MnO'] - DataCalculating['CaO']\n            DataCalculating['MnO'] = DataCalculating['CaO']\n\n        elif SorR == 'None':\n            DataCalculating['MnO'] = 0\n            DataCalculating['S'] = 0\n\n        DataCalculating['CaO'] -= DataCalculating['MnO']\n\n        Rutile = DataCalculating['S']\n\n        # =IF(AND(AF20>0),IF(AF8>=AF20,'Acmite',IF(AF8>0,'Both','Na2SiO3')),'None')\n\n        if DataCalculating['Cl'] <= 0:\n            ACorN = 'None'\n        else:\n            if DataCalculating['Fe2O3'] >= DataCalculating['Cl']:\n                ACorN = 'Acmite'\n            else:\n                if DataCalculating['Fe2O3'] > 0:\n                    ACorN = 'Both'\n                else:\n                    ACorN = 'Na2SiO3'\n\n        # =IF(AG26='Acmite',AF20,IF(AG26='Both',AF8,0))\n\n        # =IF(AG26='Na2SiO3',AF20,IF(AG26='Both',AF20-AG19,0))\n\n        if ACorN == 'Acmite':\n            DataCalculating['F'] = DataCalculating['Cl']\n            DataCalculating['Cl'] = 0\n\n        elif ACorN == 'Na2SiO3':\n            DataCalculating['Cl'] = DataCalculating['Cl']\n            DataCalculating['F'] = 0\n\n        elif ACorN == 'Both':\n            DataCalculating['F'] = DataCalculating['Fe2O3']\n            DataCalculating['Cl'] = DataCalculating['Cl'] - DataCalculating['F']\n\n        elif ACorN == 'None':\n            DataCalculating['F'] = 0\n            DataCalculating['Cl'] = 0\n\n        DataCalculating['Fe2O3'] -= DataCalculating['F']\n\n        Acmite = DataCalculating['F']\n\n        # =IF(AG8>0,IF(AG9>=AG8,'Magnetite',IF(AG9>0,'Both','Hematite')),'None')\n\n        if DataCalculating['Fe2O3'] <= 0:\n            MorH = 'None'\n        else:\n            if DataCalculating['FeO'] >= DataCalculating['Fe2O3']:\n                MorH = 'Magnetite'\n            else:\n                if DataCalculating['FeO'] > 0:\n                    MorH = 'Both'\n                else:\n                    MorH = 'Hematite'\n\n        # =IF(AH26='Magnetite',AG8,IF(AH26='Both',AG9,0))\n        # =IF(AH26='Hematite',AG8,IF(AH26='Both',AG8-AG9,0))\n\n        if MorH == 'Magnetite':\n            DataCalculating['Fe2O3'] = DataCalculating['Fe2O3']\n            DataCalculating['Ba'] = 0\n\n        elif MorH == 'Hematite':\n            DataCalculating['Fe2O3'] = 0\n            DataCalculating['Ba'] = DataCalculating['FeO']\n\n\n        elif MorH == 'Both':\n            DataCalculating['Fe2O3'] = DataCalculating['FeO']\n            DataCalculating['Ba'] = DataCalculating['Fe2O3'] - DataCalculating['FeO']\n\n\n        elif MorH == 'None':\n            DataCalculating['Fe2O3'] = 0\n            DataCalculating['Ba'] == 0\n\n        DataCalculating['FeO'] -= DataCalculating['Fe2O3']\n\n        Magnetite = DataCalculating['Fe2O3']\n        Hematite = DataCalculating['Ba']\n\n        # =IF(AH11>0,AH11/(AH11+AH9),0)\n\n        Fe2 = DataCalculating['FeO']\n        Mg = DataCalculating['MgO']\n\n        if Mg > 0:\n            DataCalced.update({'Mg/(Mg+Fe2+) in silicates': 100 * Mg / (Mg + Fe2)})\n        else:\n            DataCalced.update({'Mg/(Mg+Fe2+) in silicates': 0})\n\n        DataCalculating['FeO'] += DataCalculating['MgO']\n\n        DataCalculating['MgO'] = 0\n\n        # =IF(AI12>0,IF(AI9>=AI12,'Diopside',IF(AI9>0,'Both','Wollastonite')),'None')\n\n        if DataCalculating['CaO'] <= 0:\n            DorW = 'None'\n        else:\n            if DataCalculating['FeO'] >= DataCalculating['CaO']:\n                DorW = 'Diopside'\n            else:\n                if DataCalculating['FeO'] > 0:\n                    DorW = 'Both'\n                else:\n                    DorW = 'Wollastonite'\n\n        # =IF(AJ26='Diopside',AI12,IF(AJ26='Both',AI9,0))\n\n        # =IF(AJ26='Wollastonite',AI12,IF(AJ26='Both',AI12-AI9,0))\n\n        if DorW == 'Diopside':\n            DataCalculating['CaO'] = DataCalculating['CaO']\n            DataCalculating['S'] = 0\n\n        elif DorW == 'Wollastonite':\n            DataCalculating['S'] = DataCalculating['CaO']\n            DataCalculating['CaO'] = 0\n\n        elif DorW == 'Both':\n            DataCalculating['S'] = DataCalculating['CaO'] - DataCalculating['FeO']\n            DataCalculating['CaO'] = DataCalculating['FeO']\n\n        elif DorW == 'None':\n            DataCalculating['CaO'] = 0\n            DataCalculating['S'] = 0\n\n        DataCalculating['FeO'] -= DataCalculating['CaO']\n\n        Diopside = DataCalculating['CaO']\n\n        Quartz = DataCalculating['SiO2']\n\n        Zircon = DataCalculating['Zr']\n        K2SiO3 = DataCalculating['P2O5']\n\n        Na2SiO3 = DataCalculating['Cl']\n\n        Sphene = DataCalculating['MnO']\n\n        Hypersthene = DataCalculating['FeO']\n\n        Albite = DataCalculating['Na2O']\n\n        Orthoclase = DataCalculating['K2O']\n\n        Wollastonite = DataCalculating['S']\n\n        # =AJ5-(AL6)-(AL7)-(AL8*2)-(AL12)-(AL9)-(AL10*4)-(AL11*2)-(AL13)-(AL14*6)-(AL15*6)-(AL16)\n\n        Quartz -= (Zircon +\n                   K2SiO3 +\n                   Anorthite * 2 +\n                   Na2SiO3 +\n                   Acmite * 4 +\n                   Diopside * 2 +\n                   Sphene +\n                   Hypersthene +\n                   Albite * 6 +\n                   Orthoclase * 6 +\n                   Wollastonite)\n\n        # =IF(AL5>0,AL5,0)\n\n        if Quartz > 0:\n            Quartz = Quartz\n        else:\n            Quartz = 0\n\n        # =IF(AL13>0,IF(AL5>=0,'Hypersthene',IF(AL13+(2*AL5)>0,'Both','Olivine')),'None')\n\n        if Hypersthene <= 0:\n            HorO = 'None'\n        else:\n            if Quartz >= 0:\n                HorO = 'Hypersthene'\n            else:\n                if Hypersthene + 2 * Quartz > 0:\n                    HorO = 'Both'\n                else:\n                    HorO = 'Olivine'\n\n        # =IF(AN26='Hypersthene',AL13,IF(AN26='Both',AL13+(2*AL5),0))\n        # =IF(AN26='Olivine',AL13*0.5,IF(AN26='Both',ABS(AL5),0))\n        Old_Hypersthene = Hypersthene\n        if HorO == 'Hypersthene':\n            Hypersthene = Hypersthene\n            Olivine = 0\n\n        elif HorO == 'Both':\n            Hypersthene = Hypersthene + Quartz * 2\n            Olivine = abs(Quartz)\n\n        elif HorO == 'Olivine':\n            Olivine = Hypersthene / 2\n            Hypersthene = 0\n\n        elif HorO == 'None':\n            Hypersthene = 0\n            Olivine = 0\n\n        # =AL5+AL13-(AN13+AN17)\n        Quartz += Old_Hypersthene - (Hypersthene + Olivine)\n\n        # =IF(AL12>0,IF(AN5>=0,'Sphene',IF(AL12+AN5>0,'Both','Perovskite')),'None')\n\n        if Sphene <= 0:\n            SorP = 'None'\n        else:\n            if Quartz >= 0:\n                SorP = 'Sphene'\n            else:\n                if Sphene + Quartz > 0:\n                    SorP = 'Both'\n                else:\n                    SorP = 'Perovskite'\n\n        # =IF(AO26='Sphene',AL12,IF(AO26='Both',AL12+AN5,0))\n        # =IF(AO26='Perovskite',AL12,IF(AO26='Both',AL12-AO12,0))\n\n        Old_Sphene = Sphene\n\n        if SorP == 'Sphene':\n            Sphene = Sphene\n            Perovskite = 0\n\n        elif SorP == 'Perovskite':\n            Perovskite = Sphene\n            Sphene = 0\n\n        elif SorP == 'Both':\n            Sphene += Quartz\n            Perovskite = Old_Sphene - Sphene\n\n        elif SorP == 'None':\n            Sphene = 0\n            Perovskite = 0\n\n        Quartz += Old_Sphene - Sphene\n\n        # =IF(AL14>0,IF(AO5>=0,'Albite',IF(AL14+(AO5/4)>0,'Both','Nepheline')),'None')\n\n        if Albite <= 0:\n            AlorNe = 'None'\n        else:\n            if Quartz >= 0:\n                AlorNe = 'Albite'\n            else:\n                if Albite + (Quartz / 4) > 0:\n                    AlorNe = 'Both'\n                else:\n                    AlorNe = 'Nepheline'\n\n        # =AO5+(6*AL14)-(AP14*6)-(AP19*2)\n\n        # =IF(AP26='Albite',AL14,IF(AP26='Both',AL14+(AO5/4),0))\n        # =IF(AP26='Nepheline',AL14,IF(AP26='Both',AL14-AP14,0))\n\n        Old_Albite = Albite\n\n        if AlorNe == 'Albite':\n            Albite = Albite\n            Nepheline = 0\n\n        elif AlorNe == 'Nepheline':\n            Nepheline = Albite\n            Albite = 0\n\n        elif AlorNe == 'Both':\n            Albite += Quartz / 4\n            Nepheline = Old_Albite - Albite\n\n        elif AlorNe == 'None':\n            Nepheline = 0\n            Albite = 0\n\n        Quartz += (6 * Old_Albite) - (Albite * 6) - (Nepheline * 2)\n\n        # =IF(AL8=0,0,AL8/(AL8+(AP14*2)))\n\n        if Anorthite == 0:\n            DataCalced.update({'Plagioclase An content': 0})\n        else:\n            DataCalced.update({'Plagioclase An content': 100 * Anorthite / (Anorthite + 2 * Albite)})\n\n        # =IF(AL15>0,IF(AP5>=0,'Orthoclase',IF(AL15+(AP5/2)>0,'Both','Leucite')),'None')\n\n        if Orthoclase <= 0:\n            OorL = 'None'\n        else:\n            if Quartz >= 0:\n                OorL = 'Orthoclase'\n            else:\n                if Orthoclase + Quartz / 2 > 0:\n                    OorL = 'Both'\n                else:\n                    OorL = 'Leucite'\n\n        # =IF(AQ26='Orthoclase',AL15,IF(AQ26='Both',AL15+(AP5/2),0))\n        # =IF(AQ26='Leucite',AL15,IF(AQ26='Both',AL15-AQ15,0))\n\n        Old_Orthoclase = Orthoclase\n\n        if OorL == 'Orthoclase':\n            Orthoclase = Orthoclase\n            Leucite = 0\n\n        elif OorL == 'Leucite':\n            Leucite = Orthoclase\n            Orthoclase = 0\n\n        elif OorL == 'Both':\n            Orthoclase += Quartz / 2\n            Leucite = Old_Orthoclase - Orthoclase\n\n        elif OorL == 'None':\n            Orthoclase = 0\n            Leucite = 0\n\n        # =AP5+(AL15*6)-(AQ15*6)-(AQ20*4)\n\n        Quartz += (Old_Orthoclase * 6) - (Orthoclase * 6) - (Leucite * 4)\n\n        # =IF(AL16>0,IF(AQ5>=0,'Wollastonite',IF(AL16+(AQ5*2)>0,'Both','Larnite')),'None')\n        if Wollastonite <= 0:\n            WorB = 'None'\n        else:\n            if Quartz >= 0:\n                WorB = 'Wollastonite'\n            else:\n                if Wollastonite + Quartz / 2 > 0:\n                    WorB = 'Both'\n                else:\n                    WorB = 'Larnite'\n\n        # =IF(AR26='Wollastonite',AL16,IF(AR26='Both',AL16+(2*AQ5),0))\n        # =IF(AR26='Larnite',AL16/2,IF(AR26='Both',(AL16-AR16)/2,0))\n\n        Old_Wollastonite = Wollastonite\n        if WorB == 'Wollastonite':\n            Wollastonite = Wollastonite\n            Larnite = 0\n\n        elif WorB == 'Larnite':\n            Larnite = Wollastonite / 2\n            Wollastonite = 0\n\n        elif WorB == 'Both':\n            Wollastonite += Quartz * 2\n            Larnite = (Old_Wollastonite - Wollastonite) / 2\n\n        elif WorB == 'None':\n            Wollastonite = 0\n            Larnite = 0\n\n        # =AQ5+AL16-AR16-AR21\n        Quartz += Old_Wollastonite - Wollastonite - Larnite\n\n        # =IF(AL11>0,IF(AR5>=0,'Diopside',IF(AL11+AR5>0,'Both','LarniteOlivine')),'None')\n\n        if Diopside <= 0:\n            DorL = 'None'\n        else:\n            if Quartz >= 0:\n                DorL = 'Diopside'\n            else:\n                if Diopside + Quartz > 0:\n                    DorL = 'Both'\n                else:\n                    DorL = 'LarniteOlivine'\n\n        # =IF(AS26='Diopside',AL11,IF(AS26='Both',AL11+AR5,0))\n        # =(IF(AS26='LarniteOlivine',AL11/2,IF(AS26='Both',(AL11-AS11)/2,0)))+AN17\n        # =(IF(AS26='LarniteOlivine',AL11/2,IF(AS26='Both',(AL11-AS11)/2,0)))+AR21\n\n        Old_Diopside = Diopside\n        Old_Larnite = Larnite\n        Old_Olivine = Olivine\n        if DorL == 'Diopside':\n            Diopside = Diopside\n\n\n\n        elif DorL == 'LarniteOlivine':\n            Larnite += Diopside / 2\n            Olivine += Diopside / 2\n            Diopside = 0\n\n        elif DorL == 'Both':\n            Diopside += Quartz\n            Larnite += Old_Diopside - Diopside\n            Olivine += Old_Diopside - Diopside\n\n\n\n        elif DorL == 'None':\n            Diopside = 0\n\n        # =AR5+(AL11*2)+AN17+AR21-AS21-(AS11*2)-AS17\n        Quartz += (Old_Diopside * 2) + Old_Olivine + Old_Larnite - Larnite - (Diopside * 2) - Olivine\n\n        # =IF(AQ20>0,IF(AS5>=0,'Leucite',IF(AQ20+(AS5/2)>0,'Both','Kalsilite')),'None')\n\n        if Leucite <= 0:\n            LorK = 'None'\n        else:\n            if Quartz >= 0:\n                LorK = 'Leucite'\n            else:\n                if Leucite + Quartz / 2 > 0:\n                    LorK = 'Both'\n                else:\n                    LorK = 'Kalsilite'\n\n        # =IF(AT26='Leucite',AQ20,IF(AT26='Both',AQ20+(AS5/2),0))\n        # =IF(AT26='Kalsilite',AQ20,IF(AT26='Both',AQ20-AT20,0))\n\n        Old_Leucite = Leucite\n\n        if LorK == 'Leucite':\n            Leucite = Leucite\n            Kalsilite = 0\n\n        elif LorK == 'Kalsilite':\n            Kalsilite = Leucite\n            Leucite = 0\n\n        elif LorK == 'Both':\n            Leucite += Quartz / 2\n            Kalsilite = Old_Leucite - Leucite\n\n        elif LorK == 'None':\n            Leucite = 0\n            Kalsilite = 0\n\n        # =AS5+(AQ20*4)-(AT20*4)-(AT22*2)\n        Quartz += Old_Leucite * 4 - Leucite * 4 - Kalsilite * 2\n\n        Q = Quartz\n        A = Orthoclase\n        P = Anorthite + Albite\n        F = Nepheline + Leucite + Kalsilite\n\n        DataResult.update({'Quartz': Quartz})\n        DataResult.update({'Zircon': Zircon})\n        DataResult.update({'K2SiO3': K2SiO3})\n        DataResult.update({'Anorthite': Anorthite})\n        DataResult.update({'Na2SiO3': Na2SiO3})\n        DataResult.update({'Acmite': Acmite})\n        DataResult.update({'Diopside': Diopside})\n        DataResult.update({'Sphene': Sphene})\n        DataResult.update({'Hypersthene': Hypersthene})\n        DataResult.update({'Albite': Albite})\n        DataResult.update({'Orthoclase': Orthoclase})\n        DataResult.update({'Wollastonite': Wollastonite})\n        DataResult.update({'Olivine': Olivine})\n        DataResult.update({'Perovskite': Perovskite})\n        DataResult.update({'Nepheline': Nepheline})\n        DataResult.update({'Leucite': Leucite})\n        DataResult.update({'Larnite': Larnite})\n        DataResult.update({'Kalsilite': Kalsilite})\n        DataResult.update({'Apatite': Apatite})\n        DataResult.update({'Halite': Halite})\n        DataResult.update({'Fluorite': Fluorite})\n        DataResult.update({'Anhydrite': Anhydrite})\n        DataResult.update({'Thenardite': Thenardite})\n        DataResult.update({'Pyrite': Pyrite})\n        DataResult.update({'Magnesiochromite': Magnesiochromite})\n        DataResult.update({'Chromite': Chromite})\n        DataResult.update({'Ilmenite': Ilmenite})\n        DataResult.update({'Calcite': Calcite})\n        DataResult.update({'Na2CO3': Na2CO3})\n        DataResult.update({'Corundum': Corundum})\n        DataResult.update({'Rutile': Rutile})\n        DataResult.update({'Magnetite': Magnetite})\n        DataResult.update({'Hematite': Hematite})\n        DataResult.update({'Q Mole': Q})\n        DataResult.update({'A Mole': A})\n        DataResult.update({'P Mole': P})\n        DataResult.update({'F Mole': F})\n\n        DataWeight.update({'Quartz': Quartz * self.DataBase['Quartz'][0]})\n        DataWeight.update({'Zircon': Zircon * self.DataBase['Zircon'][0]})\n        DataWeight.update({'K2SiO3': K2SiO3 * self.DataBase['K2SiO3'][0]})\n        DataWeight.update({'Anorthite': Anorthite * self.DataBase['Anorthite'][0]})\n        DataWeight.update({'Na2SiO3': Na2SiO3 * self.DataBase['Na2SiO3'][0]})\n        DataWeight.update({'Acmite': Acmite * self.DataBase['Acmite'][0]})\n        DataWeight.update({'Diopside': Diopside * self.DataBase['Diopside'][0]})\n        DataWeight.update({'Sphene': Sphene * self.DataBase['Sphene'][0]})\n        DataWeight.update({'Hypersthene': Hypersthene * self.DataBase['Hypersthene'][0]})\n        DataWeight.update({'Albite': Albite * self.DataBase['Albite'][0]})\n        DataWeight.update({'Orthoclase': Orthoclase * self.DataBase['Orthoclase'][0]})\n        DataWeight.update({'Wollastonite': Wollastonite * self.DataBase['Wollastonite'][0]})\n        DataWeight.update({'Olivine': Olivine * self.DataBase['Olivine'][0]})\n        DataWeight.update({'Perovskite': Perovskite * self.DataBase['Perovskite'][0]})\n        DataWeight.update({'Nepheline': Nepheline * self.DataBase['Nepheline'][0]})\n        DataWeight.update({'Leucite': Leucite * self.DataBase['Leucite'][0]})\n        DataWeight.update({'Larnite': Larnite * self.DataBase['Larnite'][0]})\n        DataWeight.update({'Kalsilite': Kalsilite * self.DataBase['Kalsilite'][0]})\n        DataWeight.update({'Apatite': Apatite * self.DataBase['Apatite'][0]})\n        DataWeight.update({'Halite': Halite * self.DataBase['Halite'][0]})\n        DataWeight.update({'Fluorite': Fluorite * self.DataBase['Fluorite'][0]})\n        DataWeight.update({'Anhydrite': Anhydrite * self.DataBase['Anhydrite'][0]})\n        DataWeight.update({'Thenardite': Thenardite * self.DataBase['Thenardite'][0]})\n        DataWeight.update({'Pyrite': Pyrite * self.DataBase['Pyrite'][0]})\n        DataWeight.update({'Magnesiochromite': Magnesiochromite * self.DataBase['Magnesiochromite'][0]})\n        DataWeight.update({'Chromite': Chromite * self.DataBase['Chromite'][0]})\n        DataWeight.update({'Ilmenite': Ilmenite * self.DataBase['Ilmenite'][0]})\n        DataWeight.update({'Calcite': Calcite * self.DataBase['Calcite'][0]})\n        DataWeight.update({'Na2CO3': Na2CO3 * self.DataBase['Na2CO3'][0]})\n        DataWeight.update({'Corundum': Corundum * self.DataBase['Corundum'][0]})\n        DataWeight.update({'Rutile': Rutile * self.DataBase['Rutile'][0]})\n        DataWeight.update({'Magnetite': Magnetite * self.DataBase['Magnetite'][0]})\n        DataWeight.update({'Hematite': Hematite * self.DataBase['Hematite'][0]})\n        DataWeight.update({'Q Weight': Quartz * self.DataBase['Quartz'][0]})\n        DataWeight.update({'A Weight': Orthoclase * self.DataBase['Orthoclase'][0]})\n        DataWeight.update({'P Weight': Anorthite * self.DataBase['Anorthite'][0] + Albite * self.DataBase['Albite'][0]})\n        DataWeight.update({'F Weight': Nepheline * self.DataBase['Nepheline'][0] + Leucite * self.DataBase['Leucite'][0] + Kalsilite * self.DataBase['Kalsilite'][0]})\n\n\n        WholeVolume = 0\n        WholeMole = 0\n        tmpVolume = []\n\n        tmpVolume.append(Quartz * self.DataBase['Quartz'][0] / self.DataBase['Quartz'][1])\n        tmpVolume.append(Zircon * self.DataBase['Zircon'][0] / self.DataBase['Zircon'][1])\n        tmpVolume.append(K2SiO3 * self.DataBase['K2SiO3'][0] / self.DataBase['K2SiO3'][1])\n        tmpVolume.append(Anorthite * self.DataBase['Anorthite'][0] / self.DataBase['Anorthite'][1])\n        tmpVolume.append(Na2SiO3 * self.DataBase['Na2SiO3'][0] / self.DataBase['Na2SiO3'][1])\n        tmpVolume.append(Acmite * self.DataBase['Acmite'][0] / self.DataBase['Acmite'][1])\n        tmpVolume.append(Diopside * self.DataBase['Diopside'][0] / self.DataBase['Diopside'][1])\n        tmpVolume.append(Sphene * self.DataBase['Sphene'][0] / self.DataBase['Sphene'][1])\n        tmpVolume.append(Hypersthene * self.DataBase['Hypersthene'][0] / self.DataBase['Hypersthene'][1])\n        tmpVolume.append(Albite * self.DataBase['Albite'][0] / self.DataBase['Albite'][1])\n        tmpVolume.append(Orthoclase * self.DataBase['Orthoclase'][0] / self.DataBase['Orthoclase'][1])\n        tmpVolume.append(Wollastonite * self.DataBase['Wollastonite'][0] / self.DataBase['Wollastonite'][1])\n        tmpVolume.append(Olivine * self.DataBase['Olivine'][0] / self.DataBase['Olivine'][1])\n        tmpVolume.append(Perovskite * self.DataBase['Perovskite'][0] / self.DataBase['Perovskite'][1])\n        tmpVolume.append(Nepheline * self.DataBase['Nepheline'][0] / self.DataBase['Nepheline'][1])\n        tmpVolume.append(Leucite * self.DataBase['Leucite'][0] / self.DataBase['Leucite'][1])\n        tmpVolume.append(Larnite * self.DataBase['Larnite'][0] / self.DataBase['Larnite'][1])\n        tmpVolume.append(Kalsilite * self.DataBase['Kalsilite'][0] / self.DataBase['Kalsilite'][1])\n        tmpVolume.append(Apatite * self.DataBase['Apatite'][0] / self.DataBase['Apatite'][1])\n        tmpVolume.append(Halite * self.DataBase['Halite'][0] / self.DataBase['Halite'][1])\n        tmpVolume.append(Fluorite * self.DataBase['Fluorite'][0] / self.DataBase['Fluorite'][1])\n        tmpVolume.append(Anhydrite * self.DataBase['Anhydrite'][0] / self.DataBase['Anhydrite'][1])\n        tmpVolume.append(Thenardite * self.DataBase['Thenardite'][0] / self.DataBase['Thenardite'][1])\n        tmpVolume.append(Pyrite * self.DataBase['Pyrite'][0] / self.DataBase['Pyrite'][1])\n        tmpVolume.append(Magnesiochromite * self.DataBase['Magnesiochromite'][0] / self.DataBase['Magnesiochromite'][1])\n        tmpVolume.append(Chromite * self.DataBase['Chromite'][0] / self.DataBase['Chromite'][1])\n        tmpVolume.append(Ilmenite * self.DataBase['Ilmenite'][0] / self.DataBase['Ilmenite'][1])\n        tmpVolume.append(Calcite * self.DataBase['Calcite'][0] / self.DataBase['Calcite'][1])\n        tmpVolume.append(Na2CO3 * self.DataBase['Na2CO3'][0] / self.DataBase['Na2CO3'][1])\n        tmpVolume.append(Corundum * self.DataBase['Corundum'][0] / self.DataBase['Corundum'][1])\n        tmpVolume.append(Rutile * self.DataBase['Rutile'][0] / self.DataBase['Rutile'][1])\n        tmpVolume.append(Magnetite * self.DataBase['Magnetite'][0] / self.DataBase['Magnetite'][1])\n        tmpVolume.append(Hematite * self.DataBase['Hematite'][0] / self.DataBase['Hematite'][1])\n\n        WholeVolume = sum(tmpVolume)\n\n        DataVolume.update(\n            {'Quartz': (Quartz * self.DataBase['Quartz'][0] / self.DataBase['Quartz'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Zircon': (Zircon * self.DataBase['Zircon'][0] / self.DataBase['Zircon'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'K2SiO3': (K2SiO3 * self.DataBase['K2SiO3'][0] / self.DataBase['K2SiO3'][1]) / WholeVolume * 100})\n        DataVolume.update({'Anorthite': (Anorthite * self.DataBase['Anorthite'][0] / self.DataBase['Anorthite'][\n            1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Na2SiO3': (Na2SiO3 * self.DataBase['Na2SiO3'][0] / self.DataBase['Na2SiO3'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Acmite': (Acmite * self.DataBase['Acmite'][0] / self.DataBase['Acmite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Diopside': (Diopside * self.DataBase['Diopside'][0] / self.DataBase['Diopside'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Sphene': (Sphene * self.DataBase['Sphene'][0] / self.DataBase['Sphene'][1]) / WholeVolume * 100})\n        DataVolume.update({'Hypersthene': (Hypersthene * self.DataBase['Hypersthene'][0] / self.DataBase['Hypersthene'][\n            1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Albite': (Albite * self.DataBase['Albite'][0] / self.DataBase['Albite'][1]) / WholeVolume * 100})\n        DataVolume.update({'Orthoclase': (Orthoclase * self.DataBase['Orthoclase'][0] / self.DataBase['Orthoclase'][\n            1]) / WholeVolume * 100})\n        DataVolume.update({'Wollastonite': (Wollastonite * self.DataBase['Wollastonite'][0] /\n                                            self.DataBase['Wollastonite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Olivine': (Olivine * self.DataBase['Olivine'][0] / self.DataBase['Olivine'][1]) / WholeVolume * 100})\n        DataVolume.update({'Perovskite': (Perovskite * self.DataBase['Perovskite'][0] / self.DataBase['Perovskite'][\n            1]) / WholeVolume * 100})\n        DataVolume.update({'Nepheline': (Nepheline * self.DataBase['Nepheline'][0] / self.DataBase['Nepheline'][\n            1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Leucite': (Leucite * self.DataBase['Leucite'][0] / self.DataBase['Leucite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Larnite': (Larnite * self.DataBase['Larnite'][0] / self.DataBase['Larnite'][1]) / WholeVolume * 100})\n        DataVolume.update({'Kalsilite': (Kalsilite * self.DataBase['Kalsilite'][0] / self.DataBase['Kalsilite'][\n            1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Apatite': (Apatite * self.DataBase['Apatite'][0] / self.DataBase['Apatite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Halite': (Halite * self.DataBase['Halite'][0] / self.DataBase['Halite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Fluorite': (Fluorite * self.DataBase['Fluorite'][0] / self.DataBase['Fluorite'][1]) / WholeVolume * 100})\n        DataVolume.update({'Anhydrite': (Anhydrite * self.DataBase['Anhydrite'][0] / self.DataBase['Anhydrite'][\n            1]) / WholeVolume * 100})\n        DataVolume.update({'Thenardite': (Thenardite * self.DataBase['Thenardite'][0] / self.DataBase['Thenardite'][\n            1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Pyrite': (Pyrite * self.DataBase['Pyrite'][0] / self.DataBase['Pyrite'][1]) / WholeVolume * 100})\n        DataVolume.update({'Magnesiochromite': (Magnesiochromite * self.DataBase['Magnesiochromite'][0] /\n                                                self.DataBase['Magnesiochromite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Chromite': (Chromite * self.DataBase['Chromite'][0] / self.DataBase['Chromite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Ilmenite': (Ilmenite * self.DataBase['Ilmenite'][0] / self.DataBase['Ilmenite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Calcite': (Calcite * self.DataBase['Calcite'][0] / self.DataBase['Calcite'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Na2CO3': (Na2CO3 * self.DataBase['Na2CO3'][0] / self.DataBase['Na2CO3'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Corundum': (Corundum * self.DataBase['Corundum'][0] / self.DataBase['Corundum'][1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Rutile': (Rutile * self.DataBase['Rutile'][0] / self.DataBase['Rutile'][1]) / WholeVolume * 100})\n        DataVolume.update({'Magnetite': (Magnetite * self.DataBase['Magnetite'][0] / self.DataBase['Magnetite'][\n            1]) / WholeVolume * 100})\n        DataVolume.update(\n            {'Hematite': (Hematite * self.DataBase['Hematite'][0] / self.DataBase['Hematite'][1]) / WholeVolume * 100})\n\n        DataVolume.update({'Q': DataVolume['Quartz']})\n        DataVolume.update({'A': DataVolume['Orthoclase']})\n\n        DataVolume.update({'P': DataVolume['Anorthite'] + DataVolume['Albite']})\n        DataVolume.update({'F': DataVolume['Nepheline'] + DataVolume['Leucite'] + DataVolume['Kalsilite']})\n\n        DI = 0\n        # for i in ['Quartz', 'Anorthite', 'Albite', 'Orthoclase', 'Nepheline', 'Leucite', 'Kalsilite']:\n        # exec('DI+=' + i + '*self.DataBase[\\'' + i + '\\'][0]')\n\n        DI = Quartz + Anorthite + Albite + Orthoclase + Nepheline + Leucite + Kalsilite\n\n\n        DiWeight=0\n        DiVolume=0\n\n        DiWeight = DataWeight['Quartz']+DataWeight['Anorthite']+DataWeight['Albite']+DataWeight['Orthoclase']+DataWeight['Nepheline']+DataWeight['Leucite']+DataWeight['Kalsilite']\n        DiVolume = DataVolume['Quartz']+DataVolume['Anorthite']+DataVolume['Albite']+DataVolume['Orthoclase']+DataVolume['Nepheline']+DataVolume['Leucite']+DataVolume['Kalsilite']\n\n        # print('\\n\\n DI is\\n',DI,'\\n\\n')\n        DataCalced.update({'Differentiation Index Weight': DiWeight})\n\n        DataCalced.update({'Differentiation Index Volume': DiVolume})\n\n        return (DataResult, DataWeight, DataVolume, DataCalced)", "response": "Single Calc of a single taxonomy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retranslateUi(self):\n        _translate = QtCore.QCoreApplication.translate\n        self.talk=  _translate('MainWindow','You are using GeoPyTool ') + version +'\\n'+ _translate('MainWindow','released on ') + date + '\\n'\n\n        self.menuFile.setTitle(_translate('MainWindow', u'Data File'))\n        self.menuGeoChem.setTitle(_translate('MainWindow', u'Geochemistry'))\n        self.menuGeoCalc.setTitle(_translate('MainWindow',u'Calculation'))\n        self.menuStructure.setTitle(_translate('MainWindow', u'Structure'))\n        self.menuSedimentary.setTitle(_translate('MainWindow', u'Sedimentary'))\n        self.menuAdditional.setTitle(_translate('MainWindow', u'Additional Functions'))\n        self.menuHelp.setTitle(_translate('MainWindow', u'Help'))\n        self.menuLanguage.setTitle(_translate('MainWindow', u'Language'))\n\n\n        self.actionCombine.setText(_translate('MainWindow', u'Combine'))\n        self.actionCombine_transverse.setText(_translate('MainWindow', u'Combine_transverse'))\n\n        self.actionFlatten.setText(_translate('MainWindow',u'Flatten'))\n\n        self.actionTrans.setText(_translate('MainWindow',u'Trans'))\n\n        self.actionReFormat.setText(_translate('MainWindow',u'ReFormat'))\n\n        self.actionOpen.setText(_translate('MainWindow', u'Open Data'))\n        self.actionClose.setText(_translate('MainWindow', u'Close Data'))\n        self.actionSet.setText(_translate('MainWindow', u'Set Format'))\n        self.actionSave.setText(_translate('MainWindow', u'Save Data'))\n        self.actionQuit.setText(_translate('MainWindow', u'Quit App'))\n\n        self.actionRemoveLOI.setText('1-0 '+_translate('MainWindow',u'Remove LOI'))\n        self.actionAuto.setText('1-1 '+_translate('MainWindow', u'Auto'))\n        self.actionTAS.setText('1-2 '+ _translate('MainWindow',u'TAS'))\n        self.actionTrace.setText('1-3 '+_translate('MainWindow',u'Trace'))\n        self.actionRee.setText('1-4 '+_translate('MainWindow',u'REE'))\n        self.actionPearce.setText('1-5 '+_translate('MainWindow',u'Pearce'))\n        self.actionHarker.setText('1-6 '+_translate('MainWindow',u'Harker'))\n        self.actionCIPW.setText('1-7 '+_translate('MainWindow',u'CIPW'))\n        self.actionQAPF.setText('1-8 '+_translate('MainWindow',u'QAPF'))\n        self.actionSaccani.setText('1-9 '+_translate('MainWindow',u'Saccani Plot'))\n        self.actionK2OSiO2.setText('1-10 '+_translate('MainWindow',u'K2O-SiO2'))\n        self.actionRaman.setText('1-11 '+_translate('MainWindow',u'Raman Strength'))\n        self.actionFluidInclusion.setText('1-12 '+_translate('MainWindow',u'Fluid Inclusion'))\n        self.actionHarkerOld.setText('1-14 '+_translate('MainWindow',u'Harker Classical'))\n        self.actionTraceNew.setText('1-15 '+_translate('MainWindow',u'TraceNew'))\n\n        self.actionStereo.setText('2-1 '+_translate('MainWindow',u'Stereo'))\n        self.actionRose.setText('2-2 '+_translate('MainWindow',u'Rose'))\n\n        self.actionQFL.setText('3-1 '+_translate('MainWindow',u'QFL'))\n        self.actionQmFLt.setText('3-2 '+_translate('MainWindow',u'QmFLt'))\n        self.actionClastic.setText('3-3 '+_translate('MainWindow',u'Clastic'))\n        self.actionCIA.setText('3-4 '+ _translate('MainWindow',u'CIA and ICV'))\n\n        self.actionZirconCe.setText('4-1 '+ _translate('MainWindow',u'ZirconCe'))\n        self.actionZirconCeOld.setText('4-2 '+ _translate('MainWindow', u'ZirconCeOld'))\n        self.actionZirconTiTemp.setText('4-3 '+ _translate('MainWindow',u'ZirconTiTemp'))\n        self.actionRutileZrTemp.setText('4-4 '+_translate('MainWindow',u'RutileZrTemp'))\n        self.actionRbSrIsoTope.setText('4-5 '+_translate('MainWindow',u'Rb-Sr IsoTope'))\n        self.actionSmNdIsoTope.setText('4-6 '+_translate('MainWindow',u'Sm-Nd IsoTope'))\n        #self.actionKArIsoTope.setText(_translate('MainWindow',u'K-Ar IsoTope'))\n\n        self.actionXY.setText('5-1 '+_translate('MainWindow',u'X-Y plot'))\n        self.actionXYZ.setText('5-2 '+_translate('MainWindow',u'X-Y-Z plot'))\n        self.actionCluster.setText('5-3 '+_translate('MainWindow',u'Cluster'))\n        self.actionMultiDimension.setText('5-4 '+_translate('MainWindow',u'MultiDimension'))\n\n\n        self.actionFA.setText('5-5 '+_translate('MainWindow',u'FA'))\n\n        self.actionPCA.setText('5-6 '+_translate('MainWindow',u'PCA'))\n\n        self.actionDist.setText('5-7 '+_translate('MainWindow',u'Distance'))\n\n        self.actionStatistics.setText('5-8 '+_translate('MainWindow',u'Statistics'))\n\n\n        self.actionThreeD.setText('5-9 '+_translate('MainWindow',u'ThreeD'))\n        self.actionTwoD.setText('5-10 '+_translate('MainWindow',u'TwoD'))\n        self.actionTwoD_Grey.setText('5-11 '+_translate('MainWindow',u'TwoD Grey'))\n\n        self.actionMyHist.setText('5-12 '+_translate('MainWindow',u'Histogram + KDE Curve'))\n\n        self.actionVersionCheck.setText(_translate('MainWindow', u'Check Update'))\n        self.actionWeb.setText(_translate('MainWindow', u'English Forum'))\n        self.actionGoGithub.setText(_translate('MainWindow', u'Github'))\n\n        '''\n        self.actionCnS.setText(_translate('MainWindow',u'Simplified Chinese'))\n        self.actionCnT.setText(_translate('MainWindow', u'Traditional Chinese'))\n        self.actionEn.setText(_translate('MainWindow',u'English'))\n        '''\n\n        self.actionCnS.setText(u'\u7b80\u4f53\u4e2d\u6587')\n        self.actionCnT.setText(u'\u7e41\u9ad4\u4e2d\u6587')\n        self.actionEn.setText(u'English')\n        self.actionLoadLanguage.setText(_translate('MainWindow',u'Load Language'))", "response": "Re - translate the UI to the equivalent Qt widgets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing the rose map of single sample with different items ~", "response": "def singlerose(self, Width=1, Color=['red']):\n        '''\n        draw the rose map of single sample with different items~\n        '''\n        self.chooser_label.setText(self.ChooseItems[self.chooser.value() - 1])\n\n        self.MultipleRoseName = self.ChooseItems[self.chooser.value() - 1]\n\n        self.SingleRoseName = [(self.ChooseItems[self.chooser.value() - 1])]\n\n        Name = self.SingleRoseName\n\n        self.axes.clear()\n        # self.axes.set_xlim(-90, 450)\n        # self.axes.set_ylim(0, 90)\n\n        titles = list('NWSE')\n\n        titles = ['N', '330', '300', 'W', '240', '210', 'S', '150', '120', 'E', '60', '30']\n        self.n = len(titles)\n        self.angles = np.arange(90, 90 + 360, 360.0 / self.n)\n\n        self.angles = np.array([90., 120., 150., 180., 210., 240., 270., 300., 330.,\n                                360., 30., 60.])\n        self.axes.set_thetagrids(self.angles, labels=titles, fontsize=14)\n\n        self.raw = self._df\n\n        real_max = []\n\n        for k in range(len(Name)):\n\n            Data = []\n            S = []\n            R = []\n            for i in range(len(self.raw)):\n                S.append(self.raw.at[i, Name[k]])\n\n            s = np.linspace(0, 360, 360 / self.Gap + 1)\n            t = tuple(s.tolist())\n            count = []\n\n            for i in range(len(t)):\n                tmp_count = 0\n                for j in S:\n                    if i < len(t) - 1:\n                        if t[i] < j <= t[i + 1]:\n                            tmp_count += 1\n                count.append(tmp_count)\n\n            count_max = max(count)\n            real_max.append(count_max)\n\n        maxuse = max(real_max)\n\n        for k in range(len(Name)):\n            Data = []\n            S = []\n            R = []\n            for i in range(len(self.raw)):\n                S.append(self.raw.at[i, Name[k]])\n\n            s = np.linspace(0, 360, 360 / self.Gap + 1)\n            t = tuple(s.tolist())\n            count = []\n\n            for i in range(len(t)):\n                tmp_count = 0\n                for j in S:\n                    if i < len(t) - 1:\n                        if t[i] < j <= t[i + 1]:\n                            tmp_count += 1\n                count.append(tmp_count)\n            s = np.linspace(0, 360, 360 / self.Gap + 1)\n            t = tuple(s.tolist())\n\n            R_factor = 90 / maxuse\n\n            for i in count:\n                TMP = 90 - i * R_factor\n                R.append(TMP)\n\n            m, n = self.Trans(t, R)\n            self.axes.plot(m, n, color=Color[k], linewidth=1, alpha=0.6, marker='')\n            self.axes.fill(m, n, Color=Color[k], Alpha=0.6, )\n\n        if (self.Type_cb.isChecked()):\n            self.Type_cb.setText('Wulf')\n            list1 = [self.eqan(x) for x in range(15, 90, 15)]\n        else:\n            self.Type_cb.setText('Schmidt')\n            list1 = [self.eqar(x) for x in range(15, 90, 15)]\n\n\n        list2= list1\n\n\n        print(maxuse + 1)\n\n        try:\n            list2 = [str(x) for x in range(0, int(maxuse + 1), int((maxuse + 1.0) / 7.0))]\n        except(ValueError):\n            pass\n        list2.reverse()\n        self.axes.set_rgrids(list1, list2)\n\n        #self.axes.set_thetagrids(range(360 + 90, 0 + 90, -15), [str(x) for x in range(0, 360, 15)])\n\n\n\n        if (self.legend_cb.isChecked()):\n            self.axes.legend(bbox_to_anchor=(1.5, 1), loc=2, borderaxespad=0, prop=fontprop)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw the rose map of multiple samples ~", "response": "def multirose(self, Width=1, Name='Dip'):\n        '''\n        draw the rose map of multiple samples~\n        '''\n\n        Name = self.MultipleRoseName\n\n        self.axes.clear()\n        # self.axes.set_xlim(-90, 450)\n        # self.axes.set_ylim(0, 90)\n\n        titles = list('NWSE')\n\n        titles = ['N', '330', '300', 'W', '240', '210', 'S', '150', '120', 'E', '60', '30']\n        self.n = len(titles)\n        self.angles = np.arange(90, 90 + 360, 360.0 / self.n)\n\n        self.angles = np.array([90., 120., 150., 180., 210., 240., 270., 300., 330.,\n                                360., 30., 60.])\n        self.axes.set_thetagrids(self.angles, labels=titles, fontsize=14)\n\n        self.raw = self._df\n\n        real_max = []\n\n        S = []\n        R = []\n        Color = []\n        Label = []\n        Whole = {}\n\n        for i in range(len(self.raw)):\n            S.append(self.raw.at[i, Name])\n\n            if self.raw.at[i, 'Color'] not in Color and self.raw.at[i, 'Color'] != '':\n                Color.append(self.raw.at[i, 'Color'])\n            if self.raw.at[i, 'Label'] not in Label and self.raw.at[i, 'Label'] != '':\n                Label.append(self.raw.at[i, 'Label'])\n\n        Whole = ({k: [] for k in Label})\n\n        WholeCount = ({k: [] for k in Label})\n\n        for i in range(len(self.raw)):\n            for k in Label:\n                if self.raw.at[i, 'Label'] == k:\n                    Whole[k].append(self.raw.at[i, Name])\n\n        t = tuple(np.linspace(0, 360, 360 / self.Gap + 1).tolist())\n        real_max = 0\n\n        for j in range(len(Label)):\n\n            for i in range(len(t)):\n                tmp_count = 0\n                for u in Whole[Label[j]]:\n                    if i < len(t) - 1:\n                        if t[i] < u <= t[i + 1]:\n                            tmp_count += 1\n                real_max = max(real_max, tmp_count)\n                WholeCount[Label[j]].append(tmp_count)\n\n        maxuse = real_max\n        R_factor = 90 / maxuse\n\n        for j in range(len(Label)):\n\n            R = []\n            for i in WholeCount[Label[j]]:\n                TMP = 90 - i * R_factor\n                R.append(TMP)\n\n            m, n = self.Trans(t, R)\n            self.axes.plot(m, n, color=Color[j], linewidth=1, alpha=0.6, marker='', label=Label[j])\n            self.axes.fill(m, n, Color=Color[j], Alpha=0.6)\n\n        if (self.Type_cb.isChecked()):\n            self.Type_cb.setText('Wulf')\n            list1 = [self.eqan(x) for x in range(15, 90, 15)]\n        else:\n            self.Type_cb.setText('Schmidt')\n            list1 = [self.eqar(x) for x in range(15, 90, 15)]\n\n        list2= list1\n\n        try:\n            list2 = [str(x) for x in range(0, int(maxuse + 1), int((maxuse + 1) / 7))]\n        except(ValueError):\n            pass\n\n\n        list2.reverse()\n\n        self.axes.set_rgrids(list1, list2)\n\n        #self.axes.set_thetagrids(range(360 + 90, 0 + 90, -15), [str(x) for x in range(0, 360, 15)])\n\n        if (self.legend_cb.isChecked()):\n            self.axes.legend(bbox_to_anchor=(1.5, 1), loc=2, borderaxespad=0, prop=fontprop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\njoins the names of a multi - level index with an underscore.", "response": "def _join_names(names):\n    \"\"\"Join the names of a multi-level index with an underscore.\"\"\"\n\n    levels = (str(name) for name in names if name != '')\n    return '_'.join(levels)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next_power_of_2(x):\r\n\r\n    power_of_2 = 1 if x == 0 else 2 ** np.ceil(np.log2(x))\r\n    return power_of_2", "response": "Finds the next power of 2 value\r\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef window_design(self, window_length, beta):\r\n\r\n        self.window = np.kaiser(window_length, beta)\r\n\r\n        return self.window", "response": "Design the Kaiser window"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(self, X):\r\n\r\n        training_signal = X\r\n\r\n        self.window_design(self.window_length, self.beta)\r\n\r\n        if self.method == 'std_dev':\r\n            self.fit_freq_std_dev(training_signal)\r\n        elif self.method == 'min_max':\r\n            self.fit_freq_min_max(training_signal)\r\n        else:\r\n            raise ValueError('Unknown method: {}'.format(self.method))", "response": "Defines a spectral mask based on training data X"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine a spectral mask based on the frequency component of each .", "response": "def fit_freq_min_max(self, training_signal):\r\n        \"\"\"Defines a spectral mask based on training data using min and max values of each\r\n             frequency component\r\n\r\n        Args:\r\n            training_signal: Training data\r\n\r\n        \"\"\"\r\n\r\n        window_length = len(self.window)\r\n        window_weight = sum(self.window)\r\n        max_mask = np.zeros(int(window_length / 2) + 1)\r\n        min_mask = np.zeros(int(window_length / 2) + 1)\r\n\r\n        for i in range(0, len(training_signal) - window_length - 1):\r\n            rfft = np.fft.rfft(training_signal[i:i + window_length] * self.window)\r\n            temp = np.abs(rfft) / window_weight\r\n            max_mask = np.maximum(max_mask, temp)\r\n            min_mask = np.minimum(min_mask, temp)\r\n\r\n        self.mask_top = self.gain * max_mask\r\n        self.mask_bottom = min_mask / self.gain"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit_freq_std_dev(self, training_signal):\r\n\r\n        window_length = len(self.window)\r\n        window_weight = sum(self.window)\r\n        num_of_windows = len(training_signal) - window_length - 1\r\n        mean = np.zeros(int(window_length / 2) + 1)\r\n        pow = np.zeros(int(window_length / 2) + 1)\r\n        temp = np.zeros(int(window_length / 2) + 1)\r\n        rfft = np.fft.rfft(training_signal[0:0 + window_length] * self.window)\r\n        max = np.abs(rfft) / window_weight\r\n        min = max\r\n\r\n        for i in range(0, num_of_windows):\r\n            rfft = np.fft.rfft(training_signal[i:i + window_length] * self.window)\r\n            temp = np.abs(rfft) / window_weight\r\n            max = np.maximum(temp, max)\r\n            min = np.minimum(temp, min)\r\n            mean = mean + temp\r\n            pow = pow + np.power(temp, 2)\r\n\r\n        mean = mean / num_of_windows\r\n        pow = pow / num_of_windows\r\n        std_dev = np.sqrt(pow - np.power(mean, 2))\r\n        self.mask_top = mean + self.gain * std_dev\r\n        self.mask_bottom = np.maximum(mean - self.gain * std_dev,\r\n                                      np.zeros(int(window_length / 2) + 1))", "response": "Defines a spectral mask based on the standard deviation values of the training data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef produce(self, X):\r\n\r\n        signal = X\r\n\r\n        window_length = len(self.window)\r\n        anomalies = np.zeros(len(signal))\r\n        window_weight = sum(self.window)\r\n        for i in range(0, len(signal) - window_length - 1):\r\n            rfft = np.fft.rfft(signal[i:i + window_length] * self.window)\r\n            sig_freq = np.abs(rfft) / window_weight\r\n            anomalies[i] = 0\r\n            for m in range(0, int(window_length / 2) - 1):\r\n                if ((sig_freq[m] > self.mask_top[m]) or (sig_freq[m] < self.mask_bottom[m])):\r\n                    anomalies[i] = 1\r\n                    break\r\n\r\n        return anomalies", "response": "Returns the anomalies detected in the telemetry data based on its power spectral density."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef energy(data):\n    data = np.mean(data, axis=1)\n    return np.sum(data ** 2) / np.float64(len(data))", "response": "Computes signal energy of data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the spectral entropy of a single block of data.", "response": "def spectral_entropy(data, numOfShortBlocks=10):\n    \"\"\"Computes the spectral entropy\"\"\"\n    data = np.mean(data, axis=1)\n\n    nFFT = len(data) // 2\n    X = FFT(data, nFFT)\n    L = len(X)                         # number of frame data\n    Eol = np.sum(X ** 2)            # total spectral energy\n\n    subWinLength = int(np.floor(L / numOfShortBlocks))   # length of sub-frame\n    if L != subWinLength * numOfShortBlocks:\n        X = X[0:subWinLength * numOfShortBlocks]\n\n    # define sub-frames (using matrix reshape)\n    subWindows = X.reshape(subWinLength, numOfShortBlocks, order='F').copy()\n\n    # compute spectral sub-energies\n    s = np.sum(subWindows ** 2, axis=0) / (Eol + EPSILON)\n\n    # compute spectral entropy\n    return -np.sum(s * np.log2(s + EPSILON))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zcr(data):\n    data = np.mean(data, axis=1)\n\n    count = len(data)\n    countZ = np.sum(np.abs(np.diff(np.sign(data)))) / 2\n    return (np.float64(countZ) / np.float64(count - 1.0))", "response": "Computes zero crossing rate of segment"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spectral_flux(d0, d1):\n    # compute the spectral flux as the sum of square distances:\n    d0 = np.mean(d0, axis=1)\n    d1 = np.mean(d1, axis=1)\n    nFFT = min(len(d0) // 2, len(d1) // 2)\n    X = FFT(d0, nFFT)\n    Xprev = FFT(d1, nFFT)\n\n    # L = min(len(X), len(Xprev))\n\n    sumX = np.sum(X + EPSILON)\n    sumPrevX = np.sum(Xprev + EPSILON)\n    return np.sum((X / sumX - Xprev / sumPrevX) ** 2)", "response": "Computes the spectral flux feature of the current frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the entropy of the energy of a single file.", "response": "def energy_entropy(data, fs, numOfShortBlocks=10):\n    \"\"\"Computes entropy of energy\"\"\"\n    data = np.mean(data, axis=1)\n\n    Eol = np.sum(data ** 2)    # total data energy\n    L = len(data)\n    subWinLength = int(np.floor(L / numOfShortBlocks))\n    if L != subWinLength * numOfShortBlocks:\n        data = data[0:subWinLength * numOfShortBlocks]\n\n    # subWindows is of size [numOfShortBlocks x L]\n    subWindows = data.reshape(subWinLength, numOfShortBlocks, order='F').copy()\n\n    # Compute normalized sub-data energies:\n    s = np.sum(subWindows ** 2, axis=0) / (Eol + EPSILON)\n\n    # Compute entropy of the normalized sub-data energies:\n    Entropy = -np.sum(s * np.log2(s + EPSILON))\n    return Entropy"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spectral_centroid_and_spread(data, fs):\n    data = np.mean(data, axis=1)\n\n    nFFT = len(data) // 2\n    X = FFT(data, nFFT)\n\n    ind = (np.arange(1, len(X) + 1)) * (fs / (2.0 * len(X)))\n\n    Xt = X.copy()\n    Xt = Xt / Xt.max()\n    NUM = np.sum(ind * Xt)\n    DEN = np.sum(Xt) + EPSILON\n\n    # Centroid:\n    C = (NUM / DEN)\n\n    # Spread:\n    S = np.sqrt(np.sum(((ind - C) ** 2) * Xt) / DEN)\n\n    # Normalize:\n    C = C / (fs / 2.0)\n    S = S / (fs / 2.0)\n\n    return (C, S)", "response": "Computes the spectral centroid of frame ( given abs ( FFT )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the spectral roll - off of the current resource.", "response": "def spectral_rolloff(data, coeff):\n    \"\"\"Computes spectral roll-off\"\"\"\n    data = np.mean(data, axis=1)\n    nFFT = len(data) // 2\n    X = FFT(data, nFFT)\n\n    totalEnergy = np.sum(X ** 2)\n    fftLength = len(X)\n    Thres = coeff * totalEnergy\n\n    # Find the spectral rolloff as the frequency position where the\n    # respective spectral energy is equal to c*totalEnergy\n    CumSum = np.cumsum(X ** 2) + EPSILON\n    [a, ] = np.nonzero(CumSum > Thres)\n    if len(a) > 0:\n        mC = np.float64(a[0]) / (float(fftLength))\n    else:\n        mC = 0.0\n\n    return mC"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rolling_window_sequences(X, index, window_size, target_size, target_column):\n    out_X = list()\n    out_y = list()\n    X_index = list()\n    y_index = list()\n\n    target = X[:, target_column]\n\n    for start in range(len(X) - window_size - target_size + 1):\n        end = start + window_size\n        out_X.append(X[start:end])\n        out_y.append(target[end:end + target_size])\n        X_index.append(index[start])\n        y_index.append(index[end])\n\n    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)", "response": "Create rolling window sequences out of timeseries data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing average of values over fixed length time segments.", "response": "def time_segments_average(X, interval, time_column):\n    \"\"\"Compute average of values over fixed length time segments.\"\"\"\n    warnings.warn(_TIME_SEGMENTS_AVERAGE_DEPRECATION_WARNING, DeprecationWarning)\n\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    X = X.sort_values(time_column).set_index(time_column)\n\n    start_ts = X.index.values[0]\n    max_ts = X.index.values[-1]\n\n    values = list()\n    index = list()\n    while start_ts <= max_ts:\n        end_ts = start_ts + interval\n        subset = X.loc[start_ts:end_ts - 1]\n        means = subset.mean(skipna=True).values\n        values.append(means)\n        index.append(start_ts)\n        start_ts = end_ts\n\n    return np.asarray(values), np.asarray(index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef time_segments_aggregate(X, interval, time_column, method=['mean']):\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    X = X.sort_values(time_column).set_index(time_column)\n\n    if isinstance(method, str):\n        method = [method]\n\n    start_ts = X.index.values[0]\n    max_ts = X.index.values[-1]\n\n    values = list()\n    index = list()\n    while start_ts <= max_ts:\n        end_ts = start_ts + interval\n        subset = X.loc[start_ts:end_ts - 1]\n        aggregated = [\n            getattr(subset, agg)(skipna=True).values\n            for agg in method\n        ]\n        values.append(np.concatenate(aggregated))\n        index.append(start_ts)\n        start_ts = end_ts\n\n    return np.asarray(values), np.asarray(index)", "response": "Aggregate values over fixed length time segments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef image_transform(X, function, reshape_before=False, reshape_after=False,\n                    width=None, height=None, **kwargs):\n    \"\"\"Apply a function image by image.\n\n    Args:\n        reshape_before: whether 1d array needs to be reshaped to a 2d image\n        reshape_after: whether the returned values need to be reshaped back to a 1d array\n        width: image width used to rebuild the 2d images. Required if the image is not square.\n        height: image height used to rebuild the 2d images. Required if the image is not square.\n    \"\"\"\n\n    if not callable(function):\n        function = import_object(function)\n\n    elif not callable(function):\n        raise ValueError(\"function must be a str or a callable\")\n\n    flat_image = len(X[0].shape) == 1\n\n    if reshape_before and flat_image:\n        if not (width and height):\n            side_length = math.sqrt(X.shape[1])\n            if side_length.is_integer():\n                side_length = int(side_length)\n                width = side_length\n                height = side_length\n\n            else:\n                raise ValueError(\"Image sizes must be given for non-square images\")\n    else:\n        reshape_before = False\n\n    new_X = []\n    for image in X:\n        if reshape_before:\n            image = image.reshape((width, height))\n\n        features = function(\n            image,\n            **kwargs\n        )\n\n        if reshape_after:\n            features = np.reshape(features, X.shape[1])\n\n        new_X.append(features)\n\n    return np.array(new_X)", "response": "Apply a function image by image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute an array of absolute errors comparing predictions and expected output.", "response": "def regression_errors(y, y_hat, smoothing_window=0.01, smooth=True):\n    \"\"\"Compute an array of absolute errors comparing predictions and expected output.\n\n    If smooth is True, apply EWMA to the resulting array of errors.\n\n    Args:\n        y (array): Ground truth.\n        y_hat (array): Predictions array.\n        smoothing_window (float): Size of the smoothing window, expressed as a proportion\n            of the total length of y.\n        smooth (bool): whether the returned errors should be smoothed with EWMA.\n\n    Returns:\n        (array): errors\n    \"\"\"\n    errors = np.abs(y - y_hat)[:, 0]\n\n    if not smooth:\n        return errors\n\n    smoothing_window = int(smoothing_window * len(y))\n\n    return pd.Series(errors).ewm(span=smoothing_window).mean().values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deltas(errors, epsilon, mean, std):\n    below = errors[errors <= epsilon]\n    if not len(below):\n        return 0, 0\n\n    return mean - below.mean(), std - below.std()", "response": "Compute mean and std deltas."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef count_above(errors, epsilon):\n    above = errors > epsilon\n    total_above = len(errors[above])\n\n    above = pd.Series(above)\n    shift = above.shift(1)\n    change = above != shift\n\n    total_consecutive = sum(above & change)\n\n    return total_above, total_consecutive", "response": "Count the number of errors and continuous sequences above epsilon."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes how bad a z value is.", "response": "def z_cost(z, errors, mean, std):\n    \"\"\"Compute how bad a z value is.\n\n    The original formula is::\n\n                 (delta_mean/mean) + (delta_std/std)\n        ------------------------------------------------------\n        number of errors above + (number of sequences above)^2\n\n    which computes the \"goodness\" of `z`, meaning that the higher the value\n    the better the `z`.\n\n    In this case, we return this value inverted (we make it negative), to convert\n    it into a cost function, as later on we will use scipy to minimize it.\n    \"\"\"\n\n    epsilon = mean + z * std\n\n    delta_mean, delta_std = deltas(errors, epsilon, mean, std)\n    above, consecutive = count_above(errors, epsilon)\n\n    numerator = -(delta_mean / mean + delta_std / std)\n    denominator = above + consecutive ** 2\n\n    if denominator == 0:\n        return np.inf\n\n    return numerator / denominator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the ideal threshold.", "response": "def find_threshold(errors, z_range=(0, 10)):\n    \"\"\"Find the ideal threshold.\n\n    The ideal threshold is the one that minimizes the z_cost function.\n    \"\"\"\n\n    mean = errors.mean()\n    std = errors.std()\n\n    min_z, max_z = z_range\n    best_z = min_z\n    best_cost = np.inf\n    for z in range(min_z, max_z):\n        best = fmin(z_cost, z, args=(errors, mean, std), full_output=True, disp=False)\n        z, cost = best[0:2]\n        if cost < best_cost:\n            best_z = z[0]\n\n    return mean + best_z * std"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind sequences of values that are above epsilon.", "response": "def find_sequences(errors, epsilon):\n    \"\"\"Find sequences of values that are above epsilon.\n\n    This is done following this steps:\n\n        * create a boolean mask that indicates which value are above epsilon.\n        * shift this mask by one place, filing the empty gap with a False\n        * compare the shifted mask with the original one to see if there are changes.\n        * Consider a sequence start any point which was true and has changed\n        * Consider a sequence end any point which was false and has changed\n    \"\"\"\n    above = pd.Series(errors > epsilon)\n    shift = above.shift(1).fillna(False)\n    change = above != shift\n\n    index = above.index\n    starts = index[above & change].tolist()\n    ends = (index[~above & change] - 1).tolist()\n    if len(ends) == len(starts) - 1:\n        ends.append(len(above) - 1)\n\n    return list(zip(starts, ends))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds anomalies in the sequence that are anomalous.", "response": "def find_anomalies(errors, index, z_range=(0, 10)):\n    \"\"\"Find sequences of values that are anomalous.\n\n    We first find the ideal threshold for the set of errors that we have,\n    and then find the sequences of values that are above this threshold.\n\n    Lastly, we compute a score proportional to the maximum error in the\n    sequence, and finally return the index pairs that correspond to\n    each sequence, along with its score.\n    \"\"\"\n\n    threshold = find_threshold(errors, z_range)\n    sequences = find_sequences(errors, threshold)\n\n    anomalies = list()\n    denominator = errors.mean() + errors.std()\n    for start, stop in sequences:\n        max_error = errors[start:stop + 1].max()\n        score = (max_error - threshold) / denominator\n        anomalies.append([index[start], index[stop], score])\n\n    return np.asarray(anomalies)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply Gaussian blur to the given data.", "response": "def GaussianBlur(X, ksize_width, ksize_height, sigma_x, sigma_y):\n    \"\"\"Apply Gaussian blur to the given data.\n\n    Args:\n        X: data to blur\n        kernel_size: Gaussian kernel size\n        stddev: Gaussian kernel standard deviation (in both X and Y directions)\n    \"\"\"\n    return image_transform(\n        X,\n        cv2.GaussianBlur,\n        ksize=(ksize_width, ksize_height),\n        sigmaX=sigma_x,\n        sigmaY=sigma_y\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning that takes in a pandas dataframe and a window_size then creates a timeseries sequence with the appropriate number of values that overlap with the given value and time column.", "response": "def rolling_window_sequences(X, window_size, target_size, value_column, time_column):\n    \"\"\"\n        Function that takes in a pandas.DataFrame and a window_size then creates\n            output arrays that correspond to a timeseries sequence with window_size overlap.\n            The output arrays can be fed into a timeseries forecasting model.\n            Assumes the input is timeseries sorted.\n        Args:\n            X (pandas.DataFrame): a pandas dataframe which has 'timestamp'\n                and 'value' columns, and is sorted based on timestamp.\n                The timestamp column is in UNIX format (in seconds).\n            window_size (int): number of values that overlap to create the sequence.\n            value_column (string): name of column that has the value field.\n            time_column (string): name of column that has the time field.\n        Returns:\n            (numpy.ndarray): contains the time series sequenced data with each\n                entry having window_size rows.\n            (numpy.ndarray): acts as the label for the forecasting problem with\n                each entry having window_size rows.\n            (numpy.ndarray): the corresponding timestamps series.\n    \"\"\"\n    output_X = []\n    y = []\n    time = []\n    for start in range(len(X) - window_size - target_size):\n        end = start + window_size\n        output_X.append(X.iloc[start:end][value_column].values.reshape([-1, 1]))\n        y.append(X.iloc[end:end + target_size][value_column].values)\n        time.append(X.iloc[end + 1][time_column])\n\n    return np.asarray(output_X), np.asarray(y), np.asarray(time)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef time_segments_average(X, interval, value_column, time_column):\n    start_ts = X[time_column].iloc[0]   # min value\n    end_time = X[time_column].iloc[-1]  # max value in dataframe\n    accepted_points = []\n    while start_ts < end_time:\n        # average the values between start_ts, [start_ts + timedelta (e.g. 6hrs)]\n        upper_ts = start_ts + interval\n        mask = (X[time_column] > start_ts) & (X[time_column] <= upper_ts)\n        average_value = X.loc[mask][value_column].mean(skipna=True)\n\n        accepted_points.append([start_ts, average_value])\n        start_ts = upper_ts  # update the timestamp\n\n    return pd.DataFrame(accepted_points, columns=[time_column, value_column])", "response": "function that aggregates data in a pandas dataframe by averaging over a given interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the forecasting error for two arrays of data.", "response": "def get_forecast_errors(y_hat,\n                        y_true,\n                        window_size=5,\n                        batch_size=30,\n                        smoothing_percent=0.05,\n                        smoothed=True):\n    \"\"\"\n    Calculates the forecasting error for two arrays of data. If smoothed errors desired,\n        runs EWMA.\n    Args:\n        y_hat (list): forecasted values. len(y_hat)==len(y_true).\n        y_true (list): true values. len(y_hat)==len(y_true).\n        window_size (int):\n        batch_size (int):\n        smoothing_percent (float):\n        smoothed (bool): whether the returned errors should be smoothed with EWMA.\n    Returns:\n        (list): error residuals. Smoothed if specified by user.\n    \"\"\"\n    errors = [abs(y_h - y_t) for y_h, y_t in zip(y_hat, y_true)]\n\n    if not smoothed:\n        return errors\n\n    historical_error_window = int(window_size * batch_size * smoothing_percent)\n    moving_avg = []\n    for i in range(len(errors)):\n        left_window = i - historical_error_window\n        right_window = i + historical_error_window + 1\n        if left_window < 0:\n            left_window = 0\n\n        if right_window > len(errors):\n            right_window = len(errors)\n\n        moving_avg.append(np.mean(errors[left_window:right_window]))\n\n    return moving_avg"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts anomalies from the errors.", "response": "def extract_anomalies(y_true, smoothed_errors, window_size, batch_size, error_buffer):\n    \"\"\"\n        Extracts anomalies from the errors.\n        Args:\n            y_true ():\n            smoothed_errors ():\n            window_size (int):\n            batch_size (int):\n            error_buffer (int):\n        Returns:\n    \"\"\"\n    if len(y_true) <= batch_size * window_size:\n        raise ValueError(\"Window size (%s) larger than y_true (len=%s).\"\n                         % (batch_size, len(y_true)))\n\n    num_windows = int((len(y_true) - (batch_size * window_size)) / batch_size)\n\n    anomalies_indices = []\n\n    for i in range(num_windows + 1):\n        prev_index = i * batch_size\n        curr_index = (window_size * batch_size) + (i * batch_size)\n\n        if i == num_windows + 1:\n            curr_index = len(y_true)\n\n        window_smoothed_errors = smoothed_errors[prev_index:curr_index]\n        window_y_true = y_true[prev_index:curr_index]\n\n        epsilon, sd_threshold = compute_threshold(window_smoothed_errors, error_buffer)\n\n        window_anom_indices = get_anomalies(\n            window_smoothed_errors,\n            window_y_true,\n            sd_threshold,\n            i,\n            anomalies_indices,\n            error_buffer\n        )\n\n        # get anomalies from inverse of smoothed errors\n        # This was done in the implementation of NASA paper but\n        # wasn't referenced in the paper\n\n        # we get the inverse by flipping around the mean\n        mu = np.mean(window_smoothed_errors)\n        smoothed_errors_inv = [mu + (mu - e) for e in window_smoothed_errors]\n        epsilon_inv, sd_inv = compute_threshold(smoothed_errors_inv, error_buffer)\n        inv_anom_indices = get_anomalies(\n            smoothed_errors_inv,\n            window_y_true,\n            sd_inv,\n            i,\n            anomalies_indices,\n            len(y_true)\n        )\n\n        anomalies_indices = list(set(anomalies_indices + inv_anom_indices))\n\n        anomalies_indices.extend([i_a + i * batch_size for i_a in window_anom_indices])\n\n    # group anomalies\n    anomalies_indices = sorted(list(set(anomalies_indices)))\n    anomalies_groups = [list(group) for group in mit.consecutive_groups(anomalies_indices)]\n    anomaly_sequences = [(g[0], g[-1]) for g in anomalies_groups if not g[0] == g[-1]]\n\n    # generate \"scores\" for anomalies based on the max distance from epsilon for each sequence\n    anomalies_scores = []\n    for e_seq in anomaly_sequences:\n        denominator = np.mean(smoothed_errors) + np.std(smoothed_errors)\n        score = max([\n            abs(smoothed_errors[x] - epsilon) / denominator\n            for x in range(e_seq[0], e_seq[1])\n        ])\n\n        anomalies_scores.append(score)\n\n    return anomaly_sequences, anomalies_scores"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_threshold(smoothed_errors, error_buffer, sd_limit=12.0):\n    mu = np.mean(smoothed_errors)\n    sigma = np.std(smoothed_errors)\n\n    max_epsilon = 0\n    sd_threshold = sd_limit\n\n    # The treshold is determined dynamically by testing multiple Zs.\n    # z is drawn from an ordered set of positive values representing the\n    # number of standard deviations above mean(smoothed_errors)\n\n    # here we iterate in increments of 0.5 on the range that the NASA paper found to be good\n    for z in np.arange(2.5, sd_limit, 0.5):\n        epsilon = mu + (sigma * z)\n        below_epsilon, below_indices, above_epsilon = [], [], []\n\n        for i in range(len(smoothed_errors)):\n            e = smoothed_errors[i]\n            if e < epsilon:\n                # save to compute delta mean and delta std\n                # these are important for epsilon calculation\n                below_epsilon.append(e)\n                below_indices.append(i)\n\n            if e > epsilon:\n                # above_epsilon values are anomalies\n                for j in range(0, error_buffer):\n                    if (i + j) not in above_epsilon and (i + j) < len(smoothed_errors):\n                        above_epsilon.append(i + j)\n\n                    if (i - j) not in above_epsilon and (i - j) >= 0:\n                        above_epsilon.append(i - j)\n\n        if len(above_epsilon) == 0:\n            continue\n\n        # generate sequences\n        above_epsilon = sorted(list(set(above_epsilon)))\n        groups = [list(group) for group in mit.consecutive_groups(above_epsilon)]\n        above_sequences = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n\n        mean_perc_decrease = (mu - np.mean(below_epsilon)) / mu\n        sd_perc_decrease = (sigma - np.std(below_epsilon)) / sigma\n        epsilon = (mean_perc_decrease + sd_perc_decrease) /\\\n                  (len(above_sequences)**2 + len(above_epsilon))\n\n        # update the largest epsilon we've seen so far\n        if epsilon > max_epsilon:\n            sd_threshold = z\n            max_epsilon = epsilon\n\n    # sd_threshold can be multiplied by sigma to get epsilon\n    return max_epsilon, sd_threshold", "response": "This method calculates the epsilon threshold for an anomalies."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets anomalies from the smoothed errors and the error_buffer.", "response": "def get_anomalies(smoothed_errors, y_true, z, window, all_anomalies, error_buffer):\n    \"\"\"\n        Helper method to get anomalies.\n    \"\"\"\n\n    mu = np.mean(smoothed_errors)\n    sigma = np.std(smoothed_errors)\n\n    epsilon = mu + (z * sigma)\n\n    # compare to epsilon\n    errors_seq, anomaly_indices, max_error_below_e = group_consecutive_anomalies(\n        smoothed_errors,\n        epsilon,\n        y_true,\n        error_buffer,\n        window,\n        all_anomalies\n    )\n\n    if len(errors_seq) > 0:\n        anomaly_indices = prune_anomalies(\n            errors_seq,\n            smoothed_errors,\n            max_error_below_e,\n            anomaly_indices\n        )\n\n    return anomaly_indices"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prune_anomalies(e_seq, smoothed_errors, max_error_below_e, anomaly_indices):\n    # min accepted perc decrease btwn max errors in anomalous sequences\n    MIN_PERCENT_DECREASE = 0.05\n    e_seq_max, smoothed_errors_max = [], []\n\n    for error_seq in e_seq:\n        if len(smoothed_errors[error_seq[0]:error_seq[1]]) > 0:\n            sliced_errors = smoothed_errors[error_seq[0]:error_seq[1]]\n            e_seq_max.append(max(sliced_errors))\n            smoothed_errors_max.append(max(sliced_errors))\n\n    smoothed_errors_max.sort(reverse=True)\n\n    if max_error_below_e > 0:\n        smoothed_errors_max.append(max_error_below_e)\n    indices_remove = []\n\n    for i in range(len(smoothed_errors_max)):\n        if i < len(smoothed_errors_max) - 1:\n            delta = smoothed_errors_max[i] - smoothed_errors_max[i + 1]\n            perc_change = delta / smoothed_errors_max[i]\n            if perc_change < MIN_PERCENT_DECREASE:\n                indices_remove.append(e_seq_max.index(smoothed_errors_max[i]))\n\n    for index in sorted(indices_remove, reverse=True):\n        del e_seq[index]\n\n    pruned_indices = []\n\n    for i in anomaly_indices:\n        for error_seq in e_seq:\n            if i >= error_seq[0] and i <= error_seq[1]:\n                pruned_indices.append(i)\n\n    return pruned_indices", "response": "This method removes anomalies which don t meet a minimum separation from next anomaly."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse and set up the given nodes.", "response": "def _configure_nodes(self, nodes):\n        \"\"\"Parse and set up the given nodes.\n\n        :param nodes: nodes used to create the continuum (see doc for format).\n        \"\"\"\n        if isinstance(nodes, str):\n            nodes = [nodes]\n        elif not isinstance(nodes, (dict, list)):\n            raise ValueError(\n                'nodes configuration should be a list or a dict,'\n                ' got {}'.format(type(nodes)))\n\n        conf_changed = False\n        for node in nodes:\n            conf = {\n                'hostname': node,\n                'instance': None,\n                'nodename': node,\n                'port': None,\n                'vnodes': self._default_vnodes,\n                'weight': 1\n            }\n            current_conf = self.runtime._nodes.get(node, {})\n            nodename = node\n            # new node, trigger a ring update\n            if not current_conf:\n                conf_changed = True\n            # complex config\n            if isinstance(nodes, dict):\n                node_conf = nodes[node]\n                if isinstance(node_conf, int):\n                    conf['weight'] = node_conf\n                elif isinstance(node_conf, dict):\n                    for k, v in node_conf.items():\n                        if k in conf:\n                            conf[k] = v\n                            # changing those config trigger a ring update\n                            if k in ['nodename', 'vnodes', 'weight']:\n                                if current_conf.get(k) != v:\n                                    conf_changed = True\n                else:\n                    raise ValueError(\n                        'node configuration should be a dict or an int,'\n                        ' got {}'.format(type(node_conf)))\n            if self._weight_fn:\n                conf['weight'] = self._weight_fn(**conf)\n            # changing the weight of a node trigger a ring update\n            if current_conf.get('weight') != conf['weight']:\n                conf_changed = True\n            self.runtime._nodes[nodename] = conf\n        return conf_changed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the index of the given key in the sorted key list.", "response": "def _get_pos(self, key):\n        \"\"\"Get the index of the given key in the sorted key list.\n\n        We return the position with the nearest hash based on\n        the provided key unless we reach the end of the continuum/ring\n        in which case we return the 0 (beginning) index position.\n\n        :param key: the key to hash and look for.\n        \"\"\"\n        p = bisect(self.runtime._keys, self.hashi(key))\n        if p == len(self.runtime._keys):\n            return 0\n        else:\n            return p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get(self, key, what):\n        if not self.runtime._ring:\n            return None\n\n        pos = self._get_pos(key)\n        if what == 'pos':\n            return pos\n\n        nodename = self.runtime._ring[self.runtime._keys[pos]]\n        if what in ['hostname', 'instance', 'port', 'weight']:\n            return self.runtime._nodes[nodename][what]\n        elif what == 'dict':\n            return self.runtime._nodes[nodename]\n        elif what == 'nodename':\n            return nodename\n        elif what == 'tuple':\n            return (self.runtime._keys[pos], nodename)", "response": "Generic getter magic method.\n\n        The node with the nearest but not less hash value is returned.\n\n        :param key: the key to look for.\n        :param what: the information to look for in, allowed values:\n            - instance (default): associated node instance\n            - nodename: node name\n            - pos: index of the given key in the ring\n            - tuple: ketama compatible (pos, name) tuple\n            - weight: node weight"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of the instances of all the configured nodes.", "response": "def get_instances(self):\n        \"\"\"Returns a list of the instances of all the configured nodes.\n        \"\"\"\n        return [c.get('instance') for c in self.runtime._nodes.values()\n                if c.get('instance')]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a ketama compatible list of ( position nodename ) tuples.", "response": "def get_points(self):\n        \"\"\"Returns a ketama compatible list of (position, nodename) tuples.\n        \"\"\"\n        return [(k, self.runtime._ring[k]) for k in self.runtime._keys]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over the nodes in the hash_ring that hold the given key.", "response": "def iterate_nodes(self, key, distinct=True):\n        \"\"\"hash_ring compatibility implementation.\n\n        Given a string key it returns the nodes as a generator that\n        can hold the key.\n        The generator iterates one time through the ring\n        starting at the correct position.\n        if `distinct` is set, then the nodes returned will be unique,\n        i.e. no virtual copies will be returned.\n        \"\"\"\n        if not self.runtime._ring:\n            yield None\n        else:\n            for node in self.range(key, unique=distinct):\n                yield node['nodename']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_continuum(self):\n        numpoints = len(self.runtime._keys)\n        if numpoints:\n            print('Numpoints in continuum: {}'.format(numpoints))\n        else:\n            print('Continuum empty')\n        for p in self.get_points():\n            point, node = p\n            print('{} ({})'.format(node, point))", "response": "Prints a ketama compatible continuum report."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a generator of nodes' configuration available in the continuum / ring.", "response": "def range(self, key, size=None, unique=True):\n        \"\"\"Returns a generator of nodes' configuration available\n        in the continuum/ring.\n\n        :param key: the key to look for.\n        :param size: limit the list to at most this number of nodes.\n        :param unique: a node may only appear once in the list (default True).\n        \"\"\"\n        all_nodes = set()\n        if unique:\n            size = size or len(self.runtime._nodes)\n        else:\n            all_nodes = []\n\n        pos = self._get_pos(key)\n        for key in self.runtime._keys[pos:]:\n            nodename = self.runtime._ring[key]\n            if unique:\n                if nodename in all_nodes:\n                    continue\n                all_nodes.add(nodename)\n            else:\n                all_nodes.append(nodename)\n            yield self.runtime._nodes[nodename]\n            if len(all_nodes) == size:\n                break\n        else:\n            for i, key in enumerate(self.runtime._keys):\n                if i < pos:\n                    nodename = self.runtime._ring[key]\n                    if unique:\n                        if nodename in all_nodes:\n                            continue\n                        all_nodes.add(nodename)\n                    else:\n                        all_nodes.append(nodename)\n                    yield self.runtime._nodes[nodename]\n                    if len(all_nodes) == size:\n                        break"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmonkey patch python - memcache to implement our consistent hashring in its node selection and operations.", "response": "def patch_memcache():\n    \"\"\"Monkey patch python-memcached to implement our consistent hashring\n    in its node selection and operations.\n    \"\"\"\n\n    def _init(self, servers, *k, **kw):\n        self._old_init(servers, *k, **kw)\n\n        nodes = {}\n        for server in self.servers:\n            conf = {\n                'hostname': server.ip,\n                'instance': server,\n                'port': server.port,\n                'weight': server.weight\n            }\n            nodes[server.ip] = conf\n        self.uhashring = HashRing(nodes)\n\n    def _get_server(self, key):\n        if isinstance(key, tuple):\n            return self._old_get_server(key)\n\n        for i in range(self._SERVER_RETRIES):\n            for node in self.uhashring.range(key):\n                if node['instance'].connect():\n                    return node['instance'], key\n\n        return None, None\n\n    memcache = __import__('memcache')\n    memcache.Client._old_get_server = memcache.Client._get_server\n    memcache.Client._old_init = memcache.Client.__init__\n    memcache.Client.__init__ = _init\n    memcache.Client._get_server = _get_server"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_ring(self, nodes):\n        for node_name, node_conf in nodes:\n            for w in range(0, node_conf['vnodes'] * node_conf['weight']):\n                self._distribution[node_name] += 1\n                self._ring[self.hashi('%s-%s' % (node_name, w))] = node_name\n        self._keys = sorted(self._ring.keys())", "response": "Generate a ketama compatible continuum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_node(self, node_name):\n        try:\n            node_conf = self._nodes.pop(node_name)\n        except Exception:\n            raise KeyError('node \\'{}\\' not found, available nodes: {}'.format(\n                node_name, self._nodes.keys()))\n        else:\n            self._distribution.pop(node_name)\n            for w in range(0, node_conf['vnodes'] * node_conf['weight']):\n                del self._ring[self.hashi('%s-%s' % (node_name, w))]\n            self._keys = sorted(self._ring.keys())", "response": "Remove the given node from the continuum."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hashi(self, key, replica=0):\n        dh = self._listbytes(md5(str(key).encode('utf-8')).digest())\n        rd = replica * 4\n        return (\n            (dh[3 + rd] << 24) | (dh[2 + rd] << 16) |\n            (dh[1 + rd] << 8) | dh[0 + rd])", "response": "Returns a ketama compatible hash from the given key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the weight factor of the given node and yields its hash key for every configured replica.", "response": "def _hashi_weight_generator(self, node_name, node_conf):\n        \"\"\"Calculate the weight factor of the given node and\n        yield its hash key for every configured replica.\n\n        :param node_name: the node name.\n        \"\"\"\n        ks = (node_conf['vnodes'] * len(self._nodes) *\n              node_conf['weight']) // self._weight_sum\n        for w in range(0, ks):\n            w_node_name = '%s-%s' % (node_name, w)\n            for i in range(0, self._replicas):\n                yield self.hashi(w_node_name, replica=i)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a ketama compatible continuum.", "response": "def _create_ring(self, nodes):\n        \"\"\"Generate a ketama compatible continuum/ring.\n        \"\"\"\n        _weight_sum = 0\n        for node_conf in self._nodes.values():\n            _weight_sum += node_conf['weight']\n        self._weight_sum = _weight_sum\n\n        _distribution = Counter()\n        _keys = []\n        _ring = {}\n        for node_name, node_conf in self._nodes.items():\n            for h in self._hashi_weight_generator(node_name, node_conf):\n                _ring[h] = node_name\n                insort(_keys, h)\n                _distribution[node_name] += 1\n        self._distribution = _distribution\n        self._keys = _keys\n        self._ring = _ring"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the given node from the continuum or ring.", "response": "def _remove_node(self, node_name):\n        \"\"\"Remove the given node from the continuum/ring.\n\n        :param node_name: the node name.\n        \"\"\"\n        try:\n            self._nodes.pop(node_name)\n        except Exception:\n            raise KeyError('node \\'{}\\' not found, available nodes: {}'.format(\n                node_name, self._nodes.keys()))\n        else:\n            self._create_ring(self._nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_numpy_status():\n    numpy_status = {}\n    try:\n        import numpy\n        numpy_version = numpy.__version__\n        numpy_status['up_to_date'] = parse_version(\n            numpy_version) >= parse_version(NUMPY_MIN_VERSION)\n        numpy_status['version'] = numpy_version\n    except ImportError:\n        traceback.print_exc()\n        numpy_status['up_to_date'] = False\n        numpy_status['version'] = \"\"\n    return numpy_status", "response": "Returns a dictionary containing a boolean specifying whether NumPy is up - to - date along with the version string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsolving sparse linear assignment problem using Jonker - Volgenant algorithm.", "response": "def lapmod(n, cc, ii, kk, fast=True, return_cost=True,\n           fp_version=FP_DYNAMIC):\n    \"\"\"Solve sparse linear assignment problem using Jonker-Volgenant algorithm.\n\n    n: number of rows of the assignment cost matrix\n    cc: 1D array of all finite elements of the assignement cost matrix\n    ii: 1D array of indices of the row starts in cc. The following must hold:\n            ii[0] = 0 and ii[n+1] = len(cc).\n    kk: 1D array of the column indices so that:\n            cost[i, kk[ii[i] + k]] == cc[ii[i] + k].\n        Indices within one row must be sorted.\n    extend_cost: whether or not extend a non-square matrix [default: False]\n    cost_limit: an upper limit for a cost of a single assignment\n                [default: np.inf]\n    return_cost: whether or not to return the assignment cost\n\n    Returns (opt, x, y) where:\n      opt: cost of the assignment\n      x: vector of columns assigned to rows\n      y: vector of rows assigned to columns\n    or (x, y) if return_cost is not True.\n\n    When extend_cost and/or cost_limit is set, all unmatched entries will be\n    marked by -1 in x/y.\n    \"\"\"\n    # log = logging.getLogger('lapmod')\n\n    check_cost(n, cc, ii, kk)\n\n    if fast is True:\n        # log.debug('[----CR & RT & ARR & augmentation ----]')\n        x, y = _lapmod(n, cc, ii, kk, fp_version=fp_version)\n    else:\n        cc = np.ascontiguousarray(cc, dtype=np.float64)\n        ii = np.ascontiguousarray(ii, dtype=np.int32)\n        kk = np.ascontiguousarray(kk, dtype=np.int32)\n        x = np.empty((n,), dtype=np.int32)\n        y = np.empty((n,), dtype=np.int32)\n        v = np.empty((n,), dtype=np.float64)\n        free_rows = np.empty((n,), dtype=np.int32)\n        # log.debug('[----Column reduction & reduction transfer----]')\n        n_free_rows = _pycrrt(n, cc, ii, kk, free_rows, x, y, v)\n        # log.debug(\n        #     'free, x, y, v: %s %s %s %s', free_rows[:n_free_rows], x, y, v)\n        if n_free_rows == 0:\n            # log.info('Reduction solved it.')\n            if return_cost is True:\n                return get_cost(n, cc, ii, kk, x), x, y\n            else:\n                return x, y\n        for it in range(2):\n            # log.debug('[---Augmenting row reduction (iteration: %d)---]', it)\n            n_free_rows = _pyarr(\n                    n, cc, ii, kk, n_free_rows, free_rows, x, y, v)\n            # log.debug(\n            #   'free, x, y, v: %s %s %s %s', free_rows[:n_free_rows], x, y, v)\n            if n_free_rows == 0:\n                # log.info('Augmenting row reduction solved it.')\n                if return_cost is True:\n                    return get_cost(n, cc, ii, kk, x), x, y\n                else:\n                    return x, y\n        # log.info('[----Augmentation----]')\n        _pya(n, cc, ii, kk, n_free_rows, free_rows, x, y, v)\n        # log.debug('x, y, v: %s %s %s', x, y, v)\n    if return_cost is True:\n        return get_cost(n, cc, ii, kk, x), x, y\n    else:\n        return x, y"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_provider(cls, provider):\n        def decorator(subclass):\n            \"\"\"Register as decorator function.\"\"\"\n            cls._providers[provider] = subclass\n            subclass.name = provider\n            return subclass\n        return decorator", "response": "Register method to keep list of providers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tar_to_bigfile(self, fname, outfile):\n        fnames = []\n        tmpdir = mkdtemp()\n        \n        # Extract files to temporary directory\n        with tarfile.open(fname) as tar:\n            tar.extractall(path=tmpdir)\n        for root, _, files in os.walk(tmpdir):\n            fnames += [os.path.join(root, fname) for fname in files]\n        \n        # Concatenate\n        with open(outfile, \"w\") as out:\n            for infile in fnames:\n                for line in open(infile):\n                    out.write(line)\n                os.unlink(infile)\n        \n        # Remove temp dir\n        shutil.rmtree(tmpdir)", "response": "Convert tar of multiple FASTAs to one file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a genome file to a specific directory.", "response": "def download_genome(self, name, genome_dir, localname=None, mask=\"soft\", regex=None, invert_match=False, version=None):\n        \"\"\"\n        Download a (gzipped) genome file to a specific directory\n\n        Parameters\n        ----------\n        name : str\n            Genome / species name\n        \n        genome_dir : str\n            Directory to install genome\n\n        mask: str , optional\n            Masking, soft, hard or none (all other strings)\n        \"\"\"\n        genome_dir = os.path.expanduser(genome_dir)\n        \n        if not os.path.exists(genome_dir):\n            os.makedirs(genome_dir)\n        \n        dbname, link = self.get_genome_download_link(name, mask=mask, version=version)\n        myname = dbname \n        if localname:\n            myname = localname\n        \n        myname = myname.replace(\" \", \"_\")\n\n        gzipped = False\n        if link.endswith(\".gz\"):\n            gzipped = True\n\n        if not os.path.exists(os.path.join(genome_dir, myname)):\n            os.makedirs(os.path.join(genome_dir, myname))\n        urlcleanup()\n        response = urlopen(link)\n         \n        sys.stderr.write(\"downloading from {}...\\n\".format(link))\n        down_dir = genome_dir\n        fname = os.path.join(genome_dir, myname, myname + \".fa\")\n        if regex:\n            down_dir = mkdtemp()\n            os.mkdir(os.path.join(down_dir, myname))\n            fname = os.path.join(down_dir, myname, myname + \".fa\") \n        with open(fname, \"wb\") as f_out:\n            if gzipped:\n                # Supports both Python 2.7 as well as 3\n                with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f_in:\n                    shutil.copyfileobj(f_in, f_out)\n            else:\n                f_out.write(response.read())\n        sys.stderr.write(\"done...\\n\")\n        \n        if link.endswith(\"tar.gz\"):\n            self.tar_to_bigfile(fname, fname) \n        \n        if hasattr(self, '_post_process_download'):\n            self._post_process_download(name, down_dir, mask)\n        \n        if regex:\n            infa = fname\n            outfa = os.path.join(genome_dir, myname, myname + \".fa\") \n            filter_fasta(\n                infa,\n                outfa,\n                regex=regex,\n                v=invert_match,\n                force=True\n                )\n\n            not_included = [k for k in Fasta(infa).keys() if k not in Fasta(outfa).keys()]\n            shutil.rmtree(down_dir)\n            fname = outfa\n        \n        sys.stderr.write(\"name: {}\\n\".format(dbname))\n        sys.stderr.write(\"local name: {}\\n\".format(myname))\n        sys.stderr.write(\"fasta: {}\\n\".format(fname))\n\n        # Create readme with information\n        readme = os.path.join(genome_dir, myname, \"README.txt\")\n        with open(readme, \"w\") as f:\n            f.write(\"name: {}\\n\".format(myname))\n            f.write(\"original name: {}\\n\".format(dbname))\n            f.write(\"original filename: {}\\n\".format(os.path.split(link)[-1]))\n            f.write(\"url: {}\\n\".format(link))\n            f.write(\"mask: {}\\n\".format(mask))\n            f.write(\"date: {}\\n\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n            if regex:\n                if invert_match:\n                    f.write(\"regex: {} (inverted match)\\n\".format(regex))\n                else:\n                    f.write(\"regex: {}\\n\".format(regex))\n                f.write(\"sequences that were excluded:\\n\")\n                for seq in not_included:\n                    f.write(\"\\t{}\\n\".format(seq))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_plugins():\n    plugin_dir = os.path.dirname(os.path.realpath(__file__))\n    plugin_dir = os.path.join(plugin_dir, \"plugins\")\n    plugin_files = [x[:-3] for x in os.listdir(plugin_dir) if x.endswith(\".py\")]\n    sys.path.insert(0, plugin_dir)\n    for plugin in plugin_files:\n        __import__(plugin)", "response": "Locate and initialize all available plugins."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert(name):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "response": "Convert CamelCase to underscore"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_plugins():\n    find_plugins()\n    d = {}\n    for c in Plugin.__subclasses__():\n        ins = c()\n    \n        if ins.name() in config.get(\"plugin\", []):\n            ins.activate()\n        \n        d[ins.name()] = ins\n    \n    return d", "response": "Return dictionary of available plugins returning object containing all available plugins"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nactivates the specified plugin.", "response": "def activate(name):\n    \"\"\"Activate plugin.\n\n    Parameters\n    ----------\n    name : str\n        Plugin name.\n    \"\"\"\n    if name in plugins:\n        plugins[name].activate()\n    else:\n        raise Exception(\"plugin {} not found\".format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deactivate(name):\n    if name in plugins:\n        plugins[name].deactivate()\n    else:\n        raise Exception(\"plugin {} not found\".format(name))", "response": "Deactivate the specified plugin."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef manage_config(cmd, *args):\n    if cmd == \"file\":\n        print(config.config_file)\n    elif cmd == \"show\":\n        with open(config.config_file) as f:\n            print(f.read())\n    elif cmd == \"generate\":\n        fname = os.path.join(\n                user_config_dir(\"genomepy\"), \"{}.yaml\".format(\"genomepy\")\n            )\n        \n        if not os.path.exists(user_config_dir(\"genomepy\")):\n            os.makedirs(user_config_dir(\"genomepy\"))\n        \n        with open(fname, \"w\") as fout:\n            with open(config.config_file) as fin:\n                fout.write(fin.read())\n        print(\"Created config file {}\".format(fname))", "response": "Manage genomepy config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_available_genomes(provider=None):\n    if provider:\n        providers = [ProviderBase.create(provider)]\n    else:\n        # if provider is not specified search all providers\n        providers = [ProviderBase.create(p) for \n                        p in ProviderBase.list_providers()]\n\n    for p in providers:\n        for row in p.list_available_genomes():\n            yield [p.name] + list(row)", "response": "List all available genomes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist all available genomes.", "response": "def list_installed_genomes(genome_dir=None):\n    \"\"\"\n    List all available genomes.\n\n    Parameters\n    ----------\n    genome_dir : str\n        Directory with installed genomes.\n\n    Returns\n    -------\n    list with genome names\n    \"\"\"\n    if not genome_dir:\n        genome_dir = config.get(\"genome_dir\", None)\n    if not genome_dir:\n        raise norns.exceptions.ConfigError(\"Please provide or configure a genome_dir\")\n\n    return [f for f in os.listdir(genome_dir) if \n            _is_genome_dir(genome_dir + \"/\" + f)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(term, provider=None):\n    if provider:\n        providers = [ProviderBase.create(provider)]\n    else:\n        # if provider is not specified search all providers\n        providers = [ProviderBase.create(p) for \n                        p in ProviderBase.list_providers()]\n\n    for p in providers:\n        for row in p.search(term):\n            yield [x.encode('latin-1') for x in [p.name] + list(row)]", "response": "Search for a specific term in a sequence of objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install_genome(name, provider, version=None, genome_dir=None, localname=None, mask=\"soft\", regex=None, invert_match=False, annotation=False):\n    if not genome_dir:\n        genome_dir = config.get(\"genome_dir\", None)\n    if not genome_dir:\n        raise norns.exceptions.ConfigError(\"Please provide or configure a genome_dir\")\n   \n    genome_dir = os.path.expanduser(genome_dir)\n    localname = get_localname(name, localname)\n    \n    # Download genome from provider\n    p = ProviderBase.create(provider)\n    p.download_genome(\n        name, \n        genome_dir, \n        version=version,\n        mask=mask, \n        localname=localname, \n        regex=regex, \n        invert_match=invert_match)\n\n    if annotation:\n        # Download annotation from provider\n        p.download_annotation(name, genome_dir, localname=localname, version=version)\n\n    g = Genome(localname, genome_dir=genome_dir)\n    for plugin in get_active_plugins():\n        plugin.after_genome_download(g)\n\n    generate_env()", "response": "Install a genome from the specified provider and version."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of n random elements from a list of tuples.", "response": "def _weighted_selection(l, n):\n    \"\"\"\n        Selects  n random elements from a list of (weight, item) tuples.\n        Based on code snippet by Nick Johnson\n    \"\"\"\n    cuml = []\n    items = []\n    total_weight = 0.0\n    for weight, item in l:\n        total_weight += weight\n        cuml.append(total_weight)\n        items.append(item)\n\n    return [items[bisect.bisect(cuml, random.random()*total_weight)] for _ in range(n)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_exports():\n\tenv = []\n\tfor name in list_installed_genomes():\n\t    try:\n\t        g = Genome(name)\n\t        env_name = re.sub(r'[^\\w]+', \"_\", name).upper()\n\t        env.append(\"export {}={}\".format(env_name, g.filename))\n\t    except:\n\t        pass\n\treturn env", "response": "Print export commands for setting environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates file with exports. INDRA NODES.", "response": "def generate_env(fname=None):\n    \"\"\"Generate file with exports. \n\n    By default this is in .config/genomepy/exports.txt.\n\n    Parameters\n    ----------\n    fname: strs, optional\n        Name of the output file.\n    \"\"\"\n    config_dir = user_config_dir(\"genomepy\")\n    if os.path.exists(config_dir):\n        fname = os.path.join(config_dir, \"exports.txt\")\n        with open(fname, \"w\") as fout:\n            for env in generate_exports():\n                fout.write(\"{}\\n\".format(env))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmanage or disable plugins.", "response": "def manage_plugins(command, plugin_names=None):\n    \"\"\"Enable or disable plugins.\n    \"\"\" \n    if plugin_names is None:\n        plugin_names = []\n    active_plugins = config.get(\"plugin\", [])\n    plugins = init_plugins()\n    if command == \"enable\":\n        for name in plugin_names:\n            if name not in plugins:\n                raise ValueError(\"Unknown plugin: {}\".format(name))\n            if name not in active_plugins:\n                active_plugins.append(name)\n    elif command == \"disable\":\n        for name in plugin_names:\n            if name in active_plugins:\n                active_plugins.remove(name)\n    elif command == \"list\":\n        print(\"{:20}{}\".format(\"plugin\", \"enabled\"))\n        for plugin in sorted(plugins):\n            print(\"{:20}{}\".format(plugin, {False:\"\", True:\"*\"}[plugin in active_plugins]))\n    else:\n        raise ValueError(\"Invalid plugin command\")\n    config[\"plugin\"] = active_plugins\n    config.save()\n\n    if command in [\"enable\", \"disable\"]:\n        print(\"Enabled plugins: {}\".format(\", \".join(sorted(active_plugins))))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary with chromosomes as keys and the total number of entries of the cluster in the gap file.", "response": "def gap_sizes(self):\n        \"\"\"Return gap sizes per chromosome.\n        \n        Returns\n        -------\n        gap_sizes : dict\n            a dictionary with chromosomes as key and the total number of\n            Ns as values\n        \"\"\" \n        if not self._gap_sizes:\n            gap_file = self.props[\"gaps\"][\"gaps\"]\n            self._gap_sizes = {}\n            with open(gap_file) as f:\n                for line in f:\n                    chrom, start, end = line.strip().split(\"\\t\")\n                    start, end = int(start), int(end)\n                    self._gap_sizes[chrom] = self._gap_sizes.get(chrom, 0) + end - start\n        return self._gap_sizes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_random_sequences(self, n=10, length=200, chroms=None, max_n=0.1):\n        retries = 100\n        cutoff = length * max_n\n        if not chroms:\n            chroms = self.keys()\n   \n        try:\n            gap_sizes = self.gap_sizes()\n        except:\n            gap_sizes = {}\n        sizes = dict([(chrom, len(self[chrom]) - gap_sizes.get(chrom, 0)) for chrom in chroms])\n   \n        l = [(sizes[x], x) for x in chroms if \n                sizes[x] / len(self[x]) > 0.1 and sizes[x] > 10 * length]\n        chroms = _weighted_selection(l, n)\n        coords = []\n        \n        count = {}\n        for chrom in chroms:\n            if chrom in count:\n                count[chrom] += 1\n            else:\n                count[chrom] = 1\n\n        for chrom in chroms:\n            for i in range(retries):\n                start = int(random.random() * (sizes[chrom] - length))\n                end = start + length\n                count_n = self[chrom][start:end].seq.upper().count(\"N\")\n                if count_n <= cutoff:\n                    break\n            if count_n > cutoff:\n                raise ValueError(\"Failed to find suitable non-N sequence for {}\".format(chrom))\n            \n            coords.append([chrom, start, end])\n\n        return coords", "response": "Return random genomic sequences from these chromosomes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for genomes that contain TERM in their name or description.", "response": "def search(term, provider=None):\n    \"\"\"Search for genomes that contain TERM in their name or description.\"\"\"\n    for row in genomepy.search(term, provider):\n        print(\"\\t\".join([x.decode('utf-8', 'ignore') for x in row]))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall a genome NAME from provider PROVIDER in GENOME_DIR.", "response": "def install(name, provider, genome_dir, localname, mask, regex, match, annotation):\n    \"\"\"Install genome NAME from provider PROVIDER in directory GENOME_DIR.\"\"\"\n    genomepy.install_genome(\n            name, provider, genome_dir=genome_dir, localname=localname, mask=mask, \n            regex=regex, invert_match=not(match), annotation=annotation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a BED file with gap locations.", "response": "def generate_gap_bed(fname, outname):\n    \"\"\" Generate a BED file with gap locations.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of input FASTA file.\n\n    outname : str\n        Filename of output BED file.\n    \"\"\" \n    f = Fasta(fname)\n    with open(outname, \"w\") as bed:\n        for chrom in f.keys():\n            for m in re.finditer(r'N+', f[chrom][:].seq):\n                bed.write(\"{}\\t{}\\t{}\\n\".format(chrom, m.start(0), m.end(0)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a sizes file with length of sequences in FASTA file.", "response": "def generate_sizes(name, genome_dir):\n    \"\"\"Generate a sizes file with length of sequences in FASTA file.\"\"\"\n    fa = os.path.join(genome_dir, name, \"{}.fa\".format(name))\n    sizes = fa + \".sizes\"\n    g = Fasta(fa)\n    with open(sizes, \"w\") as f:\n        for seqname in g.keys():\n            f.write(\"{}\\t{}\\n\".format(seqname, len(g[seqname])))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_fasta(infa, outfa, regex=\".*\", v=False, force=False):\n    if infa == outfa:\n        raise ValueError(\"Input and output FASTA are the same file.\")\n\n    if os.path.exists(outfa):\n        if force:\n            os.unlink(outfa)\n            if os.path.exists(outfa + \".fai\"):\n                os.unlink(outfa + \".fai\")\n        else:\n            raise ValueError(\n                    \"{} already exists, set force to True to overwrite\".format(outfa))\n            \n    filt_function = re.compile(regex).search\n    fa = Fasta(infa, filt_function=filt_function)\n    seqs = fa.keys()\n    if v:\n        original_fa = Fasta(infa)\n        seqs = [s for s in original_fa.keys() if s not in seqs]\n        fa = original_fa\n    \n    if len(seqs) == 0:\n        raise ValueError(\"No sequences left after filtering!\")\n\n    with open(outfa, \"w\") as out:\n        for chrom in seqs:\n            out.write(\">{}\\n\".format(fa[chrom].name))\n            out.write(\"{}\\n\".format(fa[chrom][:].seq))\n\n    return Fasta(outfa)", "response": "Filter input FASTA file based on regex."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if cmd can be run.", "response": "def cmd_ok(cmd):\n    \"\"\"Returns True if cmd can be run.\n    \"\"\" \n    try:\n        sp.check_call(cmd, stderr=sp.PIPE, stdout=sp.PIPE)\n    except sp.CalledProcessError:\n        # bwa gives return code of 1 with no argument\n        pass\n    except:\n        sys.stderr.write(\"{} not found, skipping\\n\".format(cmd))\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning command and show errors if the returncode is non - zero.", "response": "def run_index_cmd(name, cmd):\n    \"\"\"Run command, show errors if the returncode is non-zero.\"\"\"\n    sys.stderr.write(\"Creating {} index...\\n\".format(name))\n    # Create index\n    p = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)\n    stdout, stderr = p.communicate()\n    if p.returncode != 0:\n        sys.stderr.write(\"Index for {} failed\\n\".format(name))\n        sys.stderr.write(stdout)\n        sys.stderr.write(stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scan_cgroups(subsys_name, filters=list()):\n    status = SubsystemStatus()\n    if subsys_name not in status.get_all():\n        raise NoSuchSubsystemError(\"No such subsystem found: \" + subsys_name)\n\n    if subsys_name not in status.get_available():\n        raise EnvironmentError(\"Disabled in the kernel: \" + subsys_name)\n\n    if subsys_name not in status.get_enabled():\n        raise EnvironmentError(\"Not enabled in the system: \" + subsys_name)\n\n    subsystem = _get_subsystem(subsys_name)\n    mount_point = status.get_path(subsys_name)\n    return _scan_cgroups_recursive(subsystem, mount_point, mount_point, filters)", "response": "This function returns a list of control group hierarchy which belong to the given subsystem."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cgroup(fullpath):\n    # Canonicalize symbolic links\n    fullpath = os.path.realpath(fullpath)\n\n    status = SubsystemStatus()\n    name = None\n    for name, path in status.paths.items():\n        if path in fullpath:\n            break\n    else:\n        raise Exception('Invalid path: ' + fullpath)\n    subsys = _get_subsystem(name)\n\n    return CGroup(subsys, fullpath)", "response": "This function returns a CGroup object which is pointed by the fullpath. It returns a CGroup object which is pointed by the fullpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_proc_cgroups(self):\n\n        \"\"\"\n        #subsys_name\thierarchy\tnum_cgroups\tenabled\n        cpuset\t0\t1\t1\n        ns\t0\t1\t1\n        cpu\t1\t10\t1\n        cpuacct\t0\t1\t1\n        memory\t0\t1\t1\n        devices\t0\t1\t1\n        freezer\t0\t1\t1\n        net_cls\t0\t1\t1\n        \"\"\"\n        for line in fileops.readlines('/proc/cgroups'):\n            m = self._RE_CGROUPS.match(line)\n            if m is None:\n                continue\n\n            name = m.group('name')\n            hierarchy = int(m.group('hier'))\n            n_cgroups = int(m.group('n'))\n            if m.group('enabled') == '1':\n                enabled = True\n            else:\n                enabled = False\n            if name not in self:\n                self[name] = {}\n            self[name]['name'] = name\n            self[name]['hierarchy'] = hierarchy\n            self[name]['num_cgroups'] = n_cgroups\n            self[name]['enabled'] = enabled", "response": "Parse the proc cgroups file and store the attributes in the self dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_proc_mount(self):\n\n        \"\"\"\n        cgroup /cgroup/cpu cgroup rw,relatime,cpuacct,cpu,release_agent=/sbin/cgroup_clean 0 0\n        cgroup /cgroup/memory cgroup rw,relatime,memory 0 0\n        cgroup /cgroup/blkio cgroup rw,relatime,blkio 0 0\n        cgroup /cgroup/freezer cgroup rw,relatime,freezer 0 0\n        \"\"\"\n\n        for line in fileops.readlines('/proc/mounts'):\n            if 'cgroup' not in line:\n                continue\n\n            items = line.split(' ')\n            path = items[1]\n            opts = items[3].split(',')\n\n            name = None\n            for opt in opts:\n                if opt in self:\n                    name = opt\n                    self.paths[name] = path\n                if 'name=' in opt:\n                    # We treat name=XXX as its name\n                    name = opt\n                    self.paths[name] = path\n                    self[name] = {}\n                    self[name]['name'] = name\n                    self[name]['enabled'] = True\n                    self[name]['hierarchy'] = 0\n                    self[name]['num_cgroups'] = 0\n            # release_agent= may appear before name=\n            for opt in opts:\n                if 'release_agent=' in opt:\n                    self[name]['release_agent'] = opt.replace('release_agent=', '')", "response": "Parse the proc mount file and store the information in the self."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse rdma. curren and rdma. max .", "response": "def parse(content):\n        \"\"\" Parse rdma.curren and rdma.max\n\n        Example contents:\n          mlx4_0 hca_handle=2 hca_object=2000\n          ocrdma1 hca_handle=3 hca_object=max\n\n        >>> RdmaStat.parse(\"mlx4_0 hca_handle=2 hca_object=2000\\\\nocrdma1 hca_handle=3 hca_object=max\")\n        {'mlx4_0': {'hca_handle': 2, 'hca_object': 2000}, 'ocrdma1': {'hca_handle': 3, 'hca_object': 'max'}}\n        \"\"\"\n        ret = {}\n        lines = content.split('\\n')\n        for line in lines:\n            m = RdmaStat._RE.match(line)\n            if m is None:\n                continue\n            name = m.group('name')\n            hca_handle = long(m.group('hca_handle'))\n            hca_object = m.group('hca_object')\n            if hca_object != \"max\":\n                hca_object = long(hca_object)\n            ret[name] = {\"hca_handle\": hca_handle, \"hca_object\": hca_object}\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_configs(self):\n        configs = {}\n        for name, default in self.configs.items():\n            cls = default.__class__\n            path = self.paths[name]\n            if os.path.exists(path):\n                try:\n                    configs[name] = self._PARSERS[cls](fileops.read(path))\n                except IOError as e:\n                    if e.errno == errno.EOPNOTSUPP:\n                        # Since 3.5 memory.memsw.* are always created even if disabled.\n                        # If disabled we will get EOPNOTSUPP when read or write them.\n                        # See commit af36f906c0f4c2ffa0482ecdf856a33dc88ae8c5 of the kernel.\n                        pass\n                    else:\n                        raise\n        return configs", "response": "This method returns a name and a current value pairs of control files which are categorised in the configs group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self):\n        pids = fileops.readlines(self.paths['cgroup.procs'])\n        self.pids = [int(pid) for pid in pids if pid != '']\n        self.n_procs = len(pids)", "response": "It updates process information of the cgroup."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a target file with arguments ( if required ) to a event_control file which we want to be notified events.", "response": "def register(self, arguments=list()):\n        \"\"\"\n        Register a target file with arguments (if required) to a event_control file\n        which we want to be notified events.\n        \"\"\"\n        target_name = self.target_name\n        if target_name in ['memory.usage_in_bytes', 'memory.memsw.usage_in_bytes']:\n            threshold = arguments[0]\n            line = \"%d %d %d\\0\" % (self.event_fd, self.target_fd, long(threshold))\n        elif target_name in ['memory.pressure_level']:\n            threshold = arguments[0]\n            line = \"%d %d %s\\0\" % (self.event_fd, self.target_fd, threshold)\n        else:\n            line = \"%d %d\\0\" % (self.event_fd, self.target_fd)\n        os.write(self.ec_fd, line)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait(self):\n        ret = os.read(self.event_fd, 64 / 8)\n        return struct.unpack('Q', ret)", "response": "It returns when an event which we have configured by set_threshold occurs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_process_container_tree(self, pids):\n        containers = []\n        procs = []\n        ppids = []\n        childs = {}\n        for pid in pids:\n            proc = process.Process(pid)\n            procs.append(proc)\n            ppids.append(proc.ppid)\n            if proc.ppid not in childs:\n                childs[proc.ppid] = []\n            childs[proc.ppid].append(proc)\n        ppids = set(ppids)\n        tops = [proc for proc in procs if proc.ppid not in pids]\n        if len(tops) == 0:\n            tops = procs\n\n        def build_tree(proc_list):\n            _containers = []\n            for proc in proc_list:\n                if not self.args.show_kthread and proc.is_kthread():\n                    continue\n\n                cont = TreeContainer(proc)\n\n                if proc.pid in childs:\n                    cont.childs = build_tree(childs[proc.pid])\n                _containers.append(cont)\n            return _containers\n\n        for top_proc in tops:\n            if not self.args.show_kthread and top_proc.is_kthread():\n                continue\n\n            cont = TreeContainer(top_proc)\n            if top_proc.pid in childs:\n                cont.childs = build_tree(childs[top_proc.pid])\n            containers.append(cont)\n\n        return containers", "response": "Builds the process container tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(data, conf=True):\n    f = Conf() if conf else []\n    lopen = []\n    index = 0\n\n    while True:\n        m = re.compile(r'^\\s*events\\s*{', re.S).search(data[index:])\n        if m:\n            e = Events()\n            lopen.insert(0, e)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*http\\s*{', re.S).search(data[index:])\n        if m:\n            h = Http()\n            lopen.insert(0, h)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*stream\\s*{', re.S).search(data[index:])\n        if m:\n            s = Stream()\n            lopen.insert(0, s)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*server\\s*{', re.S).search(data[index:])\n        if m:\n            s = Server()\n            lopen.insert(0, s)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*location\\s*([^;]*?)\\s*{', re.S).search(data[index:])\n        if m:\n            l = Location(m.group(1))\n            lopen.insert(0, l)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*if\\s*([^;]*?)\\s*{', re.S).search(data[index:])\n        if m:\n            ifs = If(m.group(1))\n            lopen.insert(0, ifs)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*upstream\\s*([^;]*?)\\s*{', re.S).search(data[index:])\n        if m:\n            u = Upstream(m.group(1))\n            lopen.insert(0, u)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*geo\\s*([^;]*?)\\s*{', re.S).search(data[index:])\n        if m:\n            g = Geo(m.group(1))\n            lopen.insert(0, g)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*map\\s*([^;]*?)\\s*{', re.S).search(data[index:])\n        if m:\n            g = Map(m.group(1))\n            lopen.insert(0, g)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*limit_except\\s*([^;]*?)\\s*{', re.S).search(data[index:])\n        if m:\n            l = LimitExcept(m.group(1))\n            lopen.insert(0, l)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*types\\s*{', re.S).search(data[index:])\n        if m:\n            l = Types()\n            lopen.insert(0, l)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^(\\s*)#[ \\r\\t\\f]*(.*?)\\n', re.S).search(data[index:])\n        if m:\n            c = Comment(m.group(2), inline='\\n' not in m.group(1))\n            if lopen and isinstance(lopen[0], Container):\n                lopen[0].add(c)\n            else:\n                f.add(c) if conf else f.append(c)\n            index += m.end() - 1\n            continue\n\n        m = re.compile(r'^\\s*}', re.S).search(data[index:])\n        if m:\n            if isinstance(lopen[0], Container):\n                c = lopen[0]\n                lopen.pop(0)\n                if lopen and isinstance(lopen[0], Container):\n                    lopen[0].add(c)\n                else:\n                    f.add(c) if conf else f.append(c)\n            index += m.end()\n            continue\n\n        double = r'\\s*\"[^\"]*\"'\n        single = r'\\s*\\'[^\\']*\\''\n        normal = r'\\s*[^;\\s]*'\n        s1 = r'{}|{}|{}'.format(double, single, normal)\n        s = r'^\\s*({})\\s*((?:{})+);'.format(s1, s1)\n        m = re.compile(s, re.S).search(data[index:])\n        if m:\n            k = Key(m.group(1), m.group(2))\n            if lopen and isinstance(lopen[0], (Container, Server)):\n                lopen[0].add(k)\n            else:\n                f.add(k) if conf else f.append(k)\n            index += m.end()\n            continue\n\n        m = re.compile(r'^\\s*(\\S+);', re.S).search(data[index:])\n        if m:\n            k = Key(m.group(1), '')\n            if lopen and isinstance(lopen[0], (Container, Server)):\n                lopen[0].add(k)\n            else:\n                f.add(k) if conf else f.append(k)\n            index += m.end()\n            continue\n\n        break\n\n    return f", "response": "Load an nginx configuration from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dumpf(obj, path):\n    with open(path, 'w') as f:\n        dump(obj, f)\n    return path", "response": "Write an nginx configuration to file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, *args):\n        for x in args:\n            self.children.remove(x)\n        return self.children", "response": "Removes objects from the Conf."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter(self, btype='', name=''):\n        filtered = []\n        for x in self.children:\n            if name and isinstance(x, Key) and x.name == name:\n                filtered.append(x)\n            elif isinstance(x, Container) and x.__class__.__name__ == btype\\\n                    and x.value == name:\n                filtered.append(x)\n            elif not name and btype and x.__class__.__name__ == btype:\n                filtered.append(x)\n        return filtered", "response": "Return child objects of this Conf that satisfy certain criteria."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the entire Conf as nginx config strings.", "response": "def as_strings(self):\n        \"\"\"Return the entire Conf as nginx config strings.\"\"\"\n        ret = []\n        for x in self.children:\n            if isinstance(x, (Key, Comment)):\n                ret.append(x.as_strings)\n            else:\n                for y in x.as_strings:\n                    ret.append(y)\n        if ret:\n            ret[-1] = re.sub('}\\n+$', '}\\n', ret[-1])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, *args):\n        self.children.extend(args)\n        bump_child_depth(self, self._depth)\n        return self.children", "response": "Add objects to the Container s child list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_list(self):\n        return [self.name, self.value, [x.as_list for x in self.children]]", "response": "Return all child objects in nested lists of strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_dict(self):\n        dicts = [x.as_dict for x in self.children]\n        return {'{0} {1}'.format(self.name, self.value): dicts}", "response": "Return all child objects in nested dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the entire Container as nginx config strings.", "response": "def as_strings(self):\n        \"\"\"Return the entire Container as nginx config strings.\"\"\"\n        ret = []\n        container_title = (INDENT * self._depth)\n        container_title += '{0}{1} {{\\n'.format(\n            self.name, (' {0}'.format(self.value) if self.value else '')\n        )\n        ret.append(container_title)\n        for x in self.children:\n            if isinstance(x, Key):\n                ret.append(INDENT + x.as_strings)\n            elif isinstance(x, Comment):\n                if x.inline and len(ret) >= 1:\n                    ret[-1] = ret[-1].rstrip('\\n') + '  ' + x.as_strings\n                else:\n                    ret.append(INDENT + x.as_strings)\n            elif isinstance(x, Container):\n                y = x.as_strings\n                ret.append('\\n' + y[0])\n                for z in y[1:]:\n                    ret.append(INDENT + z)\n            else:\n                y = x.as_strings\n                ret.append(INDENT + y)\n        ret[-1] = re.sub('}\\n+$', '}\\n', ret[-1])\n        ret.append('}\\n\\n')\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn key as nginx config string.", "response": "def as_strings(self):\n        \"\"\"Return key as nginx config string.\"\"\"\n        if self.value == '' or self.value is None:\n            return '{0};\\n'.format(self.name)\n        if '\"' not in self.value and (';' in self.value or '#' in self.value):\n            return '{0} \"{1}\";\\n'.format(self.name, self.value)\n        return '{0} {1};\\n'.format(self.name, self.value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_aws_args(aws_args):\n    if not isinstance(aws_args, dict):\n        raise errors.InvalidConfiguration(\n            'Elastic DocManager config option \"aws\" must be a dict'\n        )\n    old_session_kwargs = dict(\n        region=\"region_name\",\n        access_id=\"aws_access_key_id\",\n        secret_key=\"aws_secret_access_key\",\n    )\n    new_kwargs = {}\n    for arg in aws_args:\n        if arg in old_session_kwargs:\n            new_kwargs[old_session_kwargs[arg]] = aws_args[arg]\n        else:\n            new_kwargs[arg] = aws_args[arg]\n    return new_kwargs", "response": "Convert old style options into arguments to boto3. session. Session."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        if not self._should_auto_commit and not self._should_auto_send:\n            return\n        last_send, last_commit = 0, 0\n        while not self._stopped:\n            if self._should_auto_commit:\n                if last_commit > self._commit_interval:\n                    self._docman.commit()\n                    # commit also sends so reset both\n                    last_send, last_commit = 0, 0\n                    # Give a chance to exit the loop\n                    if self._stopped:\n                        break\n\n            if self._should_auto_send:\n                if last_send > self._send_interval:\n                    self._docman.send_buffered_operations()\n                    last_send = 0\n            time.sleep(self._sleep_interval)\n            last_send += self._sleep_interval\n            last_commit += self._sleep_interval", "response": "Periodically sends buffered operations and commit."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _index_and_mapping(self, namespace):\n        index, doc_type = namespace.split(\".\", 1)\n        return index.lower(), doc_type", "response": "Helper method for getting the index and type from a namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery Elasticsearch for documents in a time range.", "response": "def search(self, start_ts, end_ts):\n        \"\"\"Query Elasticsearch for documents in a time range.\n\n        This method is used to find documents that may be in conflict during\n        a rollback event in MongoDB.\n        \"\"\"\n        return self._stream_search(\n            index=self.meta_index_name,\n            body={\"query\": {\"range\": {\"_ts\": {\"gte\": start_ts, \"lte\": end_ts}}}},\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending buffered requests and refresh all indexes.", "response": "def commit(self):\n        \"\"\"Send buffered requests and refresh all indexes.\"\"\"\n        self.send_buffered_operations()\n        retry_until_ok(self.elastic.indices.refresh, index=\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning which stores sources for insert actions and decide if for update actions and add docs to local buffer.", "response": "def add_upsert(self, action, meta_action, doc_source, update_spec):\n        \"\"\"\n        Function which stores sources for \"insert\" actions\n        and decide if for \"update\" action has to add docs to\n        get source buffer\n        \"\"\"\n\n        # Whenever update_spec is provided to this method\n        # it means that doc source needs to be retrieved\n        # from Elasticsearch. It means also that source\n        # is not stored in local buffer\n        if update_spec:\n            self.bulk_index(action, meta_action)\n\n            # -1 -> to get latest index number\n            # -1 -> to get action instead of meta_action\n            # Update document based on source retrieved from ES\n            self.add_doc_to_update(action, update_spec, len(self.action_buffer) - 2)\n        else:\n            # Insert and update operations provide source\n            # Store it in local buffer and use for comming updates\n            # inside same buffer\n            # add_to_sources will not be called for delete operation\n            # as it does not provide doc_source\n            if doc_source:\n                self.add_to_sources(action, doc_source)\n            self.bulk_index(action, meta_action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_doc_to_update(self, action, update_spec, action_buffer_index):\n\n        doc = {\n            \"_index\": action[\"_index\"],\n            \"_type\": action[\"_type\"],\n            \"_id\": action[\"_id\"],\n        }\n\n        # If get_from_ES == True -> get document's source from Elasticsearch\n        get_from_ES = self.should_get_id(action)\n        self.doc_to_update.append((doc, update_spec, action_buffer_index, get_from_ES))", "response": "Add a document to the update list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef should_get_id(self, action):\n        mapping_ids = self.doc_to_get.setdefault(action[\"_index\"], {}).setdefault(\n            action[\"_type\"], set()\n        )\n        if action[\"_id\"] in mapping_ids:\n            # There is an update on this id already\n            return False\n        else:\n            mapping_ids.add(action[\"_id\"])\n            return True", "response": "Returns True if the action should be marked as ID False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget document sources using MGET elasticsearch API", "response": "def get_docs_sources_from_ES(self):\n        \"\"\"Get document sources using MGET elasticsearch API\"\"\"\n        docs = [doc for doc, _, _, get_from_ES in self.doc_to_update if get_from_ES]\n        if docs:\n            documents = self.docman.elastic.mget(body={\"docs\": docs}, realtime=True)\n            return iter(documents[\"docs\"])\n        else:\n            return iter([])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates local sources based on response from Elasticsearch", "response": "def update_sources(self):\n        \"\"\"Update local sources based on response from Elasticsearch\"\"\"\n        ES_documents = self.get_docs_sources_from_ES()\n\n        for doc, update_spec, action_buffer_index, get_from_ES in self.doc_to_update:\n            if get_from_ES:\n                # Update source based on response from ES\n                ES_doc = next(ES_documents)\n                if ES_doc[\"found\"]:\n                    source = ES_doc[\"_source\"]\n                else:\n                    # Document not found in elasticsearch,\n                    # Seems like something went wrong during replication\n                    LOG.error(\n                        \"mGET: Document id: %s has not been found \"\n                        \"in Elasticsearch. Due to that \"\n                        \"following update failed: %s\",\n                        doc[\"_id\"],\n                        update_spec,\n                    )\n                    self.reset_action(action_buffer_index)\n                    continue\n            else:\n                # Get source stored locally before applying update\n                # as it is up-to-date\n                source = self.get_from_sources(doc[\"_index\"], doc[\"_type\"], doc[\"_id\"])\n                if not source:\n                    LOG.error(\n                        \"mGET: Document id: %s has not been found \"\n                        \"in local sources. Due to that following \"\n                        \"update failed: %s\",\n                        doc[\"_id\"],\n                        update_spec,\n                    )\n                    self.reset_action(action_buffer_index)\n                    continue\n\n            updated = self.docman.apply_update(source, update_spec)\n\n            # Remove _id field from source\n            if \"_id\" in updated:\n                del updated[\"_id\"]\n\n            # Everytime update locally stored sources to keep them up-to-date\n            self.add_to_sources(doc, updated)\n\n            self.action_buffer[action_buffer_index][\n                \"_source\"\n            ] = self.docman._formatter.format_document(updated)\n\n        # Remove empty actions if there were errors\n        self.action_buffer = [\n            each_action for each_action in self.action_buffer if each_action\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_from_sources(self, index, doc_type, document_id):\n        return self.sources.get(index, {}).get(doc_type, {}).get(document_id, {})", "response": "Get source stored locally"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndoing clean - up before returning buffer", "response": "def clean_up(self):\n        \"\"\"Do clean-up before returning buffer\"\"\"\n        self.action_buffer = []\n        self.sources = {}\n        self.doc_to_get = {}\n        self.doc_to_update = []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the buffer which needs to be bulked to elasticsearch", "response": "def get_buffer(self):\n        \"\"\"Get buffer which needs to be bulked to elasticsearch\"\"\"\n\n        # Get sources for documents which are in Elasticsearch\n        # and they are not in local buffer\n        if self.doc_to_update:\n            self.update_sources()\n\n        ES_buffer = self.action_buffer\n        self.clean_up()\n        return ES_buffer"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_scan_parameters(self, scan_type=ScanType.ACTIVE, interval_ms=10, window_ms=10,\n                            address_type=BluetoothAddressType.RANDOM, filter_type=ScanFilter.ALL):\n        \"\"\"\"sets the le scan parameters\n\n        Args:\n            scan_type: ScanType.(PASSIVE|ACTIVE)\n            interval: ms (as float) between scans (valid range 2.5ms - 10240ms)\n                ..note:: when interval and window are equal, the scan\n                    runs continuos\n            window: ms (as float) scan duration (valid range 2.5ms - 10240ms)\n            address_type: Bluetooth address type BluetoothAddressType.(PUBLIC|RANDOM)\n                * PUBLIC = use device MAC address\n                * RANDOM = generate a random MAC address and use that\n            filter: ScanFilter.(ALL|WHITELIST_ONLY) only ALL is supported, which will\n                return all fetched bluetooth packets (WHITELIST_ONLY is not supported,\n                because OCF_LE_ADD_DEVICE_TO_WHITE_LIST command is not implemented)\n\n        Raises:\n            ValueError: A value had an unexpected format or was not in range\n        \"\"\"\n        interval_fractions = interval_ms / MS_FRACTION_DIVIDER\n        if interval_fractions < 0x0004 or interval_fractions > 0x4000:\n            raise ValueError(\n                \"Invalid interval given {}, must be in range of 2.5ms to 10240ms!\".format(\n                    interval_fractions))\n        window_fractions = window_ms / MS_FRACTION_DIVIDER\n        if window_fractions < 0x0004 or window_fractions > 0x4000:\n            raise ValueError(\n                \"Invalid window given {}, must be in range of 2.5ms to 10240ms!\".format(\n                    window_fractions))\n\n        interval_fractions, window_fractions = int(interval_fractions), int(window_fractions)\n\n        scan_parameter_pkg = struct.pack(\n            \">BHHBB\",\n            scan_type,\n            interval_fractions,\n            window_fractions,\n            address_type,\n            filter_type)\n        self.bluez.hci_send_cmd(self.socket, OGF_LE_CTL, OCF_LE_SET_SCAN_PARAMETERS,\n                                scan_parameter_pkg)", "response": "sets the le scan parameters in the current bluetooth session"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toggle_scan(self, enable, filter_duplicates=False):\n        command = struct.pack(\">BB\", enable, filter_duplicates)\n        self.bluez.hci_send_cmd(self.socket, OGF_LE_CTL, OCF_LE_SET_SCAN_ENABLE, command)", "response": "Enables or disables BLE scanning for the given ISO - 8601 ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the packet and call callback if one of the filters matches.", "response": "def process_packet(self, pkt):\n        \"\"\"Parse the packet and call callback if one of the filters matches.\"\"\"\n\n        # check if this could be a valid packet before parsing\n        # this reduces the CPU load significantly\n        if not ( \\\n            ((self.mode & ScannerMode.MODE_IBEACON) and (pkt[19:23] == b\"\\x4c\\x00\\x02\\x15\")) or \\\n            ((self.mode & ScannerMode.MODE_EDDYSTONE) and (pkt[19:21] == b\"\\xaa\\xfe\")) or \\\n            ((self.mode & ScannerMode.MODE_ESTIMOTE) and (pkt[19:21] == b\"\\x9a\\xfe\"))):\n            return\n\n        bt_addr = bt_addr_to_string(pkt[7:13])\n        rssi = bin_to_int(pkt[-1])\n        # strip bluetooth address and parse packet\n        packet = parse_packet(pkt[14:-1])\n\n        # return if packet was not an beacon advertisement\n        if not packet:\n            return\n\n        # we need to remeber which eddystone beacon has which bt address\n        # because the TLM and URL frames do not contain the namespace and instance\n        self.save_bt_addr(packet, bt_addr)\n        # properties holds the identifying information for a beacon\n        # e.g. instance and namespace for eddystone; uuid, major, minor for iBeacon\n        properties = self.get_properties(packet, bt_addr)\n\n        if self.device_filter is None and self.packet_filter is None:\n            # no filters selected\n            self.callback(bt_addr, rssi, packet, properties)\n\n        elif self.device_filter is None:\n            # filter by packet type\n            if is_one_of(packet, self.packet_filter):\n                self.callback(bt_addr, rssi, packet, properties)\n        else:\n            # filter by device and packet type\n            if self.packet_filter and not is_one_of(packet, self.packet_filter):\n                # return if packet filter does not match\n                return\n\n            # iterate over filters and call .matches() on each\n            for filtr in self.device_filter:\n                if isinstance(filtr, BtAddrFilter):\n                    if filtr.matches({'bt_addr':bt_addr}):\n                        self.callback(bt_addr, rssi, packet, properties)\n                        return\n\n                elif filtr.matches(properties):\n                    self.callback(bt_addr, rssi, packet, properties)\n                    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_bt_addr(self, packet, bt_addr):\n        if isinstance(packet, EddystoneUIDFrame):\n            # remove out old mapping\n            new_mappings = [m for m in self.eddystone_mappings if m[0] != bt_addr]\n            new_mappings.append((bt_addr, packet.properties))\n            self.eddystone_mappings = new_mappings", "response": "Save the bt_addr of the EddystoneUIDFrame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting properties of beacon depending on type.", "response": "def get_properties(self, packet, bt_addr):\n        \"\"\"Get properties of beacon depending on type.\"\"\"\n        if is_one_of(packet, [EddystoneTLMFrame, EddystoneURLFrame, \\\n                              EddystoneEncryptedTLMFrame, EddystoneEIDFrame]):\n            # here we retrieve the namespace and instance which corresponds to the\n            # eddystone beacon with this bt address\n            return self.properties_from_mapping(bt_addr)\n        else:\n            return packet.properties"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef properties_from_mapping(self, bt_addr):\n        for addr, properties in self.eddystone_mappings:\n            if addr == bt_addr:\n                return properties\n        return None", "response": "Retrieve properties ( namespace instance ) for the specified bt address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsignaling runner to stop and join thread.", "response": "def terminate(self):\n        \"\"\"Signal runner to stop and join thread.\"\"\"\n        self.toggle_scan(False)\n        self.keep_going = False\n        self.join()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an array of binary data to the iBeacon uuid format.", "response": "def data_to_uuid(data):\n    \"\"\"Convert an array of binary data to the iBeacon uuid format.\"\"\"\n    string = data_to_hexstring(data)\n    return string[0:8]+'-'+string[8:12]+'-'+string[12:16]+'-'+string[16:20]+'-'+string[20:32]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bt_addr_to_string(addr):\n    addr_str = array.array('B', addr)\n    addr_str.reverse()\n    hex_str = hexlify(addr_str.tostring()).decode('ascii')\n    # insert \":\" seperator between the bytes\n    return ':'.join(a+b for a, b in zip(hex_str[::2], hex_str[1::2]))", "response": "Convert a binary string to the hex representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning true iff obj is an instance of one of the types.", "response": "def is_one_of(obj, types):\n    \"\"\"Return true iff obj is an instance of one of the types.\"\"\"\n    for type_ in types:\n        if isinstance(obj, type_):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if class is one of the packet types.", "response": "def is_packet_type(cls):\n    \"\"\"Check if class is one the packet types.\"\"\"\n    from .packet_types import EddystoneUIDFrame, EddystoneURLFrame, \\\n                              EddystoneEncryptedTLMFrame, EddystoneTLMFrame, \\\n                              EddystoneEIDFrame, IBeaconAdvertisement, \\\n                              EstimoteTelemetryFrameA, EstimoteTelemetryFrameB\n    return (cls in [EddystoneURLFrame, EddystoneUIDFrame, EddystoneEncryptedTLMFrame, \\\n                    EddystoneTLMFrame, EddystoneEIDFrame, IBeaconAdvertisement, \\\n                    EstimoteTelemetryFrameA, EstimoteTelemetryFrameB])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a one element byte string to signed int for python 2 support.", "response": "def bin_to_int(string):\n    \"\"\"Convert a one element byte string to signed int for python 2 support.\"\"\"\n    if isinstance(string, str):\n        return struct.unpack(\"b\", string)[0]\n    else:\n        return struct.unpack(\"b\", bytes([string]))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_mode(device_filter):\n    from .device_filters import IBeaconFilter, EddystoneFilter, BtAddrFilter, EstimoteFilter\n    if device_filter is None or len(device_filter) == 0:\n        return ScannerMode.MODE_ALL\n\n    mode = ScannerMode.MODE_NONE\n    for filtr in device_filter:\n        if isinstance(filtr, IBeaconFilter):\n            mode |= ScannerMode.MODE_IBEACON\n        elif isinstance(filtr, EddystoneFilter):\n            mode |= ScannerMode.MODE_EDDYSTONE\n        elif isinstance(filtr, EstimoteFilter):\n            mode |= ScannerMode.MODE_ESTIMOTE\n        elif isinstance(filtr, BtAddrFilter):\n            mode |= ScannerMode.MODE_ALL\n            break\n\n    return mode", "response": "Determine which beacons the scanner should look for."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the filter matches the supplied properties.", "response": "def matches(self, filter_props):\n        \"\"\"Check if the filter matches the supplied properties.\"\"\"\n        if filter_props is None:\n            return False\n\n        found_one = False\n        for key, value in filter_props.items():\n            if key in self.properties and value != self.properties[key]:\n                return False\n            elif key in self.properties and value == self.properties[key]:\n                found_one = True\n\n        return found_one"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_packet(packet):\n    frame = parse_ltv_packet(packet)\n    if frame is None:\n        frame = parse_ibeacon_packet(packet)\n    return frame", "response": "Parse a beacon advertisement packet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a tag - length - value style beacon packet.", "response": "def parse_ltv_packet(packet):\n    \"\"\"Parse a tag-length-value style beacon packet.\"\"\"\n    try:\n        frame = LTVFrame.parse(packet)\n        for ltv in frame:\n            if ltv['type'] == SERVICE_DATA_TYPE:\n                data = ltv['value']\n\n                if data[\"service_identifier\"] == EDDYSTONE_UUID:\n                    return parse_eddystone_service_data(data)\n\n                elif data[\"service_identifier\"] == ESTIMOTE_UUID:\n                    return parse_estimote_service_data(data)\n\n    except ConstructError:\n        return None\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse Eddystone service data.", "response": "def parse_eddystone_service_data(data):\n    \"\"\"Parse Eddystone service data.\"\"\"\n    if data['frame_type'] == EDDYSTONE_UID_FRAME:\n        return EddystoneUIDFrame(data['frame'])\n\n    elif data['frame_type'] == EDDYSTONE_TLM_FRAME:\n        if data['frame']['tlm_version'] == EDDYSTONE_TLM_ENCRYPTED:\n            return EddystoneEncryptedTLMFrame(data['frame']['data'])\n        elif data['frame']['tlm_version'] == EDDYSTONE_TLM_UNENCRYPTED:\n            return EddystoneTLMFrame(data['frame']['data'])\n\n    elif data['frame_type'] == EDDYSTONE_URL_FRAME:\n        return EddystoneURLFrame(data['frame'])\n\n    elif data['frame_type'] == EDDYSTONE_EID_FRAME:\n        return EddystoneEIDFrame(data['frame'])\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_estimote_service_data(data):\n    if data['frame_type'] & 0xF == ESTIMOTE_TELEMETRY_FRAME:\n        protocol_version = (data['frame_type'] & 0xF0) >> 4\n        if data['frame']['subframe_type'] == ESTIMOTE_TELEMETRY_SUBFRAME_A:\n            return EstimoteTelemetryFrameA(data['frame'], protocol_version)\n        elif data['frame']['subframe_type'] == ESTIMOTE_TELEMETRY_SUBFRAME_B:\n            return EstimoteTelemetryFrameB(data['frame'], protocol_version)\n    return None", "response": "Parse Estimote service data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_motion_state(val):\n        number = val & 0b00111111\n        unit = (val & 0b11000000) >> 6\n        if unit == 1:\n            number *= 60 # minutes\n        elif unit == 2:\n            number *= 60 * 60 # hours\n        elif unit == 3 and number < 32:\n            number *= 60 * 60 * 24 # days\n        elif unit == 3:\n            number -= 32\n            number *= 60 * 60 * 24 * 7 # weeks\n        return number", "response": "Convert motion state byte to seconds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscan a PIL Image for a list of code types.", "response": "def scan_codes(code_types, image):\n    \"\"\"\n    Get *code_type* codes from a PIL Image.\n\n    *code_type* can be any of zbar supported code type [#zbar_symbologies]_:\n\n    - **EAN/UPC**: EAN-13 (`ean13`), UPC-A (`upca`), EAN-8 (`ean8`) and UPC-E (`upce`)\n    - **Linear barcode**: Code 128 (`code128`), Code 93 (`code93`), Code 39 (`code39`), Interleaved 2 of 5 (`i25`),\n      DataBar (`databar`) and DataBar Expanded (`databar-exp`)\n    - **2D**: QR Code (`qrcode`)\n    - **Undocumented**: `ean5`, `ean2`, `composite`, `isbn13`, `isbn10`, `codabar`, `pdf417`\n\n    .. [#zbar_symbologies] http://zbar.sourceforge.net/iphone/userguide/symbologies.html\n\n    Args:\n        code_types (list(str)): Code type(s) to search (see ``zbarlight.Symbologies`` for supported values).\n        image (PIL.Image.Image): Image to scan\n\n    returns:\n        A list of *code_type* code values or None\n\n    \"\"\"\n    if isinstance(code_types, str):\n        code_types = [code_types]\n        warnings.warn(\n            'Using a str for code_types is deprecated, please use a list of str instead',\n            DeprecationWarning,\n        )\n\n    # Translate symbologies\n    symbologies = [\n        Symbologies.get(code_type.upper())\n        for code_type in set(code_types)\n    ]\n\n    # Check that all symbologies are known\n    if None in symbologies:\n        bad_code_types = [code_type for code_type in code_types if code_type.upper() not in Symbologies]\n        raise UnknownSymbologieError('Unknown Symbologies: %s' % bad_code_types)\n\n    # Convert the image to be used by c-extension\n    if not Image.isImageType(image):\n        raise RuntimeError('Bad or unknown image format')\n    converted_image = image.convert('L')  # Convert image to gray scale (8 bits per pixel).\n    raw = converted_image.tobytes()  # Get image data.\n    width, height = converted_image.size  # Get image size.\n\n    return zbar_code_scanner(symbologies, raw, width, height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy_image_on_background(image, color=WHITE):\n    background = Image.new(\"RGB\", image.size, color)\n    background.paste(image, mask=image.split()[3])\n    return background", "response": "Copy an image on a background color."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef monkey_patch(cls):\n        on_read_the_docs = os.environ.get('READTHEDOCS', False)\n        if on_read_the_docs:\n            sys.modules['zbarlight._zbarlight'] = cls", "response": "Monkey path zbarlight C extension on Read The Docs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fill(framebuf, color):\n        if color:\n            fill = 0xFF\n        else:\n            fill = 0x00\n        for i in range(len(framebuf.buf)):\n            framebuf.buf[i] = fill", "response": "completely fill the buffer with a color"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw a rectangle at the given location size and color. The fill_rect method draws the rectangle both the outline and interior.", "response": "def fill_rect(framebuf, x, y, width, height, color):\n        \"\"\"Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws\n        both the outline and interior.\"\"\"\n        # pylint: disable=too-many-arguments\n        for _x in range(x, x+width):\n            offset = 7 - _x & 0x07\n            for _y in range(y, y+height):\n                index = (_y * framebuf.stride + _x) // 8\n                framebuf.buf[index] = (framebuf.buf[index] & ~(0x01 << offset)) \\\n                                      | ((color != 0) << offset)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting a given pixel to a color.", "response": "def set_pixel(framebuf, x, y, color):\n        \"\"\"Set a given pixel to a color.\"\"\"\n        index = (y >> 3) * framebuf.stride + x\n        offset = y & 0x07\n        framebuf.buf[index] = (framebuf.buf[index] & ~(0x01 << offset)) | ((color != 0) << offset)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the color of a given pixel", "response": "def get_pixel(framebuf, x, y):\n        \"\"\"Get the color of a given pixel\"\"\"\n        index = (y >> 3) * framebuf.stride + x\n        offset = y & 0x07\n        return (framebuf.buf[index] >> offset) & 0x01"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing a rectangle at the given location size and color. The fill_rect method draws the rectangle both the outline and interior.", "response": "def fill_rect(framebuf, x, y, width, height, color):\n        \"\"\"Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws\n        both the outline and interior.\"\"\"\n        # pylint: disable=too-many-arguments\n        while height > 0:\n            index = (y >> 3) * framebuf.stride + x\n            offset = y & 0x07\n            for w_w in range(width):\n                framebuf.buf[index + w_w] = (framebuf.buf[index + w_w] & ~(0x01 << offset)) |\\\n                                           ((color != 0) << offset)\n            y += 1\n            height -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw a rectangle at the given location size and color. The fill_rect method draws both the outline and interior.", "response": "def fill_rect(self, x, y, width, height, color):\n        \"\"\"Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws\n        both the outline and interior.\"\"\"\n        # pylint: disable=too-many-arguments, too-many-boolean-expressions\n        self.rect(x, y, width, height, color, fill=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the color value of the specified pixel.", "response": "def pixel(self, x, y, color=None):\n        \"\"\"If ``color`` is not given, get the color value of the specified pixel. If ``color`` is\n        given, set the specified pixel to the given color.\"\"\"\n        if self.rotation == 1:\n            x, y = y, x\n            x = self.width - x - 1\n        if self.rotation == 2:\n            x = self.width - x - 1\n            y = self.height - y - 1\n        if self.rotation == 3:\n            x, y = y, x\n            y = self.height - y - 1\n\n        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n            return None\n        if color is None:\n            return self.format.get_pixel(self, x, y)\n        self.format.set_pixel(self, x, y, color)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hline(self, x, y, width, color):\n        self.rect(x, y, width, 1, color, fill=True)", "response": "Draw a horizontal line up to a given length."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing a vertical line up to a given length.", "response": "def vline(self, x, y, height, color):\n        \"\"\"Draw a vertical line up to a given length.\"\"\"\n        self.rect(x, y, 1, height, color, fill=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rect(self, x, y, width, height, color, *, fill=False):\n        # pylint: disable=too-many-arguments\n        if self.rotation == 1:\n            x, y = y, x\n            width, height = height, width\n            x = self.width - x - width\n        if self.rotation == 2:\n            x = self.width - x - width\n            y = self.height - y - height\n        if self.rotation == 3:\n            x, y = y, x\n            width, height = height, width\n            y = self.height - y - height\n\n        # pylint: disable=too-many-boolean-expressions\n        if width < 1 or height < 1 or (x + width) <= 0 or (y + height) <= 0 or \\\n                y >= self.height or x >= self.width:\n            return\n        x_end = min(self.width-1, x + width-1)\n        y_end = min(self.height-1, y + height-1)\n        x = max(x, 0)\n        y = max(y, 0)\n        if fill:\n            self.format.fill_rect(self, x, y, x_end-x+1, y_end-y+1, color)\n        else:\n            self.format.fill_rect(self, x, y, x_end-x+1, 1, color)\n            self.format.fill_rect(self, x, y, 1, y_end-y+1, color)\n            self.format.fill_rect(self, x, y_end, x_end-x+1, 1, color)\n            self.format.fill_rect(self, x_end, y, 1, y_end-y+1, color)", "response": "Draw a rectangle at the given location size and color. The rect method draws only\n            a 1 pixel outline."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscrolls the image in x and y direction", "response": "def scroll(self, delta_x, delta_y):\n        \"\"\"shifts framebuf in x and y direction\"\"\"\n        if delta_x < 0:\n            shift_x = 0\n            xend = self.width + delta_x\n            dt_x = 1\n        else:\n            shift_x = self.width - 1\n            xend = delta_x - 1\n            dt_x = -1\n        if delta_y < 0:\n            y = 0\n            yend = self.height + delta_y\n            dt_y = 1\n        else:\n            y = self.height - 1\n            yend = delta_y - 1\n            dt_y = -1\n        while y != yend:\n            x = shift_x\n            while x != xend:\n                self.format.set_pixel(\n                    self, x, y, self.format.get_pixel(self, x - delta_x, y - delta_y))\n                x += dt_x\n            y += dt_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef text(self, string, x, y, color, *,\n             font_name=\"font5x8.bin\"):\n        \"\"\"text is not yet implemented\"\"\"\n        if not self._font or self._font.font_name != font_name:\n            # load the font!\n            self._font = BitmapFont()\n        w = self._font.font_width\n        for i, char in enumerate(string):\n            self._font.draw_char(char,\n                                 x + (i * (w + 1)),\n                                 y, self, color)", "response": "draw text from a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets buffer to value of Python Imaging Library image.", "response": "def image(self, img):\n        \"\"\"Set buffer to value of Python Imaging Library image.  The image should\n        be in 1 bit mode and a size equal to the display size.\"\"\"\n        if img.mode != '1':\n            raise ValueError('Image must be in mode 1.')\n        imwidth, imheight = img.size\n        if imwidth != self.width or imheight != self.height:\n            raise ValueError('Image must be same dimensions as display ({0}x{1}).' \\\n                .format(self.width, self.height))\n        # Grab all the pixels from the image, faster than getpixel.\n        pixels = img.load()\n        # Clear buffer\n        for i in range(len(self.buf)):\n            self.buf[i] = 0\n        # Iterate through the pixels\n        for x in range(self.width):       # yes this double loop is slow,\n            for y in range(self.height):  #  but these displays are small!\n                if pixels[(x, y)]:\n                    self.pixel(x, y, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing one character at position x y to a framebuffer in a given color.", "response": "def draw_char(self, char, x, y, framebuffer, color):\n        # pylint: disable=too-many-arguments\n        \"\"\"Draw one character at position (x,y) to a framebuffer in a given color\"\"\"\n        # Don't draw the character if it will be clipped off the visible area.\n        #if x < -self.font_width or x >= framebuffer.width or \\\n        #   y < -self.font_height or y >= framebuffer.height:\n        #    return\n        # Go through each column of the character.\n        for char_x in range(self.font_width):\n            # Grab the byte for the current column of font data.\n            self._font.seek(2 + (ord(char) * self.font_width) + char_x)\n            try:\n                line = struct.unpack('B', self._font.read(1))[0]\n            except RuntimeError:\n                continue # maybe character isnt there? go to next\n            # Go through each row in the column byte.\n            for char_y in range(self.font_height):\n                # Draw a pixel for each bit that's flipped on.\n                if (line >> char_y) & 0x1:\n                    framebuffer.pixel(x + char_x, y + char_y, color)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_auth_token_from_request(self, auth_header):\n        if not auth_header:\n            raise falcon.HTTPUnauthorized(\n                description='Missing Authorization Header')\n\n        parts = auth_header.split()\n\n        if parts[0].lower() != self.auth_header_prefix.lower():\n            raise falcon.HTTPUnauthorized(\n                description='Invalid Authorization Header: '\n                            'Must start with {0}'.format(self.auth_header_prefix))\n\n        elif len(parts) == 1:\n            raise falcon.HTTPUnauthorized(\n                description='Invalid Authorization Header: Token Missing')\n        elif len(parts) > 2:\n            raise falcon.HTTPUnauthorized(\n                description='Invalid Authorization Header: Contains extra content')\n\n        return parts[1]", "response": "Parses and returns Auth token from the request header. Raises falcon. HTTPUnauthoried exception with proper error message"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_auth_header(self, user_payload):\n        auth_token = self.get_auth_token(user_payload)\n        return '{auth_header_prefix} {auth_token}'.format(\n            auth_header_prefix=self.auth_header_prefix, auth_token=auth_token\n        )", "response": "Returns the value for the authorization header for the user_payload"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef authenticate(self, req, resp, resource):\n        payload = self._decode_jwt_token(req)\n        user = self.user_loader(payload)\n        if not user:\n            raise falcon.HTTPUnauthorized(\n                description='Invalid JWT Credentials')\n\n        return user", "response": "Extract auth token from request header decode jwt token and return user object if successful or raise falcon. HTTPUnauthoried exception"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_auth_token(self, user_payload):\n        now = datetime.utcnow()\n        payload = {\n            'user': user_payload\n        }\n        if 'iat' in self.verify_claims:\n            payload['iat'] = now\n\n        if 'nbf' in self.verify_claims:\n            payload['nbf'] = now + self.leeway\n\n        if 'exp' in self.verify_claims:\n            payload['exp'] = now + self.expiration_delta\n\n        if self.audience is not None:\n            payload['aud'] = self.audience\n\n        if self.issuer is not None:\n            payload['iss'] = self.issuer\n\n        return jwt.encode(\n            payload,\n            self.secret_key,\n            algorithm=self.algorithm,\n            json_encoder=ExtendedJSONEncoder).decode('utf-8')", "response": "Create a JWT authentication token from user_payload."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a user object if successful or raises falcon. HTTPUnauthoried exception if unsuccessful.", "response": "def authenticate(self, req, resp, resource):\n        \"\"\"\n        Extract basic auth token from request `authorization` header,  deocode the\n        token, verifies the username/password and return either a ``user``\n        object if successful else raise an `falcon.HTTPUnauthoried exception`\n        \"\"\"\n        username, password = self._extract_credentials(req)\n        user = self.user_loader(username, password)\n        if not user:\n            raise falcon.HTTPUnauthorized(\n                description='Invalid Username/Password')\n\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract username password from the user_payload and encodes the credentials username : password in base64 form and returns the auth token", "response": "def get_auth_token(self, user_payload):\n        \"\"\"\n        Extracts username, password from the `user_payload` and encode the\n        credentials `username:password` in `base64` form\n        \"\"\"\n        username = user_payload.get('username') or None\n        password = user_payload.get('password') or None\n\n        if not username or not password:\n            raise ValueError('`user_payload` must contain both username and password')\n\n        token = '{username}:{password}'.format(\n            username=username, password=password).encode('utf-8')\n\n        token_b64 = base64.b64encode(token).decode('utf-8', 'ignore')\n\n        return '{auth_header_prefix} {token_b64}'.format(\n            auth_header_prefix=self.auth_header_prefix, token_b64=token_b64)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_auth_token(self, user_payload):\n        token = user_payload.get('token') or None\n        if not token:\n            raise ValueError('`user_payload` must provide api token')\n\n        return '{auth_header_prefix} {token}'.format(\n            auth_header_prefix=self.auth_header_prefix, token=token)", "response": "Extracts the auth token from the user_payload and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_auth_token_from_request(self, auth_header):\n        if not auth_header:\n            raise falcon.HTTPUnauthorized(\n                description='Missing Authorization Header')\n\n        try:\n            auth_header_prefix, _ = auth_header.split(' ', 1)\n        except ValueError:\n            raise falcon.HTTPUnauthorized(\n                description='Invalid Authorization Header: Missing Scheme or Parameters')\n\n        if auth_header_prefix.lower() != self.auth_header_prefix.lower():\n            raise falcon.HTTPUnauthorized(\n                description='Invalid Authorization Header: '\n                            'Must start with {0}'.format(self.auth_header_prefix))\n\n        return auth_header", "response": "Parses and returns the Hawk Authorization header if it is present and well - formed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _apply_base_theme(app):\n\n    if QT_VERSION < (5,):\n        app.setStyle('plastique')\n    else:\n        app.setStyle('Fusion')\n\n    with open(_STYLESHEET) as stylesheet:\n        app.setStyleSheet(stylesheet.read())", "response": "Apply base theme to the application."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies Dark Theme to the Qt application instance.", "response": "def dark(app):\n    \"\"\" Apply Dark Theme to the Qt application instance.\n\n        Args:\n            app (QApplication): QApplication instance.\n    \"\"\"\n\n    _apply_base_theme(app)\n\n    darkPalette = QPalette()\n\n    # base\n    darkPalette.setColor(QPalette.WindowText, QColor(180, 180, 180))\n    darkPalette.setColor(QPalette.Button, QColor(53, 53, 53))\n    darkPalette.setColor(QPalette.Light, QColor(180, 180, 180))\n    darkPalette.setColor(QPalette.Midlight, QColor(90, 90, 90))\n    darkPalette.setColor(QPalette.Dark, QColor(35, 35, 35))\n    darkPalette.setColor(QPalette.Text, QColor(180, 180, 180))\n    darkPalette.setColor(QPalette.BrightText, QColor(180, 180, 180))\n    darkPalette.setColor(QPalette.ButtonText, QColor(180, 180, 180))\n    darkPalette.setColor(QPalette.Base, QColor(42, 42, 42))\n    darkPalette.setColor(QPalette.Window, QColor(53, 53, 53))\n    darkPalette.setColor(QPalette.Shadow, QColor(20, 20, 20))\n    darkPalette.setColor(QPalette.Highlight, QColor(42, 130, 218))\n    darkPalette.setColor(QPalette.HighlightedText, QColor(180, 180, 180))\n    darkPalette.setColor(QPalette.Link, QColor(56, 252, 196))\n    darkPalette.setColor(QPalette.AlternateBase, QColor(66, 66, 66))\n    darkPalette.setColor(QPalette.ToolTipBase, QColor(53, 53, 53))\n    darkPalette.setColor(QPalette.ToolTipText, QColor(180, 180, 180))\n\n    # disabled\n    darkPalette.setColor(QPalette.Disabled, QPalette.WindowText,\n                         QColor(127, 127, 127))\n    darkPalette.setColor(QPalette.Disabled, QPalette.Text,\n                         QColor(127, 127, 127))\n    darkPalette.setColor(QPalette.Disabled, QPalette.ButtonText,\n                         QColor(127, 127, 127))\n    darkPalette.setColor(QPalette.Disabled, QPalette.Highlight,\n                         QColor(80, 80, 80))\n    darkPalette.setColor(QPalette.Disabled, QPalette.HighlightedText,\n                         QColor(127, 127, 127))\n\n    app.setPalette(darkPalette)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbasing on http://code. activestate. com / recipes / 502261 - python - distutils - pkg - config/#c2", "response": "def pkgconfig(*packages, **kw):\n    \"\"\"Based on http://code.activestate.com/recipes/502261-python-distutils-pkg-config/#c2\"\"\"\n\n    flag_map = {'-I': 'include_dirs', '-L': 'library_dirs', '-l': 'libraries'}\n    output = sp.Popen([\"pkg-config\", \"--libs\", \"--cflags\"] + list(packages),\n                      stdout=sp.PIPE).communicate()[0]\n    if six.PY3:\n        output = output.decode('utf8')\n    for token in output.split():\n        if token[:2] in flag_map:\n            kw.setdefault(flag_map.get(token[:2]), []).append(token[2:])\n        else:\n            kw.setdefault('extra_compile_args', []).append(token)\n\n    kw['include_dirs'] += [np.get_include()]\n\n    return kw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inheritance_diagram_directive(name, arguments, options, content, lineno,\n                                  content_offset, block_text, state,\n                                  state_machine):\n    \"\"\"\n    Run when the inheritance_diagram directive is first encountered.\n    \"\"\"\n    node = inheritance_diagram()\n\n    class_names = arguments\n\n    # Create a graph starting with the list of classes\n    graph = InheritanceGraph(class_names)\n\n    # Create xref nodes for each target of the graph's image map and\n    # add them to the doc tree so that Sphinx can resolve the\n    # references to real URLs later.  These nodes will eventually be\n    # removed from the doctree after we're done with them.\n    for name in graph.get_all_class_names():\n        refnodes, x = xfileref_role(\n            'class', ':class:`%s`' % name, name, 0, state)\n        node.extend(refnodes)\n    # Store the graph object so we can use it to generate the\n    # dot file later\n    node['graph'] = graph\n    # Store the original content for use as a hash\n    node['parts'] = options.get('parts', 0)\n    node['content'] = \" \".join(class_names)\n    return [node]", "response": "This function is called when the inheritance_diagram directive is first encountered."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noutput the graph for HTML.", "response": "def html_output_graph(self, node):\n    \"\"\"\n    Output the graph for HTML.  This will insert a PNG with clickable\n    image map.\n    \"\"\"\n    graph = node['graph']\n    parts = node['parts']\n\n    graph_hash = get_graph_hash(node)\n    name = \"inheritance%s\" % graph_hash\n    path = '_images'\n    dest_path = os.path.join(setup.app.builder.outdir, path)\n    if not os.path.exists(dest_path):\n        os.makedirs(dest_path)\n    png_path = os.path.join(dest_path, name + \".png\")\n    path = setup.app.builder.imgpath\n\n    # Create a mapping from fully-qualified class names to URLs.\n    urls = {}\n    for child in node:\n        if child.get('refuri') is not None:\n            urls[child['reftitle']] = child.get('refuri')\n        elif child.get('refid') is not None:\n            urls[child['reftitle']] = '#' + child.get('refid')\n\n    # These arguments to dot will save a PNG file to disk and write\n    # an HTML image map to stdout.\n    image_map = graph.run_dot(['-Tpng', '-o%s' % png_path, '-Tcmapx'],\n                              name, parts, urls)\n    return ('<img src=\"%s/%s.png\" usemap=\"#%s\" class=\"inheritance\"/>%s' %\n            (path, name, name, image_map))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef latex_output_graph(self, node):\n    graph = node['graph']\n    parts = node['parts']\n\n    graph_hash = get_graph_hash(node)\n    name = \"inheritance%s\" % graph_hash\n    dest_path = os.path.abspath(os.path.join(setup.app.builder.outdir, '_images'))\n    if not os.path.exists(dest_path):\n        os.makedirs(dest_path)\n    pdf_path = os.path.abspath(os.path.join(dest_path, name + \".pdf\"))\n\n    graph.run_dot(['-Tpdf', '-o%s' % pdf_path],\n                  name, parts, graph_options={'size': '\"6.0,6.0\"'})\n    return '\\n\\\\includegraphics{%s}\\n\\n' % pdf_path", "response": "This will output the LaTeX graph for LaTeX."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_inheritance_diagram(inner_func):\n    def visitor(self, node):\n        try:\n            content = inner_func(self, node)\n        except DotException, e:\n            # Insert the exception as a warning in the document\n            warning = self.document.reporter.warning(str(e), line=node.line)\n            warning.parent = node\n            node.children = [warning]\n        else:\n            source = self.document.attributes['source']\n            self.body.append(content)\n            node.children = []\n    return visitor", "response": "A function that returns a function that will be used to visit the inheritance diagram."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting a class or module using its fully - qualified name.", "response": "def _import_class_or_module(self, name):\n        \"\"\"\n        Import a class using its fully-qualified *name*.\n        \"\"\"\n        try:\n            path, base = self.py_sig_re.match(name).groups()\n        except:\n            raise ValueError(\n                \"Invalid class or module '%s' specified for inheritance diagram\" % name)\n        fullname = (path or '') + base\n        path = (path and path.rstrip('.'))\n        if not path:\n            path = base\n        try:\n            module = __import__(path, None, None, [])\n            # We must do an import of the fully qualified name.  Otherwise if a\n            # subpackage 'a.b' is requested where 'import a' does NOT provide\n            # 'a.b' automatically, then 'a.b' will not be found below.  This\n            # second call will force the equivalent of 'import a.b' to happen\n            # after the top-level import above.\n            my_import(fullname)\n            \n        except ImportError:\n            raise ValueError(\n                \"Could not import class or module '%s' specified for inheritance diagram\" % name)\n\n        try:\n            todoc = module\n            for comp in fullname.split('.')[1:]:\n                todoc = getattr(todoc, comp)\n        except AttributeError:\n            raise ValueError(\n                \"Could not find class or module '%s' specified for inheritance diagram\" % name)\n\n        # If a class, just return it\n        if inspect.isclass(todoc):\n            return [todoc]\n        elif inspect.ismodule(todoc):\n            classes = []\n            for cls in todoc.__dict__.values():\n                if inspect.isclass(cls) and cls.__module__ == todoc.__name__:\n                    classes.append(cls)\n            return classes\n        raise ValueError(\n            \"'%s' does not resolve to a class or module\" % name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport a list of classes.", "response": "def _import_classes(self, class_names):\n        \"\"\"\n        Import a list of classes.\n        \"\"\"\n        classes = []\n        for name in class_names:\n            classes.extend(self._import_class_or_module(name))\n        return classes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all classes that are ancestors of classes.", "response": "def _all_classes(self, classes):\n        \"\"\"\n        Return a list of all classes that are ancestors of *classes*.\n        \"\"\"\n        all_classes = {}\n\n        def recurse(cls):\n            all_classes[cls] = None\n            for c in cls.__bases__:\n                if c not in all_classes:\n                    recurse(c)\n\n        for cls in classes:\n            recurse(cls)\n\n        return all_classes.keys()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_dot(self, fd, name, parts=0, urls={},\n                     graph_options={}, node_options={},\n                     edge_options={}):\n        \"\"\"\n        Generate a graphviz dot graph from the classes that\n        were passed in to __init__.\n\n        *fd* is a Python file-like object to write to.\n\n        *name* is the name of the graph\n\n        *urls* is a dictionary mapping class names to http urls\n\n        *graph_options*, *node_options*, *edge_options* are\n        dictionaries containing key/value pairs to pass on as graphviz\n        properties.\n        \"\"\"\n        g_options = self.default_graph_options.copy()\n        g_options.update(graph_options)\n        n_options = self.default_node_options.copy()\n        n_options.update(node_options)\n        e_options = self.default_edge_options.copy()\n        e_options.update(edge_options)\n\n        fd.write('digraph %s {\\n' % name)\n        fd.write(self._format_graph_options(g_options))\n\n        for cls in self.all_classes:\n            if not self.show_builtins and cls in __builtins__.values():\n                continue\n\n            name = self.class_name(cls, parts)\n\n            # Write the node\n            this_node_options = n_options.copy()\n            url = urls.get(self.class_name(cls))\n            if url is not None:\n                this_node_options['URL'] = '\"%s\"' % url\n            fd.write('  \"%s\" [%s];\\n' %\n                     (name, self._format_node_options(this_node_options)))\n\n            # Write the edges\n            for base in cls.__bases__:\n                if not self.show_builtins and base in __builtins__.values():\n                    continue\n\n                base_name = self.class_name(base, parts)\n                fd.write('  \"%s\" -> \"%s\" [%s];\\n' %\n                         (base_name, name,\n                          self._format_node_options(e_options)))\n        fd.write('}\\n')", "response": "Generate a graphviz dot graph from the classes that are passed in to __init__."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun graphviz dot over this graph returning the output of the dot.", "response": "def run_dot(self, args, name, parts=0, urls={},\n                graph_options={}, node_options={}, edge_options={}):\n        \"\"\"\n        Run graphviz 'dot' over this graph, returning whatever 'dot'\n        writes to stdout.\n\n        *args* will be passed along as commandline arguments.\n\n        *name* is the name of the graph\n\n        *urls* is a dictionary mapping class names to http urls\n\n        Raises DotException for any of the many os and\n        installation-related errors that may occur.\n        \"\"\"\n        try:\n            dot = subprocess.Popen(['dot'] + list(args),\n                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                                   close_fds=True)\n        except OSError:\n            raise DotException(\"Could not execute 'dot'.  Are you sure you have 'graphviz' installed?\")\n        except ValueError:\n            raise DotException(\"'dot' called with invalid arguments\")\n        except:\n            raise DotException(\"Unexpected error calling 'dot'\")\n\n        self.generate_dot(dot.stdin, name, parts, urls, graph_options,\n                          node_options, edge_options)\n        dot.stdin.close()\n        result = dot.stdout.read()\n        returncode = dot.wait()\n        if returncode != 0:\n            raise DotException(\"'dot' returned the errorcode %d\" % returncode)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minimize_ipopt(fun, x0, args=(), kwargs=None, method=None, jac=None, hess=None, hessp=None,\n                   bounds=None, constraints=(), tol=None, callback=None, options=None):\n    \"\"\"\n    Minimize a function using ipopt. The call signature is exactly like for\n    `scipy.optimize.mimize`. In options, all options are directly passed to\n    ipopt. Check [http://www.coin-or.org/Ipopt/documentation/node39.html] for\n    details.\n    The options `disp` and `maxiter` are automatically mapped to their\n    ipopt-equivalents `print_level` and `max_iter`.\n    \"\"\"\n    if not SCIPY_INSTALLED:\n        raise ImportError('Install SciPy to use the `minimize_ipopt` function.')\n\n    _x0 = np.atleast_1d(x0)\n    problem = IpoptProblemWrapper(fun, args=args, kwargs=kwargs, jac=jac, hess=hess,\n                                  hessp=hessp, constraints=constraints)\n    lb, ub = get_bounds(bounds)\n\n    cl, cu = get_constraint_bounds(constraints, x0)\n\n    if options is None:\n        options = {}\n\n    nlp = cyipopt.problem(n = len(_x0),\n                          m = len(cl),\n                          problem_obj=problem,\n                          lb=lb,\n                          ub=ub,\n                          cl=cl,\n                          cu=cu)\n\n    # python3 compatibility\n    convert_to_bytes(options)\n\n    # Rename some default scipy options\n    replace_option(options, b'disp', b'print_level')\n    replace_option(options, b'maxiter', b'max_iter')\n    if b'print_level' not in options:\n        options[b'print_level'] = 0\n    if b'tol' not in options:\n        options[b'tol'] = tol or 1e-8\n    if b'mu_strategy' not in options:\n        options[b'mu_strategy'] = b'adaptive'\n    if b'hessian_approximation' not in options:\n        if hess is None and hessp is None:\n            options[b'hessian_approximation'] = b'limited-memory'\n    for option, value in options.items():\n        try:\n            nlp.addOption(option, value)\n        except TypeError as e:\n            raise TypeError('Invalid option for IPOPT: {0}: {1} (Original message: \"{2}\")'.format(option, value, e))\n\n    x, info = nlp.solve(_x0)\n\n    if np.asarray(x0).shape == ():\n        x = x[0]\n\n    return OptimizeResult(x=x, success=info['status'] == 0, status=info['status'],\n                          message=info['status_msg'],\n                          fun=info['obj_val'],\n                          info=info,\n                          nfev=problem.nfev,\n                          njev=problem.njev,\n                          nit=problem.nit)", "response": "Minimize a function using ipopt."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_md_to_rst(file):\n    try:\n        from m2r import parse_from_file\n        return parse_from_file(file).replace(\n            \"artwork/\", \"http://198.27.119.65/\"\n        )\n    except ImportError:\n        # m2r may not be installed in user environment\n        return read(file)", "response": "Read Markdown file and convert to ReStructured Text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a new tinydb table at the parent database", "response": "def build_table(self):\n        \"\"\"\n        Builds a new tinydb table at the parent database\n        :return:\n        \"\"\"\n        self.table = self.parent.tinydb.table(self.tablename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a collection from the TinyDB database.", "response": "def drop(self, **kwargs):\n        \"\"\"\n        Removes a collection from the database.\n        **kwargs only because of the optional \"writeConcern\" field, but does nothing in the TinyDB database.\n        :return: Returns True when successfully drops a collection. Returns False when collection to drop does not\n        exist.\n        \"\"\"\n        if self.table:\n            self.parent.tinydb.purge_table(self.tablename)\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert_one(self, doc, *args, **kwargs):\n        if self.table is None:\n            self.build_table()\n\n        if not isinstance(doc, dict):\n            raise ValueError(u'\"doc\" must be a dict')\n\n        _id = doc[u'_id'] = doc.get('_id') or generate_id()\n\n        bypass_document_validation = kwargs.get('bypass_document_validation')\n        if bypass_document_validation is True:\n            # insert doc without validation of duplicated `_id`\n            eid = self.table.insert(doc)\n        else:\n            existing = self.find_one({'_id': _id})\n            if existing is None:\n                eid = self.table.insert(doc)\n            else:\n                raise DuplicateKeyError(\n                    u'_id:{0} already exists in collection:{1}'.format(\n                        _id, self.tablename\n                    )\n                )\n\n        return InsertOneResult(eid=eid, inserted_id=_id)", "response": "Insert one document into the collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_many(self, docs, *args, **kwargs):\n        if self.table is None:\n            self.build_table()\n\n        if not isinstance(docs, list):\n            raise ValueError(u'\"insert_many\" requires a list input')\n\n        bypass_document_validation = kwargs.get('bypass_document_validation')\n\n        if bypass_document_validation is not True:\n            # get all _id in once, to reduce I/O. (without projection)\n            existing = [doc['_id'] for doc in self.find({})]\n\n        _ids = list()\n        for doc in docs:\n\n            _id = doc[u'_id'] = doc.get('_id') or generate_id()\n\n            if bypass_document_validation is not True:\n                if _id in existing:\n                    raise DuplicateKeyError(\n                        u'_id:{0} already exists in collection:{1}'.format(\n                            _id, self.tablename\n                        )\n                    )\n                existing.append(_id)\n\n            _ids.append(_id)\n\n        results = self.table.insert_multiple(docs)\n\n        return InsertManyResult(\n            eids=[eid for eid in results],\n            inserted_ids=[inserted_id for inserted_id in _ids]\n        )", "response": "Insert many documents into the collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_query(self, query):\n        logger.debug(u'query to parse2: {}'.format(query))\n\n        # this should find all records\n        if query == {} or query is None:\n            return Query()._id != u'-1'  # noqa\n\n        q = None\n        # find the final result of the generator\n        for c in self.parse_condition(query):\n            if q is None:\n                q = c\n            else:\n                q = q & c\n\n        logger.debug(u'new query item2: {}'.format(q))\n\n        return q", "response": "Parses a tinydb Query object from the dictionary representation of the query"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_condition(self, query, prev_key=None, last_prev_key=None):\n        # use this to determine gt/lt/eq on prev_query\n        logger.debug(u'query: {} prev_query: {}'.format(query, prev_key))\n\n        q = Query()\n        conditions = None\n\n        # deal with the {'name': value} case by injecting a previous key\n        if not prev_key:\n            temp_query = copy.deepcopy(query)\n            k, v = temp_query.popitem()\n            prev_key = k\n\n        # deal with the conditions\n        for key, value in query.items():\n            logger.debug(u'conditions: {} {}'.format(key, value))\n\n            if key == u'$gte':\n                conditions = (\n                    Q(q, prev_key) >= value\n                ) if not conditions and prev_key != \"$not\" \\\n                else (conditions & (Q(q, prev_key) >= value)) if prev_key != \"$not\" \\\n                else (q[last_prev_key] < value)\n            elif key == u'$gt':\n                conditions = (\n                    Q(q, prev_key) > value\n                ) if not conditions and prev_key != \"$not\" \\\n                else (conditions & (Q(q, prev_key) > value)) if prev_key != \"$not\" \\\n                else (q[last_prev_key] <= value)\n            elif key == u'$lte':\n                conditions = (\n                    Q(q, prev_key) <= value\n                ) if not conditions and prev_key != \"$not\" \\\n                else (conditions & (Q(q, prev_key) <= value)) if prev_key != \"$not\" \\\n                else (q[last_prev_key] > value)\n            elif key == u'$lt':\n                conditions = (\n                    Q(q, prev_key) < value\n                ) if not conditions and prev_key != \"$not\" \\\n                else (conditions & (Q(q, prev_key) < value)) if prev_key != \"$not\" \\\n                else (q[last_prev_key] >= value)\n            elif key == u'$ne':\n                conditions = (\n                    Q(q, prev_key) != value\n                ) if not conditions and prev_key != \"$not\" \\\n                else (conditions & (Q(q, prev_key) != value))if prev_key != \"$not\" \\\n                else (q[last_prev_key] == value)\n            elif key == u'$not':\n                if not isinstance(value, dict) and not isinstance(value, list):\n                    conditions = (\n                        Q(q, prev_key) != value\n                    ) if not conditions and prev_key != \"$not\" \\\n                    else (conditions & (Q(q, prev_key) != value)) \\\n                    if prev_key != \"$not\" else (q[last_prev_key] >= value)\n                else:\n                    # let the value's condition be parsed below\n                    pass\n            elif key == u'$regex':\n                value = value.replace('\\\\\\\\\\\\', '|||')\n                value = value.replace('\\\\\\\\', '|||')\n                regex = value.replace('\\\\', '')\n                regex = regex.replace('|||', '\\\\')\n                currCond = (where(prev_key).matches(regex))\n                conditions = currCond if not conditions else (conditions & currCond)\n            elif key in ['$and', '$or', '$in', '$all']:\n                pass\n            else:\n\n\n                # don't want to use the previous key if this is a secondary key\n                # (fixes multiple item query that includes $ codes)\n                if not isinstance(value, dict) and not isinstance(value, list):\n                    conditions = (\n                        (Q(q, key) == value) | (Q(q, key).any([value]))\n                    ) if not conditions else (conditions & ((Q(q, key) == value) | (Q(q, key).any([value]))))\n                    prev_key = key\n\n            logger.debug(u'c: {}'.format(conditions))\n            if isinstance(value, dict):\n                # yield from self.parse_condition(value, key)\n                for parse_condition in self.parse_condition(value, key, prev_key):\n                    yield parse_condition\n            elif isinstance(value, list):\n                if key == '$and':\n                    grouped_conditions = None\n                    for spec in value:\n                        for parse_condition in self.parse_condition(spec):\n                            grouped_conditions = (\n                                parse_condition\n                                if not grouped_conditions\n                                else grouped_conditions & parse_condition\n                            )\n                    yield grouped_conditions\n                elif key == '$or':\n                    grouped_conditions = None\n                    for spec in value:\n                        for parse_condition in self.parse_condition(spec):\n                            grouped_conditions = (\n                                parse_condition\n                                if not grouped_conditions\n                                else grouped_conditions | parse_condition\n                            )\n                    yield grouped_conditions\n                elif key == '$in':\n                    # use `any` to find with list, before comparing to single string\n                    grouped_conditions = Q(q, prev_key).any(value)\n                    for val in value:\n                        for parse_condition in self.parse_condition({prev_key : val}):\n                            grouped_conditions = (\n                                parse_condition\n                                if not grouped_conditions\n                                else grouped_conditions | parse_condition\n                            )\n                    yield grouped_conditions\n                elif key == '$all':\n                    yield Q(q, prev_key).all(value)\n                else:\n                    yield Q(q, prev_key).any([value])\n            else:\n                yield conditions", "response": "Parses the condition of the current key - value pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, query, doc, *args, **kwargs):\n        if isinstance(doc, list):\n            return [\n                self.update_one(query, item, *args, **kwargs)\n                for item in doc\n            ]\n        else:\n            return self.update_one(query, doc, *args, **kwargs)", "response": "BAckwards compatibility with update"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_one(self, query, doc):\n        if self.table is None:\n            self.build_table()\n\n        if u\"$set\" in doc:\n            doc = doc[u\"$set\"]\n\n        allcond = self.parse_query(query)\n\n        try:\n            result = self.table.update(doc, allcond)\n        except:\n            # TODO: check table.update result\n            # check what pymongo does in that case\n            result = None\n\n        return UpdateResult(raw_result=result)", "response": "Updates one element of the collection with the given query and doc."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind all matching results in the table.", "response": "def find(self, filter=None, sort=None, skip=None, limit=None,\n             *args, **kwargs):\n        \"\"\"\n        Finds all matching results\n\n        :param query: dictionary representing the mongo query\n        :return: cursor containing the search results\n        \"\"\"\n        if self.table is None:\n            self.build_table()\n\n        if filter is None:\n            result = self.table.all()\n        else:\n            allcond = self.parse_query(filter)\n\n            try:\n                result = self.table.search(allcond)\n            except (AttributeError, TypeError):\n                result = []\n\n        result = TinyMongoCursor(\n            result,\n            sort=sort,\n            skip=skip,\n            limit=limit\n        )\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_one(self, filter=None):\n\n        if self.table is None:\n            self.build_table()\n\n        allcond = self.parse_query(filter)\n\n        return self.table.get(allcond)", "response": "Find one matching query element by name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, spec_or_id, multi=True, *args, **kwargs):\n        if multi:\n            return self.delete_many(spec_or_id)\n        return self.delete_one(spec_or_id)", "response": "Backwards compatibility with remove"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_one(self, query):\n        item = self.find_one(query)\n        result = self.table.remove(where(u'_id') == item[u'_id'])\n\n        return DeleteResult(raw_result=result)", "response": "Deletes one document from the collection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove all items matching the mongo query query", "response": "def delete_many(self, query):\n        \"\"\"\n        Removes all items matching the mongo query\n\n        :param query: dictionary representing the mongo query\n        :return: DeleteResult\n        \"\"\"\n        items = self.find(query)\n        result = [\n            self.table.remove(where(u'_id') == item[u'_id'])\n            for item in items\n        ]\n\n        if query == {}:\n            # need to reset TinyDB's index for docs order consistency\n            self.table._last_id = 0\n\n        return DeleteResult(raw_result=result)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npaginating list of records", "response": "def paginate(self, skip, limit):\n        \"\"\"Paginate list of records\"\"\"\n        if not self.count() or not limit:\n            return\n        skip = skip or 0\n        pages = int(ceil(self.count() / float(limit)))\n        limits = {}\n        last = 0\n        for i in range(pages):\n            current = limit * i\n            limits[last] = current\n            last = current\n        # example with count == 62\n        # {0: 20, 20: 40, 40: 60, 60: 62}\n        if limit and limit < self.count():\n            limit = limits.get(skip, self.count())\n            self.cursordat = self.cursordat[skip: limit]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _order(self, value, is_reverse=None):\n\n        def _dict_parser(dict_doc):\n            \"\"\" dict ordered by:\n            valueType_N -> key_N -> value_N\n            \"\"\"\n            result = list()\n            for key in dict_doc:\n                data = self._order(dict_doc[key])\n                res = (data[0], key, data[1])\n                result.append(res)\n            return tuple(result)\n\n        def _list_parser(list_doc):\n            \"\"\"list will iter members to compare\n            \"\"\"\n            result = list()\n            for member in list_doc:\n                result.append(self._order(member))\n            return result\n\n        # (TODO) include more data type\n        if value is None or not isinstance(value, (dict,\n                                                   list,\n                                                   basestring,\n                                                   bool,\n                                                   float,\n                                                   int)):\n            # not support/sortable value type\n            value = (0, None)\n\n        elif isinstance(value, bool):\n            value = (5, value)\n\n        elif isinstance(value, (int, float)):\n            value = (1, value)\n\n        elif isinstance(value, basestring):\n            value = (2, value)\n\n        elif isinstance(value, dict):\n            value = (3, _dict_parser(value))\n\n        elif isinstance(value, list):\n            if len(value) == 0:\n                # [] less then None\n                value = [(-1, [])]\n            else:\n                value = _list_parser(value)\n\n            if is_reverse is not None:\n                # list will firstly compare with other doc by it's smallest\n                # or largest member\n                value = max(value) if is_reverse else min(value)\n            else:\n                # if the smallest or largest member is a list\n                # then compaer with it's sub-member in list index order\n                value = (4, tuple(value))\n\n        return value", "response": "Parses the data to a sortable form by giving each data type an ID an ID an integer and assemble with the value\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsorts the current cursor object based on the input key_or_list and returns the new cursor object.", "response": "def sort(self, key_or_list, direction=None):\n        \"\"\"\n        Sorts a cursor object based on the input\n\n        :param key_or_list: a list/tuple containing the sort specification,\n        i.e. ('user_number': -1), or a basestring\n        :param direction: sorting direction, 1 or -1, needed if key_or_list\n                          is a basestring\n        :return:\n        \"\"\"\n\n        # checking input format\n\n        sort_specifier = list()\n        if isinstance(key_or_list, list):\n            if direction is not None:\n                raise ValueError('direction can not be set separately '\n                                 'if sorting by multiple fields.')\n            for pair in key_or_list:\n                if not (isinstance(pair, list) or isinstance(pair, tuple)):\n                    raise TypeError('key pair should be a list or tuple.')\n                if not len(pair) == 2:\n                    raise ValueError('Need to be (key, direction) pair')\n                if not isinstance(pair[0], basestring):\n                    raise TypeError('first item in each key pair must '\n                                    'be a string')\n                if not isinstance(pair[1], int) or not abs(pair[1]) == 1:\n                    raise TypeError('bad sort specification.')\n\n            sort_specifier = key_or_list\n\n        elif isinstance(key_or_list, basestring):\n            if direction is not None:\n                if not isinstance(direction, int) or not abs(direction) == 1:\n                    raise TypeError('bad sort specification.')\n            else:\n                # default ASCENDING\n                direction = 1\n\n            sort_specifier = [(key_or_list, direction)]\n\n        else:\n            raise ValueError('Wrong input, pass a field name and a direction,'\n                             ' or pass a list of (key, direction) pairs.')\n\n        # sorting\n\n        _cursordat = self.cursordat\n\n        total = len(_cursordat)\n        pre_sect_stack = list()\n        for pair in sort_specifier:\n\n            is_reverse = bool(1-pair[1])\n            value_stack = list()\n            for index, data in enumerate(_cursordat):\n\n                # get field value\n\n                not_found = None\n                for key in pair[0].split('.'):\n                    not_found = True\n\n                    if isinstance(data, dict) and key in data:\n                        data = copy.deepcopy(data[key])\n                        not_found = False\n\n                    elif isinstance(data, list):\n                        if not is_reverse and len(data) == 1:\n                            # MongoDB treat [{data}] as {data}\n                            # when finding fields\n                            if isinstance(data[0], dict) and key in data[0]:\n                                data = copy.deepcopy(data[0][key])\n                                not_found = False\n\n                        elif is_reverse:\n                            # MongoDB will keep finding field in reverse mode\n                            for _d in data:\n                                if isinstance(_d, dict) and key in _d:\n                                    data = copy.deepcopy(_d[key])\n                                    not_found = False\n                                    break\n\n                    if not_found:\n                        break\n\n                # parsing data for sorting\n\n                if not_found:\n                    # treat no match as None\n                    data = None\n\n                value = self._order(data, is_reverse)\n\n                # read previous section\n                pre_sect = pre_sect_stack[index] if pre_sect_stack else 0\n                # inverse if in reverse mode\n                # for keeping order as ASCENDING after sort\n                pre_sect = (total - pre_sect) if is_reverse else pre_sect\n                _ind = (total - index) if is_reverse else index\n\n                value_stack.append((pre_sect, value, _ind))\n\n            # sorting cursor data\n\n            value_stack.sort(reverse=is_reverse)\n\n            ordereddat = list()\n            sect_stack = list()\n            sect_id = -1\n            last_dat = None\n            for dat in value_stack:\n                # restore if in reverse mode\n                _ind = (total - dat[-1]) if is_reverse else dat[-1]\n                ordereddat.append(_cursordat[_ind])\n\n                # define section\n                # maintain the sorting result in next level sorting\n                if not dat[1] == last_dat:\n                    sect_id += 1\n                sect_stack.append(sect_id)\n                last_dat = dat[1]\n\n            # save result for next level sorting\n            _cursordat = ordereddat\n            pre_sect_stack = sect_stack\n\n        # done\n\n        self.cursordat = _cursordat\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hasNext(self):\n        cursor_pos = self.cursorpos + 1\n\n        try:\n            self.cursordat[cursor_pos]\n            return True\n        except IndexError:\n            return False", "response": "Returns True if the cursor has a next position False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a 1 - bit PCI. Image and dumps it to the PCD8544 LCD display.", "response": "def display(self, image):\n        \"\"\"\n        Takes a 1-bit :py:mod:`PIL.Image` and dumps it to the PCD8544\n        LCD display.\n        \"\"\"\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n\n        image = self.preprocess(image)\n\n        self.command(0x20, 0x80, 0x40)\n\n        buf = bytearray(self._w * self._h // 8)\n        off = self._offsets\n        mask = self._mask\n\n        for idx, pix in enumerate(image.getdata()):\n            if pix > 0:\n                buf[off[idx]] |= mask[idx]\n\n        self.data(list(buf))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay the contents of the image to the ST7567 LCD display.", "response": "def display(self, image):\n        \"\"\"\n        Takes a 1-bit :py:mod:`PIL.Image` and dumps it to the ST7567\n        LCD display\n        \"\"\"\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n\n        image = self.preprocess(image)\n\n        set_page_address = 0xB0\n\n        image_data = image.getdata()\n        pixels_per_page = self.width * 8\n        buf = bytearray(self.width)\n\n        for y in range(0, int(self._pages * pixels_per_page), pixels_per_page):\n            self.command(set_page_address, 0x04, 0x10)\n            set_page_address += 1\n            offsets = [y + self.width * i for i in range(8)]\n\n            for x in range(self.width):\n                buf[x] = \\\n                    (image_data[x + offsets[0]] and 0x01) | \\\n                    (image_data[x + offsets[1]] and 0x02) | \\\n                    (image_data[x + offsets[2]] and 0x04) | \\\n                    (image_data[x + offsets[3]] and 0x08) | \\\n                    (image_data[x + offsets[4]] and 0x10) | \\\n                    (image_data[x + offsets[5]] and 0x20) | \\\n                    (image_data[x + offsets[6]] and 0x40) | \\\n                    (image_data[x + offsets[7]] and 0x80)\n\n            self.data(list(buf))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef should_stream(proxy_response):\n    content_type = proxy_response.headers.get('Content-Type')\n\n    if is_html_content_type(content_type):\n        return False\n\n    try:\n        content_length = int(proxy_response.headers.get('Content-Length', 0))\n    except ValueError:\n        content_length = 0\n\n    if not content_length or content_length > MIN_STREAMING_LENGTH:\n        return True\n\n    return False", "response": "Function to verify if the proxy_response should be converted into a stream\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_charset(content_type):\n    if not content_type:\n        return DEFAULT_CHARSET\n\n    matched = _get_charset_re.search(content_type)\n    if matched:\n        # Extract the charset and strip its double quotes\n        return matched.group('charset').replace('\"', '')\n    return DEFAULT_CHARSET", "response": "Function used to retrieve the charset from a Content - Type header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef required_header(header):\n    if header in IGNORE_HEADERS:\n        return False\n\n    if header.startswith('HTTP_') or header == 'CONTENT_TYPE':\n        return True\n\n    return False", "response": "Function that verify if the header parameter is a essential header"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions used to transform headers in the request to the original format", "response": "def normalize_request_headers(request):\n    r\"\"\"Function used to transform header, replacing 'HTTP\\_' to ''\n    and replace '_' to '-'\n\n    :param request:  A HttpRequest that will be transformed\n    :returns:        A dictionary with the normalized headers\n    \"\"\"\n    norm_headers = {}\n    for header, value in request.META.items():\n        if required_header(header):\n            norm_header = header.replace('HTTP_', '').title().replace('_', '-')\n            norm_headers[norm_header] = value\n\n    return norm_headers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_items(items):\n    encoded = []\n    for key, values in items:\n        for value in values:\n            encoded.append((key.encode('utf-8'), value.encode('utf-8')))\n    return encoded", "response": "Function that encodes all elements in the list of items passed as\n    a parameter\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a string representing a valid HTTP cookie into a dictionary containing the cookie_string and the cookie_string attributes.", "response": "def cookie_from_string(cookie_string, strict_cookies=False):\n    \"\"\"Parser for HTTP header set-cookie\n    The return from this function will be used as parameters for\n    django's response.set_cookie method. Because set_cookie doesn't\n    have parameter comment, this cookie attribute will be ignored.\n\n    :param  cookie_string: A string representing a valid cookie\n    :param  strict_cookies: Whether to only accept RFC-compliant cookies\n    :returns: A dictionary containing the cookie_string attributes\n    \"\"\"\n\n    if strict_cookies:\n\n        cookies = SimpleCookie(COOKIE_PREFIX + cookie_string)\n        if not cookies.keys():\n            return None\n        cookie_name, = cookies.keys()\n        cookie_dict = {k: v for k, v in cookies[cookie_name].items()\n                       if v and k != 'comment'}\n        cookie_dict['key'] = cookie_name\n        cookie_dict['value'] = cookies[cookie_name].value\n        return cookie_dict\n\n    else:\n        valid_attrs = ('path', 'domain', 'comment', 'expires',\n                       'max_age', 'httponly', 'secure')\n\n        cookie_dict = {}\n\n        cookie_parts = cookie_string.split(';')\n        try:\n            key, value = cookie_parts[0].split('=', 1)\n            cookie_dict['key'], cookie_dict['value'] = key, unquote(value)\n        except ValueError:\n            logger.warning('Invalid cookie: `%s`', cookie_string)\n            return None\n\n        if cookie_dict['value'].startswith('='):\n            logger.warning('Invalid cookie: `%s`', cookie_string)\n            return None\n\n        for part in cookie_parts[1:]:\n            if '=' in part:\n                attr, value = part.split('=', 1)\n                value = value.strip()\n            else:\n                attr = part\n                value = ''\n\n            attr = attr.strip().lower()\n            if not attr:\n                continue\n\n            if attr in valid_attrs:\n                if attr in ('httponly', 'secure'):\n                    cookie_dict[attr] = True\n                elif attr in 'comment':\n                    # ignoring comment attr as explained in the\n                    # function docstring\n                    continue\n                else:\n                    cookie_dict[attr] = unquote(value)\n            else:\n                logger.warning('Unknown cookie attribute %s', attr)\n\n        return cookie_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unquote(value):\n    if len(value) > 1 and value[0] == '\"' and value[-1] == '\"':\n        value = value[1:-1].replace(r'\\\"', '\"')\n    return value", "response": "Removes wrapping quotes from a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions used to convert certain string values into an appropriated boolean value.", "response": "def asbool(value):\n    \"\"\"Function used to convert certain string values into an appropriated\n    boolean value.If value is not a string the built-in python\n    bool function will be used to convert the passed parameter\n\n    :param      value: an object to be converted to a boolean value\n    :returns:   A boolean value\n    \"\"\"\n\n    is_string = isinstance(value, string_types)\n\n    if is_string:\n        value = value.strip().lower()\n        if value in ('true', 'yes', 'on', 'y', 't', '1',):\n            return True\n        elif value in ('false', 'no', 'off', 'n', 'f', '0'):\n            return False\n        else:\n            raise ValueError(\"String is not true/false: %r\" % value)\n    else:\n        return bool(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if we should transform the response.", "response": "def should_transform(self):\n        \"\"\"Determine if we should transform the response\n\n        :returns:  A boolean value\n        \"\"\"\n\n        if not HAS_DIAZO:\n            self.log.info(\"HAS_DIAZO: false\")\n            return False\n\n        if asbool(self.request.META.get(DIAZO_OFF_REQUEST_HEADER)):\n            self.log.info(\"DIAZO_OFF_REQUEST_HEADER in request.META: off\")\n            return False\n\n        if asbool(self.response.get(DIAZO_OFF_RESPONSE_HEADER)):\n            self.log.info(\"DIAZO_OFF_RESPONSE_HEADER in response.get: off\")\n            return False\n\n        if self.request.is_ajax():\n            self.log.info(\"Request is AJAX\")\n            return False\n\n        if self.response.streaming:\n            self.log.info(\"Response has streaming\")\n            return False\n\n        content_type = self.response.get('Content-Type')\n        if not is_html_content_type(content_type):\n            self.log.info(\"Content-type: false\")\n            return False\n\n        content_encoding = self.response.get('Content-Encoding')\n        if content_encoding in ('zip', 'compress'):\n            self.log.info(\"Content encode is %s\", content_encoding)\n            return False\n\n        status_code = str(self.response.status_code)\n        if status_code.startswith('3') or \\\n                status_code == '204' or \\\n                status_code == '401':\n            self.log.info(\"Status code: %s\", status_code)\n            return False\n\n        if len(self.response.content) == 0:\n            self.log.info(\"Response Content is EMPTY\")\n            return False\n\n        self.log.info(\"Transform\")\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_html5_doctype(self):\n        doctype = b'<!DOCTYPE html>\\n'\n        content = doctype_re.subn(doctype, self.response.content, 1)[0]\n        self.response.content = content", "response": "Method used to transform a doctype in to a properly html5 doctype\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noutput a string to the internal buffer.", "response": "def _output(self, s):\n    \"\"\"Host header should always be first\"\"\"\n\n    if s.lower().startswith(b'host: '):\n        self._buffer.insert(1, s)\n    else:\n        self._buffer.append(s)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_django_response(proxy_response, strict_cookies=False):\n    status = proxy_response.status\n    headers = proxy_response.headers\n\n    logger.debug('Proxy response headers: %s', headers)\n\n    content_type = headers.get('Content-Type')\n\n    logger.debug('Content-Type: %s', content_type)\n\n    if should_stream(proxy_response):\n        logger.info('Content-Length is bigger than %s', DEFAULT_AMT)\n        response = StreamingHttpResponse(proxy_response.stream(DEFAULT_AMT),\n                                         status=status,\n                                         content_type=content_type)\n    else:\n        content = proxy_response.data or b''\n        response = HttpResponse(content, status=status,\n                                content_type=content_type)\n\n    logger.info('Normalizing response headers')\n    set_response_headers(response, headers)\n\n    logger.debug('Response headers: %s', getattr(response, '_headers'))\n\n    cookies = proxy_response.headers.getlist('set-cookie')\n    logger.info('Checking for invalid cookies')\n    for cookie_string in cookies:\n        cookie_dict = cookie_from_string(cookie_string,\n                                         strict_cookies=strict_cookies)\n        # if cookie is invalid cookie_dict will be None\n        if cookie_dict:\n            response.set_cookie(**cookie_dict)\n\n    logger.debug('Response cookies: %s', response.cookies)\n\n    return response", "response": "This method creates an appropriate django. http. HTTPResponse based on the proxy_response Content - Length of the proxy_response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the request headers that will be sent to upstream.", "response": "def get_request_headers(self):\n        \"\"\"Return request headers that will be sent to upstream.\n\n        The header REMOTE_USER is set to the current user\n        if AuthenticationMiddleware is enabled and\n        the view's add_remote_user property is True.\n\n        .. versionadded:: 0.9.8\n\n        \"\"\"\n        request_headers = self.get_proxy_request_headers(self.request)\n\n        if (self.add_remote_user and hasattr(self.request, 'user')\n                and self.request.user.is_active):\n            request_headers['REMOTE_USER'] = self.request.user.get_username()\n            self.log.info(\"REMOTE_USER set\")\n\n        return request_headers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning encoded query params to be used in proxied request", "response": "def get_encoded_query_params(self):\n        \"\"\"Return encoded query params to be used in proxied request\"\"\"\n        get_data = encode_items(self.request.GET.lists())\n        return urlencode(get_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stream_download(url, target_path, verbose=False):\n    response = requests.get(url, stream=True)\n    handle = open(target_path, \"wb\")\n    if verbose:\n        print(\"Beginning streaming download of %s\" % url)\n        start = datetime.now()\n        try:\n            content_length = int(response.headers['Content-Length'])\n            content_MB = content_length/1048576.0\n            print(\"Total file size: %.2f MB\" % content_MB)\n        except KeyError:\n            pass      # allow Content-Length to be missing\n    for chunk in response.iter_content(chunk_size=512):\n        if chunk:     # filter out keep-alive new chunks\n            handle.write(chunk)\n    if verbose:\n        print(\n            \"Download completed to %s in %s\" %\n            (target_path, datetime.now() - start))", "response": "Download a large file without loading it into memory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_object_id(object_id):\n    result = re.match(OBJECT_ID_RE, str(object_id))\n    if not result:\n        print(\"'%s' appears not to be a valid 990 object_id\" % object_id)\n        raise RuntimeError(OBJECT_ID_MSG)\n    return object_id", "response": "It's easy to make a mistake entering these, validate the format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_table_start(self):\n        if self.documentation:\n            standardized_table_start = {\n                'object_id': {\n                    'value': self.object_id,\n                    'ordering': -1,\n                    'line_number': 'NA',\n                    'description': 'IRS-assigned object id',\n                    'db_type': 'String(18)'\n                },\n                'ein': {\n                    'value': self.ein,\n                    'ordering': -2,\n                    'line_number': 'NA',\n                    'description': 'IRS employer id number',\n                    'db_type': 'String(9)'\n                }\n            }\n            if self.documentId:\n                standardized_table_start['documentId'] = {\n                    'value': self.documentId,\n                    'description': 'Document ID',\n                    'ordering': 0\n                }\n        else:\n            standardized_table_start = {\n                'object_id': self.object_id,\n                'ein': self.ein\n            }\n            if self.documentId:\n                standardized_table_start['documentId'] = self.documentId\n\n        return standardized_table_start", "response": "prefill the columns we need for all tables"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef debracket(string):\n    result = re.sub(BRACKET_RE, ';', str(string))\n    result = result.lstrip(';')\n    result = result.lstrip(' ')\n    result = result.replace('; ;',';')\n    return result", "response": "Eliminate the bracketed var names in doc line strings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_sked(self, object_id, sked, verbose=False):\n        self.whole_filing_data = []\n        self.filing_keyerr_data = []\n        this_filing = Filing(object_id)\n        this_filing.process(verbose=verbose)\n        this_version = this_filing.get_version()\n        if this_version in ALLOWED_VERSIONSTRINGS or ( self.csv_format and this_version in CSV_ALLOWED_VERSIONSTRINGS ):\n            this_version = this_filing.get_version()\n            ein = this_filing.get_ein()\n            sked_dict = this_filing.get_schedule(sked)\n            self._run_schedule(sked, object_id, sked_dict, ein)\n\n            this_filing.set_result(self.whole_filing_data)\n            this_filing.set_keyerrors(self.filing_keyerr_data)\n            return this_filing\n        else:\n            print(\"Filing version %s isn't supported for this operation\" % this_version )\n            return this_filing", "response": "This method runs the sked command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _download(self, force_overwrite=False, verbose=False):\n        if not force_overwrite:\n            # If the file is already there, we're done\n            if os.path.isfile(self.filepath):\n                if verbose:\n                    print(\n                        \"File already available at %s -- skipping\"\n                        % (self.filepath)\n                    )\n                return False\n        stream_download(self.URL, self.filepath, verbose=verbose)\n        return True", "response": "Download the file if it s not already there."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_schedules(self):\n        self.schedules = ['ReturnHeader990x', ]\n        self.otherforms = []\n        for sked in self.raw_irs_dict['Return']['ReturnData'].keys():\n            if not sked.startswith(\"@\"):\n                if sked in KNOWN_SCHEDULES:\n                    self.schedules.append(sked)\n                else:\n                    self.otherforms.append(sked)", "response": "Attach the known and unknown schedules"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an array because multiple sked K s are allowed", "response": "def get_parsed_sked(self, skedname):\n        \"\"\" Returns an array because multiple sked K's are allowed\"\"\"\n        if not self.processed:\n            raise Exception(\"Filing must be processed to return parsed sked\")\n        if skedname in self.schedules:\n            matching_skeds = []\n            for sked in self.result:\n                if sked['schedule_name']==skedname:\n                    matching_skeds.append(sked)\n            return matching_skeds\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the headers of the uploader instance.", "response": "def headers(self):\n        \"\"\"\n        Return headers of the uploader instance. This would include the headers of the\n        client instance.\n        \"\"\"\n        client_headers = getattr(self.client, 'headers', {})\n        return dict(self.DEFAULT_HEADERS, **client_headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef headers_as_list(self):\n        headers = self.headers\n        headers_list = ['{}: {}'.format(key, value) for key, value in iteritems(headers)]\n        return headers_list", "response": "Returns the headers as a list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_offset(self):\n        resp = requests.head(self.url, headers=self.headers)\n        offset = resp.headers.get('upload-offset')\n        if offset is None:\n            msg = 'Attempt to retrieve offset fails with status {}'.format(resp.status_code)\n            raise TusCommunicationError(msg, resp.status_code, resp.content)\n        return int(offset)", "response": "Get the offset from the tus server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning list of encoded metadata as defined by the Tus protocol.", "response": "def encode_metadata(self):\n        \"\"\"\n        Return list of encoded metadata as defined by the Tus protocol.\n        \"\"\"\n        encoded_list = []\n        for key, value in iteritems(self.metadata):\n            key_str = str(key)  # dict keys may be of any object type.\n\n            # confirm that the key does not contain unwanted characters.\n            if re.search(r'^$|[\\s,]+', key_str):\n                msg = 'Upload-metadata key \"{}\" cannot be empty nor contain spaces or commas.'\n                raise ValueError(msg.format(key_str))\n\n            value_bytes = b(value)  # python 3 only encodes bytes\n            encoded_list.append('{} {}'.format(key_str, b64encode(value_bytes).decode('ascii')))\n        return encoded_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_url(self):\n        if self.store_url and self.url_storage:\n            key = self.fingerprinter.get_fingerprint(self.get_file_stream())\n            url = self.url_storage.get_item(key)\n            if not url:\n                url = self.create_url()\n                self.url_storage.set_item(key, url)\n            return url\n        else:\n            return self.create_url()", "response": "Get the url of the tus upload."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_url(self):\n        headers = self.headers\n        headers['upload-length'] = str(self.file_size)\n        headers['upload-metadata'] = ','.join(self.encode_metadata())\n        resp = requests.post(self.client.url, headers=headers)\n        url = resp.headers.get(\"location\")\n        if url is None:\n            msg = 'Attempt to retrieve create file url with status {}'.format(resp.status_code)\n            raise TusCommunicationError(msg, resp.status_code, resp.content)\n        return urljoin(self.client.url, url)", "response": "Creates a new upload url for the required file upload."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request_length(self):\n        remainder = self.stop_at - self.offset\n        return self.chunk_size if remainder > self.chunk_size else remainder", "response": "Return length of next chunk upload."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_upload(self):\n        if self.request.status_code == 204:\n            return True\n        else:\n            raise TusUploadFailed('', self.request.status_code, self.request.response_content)", "response": "Verify that the last upload was sucessful."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a file stream instance of the upload.", "response": "def get_file_stream(self):\n        \"\"\"\n        Return a file stream instance of the upload.\n        \"\"\"\n        if self.file_stream:\n            self.file_stream.seek(0)\n            return self.file_stream\n        elif os.path.isfile(self.file_path):\n            return open(self.file_path, 'rb')\n        else:\n            raise ValueError(\"invalid file {}\".format(self.file_path))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file_size(self):\n        stream = self.get_file_stream()\n        stream.seek(0, os.SEEK_END)\n        return stream.tell()", "response": "Return the size of the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload(self, stop_at=None):\n        self.stop_at = stop_at or self.file_size\n\n        while self.offset < self.stop_at:\n            self.upload_chunk()\n        else:\n            if self.log_func:\n                self.log_func(\"maximum upload specified({} bytes) has been reached\".format(self.stop_at))", "response": "Perform a continous upload of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads a single chunk of data to the user.", "response": "def upload_chunk(self):\n        \"\"\"\n        Upload chunk of file.\n        \"\"\"\n        self._retried = 0\n        self._do_request()\n        self.offset = int(self.request.response_headers.get('upload-offset'))\n        if self.log_func:\n            msg = '{} bytes uploaded ...'.format(self.offset)\n            self.log_func(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the tus url of a file identified by the key specified.", "response": "def get_item(self, key):\n        \"\"\"\n        Return the tus url of a file, identified by the key specified.\n\n        :Args:\n            - key[str]: The unique id for the stored item (in this case, url)\n        :Returns: url[str]\n        \"\"\"\n        result = self._db.search(self._urls.key == key)\n        return result[0].get('url') if result else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore the url value under the unique key.", "response": "def set_item(self, key, url):\n        \"\"\"\n        Store the url value under the unique key.\n\n        :Args:\n            - key[str]: The unique id to which the item (in this case, url) would be stored.\n            - value[str]: The actual url value to be stored.\n        \"\"\"\n        if self._db.search(self._urls.key == key):\n            self._db.update({'url': url}, self._urls.key == key)\n        else:\n            self._db.insert({'key': key, 'url': url})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the item under the unique key from the storage.", "response": "def remove_item(self, key):\n        \"\"\"\n        Remove/Delete the url value under the unique key from storage.\n        \"\"\"\n        self._db.remove(self._urls.key==key)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_fingerprint(self, fs):\n        hasher = hashlib.md5()\n        # we encode the content to avoid python 3 uncicode errors\n        buf = self._encode_data(fs.read(self.BLOCK_SIZE))\n        while len(buf) > 0:\n            hasher.update(buf)\n            buf = fs.read(self.BLOCK_SIZE)\n        return 'md5:' + hasher.hexdigest()", "response": "Returns a unique fingerprint string based on the file stream recevied\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_poolmanager(self, connections, maxsize, block=False, **pool_kwargs):\n        try:\n            pool_kwargs['ssl_version'] = ssl.PROTOCOL_TLS\n        except AttributeError:\n            pool_kwargs['ssl_version'] = ssl.PROTOCOL_SSLv23\n        return super(SSLAdapter, self).init_poolmanager(connections, maxsize, block, **pool_kwargs)", "response": "Called to initialize the HTTPAdapter when no proxy is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls to initialize the HTTPAdapter when a proxy is used.", "response": "def proxy_manager_for(self, proxy, **proxy_kwargs):\n        \"\"\"Called to initialize the HTTPAdapter when a proxy is used.\"\"\"\n        try:\n            proxy_kwargs['ssl_version'] = ssl.PROTOCOL_TLS\n        except AttributeError:\n            proxy_kwargs['ssl_version'] = ssl.PROTOCOL_SSLv23\n        return super(SSLAdapter, self).proxy_manager_for(proxy, **proxy_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_calls(self, num_calls=1):\n        self._cull()\n        while self._outstanding_calls + num_calls > self._max_calls_per_second:\n            time.sleep(0)  # yield\n            self._cull()\n\n        self._call_times.append(self.CallRecord(time=time.time(), num_calls=num_calls))\n        self._outstanding_calls += num_calls", "response": "Makes a set of calls for the current application."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving calls more than 1 second old from the queue.", "response": "def _cull(self):\n        \"\"\"Remove calls more than 1 second old from the queue.\"\"\"\n        right_now = time.time()\n\n        cull_from = -1\n        for index in range(len(self._call_times)):\n            if right_now - self._call_times[index].time >= 1.0:\n                cull_from = index\n                self._outstanding_calls -= self._call_times[index].num_calls\n            else:\n                break\n\n        if cull_from > -1:\n            self._call_times = self._call_times[cull_from + 1:]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nproviding session-based retry functionality :param requests: A collection of Request objects. :param responses_for_requests: Dictionary mapping of requests to responses :param max_retries: The maximum number of retries to perform per session :param args: Additional arguments to pass into a retry mapping call", "response": "def map_with_retries(self, requests, responses_for_requests):\n        \"\"\"Provides session-based retry functionality\n\n        :param requests: A collection of Request objects.\n        :param responses_for_requests: Dictionary mapping of requests to responses\n        :param max_retries: The maximum number of retries to perform per session\n        :param args: Additional arguments to pass into a retry mapping call\n\n\n        \"\"\"\n        retries = []\n        response_futures = [preq.callable() for preq in requests]\n\n        for request, response_future in zip(requests, response_futures):\n            try:\n                response = response_future.result()\n                if response is not None and response.status_code == 403:\n                    logging.warning('Request to {} caused a 403 response status code.'.format(request.url))\n                    raise InvalidRequestError('Access forbidden')\n                if response is not None:\n                    responses_for_requests[request] = response\n            except RequestException as re:\n                logging.error('An exception was raised for {}: {}'.format(request.url, re))\n                if self.total_retries > 0:\n                    self.total_retries -= 1\n                    retries.append(request)\n\n        # Recursively retry failed requests with the modified total retry count\n        if retries:\n            self.map_with_retries(retries, responses_for_requests)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef multi_get(self, urls, query_params=None, to_json=True):\n        return self._multi_request(\n            MultiRequest._VERB_GET, urls, query_params,\n            data=None, to_json=to_json,\n        )", "response": "Issue multiple GET requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nissues multiple POST requests.", "response": "def multi_post(self, urls, query_params=None, data=None, to_json=True, send_as_file=False):\n        \"\"\"Issue multiple POST requests.\n\n        Args:\n            urls - A string URL or list of string URLs\n            query_params - None, a dict, or a list of dicts representing the query params\n            data - None, a dict or string, or a list of dicts and strings representing the data body.\n            to_json - A boolean, should the responses be returned as JSON blobs\n            send_as_file - A boolean, should the data be sent as a file.\n        Returns:\n            a list of dicts if to_json is set of requests.response otherwise.\n        Raises:\n            InvalidRequestError - Can not decide how many requests to issue.\n        \"\"\"\n        return self._multi_request(\n            MultiRequest._VERB_POST, urls, query_params,\n            data, to_json=to_json, send_as_file=send_as_file,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _zip_request_params(self, urls, query_params, data):\n\n        # Everybody gets to be a list\n        if not isinstance(urls, list):\n            urls = [urls]\n        if not isinstance(query_params, list):\n            query_params = [query_params]\n        if not isinstance(data, list):\n            data = [data]\n\n        # Counts must not mismatch\n        url_count = len(urls)\n        query_param_count = len(query_params)\n        data_count = len(data)\n\n        max_count = max(url_count, query_param_count, data_count)\n\n        if (\n            max_count > url_count > 1\n            or max_count > query_param_count > 1\n            or max_count > data_count > 1\n        ):\n            raise InvalidRequestError(\n                'Mismatched parameter count url_count:{0} query_param_count:{1} data_count:{2} max_count:{3}',\n                url_count, query_param_count, data_count, max_count,\n            )\n\n        # Pad out lists\n        if url_count < max_count:\n            urls = urls * max_count\n        if query_param_count < max_count:\n            query_params = query_params * max_count\n        if data_count < max_count:\n            data = data * max_count\n\n        return list(zip(urls, query_params, data))", "response": "Massages inputs and returns a list of 3 - tuples zipping them up."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _wait_for_response(self, requests):\n        failed_requests = []\n        responses_for_requests = OrderedDict.fromkeys(requests)\n\n        for retry in range(self._max_retry):\n            try:\n                logging.debug('Try #{0}'.format(retry + 1))\n                self._availability_limiter.map_with_retries(requests, responses_for_requests)\n\n                failed_requests = []\n                for request, response in responses_for_requests.items():\n                    if self._drop_404s and response is not None and response.status_code == 404:\n                        logging.warning('Request to {0} failed with status code 404, dropping.'.format(request.url))\n                    elif not response:\n                        failed_requests.append((request, response))\n\n                if not failed_requests:\n                    break\n\n                logging.warning('Try #{0}. Expected {1} successful response(s) but only got {2}.'.format(\n                    retry + 1, len(requests), len(requests) - len(failed_requests),\n                ))\n\n                # retry only for the failed requests\n                requests = [fr[0] for fr in failed_requests]\n            except InvalidRequestError:\n                raise\n            except Exception as e:\n                # log the exception for the informative purposes and pass to the next iteration\n                logging.exception('Try #{0}. Exception occured: {1}. Retrying.'.format(retry + 1, e))\n                pass\n\n        if failed_requests:\n            logging.warning('Still {0} failed request(s) after {1} retries:'.format(\n                len(failed_requests), self._max_retry,\n            ))\n            for failed_request, failed_response in failed_requests:\n                if failed_response is not None:\n                    # in case response text does contain some non-ascii characters\n                    failed_response_text = failed_response.text.encode('ascii', 'xmlcharrefreplace')\n                    logging.warning('Request to {0} failed with status code {1}. Response text: {2}'.format(\n                        failed_request.url, failed_response.status_code, failed_response_text,\n                    ))\n                else:\n                    logging.warning('Request to {0} failed with None response.'.format(failed_request.url))\n\n        return list(responses_for_requests.values())", "response": "Issues a batch of requests and waits for the responses."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _convert_to_json(self, response):\n        try:\n            return response.json()\n        except ValueError:\n            logging.warning('Expected response in JSON format from {0} but the actual response text is: {1}'.format(\n                response.request.url, response.text,\n            ))\n        return None", "response": "Converts the response to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _multi_request(self, verb, urls, query_params, data, to_json=True, send_as_file=False):\n        if not urls:\n            raise InvalidRequestError('No URL supplied')\n\n        # Break the params into batches of request_params\n        request_params = self._zip_request_params(urls, query_params, data)\n        batch_of_params = [\n            request_params[pos:pos + self._max_requests]\n            for pos in range(0, len(request_params), self._max_requests)\n        ]\n\n        # Iteratively issue each batch, applying the rate limiter if necessary\n        all_responses = []\n        for param_batch in batch_of_params:\n            if self._rate_limiter:\n                self._rate_limiter.make_calls(num_calls=len(param_batch))\n\n            prepared_requests = [\n                self._create_request(\n                    verb, url, query_params=query_param, data=datum, send_as_file=send_as_file,\n                ) for url, query_param, datum in param_batch\n            ]\n\n            responses = self._wait_for_response(prepared_requests)\n            for response in responses:\n                if response:\n                    all_responses.append(self._convert_to_json(response) if to_json else response)\n                else:\n                    all_responses.append(None)\n\n        return all_responses", "response": "Issues multiple HTTP requests and waits for responses."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef error_handling(cls, fn):\n        def wrapper(*args, **kwargs):\n            try:\n                result = fn(*args, **kwargs)\n                return result\n            except InvalidRequestError as e:\n                write_exception(e)\n\n                if hasattr(e, 'request'):\n                    write_error_message('request {0}'.format(repr(e.request)))\n                if hasattr(e, 'response'):\n                    write_error_message('response {0}'.format(repr(e.response)))\n\n                raise e\n        return wrapper", "response": "Decorator to handle errors in the API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new RedLock object and reuse stored Redis clients.", "response": "def create_lock(self, resource, **kwargs):\n        \"\"\"\n        Create a new RedLock object and reuse stored Redis clients.\n        All the kwargs it received would be passed to the RedLock's __init__\n        function.\n        \"\"\"\n        lock = RedLock(resource=resource, created_by_factory=True, **kwargs)\n        lock.redis_nodes = self.redis_nodes\n        lock.quorum = self.quorum\n        lock.factory = self\n        return lock"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef acquire_node(self, node):\n        try:\n            return node.set(self.resource, self.lock_key, nx=True, px=self.ttl)\n        except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):\n            return False", "response": "acquire a single redis node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef release_node(self, node):\n        # use the lua script to release the lock in a safe way\n        try:\n            node._release_script(keys=[self.resource], args=[self.lock_key])\n        except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):\n            pass", "response": "release a redis node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_alexa_rankings(self, domains):\n        api_name = 'alexa_rankings'\n\n        (all_responses, domains) = self._bulk_cache_lookup(api_name, domains)\n        responses = self._request_reports(domains)\n\n        for domain, response in zip(domains, responses):\n            xml_response = self._extract_response_xml(domain, response)\n            if self._cache:\n                self._cache.cache_value(api_name, domain, response)\n            all_responses[domain] = xml_response\n\n        return all_responses", "response": "Retrieves the most recent VT info for a set of domains."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _request_reports(self, domains):\n        params = [{'url': domain} for domain in domains]\n        responses = self._requests.multi_get(\n            self.BASE_URL, query_params=params, to_json=False)\n        return responses", "response": "Sends multiples requests for the resources to a particular endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the XML content of an HTTP response into a dictionary format.", "response": "def _extract_response_xml(self, domain, response):\n        \"\"\"Extract XML content of an HTTP response into dictionary format.\n\n        Args:\n            response: HTML Response objects\n        Returns:\n            A dictionary: {alexa-ranking key : alexa-ranking value}.\n        \"\"\"\n        attributes = {}\n        alexa_keys = {'POPULARITY': 'TEXT', 'REACH': 'RANK', 'RANK': 'DELTA'}\n        try:\n            xml_root = ET.fromstring(response._content)\n            for xml_child in xml_root.findall('SD//'):\n                if xml_child.tag in alexa_keys and \\\n                        alexa_keys[xml_child.tag] in xml_child.attrib:\n                    attributes[xml_child.tag.lower(\n                    )] = xml_child.attrib[alexa_keys[xml_child.tag]]\n        except ParseError:\n            # Skip ill-formatted XML and return no Alexa attributes\n            pass\n        attributes['domain'] = domain\n        return {'attributes': attributes}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a bulk cache lookup and returns a tuple with the results found and the keys missing in the cache.", "response": "def _bulk_cache_lookup(self, api_name, keys):\n        \"\"\"Performes a bulk cache lookup and returns a tuple with the results\n        found and the keys missing in the cache. If cached is not configured\n        it will return an empty dictionary of found results and the initial\n        list of keys.\n\n        Args:\n            api_name: a string name of the API.\n            keys: an enumerable of string keys.\n        Returns:\n            A tuple: (responses found, missing keys).\n        \"\"\"\n        if self._cache:\n            responses = self._cache.bulk_lookup(api_name, keys)\n            missing_keys = [key for key in keys if key not in responses.keys()]\n            return (responses, missing_keys)\n\n        return ({}, keys)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the contents of the cache to disk and clear the in memory cache.", "response": "def close(self):\n        \"\"\"Write the contents of the cache to disk (only if `update_cache`\n        parameter during the object initialization was not set to `False`) and\n        clear the in memory cache.\"\"\"\n        if self._cache:\n            if self._update_cache:\n                self._write_cache_to_file()\n            self._cache = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write_cache_to_file(self):\n        with(open(self._cache_file_name, 'w')) as fp:\n            fp.write(simplejson.dumps(self._cache))", "response": "Write the contents of the cache to a file on disk."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the contents of the cache from a file on disk.", "response": "def _read_cache_from_file(self):\n        \"\"\"Read the contents of the cache from a file on disk.\"\"\"\n        cache = {}\n        try:\n            with(open(self._cache_file_name, 'r')) as fp:\n                contents = fp.read()\n                cache = simplejson.loads(contents)\n        except (IOError, JSONDecodeError):\n            # The file could not be read. This is not a problem if the file does not exist.\n            pass\n\n        return cache"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cache_value(self, api_name, key, value):\n        self._cache.setdefault(api_name, {})\n        self._cache[api_name][key] = value", "response": "Add the value of an API call to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup_value(self, api_name, key):\n        if api_name in self._cache:\n            return self._cache[api_name].get(key, None)\n        return None", "response": "Returns the value of an API call in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a lookup on an enumerable of keys.", "response": "def bulk_lookup(self, api_name, keys):\n        \"\"\"Perform lookup on an enumerable of keys.\n\n        Args:\n            api_name: a string name of the API. Keys and values are segmented by api_name.\n            keys: an enumerable of string keys.\n        \"\"\"\n        cached_data = {}\n\n        for key in keys:\n            value = self.lookup_value(api_name, key)\n            if value is not None:\n                cached_data[key] = value\n        return cached_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cached_by_domain(api_name):\n\n    def wrapped(func):\n        def decorated(self, domains):\n            if not self._cache:\n                return func(self, domains)\n\n            all_responses = self._cache.bulk_lookup(api_name, domains)\n            domains = list(set(domains) - set(all_responses))\n\n            if domains:\n                response = func(self, domains)\n\n                if not response:\n                    raise ResponseError(\"No response for uncached domains\")\n\n                for domain in response:\n                    self._cache.cache_value(api_name, domain, response[domain])\n                    all_responses[domain] = response[domain]\n\n            return all_responses\n        return decorated\n    return wrapped", "response": "A caching wrapper for functions that take a list of domains as\n    parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef domain_score(self, domains):\n        warn(\n            'OpenDNS Domain Scores endpoint is deprecated. Use '\n            'InvestigateApi.categorization() instead', DeprecationWarning,\n        )\n        url_path = 'domains/score/'\n        return self._multi_post(url_path, domains)", "response": "Calls domain scores endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _multi_get(self, cache_api_name, fmt_url_path, url_params, query_params=None):\n        all_responses = {}\n\n        if self._cache:\n            all_responses = self._cache.bulk_lookup(cache_api_name, url_params)\n            url_params = [key for key in url_params if key not in all_responses.keys()]\n\n        if len(url_params):\n            urls = self._to_urls(fmt_url_path, url_params)\n            responses = self._requests.multi_get(urls, query_params)\n            for url_param, response in zip(url_params, responses):\n                if self._cache:\n                    self._cache.cache_value(cache_api_name, url_param, response)\n                all_responses[url_param] = response\n\n        return all_responses", "response": "Makes multiple GETs to an OpenDNS endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling security end point and adds an is_suspicious key to each response.", "response": "def security(self, domains):\n        \"\"\"Calls security end point and adds an 'is_suspicious' key to each response.\n\n        Args:\n            domains: An enumerable of strings\n        Returns:\n            A dict of {domain: security_result}\n        \"\"\"\n        api_name = 'opendns-security'\n        fmt_url_path = u'security/name/{0}.json'\n        return self._multi_get(api_name, fmt_url_path, domains)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls WHOIS Email end point", "response": "def whois_emails(self, emails):\n        \"\"\"Calls WHOIS Email end point\n\n        Args:\n            emails: An enumerable of string Emails\n        Returns:\n            A dict of {email: domain_result}\n        \"\"\"\n        api_name = 'opendns-whois-emails'\n        fmt_url_path = u'whois/emails/{0}'\n        return self._multi_get(api_name, fmt_url_path, emails)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling WHOIS Nameserver end point", "response": "def whois_nameservers(self, nameservers):\n        \"\"\"Calls WHOIS Nameserver end point\n\n        Args:\n            emails: An enumerable of nameservers\n        Returns:\n            A dict of {nameserver: domain_result}\n        \"\"\"\n        api_name = 'opendns-whois-nameservers'\n        fmt_url_path = u'whois/nameservers/{0}'\n        return self._multi_get(api_name, fmt_url_path, nameservers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall WHOIS domain end point", "response": "def whois_domains(self, domains):\n        \"\"\"Calls WHOIS domain end point\n\n        Args:\n            domains: An enumerable of domains\n        Returns:\n            A dict of {domain: domain_result}\n        \"\"\"\n        api_name = 'opendns-whois-domain'\n        fmt_url_path = u'whois/{0}'\n        return self._multi_get(api_name, fmt_url_path, domains)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef whois_domains_history(self, domains):\n        api_name = 'opendns-whois-domain-history'\n        fmt_url_path = u'whois/{0}/history'\n        return self._multi_get(api_name, fmt_url_path, domains)", "response": "Calls WHOIS domain history end point"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the domains related to input domains.", "response": "def cooccurrences(self, domains):\n        \"\"\"Get the domains related to input domains.\n\n        Args:\n            domains: an enumerable of strings domain names\n        Returns:\n            An enumerable of string domain names\n        \"\"\"\n        api_name = 'opendns-cooccurrences'\n        fmt_url_path = u'recommendations/name/{0}.json'\n        return self._multi_get(api_name, fmt_url_path, domains)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef domain_tag(self, domains):\n        api_name = 'opendns-domain_tag'\n        fmt_url_path = u'domains/{0}/latest_tags'\n        return self._multi_get(api_name, fmt_url_path, domains)", "response": "Get the data range when a domain is part of OpenDNS block list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef related_domains(self, domains):\n        api_name = 'opendns-related_domains'\n        fmt_url_path = u'links/name/{0}.json'\n        return self._multi_get(api_name, fmt_url_path, domains)", "response": "Get list of domain names that have been seen around the\n            same time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the domains related to input ips.", "response": "def rr_history(self, ips):\n        \"\"\"Get the domains related to input ips.\n\n        Args:\n            ips: an enumerable of strings as ips\n        Returns:\n            An enumerable of resource records and features\n        \"\"\"\n        api_name = 'opendns-rr_history'\n        fmt_url_path = u'dnsdb/ip/a/{0}.json'\n        return self._multi_get(api_name, fmt_url_path, ips)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dns_rr(self, ips):\n        api_name = 'opendns-dns_rr'\n        fmt_url_path = u'dnsdb/name/a/{0}.json'\n        return self._multi_get(api_name, fmt_url_path, ips)", "response": "Get the resource records related to input domains."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the information about a sample based on its hash.", "response": "def sample(self, hashes):\n        \"\"\"Get the information about a sample based on its hash.\n\n        Args:\n            hashes: an enumerable of strings as hashes\n        Returns:\n            An enumerable of arrays which contains the information\n            about the original samples\n        \"\"\"\n        api_name = 'opendns-sample'\n        fmt_url_path = u'sample/{0}'\n        return self._multi_get(api_name, fmt_url_path, hashes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search(self, patterns, start=30, limit=1000, include_category=False):\n        api_name = 'opendns-patterns'\n        fmt_url_path = u'search/{0}'\n        start = '-{0}days'.format(start)\n        include_category = str(include_category).lower()\n        query_params = {\n            'start': start,\n            'limit': limit,\n            'includecategory': include_category,\n        }\n        return self._multi_get(api_name, fmt_url_path, patterns, query_params)", "response": "Performs pattern searches against the Investigate database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms Umbrella risk score analysis on the input domains.", "response": "def risk_score(self, domains):\n        \"\"\"Performs Umbrella risk score analysis on the input domains\n\n        Args:\n            domains: an enumerable of domains\n        Returns:\n            An enumerable of associated domain risk scores\n        \"\"\"\n        api_name = 'opendns-risk_score'\n        fmt_url_path = u'domains/risk-score/{0}'\n        return self._multi_get(api_name, fmt_url_path, domains)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_all_responses(self, resources, api_endpoint, api_name):\n        all_responses, resources = self._bulk_cache_lookup(api_name, resources)\n        resource_chunks = self._prepare_resource_chunks(resources)\n        response_chunks = self._request_reports(\"resource\", resource_chunks, api_endpoint)\n        self._extract_response_chunks(all_responses, response_chunks, api_name)\n\n        return all_responses", "response": "Aux function to extract all the API endpoint responses."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve a report about the behaviour of a file.", "response": "def get_file_behaviour(self, resources):\n        \"\"\"Retrieves a report about the behaviour of a md5, sha1, and/or sha2 hash of\n        a file when executed in a sandboxed environment (Cuckoo sandbox).\n\n        Args:\n            resources: list of string hashes.\n        \"\"\"\n        api_name = 'virustotal-file-behaviour'\n        api_endpoint = 'file/behaviour'\n        return self._extract_all_responses(resources, api_endpoint, api_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file_download(self, resources):\n        api_name = 'virustotal-file-download'\n        api_endpoint = 'file/download'\n        return self._extract_all_responses(resources, api_endpoint, api_name)", "response": "Retrieves a file from its a md5 sha1 and sha2 hash."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a report about the network traffic of a file.", "response": "def get_file_network_traffic(self, resources):\n        \"\"\"Retrieves a report about the network traffic of a md5, sha1, and/or sha2 hash of\n           file, when it is executed.\n\n        Args:\n            resources: list of string hashes.\n        \"\"\"\n        api_name = 'virustotal-file-network-traffic'\n        api_endpoint = 'file/network-traffic'\n        return self._extract_all_responses(resources, api_endpoint, api_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the most recent VT report for a set of domains.", "response": "def get_domain_reports(self, domains):\n        \"\"\"Retrieves the most recent VT info for a set of domains.\n\n        Args:\n            domains: list of string domains.\n        Returns:\n            A dict with the domain as key and the VT report as value.\n        \"\"\"\n        api_name = 'virustotal-domain-reports'\n\n        (all_responses, domains) = self._bulk_cache_lookup(api_name, domains)\n        responses = self._request_reports(\"domain\", domains, 'domain/report')\n\n        for domain, response in zip(domains, responses):\n            if self._cache:\n                self._cache.cache_value(api_name, domain, response)\n            all_responses[domain] = response\n\n        return all_responses"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a live feed with the latest URLs submitted to VT.", "response": "def get_url_distribution(self, params=None):\n        \"\"\"Retrieves a live feed with the latest URLs submitted to VT.\n\n        Args:\n            resources: a dictionary with name and value for optional arguments\n        Returns:\n            A dict with the VT report.\n        \"\"\"\n        params = params or {}\n        all_responses = {}\n        api_name = 'virustotal-url-distribution'\n\n        response_chunks = self._request_reports(list(params.keys()), list(params.values()), 'url/distribution')\n        self._extract_response_chunks(all_responses, response_chunks, api_name)\n\n        return all_responses"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a scan report on a given URL.", "response": "def get_url_reports(self, resources):\n        \"\"\"Retrieves a scan report on a given URL.\n\n        Args:\n            resources: list of URLs.\n        Returns:\n            A dict with the URL as key and the VT report as value.\n        \"\"\"\n        api_name = 'virustotal-url-reports'\n\n        (all_responses, resources) = self._bulk_cache_lookup(api_name, resources)\n        resource_chunks = self._prepare_resource_chunks(resources, '\\n')\n        response_chunks = self._request_reports(\"resource\", resource_chunks, 'url/report')\n        self._extract_response_chunks(all_responses, response_chunks, api_name)\n\n        return all_responses"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the most recent VT report for a set of ips.", "response": "def get_ip_reports(self, ips):\n        \"\"\"Retrieves the most recent VT info for a set of ips.\n\n        Args:\n            ips: list of IPs.\n        Returns:\n            A dict with the IP as key and the VT report as value.\n        \"\"\"\n        api_name = 'virustotal-ip-address-reports'\n\n        (all_responses, ips) = self._bulk_cache_lookup(api_name, ips)\n        responses = self._request_reports(\"ip\", ips, 'ip-address/report')\n\n        for ip, response in zip(ips, responses):\n            if self._cache:\n                self._cache.cache_value(api_name, ip, response)\n            all_responses[ip] = response\n\n        return all_responses"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming advanced search on samples matching certain binary metadata criteria.", "response": "def get_file_search(self, query):\n        \"\"\"Performs advanced search on samples, matching certain binary/\n           metadata/detection criteria.\n           Possible queries: file size, file type, first or last submission to\n            VT, number of positives, bynary content, etc.\n\n        Args:\n            query: dictionary with search arguments\n            Example: 'query': 'type:peexe size:90kb+ positives:5+ behaviour:\"taskkill\"'\n        Returns:\n            A dict with the VT report.\n        \"\"\"\n        api_name = 'virustotal-file-search'\n\n        (all_responses, query) = self._bulk_cache_lookup(api_name, query)\n        response_chunks = self._request_reports(\"query\", query, 'file/search')\n        self._extract_response_chunks(all_responses, response_chunks, api_name)\n\n        return all_responses"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_file_clusters(self, date):\n        api_name = 'virustotal-file-clusters'\n\n        (all_responses, resources) = self._bulk_cache_lookup(api_name, date)\n        response = self._request_reports(\"date\", date, 'file/clusters')\n        self._extract_response_chunks(all_responses, response, api_name)\n\n        return all_responses", "response": "Retrieves file similarity clusters for a given time frame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prepare_resource_chunks(self, resources, resource_delim=','):\n        return [self._prepare_resource_chunk(resources, resource_delim, pos)\n                for pos in range(0, len(resources), self._resources_per_req)]", "response": "This method is used to prepare a list of concatenated resources at once according to the maximum number of resources per request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending multiples requests for the resources to a particular endpoint.", "response": "def _request_reports(self, resource_param_name, resources, endpoint_name):\n        \"\"\"Sends multiples requests for the resources to a particular endpoint.\n\n        Args:\n            resource_param_name: a string name of the resource parameter.\n            resources: list of of the resources.\n            endpoint_name: VirusTotal endpoint URL suffix.\n        Returns:\n            A list of the responses.\n        \"\"\"\n        params = [{resource_param_name: resource, 'apikey': self._api_key} for resource in resources]\n        return self._requests.multi_get(self.BASE_DOMAIN + endpoint_name, query_params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_response_chunks(self, all_responses, response_chunks, api_name):\n        for response_chunk in response_chunks:\n            if not isinstance(response_chunk, list):\n                response_chunk = [response_chunk]\n            for response in response_chunk:\n                if not response:\n                    continue\n\n                if self._cache:\n                    self._cache.cache_value(api_name, response['resource'], response)\n                all_responses[response['resource']] = response", "response": "Extracts and caches the responses from the response chunks in case\n        is set to True."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_editor_widget(self, request, plugins, plugin):\n        cancel_url_name = self.get_admin_url_name('delete_on_cancel')\n        cancel_url = reverse('admin:%s' % cancel_url_name)\n\n        render_plugin_url_name = self.get_admin_url_name('render_plugin')\n        render_plugin_url = reverse('admin:%s' % render_plugin_url_name)\n\n        action_token = self.get_action_token(request, plugin)\n\n        # should we delete the text plugin when\n        # the user cancels?\n        delete_text_on_cancel = (\n            'delete-on-cancel' in request.GET and  # noqa\n            not plugin.get_plugin_instance()[0]\n        )\n\n        widget = TextEditorWidget(\n            installed_plugins=plugins, pk=plugin.pk,\n            placeholder=plugin.placeholder,\n            plugin_language=plugin.language,\n            configuration=self.ckeditor_configuration,\n            render_plugin_url=render_plugin_url,\n            cancel_url=cancel_url,\n            action_token=action_token,\n            delete_on_cancel=delete_text_on_cancel,\n        )\n        return widget", "response": "Returns the Django form Widget to be used for the text area."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a subclass of Form to be used by this plugin", "response": "def get_form_class(self, request, plugins, plugin):\n        \"\"\"\n        Returns a subclass of Form to be used by this plugin\n        \"\"\"\n        widget = self.get_editor_widget(\n            request=request,\n            plugins=plugins,\n            plugin=plugin,\n        )\n\n        instance = plugin.get_plugin_instance()[0]\n\n        if instance:\n            context = RequestContext(request)\n            context['request'] = request\n            rendered_text = plugin_tags_to_admin_html(\n                text=instance.body,\n                context=context,\n            )\n        else:\n            rendered_text = None\n\n        # We avoid mutating the Form declared above by subclassing\n        class TextPluginForm(self.form):\n            body = CharField(widget=widget, required=False)\n\n            def __init__(self, *args, **kwargs):\n                initial = kwargs.pop('initial', {})\n\n                if rendered_text:\n                    initial['body'] = rendered_text\n                super(TextPluginForm, self).__init__(*args, initial=initial, **kwargs)\n        return TextPluginForm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting plugin object tags into HTML.", "response": "def _plugin_tags_to_html(text, output_func):\n    \"\"\"\n    Convert plugin object 'tags' into the form for public site.\n\n    context is the template context to use, placeholder is the placeholder name\n    \"\"\"\n    plugins_by_id = get_plugins_from_text(text)\n\n    def _render_tag(m):\n        try:\n            plugin_id = int(m.groupdict()['pk'])\n            obj = plugins_by_id[plugin_id]\n        except KeyError:\n            # Object must have been deleted.  It cannot be rendered to\n            # end user so just remove it from the HTML altogether\n            return u''\n        else:\n            obj._render_meta.text_enabled = True\n            return output_func(obj, m)\n    return OBJ_ADMIN_RE.sub(_render_tag, text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncleans HTML from XSS vulnerabilities using html5lib", "response": "def clean_html(data, full=True, parser=DEFAULT_PARSER):\n    \"\"\"\n    Cleans HTML from XSS vulnerabilities using html5lib\n    If full is False, only the contents inside <body> will be returned (without\n    the <body> tags).\n    \"\"\"\n    if full:\n        dom_tree = parser.parse(data)\n    else:\n        dom_tree = parser.parseFragment(data)\n    walker = treewalkers.getTreeWalker('dom')\n    kwargs = _filter_kwargs()\n    stream = TextSanitizer(walker(dom_tree), **kwargs)\n    s = serializer.HTMLSerializer(\n        omit_optional_tags=False,\n        quote_attr_values='always',\n    )\n    return u''.join(s.serialize(stream))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_images(data, plugin):\n    if not settings.TEXT_SAVE_IMAGE_FUNCTION:\n        return data\n    tree_builder = html5lib.treebuilders.getTreeBuilder('dom')\n    parser = html5lib.html5parser.HTMLParser(tree=tree_builder)\n    dom = parser.parse(data)\n    found = False\n    for img in dom.getElementsByTagName('img'):\n        src = img.getAttribute('src')\n        if not src.startswith('data:'):\n            # nothing to do\n            continue\n        width = img.getAttribute('width')\n        height = img.getAttribute('height')\n        # extract the image data\n        data_re = re.compile(r'data:(?P<mime_type>[^\"]*);(?P<encoding>[^\"]*),(?P<data>[^\"]*)')\n        m = data_re.search(src)\n        dr = m.groupdict()\n        mime_type = dr['mime_type']\n        image_data = dr['data']\n        if mime_type.find(';'):\n            mime_type = mime_type.split(';')[0]\n        try:\n            image_data = base64.b64decode(image_data)\n        except Exception:\n            image_data = base64.urlsafe_b64decode(image_data)\n        try:\n            image_type = mime_type.split('/')[1]\n        except IndexError:\n            # No image type specified -- will convert to jpg below if it's valid image data\n            image_type = ''\n        image = BytesIO(image_data)\n        # genarate filename and normalize image format\n        if image_type == 'jpg' or image_type == 'jpeg':\n            file_ending = 'jpg'\n        elif image_type == 'png':\n            file_ending = 'png'\n        elif image_type == 'gif':\n            file_ending = 'gif'\n        else:\n            # any not \"web-safe\" image format we try to convert to jpg\n            im = Image.open(image)\n            new_image = BytesIO()\n            file_ending = 'jpg'\n            im.save(new_image, 'JPEG')\n            new_image.seek(0)\n            image = new_image\n        filename = u'%s.%s' % (uuid.uuid4(), file_ending)\n        # transform image into a cms plugin\n        image_plugin = img_data_to_plugin(\n            filename, image, parent_plugin=plugin, width=width, height=height\n        )\n        # render the new html for the plugin\n        new_img_html = plugin_to_tag(image_plugin)\n        # replace the original image node with the newly created cms plugin html\n        img.parentNode.replaceChild(parser.parseFragment(new_img_html).childNodes[0], img)\n        found = True\n    if found:\n        return u''.join([y.toxml() for y in dom.getElementsByTagName('body')[0].childNodes])\n    else:\n        return data", "response": "Extracts base64 encoded images from drag and drop actions in browser and saves them as plugins\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_config_filename(root_dir=None):\n        root_dir = Path(root_dir) if root_dir else Path('.').abspath()\n        locale_dir = root_dir / 'locale'\n        if not os.path.exists(locale_dir):\n            locale_dir = root_dir / 'conf' / 'locale'\n        return locale_dir / BASE_CONFIG_FILENAME", "response": "Returns the default name of the configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the config file and returns the data as dict", "response": "def read_config(self, filename):\n        \"\"\"\n        Returns data found in config file (as dict), or raises exception if file not found\n        \"\"\"\n        if not os.path.exists(filename):\n            raise Exception(\"Configuration file cannot be found: %s\" % filename)\n        with io.open(filename, encoding='UTF-8') as stream:\n            return yaml.safe_load(stream)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rtl_langs(self):\n        def is_rtl(lang):\n            \"\"\"\n            Returns True if lang is a RTL language\n\n            args:\n                lang (str): The language to be checked\n\n            Returns:\n                True if lang is an RTL language.\n            \"\"\"\n            # Base RTL langs are Arabic, Farsi, Hebrew, and Urdu\n            base_rtl = ['ar', 'fa', 'he', 'ur']\n\n            # do this to capture both 'fa' and 'fa_IR'\n            return any([lang.startswith(base_code) for base_code in base_rtl])\n\n        return sorted(set([lang for lang in self.translated_locales if is_rtl(lang)]))", "response": "Returns the set of RTL language codes present in self. locales."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean_conf_folder(self, locale):\n        dirname = self.configuration.get_messages_dir(locale)\n        dirname.removedirs_p()", "response": "Remove the configuration directory for locale"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, args):\n        changes_detected = self.detect_changes()\n        message = self.get_message(changes_detected)\n        print(message)\n        return int(changes_detected)", "response": "This is the main entry point of the script."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsegmenting all the pofiles for locale.", "response": "def segment_pofiles(configuration, locale):\n    \"\"\"Segment all the pofiles for `locale`.\n\n    Returns a set of filenames, all the segment files written.\n\n    \"\"\"\n    files_written = set()\n    for filename, segments in configuration.segment.items():\n        filename = configuration.get_messages_dir(locale) / filename\n        files_written.update(segment_pofile(filename, segments))\n    return files_written"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef segment_pofile(filename, segments):\n    reading_msg = \"Reading {num} entries from {file}\"\n    writing_msg = \"Writing {num} entries to {file}\"\n    source_po = polib.pofile(filename)\n    LOG.info(reading_msg.format(file=filename, num=len(source_po)))  # pylint: disable=logging-format-interpolation\n\n    # A new pofile just like the source, but with no messages. We'll put\n    # anything not segmented into this file.\n    remaining_po = copy.deepcopy(source_po)\n    remaining_po[:] = []\n\n    # Turn the segments dictionary into two structures: segment_patterns is a\n    # list of (pattern, segmentfile) pairs.  segment_po_files is a dict mapping\n    # segment file names to pofile objects of their contents.\n    segment_po_files = {filename: remaining_po}\n    segment_patterns = []\n    for segmentfile, patterns in segments.items():\n        segment_po_files[segmentfile] = copy.deepcopy(remaining_po)\n        segment_patterns.extend((pat, segmentfile) for pat in patterns)\n\n    # Examine each message in the source file. If all of its occurrences match\n    # a pattern for the same segment, it goes in that segment.  Otherwise, it\n    # goes in remaining.\n    for msg in source_po:\n        msg_segments = set()\n        for occ_file, _ in msg.occurrences:\n            for pat, segment_file in segment_patterns:\n                if fnmatch.fnmatch(occ_file, pat):\n                    msg_segments.add(segment_file)\n                    break\n            else:\n                msg_segments.add(filename)\n\n        assert msg_segments\n        if len(msg_segments) == 1:\n            # This message belongs in this segment.\n            segment_file = msg_segments.pop()\n            segment_po_files[segment_file].append(msg)\n        else:\n            # It's in more than one segment, so put it back in the main file.\n            remaining_po.append(msg)\n\n    # Write out the results.\n    files_written = set()\n    for segment_file, pofile in segment_po_files.items():\n        out_file = filename.dirname() / segment_file\n        if not pofile:\n            LOG.error(\"No messages to write to %s, did you run segment twice?\", out_file)\n        else:\n            LOG.info(writing_msg.format(file=out_file, num=len(pofile)))  # pylint: disable=logging-format-interpolation\n            pofile.save(out_file)\n            files_written.add(out_file)\n\n    return files_written", "response": "Segment a. po file into a set of segment files."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the parser for the section.", "response": "def get_parser(parser):\n    \"\"\"\n    Grabs the parser.\n\n    args:\n        parser: The parser\n    \"\"\"\n    parser.description = textwrap.dedent(\"\"\"\n        Segment the .po files in LOCALE(s) based on the segmenting rules in\n        config.yaml.\n\n        Note that segmenting is *not* idempotent: it modifies the input file, so\n        be careful that you don't run it twice on the same file.\n    \"\"\".strip())\n    parser.add_argument(\"locale\", nargs=\"+\", help=\"a locale to segment\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd arguments to the parser", "response": "def add_args(self):\n        \"\"\"\n        Adds arguments\n        \"\"\"\n        self.parser.description = textwrap.dedent(\"\"\"\n        Segment the .po files in LOCALE(s) based on the segmenting rules in\n        config.yaml.\n\n        Note that segmenting is *not* idempotent: it modifies the input file, so\n        be careful that you don't run it twice on the same file.\n        \"\"\".strip())\n        self.parser.add_argument(\"locale\", nargs=\"+\", help=\"a locale to segment\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fix_header(pofile):\n\n    # By default, django-admin.py makemessages creates this header:\n    #\n    #   SOME DESCRIPTIVE TITLE.\n    #   Copyright (C) YEAR THE PACKAGE'S COPYRIGHT HOLDER\n    #   This file is distributed under the same license as the PACKAGE package.\n    #   FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.\n\n    pofile.metadata_is_fuzzy = []   # remove [u'fuzzy']\n    header = pofile.header\n    fixes = (\n        ('SOME DESCRIPTIVE TITLE', EDX_MARKER),\n        ('Translations template for PROJECT.', EDX_MARKER),\n        ('YEAR', str(datetime.utcnow().year)),\n        ('ORGANIZATION', 'edX'),\n        (\"THE PACKAGE'S COPYRIGHT HOLDER\", \"EdX\"),\n        (\n            'This file is distributed under the same license as the PROJECT project.',\n            'This file is distributed under the GNU AFFERO GENERAL PUBLIC LICENSE.'\n        ),\n        (\n            'This file is distributed under the same license as the PACKAGE package.',\n            'This file is distributed under the GNU AFFERO GENERAL PUBLIC LICENSE.'\n        ),\n        ('FIRST AUTHOR <EMAIL@ADDRESS>', 'EdX Team <info@edx.org>'),\n    )\n    for src, dest in fixes:\n        header = header.replace(src, dest)\n    pofile.header = header", "response": "Fixes the header of the pofile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef strip_key_strings(pofile):\n    newlist = [entry for entry in pofile if not is_key_string(entry.msgid)]\n    del pofile[:]\n    pofile += newlist", "response": "Removes all entries in PO which are key strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef base(self, path1, *paths):\n        root_dir = self.configuration.root_dir\n        return root_dir.relpathto(path1.joinpath(*paths))", "response": "Return a relative path from config. BASE_DIR to path1"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenames a file in the source directory.", "response": "def rename_source_file(self, src, dst):\n        \"\"\"\n        Rename a file in the source directory.\n        \"\"\"\n        try:\n            os.rename(self.source_msgs_dir.joinpath(src), self.source_msgs_dir.joinpath(dst))\n        except OSError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_valid_commands():\n    modules = [m.basename().split('.')[0] for m in Path(__file__).dirname().files('*.py')]\n    commands = []\n    for modname in modules:\n        if modname == 'main':\n            continue\n        mod = importlib.import_module('i18n.%s' % modname)\n        if hasattr(mod, 'main'):\n            commands.append(modname)\n    return commands", "response": "Returns a list of valid commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites out an error message specifying the valid commands.", "response": "def error_message():\n    \"\"\"\n    Writes out error message specifying the valid commands.\n\n    Returns:\n        Failure code for system exit\n    \"\"\"\n    sys.stderr.write('valid commands:\\n')\n    for cmd in get_valid_commands():\n        sys.stderr.write('\\t%s\\n' % cmd)\n    return -1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute the given command. Returns error_message if command is not valid. Returns: Output of the given command or error message if command is not valid.", "response": "def main():\n    \"\"\"\n    Executes the given command. Returns error_message if command is not valid.\n\n    Returns:\n        Output of the given command or error message if command is not valid.\n    \"\"\"\n    try:\n        command = sys.argv[1]\n    except IndexError:\n        return error_message()\n\n    try:\n        module = importlib.import_module('i18n.%s' % command)\n        module.main.args = sys.argv[2:]\n    except (ImportError, AttributeError):\n        return error_message()\n\n    return module.main()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_po_files(configuration, locale_dir, root_dir=None, report_empty=False, check_all=False):\n    found_problems = False\n\n    # List of .po files that are the product of a merge (see generate.py).\n    merged_files = configuration.generate_merge.keys()\n\n    for dirpath, __, filenames in os.walk(root_dir if root_dir else locale_dir):\n        for name in filenames:\n            __, ext = os.path.splitext(name)\n            filename = os.path.join(dirpath, name)\n\n            # Validate only .po files that are not product of a merge (see generate.py) unless check_all is true.\n            # If django-partial.po has a problem, then django.po will also, so don't report it.\n            if ext.lower() == '.po' and (check_all or os.path.basename(filename) not in merged_files):\n\n                # First validate the format of this file\n                if msgfmt_check_po_file(locale_dir, filename):\n                    found_problems = True\n\n                # Check that the translated strings are valid, and optionally\n                # check for empty translations. But don't check English.\n                if \"/locale/en/\" not in filename:\n                    problems = check_messages(filename, report_empty)\n                    if problems:\n                        report_problems(filename, problems)\n                        found_problems = True\n\n                    dup_filename = filename.replace('.po', '.dup')\n                    has_duplicates = os.path.exists(dup_filename)\n                    if has_duplicates:\n                        log.warning(\" Duplicates found in %s, details in .dup file\", dup_filename)\n                        found_problems = True\n\n                    if not (problems or has_duplicates):\n                        log.info(\" No problems found in %s\", filename)\n\n    return found_problems", "response": "Validate all of the. po files in the root directory that are not product of a merge."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef msgfmt_check_po_file(locale_dir, filename):\n    found_problems = False\n\n    # Use relative paths to make output less noisy.\n    rfile = os.path.relpath(filename, locale_dir)\n    out, err = call('msgfmt -c -o /dev/null {}'.format(rfile), working_directory=locale_dir)\n    if err:\n        log.info(u'\\n' + out.decode('utf8'))\n        log.warning(u'\\n' + err.decode('utf8'))\n        found_problems = True\n\n    return found_problems", "response": "Check if a. po file is valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the set of tags in a message string.", "response": "def tags_in_string(msg):\n    \"\"\"\n    Return the set of tags in a message string.\n\n    Tags includes HTML tags, data placeholders, etc.\n\n    Skips tags that might change due to translations: HTML entities, <abbr>,\n    and so on.\n\n    \"\"\"\n    def is_linguistic_tag(tag):\n        \"\"\"Is this tag one that can change with the language?\"\"\"\n        if tag.startswith(\"&\"):\n            return True\n        if any(x in tag for x in [\"<abbr>\", \"<abbr \", \"</abbr>\"]):\n            return True\n        return False\n\n    __, tags = Converter().detag_string(msg)\n    return set(t for t in tags if not is_linguistic_tag(t))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndoes msg have characters outside the Basic Multilingual Plane?", "response": "def astral(msg):\n    \"\"\"Does `msg` have characters outside the Basic Multilingual Plane?\"\"\"\n    # Python2 narrow builds present astral characters as surrogate pairs.\n    # By encoding as utf32, and decoding DWORDS, we can get at the real code\n    # points.\n    utf32 = msg.encode(\"utf32\")[4:]         # [4:] to drop the bom\n    code_points = struct.unpack(\"%dI\" % (len(utf32) / 4), utf32)\n    return any(cp > 0xFFFF for cp in code_points)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the messages in filename for BMP characters and translates them into a list of tuples.", "response": "def check_messages(filename, report_empty=False):\n    \"\"\"\n    Checks messages in `filename` in various ways:\n\n    * Translations must have the same slots as the English.\n\n    * Messages can't have astral characters in them.\n\n    If `report_empty` is True, will also report empty translation strings.\n\n    Returns the problems, a list of tuples. Each is a description, a msgid, and\n    then zero or more translations.\n\n    \"\"\"\n    problems = []\n    pomsgs = polib.pofile(filename)\n    for msg in pomsgs:\n        # Check for characters Javascript can't support.\n        # https://code.djangoproject.com/ticket/21725\n        if astral(msg.msgstr):\n            problems.append((\"Non-BMP char\", msg.msgid, msg.msgstr))\n\n        if is_format_message(msg):\n            # LONG_DATE_FORMAT, etc, have %s etc in them, and that's ok.\n            continue\n\n        if msg.msgid_plural:\n            # Plurals: two strings in, N strings out.\n            source = msg.msgid + \" | \" + msg.msgid_plural\n            translation = \" | \".join(v for k, v in sorted(msg.msgstr_plural.items()))\n            empty = any(not t.strip() for t in msg.msgstr_plural.values())\n        else:\n            # Singular: just one string in and one string out.\n            source = msg.msgid\n            translation = msg.msgstr\n            empty = not msg.msgstr.strip()\n\n        if empty:\n            if report_empty:\n                problems.append((\"Empty translation\", source))\n        else:\n            id_tags = tags_in_string(source)\n            tx_tags = tags_in_string(translation)\n\n            # Check if tags don't match\n            if id_tags != tx_tags:\n                id_has = u\", \".join(sorted(u'\"{}\"'.format(t) for t in id_tags - tx_tags))\n                tx_has = u\", \".join(sorted(u'\"{}\"'.format(t) for t in tx_tags - id_tags))\n                if id_has and tx_has:\n                    diff = u\"{} vs {}\".format(id_has, tx_has)\n                elif id_has:\n                    diff = u\"{} missing\".format(id_has)\n                else:\n                    diff = u\"{} added\".format(tx_has)\n                problems.append((\n                    \"Different tags in source and translation\",\n                    source,\n                    translation,\n                    diff\n                ))\n\n    return problems"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef report_problems(filename, problems):\n    problem_file = filename.replace(\".po\", \".prob\")\n    id_filler = textwrap.TextWrapper(width=79, initial_indent=\"  msgid: \", subsequent_indent=\" \" * 9)\n    tx_filler = textwrap.TextWrapper(width=79, initial_indent=\"  -----> \", subsequent_indent=\" \" * 9)\n    with codecs.open(problem_file, \"w\", encoding=\"utf8\") as prob_file:\n        for problem in problems:\n            desc, msgid = problem[:2]\n            prob_file.write(u\"{}\\n{}\\n\".format(desc, id_filler.fill(msgid)))\n            info = u\"{}\\n{}\\n\".format(desc, id_filler.fill(msgid))\n            for translation in problem[2:]:\n                prob_file.write(u\"{}\\n\".format(tx_filler.fill(translation)))\n                info += u\"{}\\n\".format(tx_filler.fill(translation))\n            log.info(info)\n            prob_file.write(u\"\\n\")\n\n    log.error(\" %s problems in %s, details in .prob file\", len(problems), filename)", "response": "Report on the problems found in filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, args):\n        exit_code = 0\n\n        if args.verbose:\n            log_level = logging.INFO\n        else:\n            log_level = logging.WARNING\n        logging.basicConfig(stream=sys.stdout, level=log_level)\n\n        languages = args.language or []\n        locale_dir = self.configuration.locale_dir\n\n        if not languages:\n            # validate all languages\n            if validate_po_files(self.configuration, locale_dir, report_empty=args.empty, check_all=args.check_all):\n                exit_code = 1\n        else:\n            # languages will be a list of language codes; test each language.\n            for language in languages:\n                root_dir = self.configuration.locale_dir / language\n                # Assert that a directory for this language code exists on the system\n                if not root_dir.isdir():\n                    log.error(\" %s is not a valid directory.\\nSkipping language '%s'\", root_dir, language)\n                    continue\n                # If we found the language code's directory, validate the files.\n                if validate_po_files(self.configuration, locale_dir, root_dir=root_dir, report_empty=args.empty,\n                                     check_all=args.check_all):\n                    exit_code = 1\n\n        return exit_code", "response": "Main entry point for script\nQuickValidate."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge(configuration, locale, target='django.po', sources=('django-partial.po',), fail_if_missing=True):\n    LOG.info('Merging %s locale %s', target, locale)\n    locale_directory = configuration.get_messages_dir(locale)\n    try:\n        validate_files(locale_directory, sources)\n    except Exception:  # pylint: disable=broad-except\n        if not fail_if_missing:\n            return\n        raise\n\n    # merged file is merged.po\n    merge_cmd = 'msgcat -o merged.po ' + ' '.join(sources)\n    execute(merge_cmd, working_directory=locale_directory)\n\n    # clean up redunancies in the metadata\n    merged_filename = locale_directory.joinpath('merged.po')\n    duplicate_entries = clean_pofile(merged_filename)\n\n    # rename merged.po -> django.po (default)\n    target_filename = locale_directory.joinpath(target)\n    os.rename(merged_filename, target_filename)\n\n    # Write duplicate messages to a file\n    if duplicate_entries:\n        dup_file = target_filename.replace(\".po\", \".dup\")\n        with codecs.open(dup_file, \"w\", encoding=\"utf8\") as dfile:\n            for (entry, translations) in duplicate_entries:\n                dfile.write(u\"{}\\n\".format(entry))\n                dfile.write(u\"Translations found were:\\n\\t{}\\n\\n\".format(translations))\n        LOG.warning(\" %s duplicates in %s, details in .dup file\", len(duplicate_entries), target_filename)", "response": "Merge the sources and target files into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_files(configuration, locale, fail_if_missing=True):\n    for target, sources in configuration.generate_merge.items():\n        merge(configuration, locale, target, sources, fail_if_missing)", "response": "Merge all the files in locale into a single tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncleans a. po file and return a list of duplicate entries.", "response": "def clean_pofile(pofile_path):\n    \"\"\"\n    Clean various aspect of a .po file.\n\n    Fixes:\n\n        - Removes the fuzzy flag on metadata.\n\n        - Removes occurrence line numbers so that the generated files don't\n          generate a lot of line noise when they're committed.\n\n    Returns a list of any duplicate entries found.\n    \"\"\"\n    # Reading in the .po file and saving it again fixes redundancies.\n    pomsgs = pofile(pofile_path)\n    # The msgcat tool marks the metadata as fuzzy, but it's ok as it is.\n    pomsgs.metadata_is_fuzzy = False\n    duplicate_entries = []\n\n    for entry in pomsgs:\n        # Remove line numbers\n        entry.occurrences = [(filename, None) for filename, __ in entry.occurrences]\n        # Check for merge conflicts. Pick the first, and emit a warning.\n        if 'fuzzy' in entry.flags:\n            # Remove fuzzy from flags\n            entry.flags = [f for f in entry.flags if f != 'fuzzy']\n            # Save a warning message\n            dup_msg = 'Multiple translations found for single string.\\n\\tString \"{0}\"\\n\\tPresent in files {1}'.format(\n                entry.msgid,\n                [f for (f, __) in entry.occurrences]\n            )\n            duplicate_entries.append((dup_msg, entry.msgstr))\n\n            # Pick the first entry\n            for msgstr in DUPLICATE_ENTRY_PATTERN.split(entry.msgstr):\n                # Ignore any empty strings that may result from the split call\n                if msgstr:\n                    # Set the first one we find to be the right one. Strip to remove extraneous\n                    # new lines that exist.\n                    entry.msgstr = msgstr.strip()\n\n                    # Raise error if there's new lines starting or ending the id string.\n                    if entry.msgid.startswith('\\n') or entry.msgid.endswith('\\n'):\n                        raise ValueError(\n                            u'{} starts or ends with a new line character, which is not allowed. '\n                            'Please fix before continuing. Source string is found in {}'.format(\n                                entry.msgid, entry.occurrences\n                            ).encode('utf-8')\n                        )\n                    break\n\n    pomsgs.save()\n    return duplicate_entries"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_files(directory, files_to_merge):\n    for file_path in files_to_merge:\n        pathname = directory.joinpath(file_path)\n        if not pathname.exists():\n            raise Exception(\"I18N: Cannot generate because file not found: {0}\".format(pathname))\n        # clean sources\n        clean_pofile(pathname)", "response": "Validate that the given files exist in the given directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, args):\n        logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n        configuration = self.configuration\n        if args.ltr:\n            langs = configuration.ltr_langs\n        elif args.rtl:\n            langs = configuration.rtl_langs\n        else:\n            langs = configuration.translated_locales\n\n        for locale in langs:\n            merge_files(configuration, locale, fail_if_missing=args.strict)\n        # Dummy text is not required. Don't raise exception if files are missing.\n        for locale in configuration.dummy_locales:\n            merge_files(configuration, locale, fail_if_missing=False)\n        # Merge the source locale, so we have the canonical .po files.\n        if configuration.source_locale not in langs:\n            merge_files(configuration, configuration.source_locale, fail_if_missing=args.strict)\n\n        compile_cmd = 'django-admin.py compilemessages -v{}'.format(args.verbose)\n        if args.verbose:\n            stderr = None\n        else:\n            stderr = DEVNULL\n        execute(compile_cmd, working_directory=configuration.root_dir, stderr=stderr)\n\n        # Check for any mapped languages and copy directories around accordingly\n        for source_locale, dest_locale in configuration.edx_lang_map.items():\n            source_dirname = configuration.get_messages_dir(source_locale)\n            dest_dirname = configuration.get_messages_dir(dest_locale)\n            LOG.info(\"Copying mapped locale %s to %s\", source_dirname, dest_dirname)\n\n            path.rmtree_p(path(dest_dirname))\n            path.copytree(path(source_dirname), path(dest_dirname))", "response": "Main entry point for script\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_dummy(filename, locale, converter):\n    if not Path(filename).exists():\n        raise IOError('File does not exist: %r' % filename)\n    pofile = polib.pofile(filename)\n    for msg in pofile:\n        # Some strings are actually formatting strings, don't dummy-ify them,\n        # or dates will look like \"D\u00c0T\u00c9_T\u00ccM\u00c9_F\u00d6RM\u00c0T \u2c60'\u03c3# EST\"\n        if is_format_message(msg):\n            continue\n        converter.convert_msg(msg)\n\n    pofile.metadata['Language'] = locale\n\n    # Apply declaration for English pluralization rules so that ngettext will\n    # do something reasonable.\n    pofile.metadata['Plural-Forms'] = 'nplurals=2; plural=(n != 1);'\n\n    new_file = new_filename(filename, locale)\n    new_file.parent.makedirs_p()\n    pofile.save(new_file)\n    clean_pofile(new_file)", "response": "Takes a source po file reads it writes out a new po file containing a dummy translation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a filename derived from original_filename using new_locale as the locale", "response": "def new_filename(original_filename, new_locale):\n    \"\"\"Returns a filename derived from original_filename, using new_locale as the locale\"\"\"\n    orig_file = Path(original_filename)\n    new_file = orig_file.parent.parent.parent / new_locale / orig_file.parent.name / orig_file.name\n    return new_file.abspath()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting one POEntry object and converts it to a polib. POEntry object.", "response": "def convert_msg(self, msg):\n        \"\"\"\n        Takes one POEntry object and converts it (adds a dummy translation to it)\n        msg is an instance of polib.POEntry\n        \"\"\"\n        source = msg.msgid\n        if not source:\n            # don't translate empty string\n            return\n\n        plural = msg.msgid_plural\n        if plural:\n            # translate singular and plural\n            foreign_single = self.convert(source)\n            foreign_plural = self.convert(plural)\n            plural = {\n                '0': self.final_newline(source, foreign_single),\n                '1': self.final_newline(plural, foreign_plural),\n            }\n            msg.msgstr_plural = plural\n        else:\n            foreign = self.convert(source)\n            msg.msgstr = self.final_newline(source, foreign)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npadding a string to fit the lorem ipsum text.", "response": "def pad(self, string):\n        \"\"\"\n        Add some lorem ipsum text to the end of string to simulate more verbose languages (like German).\n        Padding factor extrapolated by guidelines at http://www.w3.org/International/articles/article-text-size.en\n        \"\"\"\n        size = len(string)\n        target = size * (4.75 - size ** 0.27)\n        pad_len = int(target) - size\n        return string + self.LOREM[:pad_len] + \"#\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate dummy strings for all source po files.", "response": "def run(self, args):\n        \"\"\"\n        Generate dummy strings for all source po files.\n        \"\"\"\n        configuration = self.configuration\n        source_messages_dir = configuration.source_messages_dir\n        for locale, converter in zip(configuration.dummy_locales, [Dummy(), Dummy2(), ArabicDummy()]):\n            print('Processing source language files into dummy strings, locale \"{}\"'.format(locale))\n            for source_file in configuration.source_messages_dir.walkfiles('*.po'):\n                if args.verbose:\n                    print('   ', source_file.relpath())\n                make_dummy(source_messages_dir.joinpath(source_file), locale, converter)\n        if args.verbose:\n            print()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a shell command in a given working_directory.", "response": "def execute(command, working_directory=config.BASE_DIR, stderr=sp.STDOUT):\n    \"\"\"\n    Executes shell command in a given working_directory.\n    Command is a string to pass to the shell.\n    Output is ignored.\n    \"\"\"\n    LOG.info(\"Executing in %s ...\", working_directory)\n    LOG.info(command)\n    sp.check_call(command, cwd=working_directory, stderr=stderr, shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting shell command in a given working_directory.", "response": "def call(command, working_directory=config.BASE_DIR):\n    \"\"\"\n    Executes shell command in a given working_directory.\n    Command is a list of strings to execute as a command line.\n    Returns a tuple of two byte strings: (stdout, stderr)\n\n    \"\"\"\n    LOG.info(command)\n    proc = sp.Popen(command, stdout=sp.PIPE, stderr=sp.PIPE, cwd=working_directory, shell=True)\n    out, err = proc.communicate()\n    return (out, err)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_file(filename, verbose=True):\n    if verbose:\n        LOG.info('Deleting file %s', os.path.relpath(filename, config.BASE_DIR))\n    if not os.path.exists(filename):\n        LOG.warning(\"File does not exist: %s\", os.path.relpath(filename, config.BASE_DIR))\n    else:\n        os.remove(filename)", "response": "Remove a file from the base directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npushes all the source English files to Transifex.", "response": "def push(*resources):\n    \"\"\"\n    Push translation source English files to Transifex.\n\n    Arguments name specific resources to push. Otherwise, push all the source\n    files.\n    \"\"\"\n    cmd = 'tx push -s'\n    if resources:\n        for resource in resources:\n            execute(cmd + ' -r {resource}'.format(resource=resource))\n    else:\n        execute(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pull(configuration, *resources):\n    print(\"Pulling conf/locale/config.yaml:locales from Transifex...\")\n\n    for lang in configuration.translated_locales:\n        cmd = 'tx pull -f --mode=reviewed --minimum-perc=3 -l {lang}'.format(lang=lang)\n        if resources:\n            for resource in resources:\n                execute(cmd + ' -r {resource}'.format(resource=resource))\n        else:\n            execute(cmd)\n    clean_translated_locales(configuration)", "response": "Pull all translations from all languages listed in conf. yaml\n    and clean them up."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pull_all_ltr(configuration):\n    print(\"Pulling all translated LTR languages from transifex...\")\n    for lang in configuration.ltr_langs:\n        print('rm -rf conf/locale/' + lang)\n        execute('rm -rf conf/locale/' + lang)\n        execute('tx pull -l ' + lang)\n    clean_translated_locales(configuration, langs=configuration.ltr_langs)", "response": "Pulls all translations from transifex"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pull_all_rtl(configuration):\n    print(\"Pulling all translated RTL languages from transifex...\")\n    for lang in configuration.rtl_langs:\n        print('rm -rf conf/locale/' + lang)\n        execute('rm -rf conf/locale/' + lang)\n        execute('tx pull -l ' + lang)\n    clean_translated_locales(configuration, langs=configuration.rtl_langs)", "response": "Pulls all translations for RTL languages"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_translated_locales(configuration, langs=None):\n    if not langs:\n        langs = configuration.translated_locales\n    for locale in langs:\n        clean_locale(configuration, locale)", "response": "Strips out the warning from all translated po files\n    about being an English source file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_locale(configuration, locale):\n    dirname = configuration.get_messages_dir(locale)\n    if not dirname.exists():\n        # Happens when we have a supported locale that doesn't exist in Transifex\n        return\n    for filename in dirname.files('*.po'):\n        clean_file(configuration, dirname.joinpath(filename))", "response": "Cleans up the warning from all of a locale s translated po files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_file(configuration, filename):\n    pofile = polib.pofile(filename)\n\n    if pofile.header.find(EDX_MARKER) != -1:\n        new_header = get_new_header(configuration, pofile)\n        new = pofile.header.replace(EDX_MARKER, new_header)\n        pofile.header = new\n        pofile.save()", "response": "Cleans up the warning from a translated po file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_new_header(configuration, pofile):\n    team = pofile.metadata.get('Language-Team', None)\n    if not team:\n        return TRANSIFEX_HEADER.format(configuration.TRANSIFEX_URL)\n    return TRANSIFEX_HEADER.format(team)", "response": "Get the new header for the po file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert(self, string):\n        (string, tags) = self.detag_string(string)\n        string = self.inner_convert_string(string)\n        string = self.retag_string(string, tags)\n        return string", "response": "Converts a string containing html tags into tags"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts tags from string.", "response": "def detag_string(self, string):\n        \"\"\"Extracts tags from string.\n\n           returns (string, list) where\n           string: string has tags replaced by indices (<BR>... => <0>, <1>, <2>, etc.)\n           list: list of the removed tags ('<BR>', '<I>', '</I>')\n        \"\"\"\n        counter = itertools.count(0)\n        count = lambda m: '<%s>' % next(counter)\n        tags = self.tag_pattern.findall(string)\n        tags = [''.join(tag) for tag in tags]\n        (new, nfound) = self.tag_pattern.subn(count, string)\n        if len(tags) != nfound:\n            raise Exception('tags dont match:' + string)\n        return (new, tags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retag_string(self, string, tags):\n        for i, tag in enumerate(tags):\n            bracketed = '<%s>' % i\n            string = re.sub(bracketed, tag, string, 1)\n        return string", "response": "substitutes each tag back into string into occurrences of <0 > and <1 etc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef options(self):\n        arg = self.get(0)\n        if arg.startswith('-') and not self.is_asking_for_help:\n            return arg[1:]\n        return ''.join(x for x in arg if x in 'dgktz')", "response": "Train tickets query options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield a list of trains.", "response": "def trains(self):\n        \"\"\"Filter rows according to `headers`\"\"\"\n        for row in self._rows:\n            train_no = row.get('station_train_code')\n            initial = train_no[0].lower()\n            if not self._opts or initial in self._opts:\n                train = [\n                    # Column: '\u8f66\u6b21'\n                    train_no,\n                    # Column: '\u8f66\u7ad9'\n                    '\\n'.join([\n                        colored.green(row.get('from_station_name')),\n                        colored.red(row.get('to_station_name')),\n                    ]),\n                    # Column: '\u65f6\u95f4'\n                    '\\n'.join([\n                        colored.green(row.get('start_time')),\n                        colored.red(row.get('arrive_time')),\n                    ]),\n                    # Column: '\u5386\u65f6'\n                    self._get_duration(row),\n                    # Column: '\u5546\u52a1'\n                    row.get('swz_num'),\n                    # Column: '\u4e00\u7b49'\n                    row.get('zy_num'),\n                    # Column: '\u4e8c\u7b49'\n                    row.get('ze_num'),\n                    # Column: '\u8f6f\u5367'\n                    row.get('rw_num'),\n                    # Column: '\u786c\u5367'\n                    row.get('yw_num'),\n                    # Column: '\u8f6f\u5ea7'\n                    row.get('rz_num'),\n                    # Column: '\u786c\u5ea7'\n                    row.get('yz_num'),\n                    # Column: '\u65e0\u5ea7'\n                    row.get('wz_num')\n                ]\n                yield train"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse PrettyTable to perform formatted outprint.", "response": "def pretty_print(self):\n        \"\"\"Use `PrettyTable` to perform formatted outprint.\"\"\"\n        pt = PrettyTable()\n        if len(self) == 0:\n            pt._set_field_names(['Sorry,'])\n            pt.add_row([TRAIN_NOT_FOUND])\n        else:\n            pt._set_field_names(self.headers)\n            for train in self.trains:\n                pt.add_row(train)\n        print(pt)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck and return a valid query date.", "response": "def _valid_date(self):\n        \"\"\"Check and return a valid query date.\"\"\"\n        date = self._parse_date(self.date)\n\n        if not date:\n            exit_after_echo(INVALID_DATE)\n\n        try:\n            date = datetime.strptime(date, '%Y%m%d')\n        except ValueError:\n            exit_after_echo(INVALID_DATE)\n\n        # A valid query date should within 50 days.\n        offset = date - datetime.today()\n        if offset.days not in range(-1, 50):\n            exit_after_echo(INVALID_DATE)\n\n        return datetime.strftime(date, '%Y-%m-%d')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_date(date):\n        result = ''.join(re.findall('\\d', date))\n        l = len(result)\n\n        # User only input month and day, eg 6-1, 6.26, 0626...\n        if l in (2, 3, 4):\n            year = str(datetime.today().year)\n            return year + result\n\n        # User input full format date, eg 201661, 2016-6-26, 20160626...\n        if l in (6, 7, 8):\n            return result\n\n        return ''", "response": "Parse from the user input date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_params(self):\n        d = OrderedDict()\n        d['purpose_codes'] = 'ADULT'\n        d['queryDate'] = self._valid_date\n        d['from_station'] = self._from_station_telecode\n        d['to_station'] = self._to_station_telecode\n        return d", "response": "Build the params dictionary for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a date range according to the days user input.", "response": "def date_range(self):\n        \"\"\"Generate date range according to the `days` user input.\"\"\"\n        try:\n            days = int(self.days)\n        except ValueError:\n            exit_after_echo(QUERY_DAYS_INVALID)\n\n        if days < 1:\n            exit_after_echo(QUERY_DAYS_INVALID)\n        start = datetime.today()\n        end = start + timedelta(days=days)\n        return (\n            datetime.strftime(start, '%Y-%m-%d'),\n            datetime.strftime(end, '%Y-%m-%d')\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse \u4e3b\u9898 \u65f6\u95f4 \u573a\u9986 \u7968\u4ef7 in every item.", "response": "def parse(self, items):\n        \"\"\"Parse `\u4e3b\u9898`, `\u65f6\u95f4`, `\u573a\u9986`, \u7968\u4ef7` in every item.\"\"\"\n        rows = []\n        for i, item in enumerate(items):\n            theme = colored.green(item.find(class_='ico').a.text.strip())\n            text = item.find(class_='mt10').text.strip()\n            mix = re.sub('\\s+', ' ', text).split('\uff1a')\n            time = mix[1][:-3]\n            place = mix[2][:-7]\n            # display time below theme\n            theme_time = '\\n'.join([theme, colored.red(time)])\n            price = item.find(class_='price-sort').text.strip()\n            rows.append([theme_time, price, place])\n        return rows"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying all putian hospitals in a city", "response": "def query(params):\n    \"\"\"`params` is a city name or a city name + hospital name.\n\n    CLI:\n\n        1. query all putian hospitals in a city:\n\n        $ iquery -p \u5357\u4eac\n        +------+\n        | \u5357\u4eac |\n        +------+\n        |...   |\n        +------+\n        |...   |\n        +------+\n        ...\n\n\n        2. query if the hospital in the city is putian\n           series, you can only input hospital's short name:\n\n        $ iquery -p \u5357\u4eac \u66d9\u5149\n        +------------+\n        |\u5357\u4eac\u66d9\u5149\u533b\u9662|\n        +------------+\n        |    True    |\n        +------------+\n\n    \"\"\"\n\n    r = requests_get(QUERY_URL, verify=True)\n\n    return HospitalCollection(r.json(), params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(song_name):\n    r = requests_get(SONG_SEARCH_URL.format(song_name))\n\n    try:\n        # Get the first result.\n        song_url = re.search(r'(http://www.xiami.com/song/\\d+)', r.text).group(0)\n    except AttributeError:\n        exit_after_echo(SONG_NOT_FOUND)\n\n    return SongPage(song_url)", "response": "Query the song page by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields all lotteries in the table.", "response": "def lotteries(self):\n        \"\"\"\u7528\u4e8e\u751f\u6210\u6240\u6709\u5f69\u79cd\u6700\u8fd1\u5f00\u5956\u4fe1\u606f\"\"\"\n        for idx, row in enumerate(self._rows):\n            i = pq(row)\n            cz = i('td:eq(0)').text().strip()\n            if cz in self.need_to_show:\n                qh = i('td:eq(1)').text().strip()\n                kjsj = i('td:eq(2)').text().strip()\n                hm_r = colored.red(i('td:eq(3) span.ball_1').text().strip())\n                hm_g = colored.green(i('td:eq(3) span.ball_2').text().strip())\n                kjhm = ' '.join([hm_r, hm_g])\n                jcgc = i('td:eq(4)').text().strip()\n\n                lottery = [idx, cz, qh, kjsj, kjhm, jcgc]\n                yield lottery"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_lottery_detail_by_id(self, id):\n        header = '\u7f16\u53f7 \u671f\u53f7 \u5f00\u5956\u65e5\u671f \u5f00\u5956\u53f7\u7801'.split()\n        pt = PrettyTable()\n        pt._set_field_names(header)\n\n        url = QUERY_DETAIL_URL.format(id=id)\n        import requests\n        content = requests.get(url).text\n        d = pq(content)\n        if d('table.historylist'):\n            # \u8f93\u51fa\u5f69\u79cd\n            info = d('div.historyHd1 h2').text()\n            print(info)\n            # \u8f93\u51fatable\n            rows = d('table.historylist>tbody>tr')\n            for idx, row in enumerate(rows):\n                i = pq(row)\n                qh = i('td:eq(0)').text().strip()\n                kjrq = i('td:eq(1)').text().strip()\n                hm_r = colored.red(i('td:eq(2) td.redBalls').text().strip())\n                hm_g = colored.green(i('td:eq(2) td.blueBalls').text().strip())\n                kjhm = ' '.join([hm_r, hm_g])\n                item = [idx + 1, qh, kjrq, kjhm]\n                pt.add_row(item)\n            print(pt)\n        elif d('table#draw_list'):\n            # \u8f93\u51fa\u5f69\u79cd\n            info = d('div.cpinfo>div.title').text()\n            print(info)\n            # \u8f93\u51fatable\n            rows = d('table#draw_list>tbody>tr')\n            for idx, row in enumerate(rows):\n                i = pq(row)\n                qh = i('td.td2').text().strip()\n                kjrq = i('td.td1').text().strip()\n                hm_r = colored.red(i('td.td3 span.ball_1').text().strip())\n                hm_g = colored.green(i('td.td3 span.ball_2').text().strip())\n                kjhm = ' '.join([hm_r, hm_g])\n                item = [idx + 1, qh, kjrq, kjhm]\n                pt.add_row(item)\n            print(pt)\n        else:\n            print('\u8bf7\u8054\u7cfb\u4f5c\u8005')", "response": "Get lottery detail by id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query():\n\n    r = requests_get(QUERY_URL)\n\n    try:\n        rows = r.json()['subject_collection_items']\n    except (IndexError, TypeError):\n        rows = []\n\n    return MoviesCollection(rows)", "response": "Query hot movies infomation from douban."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef action_draft(self):\n        for rec in self:\n            if not rec.state == 'cancelled':\n                raise UserError(\n                    _('You need to cancel it before reopening.'))\n            if not (rec.am_i_owner or rec.am_i_approver):\n                raise UserError(\n                    _('You are not authorized to do this.\\r\\n'\n                      'Only owners or approvers can reopen Change Requests.'))\n            rec.write({'state': 'draft'})", "response": "Set a change request as draft"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset a change request as to approve", "response": "def action_to_approve(self):\n        \"\"\"Set a change request as to approve\"\"\"\n        template = self.env.ref(\n            'document_page_approval.email_template_new_draft_need_approval')\n        approver_gid = self.env.ref(\n            'document_page_approval.group_document_approver_user')\n        for rec in self:\n            if rec.state != 'draft':\n                raise UserError(\n                    _(\"Can't approve pages in '%s' state.\") % rec.state)\n            if not (rec.am_i_owner or rec.am_i_approver):\n                raise UserError(\n                    _('You are not authorized to do this.\\r\\n'\n                      'Only owners or approvers can request approval.'))\n            # request approval\n            if rec.is_approval_required:\n                rec.write({'state': 'to approve'})\n                guids = [g.id for g in rec.page_id.approver_group_ids]\n                users = self.env['res.users'].search([\n                    ('groups_id', 'in', guids),\n                    ('groups_id', 'in', approver_gid.id)])\n                rec.message_subscribe_users([u.id for u in users])\n                rec.message_post_with_template(template.id)\n            else:\n                # auto-approve if approval is not required\n                rec.action_approve()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef action_approve(self):\n        for rec in self:\n            if rec.state not in ['draft', 'to approve']:\n                raise UserError(\n                    _(\"Can't approve page in '%s' state.\") % rec.state)\n            if not rec.am_i_approver:\n                raise UserError(_(\n                    'You are not authorized to do this.\\r\\n'\n                    'Only approvers with these groups can approve this: '\n                    ) % ', '.join(\n                        [g.display_name\n                            for g in rec.page_id.approver_group_ids]))\n            # Update state\n            rec.write({\n                'state': 'approved',\n                'approved_date': fields.datetime.now(),\n                'approved_uid': self.env.uid,\n            })\n            # Trigger computed field update\n            rec.page_id._compute_history_head()\n            # Notify state change\n            rec.message_post(\n                subtype='mt_comment',\n                body=_(\n                    'Change request has been approved by %s.'\n                    ) % (self.env.user.name)\n            )\n            # Notify followers a new version is available\n            rec.page_id.message_post(\n                subtype='mt_comment',\n                body=_(\n                    'New version of the document %s approved.'\n                    ) % (rec.page_id.name)\n            )", "response": "Set a change request as approved."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef action_cancel(self):\n        self.write({'state': 'cancelled'})\n        for rec in self:\n            rec.message_post(\n                subtype='mt_comment',\n                body=_(\n                    'Change request <b>%s</b> has been cancelled by %s.'\n                    ) % (rec.display_name, self.env.user.name)\n                )", "response": "Set a change request as cancelled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if current user is the owner", "response": "def _compute_am_i_owner(self):\n        \"\"\"Check if current user is the owner\"\"\"\n        for rec in self:\n            rec.am_i_owner = (rec.create_uid == self.env.user)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the page url.", "response": "def _compute_page_url(self):\n        \"\"\"Compute the page url.\"\"\"\n        for page in self:\n            base_url = self.env['ir.config_parameter'].sudo().get_param(\n                'web.base.url',\n                default='http://localhost:8069'\n            )\n\n            page.page_url = (\n                '{}/web#db={}&id={}&view_type=form&'\n                'model=document.page.history').format(\n                    base_url,\n                    self.env.cr.dbname,\n                    page.id\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows a diff between this version and the previous version", "response": "def _compute_diff(self):\n        \"\"\"Shows a diff between this version and the previous version\"\"\"\n        history = self.env['document.page.history']\n        for rec in self:\n            domain = [\n                ('page_id', '=', rec.page_id.id),\n                ('state', '=', 'approved')]\n            if rec.approved_date:\n                domain.append(('approved_date', '<', rec.approved_date))\n            prev = history.search(domain, limit=1, order='approved_date DESC')\n            if prev:\n                rec.diff = self.getDiff(prev.id, rec.id)\n            else:\n                rec.diff = self.getDiff(False, rec.id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the URL with the given name as an ir. attachment record.", "response": "def action_add_url(self):\n        \"\"\"Adds the URL with the given name as an ir.attachment record.\"\"\"\n        if not self.env.context.get('active_model'):\n            return\n        attachment_obj = self.env['ir.attachment']\n        for form in self:\n            url = parse.urlparse(form.url)\n            if not url.scheme:\n                url = parse.urlparse('%s%s' % ('http://', form.url))\n            for active_id in self.env.context.get('active_ids', []):\n                attachment = {\n                    'name': form.name,\n                    'type': 'url',\n                    'url': url.geturl(),\n                    'res_id': active_id,\n                    'res_model': self.env.context['active_model'],\n                }\n                attachment_obj.create(attachment)\n        return {'type': 'ir.actions.act_close_wizard_and_reload_view'}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes if the document required approval based on his parents.", "response": "def _compute_is_approval_required(self):\n        \"\"\"Check if the document required approval based on his parents.\"\"\"\n        for page in self:\n            res = page.approval_required\n            if page.parent_id:\n                res = res or page.parent_id.is_approval_required\n            page.is_approval_required = res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_approver_group_ids(self):\n        for page in self:\n            res = page.approver_gid\n            if page.parent_id:\n                res = res | page.parent_id.approver_group_ids\n            page.approver_group_ids = res", "response": "Compute the approver group IDs based on his parents."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_am_i_approver(self):\n        for rec in self:\n            rec.am_i_approver = rec.can_user_approve_this_page(self.env.user)", "response": "Compute the am_i_approver attribute of all the related objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a user can approve this page.", "response": "def can_user_approve_this_page(self, user):\n        \"\"\"Check if a user can approve this page.\"\"\"\n        self.ensure_one()\n        # if it's not required, anyone can approve\n        if not self.is_approval_required:\n            return True\n        # if user belongs to 'Knowledge / Manager', he can approve anything\n        if user.has_group('document_page.group_document_manager'):\n            return True\n        # to approve, user must have approver rights\n        if not user.has_group(\n                'document_page_approval.group_document_approver_user'):\n            return False\n        # if there aren't any approver_groups_defined, user can approve\n        if not self.approver_group_ids:\n            return True\n        # to approve, user must belong to any of the approver groups\n        return len(user.groups_id & self.approver_group_ids) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the location status.", "response": "def status(self):\n        \"\"\"Retrieves the location status.\"\"\"\n        response = requests.get(\n            \"https://tccna.honeywell.com/WebAPI/emea/api/v1/\"\n            \"location/%s/status?includeTemperatureControlSystems=True\" %\n            self.locationId,\n            headers=self.client._headers()                                       # pylint: disable=protected-access\n        )\n        response.raise_for_status()\n        data = response.json()\n\n        # Now feed into other elements\n        for gw_data in data['gateways']:\n            gateway = self.gateways[gw_data['gatewayId']]\n\n            for sys in gw_data[\"temperatureControlSystems\"]:\n                system = gateway.control_systems[sys['systemId']]\n\n                system.__dict__.update(\n                    {'systemModeStatus': sys['systemModeStatus'],\n                     'activeFaults': sys['activeFaults']})\n\n                if 'dhw' in sys:\n                    system.hotwater.__dict__.update(sys['dhw'])\n\n                for zone_data in sys[\"zones\"]:\n                    zone = system.zones[zone_data['name']]\n                    zone.__dict__.update(zone_data)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the DHW on or permanently.", "response": "def set_dhw_on(self, until=None):\n        \"\"\"Sets the DHW on until a given time, or permanently.\"\"\"\n        if until is None:\n            data = {\"Mode\": \"PermanentOverride\",\n                    \"State\": \"On\",\n                    \"UntilTime\": None}\n        else:\n            data = {\"Mode\": \"TemporaryOverride\",\n                    \"State\": \"On\",\n                    \"UntilTime\": until.strftime('%Y-%m-%dT%H:%M:%SZ')}\n\n        self._set_dhw(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the DHW off until a given time or permanently.", "response": "def set_dhw_off(self, until=None):\n        \"\"\"Sets the DHW off until a given time, or permanently.\"\"\"\n        if until is None:\n            data = {\"Mode\": \"PermanentOverride\",\n                    \"State\": \"Off\",\n                    \"UntilTime\": None}\n        else:\n            data = {\"Mode\": \"TemporaryOverride\",\n                    \"State\": \"Off\",\n                    \"UntilTime\": until.strftime('%Y-%m-%dT%H:%M:%SZ')}\n\n        self._set_dhw(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef schedule(self):\n        response = requests.get(\n            \"https://tccna.honeywell.com/WebAPI/emea/api/v1\"\n            \"/%s/%s/schedule\" % (self.zone_type, self.zoneId),\n            headers=self.client._headers()                                       # pylint: disable=no-member,protected-access\n        )\n        response.raise_for_status()\n\n        mapping = [\n            ('dailySchedules', 'DailySchedules'),\n            ('dayOfWeek', 'DayOfWeek'),\n            ('temperature', 'TargetTemperature'),\n            ('timeOfDay', 'TimeOfDay'),\n            ('switchpoints', 'Switchpoints'),\n            ('dhwState', 'DhwState'),\n        ]\n\n        response_data = response.text\n        for from_val, to_val in mapping:\n            response_data = response_data.replace(from_val, to_val)\n\n        data = json.loads(response_data)\n        # change the day name string to a number offset (0 = Monday)\n        for day_of_week, schedule in enumerate(data['DailySchedules']):\n            schedule['DayOfWeek'] = day_of_week\n        return data", "response": "Gets the schedule for the given zone"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_schedule(self, zone_info):\n        # must only POST json, otherwise server API handler raises exceptions\n        try:\n            json.loads(zone_info)\n\n        except ValueError as error:\n            raise ValueError(\"zone_info must be valid JSON: \", error)\n\n        headers = dict(self.client._headers())                                   # pylint: disable=protected-access\n        headers['Content-Type'] = 'application/json'\n\n        response = requests.put(\n            \"https://tccna.honeywell.com/WebAPI/emea/api/v1\"\n            \"/%s/%s/schedule\" % (self.zone_type, self.zoneId),\n            data=zone_info, headers=headers\n        )\n        response.raise_for_status()\n\n        return response.json()", "response": "Sets the schedule for this zone"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the current temperatures for each zone. Returns a generator.", "response": "def temperatures(self, force_refresh=False):\n        \"\"\"Retrieve the current details for each zone. Returns a generator.\"\"\"\n        self._populate_full_data(force_refresh)\n        for device in self.full_data['devices']:\n            set_point = 0\n            status = \"\"\n            if 'heatSetpoint' in device['thermostat']['changeableValues']:\n                set_point = float(\n                    device['thermostat']['changeableValues'][\"heatSetpoint\"][\"value\"])\n                status = device['thermostat']['changeableValues'][\"heatSetpoint\"][\"status\"]\n            else:\n                status = device['thermostat']['changeableValues']['status']\n            yield {'thermostat': device['thermostatModelType'],\n                   'id': device['deviceID'],\n                   'name': device['name'],\n                   'temp': float(device['thermostat']['indoorTemperature']),\n                   'setpoint': set_point,\n                   'status': status,\n                   'mode': device['thermostat']['changeableValues']['mode']}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the set of modes the device can be assigned to.", "response": "def get_modes(self, zone):\n        \"\"\"Returns the set of modes the device can be assigned.\"\"\"\n        self._populate_full_data()\n        device = self._get_device(zone)\n        return device['thermostat']['allowedModes']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the temperature of the given zone.", "response": "def set_temperature(self, zone, temperature, until=None):\n        \"\"\"Sets the temperature of the given zone.\"\"\"\n        if until is None:\n            data = {\"Value\": temperature, \"Status\": \"Hold\", \"NextTime\": None}\n        else:\n            data = {\"Value\": temperature,\n                    \"Status\": \"Temporary\",\n                    \"NextTime\": until.strftime('%Y-%m-%dT%H:%M:%SZ')}\n\n        self._set_heat_setpoint(zone, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets DHW to On Auto or Auto either indefinitely or until a specific time.", "response": "def _set_dhw(self, status=\"Scheduled\", mode=None, next_time=None):\n        \"\"\"Set DHW to On, Off or Auto, either indefinitely, or until a\n        specified time.\n        \"\"\"\n        data = {\"Status\": status,\n                \"Mode\": mode,\n                \"NextTime\": next_time,\n                \"SpecialModes\": None,\n                \"HeatSetpoint\": None,\n                \"CoolSetpoint\": None}\n\n        self._populate_full_data()\n        dhw_zone = self._get_dhw_zone()\n        if dhw_zone is None:\n            raise Exception('No DHW zone reported from API')\n        url = (self.hostname + \"/WebAPI/api/devices\"\n               \"/%s/thermostat/changeableValues\" % dhw_zone)\n\n        response = self._do_request('put', url, json.dumps(data))\n\n        task_id = self._get_task_id(response)\n\n        while self._get_task_status(task_id) != 'Succeeded':\n            time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting DHW to on either indefinitely or until a specified time.", "response": "def set_dhw_on(self, until=None):\n        \"\"\"Set DHW to on, either indefinitely, or until a specified time.\n\n        When On, the DHW controller will work to keep its target temperature\n        at/above its target temperature.  After the specified time, it will\n        revert to its scheduled behaviour.\n        \"\"\"\n\n        time_until = None if until is None else until.strftime(\n            '%Y-%m-%dT%H:%M:%SZ')\n\n        self._set_dhw(status=\"Hold\", mode=\"DHWOn\", next_time=time_until)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _headers(self):\n        if not self.access_token or not self.access_token_expires:\n            self._basic_login()\n\n        elif datetime.now() > self.access_token_expires - timedelta(seconds=30):\n            self._basic_login()\n\n        return {'Accept': HEADER_ACCEPT,\n                'Authorization': 'bearer ' + self.access_token}", "response": "Ensure the Authorization Header has a valid Access Token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nauthenticating using the user credentials.", "response": "def _basic_login(self):\n        \"\"\"Obtain a new access token from the vendor.\n\n        First, try using the refresh_token, if one is available, otherwise\n        authenticate using the user credentials.\n        \"\"\"\n        _LOGGER.debug(\"No/Expired/Invalid access_token, re-authenticating...\")\n        self.access_token = self.access_token_expires = None\n\n        if self.refresh_token:\n            _LOGGER.debug(\"Trying refresh_token...\")\n            credentials = {'grant_type': \"refresh_token\",\n                           'scope': \"EMEA-V1-Basic EMEA-V1-Anonymous\",\n                           'refresh_token': self.refresh_token}\n\n            try:\n                self._obtain_access_token(credentials)\n            except (requests.HTTPError, KeyError, ValueError):\n                _LOGGER.warning(\n                    \"Invalid refresh_token, will try user credentials.\")\n                self.refresh_token = None\n\n        if not self.refresh_token:\n            _LOGGER.debug(\"Trying user credentials...\")\n            credentials = {'grant_type': \"password\",\n                           'scope': \"EMEA-V1-Basic EMEA-V1-Anonymous \"\n                                    \"EMEA-V1-Get-Current-User-Account\",\n                           'Username': self.username,\n                           'Password': self.password}\n\n            self._obtain_access_token(credentials)\n\n        _LOGGER.debug(\"refresh_token = %s\", self.refresh_token)\n        _LOGGER.debug(\"access_token = %s\", self.access_token)\n        _LOGGER.debug(\"access_token_expires = %s\",\n                      self.access_token_expires.strftime(\"%Y-%m-%d %H:%M:%S\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the user account information.", "response": "def user_account(self):\n        \"\"\"Return the user account information.\"\"\"\n        self.account_info = None\n\n        url = 'https://tccna.honeywell.com/WebAPI/emea/api/v1/userAccount'\n\n        response = requests.get(url, headers=self._headers())\n        response.raise_for_status()\n\n        self.account_info = response.json()\n        return self.account_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the details of the installation.", "response": "def installation(self):\n        \"\"\"Return the details of the installation.\"\"\"\n        self.locations = []\n\n        url = (\"https://tccna.honeywell.com/WebAPI/emea/api/v1/location\"\n               \"/installationInfo?userId=%s\"\n               \"&includeTemperatureControlSystems=True\"\n               % self.account_info['userId'])\n\n        response = requests.get(url, headers=self._headers())\n        response.raise_for_status()\n\n        self.installation_info = response.json()\n        self.system_id = (self.installation_info[0]['gateways'][0]\n                          ['temperatureControlSystems'][0]['systemId'])\n\n        for loc_data in self.installation_info:\n            self.locations.append(Location(self, loc_data))\n\n        return self.installation_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef full_installation(self, location=None):\n        url = (\"https://tccna.honeywell.com/WebAPI/emea/api/v1/location\"\n               \"/%s/installationInfo?includeTemperatureControlSystems=True\"\n               % self._get_location(location))\n\n        response = requests.get(url, headers=self._headers())\n        response.raise_for_status()\n\n        return response.json()", "response": "Return the full details of the installation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the detail of the gateway.", "response": "def gateway(self):\n        \"\"\"Return the detail of the gateway.\"\"\"\n        url = 'https://tccna.honeywell.com/WebAPI/emea/api/v1/gateway'\n\n        response = requests.get(url, headers=self._headers())\n        response.raise_for_status()\n\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef temperatures(self):\n        self.location.status()\n\n        if self.hotwater:\n            yield {\n                'thermostat': 'DOMESTIC_HOT_WATER',\n                'id': self.hotwater.dhwId,\n                'name': '',\n                'temp': self.hotwater.temperatureStatus['temperature'],          # pylint: disable=no-member\n                'setpoint': ''\n            }\n\n        for zone in self._zones:\n            zone_info = {\n                'thermostat': 'EMEA_ZONE',\n                'id': zone.zoneId,\n                'name': zone.name,\n                'temp': None,\n                'setpoint': zone.setpointStatus['targetHeatTemperature']\n            }\n\n            if zone.temperatureStatus['isAvailable']:\n                zone_info['temp'] = zone.temperatureStatus['temperature']\n            yield zone_info", "response": "Return a generator with the details of each zone."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zone_schedules_backup(self, filename):\n        _LOGGER.info(\"Backing up schedules from ControlSystem: %s (%s)...\",\n                     self.systemId, self.location.name)\n\n        schedules = {}\n\n        if self.hotwater:\n            _LOGGER.info(\"Retrieving DHW schedule: %s...\",\n                         self.hotwater.zoneId)\n\n            schedule = self.hotwater.schedule()\n            schedules[self.hotwater.zoneId] = {\n                'name': 'Domestic Hot Water',\n                'schedule': schedule}\n\n        for zone in self._zones:\n            zone_id = zone.zoneId\n            name = zone.name\n\n            _LOGGER.info(\"Retrieving Zone schedule: %s - %s\", zone_id, name)\n\n            schedule = zone.schedule()\n            schedules[zone_id] = {'name': name, 'schedule': schedule}\n\n        schedule_db = json.dumps(schedules, indent=4)\n\n        _LOGGER.info(\"Writing to backup file: %s...\", filename)\n        with open(filename, 'w') as file_output:\n            file_output.write(schedule_db)\n\n        _LOGGER.info(\"Backup completed.\")", "response": "Backup all zones on control system to the given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrestore all zones on control system from the given file.", "response": "def zone_schedules_restore(self, filename):\n        \"\"\"Restore all zones on control system from the given file.\"\"\"\n        _LOGGER.info(\"Restoring schedules to ControlSystem %s (%s)...\",\n                     self.systemId, self.location)\n\n        _LOGGER.info(\"Reading from backup file: %s...\", filename)\n        with open(filename, 'r') as file_input:\n            schedule_db = file_input.read()\n            schedules = json.loads(schedule_db)\n\n            for zone_id, zone_schedule in schedules.items():\n                name = zone_schedule['name']\n                zone_info = zone_schedule['schedule']\n\n                _LOGGER.info(\"Restoring schedule for: %s - %s...\",\n                             zone_id, name)\n\n                if self.hotwater and self.hotwater.zoneId == zone_id:\n                    self.hotwater.set_schedule(json.dumps(zone_info))\n                else:\n                    self.zones_by_id[zone_id].set_schedule(\n                        json.dumps(zone_info))\n\n        _LOGGER.info(\"Restore completed.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting items used for ordering in a package.", "response": "def cli(env, package_keyname, keyword, category):\n    \"\"\"List package items used for ordering.\n\n    The item keyNames listed can be used with `slcli order place` to specify\n    the items that are being ordered in the package.\n\n    .. Note::\n        Items with a numbered category, like disk0 or gpu0, can be included\n        multiple times in an order to match how many of the item you want to order.\n\n    ::\n\n        # List all items in the VSI package\n        slcli order item-list CLOUD_SERVER\n\n        # List Ubuntu OSes from the os category of the Bare Metal package\n        slcli order item-list BARE_METAL_SERVER --category os --keyword ubuntu\n\n    \"\"\"\n    table = formatting.Table(COLUMNS)\n    manager = ordering.OrderingManager(env.client)\n\n    _filter = {'items': {}}\n    if keyword:\n        _filter['items']['description'] = {'operation': '*= %s' % keyword}\n    if category:\n        _filter['items']['categories'] = {'categoryCode': {'operation': '_= %s' % category}}\n\n    items = manager.list_items(package_keyname, filter=_filter)\n    sorted_items = sort_items(items)\n\n    categories = sorted_items.keys()\n    for catname in sorted(categories):\n        for item in sorted_items[catname]:\n            table.add_row([catname, item['keyName'], item['description'], get_price(item)])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort_items(items):\n\n    sorted_items = {}\n    for item in items:\n        category = lookup(item, 'itemCategory', 'categoryCode')\n        if sorted_items.get(category) is None:\n            sorted_items[category] = []\n        sorted_items[category].append(item)\n\n    return sorted_items", "response": "sorts the items into a dictionary of categories with a list of items"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_api_key(client, username, secret):\n\n    # Try to use a client with username/api key\n    if len(secret) == 64:\n        try:\n            client['Account'].getCurrentUser()\n            return secret\n        except SoftLayer.SoftLayerAPIError as ex:\n            if 'invalid api token' not in ex.faultString.lower():\n                raise\n    else:\n        # Try to use a client with username/password\n        client.authenticate_with_password(username, secret)\n\n        user_record = client['Account'].getCurrentUser(mask='id, apiAuthenticationKeys')\n        api_keys = user_record['apiAuthenticationKeys']\n        if len(api_keys) == 0:\n            return client['User_Customer'].addApiAuthenticationKey(id=user_record['id'])\n        return api_keys[0]['authenticationKey']", "response": "Attempts API - Key and password auth to get an API key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nasking for username secret endpoint_url and timeout.", "response": "def get_user_input(env):\n    \"\"\"Ask for username, secret (api_key or password) and endpoint_url.\"\"\"\n\n    defaults = config.get_settings_from_client(env.client)\n\n    # Ask for username\n    username = env.input('Username', default=defaults['username'])\n\n    # Ask for 'secret' which can be api_key or their password\n    secret = env.getpass('API Key or Password', default=defaults['api_key'])\n\n    # Ask for which endpoint they want to use\n    endpoint = defaults.get('endpoint_url', 'public')\n    endpoint_type = env.input(\n        'Endpoint (public|private|custom)', default=endpoint)\n    endpoint_type = endpoint_type.lower()\n\n    if endpoint_type == 'public':\n        endpoint_url = SoftLayer.API_PUBLIC_ENDPOINT\n    elif endpoint_type == 'private':\n        endpoint_url = SoftLayer.API_PRIVATE_ENDPOINT\n    else:\n        if endpoint_type == 'custom':\n            endpoint_url = env.input('Endpoint URL', default=endpoint)\n        else:\n            endpoint_url = endpoint_type\n\n    # Ask for timeout\n    timeout = env.input('Timeout', default=defaults['timeout'] or 0)\n\n    return username, secret, endpoint_url, timeout"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the load balancer kind and actual id from the kind : id form.", "response": "def parse_id(input_id):\n    \"\"\"Parse the load balancer kind and actual id from the \"kind:id\" form.\"\"\"\n    parts = input_id.split(':')\n    if len(parts) != 2:\n        raise exceptions.CLIAbort(\n            'Invalid ID %s: ID should be of the form \"kind:id\"' % input_id)\n    return parts[0], int(parts[1])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, **kwargs):\n\n    mgr = SoftLayer.DedicatedHostManager(env.client)\n    tables = []\n\n    if not kwargs['flavor'] and not kwargs['datacenter']:\n        options = mgr.get_create_options()\n\n        # Datacenters\n        dc_table = formatting.Table(['datacenter', 'value'])\n        dc_table.sortby = 'value'\n        for location in options['locations']:\n            dc_table.add_row([location['name'], location['key']])\n        tables.append(dc_table)\n\n        dh_table = formatting.Table(['Dedicated Virtual Host Flavor(s)', 'value'])\n        dh_table.sortby = 'value'\n        for item in options['dedicated_host']:\n            dh_table.add_row([item['name'], item['key']])\n        tables.append(dh_table)\n    else:\n        if kwargs['flavor'] is None or kwargs['datacenter'] is None:\n            raise exceptions.ArgumentError('Both a flavor and datacenter need '\n                                           'to be passed as arguments '\n                                           'ex. slcli dh create-options -d '\n                                           'ams01 -f '\n                                           '56_CORES_X_242_RAM_X_1_4_TB')\n        router_opt = mgr.get_router_options(kwargs['datacenter'], kwargs['flavor'])\n        br_table = formatting.Table(\n            ['Available Backend Routers'])\n        for router in router_opt:\n            br_table.add_row([router['hostname']])\n        tables.append(br_table)\n\n    env.fout(formatting.listing(tables, separator='\\n'))", "response": "Host order options for a given dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay a list of cancellation reasons.", "response": "def cli(env):\n    \"\"\"Display a list of cancellation reasons.\"\"\"\n\n    table = formatting.Table(['Code', 'Reason'])\n    table.align['Code'] = 'r'\n    table.align['Reason'] = 'l'\n\n    mgr = SoftLayer.HardwareManager(env.client)\n\n    for code, reason in mgr.get_cancellation_reasons().items():\n        table.add_row([code, reason])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env):\n    manager = SoftLayer.IPSECManager(env.client)\n    contexts = manager.get_tunnel_contexts()\n\n    table = formatting.Table(['id',\n                              'name',\n                              'friendly name',\n                              'internal peer IP address',\n                              'remote peer IP address',\n                              'created'])\n    for context in contexts:\n        table.add_row([context.get('id', ''),\n                       context.get('name', ''),\n                       context.get('friendlyName', ''),\n                       context.get('internalPeerIpAddress', ''),\n                       context.get('customerPeerIpAddress', ''),\n                       context.get('createDate', '')])\n    env.fout(table)", "response": "List IPSec VPN tunnel contexts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, columns, sortby, volume_id):\n    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n\n    legal_centers = file_storage_manager.get_replication_locations(\n        volume_id\n    )\n\n    if not legal_centers:\n        click.echo(\"No data centers compatible for replication.\")\n    else:\n        table = formatting.KeyValueTable(columns.columns)\n        table.sortby = sortby\n        for legal_center in legal_centers:\n            table.add_row([value or formatting.blank()\n                           for value in columns.row(legal_center)])\n\n        env.fout(table)", "response": "List suitable replication datacenters for the given volume."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, identifier, sortby, cpu, domain, hostname, memory, tag, columns):\n\n    mgr = SoftLayer.DedicatedHostManager(env.client)\n    guests = mgr.list_guests(host_id=identifier,\n                             cpus=cpu,\n                             hostname=hostname,\n                             domain=domain,\n                             memory=memory,\n                             tags=tag,\n                             mask=columns.mask())\n\n    table = formatting.Table(columns.columns)\n    table.sortby = sortby\n\n    for guest in guests:\n        table.add_row([value or formatting.blank()\n                       for value in columns.row(guest)])\n\n    env.fout(table)", "response": "List guests which are in a dedicated host server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, identifier, count):\n\n    mgr = SoftLayer.TicketManager(env.client)\n\n    ticket_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'ticket')\n    env.fout(ticket.get_ticket_results(mgr, ticket_id, update_count=count))", "response": "Get details for a single block."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset up urllib sessions", "response": "def get_session(user_agent):\n    \"\"\"Sets up urllib sessions\"\"\"\n\n    client = requests.Session()\n    client.headers.update({\n        'Content-Type': 'application/json',\n        'User-Agent': user_agent,\n    })\n    retry = Retry(connect=3, backoff_factor=3)\n    adapter = HTTPAdapter(max_retries=retry)\n    client.mount('https://', adapter)\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the object mask.", "response": "def _format_object_mask(objectmask):\n    \"\"\"Format the new style object mask.\n\n    This wraps the user mask with mask[USER_MASK] if it does not already\n    have one. This makes it slightly easier for users.\n\n    :param objectmask: a string-based object mask\n\n    \"\"\"\n    objectmask = objectmask.strip()\n\n    if (not objectmask.startswith('mask') and\n            not objectmask.startswith('[')):\n        objectmask = \"mask[%s]\" % objectmask\n    return objectmask"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the client session object", "response": "def client(self):\n        \"\"\"Returns client session object\"\"\"\n\n        if self._client is None:\n            self._client = get_session(self.user_agent)\n        return self._client"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_reproduceable(self, request):\n        from string import Template\n        output = Template('''============= testing.py =============\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom xml.etree import ElementTree\nclient = requests.Session()\nclient.headers.update({'Content-Type': 'application/json', 'User-Agent': 'softlayer-python/testing',})\nretry = Retry(connect=3, backoff_factor=3)\nadapter = HTTPAdapter(max_retries=retry)\nclient.mount('https://', adapter)\nurl = '$url'\npayload = \"\"\"$payload\"\"\"\ntransport_headers = $transport_headers\ntimeout = $timeout\nverify = $verify\ncert = $cert\nproxy = $proxy\nresponse = client.request('POST', url, data=payload, headers=transport_headers, timeout=timeout,\n               verify=verify, cert=cert, proxies=proxy)\nxml = ElementTree.fromstring(response.content)\nElementTree.dump(xml)\n==========================''')\n\n        safe_payload = re.sub(r'<string>[a-z0-9]{64}</string>', r'<string>API_KEY_GOES_HERE</string>', request.payload)\n        safe_payload = re.sub(r'(\\s+)', r' ', safe_payload)\n        substitutions = dict(url=request.url, payload=safe_payload, transport_headers=request.transport_headers,\n                             timeout=self.timeout, verify=request.verify, cert=request.cert,\n                             proxy=_proxies_dict(self.proxy))\n        return output.substitute(substitutions)", "response": "Prints out the minimal python code to reproduce a specific request object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_reproduceable(self, request):\n        command = \"curl -u $SL_USER:$SL_APIKEY -X {method} -H {headers} {data} '{uri}'\"\n\n        method = REST_SPECIAL_METHODS.get(request.method)\n\n        if method is None:\n            method = 'GET'\n        if request.args:\n            method = 'POST'\n\n        data = ''\n        if request.payload is not None:\n            data = \"-d '{}'\".format(request.payload)\n\n        headers = ['\"{0}: {1}\"'.format(k, v) for k, v in request.transport_headers.items()]\n        headers = \" -H \".join(headers)\n        return command.format(method=method, headers=headers, data=data, uri=request.url)", "response": "Prints out the minimal python code to reproduce a specific request"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_transport_log(self, call):\n        output = \"Returned Data: \\n{}\".format(call.result)\n        LOGGER.debug(output)", "response": "Prints the result of an API call to the log"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, identifier, enable):\n\n    mgr = SoftLayer.HardwareManager(env.client)\n    hw_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'hardware')\n    result = env.client['Hardware_Server'].toggleManagementInterface(enable, id=hw_id)\n    env.fout(result)", "response": "Toggle the IPMI interface on and off."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist number of block storage volumes per datacenter.", "response": "def cli(env, sortby, datacenter):\n    \"\"\"List number of block storage volumes per datacenter.\"\"\"\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    mask = \"mask[serviceResource[datacenter[name]],\"\\\n           \"replicationPartners[serviceResource[datacenter[name]]]]\"\n    block_volumes = block_manager.list_block_volumes(datacenter=datacenter,\n                                                     mask=mask)\n\n    # cycle through all block volumes and count datacenter occurences.\n    datacenters = dict()\n    for volume in block_volumes:\n        service_resource = volume['serviceResource']\n        if 'datacenter' in service_resource:\n            datacenter_name = service_resource['datacenter']['name']\n            if datacenter_name not in datacenters.keys():\n                datacenters[datacenter_name] = 1\n            else:\n                datacenters[datacenter_name] += 1\n\n    table = formatting.KeyValueTable(DEFAULT_COLUMNS)\n    table.sortby = sortby\n    for datacenter_name in datacenters:\n        table.add_row([datacenter_name, datacenters[datacenter_name]])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, account_id, content_url):\n\n    manager = SoftLayer.CDNManager(env.client)\n    manager.load_content(account_id, content_url)", "response": "Cache one or more files on all edge nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets details about a VLAN.", "response": "def cli(env, identifier, no_vs, no_hardware):\n    \"\"\"Get details about a VLAN.\"\"\"\n\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    vlan_id = helpers.resolve_id(mgr.resolve_vlan_ids, identifier, 'VLAN')\n    vlan = mgr.get_vlan(vlan_id)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['id', vlan['id']])\n    table.add_row(['number', vlan['vlanNumber']])\n    table.add_row(['datacenter',\n                   vlan['primaryRouter']['datacenter']['longName']])\n    table.add_row(['primary_router',\n                   vlan['primaryRouter']['fullyQualifiedDomainName']])\n    table.add_row(['firewall',\n                   'Yes' if vlan['firewallInterfaces'] else 'No'])\n    subnets = []\n    for subnet in vlan.get('subnets', []):\n        subnet_table = formatting.KeyValueTable(['name', 'value'])\n        subnet_table.align['name'] = 'r'\n        subnet_table.align['value'] = 'l'\n        subnet_table.add_row(['id', subnet['id']])\n        subnet_table.add_row(['identifier', subnet['networkIdentifier']])\n        subnet_table.add_row(['netmask', subnet['netmask']])\n        subnet_table.add_row(['gateway', subnet.get('gateway', '-')])\n        subnet_table.add_row(['type', subnet['subnetType']])\n        subnet_table.add_row(['usable ips',\n                              subnet['usableIpAddressCount']])\n        subnets.append(subnet_table)\n\n    table.add_row(['subnets', subnets])\n\n    server_columns = ['hostname', 'domain', 'public_ip', 'private_ip']\n\n    if not no_vs:\n        if vlan.get('virtualGuests'):\n            vs_table = formatting.KeyValueTable(server_columns)\n            for vsi in vlan['virtualGuests']:\n                vs_table.add_row([vsi.get('hostname'),\n                                  vsi.get('domain'),\n                                  vsi.get('primaryIpAddress'),\n                                  vsi.get('primaryBackendIpAddress')])\n            table.add_row(['vs', vs_table])\n        else:\n            table.add_row(['vs', 'none'])\n\n    if not no_hardware:\n        if vlan.get('hardware'):\n            hw_table = formatting.Table(server_columns)\n            for hardware in vlan['hardware']:\n                hw_table.add_row([hardware.get('hostname'),\n                                  hardware.get('domain'),\n                                  hardware.get('primaryIpAddress'),\n                                  hardware.get('primaryBackendIpAddress')])\n            table.add_row(['hardware', hw_table])\n        else:\n            table.add_row(['hardware', 'none'])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, volume_id, lun_id):\n\n    block_storage_manager = SoftLayer.BlockStorageManager(env.client)\n\n    res = block_storage_manager.create_or_update_lun_id(volume_id, lun_id)\n\n    if 'value' in res and lun_id == res['value']:\n        click.echo(\n            'Block volume with id %s is reporting LUN ID %s' % (res['volumeId'], res['value']))\n    else:\n        click.echo(\n            'Failed to confirm the new LUN ID on volume %s' % (volume_id))", "response": "Set the LUN ID on an existing block storage volume."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget Load balancer details.", "response": "def cli(env, identifier):\n    \"\"\"Get Load balancer details.\"\"\"\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    _, loadbal_id = loadbal.parse_id(identifier)\n\n    load_balancer = mgr.get_local_lb(loadbal_id)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'l'\n    table.align['value'] = 'l'\n    table.add_row(['ID', 'local:%s' % load_balancer['id']])\n    table.add_row(['IP Address', load_balancer['ipAddress']['ipAddress']])\n    name = load_balancer['loadBalancerHardware'][0]['datacenter']['name']\n    table.add_row(['Datacenter', name])\n    table.add_row(['Connections limit', load_balancer['connectionLimit']])\n    table.add_row(['Dedicated', load_balancer['dedicatedFlag']])\n    table.add_row(['HA', load_balancer['highAvailabilityFlag']])\n    table.add_row(['SSL Enabled', load_balancer['sslEnabledFlag']])\n    table.add_row(['SSL Active', load_balancer['sslActiveFlag']])\n\n    index0 = 1\n    for virtual_server in load_balancer['virtualServers']:\n        for group in virtual_server['serviceGroups']:\n            service_group_table = formatting.KeyValueTable(['name', 'value'])\n\n            table.add_row(['Service Group %s' % index0, service_group_table])\n            index0 += 1\n\n            service_group_table.add_row(['Guest ID',\n                                         virtual_server['id']])\n            service_group_table.add_row(['Port', virtual_server['port']])\n            service_group_table.add_row(['Allocation',\n                                         '%s %%' %\n                                         virtual_server['allocation']])\n            service_group_table.add_row(['Routing Type',\n                                         '%s:%s' %\n                                         (group['routingTypeId'],\n                                          group['routingType']['name'])])\n            service_group_table.add_row(['Routing Method',\n                                         '%s:%s' %\n                                         (group['routingMethodId'],\n                                          group['routingMethod']['name'])])\n\n            index1 = 1\n            for service in group['services']:\n                service_table = formatting.KeyValueTable(['name', 'value'])\n\n                service_group_table.add_row(['Service %s' % index1,\n                                             service_table])\n                index1 += 1\n\n                health_check = service['healthChecks'][0]\n                service_table.add_row(['Service ID', service['id']])\n                service_table.add_row(['IP Address',\n                                       service['ipAddress']['ipAddress']])\n                service_table.add_row(['Port', service['port']])\n                service_table.add_row(['Health Check',\n                                       '%s:%s' %\n                                       (health_check['healthCheckTypeId'],\n                                        health_check['type']['name'])])\n                service_table.add_row(\n                    ['Weight', service['groupReferences'][0]['weight']])\n                service_table.add_row(['Enabled', service['enabled']])\n                service_table.add_row(['Status', service['status']])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef output_diagnostics(env, result, verbose=0, **kwargs):\n\n    if verbose > 0:\n        diagnostic_table = formatting.Table(['name', 'value'])\n        diagnostic_table.add_row(['execution_time', '%fs' % (time.time() - START_TIME)])\n\n        api_call_value = []\n        for call in env.client.transport.get_last_calls():\n            api_call_value.append(\"%s::%s (%fs)\" % (call.service, call.method, call.end_time - call.start_time))\n\n        diagnostic_table.add_row(['api_calls', api_call_value])\n        diagnostic_table.add_row(['version', consts.USER_AGENT])\n        diagnostic_table.add_row(['python_version', sys.version])\n        diagnostic_table.add_row(['library_location', os.path.dirname(SoftLayer.__file__)])\n\n        env.err(env.fmt(diagnostic_table))\n\n    if verbose > 1:\n        for call in env.client.transport.get_last_calls():\n            call_table = formatting.Table(['', '{}::{}'.format(call.service, call.method)])\n            nice_mask = ''\n            if call.mask is not None:\n                nice_mask = call.mask\n\n            call_table.add_row(['id', call.identifier])\n            call_table.add_row(['mask', nice_mask])\n            call_table.add_row(['filter', call.filter])\n            call_table.add_row(['limit', call.limit])\n            call_table.add_row(['offset', call.offset])\n            env.err(env.fmt(call_table))\n\n    if verbose > 2:\n        for call in env.client.transport.get_last_calls():\n            env.err(env.client.transport.print_reproduceable(call))", "response": "Output the diagnostic information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all sub - commands.", "response": "def list_commands(self, ctx):\n        \"\"\"List all sub-commands.\"\"\"\n        env = ctx.ensure_object(environment.Environment)\n        env.load()\n\n        return sorted(env.list_commands(*self.path))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_command(self, ctx, name):\n        env = ctx.ensure_object(environment.Environment)\n        env.load()\n\n        # Do alias lookup (only available for root commands)\n        if len(self.path) == 0:\n            name = env.resolve_alias(name)\n\n        new_path = list(self.path)\n        new_path.append(name)\n        module = env.get_command(*new_path)\n        if isinstance(module, types.ModuleType):\n            return CommandLoader(*new_path, help=module.__doc__ or '')\n        else:\n            return module", "response": "Get command for click."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, volume_id, hardware_id, virtual_id, ip_address_id, ip_address):\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    ip_address_id_list = list(ip_address_id)\n\n    # Convert actual IP Addresses to their SoftLayer ids\n    if ip_address is not None:\n        network_manager = SoftLayer.NetworkManager(env.client)\n        for ip_address_value in ip_address:\n            ip_address_object = network_manager.ip_lookup(ip_address_value)\n            ip_address_id_list.append(ip_address_object['id'])\n\n    block_manager.deauthorize_host_to_volume(volume_id,\n                                             hardware_id,\n                                             virtual_id,\n                                             ip_address_id_list)\n\n    # If no exception was raised, the command succeeded\n    click.echo('Access to %s was revoked for the specified hosts' % volume_id)", "response": "Revokes authorization for hosts accessing a given volume"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, **kwargs):\n    mgr = SoftLayer.DedicatedHostManager(env.client)\n\n    order = {\n        'hostname': kwargs['hostname'],\n        'domain': kwargs['domain'],\n        'flavor': kwargs['flavor'],\n        'location': kwargs['datacenter'],\n        'hourly': kwargs.get('billing') == 'hourly',\n    }\n\n    if kwargs['router']:\n        order['router'] = kwargs['router']\n\n    do_create = not (kwargs['export'] or kwargs['verify'])\n\n    output = None\n\n    result = mgr.verify_order(**order)\n    table = formatting.Table(['Item', 'cost'])\n    table.align['Item'] = 'r'\n    table.align['cost'] = 'r'\n    if len(result['prices']) != 1:\n        raise exceptions.ArgumentError(\"More than 1 price was found or no \"\n                                       \"prices found\")\n    price = result['prices']\n    if order['hourly']:\n        total = float(price[0].get('hourlyRecurringFee', 0.0))\n    else:\n        total = float(price[0].get('recurringFee', 0.0))\n\n    if order['hourly']:\n        table.add_row(['Total hourly cost', \"%.2f\" % total])\n    else:\n        table.add_row(['Total monthly cost', \"%.2f\" % total])\n\n    output = []\n    output.append(table)\n    output.append(formatting.FormattedItem(\n        '',\n        ' -- ! Prices reflected here are retail and do not '\n        'take account level discounts and are not guaranteed.'))\n\n    if kwargs['export']:\n        export_file = kwargs.pop('export')\n        template.export_to_template(export_file, kwargs,\n                                    exclude=['wait', 'verify'])\n        env.fout('Successfully exported options to a template file.')\n\n    if do_create:\n        if not env.skip_confirmations and not formatting.confirm(\n                \"This action will incur charges on your account. \"\n                \"Continue?\"):\n            raise exceptions.CLIAbort('Aborting dedicated host order.')\n\n        result = mgr.place_order(**order)\n\n        table = formatting.KeyValueTable(['name', 'value'])\n        table.align['name'] = 'r'\n        table.align['value'] = 'l'\n        table.add_row(['id', result['orderId']])\n        table.add_row(['created', result['orderDate']])\n        output.append(table)\n\n    env.fout(output)", "response": "Order a dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new load balancer service group.", "response": "def cli(env, identifier, allocation, port, routing_type, routing_method):\n    \"\"\"Adds a new load_balancer service.\"\"\"\n\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    _, loadbal_id = loadbal.parse_id(identifier)\n\n    mgr.add_service_group(loadbal_id,\n                          allocation=allocation,\n                          port=port,\n                          routing_type=routing_type,\n                          routing_method=routing_method)\n\n    env.fout('Load balancer service group is being added!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncancels all virtual guests of the dedicated host immediately.", "response": "def cli(env, identifier):\n    \"\"\"Cancel all virtual guests of the dedicated host immediately.\n\n       Use the 'slcli vs cancel' command to cancel an specific guest\n    \"\"\"\n\n    dh_mgr = SoftLayer.DedicatedHostManager(env.client)\n\n    host_id = helpers.resolve_id(dh_mgr.resolve_ids, identifier, 'dedicated host')\n\n    if not (env.skip_confirmations or formatting.no_going_back(host_id)):\n        raise exceptions.CLIAbort('Aborted')\n\n    table = formatting.Table(['id', 'server name', 'status'])\n\n    result = dh_mgr.cancel_guests(host_id)\n\n    if result:\n        for status in result:\n            table.add_row([\n                status['id'],\n                status['fqdn'],\n                status['status']\n            ])\n\n        env.fout(table)\n    else:\n        click.secho('There is not any guest into the dedicated host %s' % host_id, fg='red')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\norder a duplicate file storage volume.", "response": "def cli(env, origin_volume_id, origin_snapshot_id, duplicate_size,\n        duplicate_iops, duplicate_tier, duplicate_snapshot_size, billing):\n    \"\"\"Order a duplicate file storage volume.\"\"\"\n    file_manager = SoftLayer.FileStorageManager(env.client)\n\n    hourly_billing_flag = False\n    if billing.lower() == \"hourly\":\n        hourly_billing_flag = True\n\n    if duplicate_tier is not None:\n        duplicate_tier = float(duplicate_tier)\n\n    try:\n        order = file_manager.order_duplicate_volume(\n            origin_volume_id,\n            origin_snapshot_id=origin_snapshot_id,\n            duplicate_size=duplicate_size,\n            duplicate_iops=duplicate_iops,\n            duplicate_tier_level=duplicate_tier,\n            duplicate_snapshot_size=duplicate_snapshot_size,\n            hourly_billing_flag=hourly_billing_flag\n        )\n    except ValueError as ex:\n        raise exceptions.ArgumentError(str(ex))\n\n    if 'placedOrder' in order.keys():\n        click.echo(\"Order #{0} placed successfully!\".format(\n            order['placedOrder']['id']))\n        for item in order['placedOrder']['items']:\n            click.echo(\" > %s\" % item['description'])\n    else:\n        click.echo(\"Order could not be placed! Please verify your options \" +\n                   \"and try again.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a subnet from an IPSEC tunnel context.", "response": "def cli(env, context_id, subnet_id, subnet_type):\n    \"\"\"Remove a subnet from an IPSEC tunnel context.\n\n    The subnet id to remove must be specified.\n\n    Remote subnets are deleted upon removal from a tunnel context.\n\n    A separate configuration request should be made to realize changes on\n    network devices.\n    \"\"\"\n    manager = SoftLayer.IPSECManager(env.client)\n    # ensure context can be retrieved by given id\n    manager.get_tunnel_context(context_id)\n\n    succeeded = False\n    if subnet_type == 'internal':\n        succeeded = manager.remove_internal_subnet(context_id, subnet_id)\n    elif subnet_type == 'remote':\n        succeeded = manager.remove_remote_subnet(context_id, subnet_id)\n    elif subnet_type == 'service':\n        succeeded = manager.remove_service_subnet(context_id, subnet_id)\n\n    if succeeded:\n        env.out('Removed {} subnet #{}'.format(subnet_type, subnet_id))\n    else:\n        raise CLIHalt('Failed to remove {} subnet #{}'\n                      .format(subnet_type, subnet_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, is_open):\n    ticket_mgr = SoftLayer.TicketManager(env.client)\n    table = formatting.Table([\n        'id', 'assigned_user', 'title', 'last_edited', 'status', 'updates', 'priority'\n    ])\n\n    tickets = ticket_mgr.list_tickets(open_status=is_open, closed_status=not is_open)\n    for ticket in tickets:\n        user = formatting.blank()\n        if ticket.get('assignedUser'):\n            user = \"%s %s\" % (ticket['assignedUser']['firstName'], ticket['assignedUser']['lastName'])\n\n        table.add_row([\n            ticket['id'],\n            user,\n            click.wrap_text(ticket['title']),\n            ticket['lastEditDate'],\n            ticket['status']['name'],\n            ticket.get('updateCount', 0),\n            ticket.get('priority', 0)\n        ])\n\n    env.fout(table)", "response": "List available resource items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget details about a security group.", "response": "def cli(env, identifier):\n    \"\"\"Get details about a security group.\"\"\"\n\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    secgroup = mgr.get_securitygroup(identifier)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['id', secgroup['id']])\n    table.add_row(['name', secgroup.get('name') or formatting.blank()])\n    table.add_row(['description',\n                   secgroup.get('description') or formatting.blank()])\n\n    rule_table = formatting.Table(['id', 'remoteIp', 'remoteGroupId',\n                                   'direction', 'ethertype', 'portRangeMin',\n                                   'portRangeMax', 'protocol'])\n    for rule in secgroup.get('rules', []):\n        rg_id = rule.get('remoteGroup', {}).get('id') or formatting.blank()\n        port_min = rule.get('portRangeMin')\n        port_max = rule.get('portRangeMax')\n        if port_min is None:\n            port_min = formatting.blank()\n        if port_max is None:\n            port_max = formatting.blank()\n        rule_table.add_row([rule['id'],\n                            rule.get('remoteIp') or formatting.blank(),\n                            rule.get('remoteGroupId', rg_id),\n                            rule['direction'],\n                            rule.get('ethertype') or formatting.blank(),\n                            port_min,\n                            port_max,\n                            rule.get('protocol') or formatting.blank()])\n\n    table.add_row(['rules', rule_table])\n\n    vsi_table = formatting.Table(['id', 'hostname', 'interface', 'ipAddress'])\n\n    for binding in secgroup.get('networkComponentBindings', []):\n        try:\n            vsi = binding['networkComponent']['guest']\n            vsi_id = vsi['id']\n            hostname = vsi['hostname']\n            interface = ('PRIVATE' if binding['networkComponent']['port'] == 0\n                         else 'PUBLIC')\n            ip_address = (vsi['primaryBackendIpAddress']\n                          if binding['networkComponent']['port'] == 0\n                          else vsi['primaryIpAddress'])\n        except KeyError:\n            vsi_id = \"N/A\"\n            hostname = \"Not enough permission to view\"\n            interface = \"N/A\"\n            ip_address = \"N/A\"\n        vsi_table.add_row([vsi_id, hostname, interface, ip_address])\n\n    table.add_row(['servers', vsi_table])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, record, record_type, data, zone, ttl, priority, protocol, port, service, weight):\n\n    manager = SoftLayer.DNSManager(env.client)\n    record_type = record_type.upper()\n\n    if zone and record_type != 'PTR':\n        zone_id = helpers.resolve_id(manager.resolve_ids, zone, name='zone')\n\n        if record_type == 'MX':\n            manager.create_record_mx(zone_id, record, data, ttl=ttl, priority=priority)\n        elif record_type == 'SRV':\n            manager.create_record_srv(zone_id, record, data, protocol, port, service,\n                                      ttl=ttl, priority=priority, weight=weight)\n        else:\n            manager.create_record(zone_id, record, record_type, data, ttl=ttl)\n\n    elif record_type == 'PTR':\n        manager.create_record_ptr(record, data, ttl=ttl)\n    else:\n        raise exceptions.CLIAbort(\"%s isn't a valid record type or zone is missing\" % record_type)\n\n    click.secho(\"%s record added successfully\" % record_type, fg='green')", "response": "Add resource record.\n\n    Each resource record contains a RECORD and DATA property, defining a resource's name and it's target data.\n    Domains contain multiple types of resource records so it can take one of the following values: A, AAAA, CNAME,\n    MX, SPF, SRV, and PTR.\n\n    About reverse records (PTR), the RECORD value must to be the public Ip Address of device you would like to manage\n    reverse DNS.\n\n        slcli dns record-add 10.10.8.21 PTR myhost.com --ttl=900\n\n    Examples:\n\n        slcli dns record-add myhost.com A 192.168.1.10 --zone=foobar.com --ttl=900\n\n        slcli dns record-add myhost.com AAAA 2001:DB8::1 --zone=foobar.com\n\n        slcli dns record-add 192.168.1.2 MX 192.168.1.10 --zone=foobar.com --priority=11 --ttl=1800\n\n        slcli dns record-add myhost.com TXT \"txt-verification=rXOxyZounZs87oacJSKvbUSIQ\" --zone=2223334\n\n        slcli dns record-add myhost.com SPF \"v=spf1 include:_spf.google.com ~all\" --zone=2223334\n\n        slcli dns record-add myhost.com SRV 192.168.1.10 --zone=2223334 --service=foobar --port=80 --protocol=TCP"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncancel an existing load balancer.", "response": "def cli(env, identifier):\n    \"\"\"Cancel an existing load balancer.\"\"\"\n\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    _, loadbal_id = loadbal.parse_id(identifier)\n\n    if not (env.skip_confirmations or\n            formatting.confirm(\"This action will cancel a load balancer. \"\n                               \"Continue?\")):\n        raise exceptions.CLIAbort('Aborted.')\n\n    mgr.cancel_lb(loadbal_id)\n    env.fout('Load Balancer with id %s is being cancelled!' % identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all available package presets.", "response": "def cli(env, package_keyname, keyword):\n    \"\"\"List package presets.\n\n    .. Note::\n        Presets are set CPU / RAM / Disk allotments. You still need to specify required items.\n        Some packages do not have presets.\n\n    ::\n\n        # List the presets for Bare Metal servers\n        slcli order preset-list BARE_METAL_SERVER\n\n        # List the Bare Metal server presets that include a GPU\n        slcli order preset-list BARE_METAL_SERVER --keyword gpu\n\n    \"\"\"\n    table = formatting.Table(COLUMNS)\n    manager = ordering.OrderingManager(env.client)\n\n    _filter = {}\n    if keyword:\n        _filter = {'activePresets': {'name': {'operation': '*= %s' % keyword}}}\n    presets = manager.list_presets(package_keyname, filter=_filter)\n\n    for preset in presets:\n        table.add_row([\n            preset['name'],\n            preset['keyName'],\n            preset['description']\n        ])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, identifier):\n\n    mgr = SoftLayer.UserManager(env.client)\n    user_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'username')\n    object_mask = \"mask[id, permissions, isMasterUserFlag, roles]\"\n\n    user = mgr.get_user(user_id, object_mask)\n    all_permissions = mgr.get_all_permissions()\n    user_permissions = perms_to_dict(user['permissions'])\n\n    if user['isMasterUserFlag']:\n        click.secho('This account is the Master User and has all permissions enabled', fg='green')\n\n    env.fout(roles_table(user))\n    env.fout(permission_table(user_permissions, all_permissions))", "response": "List all roles and permissions for a user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a table of available permissions", "response": "def permission_table(user_permissions, all_permissions):\n    \"\"\"Creates a table of available permissions\"\"\"\n\n    table = formatting.Table(['Description', 'KeyName', 'Assigned'])\n    table.align['KeyName'] = 'l'\n    table.align['Description'] = 'l'\n    table.align['Assigned'] = 'l'\n    for perm in all_permissions:\n        assigned = user_permissions.get(perm['keyName'], False)\n        table.add_row([perm['name'], perm['keyName'], assigned])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a table for a users roles", "response": "def roles_table(user):\n    \"\"\"Creates a table for a users roles\"\"\"\n    table = formatting.Table(['id', 'Role Name', 'Description'])\n    for role in user['roles']:\n        table.add_row([role['id'], role['name'], role['description']])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_with_like_args(ctx, _, value):\n    if value is None:\n        return\n\n    env = ctx.ensure_object(environment.Environment)\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, value, 'VS')\n    like_details = vsi.get_instance(vs_id)\n    like_args = {\n        'hostname': like_details['hostname'],\n        'domain': like_details['domain'],\n        'hourly': like_details['hourlyBillingFlag'],\n        'datacenter': like_details['datacenter']['name'],\n        'network': like_details['networkComponents'][0]['maxSpeed'],\n        'userdata': like_details['userData'] or None,\n        'postinstall': like_details.get('postInstallScriptUri'),\n        'dedicated': like_details['dedicatedAccountHostOnlyFlag'],\n        'private': like_details['privateNetworkOnlyFlag'],\n        'placement_id': like_details.get('placementGroupId', None),\n    }\n\n    like_args['flavor'] = utils.lookup(like_details,\n                                       'billingItem',\n                                       'orderItem',\n                                       'preset',\n                                       'keyName')\n    if not like_args['flavor']:\n        like_args['cpu'] = like_details['maxCpu']\n        like_args['memory'] = '%smb' % like_details['maxMemory']\n\n    tag_refs = like_details.get('tagReferences', None)\n    if tag_refs is not None and len(tag_refs) > 0:\n        like_args['tag'] = [t['tag']['name'] for t in tag_refs]\n\n    # Handle mutually exclusive options\n    like_image = utils.lookup(like_details,\n                              'blockDeviceTemplateGroup',\n                              'globalIdentifier')\n    like_os = utils.lookup(like_details,\n                           'operatingSystem',\n                           'softwareLicense',\n                           'softwareDescription',\n                           'referenceCode')\n    if like_image:\n        like_args['image'] = like_image\n    elif like_os:\n        like_args['os'] = like_os\n\n    if ctx.default_map is None:\n        ctx.default_map = {}\n    ctx.default_map.update(like_args)", "response": "Update arguments with options taken from a currently running VS."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert CLI arguments to args for VSManager. create_instance.", "response": "def _parse_create_args(client, args):\n    \"\"\"Converts CLI arguments to args for VSManager.create_instance.\n\n    :param dict args: CLI arguments\n    \"\"\"\n    data = {\n        \"hourly\": args.get('billing', 'hourly') == 'hourly',\n        \"cpus\": args.get('cpu', None),\n        \"ipv6\": args.get('ipv6', None),\n        \"disks\": args.get('disk', None),\n        \"os_code\": args.get('os', None),\n        \"memory\": args.get('memory', None),\n        \"flavor\": args.get('flavor', None),\n        \"domain\": args.get('domain', None),\n        \"host_id\": args.get('host_id', None),\n        \"private\": args.get('private', None),\n        \"hostname\": args.get('hostname', None),\n        \"nic_speed\": args.get('network', None),\n        \"boot_mode\": args.get('boot_mode', None),\n        \"dedicated\": args.get('dedicated', None),\n        \"post_uri\": args.get('postinstall', None),\n        \"datacenter\": args.get('datacenter', None),\n        \"public_vlan\": args.get('vlan_public', None),\n        \"private_vlan\": args.get('vlan_private', None),\n        \"public_subnet\": args.get('subnet_public', None),\n        \"private_subnet\": args.get('subnet_private', None),\n    }\n\n    # The primary disk is included in the flavor and the local_disk flag is not needed\n    # Setting it to None prevents errors from the flag not matching the flavor\n    if not args.get('san') and args.get('flavor'):\n        data['local_disk'] = None\n    else:\n        data['local_disk'] = not args.get('san')\n\n    if args.get('image'):\n        if args.get('image').isdigit():\n            image_mgr = SoftLayer.ImageManager(client)\n            image_details = image_mgr.get_image(args.get('image'), mask=\"id,globalIdentifier\")\n            data['image_id'] = image_details['globalIdentifier']\n        else:\n            data['image_id'] = args['image']\n\n    if args.get('userdata'):\n        data['userdata'] = args['userdata']\n    elif args.get('userfile'):\n        with open(args['userfile'], 'r') as userfile:\n            data['userdata'] = userfile.read()\n\n    # Get the SSH keys\n    if args.get('key'):\n        keys = []\n        for key in args.get('key'):\n            resolver = SoftLayer.SshKeyManager(client).resolve_ids\n            key_id = helpers.resolve_id(resolver, key, 'SshKey')\n            keys.append(key_id)\n        data['ssh_keys'] = keys\n\n    if args.get('public_security_group'):\n        pub_groups = args.get('public_security_group')\n        data['public_security_groups'] = [group for group in pub_groups]\n\n    if args.get('private_security_group'):\n        priv_groups = args.get('private_security_group')\n        data['private_security_groups'] = [group for group in priv_groups]\n\n    if args.get('tag', False):\n        data['tags'] = ','.join(args['tag'])\n\n    if args.get('host_id'):\n        data['host_id'] = args['host_id']\n\n    if args.get('placementgroup'):\n        resolver = SoftLayer.managers.PlacementManager(client).resolve_ids\n        data['placement_id'] = helpers.resolve_id(resolver, args.get('placementgroup'), 'PlacementGroup')\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, **args):\n\n    vsi = SoftLayer.VSManager(env.client)\n    _validate_args(env, args)\n    create_args = _parse_create_args(env.client, args)\n    test = args.get('test', False)\n    do_create = not (args.get('export') or test)\n\n    if do_create:\n        if not (env.skip_confirmations or formatting.confirm(\n                \"This action will incur charges on your account. Continue?\")):\n            raise exceptions.CLIAbort('Aborting virtual server order.')\n\n    if args.get('export'):\n        export_file = args.pop('export')\n        template.export_to_template(export_file, args, exclude=['wait', 'test'])\n        env.fout('Successfully exported options to a template file.')\n\n    else:\n        result = vsi.order_guest(create_args, test)\n        output = _build_receipt_table(result, args.get('billing'), test)\n\n        if do_create:\n            env.fout(_build_guest_table(result))\n        env.fout(output)\n\n        if args.get('wait'):\n            virtual_guests = utils.lookup(result, 'orderDetails', 'virtualGuests')\n            guest_id = virtual_guests[0]['id']\n            click.secho(\"Waiting for %s to finish provisioning...\" % guest_id, fg='green')\n            ready = vsi.wait_for_ready(guest_id, args.get('wait') or 1)\n            if ready is False:\n                env.out(env.fmt(output))\n                raise exceptions.CLIHalt(code=1)", "response": "Order or create virtual server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a table that contains the total recurring fee of the items prices", "response": "def _build_receipt_table(result, billing=\"hourly\", test=False):\n    \"\"\"Retrieve the total recurring fee of the items prices\"\"\"\n    title = \"OrderId: %s\" % (result.get('orderId', 'No order placed'))\n    table = formatting.Table(['Cost', 'Description'], title=title)\n    table.align['Cost'] = 'r'\n    table.align['Description'] = 'l'\n    total = 0.000\n    if test:\n        prices = result['prices']\n    else:\n        prices = result['orderDetails']['prices']\n\n    for item in prices:\n        rate = 0.000\n        if billing == \"hourly\":\n            rate += float(item.get('hourlyRecurringFee', 0.000))\n        else:\n            rate += float(item.get('recurringFee', 0.000))\n        total += rate\n        table.add_row([rate, item['item']['description']])\n    table.add_row([\"%.3f\" % total, \"Total %s cost\" % billing])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_args(env, args):\n\n    if all([args['cpu'], args['flavor']]):\n        raise exceptions.ArgumentError(\n            '[-c | --cpu] not allowed with [-f | --flavor]')\n\n    if all([args['memory'], args['flavor']]):\n        raise exceptions.ArgumentError(\n            '[-m | --memory] not allowed with [-f | --flavor]')\n\n    if all([args['dedicated'], args['flavor']]):\n        raise exceptions.ArgumentError(\n            '[-d | --dedicated] not allowed with [-f | --flavor]')\n\n    if all([args['host_id'], args['flavor']]):\n        raise exceptions.ArgumentError(\n            '[-h | --host-id] not allowed with [-f | --flavor]')\n\n    if all([args['userdata'], args['userfile']]):\n        raise exceptions.ArgumentError(\n            '[-u | --userdata] not allowed with [-F | --userfile]')\n\n    image_args = [args['os'], args['image']]\n    if all(image_args):\n        raise exceptions.ArgumentError(\n            '[-o | --os] not allowed with [--image]')\n\n    while not any([args['os'], args['image']]):\n        args['os'] = env.input(\"Operating System Code\", default=\"\", show_default=False)\n        if not args['os']:\n            args['image'] = env.input(\"Image\", default=\"\", show_default=False)", "response": "Raises an ArgumentError if the given arguments are not valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupgrades a virtual server.", "response": "def cli(env, identifier, cpu, private, memory, network, flavor):\n    \"\"\"Upgrade a virtual server.\"\"\"\n\n    vsi = SoftLayer.VSManager(env.client)\n\n    if not any([cpu, memory, network, flavor]):\n        raise exceptions.ArgumentError(\"Must provide [--cpu], [--memory], [--network], or [--flavor] to upgrade\")\n\n    if private and not cpu:\n        raise exceptions.ArgumentError(\"Must specify [--cpu] when using [--private]\")\n\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    if not (env.skip_confirmations or formatting.confirm(\"This action will incur charges on your account. Continue?\")):\n        raise exceptions.CLIAbort('Aborted')\n\n    if memory:\n        memory = int(memory / 1024)\n\n    if not vsi.upgrade(vs_id, cpus=cpu, memory=memory, nic_speed=network, public=not private, preset=flavor):\n        raise exceptions.CLIAbort('VS Upgrade Failed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npowers on a server.", "response": "def power_on(env, identifier):\n    \"\"\"Power on a server.\"\"\"\n\n    mgr = SoftLayer.HardwareManager(env.client)\n    hw_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'hardware')\n    env.client['Hardware_Server'].powerOn(id=hw_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef power_cycle(env, identifier):\n\n    mgr = SoftLayer.HardwareManager(env.client)\n    hw_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'hardware')\n\n    if not (env.skip_confirmations or\n            formatting.confirm('This will power off the server with id %s. '\n                               'Continue?' % hw_id)):\n        raise exceptions.CLIAbort('Aborted.')\n\n    env.client['Hardware_Server'].powerCycle(id=hw_id)", "response": "Power cycle a server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nediting details of a security group.", "response": "def cli(env, group_id, name, description):\n    \"\"\"Edit details of a security group.\"\"\"\n    mgr = SoftLayer.NetworkManager(env.client)\n    data = {}\n    if name:\n        data['name'] = name\n    if description:\n        data['description'] = description\n\n    if not mgr.edit_securitygroup(group_id, **data):\n        raise exceptions.CLIAbort(\"Failed to edit security group\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, date_min, date_max, obj_event, obj_id, obj_type, utc_offset, metadata, limit):\n    columns = ['Event', 'Object', 'Type', 'Date', 'Username']\n\n    event_mgr = SoftLayer.EventLogManager(env.client)\n    user_mgr = SoftLayer.UserManager(env.client)\n    request_filter = event_mgr.build_filter(date_min, date_max, obj_event, obj_id, obj_type, utc_offset)\n    logs = event_mgr.get_event_logs(request_filter)\n    log_time = \"%Y-%m-%dT%H:%M:%S.%f%z\"\n    user_data = {}\n\n    if metadata:\n        columns.append('Metadata')\n\n    row_count = 0\n    click.secho(\", \".join(columns))\n    for log in logs:\n        if log is None:\n            click.secho('No logs available for filter %s.' % request_filter, fg='red')\n            return\n\n        user = log['userType']\n        label = log.get('label', '')\n        if user == \"CUSTOMER\":\n            username = user_data.get(log['userId'])\n            if username is None:\n                username = user_mgr.get_user(log['userId'], \"mask[username]\")['username']\n                user_data[log['userId']] = username\n            user = username\n\n        if metadata:\n            metadata_data = log['metaData'].strip(\"\\n\\t\")\n\n            click.secho(\"'{0}','{1}','{2}','{3}','{4}','{5}'\".format(\n                log['eventName'],\n                label,\n                log['objectName'],\n                utils.clean_time(log['eventCreateDate'], in_format=log_time),\n                user,\n                metadata_data))\n        else:\n            click.secho(\"'{0}','{1}','{2}','{3}','{4}'\".format(\n                log['eventName'],\n                label,\n                log['objectName'],\n                utils.clean_time(log['eventCreateDate'], in_format=log_time),\n                user))\n\n        row_count = row_count + 1\n        if row_count >= limit and limit != -1:\n            return", "response": "Get Event Logs in a SeLXML format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env, volume_id, new_size, new_iops, new_tier):\n    file_manager = SoftLayer.FileStorageManager(env.client)\n\n    if new_tier is not None:\n        new_tier = float(new_tier)\n\n    try:\n        order = file_manager.order_modified_volume(\n            volume_id,\n            new_size=new_size,\n            new_iops=new_iops,\n            new_tier_level=new_tier,\n        )\n    except ValueError as ex:\n        raise exceptions.ArgumentError(str(ex))\n\n    if 'placedOrder' in order.keys():\n        click.echo(\"Order #{0} placed successfully!\".format(order['placedOrder']['id']))\n        for item in order['placedOrder']['items']:\n            click.echo(\" > %s\" % item['description'])\n    else:\n        click.echo(\"Order could not be placed! Please verify your options and try again.\")", "response": "Modify an existing file storage volume."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all users on an account", "response": "def list_users(self, objectmask=None, objectfilter=None):\n        \"\"\"Lists all users on an account\n\n        :param string objectmask: Used to overwrite the default objectmask.\n        :param dictionary objectfilter: If you want to use an objectfilter.\n        :returns: A list of dictionaries that describe each user\n\n        Example::\n            result = mgr.list_users()\n        \"\"\"\n\n        if objectmask is None:\n            objectmask = \"\"\"mask[id, username, displayName, userStatus[name], hardwareCount, virtualGuestCount,\n                                 email, roles]\"\"\"\n\n        return self.account_service.getUsers(mask=objectmask, filter=objectfilter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user(self, user_id, objectmask=None):\n        if objectmask is None:\n            objectmask = \"mask[userStatus[name], parent[id, username]]\"\n        return self.user_service.getObject(id=user_id, mask=objectmask)", "response": "Calls SoftLayer_User_Customer::getObject\n\n        :param int user_id: Id of the user\n        :param string objectmask: default is 'mask[userStatus[name], parent[id, username]]'\n        :returns: A user object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall SoftLayer_Account :: getCurrentUser", "response": "def get_current_user(self, objectmask=None):\n        \"\"\"Calls SoftLayer_Account::getCurrentUser\"\"\"\n\n        if objectmask is None:\n            objectmask = \"mask[userStatus[name], parent[id, username]]\"\n        return self.account_service.getCurrentUser(mask=objectmask)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_permissions(self):\n        if self.all_permissions is None:\n            permissions = self.client.call('User_Customer_CustomerPermission_Permission', 'getAllObjects')\n            self.all_permissions = sorted(permissions, key=itemgetter('keyName'))\n        return self.all_permissions", "response": "Calls SoftLayer_User_Customer_Permissions_Permission :: getAllObjects\n        Stores the result in self. all_permissions\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nenables a list of permissions for a user", "response": "def add_permissions(self, user_id, permissions):\n        \"\"\"Enables a list of permissions for a user\n\n        :param int id: user id to set\n        :param list permissions: List of permissions keynames to enable\n        :returns: True on success, Exception otherwise\n\n        Example::\n            add_permissions(123, ['BANDWIDTH_MANAGE'])\n        \"\"\"\n        pretty_permissions = self.format_permission_object(permissions)\n        LOGGER.warning(\"Adding the following permissions to %s: %s\", user_id, pretty_permissions)\n        return self.user_service.addBulkPortalPermission(pretty_permissions, id=user_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_permissions(self, user_id, permissions):\n        pretty_permissions = self.format_permission_object(permissions)\n        LOGGER.warning(\"Removing the following permissions to %s: %s\", user_id, pretty_permissions)\n        return self.user_service.removeBulkPortalPermission(pretty_permissions, id=user_id)", "response": "Disables a list of permissions for a user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets user_id s permission to be the same as from_user_id s permission.", "response": "def permissions_from_user(self, user_id, from_user_id):\n        \"\"\"Sets user_id's permission to be the same as from_user_id's\n\n        Any permissions from_user_id has will be added to user_id.\n        Any permissions from_user_id doesn't have will be removed from user_id.\n\n        :param int user_id: The user to change permissions.\n        :param int from_user_id: The use to base permissions from.\n        :returns: True on success, Exception otherwise.\n        \"\"\"\n\n        from_permissions = self.get_user_permissions(from_user_id)\n        self.add_permissions(user_id, from_permissions)\n        all_permissions = self.get_all_permissions()\n        remove_permissions = []\n\n        for permission in all_permissions:\n            # If permission does not exist for from_user_id add it to the list to be removed\n            if _keyname_search(from_permissions, permission['keyName']):\n                continue\n            else:\n                remove_permissions.append({'keyName': permission['keyName']})\n\n        self.remove_permissions(user_id, remove_permissions)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_permissions(self, user_id):\n        permissions = self.user_service.getPermissions(id=user_id)\n        return sorted(permissions, key=itemgetter('keyName'))", "response": "Returns a sorted list of a users permissions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_logins(self, user_id, start_date=None):\n\n        if start_date is None:\n            date_object = datetime.datetime.today() - datetime.timedelta(days=30)\n            start_date = date_object.strftime(\"%m/%d/%Y 0:0:0\")\n\n        date_filter = {\n            'loginAttempts': {\n                'createDate': {\n                    'operation': 'greaterThanDate',\n                    'options': [{'name': 'date', 'value': [start_date]}]\n                }\n            }\n        }\n        login_log = self.user_service.getLoginAttempts(id=user_id, filter=date_filter)\n        return login_log", "response": "Gets the login history for a user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_events(self, user_id, start_date=None):\n\n        if start_date is None:\n            date_object = datetime.datetime.today() - datetime.timedelta(days=30)\n            start_date = date_object.strftime(\"%Y-%m-%dT00:00:00\")\n\n        object_filter = {\n            'userId': {\n                'operation': user_id\n            },\n            'eventCreateDate': {\n                'operation': 'greaterThanDate',\n                'options': [{'name': 'date', 'value': [start_date]}]\n            }\n        }\n\n        events = self.client.call('Event_Log', 'getAllObjects', filter=object_filter)\n        if events is None:\n            events = [{'eventName': 'No Events Found'}]\n        return events", "response": "Get the events for a specific user"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_id_from_username(self, username):\n        _mask = \"mask[id, username]\"\n        _filter = {'users': {'username': utils.query_filter(username)}}\n        user = self.list_users(_mask, _filter)\n        if len(user) == 1:\n            return [user[0]['id']]\n        elif len(user) > 1:\n            raise exceptions.SoftLayerError(\"Multiple users found with the name: %s\" % username)\n        else:\n            raise exceptions.SoftLayerError(\"Unable to find user id for %s\" % username)", "response": "Looks up a username s id\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting a list of permission key names into something the SLAPI will respect.", "response": "def format_permission_object(self, permissions):\n        \"\"\"Formats a list of permission key names into something the SLAPI will respect.\n\n        :param list permissions: A list of SLAPI permissions keyNames.\n                                 keyName of ALL will return all permissions.\n        :returns: list of dictionaries that can be sent to the api to add or remove permissions\n        :throws SoftLayerError: If any permission is invalid this exception will be thrown.\n        \"\"\"\n        pretty_permissions = []\n        available_permissions = self.get_all_permissions()\n        # pp(available_permissions)\n        for permission in permissions:\n            # Handle data retrieved directly from the API\n            if isinstance(permission, dict):\n                permission = permission['keyName']\n            permission = permission.upper()\n            if permission == 'ALL':\n                return available_permissions\n            # Search through available_permissions to make sure what the user entered was valid\n            if _keyname_search(available_permissions, permission):\n                pretty_permissions.append({'keyName': permission})\n            else:\n                raise exceptions.SoftLayerError(\"'%s' is not a valid permission\" % permission)\n        return pretty_permissions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting configuration of a tunnel context.", "response": "def cli(env, context_id):\n    \"\"\"Request configuration of a tunnel context.\n\n    This action will update the advancedConfigurationFlag on the context\n    instance and further modifications against the context will be prevented\n    until all changes can be propgated to network devices.\n    \"\"\"\n    manager = SoftLayer.IPSECManager(env.client)\n    # ensure context can be retrieved by given id\n    manager.get_tunnel_context(context_id)\n\n    succeeded = manager.apply_configuration(context_id)\n    if succeeded:\n        env.out('Configuration request received for context #{}'\n                .format(context_id))\n    else:\n        raise CLIHalt('Failed to enqueue configuration request for context #{}'\n                      .format(context_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns zone ID based on a zone name.", "response": "def _get_zone_id_from_name(self, name):\n        \"\"\"Return zone ID based on a zone.\"\"\"\n        results = self.client['Account'].getDomains(\n            filter={\"domains\": {\"name\": utils.query_filter(name)}})\n        return [x['id'] for x in results]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a zone and its records.", "response": "def get_zone(self, zone_id, records=True):\n        \"\"\"Get a zone and its records.\n\n        :param zone: the zone name\n        :returns: A dictionary containing a large amount of information about\n                  the specified zone.\n\n        \"\"\"\n        mask = None\n        if records:\n            mask = 'resourceRecords'\n        return self.service.getObject(id=zone_id, mask=mask)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_zone(self, zone, serial=None):\n        return self.service.createObject({\n            'name': zone,\n            'serial': serial or time.strftime('%Y%m%d01'),\n            \"resourceRecords\": {}})", "response": "Create a zone for the specified zone."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a resource record on a domain.", "response": "def create_record(self, zone_id, record, record_type, data, ttl=60):\n        \"\"\"Create a resource record on a domain.\n\n        :param integer id: the zone's ID\n        :param record: the name of the record to add\n        :param record_type: the type of record (A, AAAA, CNAME, TXT, etc.)\n        :param data: the record's value\n        :param integer ttl: the TTL or time-to-live value (default: 60)\n\n        \"\"\"\n        resource_record = self._generate_create_dict(record, record_type, data,\n                                                     ttl, domainId=zone_id)\n        return self.record.createObject(resource_record)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_record_mx(self, zone_id, record, data, ttl=60, priority=10):\n        resource_record = self._generate_create_dict(record, 'MX', data, ttl,\n                                                     domainId=zone_id, mxPriority=priority)\n        return self.record.createObject(resource_record)", "response": "Create a mx resource record on a domain."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a resource record on a domain.", "response": "def create_record_srv(self, zone_id, record, data, protocol, port, service,\n                          ttl=60, priority=20, weight=10):\n        \"\"\"Create a resource record on a domain.\n\n        :param integer id: the zone's ID\n        :param record: the name of the record to add\n        :param data: the record's value\n        :param string protocol: the protocol of the service, usually either TCP or UDP.\n        :param integer port: the TCP or UDP port on which the service is to be found.\n        :param string service: the symbolic name of the desired service.\n        :param integer ttl: the TTL or time-to-live value (default: 60)\n        :param integer priority: the priority of the target host (default: 20)\n        :param integer weight: relative weight for records with same priority (default: 10)\n\n        \"\"\"\n        resource_record = self._generate_create_dict(record, 'SRV', data, ttl, domainId=zone_id,\n                                                     priority=priority, protocol=protocol, port=port,\n                                                     service=service, weight=weight)\n\n        # The createObject won't creates SRV records unless we send the following complexType.\n        resource_record['complexType'] = 'SoftLayer_Dns_Domain_ResourceRecord_SrvType'\n\n        return self.record.createObject(resource_record)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a reverse record.", "response": "def create_record_ptr(self, record, data, ttl=60):\n        \"\"\"Create a reverse record.\n\n        :param record: the public ip address of device for which you would like to manage reverse DNS.\n        :param data: the record's value\n        :param integer ttl: the TTL or time-to-live value (default: 60)\n\n        \"\"\"\n        resource_record = self._generate_create_dict(record, 'PTR', data, ttl)\n\n        return self.record.createObject(resource_record)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dict appropriate to pass into Dns_Domain_ResourceRecord :: createObject", "response": "def _generate_create_dict(record, record_type, data, ttl, **kwargs):\n        \"\"\"Returns a dict appropriate to pass into Dns_Domain_ResourceRecord::createObject\"\"\"\n\n        # Basic dns record structure\n        resource_record = {\n            'host': record,\n            'data': data,\n            'ttl': ttl,\n            'type': record_type\n        }\n\n        for (key, value) in kwargs.items():\n            resource_record.setdefault(key, value)\n\n        return resource_record"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_records(self, zone_id, ttl=None, data=None, host=None,\n                    record_type=None):\n        \"\"\"List, and optionally filter, records within a zone.\n\n        :param zone: the zone name in which to search.\n        :param int ttl: time in seconds\n        :param str data: the records data\n        :param str host: record's host\n        :param str record_type: the type of record\n\n        :returns: A list of dictionaries representing the matching records\n                  within the specified zone.\n        \"\"\"\n        _filter = utils.NestedDict()\n\n        if ttl:\n            _filter['resourceRecords']['ttl'] = utils.query_filter(ttl)\n\n        if host:\n            _filter['resourceRecords']['host'] = utils.query_filter(host)\n\n        if data:\n            _filter['resourceRecords']['data'] = utils.query_filter(data)\n\n        if record_type:\n            _filter['resourceRecords']['type'] = utils.query_filter(\n                record_type.lower())\n\n        results = self.service.getResourceRecords(\n            id=zone_id,\n            mask='id,expire,domainId,host,minimum,refresh,retry,'\n            'mxPriority,ttl,type,data,responsiblePerson',\n            filter=_filter.to_dict(),\n        )\n\n        return results", "response": "List and optionally filter records within a zone."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cancel_guests(self, host_id):\n        result = []\n\n        guests = self.host.getGuests(id=host_id, mask='id,fullyQualifiedDomainName')\n\n        if guests:\n            for vs in guests:\n                status_info = {\n                    'id': vs['id'],\n                    'fqdn': vs['fullyQualifiedDomainName'],\n                    'status': self._delete_guest(vs['id'])\n                }\n                result.append(status_info)\n\n        return result", "response": "Cancel all guests into the dedicated host immediately."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_guests(self, host_id, tags=None, cpus=None, memory=None, hostname=None,\n                    domain=None, local_disk=None, nic_speed=None, public_ip=None,\n                    private_ip=None, **kwargs):\n        \"\"\"Retrieve a list of all virtual servers on the dedicated host.\n\n        Example::\n\n            # Print out a list of instances with 4 cpu cores in the host id 12345.\n\n            for vsi in mgr.list_guests(host_id=12345, cpus=4):\n               print vsi['fullyQualifiedDomainName'], vsi['primaryIpAddress']\n\n            # Using a custom object-mask. Will get ONLY what is specified\n            object_mask = \"mask[hostname,monitoringRobot[robotStatus]]\"\n            for vsi in mgr.list_guests(mask=object_mask,cpus=4):\n                print vsi\n\n        :param integer host_id: the identifier of dedicated host\n        :param list tags: filter based on list of tags\n        :param integer cpus: filter based on number of CPUS\n        :param integer memory: filter based on amount of memory\n        :param string hostname: filter based on hostname\n        :param string domain: filter based on domain\n        :param string local_disk: filter based on local_disk\n        :param integer nic_speed: filter based on network speed (in MBPS)\n        :param string public_ip: filter based on public ip address\n        :param string private_ip: filter based on private ip address\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        :returns: Returns a list of dictionaries representing the matching\n                  virtual servers\n        \"\"\"\n        if 'mask' not in kwargs:\n            items = [\n                'id',\n                'globalIdentifier',\n                'hostname',\n                'domain',\n                'fullyQualifiedDomainName',\n                'primaryBackendIpAddress',\n                'primaryIpAddress',\n                'lastKnownPowerState.name',\n                'hourlyBillingFlag',\n                'powerState',\n                'maxCpu',\n                'maxMemory',\n                'datacenter',\n                'activeTransaction.transactionStatus[friendlyName,name]',\n                'status',\n            ]\n            kwargs['mask'] = \"mask[%s]\" % ','.join(items)\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n\n        if tags:\n            _filter['guests']['tagReferences']['tag']['name'] = {\n                'operation': 'in',\n                'options': [{'name': 'data', 'value': tags}],\n            }\n\n        if cpus:\n            _filter['guests']['maxCpu'] = utils.query_filter(cpus)\n\n        if memory:\n            _filter['guests']['maxMemory'] = utils.query_filter(memory)\n\n        if hostname:\n            _filter['guests']['hostname'] = utils.query_filter(hostname)\n\n        if domain:\n            _filter['guests']['domain'] = utils.query_filter(domain)\n\n        if local_disk is not None:\n            _filter['guests']['localDiskFlag'] = (\n                utils.query_filter(bool(local_disk)))\n\n        if nic_speed:\n            _filter['guests']['networkComponents']['maxSpeed'] = (\n                utils.query_filter(nic_speed))\n\n        if public_ip:\n            _filter['guests']['primaryIpAddress'] = (\n                utils.query_filter(public_ip))\n\n        if private_ip:\n            _filter['guests']['primaryBackendIpAddress'] = (\n                utils.query_filter(private_ip))\n\n        kwargs['filter'] = _filter.to_dict()\n        kwargs['iter'] = True\n        return self.host.getGuests(id=host_id, **kwargs)", "response": "Retrieve a list of all virtual servers on the dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a list of all dedicated hosts on the account.", "response": "def list_instances(self, tags=None, cpus=None, memory=None, hostname=None,\n                       disk=None, datacenter=None, **kwargs):\n        \"\"\"Retrieve a list of all dedicated hosts on the account\n\n        :param list tags: filter based on list of tags\n        :param integer cpus: filter based on number of CPUS\n        :param integer memory: filter based on amount of memory\n        :param string hostname: filter based on hostname\n        :param string disk: filter based on disk\n        :param string datacenter: filter based on datacenter\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        :returns: Returns a list of dictionaries representing the matching dedicated host.\n\n        \"\"\"\n        if 'mask' not in kwargs:\n            items = [\n                'id',\n                'name',\n                'cpuCount',\n                'diskCapacity',\n                'memoryCapacity',\n                'datacenter',\n                'guestCount',\n            ]\n            kwargs['mask'] = \"mask[%s]\" % ','.join(items)\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n        if tags:\n            _filter['dedicatedHosts']['tagReferences']['tag']['name'] = {\n                'operation': 'in',\n                'options': [{'name': 'data', 'value': tags}],\n            }\n\n        if hostname:\n            _filter['dedicatedHosts']['name'] = (\n                utils.query_filter(hostname)\n            )\n\n        if cpus:\n            _filter['dedicatedHosts']['cpuCount'] = utils.query_filter(cpus)\n\n        if disk:\n            _filter['dedicatedHosts']['diskCapacity'] = (\n                utils.query_filter(disk))\n\n        if memory:\n            _filter['dedicatedHosts']['memoryCapacity'] = (\n                utils.query_filter(memory))\n\n        if datacenter:\n            _filter['dedicatedHosts']['datacenter']['name'] = (\n                utils.query_filter(datacenter))\n\n        kwargs['filter'] = _filter.to_dict()\n        return self.account.getDedicatedHosts(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets details about a dedicated host.", "response": "def get_host(self, host_id, **kwargs):\n        \"\"\"Get details about a dedicated host.\n\n        :param integer : the host ID\n        :returns: A dictionary containing host information.\n\n        Example::\n\n            # Print out host ID 12345.\n            dh = mgr.get_host(12345)\n            print dh\n\n            # Print out only name and backendRouter for instance 12345\n            object_mask = \"mask[name,backendRouter[id]]\"\n            dh = mgr.get_host(12345, mask=mask)\n            print dh\n\n        \"\"\"\n        if 'mask' not in kwargs:\n            kwargs['mask'] = ('''\n                id,\n                name,\n                cpuCount,\n                memoryCapacity,\n                diskCapacity,\n                createDate,\n                modifyDate,\n                backendRouter[\n                    id,\n                    hostname,\n                    domain\n                ],\n                billingItem[\n                    id,\n                    nextInvoiceTotalRecurringAmount,\n                    children[\n                        categoryCode,\n                        nextInvoiceTotalRecurringAmount\n                    ],\n                    orderItem[\n                        id,\n                        order.userRecord[\n                            username\n                        ]\n                    ]\n                ],\n                datacenter[\n                    id,\n                    name,\n                    longName\n                ],\n                guests[\n                    id,\n                    hostname,\n                    domain,\n                    uuid\n                ],\n                guestCount\n            ''')\n\n        return self.host.getObject(id=host_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplace an order for a dedicated host.", "response": "def place_order(self, hostname, domain, location, flavor, hourly, router=None):\n        \"\"\"Places an order for a dedicated host.\n\n        See get_create_options() for valid arguments.\n\n        :param string hostname: server hostname\n        :param string domain: server domain name\n        :param string location: location (datacenter) name\n        :param boolean hourly: True if using hourly pricing (default).\n                               False for monthly.\n        :param int router: an optional value for selecting a backend router\n        \"\"\"\n        create_options = self._generate_create_dict(hostname=hostname,\n                                                    router=router,\n                                                    domain=domain,\n                                                    flavor=flavor,\n                                                    datacenter=location,\n                                                    hourly=hourly)\n\n        return self.client['Product_Order'].placeOrder(create_options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_order(self, hostname, domain, location, hourly, flavor, router=None):\n\n        create_options = self._generate_create_dict(hostname=hostname,\n                                                    router=router,\n                                                    domain=domain,\n                                                    flavor=flavor,\n                                                    datacenter=location,\n                                                    hourly=hourly)\n\n        return self.client['Product_Order'].verifyOrder(create_options)", "response": "Verifies an order for a dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntranslate args into a dictionary for creating a dedicated host.", "response": "def _generate_create_dict(self,\n                              hostname=None,\n                              domain=None,\n                              flavor=None,\n                              router=None,\n                              datacenter=None,\n                              hourly=True):\n        \"\"\"Translates args into a dictionary for creating a dedicated host.\"\"\"\n        package = self._get_package()\n        item = self._get_item(package, flavor)\n\n        location = self._get_location(package['regions'], datacenter)\n        price = self._get_price(item)\n\n        routers = self._get_backend_router(\n            location['location']['locationPackageDetails'], item)\n\n        router = self._get_default_router(routers, router)\n\n        hardware = {\n            'hostname': hostname,\n            'domain': domain,\n            'primaryBackendNetworkComponent': {\n                'router': {\n                    'id': router\n                }\n            }\n        }\n\n        complex_type = \"SoftLayer_Container_Product_Order_Virtual_DedicatedHost\"\n\n        order = {\n            \"complexType\": complex_type,\n            \"quantity\": 1,\n            'location': location['keyname'],\n            'packageId': package['id'],\n            'prices': [{'id': price}],\n            'hardware': [hardware],\n            'useHourlyPricing': hourly,\n        }\n        return order"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_location(self, regions, datacenter):\n        for region in regions:\n            # list of locations\n            if region['location']['location']['name'] == datacenter:\n                return region\n\n        raise SoftLayer.SoftLayerError(\"Could not find valid location for: '%s'\" % datacenter)", "response": "Get the longer key with a short location name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning valid options for ordering a dedicated host.", "response": "def get_create_options(self):\n        \"\"\"Returns valid options for ordering a dedicated host.\"\"\"\n        package = self._get_package()\n        # Locations\n        locations = []\n        for region in package['regions']:\n            locations.append({\n                'name': region['location']['location']['longName'],\n                'key': region['location']['location']['name'],\n            })\n        # flavors\n        dedicated_host = []\n        for item in package['items']:\n            if item['itemCategory']['categoryCode'] == \\\n                    'dedicated_virtual_hosts':\n                dedicated_host.append({\n                    'name': item['description'],\n                    'key': item['keyName'],\n                })\n\n        return {'locations': locations, 'dedicated_host': dedicated_host}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_price(self, package):\n\n        for price in package['prices']:\n            if not price.get('locationGroupId'):\n                return price['id']\n\n        raise SoftLayer.SoftLayerError(\"Could not find valid price\")", "response": "Returns valid price for ordering a dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_item(self, package, flavor):\n\n        for item in package['items']:\n            if item['keyName'] == flavor:\n                return item\n\n        raise SoftLayer.SoftLayerError(\"Could not find valid item for: '%s'\" % flavor)", "response": "Returns the item for ordering a dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_backend_router(self, locations, item):\n        mask = '''\n            id,\n            hostname\n        '''\n        cpu_count = item['capacity']\n\n        for capacity in item['bundleItems']:\n            for category in capacity['categories']:\n                if category['categoryCode'] == 'dedicated_host_ram':\n                    mem_capacity = capacity['capacity']\n                if category['categoryCode'] == 'dedicated_host_disk':\n                    disk_capacity = capacity['capacity']\n\n        for hardwareComponent in item['bundleItems']:\n            if hardwareComponent['keyName'].find(\"GPU\") != -1:\n                hardwareComponentType = hardwareComponent['hardwareGenericComponentModel']['hardwareComponentType']\n                gpuComponents = [\n                    {\n                        'hardwareComponentModel': {\n                            'hardwareGenericComponentModel': {\n                                'id': hardwareComponent['hardwareGenericComponentModel']['id'],\n                                'hardwareComponentType': {\n                                    'keyName': hardwareComponentType['keyName']\n                                }\n                            }\n                        }\n                    },\n                    {\n                        'hardwareComponentModel': {\n                            'hardwareGenericComponentModel': {\n                                'id': hardwareComponent['hardwareGenericComponentModel']['id'],\n                                'hardwareComponentType': {\n                                    'keyName': hardwareComponentType['keyName']\n                                }\n                            }\n                        }\n                    }\n                ]\n\n        if locations is not None:\n            for location in locations:\n                if location['locationId'] is not None:\n                    loc_id = location['locationId']\n                    host = {\n                        'cpuCount': cpu_count,\n                        'memoryCapacity': mem_capacity,\n                        'diskCapacity': disk_capacity,\n                        'datacenter': {\n                            'id': loc_id\n                        }\n                    }\n                    if item['keyName'].find(\"GPU\") != -1:\n                        host['pciDevices'] = gpuComponents\n                    routers = self.host.getAvailableRouters(host, mask=mask)\n                    return routers\n\n        raise SoftLayer.SoftLayerError(\"Could not find available routers\")", "response": "Returns valid router options for ordering a dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_default_router(self, routers, router_name=None):\n        if router_name is None:\n            for router in routers:\n                if router['id'] is not None:\n                    return router['id']\n        else:\n            for router in routers:\n                if router['hostname'] == router_name:\n                    return router['id']\n\n        raise SoftLayer.SoftLayerError(\"Could not find valid default router\")", "response": "Returns the default router for ordering a dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_router_options(self, datacenter=None, flavor=None):\n        package = self._get_package()\n\n        location = self._get_location(package['regions'], datacenter)\n        item = self._get_item(package, flavor)\n\n        return self._get_backend_router(location['location']['locationPackageDetails'], item)", "response": "Returns available backend routers for the dedicated host."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _delete_guest(self, guest_id):\n        msg = 'Cancelled'\n        try:\n            self.guest.deleteObject(id=guest_id)\n        except SoftLayer.SoftLayerAPIError as e:\n            msg = 'Exception: ' + e.faultString\n\n        return msg", "response": "Deletes a guest and returns Cancelled or and Exception message"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _click_resolve_command(root, parts):\n    location = root\n    incomplete = ''\n    for part in parts:\n        incomplete = part\n\n        if not part[0:2].isalnum():\n            continue\n\n        try:\n            next_location = location.get_command(click.Context(location),\n                                                 part)\n            if next_location is not None:\n                location = next_location\n                incomplete = ''\n        except AttributeError:\n            break\n    return location, incomplete", "response": "Return the click command and the left over text given some vargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables or Disable specific permissions.", "response": "def cli(env, identifier, enable, permission, from_user):\n    \"\"\"Enable or Disable specific permissions.\"\"\"\n    mgr = SoftLayer.UserManager(env.client)\n    user_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'username')\n    result = False\n    if from_user:\n        from_user_id = helpers.resolve_id(mgr.resolve_ids, from_user, 'username')\n        result = mgr.permissions_from_user(user_id, from_user_id)\n    elif enable:\n        result = mgr.add_permissions(user_id, permission)\n    else:\n        result = mgr.remove_permissions(user_id, permission)\n\n    if result:\n        click.secho(\"Permissions updated successfully: %s\" % \", \".join(permission), fg='green')\n    else:\n        click.secho(\"Failed to update permissions: %s\" % \", \".join(permission), fg='red')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists CDN accounts for the active user.", "response": "def list_accounts(self):\n        \"\"\"Lists CDN accounts for the active user.\"\"\"\n\n        account = self.client['Account']\n        mask = 'cdnAccounts[%s]' % ', '.join(['id',\n                                              'createDate',\n                                              'cdnAccountName',\n                                              'cdnSolutionName',\n                                              'cdnAccountNote',\n                                              'status'])\n        return account.getObject(mask=mask).get('cdnAccounts', [])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_account(self, account_id, **kwargs):\n\n        if 'mask' not in kwargs:\n            kwargs['mask'] = 'status'\n\n        return self.account.getObject(id=account_id, **kwargs)", "response": "Retrieves a CDN account with the specified ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_origins(self, account_id, **kwargs):\n\n        return self.account.getOriginPullMappingInformation(id=account_id,\n                                                            **kwargs)", "response": "Retrieves the list of origin pull mappings for a specified CDN account."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an origin pull mapping to an origin - pull.", "response": "def add_origin(self, account_id, media_type, origin_url, cname=None,\n                   secure=False):\n        \"\"\"Adds an original pull mapping to an origin-pull.\n\n        :param int account_id: the numeric ID associated with the CDN account.\n        :param string media_type: the media type/protocol associated with this\n                                  origin pull mapping; valid values are HTTP,\n                                  FLASH, and WM.\n        :param string origin_url: the base URL from which content should be\n                                  pulled.\n        :param string cname: an optional CNAME that should be associated with\n                             this origin pull rule; only the hostname should be\n                             included (i.e., no 'http://', directories, etc.).\n        :param boolean secure: specifies whether this is an SSL origin pull\n                               rule, if SSL is enabled on your account\n                               (defaults to false).\n        \"\"\"\n\n        config = {'mediaType': media_type,\n                  'originUrl': origin_url,\n                  'isSecureContent': secure}\n\n        if cname:\n            config['cname'] = cname\n\n        return self.account.createOriginPullMapping(config, id=account_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving an origin pull mapping with the given origin pull ID.", "response": "def remove_origin(self, account_id, origin_id):\n        \"\"\"Removes an origin pull mapping with the given origin pull ID.\n\n        :param int account_id: the CDN account ID from which the mapping should\n                               be deleted.\n        :param int origin_id: the origin pull mapping ID to delete.\n        \"\"\"\n\n        return self.account.deleteOriginPullRule(origin_id, id=account_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npurging one or more URLs from the CDN edge nodes.", "response": "def purge_content(self, account_id, urls):\n        \"\"\"Purges one or more URLs from the CDN edge nodes.\n\n        :param int account_id: the CDN account ID from which content should\n                               be purged.\n        :param urls: a string or a list of strings representing the CDN URLs\n                     that should be purged.\n        :returns: a list of SoftLayer_Container_Network_ContentDelivery_PurgeService_Response objects\n                  which indicates if the purge for each url was SUCCESS, FAILED or INVALID_URL.\n        \"\"\"\n\n        if isinstance(urls, six.string_types):\n            urls = [urls]\n\n        content_list = []\n        for i in range(0, len(urls), MAX_URLS_PER_PURGE):\n            content = self.account.purgeCache(urls[i:i + MAX_URLS_PER_PURGE], id=account_id)\n            content_list.extend(content)\n\n        return content_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_lb_pkgs(self):\n\n        _filter = {'items': {'description':\n                             utils.query_filter('*Load Balancer*')}}\n\n        packages = self.prod_pkg.getItems(id=0, filter=_filter)\n        pkgs = []\n        for package in packages:\n            if not package['description'].startswith('Global'):\n                pkgs.append(package)\n        return pkgs", "response": "Retrieves the local load balancer packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_location(self, datacenter_name):\n\n        datacenters = self.client['Location'].getDataCenters()\n        for datacenter in datacenters:\n            if datacenter['name'] == datacenter_name:\n                return datacenter['id']\n        return 'FIRST_AVAILABLE'", "response": "Returns the location of the load balancer in the given datacenter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels the specified load balancer.", "response": "def cancel_lb(self, loadbal_id):\n        \"\"\"Cancels the specified load balancer.\n\n        :param int loadbal_id: Load Balancer ID to be cancelled.\n        \"\"\"\n\n        lb_billing = self.lb_svc.getBillingItem(id=loadbal_id)\n        billing_id = lb_billing['id']\n        billing_item = self.client['Billing_Item']\n        return billing_item.cancelService(id=billing_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a local load balancer in the specified data center.", "response": "def add_local_lb(self, price_item_id, datacenter):\n        \"\"\"Creates a local load balancer in the specified data center.\n\n        :param int price_item_id: The price item ID for the load balancer\n        :param string datacenter: The datacenter to create the loadbalancer in\n        :returns: A dictionary containing the product order\n        \"\"\"\n\n        product_order = {\n            'complexType': 'SoftLayer_Container_Product_Order_Network_'\n                           'LoadBalancer',\n            'quantity': 1,\n            'packageId': 0,\n            \"location\": self._get_location(datacenter),\n            'prices': [{'id': price_item_id}]\n        }\n        return self.client['Product_Order'].placeOrder(product_order)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a specific local load balancer given the id.", "response": "def get_local_lb(self, loadbal_id, **kwargs):\n        \"\"\"Returns a specified local load balancer given the id.\n\n        :param int loadbal_id: The id of the load balancer to retrieve\n        :returns: A dictionary containing the details of the load balancer\n        \"\"\"\n\n        if 'mask' not in kwargs:\n            kwargs['mask'] = ('loadBalancerHardware[datacenter], '\n                              'ipAddress, virtualServers[serviceGroups'\n                              '[routingMethod,routingType,services'\n                              '[healthChecks[type], groupReferences,'\n                              ' ipAddress]]]')\n\n        return self.lb_svc.getObject(id=loadbal_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a service from the loadbal_id.", "response": "def delete_service(self, service_id):\n        \"\"\"Deletes a service from the loadbal_id.\n\n        :param int service_id: The id of the service to delete\n        \"\"\"\n\n        svc = self.client['Network_Application_Delivery_Controller_'\n                          'LoadBalancer_Service']\n\n        return svc.deleteObject(id=service_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a service group from the loadbal_id.", "response": "def delete_service_group(self, group_id):\n        \"\"\"Deletes a service group from the loadbal_id.\n\n        :param int group_id: The id of the service group to delete\n        \"\"\"\n\n        svc = self.client['Network_Application_Delivery_Controller_'\n                          'LoadBalancer_VirtualServer']\n\n        return svc.deleteObject(id=group_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntoggle the service status.", "response": "def toggle_service_status(self, service_id):\n        \"\"\"Toggles the service status.\n\n        :param int service_id: The id of the service to delete\n        \"\"\"\n\n        svc = self.client['Network_Application_Delivery_Controller_'\n                          'LoadBalancer_Service']\n        return svc.toggleStatus(id=service_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nedit an existing service properties.", "response": "def edit_service(self, loadbal_id, service_id, ip_address_id=None,\n                     port=None, enabled=None, hc_type=None, weight=None):\n        \"\"\"Edits an existing service properties.\n\n        :param int loadbal_id: The id of the loadbal where the service resides\n        :param int service_id: The id of the service to edit\n        :param string ip_address: The ip address of the service\n        :param int port: the port of the service\n        :param bool enabled: enable or disable the search\n        :param int hc_type: The health check type\n        :param int weight: the weight to give to the service\n        \"\"\"\n\n        _filter = {\n            'virtualServers': {\n                'serviceGroups': {\n                    'services': {'id': utils.query_filter(service_id)}}}}\n\n        mask = 'serviceGroups[services[groupReferences,healthChecks]]'\n\n        virtual_servers = self.lb_svc.getVirtualServers(id=loadbal_id,\n                                                        filter=_filter,\n                                                        mask=mask)\n\n        for service in virtual_servers[0]['serviceGroups'][0]['services']:\n            if service['id'] == service_id:\n                if enabled is not None:\n                    service['enabled'] = int(enabled)\n                if port is not None:\n                    service['port'] = port\n                if weight is not None:\n                    service['groupReferences'][0]['weight'] = weight\n                if hc_type is not None:\n                    service['healthChecks'][0]['healthCheckTypeId'] = hc_type\n                if ip_address_id is not None:\n                    service['ipAddressId'] = ip_address_id\n\n        template = {'virtualServers': list(virtual_servers)}\n\n        load_balancer = self.lb_svc.editObject(template, id=loadbal_id)\n        return load_balancer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new service to the load balancer.", "response": "def add_service(self, loadbal_id, service_group_id, ip_address_id,\n                    port=80, enabled=True, hc_type=21, weight=1):\n        \"\"\"Adds a new service to the service group.\n\n        :param int loadbal_id: The id of the loadbal where the service resides\n        :param int service_group_id: The group to add the service to\n        :param int ip_address id: The ip address ID of the service\n        :param int port: the port of the service\n        :param bool enabled: Enable or disable the service\n        :param int hc_type: The health check type\n        :param int weight: the weight to give to the service\n        \"\"\"\n        kwargs = utils.NestedDict({})\n        kwargs['mask'] = ('virtualServers['\n                          'serviceGroups[services[groupReferences]]]')\n\n        load_balancer = self.lb_svc.getObject(id=loadbal_id, **kwargs)\n        virtual_servers = load_balancer['virtualServers']\n        for virtual_server in virtual_servers:\n            if virtual_server['id'] == service_group_id:\n                service_template = {\n                    'enabled': int(enabled),\n                    'port': port,\n                    'ipAddressId': ip_address_id,\n                    'healthChecks': [\n                        {\n                            'healthCheckTypeId': hc_type\n                        }\n                    ],\n                    'groupReferences': [\n                        {\n                            'weight': weight\n                        }\n                    ]\n                }\n                services = virtual_server['serviceGroups'][0]['services']\n                services.append(service_template)\n\n        return self.lb_svc.editObject(load_balancer, id=loadbal_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new service group to the load balancer.", "response": "def add_service_group(self, lb_id, allocation=100, port=80,\n                          routing_type=2, routing_method=10):\n        \"\"\"Adds a new service group to the load balancer.\n\n        :param int loadbal_id: The id of the loadbal where the service resides\n        :param int allocation: percent of connections to allocate toward the\n                               group\n        :param int port: the port of the service group\n        :param int routing_type: the routing type to set on the service group\n        :param int routing_method: The routing method to set on the group\n        \"\"\"\n\n        mask = 'virtualServers[serviceGroups[services[groupReferences]]]'\n        load_balancer = self.lb_svc.getObject(id=lb_id, mask=mask)\n        service_template = {\n            'port': port,\n            'allocation': allocation,\n            'serviceGroups': [\n                {\n                    'routingTypeId': routing_type,\n                    'routingMethodId': routing_method\n                }\n            ]\n        }\n\n        load_balancer['virtualServers'].append(service_template)\n        return self.lb_svc.editObject(load_balancer, id=lb_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nediting an existing service group.", "response": "def edit_service_group(self, loadbal_id, group_id, allocation=None,\n                           port=None, routing_type=None, routing_method=None):\n        \"\"\"Edit an existing service group.\n\n        :param int loadbal_id: The id of the loadbal where the service resides\n        :param int group_id: The id of the service group\n        :param int allocation: the % of connections to allocate to the group\n        :param int port: the port of the service group\n        :param int routing_type: the routing type to set on the service group\n        :param int routing_method: The routing method to set on the group\n        \"\"\"\n\n        mask = 'virtualServers[serviceGroups[services[groupReferences]]]'\n\n        load_balancer = self.lb_svc.getObject(id=loadbal_id, mask=mask)\n        virtual_servers = load_balancer['virtualServers']\n\n        for virtual_server in virtual_servers:\n            if virtual_server['id'] == group_id:\n                service_group = virtual_server['serviceGroups'][0]\n                if allocation is not None:\n                    virtual_server['allocation'] = allocation\n                if port is not None:\n                    virtual_server['port'] = port\n                if routing_type is not None:\n                    service_group['routingTypeId'] = routing_type\n                if routing_method is not None:\n                    service_group['routingMethodId'] = routing_method\n                break\n\n        return self.lb_svc.editObject(load_balancer, id=loadbal_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresetting all the connections on the service group.", "response": "def reset_service_group(self, loadbal_id, group_id):\n        \"\"\"Resets all the connections on the service group.\n\n        :param int loadbal_id: The id of the loadbal\n        :param int group_id: The id of the service group to reset\n        \"\"\"\n\n        _filter = {'virtualServers': {'id': utils.query_filter(group_id)}}\n        virtual_servers = self.lb_svc.getVirtualServers(id=loadbal_id,\n                                                        filter=_filter,\n                                                        mask='serviceGroups')\n        actual_id = virtual_servers[0]['serviceGroups'][0]['id']\n\n        svc = self.client['Network_Application_Delivery_Controller'\n                          '_LoadBalancer_Service_Group']\n        return svc.kickAllConnections(id=actual_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, identifier):\n\n    image_mgr = SoftLayer.ImageManager(env.client)\n    image_id = helpers.resolve_id(image_mgr.resolve_ids, identifier, 'image')\n\n    image = image_mgr.get_image(image_id, mask=image_mod.DETAIL_MASK)\n    disk_space = 0\n    datacenters = []\n    for child in image.get('children'):\n        disk_space = int(child.get('blockDevicesDiskSpaceTotal', 0))\n        if child.get('datacenter'):\n            datacenters.append(utils.lookup(child, 'datacenter', 'name'))\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['id', image['id']])\n    table.add_row(['global_identifier',\n                   image.get('globalIdentifier', formatting.blank())])\n    table.add_row(['name', image['name'].strip()])\n    table.add_row(['status', formatting.FormattedItem(\n        utils.lookup(image, 'status', 'keyname'),\n        utils.lookup(image, 'status', 'name'),\n    )])\n    table.add_row([\n        'active_transaction',\n        formatting.transaction_status(image.get('transaction')),\n    ])\n    table.add_row(['account', image.get('accountId', formatting.blank())])\n    table.add_row(['visibility',\n                   image_mod.PUBLIC_TYPE if image['publicFlag']\n                   else image_mod.PRIVATE_TYPE])\n    table.add_row(['type',\n                   formatting.FormattedItem(\n                       utils.lookup(image, 'imageType', 'keyName'),\n                       utils.lookup(image, 'imageType', 'name'),\n                   )])\n    table.add_row(['flex', image.get('flexImageFlag')])\n    table.add_row(['note', image.get('note')])\n    table.add_row(['created', image.get('createDate')])\n    table.add_row(['disk_space', formatting.b_to_gb(disk_space)])\n    table.add_row(['datacenters', formatting.listing(sorted(datacenters),\n                                                     separator=',')])\n\n    env.fout(table)", "response": "Get details for an image."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nenable snapshots for a given file storage volume on the specified schedule.", "response": "def cli(env, volume_id, schedule_type, retention_count,\n        minute, hour, day_of_week):\n    \"\"\"Enables snapshots for a given volume on the specified schedule\"\"\"\n    file_manager = SoftLayer.FileStorageManager(env.client)\n\n    valid_schedule_types = {'INTERVAL', 'HOURLY', 'DAILY', 'WEEKLY'}\n    valid_days = {'SUNDAY', 'MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY',\n                  'FRIDAY', 'SATURDAY'}\n\n    if schedule_type not in valid_schedule_types:\n        raise exceptions.CLIAbort(\n            '--schedule-type must be INTERVAL, HOURLY, ' +\n            'DAILY, or WEEKLY, not ' + schedule_type)\n\n    if schedule_type == 'INTERVAL' and (minute < 30 or minute > 59):\n        raise exceptions.CLIAbort(\n            '--minute value must be between 30 and 59')\n    if minute < 0 or minute > 59:\n        raise exceptions.CLIAbort(\n            '--minute value must be between 0 and 59')\n    if hour < 0 or hour > 23:\n        raise exceptions.CLIAbort(\n            '--hour value must be between 0 and 23')\n    if day_of_week not in valid_days:\n        raise exceptions.CLIAbort(\n            '--day_of_week value must be a valid day (ex: SUNDAY)')\n\n    enabled = file_manager.enable_snapshots(volume_id, schedule_type,\n                                            retention_count, minute,\n                                            hour, day_of_week)\n\n    if enabled:\n        click.echo('%s snapshots have been enabled for volume %s'\n                   % (schedule_type, volume_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_upcoming_events(self):\n        mask = \"mask[id, subject, startDate, endDate, statusCode, acknowledgedFlag, impactedResourceCount, updateCount]\"\n        _filter = {\n            'endDate': {\n                'operation': '> sysdate'\n            },\n            'startDate': {\n                'operation': 'orderBy',\n                'options': [{\n                    'name': 'sort',\n                    'value': ['ASC']\n                }]\n            }\n        }\n        return self.client.call('Notification_Occurrence_Event', 'getAllObjects', filter=_filter, mask=mask, iter=True)", "response": "Retreives a list of Notification_Occurrence_Events that have not ended yet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_event(self, event_id):\n        mask = \"\"\"mask[\n            acknowledgedFlag,\n            attachments,\n            impactedResources,\n            statusCode,\n            updates,\n            notificationOccurrenceEventType]\n        \"\"\"\n        return self.client.call('Notification_Occurrence_Event', 'getObject', id=event_id, mask=mask)", "response": "Gets details about a maintenance event."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets an accounts invoices.", "response": "def get_invoices(self, limit=50, closed=False, get_all=False):\n        \"\"\"Gets an accounts invoices.\n\n        :param int limit: Number of invoices to get back in a single call.\n        :param bool closed: If True, will also get CLOSED invoices\n        :param bool get_all: If True, will paginate through invoices until all have been retrieved.\n        :return: Billing_Invoice\n        \"\"\"\n        mask = \"mask[invoiceTotalAmount, itemCount]\"\n        _filter = {\n            'invoices': {\n                'createDate': {\n                    'operation': 'orderBy',\n                    'options': [{\n                        'name': 'sort',\n                        'value': ['DESC']\n                    }]\n                },\n                'statusCode': {'operation': 'OPEN'},\n            }\n        }\n        if closed:\n            del _filter['invoices']['statusCode']\n\n        return self.client.call('Account', 'getInvoices', mask=mask, filter=_filter, iter=get_all, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all topLevelBillingItems from a specific invoice", "response": "def get_billing_items(self, identifier):\n        \"\"\"Gets all topLevelBillingItems from a specific invoice\n\n        :param int identifier: Invoice Id\n        :return: Billing_Invoice_Item\n        \"\"\"\n\n        mask = \"\"\"mask[\n            id, description, hostName, domainName, oneTimeAfterTaxAmount, recurringAfterTaxAmount, createDate,\n            categoryCode,\n            category[name],\n            location[name],\n            children[id, category[name], description, oneTimeAfterTaxAmount, recurringAfterTaxAmount]\n        ]\"\"\"\n        return self.client.call(\n            'Billing_Invoice',\n            'getInvoiceTopLevelItems',\n            id=identifier,\n            mask=mask,\n            iter=True,\n            limit=100\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets price options to create a load balancer with.", "response": "def cli(env):\n    \"\"\"Get price options to create a load balancer with.\"\"\"\n\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    table = formatting.Table(['price_id', 'capacity', 'description', 'price'])\n\n    table.sortby = 'price'\n    table.align['price'] = 'r'\n    table.align['capacity'] = 'r'\n    table.align['id'] = 'r'\n\n    packages = mgr.get_lb_pkgs()\n\n    for package in packages:\n        table.add_row([\n            package['prices'][0]['id'],\n            package.get('capacity'),\n            package['description'],\n            '%.2f' % float(package['prices'][0]['recurringFee'])\n        ])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist virtual server credentials.", "response": "def cli(env, identifier):\n    \"\"\"List virtual server credentials.\"\"\"\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    instance = vsi.get_instance(vs_id)\n\n    table = formatting.Table(['username', 'password'])\n    for item in instance['operatingSystem']['passwords']:\n        table.add_row([item['username'], item['password']])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting all sub - commands.", "response": "def list_commands(self, ctx):\n        \"\"\"List all sub-commands.\"\"\"\n        commands = []\n        for filename in os.listdir(self.path):\n            if filename == '__init__.py':\n                continue\n            if filename.endswith('.py'):\n                commands.append(filename[:-3].replace(\"_\", \"-\"))\n        commands.sort()\n        return commands"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_command(self, ctx, cmd_name):\n        path = \"%s.%s\" % (__name__, cmd_name)\n        path = path.replace(\"-\", \"_\")\n        module = importlib.import_module(path)\n        return getattr(module, 'cli')", "response": "Get command for click."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, identifier, wait):\n\n    compute = SoftLayer.HardwareManager(env.client)\n    compute_id = helpers.resolve_id(compute.resolve_ids, identifier,\n                                    'hardware')\n    ready = compute.wait_for_ready(compute_id, wait)\n    if ready:\n        env.fout(\"READY\")\n    else:\n        raise exceptions.CLIAbort(\"Server %s not ready\" % compute_id)", "response": "Check if a server is ready."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cli(env):\n\n    mask = ('openTicketCount, closedTicketCount, '\n            'openBillingTicketCount, openOtherTicketCount, '\n            'openSalesTicketCount, openSupportTicketCount, '\n            'openAccountingTicketCount')\n    account = env.client['Account'].getObject(mask=mask)\n    table = formatting.Table(['Status', 'count'])\n\n    nested = formatting.Table(['Type', 'count'])\n    nested.add_row(['Accounting',\n                    account['openAccountingTicketCount']])\n    nested.add_row(['Billing', account['openBillingTicketCount']])\n    nested.add_row(['Sales', account['openSalesTicketCount']])\n    nested.add_row(['Support', account['openSupportTicketCount']])\n    nested.add_row(['Other', account['openOtherTicketCount']])\n    nested.add_row(['Total', account['openTicketCount']])\n    table.add_row(['Open', nested])\n    table.add_row(['Closed', account['closedTicketCount']])\n\n    env.fout(table)", "response": "Summary info about tickets."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all global IPs.", "response": "def cli(env, ip_version):\n    \"\"\"List all global IPs.\"\"\"\n\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    table = formatting.Table(['id', 'ip', 'assigned', 'target'])\n\n    version = None\n    if ip_version == 'v4':\n        version = 4\n    elif ip_version == 'v6':\n        version = 6\n\n    ips = mgr.list_global_ips(version=version)\n\n    for ip_address in ips:\n        assigned = 'No'\n        target = 'None'\n        if ip_address.get('destinationIpAddress'):\n            dest = ip_address['destinationIpAddress']\n            assigned = 'Yes'\n            target = dest['ipAddress']\n            virtual_guest = dest.get('virtualGuest')\n            if virtual_guest:\n                target += (' (%s)'\n                           % virtual_guest['fullyQualifiedDomainName'])\n            elif ip_address['destinationIpAddress'].get('hardware'):\n                target += (' (%s)'\n                           % dest['hardware']['fullyQualifiedDomainName'])\n\n        table.add_row([ip_address['id'],\n                       ip_address['ipAddress']['ipAddress'],\n                       assigned,\n                       target])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_client_from_env(username=None,\n                           api_key=None,\n                           endpoint_url=None,\n                           timeout=None,\n                           auth=None,\n                           config_file=None,\n                           proxy=None,\n                           user_agent=None,\n                           transport=None,\n                           verify=True):\n    \"\"\"Creates a SoftLayer API client using your environment.\n\n    Settings are loaded via keyword arguments, environemtal variables and\n    config file.\n\n    :param username: an optional API username if you wish to bypass the\n        package's built-in username\n    :param api_key: an optional API key if you wish to bypass the package's\n        built in API key\n    :param endpoint_url: the API endpoint base URL you wish to connect to.\n        Set this to API_PRIVATE_ENDPOINT to connect via SoftLayer's private\n        network.\n    :param proxy: proxy to be used to make API calls\n    :param integer timeout: timeout for API requests\n    :param auth: an object which responds to get_headers() to be inserted into\n        the xml-rpc headers. Example: `BasicAuthentication`\n    :param config_file: A path to a configuration file used to load settings\n    :param user_agent: an optional User Agent to report when making API\n        calls if you wish to bypass the packages built in User Agent string\n    :param transport: An object that's callable with this signature:\n                      transport(SoftLayer.transports.Request)\n    :param bool verify: decide to verify the server's SSL/TLS cert. DO NOT SET\n                        TO FALSE WITHOUT UNDERSTANDING THE IMPLICATIONS.\n\n    Usage:\n\n        >>> import SoftLayer\n        >>> client = SoftLayer.create_client_from_env()\n        >>> resp = client.call('Account', 'getObject')\n        >>> resp['companyName']\n        'Your Company'\n\n    \"\"\"\n    settings = config.get_client_settings(username=username,\n                                          api_key=api_key,\n                                          endpoint_url=endpoint_url,\n                                          timeout=timeout,\n                                          proxy=proxy,\n                                          verify=verify,\n                                          config_file=config_file)\n\n    if transport is None:\n        url = settings.get('endpoint_url')\n        if url is not None and '/rest' in url:\n            # If this looks like a rest endpoint, use the rest transport\n            transport = transports.RestTransport(\n                endpoint_url=settings.get('endpoint_url'),\n                proxy=settings.get('proxy'),\n                timeout=settings.get('timeout'),\n                user_agent=user_agent,\n                verify=verify,\n            )\n        else:\n            # Default the transport to use XMLRPC\n            transport = transports.XmlRpcTransport(\n                endpoint_url=settings.get('endpoint_url'),\n                proxy=settings.get('proxy'),\n                timeout=settings.get('timeout'),\n                user_agent=user_agent,\n                verify=verify,\n            )\n\n    # If we have enough information to make an auth driver, let's do it\n    if auth is None and settings.get('username') and settings.get('api_key'):\n        # NOTE(kmcdonald): some transports mask other transports, so this is\n        # a way to find the 'real' one\n        real_transport = getattr(transport, 'transport', transport)\n\n        if isinstance(real_transport, transports.XmlRpcTransport):\n            auth = slauth.BasicAuthentication(\n                settings.get('username'),\n                settings.get('api_key'),\n            )\n\n        elif isinstance(real_transport, transports.RestTransport):\n            auth = slauth.BasicHTTPAuthentication(\n                settings.get('username'),\n                settings.get('api_key'),\n            )\n\n    return BaseClient(auth=auth, transport=transport)", "response": "Creates a new API client using the environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming Username and Password Authentication", "response": "def authenticate_with_password(self, username, password,\n                                   security_question_id=None,\n                                   security_question_answer=None):\n        \"\"\"Performs Username/Password Authentication\n\n        :param string username: your SoftLayer username\n        :param string password: your SoftLayer password\n        :param int security_question_id: The security question id to answer\n        :param string security_question_answer: The answer to the security\n                                                question\n\n        \"\"\"\n        self.auth = None\n        res = self.call('User_Customer', 'getPortalLoginToken',\n                        username,\n                        password,\n                        security_question_id,\n                        security_question_answer)\n        self.auth = slauth.TokenAuthentication(res['userId'], res['hash'])\n        return res['userId'], res['hash']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call(self, service, method, *args, **kwargs):\n        if kwargs.pop('iter', False):\n            # Most of the codebase assumes a non-generator will be returned, so casting to list\n            # keeps those sections working\n            return list(self.iter_call(service, method, *args, **kwargs))\n\n        invalid_kwargs = set(kwargs.keys()) - VALID_CALL_ARGS\n        if invalid_kwargs:\n            raise TypeError(\n                'Invalid keyword arguments: %s' % ','.join(invalid_kwargs))\n\n        if self._prefix and not service.startswith(self._prefix):\n            service = self._prefix + service\n\n        http_headers = {'Accept': '*/*'}\n\n        if kwargs.get('compress', True):\n            http_headers['Accept-Encoding'] = 'gzip, deflate, compress'\n        else:\n            http_headers['Accept-Encoding'] = None\n\n        if kwargs.get('raw_headers'):\n            http_headers.update(kwargs.get('raw_headers'))\n\n        request = transports.Request()\n        request.service = service\n        request.method = method\n        request.args = args\n        request.transport_headers = http_headers\n        request.identifier = kwargs.get('id')\n        request.mask = kwargs.get('mask')\n        request.filter = kwargs.get('filter')\n        request.limit = kwargs.get('limit')\n        request.offset = kwargs.get('offset')\n        if kwargs.get('verify') is not None:\n            request.verify = kwargs.get('verify')\n\n        if self.auth:\n            extra_headers = self.auth.get_headers()\n            if extra_headers:\n                warnings.warn(\"auth.get_headers() is deprecated and will be \"\n                              \"removed in the next major version\",\n                              DeprecationWarning)\n                request.headers.update(extra_headers)\n\n            request = self.auth.get_request(request)\n\n        request.headers.update(kwargs.get('headers', {}))\n        return self.transport(request)", "response": "Make a SoftLayer API call."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a SoftLayer API call on a given service", "response": "def call(self, name, *args, **kwargs):\n        \"\"\"Make a SoftLayer API call\n\n        :param service: the name of the SoftLayer API service\n        :param method: the method to call on the service\n        :param \\\\*args: same optional arguments that ``BaseClient.call`` takes\n        :param \\\\*\\\\*kwargs: same optional keyword arguments that\n                           ``BaseClient.call`` takes\n\n        :param service: the name of the SoftLayer API service\n\n        Usage:\n            >>> import SoftLayer\n            >>> client = SoftLayer.create_client_from_env()\n            >>> client['Account'].getVirtualGuests(mask=\"id\", limit=10)\n            [...]\n\n        \"\"\"\n        return self.client.call(self.name, name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_call(self, name, *args, **kwargs):\n        return self.client.iter_call(self.name, name, *args, **kwargs)", "response": "A generator that returns a list of items from the API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, identifier):\n\n    mgr = SoftLayer.ObjectStorageManager(env.client)\n    credential_list = mgr.list_credential(identifier)\n    table = formatting.Table(['id', 'password', 'username', 'type_name'])\n\n    for credential in credential_list:\n        table.add_row([\n            credential['id'],\n            credential['password'],\n            credential['username'],\n            credential['type']['name']\n        ])\n\n    env.fout(table)", "response": "Retrieve credentials used for generating an AWS signature. Max of 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef out(self, output, newline=True):\n        click.echo(output, nl=newline)", "response": "Outputs a string to the console"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef err(self, output, newline=True):\n        click.echo(output, nl=newline, err=True)", "response": "Outputs an error string to the console"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats the input and output to the console ( stdout", "response": "def fout(self, output, newline=True):\n        \"\"\"Format the input and output to the console (stdout).\"\"\"\n        if output is not None:\n            self.out(self.fmt(output), newline=newline)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nproviding a command prompt.", "response": "def input(self, prompt, default=None, show_default=True):\n        \"\"\"Provide a command prompt.\"\"\"\n        return click.prompt(prompt, default=default, show_default=show_default)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getpass(self, prompt, default=None):\n        return click.prompt(prompt, hide_input=True, default=default)", "response": "Provide a password prompt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_commands(self, *path):\n        path_str = ':'.join(path)\n\n        commands = []\n        for command in self.commands:\n\n            # Filter based on prefix and the segment length\n            if all([command.startswith(path_str),\n                    len(path) == command.count(\":\")]):\n\n                # offset is used to exclude the path that the caller requested.\n                offset = len(path_str) + 1 if path_str else 0\n                commands.append(command[offset:])\n\n        return sorted(commands)", "response": "List the commands that are in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the command at the given path or raise error.", "response": "def get_command(self, *path):\n        \"\"\"Return command at the given path or raise error.\"\"\"\n        path_str = ':'.join(path)\n\n        if path_str in self.commands:\n            return self.commands[path_str].load()\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_alias(self, path_str):\n        if path_str in self.aliases:\n            return self.aliases[path_str]\n        return path_str", "response": "Returns the actual command name. Uses the alias mapping."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(self):\n        if self._modules_loaded is True:\n            return\n\n        self.load_modules_from_python(routes.ALL_ROUTES)\n        self.aliases.update(routes.ALL_ALIASES)\n        self._load_modules_from_entry_points('softlayer.cli')\n\n        self._modules_loaded = True", "response": "Loads all modules from the entry point."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload modules from the native python source.", "response": "def load_modules_from_python(self, route_list):\n        \"\"\"Load modules from the native python source.\"\"\"\n        for name, modpath in route_list:\n            if ':' in modpath:\n                path, attr = modpath.split(':', 1)\n            else:\n                path, attr = modpath, None\n            self.commands[name] = ModuleLoader(path, attr=attr)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_modules_from_entry_points(self, entry_point_group):\n        for obj in pkg_resources.iter_entry_points(group=entry_point_group,\n                                                   name=None):\n            self.commands[obj.name] = obj", "response": "Load modules from the entry_points."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new SLAPI client to the environment.", "response": "def ensure_client(self, config_file=None, is_demo=False, proxy=None):\n        \"\"\"Create a new SLAPI client to the environment.\n\n        This will be a no-op if there is already a client in this environment.\n        \"\"\"\n        if self.client is not None:\n            return\n\n        # Environment can be passed in explicitly. This is used for testing\n        if is_demo:\n            client = SoftLayer.BaseClient(\n                transport=SoftLayer.FixtureTransport(),\n                auth=None,\n            )\n        else:\n            # Create SL Client\n            client = SoftLayer.create_client_from_env(\n                proxy=proxy,\n                config_file=config_file,\n            )\n        self.client = client"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading and return the module and attribute.", "response": "def load(self):\n        \"\"\"load and return the module/attribute.\"\"\"\n        module = importlib.import_module(self.import_path)\n        if self.attr:\n            return getattr(module, self.attr)\n        return module"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef multi_option(*param_decls, **attrs):\n    attrhelp = attrs.get('help', None)\n    if attrhelp is not None:\n        newhelp = attrhelp + \" (multiple occurrence permitted)\"\n        attrs['help'] = newhelp\n    attrs['multiple'] = True\n    return click.option(*param_decls, **attrs)", "response": "modify help text and indicate option is permitted multiple times\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving a single id using a resolver function.", "response": "def resolve_id(resolver, identifier, name='object'):\n    \"\"\"Resolves a single id using a resolver function.\n\n    :param resolver: function that resolves ids. Should return None or a list of ids.\n    :param string identifier: a string identifier used to resolve ids\n    :param string name: the object type, to be used in error messages\n\n    \"\"\"\n    try:\n        return int(identifier)\n    except ValueError:\n        pass  # It was worth a shot\n\n    ids = resolver(identifier)\n\n    if len(ids) == 0:\n        raise exceptions.CLIAbort(\"Error: Unable to find %s '%s'\" % (name, identifier))\n\n    if len(ids) > 1:\n        raise exceptions.CLIAbort(\n            \"Error: Multiple %s found for '%s': %s\" %\n            (name, identifier, ', '.join([str(_id) for _id in ids])))\n\n    return ids[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, volume_id, capacity, tier, upgrade):\n    file_manager = SoftLayer.FileStorageManager(env.client)\n\n    if tier is not None:\n        tier = float(tier)\n\n    try:\n        order = file_manager.order_snapshot_space(\n            volume_id,\n            capacity=capacity,\n            tier=tier,\n            upgrade=upgrade\n        )\n    except ValueError as ex:\n        raise exceptions.ArgumentError(str(ex))\n\n    if 'placedOrder' in order.keys():\n        click.echo(\"Order #{0} placed successfully!\".format(\n            order['placedOrder']['id']))\n        for item in order['placedOrder']['items']:\n            click.echo(\" > %s\" % item['description'])\n        if 'status' in order['placedOrder'].keys():\n            click.echo(\" > Order status: %s\" % order['placedOrder']['status'])\n    else:\n        click.echo(\"Order could not be placed! Please verify your options \" +\n                   \"and try again.\")", "response": "Order snapshot space for a file storage volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nedits a virtual server s details.", "response": "def cli(env, identifier, domain, userfile, tag, hostname, userdata,\n        public_speed, private_speed):\n    \"\"\"Edit a virtual server's details.\"\"\"\n\n    if userdata and userfile:\n        raise exceptions.ArgumentError(\n            '[-u | --userdata] not allowed with [-F | --userfile]')\n\n    data = {}\n\n    if userdata:\n        data['userdata'] = userdata\n    elif userfile:\n        with open(userfile, 'r') as userfile_obj:\n            data['userdata'] = userfile_obj.read()\n\n    data['hostname'] = hostname\n    data['domain'] = domain\n\n    if tag:\n        data['tags'] = ','.join(tag)\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    if not vsi.edit(vs_id, **data):\n        raise exceptions.CLIAbort(\"Failed to update virtual server\")\n\n    if public_speed is not None:\n        vsi.change_port_speed(vs_id, True, int(public_speed))\n\n    if private_speed is not None:\n        vsi.change_port_speed(vs_id, False, int(private_speed))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\norders a file storage replica volume.", "response": "def cli(env, volume_id, snapshot_schedule, location, tier):\n    \"\"\"Order a file storage replica volume.\"\"\"\n    file_manager = SoftLayer.FileStorageManager(env.client)\n\n    if tier is not None:\n        tier = float(tier)\n\n    try:\n        order = file_manager.order_replicant_volume(\n            volume_id,\n            snapshot_schedule=snapshot_schedule,\n            location=location,\n            tier=tier,\n        )\n    except ValueError as ex:\n        raise exceptions.ArgumentError(str(ex))\n\n    if 'placedOrder' in order.keys():\n        click.echo(\"Order #{0} placed successfully!\".format(\n            order['placedOrder']['id']))\n        for item in order['placedOrder']['items']:\n            click.echo(\" > %s\" % item['description'])\n    else:\n        click.echo(\"Order could not be placed! Please verify your options \" +\n                   \"and try again.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, volume_id, replicant_id):\n    block_storage_manager = SoftLayer.BlockStorageManager(env.client)\n\n    success = block_storage_manager.failback_from_replicant(\n        volume_id,\n        replicant_id\n    )\n\n    if success:\n        click.echo(\"Failback from replicant is now in progress.\")\n    else:\n        click.echo(\"Failback operation could not be initiated.\")", "response": "Failback a block volume from the given replicant volume."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, identifier):\n    manager = PlacementManager(env.client)\n    group_id = helpers.resolve_id(manager.resolve_ids, identifier, 'placement_group')\n    result = manager.get_object(group_id)\n    table = formatting.Table([\"Id\", \"Name\", \"Backend Router\", \"Rule\", \"Created\"])\n\n    table.add_row([\n        result['id'],\n        result['name'],\n        result['backendRouter']['hostname'],\n        result['rule']['name'],\n        result['createDate']\n    ])\n    guest_table = formatting.Table([\n        \"Id\",\n        \"FQDN\",\n        \"Primary IP\",\n        \"Backend IP\",\n        \"CPU\",\n        \"Memory\",\n        \"Provisioned\",\n        \"Transaction\"\n    ])\n    for guest in result['guests']:\n        guest_table.add_row([\n            guest.get('id'),\n            guest.get('fullyQualifiedDomainName'),\n            guest.get('primaryIpAddress'),\n            guest.get('primaryBackendIpAddress'),\n            guest.get('maxCpu'),\n            guest.get('maxMemory'),\n            guest.get('provisionDate'),\n            formatting.active_txn(guest)\n        ])\n\n    env.fout(table)\n    env.fout(guest_table)", "response": "View details of a placement group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an update to an existing ticket.", "response": "def cli(env, identifier, body):\n    \"\"\"Adds an update to an existing ticket.\"\"\"\n    mgr = SoftLayer.TicketManager(env.client)\n\n    ticket_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'ticket')\n\n    if body is None:\n        body = click.edit('\\n\\n' + ticket.TEMPLATE_MSG)\n\n    mgr.update_ticket(ticket_id=ticket_id, body=body)\n    env.fout(\"Ticket Updated!\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_rules(content=None):\n    rules = content.split(DELIMITER)\n    parsed_rules = list()\n    order = 1\n    for rule in rules:\n        if rule.strip() == '':\n            continue\n        parsed_rule = {}\n        lines = rule.split(\"\\n\")\n        parsed_rule['orderValue'] = order\n        order += 1\n        for line in lines:\n            if line.strip() == '':\n                continue\n            key_value = line.strip().split(':')\n            key = key_value[0].strip()\n            value = key_value[1].strip()\n            if key == 'action':\n                parsed_rule['action'] = value\n            elif key == 'protocol':\n                parsed_rule['protocol'] = value\n            elif key == 'source_ip_address':\n                parsed_rule['sourceIpAddress'] = value\n            elif key == 'source_ip_subnet_mask':\n                parsed_rule['sourceIpSubnetMask'] = value\n            elif key == 'destination_ip_address':\n                parsed_rule['destinationIpAddress'] = value\n            elif key == 'destination_ip_subnet_mask':\n                parsed_rule['destinationIpSubnetMask'] = value\n            elif key == 'destination_port_range_start':\n                parsed_rule['destinationPortRangeStart'] = int(value)\n            elif key == 'destination_port_range_end':\n                parsed_rule['destinationPortRangeEnd'] = int(value)\n            elif key == 'version':\n                parsed_rule['version'] = int(value)\n        parsed_rules.append(parsed_rule)\n    return parsed_rules", "response": "Helper function to parse the input from the user into a list of rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rule_list(env, securitygroup_id, sortby):\n\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    table = formatting.Table(COLUMNS)\n    table.sortby = sortby\n\n    rules = mgr.list_securitygroup_rules(securitygroup_id)\n    for rule in rules:\n        port_min = rule.get('portRangeMin')\n        port_max = rule.get('portRangeMax')\n        if port_min is None:\n            port_min = formatting.blank()\n        if port_max is None:\n            port_max = formatting.blank()\n\n        table.add_row([\n            rule['id'],\n            rule.get('remoteIp') or formatting.blank(),\n            rule.get('remoteGroupId') or formatting.blank(),\n            rule['direction'],\n            rule.get('ethertype') or formatting.blank(),\n            port_min,\n            port_max,\n            rule.get('protocol') or formatting.blank(),\n            rule.get('createDate') or formatting.blank(),\n            rule.get('modifyDate') or formatting.blank()\n        ])\n\n    env.fout(table)", "response": "List security group rules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a security group rule to a security group.", "response": "def add(env, securitygroup_id, remote_ip, remote_group,\n        direction, ethertype, port_max, port_min, protocol):\n    \"\"\"Add a security group rule to a security group.\n\n    \\b\n    Examples:\n        # Add an SSH rule (TCP port 22) to a security group\n        slcli sg rule-add 384727 \\\\\n            --direction ingress \\\\\n            --protocol tcp \\\\\n            --port-min 22 \\\\\n            --port-max 22\n\n    \\b\n        # Add a ping rule (ICMP type 8 code 0) to a security group\n        slcli sg rule-add 384727 \\\\\n            --direction ingress \\\\\n            --protocol icmp \\\\\n            --port-min 8 \\\\\n            --port-max 0\n    \"\"\"\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    ret = mgr.add_securitygroup_rule(securitygroup_id, remote_ip, remote_group,\n                                     direction, ethertype, port_max,\n                                     port_min, protocol)\n\n    if not ret:\n        raise exceptions.CLIAbort(\"Failed to add security group rule\")\n\n    table = formatting.Table(REQUEST_RULES_COLUMNS)\n    table.add_row([ret['requestId'], str(ret['rules'])])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nedit a security group rule.", "response": "def edit(env, securitygroup_id, rule_id, remote_ip, remote_group,\n         direction, ethertype, port_max, port_min, protocol):\n    \"\"\"Edit a security group rule in a security group.\"\"\"\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    data = {}\n    if remote_ip:\n        data['remote_ip'] = remote_ip\n    if remote_group:\n        data['remote_group'] = remote_group\n    if direction:\n        data['direction'] = direction\n    if ethertype:\n        data['ethertype'] = ethertype\n    if port_max is not None:\n        data['port_max'] = port_max\n    if port_min is not None:\n        data['port_min'] = port_min\n    if protocol:\n        data['protocol'] = protocol\n\n    ret = mgr.edit_securitygroup_rule(securitygroup_id, rule_id, **data)\n\n    if not ret:\n        raise exceptions.CLIAbort(\"Failed to edit security group rule\")\n\n    table = formatting.Table(REQUEST_BOOL_COLUMNS)\n    table.add_row([ret['requestId']])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(env, securitygroup_id, rule_id):\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    ret = mgr.remove_securitygroup_rule(securitygroup_id, rule_id)\n\n    if not ret:\n        raise exceptions.CLIAbort(\"Failed to remove security group rule\")\n\n    table = formatting.Table(REQUEST_BOOL_COLUMNS)\n    table.add_row([ret['requestId']])\n\n    env.fout(table)", "response": "Remove a rule from a security group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    print(\"ERROR: Use the 'slcli' command instead.\", file=sys.stderr)\n    print(\"> slcli %s\" % ' '.join(sys.argv[1:]), file=sys.stderr)\n    exit(-1)", "response": "Main function for the deprecated sl command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, identifier, allocation, port, routing_type, routing_method):\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    loadbal_id, group_id = loadbal.parse_id(identifier)\n\n    # check if any input is provided\n    if not any([allocation, port, routing_type, routing_method]):\n        raise exceptions.CLIAbort(\n            'At least one property is required to be changed!')\n\n    mgr.edit_service_group(loadbal_id,\n                           group_id,\n                           allocation=allocation,\n                           port=port,\n                           routing_type=routing_type,\n                           routing_method=routing_method)\n\n    env.fout('Load balancer service group %s is being updated!' % identifier)", "response": "Edit an existing load balancer service group."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetails of a specific event and ability to acknowledge event.", "response": "def cli(env, identifier, ack):\n    \"\"\"Details of a specific event, and ability to acknowledge event.\"\"\"\n\n    # Print a list of all on going maintenance\n    manager = AccountManager(env.client)\n    event = manager.get_event(identifier)\n\n    if ack:\n        manager.ack_event(identifier)\n\n    env.fout(basic_event_table(event))\n    env.fout(impacted_table(event))\n    env.fout(update_table(event))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting a basic event table", "response": "def basic_event_table(event):\n    \"\"\"Formats a basic event table\"\"\"\n    table = formatting.Table([\"Id\", \"Status\", \"Type\", \"Start\", \"End\"],\n                             title=utils.clean_splitlines(event.get('subject')))\n\n    table.add_row([\n        event.get('id'),\n        utils.lookup(event, 'statusCode', 'name'),\n        utils.lookup(event, 'notificationOccurrenceEventType', 'keyName'),\n        utils.clean_time(event.get('startDate')),\n        utils.clean_time(event.get('endDate'))\n    ])\n\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef impacted_table(event):\n    table = formatting.Table([\n        \"Type\", \"Id\", \"Hostname\", \"PrivateIp\", \"Label\"\n    ])\n    for item in event.get('impactedResources', []):\n        table.add_row([\n            item.get('resourceType'),\n            item.get('resourceTableId'),\n            item.get('hostname'),\n            item.get('privateIp'),\n            item.get('filterLabel')\n        ])\n    return table", "response": "Formats a basic impacted resources table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a basic event update table", "response": "def update_table(event):\n    \"\"\"Formats a basic event update table\"\"\"\n    update_number = 0\n    for update in event.get('updates', []):\n        header = \"======= Update #%s on %s =======\" % (update_number, utils.clean_time(update.get('startDate')))\n        click.secho(header, fg='green')\n        update_number = update_number + 1\n        text = update.get('contents')\n        # deals with all the \\r\\n from the API\n        click.secho(utils.clean_splitlines(text))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist object storage endpoints.", "response": "def cli(env):\n    \"\"\"List object storage endpoints.\"\"\"\n\n    mgr = SoftLayer.ObjectStorageManager(env.client)\n    endpoints = mgr.list_endpoints()\n\n    table = formatting.Table(['datacenter', 'public', 'private'])\n    for endpoint in endpoints:\n        table.add_row([\n            endpoint['datacenter']['name'],\n            endpoint['public'],\n            endpoint['private'],\n        ])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist snapshot schedules for a given volume.", "response": "def cli(env, volume_id):\n    \"\"\"Lists snapshot schedules for a given volume\"\"\"\n\n    file_manager = SoftLayer.FileStorageManager(env.client)\n\n    snapshot_schedules = file_manager.list_volume_schedules(volume_id)\n\n    table = formatting.Table(['id',\n                              'active',\n                              'type',\n                              'replication',\n                              'date_created',\n                              'minute',\n                              'hour',\n                              'day',\n                              'week',\n                              'day_of_week',\n                              'date_of_month',\n                              'month_of_year',\n                              'maximum_snapshots'])\n\n    for schedule in snapshot_schedules:\n\n        if 'REPLICATION' in schedule['type']['keyname']:\n            replication = '*'\n        else:\n            replication = formatting.blank()\n\n        file_schedule_type = schedule['type']['keyname'].replace('REPLICATION_', '')\n        file_schedule_type = file_schedule_type.replace('SNAPSHOT_', '')\n\n        property_list = ['MINUTE', 'HOUR', 'DAY', 'WEEK',\n                         'DAY_OF_WEEK', 'DAY_OF_MONTH',\n                         'MONTH_OF_YEAR', 'SNAPSHOT_LIMIT']\n\n        schedule_properties = []\n        for prop_key in property_list:\n            item = formatting.blank()\n            for schedule_property in schedule.get('properties', []):\n                if schedule_property['type']['keyname'] == prop_key:\n                    if schedule_property['value'] == '-1':\n                        item = '*'\n                    else:\n                        item = schedule_property['value']\n                    break\n            schedule_properties.append(item)\n\n        table_row = [\n            schedule['id'],\n            '*' if schedule.get('active', '') else '',\n            file_schedule_type,\n            replication,\n            schedule.get('createDate', '')\n        ]\n        table_row.extend(schedule_properties)\n        table.add_row(table_row)\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npulling out settings from a SoftLayer. BaseClient instance.", "response": "def get_settings_from_client(client):\n    \"\"\"Pull out settings from a SoftLayer.BaseClient instance.\n\n    :param client: SoftLayer.BaseClient instance\n    \"\"\"\n    settings = {\n        'username': '',\n        'api_key': '',\n        'timeout': '',\n        'endpoint_url': '',\n    }\n    try:\n        settings['username'] = client.auth.username\n        settings['api_key'] = client.auth.api_key\n    except AttributeError:\n        pass\n\n    transport = _resolve_transport(client.transport)\n    try:\n        settings['timeout'] = transport.timeout\n        settings['endpoint_url'] = transport.endpoint_url\n    except AttributeError:\n        pass\n\n    return settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef config_table(settings):\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n    table.add_row(['Username', settings['username'] or 'not set'])\n    table.add_row(['API Key', settings['api_key'] or 'not set'])\n    table.add_row(['Endpoint URL', settings['endpoint_url'] or 'not set'])\n    table.add_row(['Timeout', settings['timeout'] or 'not set'])\n    return table", "response": "Returns a config table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, name, backend_router_id, flavor, instances, test=False):\n    manager = CapacityManager(env.client)\n\n    result = manager.create(\n        name=name,\n        backend_router_id=backend_router_id,\n        flavor=flavor,\n        instances=instances,\n        test=test)\n    if test:\n        table = formatting.Table(['Name', 'Value'], \"Test Order\")\n        container = result['orderContainers'][0]\n        table.add_row(['Name', container['name']])\n        table.add_row(['Location', container['locationObject']['longName']])\n        for price in container['prices']:\n            table.add_row(['Contract', price['item']['description']])\n        table.add_row(['Hourly Total', result['postTaxRecurring']])\n    else:\n        table = formatting.Table(['Name', 'Value'], \"Reciept\")\n        table.add_row(['Order Date', result['orderDate']])\n        table.add_row(['Order ID', result['orderId']])\n        table.add_row(['status', result['placedOrder']['status']])\n        table.add_row(['Hourly Total', result['orderDetails']['postTaxRecurring']])\n    env.fout(table)", "response": "Create a Reserved Capacity instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists origin pull mappings.", "response": "def cli(env, account_id):\n    \"\"\"List origin pull mappings.\"\"\"\n\n    manager = SoftLayer.CDNManager(env.client)\n    origins = manager.get_origins(account_id)\n\n    table = formatting.Table(['id', 'media_type', 'cname', 'origin_url'])\n\n    for origin in origins:\n        table.add_row([origin['id'],\n                       origin['mediaType'],\n                       origin.get('cname', formatting.blank()),\n                       origin['originUrl']])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env):\n    manager = PlacementManager(env.client)\n\n    routers = manager.get_routers()\n    env.fout(get_router_table(routers))\n\n    rules = manager.get_all_rules()\n    env.fout(get_rule_table(rules))", "response": "List options for creating a placement group."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats output from _get_routers and returns a table.", "response": "def get_router_table(routers):\n    \"\"\"Formats output from _get_routers and returns a table. \"\"\"\n    table = formatting.Table(['Datacenter', 'Hostname', 'Backend Router Id'], \"Available Routers\")\n    for router in routers:\n        datacenter = router['topLevelLocation']['longName']\n        table.add_row([datacenter, router['hostname'], router['id']])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_rule_table(rules):\n    table = formatting.Table(['Id', 'KeyName'], \"Rules\")\n    for rule in rules:\n        table.add_row([rule['id'], rule['keyName']])\n    return table", "response": "Formats output from get_all_rules and returns a table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget details for a virtual server.", "response": "def cli(env, identifier, passwords=False, price=False):\n    \"\"\"Get details for a virtual server.\"\"\"\n\n    vsi = SoftLayer.VSManager(env.client)\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    result = vsi.get_instance(vs_id)\n    result = utils.NestedDict(result)\n\n    table.add_row(['id', result['id']])\n    table.add_row(['guid', result['globalIdentifier']])\n    table.add_row(['hostname', result['hostname']])\n    table.add_row(['domain', result['domain']])\n    table.add_row(['fqdn', result['fullyQualifiedDomainName']])\n    table.add_row(['status', formatting.FormattedItem(\n        result['status']['keyName'] or formatting.blank(),\n        result['status']['name'] or formatting.blank()\n    )])\n    table.add_row(['state', formatting.FormattedItem(\n        utils.lookup(result, 'powerState', 'keyName'),\n        utils.lookup(result, 'powerState', 'name'),\n    )])\n    table.add_row(['active_transaction', formatting.active_txn(result)])\n    table.add_row(['datacenter',\n                   result['datacenter']['name'] or formatting.blank()])\n    _cli_helper_dedicated_host(env, result, table)\n    operating_system = utils.lookup(result,\n                                    'operatingSystem',\n                                    'softwareLicense',\n                                    'softwareDescription') or {}\n    table.add_row(['os', operating_system.get('name') or formatting.blank()])\n    table.add_row(['os_version',\n                   operating_system.get('version') or formatting.blank()])\n    table.add_row(['cores', result['maxCpu']])\n    table.add_row(['memory', formatting.mb_to_gb(result['maxMemory'])])\n    table.add_row(['public_ip',\n                   result['primaryIpAddress'] or formatting.blank()])\n    table.add_row(['private_ip',\n                   result['primaryBackendIpAddress'] or formatting.blank()])\n    table.add_row(['private_only', result['privateNetworkOnlyFlag']])\n    table.add_row(['private_cpu', result['dedicatedAccountHostOnlyFlag']])\n    table.add_row(['created', result['createDate']])\n    table.add_row(['modified', result['modifyDate']])\n    if utils.lookup(result, 'billingItem') != []:\n        table.add_row(['owner', formatting.FormattedItem(\n            utils.lookup(result, 'billingItem', 'orderItem',\n                         'order', 'userRecord',\n                         'username') or formatting.blank(),\n        )])\n    else:\n        table.add_row(['owner', formatting.blank()])\n\n    vlan_table = formatting.Table(['type', 'number', 'id'])\n    for vlan in result['networkVlans']:\n        vlan_table.add_row([\n            vlan['networkSpace'], vlan['vlanNumber'], vlan['id']])\n    table.add_row(['vlans', vlan_table])\n\n    if result.get('networkComponents'):\n        secgroup_table = formatting.Table(['interface', 'id', 'name'])\n        has_secgroups = False\n        for comp in result.get('networkComponents'):\n            interface = 'PRIVATE' if comp['port'] == 0 else 'PUBLIC'\n            for binding in comp['securityGroupBindings']:\n                has_secgroups = True\n                secgroup = binding['securityGroup']\n                secgroup_table.add_row([\n                    interface, secgroup['id'],\n                    secgroup.get('name') or formatting.blank()])\n        if has_secgroups:\n            table.add_row(['security_groups', secgroup_table])\n\n    if result.get('notes'):\n        table.add_row(['notes', result['notes']])\n\n    if price:\n        total_price = utils.lookup(result,\n                                   'billingItem',\n                                   'nextInvoiceTotalRecurringAmount') or 0\n        total_price += sum(p['nextInvoiceTotalRecurringAmount']\n                           for p\n                           in utils.lookup(result,\n                                           'billingItem',\n                                           'children') or [])\n        table.add_row(['price_rate', total_price])\n\n    if passwords:\n        pass_table = formatting.Table(['software', 'username', 'password'])\n\n        for component in result['softwareComponents']:\n            for item in component['passwords']:\n                pass_table.add_row([\n                    utils.lookup(component,\n                                 'softwareLicense',\n                                 'softwareDescription',\n                                 'name'),\n                    item['username'],\n                    item['password'],\n                ])\n\n        table.add_row(['users', pass_table])\n\n    table.add_row(['tags', formatting.tags(result['tagReferences'])])\n\n    # Test to see if this actually has a primary (public) ip address\n    try:\n        if not result['privateNetworkOnlyFlag']:\n            ptr_domains = env.client.call(\n                'Virtual_Guest', 'getReverseDomainRecords',\n                id=vs_id,\n            )\n\n            for ptr_domain in ptr_domains:\n                for ptr in ptr_domain['resourceRecords']:\n                    table.add_row(['ptr', ptr['data']])\n    except SoftLayer.SoftLayerAPIError:\n        pass\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets details on dedicated host for a virtual server.", "response": "def _cli_helper_dedicated_host(env, result, table):\n    \"\"\"Get details on dedicated host for a virtual server.\"\"\"\n\n    dedicated_host_id = utils.lookup(result, 'dedicatedHost', 'id')\n    if dedicated_host_id:\n        table.add_row(['dedicated_host_id', dedicated_host_id])\n        # Try to find name of dedicated host\n        try:\n            dedicated_host = env.client.call('Virtual_DedicatedHost', 'getObject',\n                                             id=dedicated_host_id)\n        except SoftLayer.SoftLayerAPIError:\n            LOGGER.error('Unable to get dedicated host id %s', dedicated_host_id)\n            dedicated_host = {}\n        table.add_row(['dedicated_host',\n                       dedicated_host.get('name') or formatting.blank()])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a virtual server is ready.", "response": "def cli(env, identifier, wait):\n    \"\"\"Check if a virtual server is ready.\"\"\"\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    ready = vsi.wait_for_ready(vs_id, wait)\n    if ready:\n        env.fout(\"READY\")\n    else:\n        raise exceptions.CLIAbort(\"Instance %s not ready\" % vs_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels a dedicated host server immediately", "response": "def cli(env, identifier):\n    \"\"\"Cancel a dedicated host server immediately\"\"\"\n\n    mgr = SoftLayer.DedicatedHostManager(env.client)\n\n    host_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'dedicated host')\n\n    if not (env.skip_confirmations or formatting.no_going_back(host_id)):\n        raise exceptions.CLIAbort('Aborted')\n\n    mgr.cancel_host(host_id)\n\n    click.secho('Dedicated Host %s was cancelled' % host_id, fg='green')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget details about an image.", "response": "def get_image(self, image_id, **kwargs):\n        \"\"\"Get details about an image.\n\n        :param int image: The ID of the image.\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        \"\"\"\n        if 'mask' not in kwargs:\n            kwargs['mask'] = IMAGE_MASK\n\n        return self.vgbdtg.getObject(id=image_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting all private images.", "response": "def list_private_images(self, guid=None, name=None, **kwargs):\n        \"\"\"List all private images.\n\n        :param string guid: filter based on GUID\n        :param string name: filter based on name\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        \"\"\"\n        if 'mask' not in kwargs:\n            kwargs['mask'] = IMAGE_MASK\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n        if name:\n            _filter['privateBlockDeviceTemplateGroups']['name'] = (\n                utils.query_filter(name))\n\n        if guid:\n            _filter['privateBlockDeviceTemplateGroups']['globalIdentifier'] = (\n                utils.query_filter(guid))\n\n        kwargs['filter'] = _filter.to_dict()\n\n        account = self.client['Account']\n        return account.getPrivateBlockDeviceTemplateGroups(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all public images.", "response": "def list_public_images(self, guid=None, name=None, **kwargs):\n        \"\"\"List all public images.\n\n        :param string guid: filter based on GUID\n        :param string name: filter based on name\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        \"\"\"\n        if 'mask' not in kwargs:\n            kwargs['mask'] = IMAGE_MASK\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n        if name:\n            _filter['name'] = utils.query_filter(name)\n\n        if guid:\n            _filter['globalIdentifier'] = utils.query_filter(guid)\n\n        kwargs['filter'] = _filter.to_dict()\n\n        return self.vgbdtg.getPublicImages(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_ids_from_name_public(self, name):\n        results = self.list_public_images(name=name)\n        return [result['id'] for result in results]", "response": "Get public images which match the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_ids_from_name_private(self, name):\n        results = self.list_private_images(name=name)\n        return [result['id'] for result in results]", "response": "Get private images which match the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nedits image related details.", "response": "def edit(self, image_id, name=None, note=None, tag=None):\n        \"\"\"Edit image related details.\n\n        :param int image_id: The ID of the image\n        :param string name: Name of the Image.\n        :param string note: Note of the image.\n        :param string tag: Tags of the image to be updated to.\n        \"\"\"\n        obj = {}\n        if name:\n            obj['name'] = name\n        if note:\n            obj['note'] = note\n        if obj:\n            self.vgbdtg.editObject(obj, id=image_id)\n        if tag:\n            self.vgbdtg.setTags(str(tag), id=image_id)\n\n        return bool(name or note or tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports an image from an object storage object.", "response": "def import_image_from_uri(self, name, uri, os_code=None, note=None,\n                              ibm_api_key=None, root_key_crn=None,\n                              wrapped_dek=None, cloud_init=False,\n                              byol=False, is_encrypted=False):\n        \"\"\"Import a new image from object storage.\n\n        :param string name: Name of the new image\n        :param string uri: The URI for an object storage object\n            (.vhd/.iso file) of the format:\n            swift://<objectStorageAccount>@<cluster>/<container>/<objectPath>\n            or (.vhd/.iso/.raw file) of the format:\n            cos://<regionName>/<bucketName>/<objectPath> if using IBM Cloud\n            Object Storage\n        :param string os_code: The reference code of the operating system\n        :param string note: Note to add to the image\n        :param string ibm_api_key: Ibm Api Key needed to communicate with ICOS\n            and your KMS\n        :param string root_key_crn: CRN of the root key in your KMS. Go to your\n            KMS (Key Protect or Hyper Protect) provider to get the CRN for your\n            root key.  An example CRN:\n            crn:v1:bluemix:public:hs-crypto:us-south:acctID:serviceID:key:keyID'\n            Used only when is_encrypted is True.\n        :param string wrapped_dek: Wrapped Data Encryption Key provided by\n            your KMS. Used only when is_encrypted is True.\n        :param boolean cloud_init: Specifies if image is cloud-init\n        :param boolean byol: Specifies if image is bring your own license\n        :param boolean is_encrypted: Specifies if image is encrypted\n        \"\"\"\n        if 'cos://' in uri:\n            return self.vgbdtg.createFromIcos({\n                'name': name,\n                'note': note,\n                'operatingSystemReferenceCode': os_code,\n                'uri': uri,\n                'ibmApiKey': ibm_api_key,\n                'crkCrn': root_key_crn,\n                'wrappedDek': wrapped_dek,\n                'cloudInit': cloud_init,\n                'byol': byol,\n                'isEncrypted': is_encrypted\n            })\n        else:\n            return self.vgbdtg.createFromExternalSource({\n                'name': name,\n                'note': note,\n                'operatingSystemReferenceCode': os_code,\n                'uri': uri,\n            })"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_image_to_uri(self, image_id, uri, ibm_api_key=None):\n        if 'cos://' in uri:\n            return self.vgbdtg.copyToIcos({\n                'uri': uri,\n                'ibmApiKey': ibm_api_key\n            }, id=image_id)\n        else:\n            return self.vgbdtg.copyToExternalSource({'uri': uri}, id=image_id)", "response": "Export an image into the given object storage"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes an Object Storage Account.", "response": "def cli(env, identifier, credential_id):\n    \"\"\"Delete the credential of an Object Storage Account.\"\"\"\n\n    mgr = SoftLayer.ObjectStorageManager(env.client)\n    credential = mgr.delete_credential(identifier, credential_id=credential_id)\n\n    env.fout(credential)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_object(self, identifier, mask=None):\n        if mask is None:\n            mask = \"mask[instances[billingItem[item[keyName],category], guest], backendRouter[datacenter]]\"\n        result = self.client.call(self.rcg_service, 'getObject', id=identifier, mask=mask)\n        return result", "response": "Get a Reserved Capacity Group object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_create_options(self):\n        mask = \"mask[attributes,prices[pricingLocationGroup]]\"\n        results = self.ordering_manager.list_items(self.capacity_package, mask=mask)\n        return results", "response": "List available reserved capacity plans"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_available_routers(self, dc=None):\n        mask = \"mask[locations]\"\n        # Step 1, get the package id\n        package = self.ordering_manager.get_package_by_key(self.capacity_package, mask=\"id\")\n\n        # Step 2, get the regions this package is orderable in\n        regions = self.client.call('Product_Package', 'getRegions', id=package['id'], mask=mask, iter=True)\n        _filter = None\n        routers = {}\n        if dc is not None:\n            _filter = {'datacenterName': {'operation': dc}}\n\n        # Step 3, for each location in each region, get the pod details, which contains the router id\n        pods = self.client.call('Network_Pod', 'getAllObjects', filter=_filter, iter=True)\n        for region in regions:\n            routers[region['keyname']] = []\n            for location in region['locations']:\n                location['location']['pods'] = list()\n                for pod in pods:\n                    if pod['datacenterName'] == location['location']['name']:\n                        location['location']['pods'].append(pod)\n\n        # Step 4, return the data.\n        return regions", "response": "Pulls down all backendRouterIds that are available for a specific location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\norder a Virtual_ReservedCapacityGroup :param string name: Name for the new reserved capacity :param int backend_router_id: This selects the pod. See create_options for a list :param string flavor: Capacity KeyName, see create_options for a list :param int instances: Number of guest this capacity can support :param bool test: If True, don't actually order, just test.", "response": "def create(self, name, backend_router_id, flavor, instances, test=False):\n        \"\"\"Orders a Virtual_ReservedCapacityGroup\n\n        :param string name: Name for the new reserved capacity\n        :param int backend_router_id: This selects the pod. See create_options for a list\n        :param string flavor: Capacity KeyName, see create_options for a list\n        :param int instances: Number of guest this capacity can support\n        :param bool test: If True, don't actually order, just test.\n        \"\"\"\n\n        # Since orderManger needs a DC id, just send in 0, the API will ignore it\n        args = (self.capacity_package, 0, [flavor])\n        extras = {\"backendRouterId\": backend_router_id, \"name\": name}\n        kwargs = {\n            'extras': extras,\n            'quantity': instances,\n            'complex_type': 'SoftLayer_Container_Product_Order_Virtual_ReservedCapacity',\n            'hourly': True\n        }\n        if test:\n            receipt = self.ordering_manager.verify_order(*args, **kwargs)\n        else:\n            receipt = self.ordering_manager.place_order(*args, **kwargs)\n        return receipt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_guest(self, capacity_id, test, guest_object):\n\n        vs_manager = VSManager(self.client)\n        mask = \"mask[instances[id, billingItem[id, item[id,keyName]]], backendRouter[id, datacenter[name]]]\"\n        capacity = self.get_object(capacity_id, mask=mask)\n        try:\n            capacity_flavor = capacity['instances'][0]['billingItem']['item']['keyName']\n            flavor = _flavor_string(capacity_flavor, guest_object['primary_disk'])\n        except KeyError:\n            raise SoftLayer.SoftLayerError(\"Unable to find capacity Flavor.\")\n\n        guest_object['flavor'] = flavor\n        guest_object['datacenter'] = capacity['backendRouter']['datacenter']['name']\n\n        # Reserved capacity only supports SAN as of 20181008\n        guest_object['local_disk'] = False\n        template = vs_manager.verify_create_instance(**guest_object)\n        template['reservedCapacityId'] = capacity_id\n        if guest_object.get('ipv6'):\n            ipv6_price = self.ordering_manager.get_price_id_list('PUBLIC_CLOUD_SERVER', ['1_IPV6_ADDRESS'])\n            template['prices'].append({'id': ipv6_price[0]})\n\n        if test:\n            result = self.client.call('Product_Order', 'verifyOrder', template)\n        else:\n            result = self.client.call('Product_Order', 'placeOrder', template)\n\n        return result", "response": "Turns an empty Reserve Capacity into a real Virtual Guest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting Reserved Capacity Groups.", "response": "def cli(env):\n    \"\"\"List Reserved Capacity groups.\"\"\"\n    manager = CapacityManager(env.client)\n    result = manager.list()\n    table = formatting.Table(\n        [\"ID\", \"Name\", \"Capacity\", \"Flavor\", \"Location\", \"Created\"],\n        title=\"Reserved Capacity\"\n    )\n    for r_c in result:\n        occupied_string = \"#\" * int(r_c.get('occupiedInstanceCount', 0))\n        available_string = \"-\" * int(r_c.get('availableInstanceCount', 0))\n\n        try:\n            flavor = r_c['instances'][0]['billingItem']['description']\n            # cost = float(r_c['instances'][0]['billingItem']['hourlyRecurringFee'])\n        except KeyError:\n            flavor = \"Unknown Billing Item\"\n        location = r_c['backendRouter']['hostname']\n        capacity = \"%s%s\" % (occupied_string, available_string)\n        table.add_row([r_c['id'], r_c['name'], capacity, flavor, location, r_c['createDate']])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, package_keyname, location, preset, name, send_email, complex_type,\n        extras, order_items):\n    \"\"\"Place a quote.\n\n    This CLI command is used for creating  a quote of the specified package in\n    the given location (denoted by a datacenter's long name). Orders made via the CLI\n    can then be converted to be made programmatically by calling\n    SoftLayer.OrderingManager.place_quote() with the same keynames.\n\n    Packages for ordering can be retrieved from `slcli order package-list`\n    Presets for ordering can be retrieved from `slcli order preset-list` (not all packages\n    have presets)\n\n    Items can be retrieved from `slcli order item-list`. In order to find required\n    items for the order, use `slcli order category-list`, and then provide the\n    --category option for each category code in `slcli order item-list`.\n\n\n    Example::\n\n        # Place quote a VSI with 4 CPU, 16 GB RAM, 100 GB SAN disk,\n        # Ubuntu 16.04, and 1 Gbps public & private uplink in dal13\n        slcli order place-quote --name \"foobar\" --send-email CLOUD_SERVER DALLAS13 \\\\\n            GUEST_CORES_4 \\\\\n            RAM_16_GB \\\\\n            REBOOT_REMOTE_CONSOLE \\\\\n            1_GBPS_PUBLIC_PRIVATE_NETWORK_UPLINKS \\\\\n            BANDWIDTH_0_GB_2 \\\\\n            1_IP_ADDRESS \\\\\n            GUEST_DISK_100_GB_SAN \\\\\n            OS_UBUNTU_16_04_LTS_XENIAL_XERUS_MINIMAL_64_BIT_FOR_VSI \\\\\n            MONITORING_HOST_PING \\\\\n            NOTIFICATION_EMAIL_AND_TICKET \\\\\n            AUTOMATED_NOTIFICATION \\\\\n            UNLIMITED_SSL_VPN_USERS_1_PPTP_VPN_USER_PER_ACCOUNT \\\\\n            NESSUS_VULNERABILITY_ASSESSMENT_REPORTING \\\\\n            --extras '{\"virtualGuests\": [{\"hostname\": \"test\", \"domain\": \"softlayer.com\"}]}' \\\\\n            --complex-type SoftLayer_Container_Product_Order_Virtual_Guest\n\n    \"\"\"\n    manager = ordering.OrderingManager(env.client)\n\n    if extras:\n        try:\n            extras = json.loads(extras)\n        except ValueError as err:\n            raise exceptions.CLIAbort(\"There was an error when parsing the --extras value: {}\".format(err))\n\n    args = (package_keyname, location, order_items)\n    kwargs = {'preset_keyname': preset,\n              'extras': extras,\n              'quantity': 1,\n              'quote_name': name,\n              'send_email': send_email,\n              'complex_type': complex_type}\n\n    order = manager.place_quote(*args, **kwargs)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n    table.add_row(['id', order['quote']['id']])\n    table.add_row(['name', order['quote']['name']])\n    table.add_row(['created', order['orderDate']])\n    table.add_row(['expires', order['quote']['expirationDate']])\n    table.add_row(['status', order['quote']['status']])\n    env.fout(table)", "response": "This is the main entry point for the order command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, volume_id, notes):\n    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n    snapshot = file_storage_manager.create_snapshot(volume_id, notes=notes)\n\n    if 'id' in snapshot:\n        click.echo('New snapshot created with id: %s' % snapshot['id'])\n    else:\n        click.echo('Error occurred while creating snapshot.\\n'\n                   'Ensure volume is not failed over or in another '\n                   'state which prevents taking snapshots.')", "response": "Creates a snapshot on a given volume."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env):\n    table = formatting.Table([\n        'Id', 'Name', 'Created', 'Expiration', 'Status', 'Package Name', 'Package Id'\n    ])\n    table.align['Name'] = 'l'\n    table.align['Package Name'] = 'r'\n    table.align['Package Id'] = 'l'\n\n    manager = ordering.OrderingManager(env.client)\n    items = manager.get_quotes()\n\n    for item in items:\n        package = item['order']['items'][0]['package']\n        table.add_row([\n            item.get('id'),\n            item.get('name'),\n            clean_time(item.get('createDate')),\n            clean_time(item.get('modifyDate')),\n            item.get('status'),\n            package.get('keyName'),\n            package.get('id')\n        ])\n    env.fout(table)", "response": "List all active quotes on an account"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, snapshot_id):\n    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n    deleted = file_storage_manager.delete_snapshot(snapshot_id)\n\n    if deleted:\n        click.echo('Snapshot %s deleted' % snapshot_id)", "response": "Deletes a snapshot on a given volume"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, identifier, enabled, port, weight, healthcheck_type, ip_address):\n\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    loadbal_id, group_id = loadbal.parse_id(identifier)\n\n    # check if the IP is valid\n    ip_address_id = None\n    if ip_address:\n        ip_service = env.client['Network_Subnet_IpAddress']\n        ip_record = ip_service.getByIpAddress(ip_address)\n        if len(ip_record) > 0:\n            ip_address_id = ip_record['id']\n\n    mgr.add_service(loadbal_id,\n                    group_id,\n                    ip_address_id=ip_address_id,\n                    enabled=enabled,\n                    port=port,\n                    weight=weight,\n                    hc_type=healthcheck_type)\n    env.fout('Load balancer service is being added!')", "response": "Adds a new load balancer service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nenter a shell for slcli.", "response": "def cli(ctx, env):\n    \"\"\"Enters a shell for slcli.\"\"\"\n\n    # Set up the environment\n    env = copy.deepcopy(env)\n    env.load_modules_from_python(routes.ALL_ROUTES)\n    env.aliases.update(routes.ALL_ALIASES)\n    env.vars['global_args'] = ctx.parent.params\n    env.vars['is_shell'] = True\n    env.vars['last_exit_code'] = 0\n\n    # Set up prompt_toolkit settings\n    app_path = click.get_app_dir('softlayer_shell')\n    if not os.path.exists(app_path):\n        os.makedirs(app_path)\n    complete = completer.ShellCompleter(core.cli)\n\n    while True:\n        try:\n            line = p_shortcuts.prompt(\n                completer=complete,\n                complete_while_typing=True,\n                auto_suggest=p_auto_suggest.AutoSuggestFromHistory(),\n            )\n\n            # Parse arguments\n            try:\n                args = shlex.split(line)\n            except ValueError as ex:\n                print(\"Invalid Command: %s\" % ex)\n                continue\n\n            if not args:\n                continue\n\n            # Run Command\n            try:\n                # Reset client so that the client gets refreshed\n                env.client = None\n                core.main(args=list(get_env_args(env)) + args,\n                          obj=env,\n                          prog_name=\"\",\n                          reraise_exceptions=True)\n            except SystemExit as ex:\n                env.vars['last_exit_code'] = ex.code\n            except EOFError:\n                return\n            except ShellExit:\n                return\n            except Exception as ex:\n                env.vars['last_exit_code'] = 1\n                traceback.print_exc(file=sys.stderr)\n\n        except KeyboardInterrupt:\n            env.vars['last_exit_code'] = 130"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding options to inject into the slcli command from the environment.", "response": "def get_env_args(env):\n    \"\"\"Yield options to inject into the slcli command from the environment.\"\"\"\n    for arg, val in env.vars.get('global_args', {}).items():\n        if val is True:\n            yield '--%s' % arg\n        elif isinstance(val, int):\n            for _ in range(val):\n                yield '--%s' % arg\n        elif val is None:\n            continue\n        else:\n            yield '--%s=%s' % (arg, val)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a placement group.", "response": "def cli(env, identifier, purge):\n    \"\"\"Delete a placement group.\n\n    Placement Group MUST be empty before you can delete it.\n\n    IDENTIFIER can be either the Name or Id of the placement group you want to view\n    \"\"\"\n    manager = PlacementManager(env.client)\n    group_id = helpers.resolve_id(manager.resolve_ids, identifier, 'placement_group')\n\n    if purge:\n        placement_group = manager.get_object(group_id)\n        guest_list = ', '.join([guest['fullyQualifiedDomainName'] for guest in placement_group['guests']])\n        if len(placement_group['guests']) < 1:\n            raise exceptions.CLIAbort('No virtual servers were found in placement group %s' % identifier)\n\n        click.secho(\"You are about to delete the following guests!\\n%s\" % guest_list, fg='red')\n        if not (env.skip_confirmations or formatting.confirm(\"This action will cancel all guests! Continue?\")):\n            raise exceptions.CLIAbort('Aborting virtual server order.')\n        vm_manager = VSManager(env.client)\n        for guest in placement_group['guests']:\n            click.secho(\"Deleting %s...\" % guest['fullyQualifiedDomainName'])\n            vm_manager.cancel_instance(guest['id'])\n        return\n\n    click.secho(\"You are about to delete the following placement group! %s\" % identifier, fg='red')\n    if not (env.skip_confirmations or formatting.confirm(\"This action will cancel the placement group! Continue?\")):\n        raise exceptions.CLIAbort('Aborting virtual server order.')\n    cancel_result = manager.delete(group_id)\n    if cancel_result:\n        click.secho(\"Placement Group %s has been canceld.\" % identifier, fg='green')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate an IPSEC tunnel context.", "response": "def cli(env, context_id, translation_id, static_ip, remote_ip, note):\n    \"\"\"Update an address translation for an IPSEC tunnel context.\n\n    A separate configuration request should be made to realize changes on\n    network devices.\n    \"\"\"\n    manager = SoftLayer.IPSECManager(env.client)\n    succeeded = manager.update_translation(context_id,\n                                           translation_id,\n                                           static_ip=static_ip,\n                                           remote_ip=remote_ip,\n                                           notes=note)\n    if succeeded:\n        env.out('Updated translation #{}'.format(translation_id))\n    else:\n        raise CLIHalt('Failed to update translation #{}'.format(translation_id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting invoices and all that mess", "response": "def cli(env, identifier, details):\n    \"\"\"Invoices and all that mess\"\"\"\n\n    manager = AccountManager(env.client)\n    top_items = manager.get_billing_items(identifier)\n\n    title = \"Invoice %s\" % identifier\n    table = formatting.Table([\"Item Id\", \"Category\", \"Description\", \"Single\",\n                              \"Monthly\", \"Create Date\", \"Location\"], title=title)\n    table.align['category'] = 'l'\n    table.align['description'] = 'l'\n    for item in top_items:\n        fqdn = \"%s.%s\" % (item.get('hostName', ''), item.get('domainName', ''))\n        # category id=2046, ram_usage doesn't have a name...\n        category = utils.lookup(item, 'category', 'name') or item.get('categoryCode')\n        description = nice_string(item.get('description'))\n        if fqdn != '.':\n            description = \"%s (%s)\" % (item.get('description'), fqdn)\n        table.add_row([\n            item.get('id'),\n            category,\n            nice_string(description),\n            \"$%.2f\" % float(item.get('oneTimeAfterTaxAmount')),\n            \"$%.2f\" % float(item.get('recurringAfterTaxAmount')),\n            utils.clean_time(item.get('createDate'), out_format=\"%Y-%m-%d\"),\n            utils.lookup(item, 'location', 'name')\n        ])\n        if details:\n            for child in item.get('children', []):\n                table.add_row([\n                    '>>>',\n                    utils.lookup(child, 'category', 'name'),\n                    nice_string(child.get('description')),\n                    \"$%.2f\" % float(child.get('oneTimeAfterTaxAmount')),\n                    \"$%.2f\" % float(child.get('recurringAfterTaxAmount')),\n                    '---',\n                    '---'\n                ])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(ctx, env):\n    env.out(\"Welcome to the SoftLayer shell.\")\n    env.out(\"\")\n\n    formatter = formatting.HelpFormatter()\n    commands = []\n    shell_commands = []\n    for name in cli_core.cli.list_commands(ctx):\n        command = cli_core.cli.get_command(ctx, name)\n        if command.short_help is None:\n            command.short_help = command.help\n        details = (name, command.short_help)\n        if name in dict(routes.ALL_ROUTES):\n            shell_commands.append(details)\n        else:\n            commands.append(details)\n\n    with formatter.section('Shell Commands'):\n        formatter.write_dl(shell_commands)\n\n    with formatter.section('Commands'):\n        formatter.write_dl(commands)\n\n    for line in formatter.buffer:\n        env.out(line, newline=False)", "response": "Print shell help text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, identifier, postinstall, key):\n\n    hardware = SoftLayer.HardwareManager(env.client)\n    hardware_id = helpers.resolve_id(hardware.resolve_ids,\n                                     identifier,\n                                     'hardware')\n    key_list = []\n    if key:\n        for single_key in key:\n            resolver = SoftLayer.SshKeyManager(env.client).resolve_ids\n            key_id = helpers.resolve_id(resolver, single_key, 'SshKey')\n            key_list.append(key_id)\n    if not (env.skip_confirmations or formatting.no_going_back(hardware_id)):\n        raise exceptions.CLIAbort('Aborted')\n\n    hardware.reload(hardware_id, postinstall, key_list)", "response": "Reload operating system on a server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nedits the properties of a service group.", "response": "def cli(env, identifier, enabled, port, weight, healthcheck_type, ip_address):\n    \"\"\"Edit the properties of a service group.\"\"\"\n\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    loadbal_id, service_id = loadbal.parse_id(identifier)\n\n    # check if any input is provided\n    if ((not any([ip_address, weight, port, healthcheck_type])) and\n            enabled is None):\n        raise exceptions.CLIAbort(\n            'At least one property is required to be changed!')\n\n    # check if the IP is valid\n    ip_address_id = None\n    if ip_address:\n        ip_service = env.client['Network_Subnet_IpAddress']\n        ip_record = ip_service.getByIpAddress(ip_address)\n        ip_address_id = ip_record['id']\n\n    mgr.edit_service(loadbal_id,\n                     service_id,\n                     ip_address_id=ip_address_id,\n                     enabled=enabled,\n                     port=port,\n                     weight=weight,\n                     hc_type=healthcheck_type)\n    env.fout('Load balancer service %s is being modified!' % identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, target, firewall_type, high_availability):\n\n    mgr = SoftLayer.FirewallManager(env.client)\n\n    if not env.skip_confirmations:\n        if firewall_type == 'vlan':\n            pkg = mgr.get_dedicated_package(ha_enabled=high_availability)\n        elif firewall_type == 'vs':\n            pkg = mgr.get_standard_package(target, is_virt=True)\n        elif firewall_type == 'server':\n            pkg = mgr.get_standard_package(target, is_virt=False)\n\n        if not pkg:\n            exceptions.CLIAbort(\n                \"Unable to add firewall - Is network public enabled?\")\n\n        env.out(\"******************\")\n        env.out(\"Product: %s\" % pkg[0]['description'])\n        env.out(\"Price: $%s monthly\" % pkg[0]['prices'][0]['recurringFee'])\n        env.out(\"******************\")\n\n        if not formatting.confirm(\"This action will incur charges on your \"\n                                  \"account. Continue?\"):\n            raise exceptions.CLIAbort('Aborted.')\n\n    if firewall_type == 'vlan':\n        mgr.add_vlan_firewall(target, ha_enabled=high_availability)\n    elif firewall_type == 'vs':\n        mgr.add_standard_firewall(target, is_virt=True)\n    elif firewall_type == 'server':\n        mgr.add_standard_firewall(target, is_virt=False)\n\n    env.fout(\"Firewall is being created!\")", "response": "Create new firewall.\n\n    TARGET: Id of the server the firewall will protect"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npurging cached files from all edge nodes.", "response": "def cli(env, account_id, content_url):\n    \"\"\"Purge cached files from all edge nodes.\n\n    Examples:\n         slcli cdn purge 97794 http://example.com/cdn/file.txt\n         slcli cdn purge 97794 http://example.com/cdn/file.txt https://dal01.example.softlayer.net/image.png\n    \"\"\"\n\n    manager = SoftLayer.CDNManager(env.client)\n    content_list = manager.purge_content(account_id, content_url)\n\n    table = formatting.Table(['url', 'status'])\n\n    for content in content_list:\n        table.add_row([\n            content['url'],\n            content['statusCode']\n        ])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretries calling the decorated function using an exponential backoff. http://www.saltycrane.com/blog/2009/11/trying-out-retry-decorator-python/ original from: http://wiki.python.org/moin/PythonDecoratorLibrary#Retry :param ex: the exception to check. may be a tuple of exceptions to check :param tries: number of times to try (not retry) before giving up :param delay: initial delay between retries in seconds. A random 0-5s will be added to this number to stagger calls. :param backoff: backoff multiplier e.g. value of 2 will double the delay each retry :param logger: logger to use. If None, print", "response": "def retry(ex=RETRIABLE, tries=4, delay=5, backoff=2, logger=None):\n    \"\"\"Retry calling the decorated function using an exponential backoff.\n\n    http://www.saltycrane.com/blog/2009/11/trying-out-retry-decorator-python/\n    original from: http://wiki.python.org/moin/PythonDecoratorLibrary#Retry\n\n    :param ex: the exception to check. may be a tuple of exceptions to check\n    :param tries: number of times to try (not retry) before giving up\n    :param delay: initial delay between retries in seconds.\n        A random 0-5s will be added to this number to stagger calls.\n    :param backoff: backoff multiplier e.g. value of 2 will double the delay each retry\n    :param logger: logger to use. If None, print\n    \"\"\"\n    def deco_retry(func):\n        \"\"\"@retry(arg[, ...]) -> true decorator\"\"\"\n\n        @wraps(func)\n        def f_retry(*args, **kwargs):\n            \"\"\"true decorator -> decorated function\"\"\"\n            mtries, mdelay = tries, delay\n            while mtries > 1:\n                try:\n                    return func(*args, **kwargs)\n                except ex as error:\n                    sleeping = mdelay + randint(0, 5)\n                    msg = \"%s, Retrying in %d seconds...\" % (str(error), sleeping)\n                    if logger:\n                        logger.warning(msg)\n                    sleep(sleeping)\n                    mtries -= 1\n                    mdelay *= backoff\n            return func(*args, **kwargs)\n\n        return f_retry  # true decorator\n\n    return deco_retry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, package_keyname):\n    manager = ordering.OrderingManager(env.client)\n    table = formatting.Table(COLUMNS)\n\n    locations = manager.package_locations(package_keyname)\n    for region in locations:\n        for datacenter in region['locations']:\n            table.add_row([\n                datacenter['location']['id'],\n                datacenter['location']['name'],\n                region['description'],\n                region['keyname']\n            ])\n    env.fout(table)", "response": "List Datacenters a package can be ordered in."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, abuse, address1, address2, city, company, country, firstname,\n        lastname, postal, public, state):\n    \"\"\"Edit the RWhois data on the account.\"\"\"\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    update = {\n        'abuse_email': abuse,\n        'address1': address1,\n        'address2': address2,\n        'company_name': company,\n        'city': city,\n        'country': country,\n        'first_name': firstname,\n        'last_name': lastname,\n        'postal_code': postal,\n        'state': state,\n        'private_residence': public,\n    }\n\n    if public is True:\n        update['private_residence'] = False\n    elif public is False:\n        update['private_residence'] = True\n\n    check = [x for x in update.values() if x is not None]\n    if not check:\n        raise exceptions.CLIAbort(\n            \"You must specify at least one field to update.\")\n\n    mgr.edit_rwhois(**update)", "response": "Edit the RWhois data on the account."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_create_args(client, args):\n    data = {}\n\n    if args.get('quantity'):\n        data['quantity'] = int(args.get('quantity'))\n    if args.get('postinstall'):\n        data['provisionScripts'] = [args.get('postinstall')]\n    if args.get('complex_type'):\n        data['complexType'] = args.get('complex_type')\n\n    if args.get('fqdn'):\n        servers = []\n        for name in args.get('fqdn'):\n            fqdn = name.split(\".\", 1)\n            servers.append({'hostname': fqdn[0], 'domain': fqdn[1]})\n        data['hardware'] = servers\n\n    if args.get('image'):\n        if args.get('image').isdigit():\n            image_mgr = ImageManager(client)\n            image_details = image_mgr.get_image(args.get('image'), mask=\"id,globalIdentifier\")\n            data['imageTemplateGlobalIdentifier'] = image_details['globalIdentifier']\n        else:\n            data['imageTemplateGlobalIdentifier'] = args['image']\n\n    userdata = None\n    if args.get('userdata'):\n        userdata = args['userdata']\n    elif args.get('userfile'):\n        with open(args['userfile'], 'r') as userfile:\n            userdata = userfile.read()\n    if userdata:\n        for hardware in data['hardware']:\n            hardware['userData'] = [{'value': userdata}]\n\n    # Get the SSH keys\n    if args.get('key'):\n        keys = []\n        for key in args.get('key'):\n            resolver = SshKeyManager(client).resolve_ids\n            key_id = helpers.resolve_id(resolver, key, 'SshKey')\n            keys.append(key_id)\n        data['sshKeys'] = keys\n\n    return data", "response": "Converts CLI arguments to args for VSManager. create_instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nviewing and Order a quote.", "response": "def cli(env, quote, **args):\n    \"\"\"View and Order a quote\n\n    \\f\n    :note:\n        The hostname and domain are split out from the fully qualified domain name.\n\n        If you want to order multiple servers, you need to specify each FQDN. Postinstall, userdata, and\n        sshkeys are applied to all servers in an order.\n\n    ::\n\n        slcli order quote 12345 --fqdn testing.tester.com \\\\\n            --complex-type SoftLayer_Container_Product_Order_Virtual_Guest -k sshKeyNameLabel\\\\\n            -i https://domain.com/runthis.sh --userdata DataGoesHere\n\n    \"\"\"\n    table = formatting.Table([\n        'Id', 'Name', 'Created', 'Expiration', 'Status'\n    ])\n    create_args = _parse_create_args(env.client, args)\n\n    manager = ordering.OrderingManager(env.client)\n    quote_details = manager.get_quote_details(quote)\n\n    package = quote_details['order']['items'][0]['package']\n    create_args['packageId'] = package['id']\n\n    if args.get('verify'):\n        result = manager.verify_quote(quote, create_args)\n        verify_table = formatting.Table(['keyName', 'description', 'cost'])\n        verify_table.align['keyName'] = 'l'\n        verify_table.align['description'] = 'l'\n        for price in result['prices']:\n            cost_key = 'hourlyRecurringFee' if result['useHourlyPricing'] is True else 'recurringFee'\n            verify_table.add_row([\n                price['item']['keyName'],\n                price['item']['description'],\n                price[cost_key] if cost_key in price else formatting.blank()\n            ])\n        env.fout(verify_table)\n    else:\n        result = manager.order_quote(quote, create_args)\n        table = formatting.KeyValueTable(['name', 'value'])\n        table.align['name'] = 'r'\n        table.align['value'] = 'l'\n        table.add_row(['id', result['orderId']])\n        table.add_row(['created', result['orderDate']])\n        table.add_row(['status', result['placedOrder']['status']])\n        env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nauthorizing hosts to Block Storage Volumes ova", "response": "def authorize_host_to_volume(self, volume_id,\n                                 hardware_ids=None,\n                                 virtual_guest_ids=None,\n                                 ip_address_ids=None,\n                                 **kwargs):\n        \"\"\"Authorizes hosts to Block Storage Volumes\n\n        :param volume_id: The Block volume to authorize hosts to\n        :param hardware_ids: A List of SoftLayer_Hardware ids\n        :param virtual_guest_ids: A List of SoftLayer_Virtual_Guest ids\n        :param ip_address_ids: A List of SoftLayer_Network_Subnet_IpAddress ids\n        :return: Returns an array of\n                SoftLayer_Network_Storage_Allowed_Host objects\n                which now have access to the given Block volume\n        \"\"\"\n        host_templates = []\n        storage_utils.populate_host_templates(host_templates,\n                                              hardware_ids,\n                                              virtual_guest_ids,\n                                              ip_address_ids,\n                                              None)\n\n        return self.client.call('Network_Storage', 'allowAccessFromHostList',\n                                host_templates, id=volume_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplaces an order for a replicant block volume.", "response": "def order_replicant_volume(self, volume_id, snapshot_schedule,\n                               location, tier=None, os_type=None):\n        \"\"\"Places an order for a replicant block volume.\n\n        :param volume_id: The ID of the primary volume to be replicated\n        :param snapshot_schedule: The primary volume's snapshot\n                                  schedule to use for replication\n        :param location: The location for the ordered replicant volume\n        :param tier: The tier (IOPS per GB) of the primary volume\n        :param os_type: The OS type of the primary volume\n        :return: Returns a SoftLayer_Container_Product_Order_Receipt\n        \"\"\"\n\n        block_mask = 'billingItem[activeChildren,hourlyFlag],'\\\n                     'storageTierLevel,osType,staasVersion,'\\\n                     'hasEncryptionAtRest,snapshotCapacityGb,schedules,'\\\n                     'intervalSchedule,hourlySchedule,dailySchedule,'\\\n                     'weeklySchedule,storageType[keyName],provisionedIops'\n        block_volume = self.get_block_volume_details(volume_id,\n                                                     mask=block_mask)\n\n        if os_type is None:\n            if isinstance(utils.lookup(block_volume, 'osType', 'keyName'),\n                          str):\n                os_type = block_volume['osType']['keyName']\n            else:\n                raise exceptions.SoftLayerError(\n                    \"Cannot find primary volume's os-type \"\n                    \"automatically; must specify manually\")\n\n        order = storage_utils.prepare_replicant_order_object(\n            self, snapshot_schedule, location, tier, block_volume, 'block'\n        )\n\n        order['osFormatType'] = {'keyName': os_type}\n\n        return self.client.call('Product_Order', 'placeOrder', order)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef order_duplicate_volume(self, origin_volume_id, origin_snapshot_id=None,\n                               duplicate_size=None, duplicate_iops=None,\n                               duplicate_tier_level=None,\n                               duplicate_snapshot_size=None,\n                               hourly_billing_flag=False):\n        \"\"\"Places an order for a duplicate block volume.\n\n        :param origin_volume_id: The ID of the origin volume to be duplicated\n        :param origin_snapshot_id: Origin snapshot ID to use for duplication\n        :param duplicate_size: Size/capacity for the duplicate volume\n        :param duplicate_iops: The IOPS per GB for the duplicate volume\n        :param duplicate_tier_level: Tier level for the duplicate volume\n        :param duplicate_snapshot_size: Snapshot space size for the duplicate\n        :param hourly_billing_flag: Billing type, monthly (False)\n            or hourly (True), default to monthly.\n        :return: Returns a SoftLayer_Container_Product_Order_Receipt\n        \"\"\"\n\n        block_mask = 'id,billingItem[location,hourlyFlag],snapshotCapacityGb,'\\\n                     'storageType[keyName],capacityGb,originalVolumeSize,'\\\n                     'provisionedIops,storageTierLevel,osType[keyName],'\\\n                     'staasVersion,hasEncryptionAtRest'\n        origin_volume = self.get_block_volume_details(origin_volume_id,\n                                                      mask=block_mask)\n\n        if isinstance(utils.lookup(origin_volume, 'osType', 'keyName'), str):\n            os_type = origin_volume['osType']['keyName']\n        else:\n            raise exceptions.SoftLayerError(\n                \"Cannot find origin volume's os-type\")\n\n        order = storage_utils.prepare_duplicate_order_object(\n            self, origin_volume, duplicate_iops, duplicate_tier_level,\n            duplicate_size, duplicate_snapshot_size, 'block',\n            hourly_billing_flag\n        )\n\n        order['osFormatType'] = {'keyName': os_type}\n\n        if origin_snapshot_id is not None:\n            order['duplicateOriginSnapshotId'] = origin_snapshot_id\n\n        return self.client.call('Product_Order', 'placeOrder', order)", "response": "Places an order for a duplicate block volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplace an order for modifying an existing block volume.", "response": "def order_modified_volume(self, volume_id, new_size=None, new_iops=None, new_tier_level=None):\n        \"\"\"Places an order for modifying an existing block volume.\n\n        :param volume_id: The ID of the volume to be modified\n        :param new_size: The new size/capacity for the volume\n        :param new_iops: The new IOPS for the volume\n        :param new_tier_level: The new tier level for the volume\n        :return: Returns a SoftLayer_Container_Product_Order_Receipt\n        \"\"\"\n\n        mask_items = [\n            'id',\n            'billingItem',\n            'storageType[keyName]',\n            'capacityGb',\n            'provisionedIops',\n            'storageTierLevel',\n            'staasVersion',\n            'hasEncryptionAtRest',\n        ]\n        block_mask = ','.join(mask_items)\n        volume = self.get_block_volume_details(volume_id, mask=block_mask)\n\n        order = storage_utils.prepare_modify_order_object(\n            self, volume, new_iops, new_tier_level, new_size\n        )\n\n        return self.client.call('Product_Order', 'placeOrder', order)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplaces an order for a block volume.", "response": "def order_block_volume(self, storage_type, location, size, os_type,\n                           iops=None, tier_level=None, snapshot_size=None,\n                           service_offering='storage_as_a_service',\n                           hourly_billing_flag=False):\n        \"\"\"Places an order for a block volume.\n\n        :param storage_type: 'performance' or 'endurance'\n        :param location: Datacenter in which to order iSCSI volume\n        :param size: Size of the desired volume, in GB\n        :param os_type: OS Type to use for volume alignment, see help for list\n        :param iops: Number of IOPs for a \"Performance\" order\n        :param tier_level: Tier level to use for an \"Endurance\" order\n        :param snapshot_size: The size of optional snapshot space,\n            if snapshot space should also be ordered (None if not ordered)\n        :param service_offering: Requested offering package to use in the order\n            ('storage_as_a_service', 'enterprise', or 'performance')\n        :param hourly_billing_flag: Billing type, monthly (False)\n            or hourly (True), default to monthly.\n        \"\"\"\n        order = storage_utils.prepare_volume_order_object(\n            self, storage_type, location, size, iops, tier_level,\n            snapshot_size, service_offering, 'block', hourly_billing_flag\n        )\n\n        order['osFormatType'] = {'keyName': os_type}\n\n        return self.client.call('Product_Order', 'placeOrder', order)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a snapshot on the given block volume.", "response": "def create_snapshot(self, volume_id, notes='', **kwargs):\n        \"\"\"Creates a snapshot on the given block volume.\n\n        :param integer volume_id: The id of the volume\n        :param string notes: The notes or \"name\" to assign the snapshot\n        :return: Returns the id of the new snapshot\n        \"\"\"\n\n        return self.client.call('Network_Storage', 'createSnapshot',\n                                notes, id=volume_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplace an order for snapshot space for the given block volume.", "response": "def order_snapshot_space(self, volume_id, capacity, tier,\n                             upgrade, **kwargs):\n        \"\"\"Orders snapshot space for the given block volume.\n\n        :param integer volume_id: The id of the volume\n        :param integer capacity: The capacity to order, in GB\n        :param float tier: The tier level of the block volume, in IOPS per GB\n        :param boolean upgrade: Flag to indicate if this order is an upgrade\n        :return: Returns a SoftLayer_Container_Product_Order_Receipt\n        \"\"\"\n        block_mask = 'id,billingItem[location,hourlyFlag],'\\\n            'storageType[keyName],storageTierLevel,provisionedIops,'\\\n            'staasVersion,hasEncryptionAtRest'\n        block_volume = self.get_block_volume_details(volume_id,\n                                                     mask=block_mask,\n                                                     **kwargs)\n\n        order = storage_utils.prepare_snapshot_order_object(\n            self, block_volume, capacity, tier, upgrade)\n\n        return self.client.call('Product_Order', 'placeOrder', order)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenable snapshots for a specific block volume at a given schedule", "response": "def enable_snapshots(self, volume_id, schedule_type, retention_count,\n                         minute, hour, day_of_week, **kwargs):\n        \"\"\"Enables snapshots for a specific block volume at a given schedule\n\n        :param integer volume_id: The id of the volume\n        :param string schedule_type: 'HOURLY'|'DAILY'|'WEEKLY'\n        :param integer retention_count: Number of snapshots to be kept\n        :param integer minute: Minute when to take snapshot\n        :param integer hour: Hour when to take snapshot\n        :param string day_of_week: Day when to take snapshot\n        :return: Returns whether successfully scheduled or not\n        \"\"\"\n\n        return self.client.call('Network_Storage', 'enableSnapshots',\n                                schedule_type,\n                                retention_count,\n                                minute,\n                                hour,\n                                day_of_week,\n                                id=volume_id,\n                                **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisable snapshots for a specific block volume at a given schedule", "response": "def disable_snapshots(self, volume_id, schedule_type):\n        \"\"\"Disables snapshots for a specific block volume at a given schedule\n\n        :param integer volume_id: The id of the volume\n        :param string schedule_type: 'HOURLY'|'DAILY'|'WEEKLY'\n        :return: Returns whether successfully disabled or not\n        \"\"\"\n\n        return self.client.call('Network_Storage', 'disableSnapshots',\n                                schedule_type, id=volume_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_volume_schedules(self, volume_id):\n        volume_detail = self.client.call(\n            'Network_Storage',\n            'getObject',\n            id=volume_id,\n            mask='schedules[type,properties[type]]')\n\n        return utils.lookup(volume_detail, 'schedules')", "response": "Lists the schedules assigned to a given volume"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring a specific volume from a snapshot", "response": "def restore_from_snapshot(self, volume_id, snapshot_id):\n        \"\"\"Restores a specific volume from a snapshot\n\n        :param integer volume_id: The id of the volume\n        :param integer snapshot_id: The id of the restore point\n        :return: Returns whether succesfully restored or not\n        \"\"\"\n\n        return self.client.call('Network_Storage', 'restoreFromSnapshot',\n                                snapshot_id, id=volume_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncancel the given block storage volume.", "response": "def cancel_block_volume(self, volume_id,\n                            reason='No longer needed',\n                            immediate=False):\n        \"\"\"Cancels the given block storage volume.\n\n        :param integer volume_id: The volume ID\n        :param string reason: The reason for cancellation\n        :param boolean immediate_flag: Cancel immediately or on anniversary date\n        \"\"\"\n        block_volume = self.get_block_volume_details(\n            volume_id,\n            mask='mask[id,billingItem[id,hourlyFlag]]')\n\n        if 'billingItem' not in block_volume:\n            raise exceptions.SoftLayerError(\"Block Storage was already cancelled\")\n\n        billing_item_id = block_volume['billingItem']['id']\n\n        if utils.lookup(block_volume, 'billingItem', 'hourlyFlag'):\n            immediate = True\n\n        return self.client['Billing_Item'].cancelItem(\n            immediate,\n            True,\n            reason,\n            id=billing_item_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the password for an access host", "response": "def set_credential_password(self, access_id, password):\n        \"\"\"Sets the password for an access host\n\n        :param integer access_id: id of the access host\n        :param string password: password to  set\n        \"\"\"\n\n        return self.client.call('Network_Storage_Allowed_Host', 'setCredentialPassword',\n                                password, id=access_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_or_update_lun_id(self, volume_id, lun_id):\n        return self.client.call('Network_Storage', 'createOrUpdateLunId',\n                                lun_id, id=volume_id)", "response": "Create or update the LUN ID on a volume."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all packages of a certain type.", "response": "def get_packages_of_type(self, package_types, mask=None):\n        \"\"\"Get packages that match a certain type.\n\n        Each ordering package has a type, so return all packages that match\n        the types we are looking for\n\n        :param list package_types: List of strings representing the package\n                                   type keynames we are interested in.\n        :param string mask: Mask to specify the properties we want to retrieve\n        \"\"\"\n\n        _filter = {\n            'type': {\n                'keyName': {\n                    'operation': 'in',\n                    'options': [\n                        {'name': 'data',\n                         'value': package_types}\n                    ],\n                },\n            },\n        }\n\n        packages = self.package_svc.getAllObjects(mask=mask, filter=_filter)\n        packages = self.filter_outlet_packages(packages)\n        return packages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_outlet_packages(packages):\n\n        non_outlet_packages = []\n\n        for package in packages:\n            if all(['OUTLET' not in package.get('description', '').upper(),\n                    'OUTLET' not in package.get('name', '').upper()]):\n                non_outlet_packages.append(package)\n\n        return non_outlet_packages", "response": "Remove packages that are not part of the OUTLET."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_only_active_packages(packages):\n\n        active_packages = []\n\n        for package in packages:\n            if package['isActive']:\n                active_packages.append(package)\n\n        return active_packages", "response": "Return only active packages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_package_by_type(self, package_type, mask=None):\n        packages = self.get_packages_of_type([package_type], mask)\n        if len(packages) == 0:\n            return None\n        else:\n            return packages.pop()", "response": "Get a single package of a given type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the product package ID of a given type.", "response": "def get_package_id_by_type(self, package_type):\n        \"\"\"Return the package ID of a Product Package with a given type.\n\n        :param string package_type: representing the package type key name we are interested in\n        :raises ValueError: when no package of the given type is found\n        \"\"\"\n\n        mask = \"mask[id, name, description, isActive, type[keyName]]\"\n        package = self.get_package_by_type(package_type, mask)\n        if package:\n            return package['id']\n        else:\n            raise ValueError(\"No package found for type: \" + package_type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a list of active quotes.", "response": "def get_quotes(self):\n        \"\"\"Retrieve a list of active quotes.\n\n        :returns: a list of SoftLayer_Billing_Order_Quote\n        \"\"\"\n        mask = \"mask[order[id,items[id,package[id,keyName]]]]\"\n        quotes = self.client['Account'].getActiveQuotes(mask=mask)\n        return quotes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves quote details. :param quote_id: ID number of target quote", "response": "def get_quote_details(self, quote_id):\n        \"\"\"Retrieve quote details.\n\n        :param quote_id: ID number of target quote\n        \"\"\"\n\n        mask = \"mask[order[id,items[package[id,keyName]]]]\"\n        quote = self.client['Billing_Order_Quote'].getObject(id=quote_id, mask=mask)\n        return quote"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_order_container(self, quote_id):\n\n        quote = self.client['Billing_Order_Quote']\n        container = quote.getRecalculatedOrderContainer(id=quote_id)\n        return container", "response": "Generate an order container from a quote object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_order_template(self, quote_id, extra, quantity=1):\n\n        if not isinstance(extra, dict):\n            raise ValueError(\"extra is not formatted properly\")\n\n        container = self.get_order_container(quote_id)\n\n        container['quantity'] = quantity\n        for key in extra.keys():\n            container[key] = extra[key]\n\n        return container", "response": "Generate a complete order template."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify that a quote order is valid.", "response": "def verify_quote(self, quote_id, extra):\n        \"\"\"Verifies that a quote order is valid.\n\n        ::\n\n            extras = {\n                'hardware': {'hostname': 'test', 'domain': 'testing.com'},\n                'quantity': 2\n            }\n            manager = ordering.OrderingManager(env.client)\n            result = manager.verify_quote(12345, extras)\n\n\n        :param int quote_id: ID for the target quote\n        :param dictionary extra: Overrides for the defaults of SoftLayer_Container_Product_Order\n        :param int quantity: Quantity to override default\n        \"\"\"\n        container = self.generate_order_template(quote_id, extra)\n        clean_container = {}\n\n        # There are a few fields that wil cause exceptions in the XML endpoing if you send in ''\n        # reservedCapacityId and hostId specifically. But we clean all just to be safe.\n        # This for some reason is only a problem on verify_quote.\n        for key in container.keys():\n            if container.get(key) != '':\n                clean_container[key] = container[key]\n\n        return self.client.call('SoftLayer_Billing_Order_Quote', 'verifyOrder', clean_container, id=quote_id)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef order_quote(self, quote_id, extra):\n\n        container = self.generate_order_template(quote_id, extra)\n        return self.client.call('SoftLayer_Billing_Order_Quote', 'placeOrder', container, id=quote_id)", "response": "Places an order using a quote."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_package_by_key(self, package_keyname, mask=None):\n        _filter = {'keyName': {'operation': package_keyname}}\n\n        packages = self.package_svc.getAllObjects(mask=mask, filter=_filter)\n        if len(packages) == 0:\n            raise exceptions.SoftLayerError(\"Package {} does not exist\".format(package_keyname))\n\n        return packages.pop()", "response": "Get a single package with a given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_categories(self, package_keyname, **kwargs):\n        get_kwargs = {}\n        get_kwargs['mask'] = kwargs.get('mask', CATEGORY_MASK)\n\n        if 'filter' in kwargs:\n            get_kwargs['filter'] = kwargs['filter']\n\n        package = self.get_package_by_key(package_keyname, mask='id')\n        categories = self.package_svc.getConfiguration(id=package['id'], **get_kwargs)\n        return categories", "response": "List the categories associated with the given package."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the items for the given package.", "response": "def list_items(self, package_keyname, **kwargs):\n        \"\"\"List the items for the given package.\n\n        :param str package_keyname: The package for which to get the items.\n        :returns: List of items in the package\n\n        \"\"\"\n        get_kwargs = {}\n        get_kwargs['mask'] = kwargs.get('mask', ITEM_MASK)\n\n        if 'filter' in kwargs:\n            get_kwargs['filter'] = kwargs['filter']\n\n        package = self.get_package_by_key(package_keyname, mask='id')\n        items = self.package_svc.getItems(id=package['id'], **get_kwargs)\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_packages(self, **kwargs):\n        get_kwargs = {}\n        get_kwargs['mask'] = kwargs.get('mask', PACKAGE_MASK)\n\n        if 'filter' in kwargs:\n            get_kwargs['filter'] = kwargs['filter']\n\n        packages = self.package_svc.getAllObjects(**get_kwargs)\n\n        return [package for package in packages if package['isActive']]", "response": "List active packages.\n\n        :returns: List of active packages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the active presets for the given package.", "response": "def list_presets(self, package_keyname, **kwargs):\n        \"\"\"Gets active presets for the given package.\n\n        :param str package_keyname: The package for which to get presets\n        :returns: A list of package presets that can be used for ordering\n\n        \"\"\"\n        get_kwargs = {}\n        get_kwargs['mask'] = kwargs.get('mask', PRESET_MASK)\n\n        if 'filter' in kwargs:\n            get_kwargs['filter'] = kwargs['filter']\n\n        package = self.get_package_by_key(package_keyname, mask='id')\n        acc_presets = self.package_svc.getAccountRestrictedActivePresets(id=package['id'], **get_kwargs)\n        active_presets = self.package_svc.getActivePresets(id=package['id'], **get_kwargs)\n        return active_presets + acc_presets"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_preset_by_key(self, package_keyname, preset_keyname, mask=None):\n        preset_operation = '_= %s' % preset_keyname\n        _filter = {\n            'activePresets': {\n                'keyName': {\n                    'operation': preset_operation\n                }\n            },\n            'accountRestrictedActivePresets': {\n                'keyName': {\n                    'operation': preset_operation\n                }\n            }\n        }\n\n        presets = self.list_presets(package_keyname, mask=mask, filter=_filter)\n\n        if len(presets) == 0:\n            raise exceptions.SoftLayerError(\n                \"Preset {} does not exist in package {}\".format(preset_keyname,\n                                                                package_keyname))\n\n        return presets[0]", "response": "Gets a single preset with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a list of item keynames to a list of price IDs. This function is used to convert a list of item keynames into a list of price IDs that are used in the Product_Order verifyOrder() and placeOrder() functions. :param str package_keyname: The package associated with the prices :param list item_keynames: A list of item keyname strings :param str core: preset guest core capacity. :returns: A list of price IDs associated with the given item keynames in the given package", "response": "def get_price_id_list(self, package_keyname, item_keynames, core=None):\n        \"\"\"Converts a list of item keynames to a list of price IDs.\n\n        This function is used to convert a list of item keynames into\n        a list of price IDs that are used in the Product_Order verifyOrder()\n        and placeOrder() functions.\n\n        :param str package_keyname: The package associated with the prices\n        :param list item_keynames: A list of item keyname strings\n        :param str core: preset guest core capacity.\n        :returns: A list of price IDs associated with the given item\n                  keynames in the given package\n\n        \"\"\"\n        mask = 'id, itemCategory, keyName, prices[categories]'\n        items = self.list_items(package_keyname, mask=mask)\n\n        prices = []\n        category_dict = {\"gpu0\": -1, \"pcie_slot0\": -1}\n\n        for item_keyname in item_keynames:\n            try:\n                # Need to find the item in the package that has a matching\n                # keyName with the current item we are searching for\n                matching_item = [i for i in items\n                                 if i['keyName'] == item_keyname][0]\n            except IndexError:\n                raise exceptions.SoftLayerError(\n                    \"Item {} does not exist for package {}\".format(item_keyname,\n                                                                   package_keyname))\n\n            # we want to get the price ID that has no location attached to it,\n            # because that is the most generic price. verifyOrder/placeOrder\n            # can take that ID and create the proper price for us in the location\n            # in which the order is made\n            item_category = matching_item['itemCategory']['categoryCode']\n            if item_category not in category_dict:\n                price_id = self.get_item_price_id(core, matching_item['prices'])\n            else:\n                # GPU and PCIe items has two generic prices and they are added to the list\n                # according to the number of items in the order.\n                category_dict[item_category] += 1\n                category_code = item_category[:-1] + str(category_dict[item_category])\n                price_id = [p['id'] for p in matching_item['prices']\n                            if not p['locationGroupId']\n                            and p['categories'][0]['categoryCode'] == category_code][0]\n\n            prices.append(price_id)\n\n        return prices"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_item_price_id(core, prices):\n        price_id = None\n        for price in prices:\n            if not price['locationGroupId']:\n                capacity_min = int(price.get('capacityRestrictionMinimum', -1))\n                capacity_max = int(price.get('capacityRestrictionMaximum', -1))\n                # return first match if no restirction, or no core to check\n                if capacity_min == -1 or core is None:\n                    price_id = price['id']\n                # this check is mostly to work nicely with preset configs\n                elif capacity_min <= int(core) <= capacity_max:\n                    price_id = price['id']\n        return price_id", "response": "get item price id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget preset item prices.", "response": "def get_preset_prices(self, preset):\n        \"\"\"Get preset item prices.\n\n        Retrieve a SoftLayer_Product_Package_Preset record.\n\n        :param int preset: preset identifier.\n        :returns: A list of price IDs associated with the given preset_id.\n\n        \"\"\"\n        mask = 'mask[prices[item]]'\n\n        prices = self.package_preset.getObject(id=preset, mask=mask)\n        return prices"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_item_prices(self, package_id):\n        mask = 'mask[pricingLocationGroup[locations]]'\n\n        prices = self.package_svc.getItemPrices(id=package_id, mask=mask)\n        return prices", "response": "Get a list of item prices associated with a given package."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies an order with the given package and items.", "response": "def verify_order(self, package_keyname, location, item_keynames, complex_type=None,\n                     hourly=True, preset_keyname=None, extras=None, quantity=1):\n        \"\"\"Verifies an order with the given package and prices.\n\n        This function takes in parameters needed for an order and verifies the order\n        to ensure the given items are compatible with the given package.\n\n        :param str package_keyname: The keyname for the package being ordered\n        :param str location: The datacenter location string for ordering (Ex: DALLAS13)\n        :param list item_keynames: The list of item keyname strings to order. To see list of\n                                   possible keynames for a package, use list_items()\n                                   (or `slcli order item-list`)\n        :param str complex_type: The complex type to send with the order. Typically begins\n                                 with `SoftLayer_Container_Product_Order_`.\n        :param bool hourly: If true, uses hourly billing, otherwise uses monthly billing\n        :param string preset_keyname: If needed, specifies a preset to use for that package.\n                                      To see a list of possible keynames for a package, use\n                                      list_preset() (or `slcli order preset-list`)\n        :param dict extras: The extra data for the order in dictionary format.\n                            Example: A VSI order requires hostname and domain to be set, so\n                            extras will look like the following:\n                            'virtualGuests': [{'hostname': 'test', 'domain': 'softlayer.com'}]}\n        :param int quantity: The number of resources to order\n\n        \"\"\"\n        order = self.generate_order(package_keyname, location, item_keynames,\n                                    complex_type=complex_type, hourly=hourly,\n                                    preset_keyname=preset_keyname,\n                                    extras=extras, quantity=quantity)\n        return self.order_svc.verifyOrder(order)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef place_order(self, package_keyname, location, item_keynames, complex_type=None,\n                    hourly=True, preset_keyname=None, extras=None, quantity=1):\n        \"\"\"Places an order with the given package and prices.\n\n        This function takes in parameters needed for an order and places the order.\n\n        :param str package_keyname: The keyname for the package being ordered\n        :param str location: The datacenter location string for ordering (Ex: DALLAS13)\n        :param list item_keynames: The list of item keyname strings to order. To see list of\n                                   possible keynames for a package, use list_items()\n                                   (or `slcli order item-list`)\n        :param str complex_type: The complex type to send with the order. Typically begins\n                                 with `SoftLayer_Container_Product_Order_`.\n        :param bool hourly: If true, uses hourly billing, otherwise uses monthly billing\n        :param string preset_keyname: If needed, specifies a preset to use for that package.\n                                      To see a list of possible keynames for a package, use\n                                      list_preset() (or `slcli order preset-list`)\n        :param dict extras: The extra data for the order in dictionary format.\n                            Example: A VSI order requires hostname and domain to be set, so\n                            extras will look like the following:\n                            {'virtualGuests': [{'hostname': 'test', domain': 'softlayer.com'}]}\n        :param int quantity: The number of resources to order\n\n        \"\"\"\n        order = self.generate_order(package_keyname, location, item_keynames,\n                                    complex_type=complex_type, hourly=hourly,\n                                    preset_keyname=preset_keyname,\n                                    extras=extras, quantity=quantity)\n        return self.order_svc.placeOrder(order)", "response": "Places an order with the given package and prices."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplaces a quote with the given package and prices.", "response": "def place_quote(self, package_keyname, location, item_keynames, complex_type=None,\n                    preset_keyname=None, extras=None, quantity=1, quote_name=None, send_email=False):\n        \"\"\"Place a quote with the given package and prices.\n\n        This function takes in parameters needed for an order and places the quote.\n\n        :param str package_keyname: The keyname for the package being ordered\n        :param str location: The datacenter location string for ordering (Ex: DALLAS13)\n        :param list item_keynames: The list of item keyname strings to order. To see list of\n                                   possible keynames for a package, use list_items()\n                                   (or `slcli order item-list`)\n        :param str complex_type: The complex type to send with the order. Typically begins\n                                 with `SoftLayer_Container_Product_Order_`.\n        :param string preset_keyname: If needed, specifies a preset to use for that package.\n                                      To see a list of possible keynames for a package, use\n                                      list_preset() (or `slcli order preset-list`)\n        :param dict extras: The extra data for the order in dictionary format.\n                            Example: A VSI order requires hostname and domain to be set, so\n                            extras will look like the following:\n                            {'virtualGuests': [{'hostname': 'test', domain': 'softlayer.com'}]}\n        :param int quantity: The number of resources to order\n        :param string quote_name: A custom name to be assigned to the quote (optional).\n        :param bool send_email: This flag indicates that the quote should be sent to the email\n                                address associated with the account or order.\n        \"\"\"\n        order = self.generate_order(package_keyname, location, item_keynames, complex_type=complex_type,\n                                    hourly=False, preset_keyname=preset_keyname, extras=extras, quantity=quantity)\n\n        order['quoteName'] = quote_name\n        order['sendQuoteEmailFlag'] = send_email\n\n        return self.order_svc.placeQuote(order)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an order with the given package and prices. This function takes in parameters needed for an order and generates an order dictionary. This dictionary can then be used in either verify or placeOrder(). :param str package_keyname: The keyname for the package being ordered :param str location: The datacenter location string for ordering (Ex: DALLAS13) :param list item_keynames: The list of item keyname strings to order. To see list of possible keynames for a package, use list_items() (or `slcli order item-list`) :param str complex_type: The complex type to send with the order. Typically begins with `SoftLayer_Container_Product_Order_`. :param bool hourly: If true, uses hourly billing, otherwise uses monthly billing :param string preset_keyname: If needed, specifies a preset to use for that package. To see a list of possible keynames for a package, use list_preset() (or `slcli order preset-list`) :param dict extras: The extra data for the order in dictionary format. Example: A VSI order requires hostname and domain to be set, so extras will look like the following: {'virtualGuests': [{'hostname': 'test', 'domain': 'softlayer.com'}]} :param int quantity: The number of resources to order", "response": "def generate_order(self, package_keyname, location, item_keynames, complex_type=None,\n                       hourly=True, preset_keyname=None, extras=None, quantity=1):\n        \"\"\"Generates an order with the given package and prices.\n\n        This function takes in parameters needed for an order and generates an order\n        dictionary. This dictionary can then be used in either verify or placeOrder().\n\n        :param str package_keyname: The keyname for the package being ordered\n        :param str location: The datacenter location string for ordering (Ex: DALLAS13)\n        :param list item_keynames: The list of item keyname strings to order. To see list of\n                                   possible keynames for a package, use list_items()\n                                   (or `slcli order item-list`)\n        :param str complex_type: The complex type to send with the order. Typically begins\n                                 with `SoftLayer_Container_Product_Order_`.\n        :param bool hourly: If true, uses hourly billing, otherwise uses monthly billing\n        :param string preset_keyname: If needed, specifies a preset to use for that package.\n                                      To see a list of possible keynames for a package, use\n                                      list_preset() (or `slcli order preset-list`)\n        :param dict extras: The extra data for the order in dictionary format.\n                            Example: A VSI order requires hostname and domain to be set, so\n                            extras will look like the following:\n                            {'virtualGuests': [{'hostname': 'test', 'domain': 'softlayer.com'}]}\n        :param int quantity: The number of resources to order\n\n        \"\"\"\n        container = {}\n        order = {}\n        extras = extras or {}\n\n        package = self.get_package_by_key(package_keyname, mask='id')\n\n        # if there was extra data given for the order, add it to the order\n        # example: VSIs require hostname and domain set on the order, so\n        # extras will be {'virtualGuests': [{'hostname': 'test',\n        #                                    'domain': 'softlayer.com'}]}\n        order.update(extras)\n        order['packageId'] = package['id']\n        order['quantity'] = quantity\n        order['location'] = self.get_location_id(location)\n        order['useHourlyPricing'] = hourly\n\n        preset_core = None\n        if preset_keyname:\n            preset_id = self.get_preset_by_key(package_keyname, preset_keyname)['id']\n            preset_items = self.get_preset_prices(preset_id)\n            for item in preset_items['prices']:\n                if item['item']['itemCategory']['categoryCode'] == \"guest_core\":\n                    preset_core = item['item']['capacity']\n            order['presetId'] = preset_id\n\n        if not complex_type:\n            raise exceptions.SoftLayerError(\"A complex type must be specified with the order\")\n        order['complexType'] = complex_type\n\n        price_ids = self.get_price_id_list(package_keyname, item_keynames, preset_core)\n        order['prices'] = [{'id': price_id} for price_id in price_ids]\n\n        container['orderContainers'] = [order]\n\n        return container"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef package_locations(self, package_keyname):\n        mask = \"mask[description, keyname, locations]\"\n\n        package = self.get_package_by_key(package_keyname, mask='id')\n\n        regions = self.package_svc.getRegions(id=package['id'], mask=mask)\n        return regions", "response": "List datacenter locations for a package keyname"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_location_id(self, location):\n\n        if isinstance(location, int):\n            return location\n        mask = \"mask[id,name,regions[keyname]]\"\n        if match(r'[a-zA-Z]{3}[0-9]{2}', location) is not None:\n            search = {'name': {'operation': location}}\n        else:\n            search = {'regions': {'keyname': {'operation': location}}}\n        datacenter = self.client.call('SoftLayer_Location', 'getDatacenters', mask=mask, filter=search)\n        if len(datacenter) != 1:\n            raise exceptions.SoftLayerError(\"Unable to find location: %s\" % location)\n        return datacenter[0]['id']", "response": "Finds the location ID of a given datacenter"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a global IP address to the account.", "response": "def add_global_ip(self, version=4, test_order=False):\n        \"\"\"Adds a global IP address to the account.\n\n        :param int version: Specifies whether this is IPv4 or IPv6\n        :param bool test_order: If true, this will only verify the order.\n        \"\"\"\n        # This method is here to improve the public interface from a user's\n        # perspective since ordering a single global IP through the subnet\n        # interface is not intuitive.\n        return self.add_subnet('global', version=version,\n                               test_order=test_order)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a rule to a security group.", "response": "def add_securitygroup_rule(self, group_id, remote_ip=None,\n                               remote_group=None, direction=None,\n                               ethertype=None, port_max=None,\n                               port_min=None, protocol=None):\n        \"\"\"Add a rule to a security group\n\n        :param int group_id: The ID of the security group to add this rule to\n        :param str remote_ip: The remote IP or CIDR to enforce the rule on\n        :param int remote_group: The remote security group ID to enforce\n                                 the rule on\n        :param str direction: The direction to enforce (egress or ingress)\n        :param str ethertype: The ethertype to enforce (IPv4 or IPv6)\n        :param int port_max: The upper port bound to enforce\n                             (icmp code if the protocol is icmp)\n        :param int port_min: The lower port bound to enforce\n                             (icmp type if the protocol is icmp)\n        :param str protocol: The protocol to enforce (icmp, udp, tcp)\n        \"\"\"\n        rule = {'direction': direction}\n        if ethertype is not None:\n            rule['ethertype'] = ethertype\n        if port_max is not None:\n            rule['portRangeMax'] = port_max\n        if port_min is not None:\n            rule['portRangeMin'] = port_min\n        if protocol is not None:\n            rule['protocol'] = protocol\n        if remote_ip is not None:\n            rule['remoteIp'] = remote_ip\n        if remote_group is not None:\n            rule['remoteGroupId'] = remote_group\n        return self.add_securitygroup_rules(group_id, [rule])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd rules to a security group", "response": "def add_securitygroup_rules(self, group_id, rules):\n        \"\"\"Add rules to a security group\n\n        :param int group_id: The ID of the security group to add the rules to\n        :param list rules: The list of rule dictionaries to add\n        \"\"\"\n        if not isinstance(rules, list):\n            raise TypeError(\"The rules provided must be a list of dictionaries\")\n        return self.security_group.addRules(rules, id=group_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a subnet to the local cache.", "response": "def add_subnet(self, subnet_type, quantity=None, vlan_id=None, version=4,\n                   test_order=False):\n        \"\"\"Orders a new subnet\n\n        :param str subnet_type: Type of subnet to add: private, public, global\n        :param int quantity: Number of IPs in the subnet\n        :param int vlan_id: VLAN id for the subnet to be placed into\n        :param int version: 4 for IPv4, 6 for IPv6\n        :param bool test_order: If true, this will only verify the order.\n        \"\"\"\n        package = self.client['Product_Package']\n        category = 'sov_sec_ip_addresses_priv'\n        desc = ''\n        if version == 4:\n            if subnet_type == 'global':\n                quantity = 0\n                category = 'global_ipv4'\n            elif subnet_type == 'public':\n                category = 'sov_sec_ip_addresses_pub'\n        else:\n            category = 'static_ipv6_addresses'\n            if subnet_type == 'global':\n                quantity = 0\n                category = 'global_ipv6'\n                desc = 'Global'\n            elif subnet_type == 'public':\n                desc = 'Portable'\n\n        # In the API, every non-server item is contained within package ID 0.\n        # This means that we need to get all of the items and loop through them\n        # looking for the items we need based upon the category, quantity, and\n        # item description.\n        price_id = None\n        quantity_str = str(quantity)\n        for item in package.getItems(id=0, mask='itemCategory'):\n            category_code = utils.lookup(item, 'itemCategory', 'categoryCode')\n            if all([category_code == category,\n                    item.get('capacity') == quantity_str,\n                    version == 4 or (version == 6 and\n                                     desc in item['description'])]):\n                price_id = item['prices'][0]['id']\n                break\n\n        order = {\n            'packageId': 0,\n            'prices': [{'id': price_id}],\n            'quantity': 1,\n            # This is necessary in order for the XML-RPC endpoint to select the\n            # correct order container\n            'complexType': 'SoftLayer_Container_Product_Order_Network_Subnet',\n        }\n\n        if subnet_type != 'global':\n            order['endPointVlanId'] = vlan_id\n\n        if test_order:\n            return self.client['Product_Order'].verifyOrder(order)\n        else:\n            return self.client['Product_Order'].placeOrder(order)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assign_global_ip(self, global_ip_id, target):\n        return self.client['Network_Subnet_IpAddress_Global'].route(\n            target, id=global_ip_id)", "response": "Assigns a global IP address to a specified target."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attach_securitygroup_components(self, group_id, component_ids):\n        return self.security_group.attachNetworkComponents(component_ids,\n                                                           id=group_id)", "response": "Attaches network components to a security group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncancel the specified global IP address.", "response": "def cancel_global_ip(self, global_ip_id):\n        \"\"\"Cancels the specified global IP address.\n\n        :param int id: The ID of the global IP to be cancelled.\n        \"\"\"\n        service = self.client['Network_Subnet_IpAddress_Global']\n        ip_address = service.getObject(id=global_ip_id, mask='billingItem')\n        billing_id = ip_address['billingItem']['id']\n\n        return self.client['Billing_Item'].cancelService(id=billing_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncancels the specified subnet.", "response": "def cancel_subnet(self, subnet_id):\n        \"\"\"Cancels the specified subnet.\n\n        :param int subnet_id: The ID of the subnet to be cancelled.\n        \"\"\"\n        subnet = self.get_subnet(subnet_id, mask='id, billingItem.id')\n        if \"billingItem\" not in subnet:\n            raise exceptions.SoftLayerError(\"subnet %s can not be cancelled\"\n                                            \" \" % subnet_id)\n        billing_id = subnet['billingItem']['id']\n        return self.client['Billing_Item'].cancelService(id=billing_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a security group.", "response": "def create_securitygroup(self, name=None, description=None):\n        \"\"\"Creates a security group.\n\n        :param string name: The name of the security group\n        :param string description: The description of the security group\n        \"\"\"\n\n        create_dict = {'name': name, 'description': description}\n        return self.security_group.createObject(create_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetaches network components from a security group.", "response": "def detach_securitygroup_components(self, group_id, component_ids):\n        \"\"\"Detaches network components from a security group.\n\n        :param int group_id: The ID of the security group\n        :param list component_ids: The IDs of the network components to detach\n        \"\"\"\n        return self.security_group.detachNetworkComponents(component_ids,\n                                                           id=group_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef edit_securitygroup(self, group_id, name=None, description=None):\n        successful = False\n        obj = {}\n        if name:\n            obj['name'] = name\n        if description:\n            obj['description'] = description\n\n        if obj:\n            successful = self.security_group.editObject(obj, id=group_id)\n\n        return successful", "response": "Edit the details of a security group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef edit_securitygroup_rule(self, group_id, rule_id, remote_ip=None,\n                                remote_group=None, direction=None,\n                                ethertype=None, port_max=None,\n                                port_min=None, protocol=None):\n        \"\"\"Edit a security group rule.\n\n        :param int group_id: The ID of the security group the rule belongs to\n        :param int rule_id: The ID of the rule to edit\n        :param str remote_ip: The remote IP or CIDR to enforce the rule on\n        :param int remote_group: The remote security group ID to enforce\n                                          the rule on\n        :param str direction: The direction to enforce (egress or ingress)\n        :param str ethertype: The ethertype to enforce (IPv4 or IPv6)\n        :param str port_max: The upper port bound to enforce\n        :param str port_min: The lower port bound to enforce\n        :param str protocol: The protocol to enforce (icmp, udp, tcp)\n        \"\"\"\n        successful = False\n        obj = {}\n        if remote_ip is not None:\n            obj['remoteIp'] = remote_ip\n        if remote_group is not None:\n            obj['remoteGroupId'] = remote_group\n        if direction is not None:\n            obj['direction'] = direction\n        if ethertype is not None:\n            obj['ethertype'] = ethertype\n        if port_max is not None:\n            obj['portRangeMax'] = port_max\n        if port_min is not None:\n            obj['portRangeMin'] = port_min\n        if protocol is not None:\n            obj['protocol'] = protocol\n\n        if obj:\n            obj['id'] = rule_id\n            successful = self.security_group.editRules([obj], id=group_id)\n\n        return successful", "response": "Edit a security group rule."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ip_lookup(self, ip_address):\n        obj = self.client['Network_Subnet_IpAddress']\n        return obj.getByIpAddress(ip_address, mask='hardware, virtualGuest')", "response": "Looks up an IP address and returns network information about it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_securitygroup(self, group_id, **kwargs):\n        if 'mask' not in kwargs:\n            kwargs['mask'] = (\n                'id,'\n                'name,'\n                'description,'\n                '''rules[id, remoteIp, remoteGroupId,\n                         direction, ethertype, portRangeMin,\n                         portRangeMax, protocol, createDate, modifyDate],'''\n                '''networkComponentBindings[\n                    networkComponent[\n                        id,\n                        port,\n                        guest[\n                            id,\n                            hostname,\n                            primaryBackendIpAddress,\n                            primaryIpAddress\n                        ]\n                    ]\n                ]'''\n            )\n\n        return self.security_group.getObject(id=group_id, **kwargs)", "response": "Returns the information about the given security group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_subnet(self, subnet_id, **kwargs):\n        if 'mask' not in kwargs:\n            kwargs['mask'] = DEFAULT_SUBNET_MASK\n\n        return self.subnet.getObject(id=subnet_id, **kwargs)", "response": "Returns information about a single subnet."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary containing a large amount of information about a single VLAN.", "response": "def get_vlan(self, vlan_id):\n        \"\"\"Returns information about a single VLAN.\n\n        :param int id: The unique identifier for the VLAN\n        :returns: A dictionary containing a large amount of information about\n                  the specified VLAN.\n\n        \"\"\"\n        return self.vlan.getObject(id=vlan_id, mask=DEFAULT_GET_VLAN_MASK)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all global IP addresses on the account.", "response": "def list_global_ips(self, version=None, identifier=None, **kwargs):\n        \"\"\"Returns a list of all global IP address records on the account.\n\n        :param int version: Only returns IPs of this version (4 or 6)\n        :param string identifier: If specified, the list will only contain the\n                                  global ips matching this network identifier.\n        \"\"\"\n        if 'mask' not in kwargs:\n            mask = ['destinationIpAddress[hardware, virtualGuest]',\n                    'ipAddress']\n            kwargs['mask'] = ','.join(mask)\n\n        _filter = utils.NestedDict({})\n\n        if version:\n            ver = utils.query_filter(version)\n            _filter['globalIpRecords']['ipAddress']['subnet']['version'] = ver\n\n        if identifier:\n            subnet_filter = _filter['globalIpRecords']['ipAddress']['subnet']\n            subnet_filter['networkIdentifier'] = utils.query_filter(identifier)\n\n        kwargs['filter'] = _filter.to_dict()\n        return self.account.getGlobalIpRecords(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_subnets(self, identifier=None, datacenter=None, version=0,\n                     subnet_type=None, network_space=None, **kwargs):\n        \"\"\"Display a list of all subnets on the account.\n\n        This provides a quick overview of all subnets including information\n        about data center residence and the number of devices attached.\n\n        :param string identifier: If specified, the list will only contain the\n                                    subnet matching this network identifier.\n        :param string datacenter: If specified, the list will only contain\n                                    subnets in the specified data center.\n        :param int version: Only returns subnets of this version (4 or 6).\n        :param string subnet_type: If specified, it will only returns subnets\n                                     of this type.\n        :param string network_space: If specified, it will only returns subnets\n                                       with the given address space label.\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        \"\"\"\n        if 'mask' not in kwargs:\n            kwargs['mask'] = DEFAULT_SUBNET_MASK\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n\n        if identifier:\n            _filter['subnets']['networkIdentifier'] = (\n                utils.query_filter(identifier))\n        if datacenter:\n            _filter['subnets']['datacenter']['name'] = (\n                utils.query_filter(datacenter))\n        if version:\n            _filter['subnets']['version'] = utils.query_filter(version)\n        if subnet_type:\n            _filter['subnets']['subnetType'] = utils.query_filter(subnet_type)\n        else:\n            # This filters out global IPs from the subnet listing.\n            _filter['subnets']['subnetType'] = {'operation': '!= GLOBAL_IP'}\n        if network_space:\n            _filter['subnets']['networkVlan']['networkSpace'] = (\n                utils.query_filter(network_space))\n\n        kwargs['filter'] = _filter.to_dict()\n        kwargs['iter'] = True\n        return self.client.call('Account', 'getSubnets', **kwargs)", "response": "This method returns a list of all subnets on the account."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_vlans(self, datacenter=None, vlan_number=None, name=None, **kwargs):\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n\n        if vlan_number:\n            _filter['networkVlans']['vlanNumber'] = (\n                utils.query_filter(vlan_number))\n\n        if name:\n            _filter['networkVlans']['name'] = utils.query_filter(name)\n\n        if datacenter:\n            _filter['networkVlans']['primaryRouter']['datacenter']['name'] = (\n                utils.query_filter(datacenter))\n\n        kwargs['filter'] = _filter.to_dict()\n\n        if 'mask' not in kwargs:\n            kwargs['mask'] = DEFAULT_VLAN_MASK\n\n        kwargs['iter'] = True\n        return self.account.getNetworkVlans(**kwargs)", "response": "This method returns a list of all VLANs on the account."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_securitygroup_rules(self, group_id):\n        return self.security_group.getRules(id=group_id, iter=True)", "response": "List the security group rules associated with a security group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving rules from a security group.", "response": "def remove_securitygroup_rules(self, group_id, rules):\n        \"\"\"Remove rules from a security group.\n\n        :param int group_id: The ID of the security group\n        :param list rules: The list of IDs to remove\n        \"\"\"\n        return self.security_group.removeRules(rules, id=group_id)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_event_logs_by_request_id(self, request_id):\n\n        # Get all relevant event logs\n        unfiltered_logs = self._get_cci_event_logs() + self._get_security_group_event_logs()\n\n        # Grab only those that have the specific request id\n        filtered_logs = []\n\n        for unfiltered_log in unfiltered_logs:\n            try:\n                metadata = json.loads(unfiltered_log['metaData'])\n                if 'requestId' in metadata:\n                    if metadata['requestId'] == request_id:\n                        filtered_logs.append(unfiltered_log)\n            except ValueError:\n                continue\n\n        return filtered_logs", "response": "Gets all event logs that have the given request id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _list_global_ips_by_identifier(self, identifier):\n        results = self.list_global_ips(identifier=identifier, mask='id')\n        return [result['id'] for result in results]", "response": "Returns a list of the global IP matching the identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _list_subnets_by_identifier(self, identifier):\n        identifier = identifier.split('/', 1)[0]\n\n        results = self.list_subnets(identifier=identifier, mask='id')\n        return [result['id'] for result in results]", "response": "Returns a list of IDs of the subnet matching the identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of IDs of VLANs which match the given name.", "response": "def _list_vlans_by_name(self, name):\n        \"\"\"Returns a list of IDs of VLANs which match the given VLAN name.\n\n        :param string name: a VLAN name\n        :returns: List of matching IDs\n        \"\"\"\n        results = self.list_vlans(name=name, mask='id')\n        return [result['id'] for result in results]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_nas_credentials(self, identifier, **kwargs):\n        result = self.network_storage.getObject(id=identifier, **kwargs)\n        return result", "response": "Returns a list of IDs of VLANs which match the given VLAN name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, columns, sortby, volume_id):\n    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n\n    legal_volumes = file_storage_manager.get_replication_partners(\n        volume_id\n    )\n\n    if not legal_volumes:\n        click.echo(\"There are no replication partners for the given volume.\")\n    else:\n        table = formatting.Table(columns.columns)\n        table.sortby = sortby\n\n        for legal_volume in legal_volumes:\n            table.add_row([value or formatting.blank()\n                           for value in columns.row(legal_volume)])\n\n        env.fout(table)", "response": "List existing replicant volumes for a given file volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the ticket details from the ticket.", "response": "def get_ticket_results(mgr, ticket_id, update_count=1):\n    \"\"\"Get output about a ticket.\n\n    :param integer id: the ticket ID\n    :param integer update_count: number of entries to retrieve from ticket\n    :returns: a KeyValue table containing the details of the ticket\n\n    \"\"\"\n    ticket = mgr.get_ticket(ticket_id)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['id', ticket['id']])\n    table.add_row(['title', ticket['title']])\n    table.add_row(['priority', PRIORITY_MAP[ticket.get('priority', 0)]])\n    if ticket.get('assignedUser'):\n        user = ticket['assignedUser']\n        table.add_row([\n            'user',\n            \"%s %s\" % (user.get('firstName'), user.get('lastName')),\n        ])\n\n    table.add_row(['status', ticket['status']['name']])\n    table.add_row(['created', ticket.get('createDate')])\n    table.add_row(['edited', ticket.get('lastEditDate')])\n\n    # Only show up to the specified update count\n    updates = ticket.get('updates', [])\n    count = min(len(updates), update_count)\n    count_offset = len(updates) - count + 1  # Display as one-indexed\n    for i, update in enumerate(updates[-count:]):\n        wrapped_entry = \"\"\n\n        # Add user details (fields are different between employee and users)\n        editor = update.get('editor')\n        if editor:\n            if editor.get('displayName'):\n                wrapped_entry += \"By %s (Employee)\\n\" % (editor['displayName'])\n            if editor.get('firstName'):\n                wrapped_entry += \"By %s %s\\n\" % (editor.get('firstName'),\n                                                 editor.get('lastName'))\n\n        # NOTE(kmcdonald): Windows new-line characters need to be stripped out\n        wrapped_entry += click.wrap_text(update['entry'].replace('\\r', ''))\n        table.add_row(['update %s' % (count_offset + i,), wrapped_entry])\n\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a new subnet to your account.", "response": "def cli(env, network, quantity, vlan_id, ipv6, test):\n    \"\"\"Add a new subnet to your account. Valid quantities vary by type.\n\n    \\b\n    Type    - Valid Quantities (IPv4)\n    public  - 4, 8, 16, 32\n    private - 4, 8, 16, 32, 64\n\n    \\b\n    Type    - Valid Quantities (IPv6)\n    public  - 64\n\"\"\"\n\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    if not (test or env.skip_confirmations):\n        if not formatting.confirm(\"This action will incur charges on your \"\n                                  \"account. Continue?\"):\n            raise exceptions.CLIAbort('Cancelling order.')\n\n    version = 4\n    if ipv6:\n        version = 6\n\n    try:\n        result = mgr.add_subnet(network, quantity=quantity, vlan_id=vlan_id, version=version, test_order=test)\n    except SoftLayer.SoftLayerAPIError:\n        raise exceptions.CLIAbort('There is no price id for {} {} ipv{}'.format(quantity, network, version))\n\n    table = formatting.Table(['Item', 'cost'])\n    table.align['Item'] = 'r'\n    table.align['cost'] = 'r'\n\n    total = 0.0\n    if 'prices' in result:\n        for price in result['prices']:\n            total += float(price.get('recurringFee', 0.0))\n            rate = \"%.2f\" % float(price['recurringFee'])\n\n            table.add_row([price['item']['description'], rate])\n\n    table.add_row(['Total monthly cost', \"%.2f\" % total])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_tickets(self, open_status=True, closed_status=True):\n        mask = \"\"\"mask[id, title, assignedUser[firstName, lastName], priority,\n                  createDate, lastEditDate, accountId, status, updateCount]\"\"\"\n\n        call = 'getTickets'\n        if not all([open_status, closed_status]):\n            if open_status:\n                call = 'getOpenTickets'\n            elif closed_status:\n                call = 'getClosedTickets'\n            else:\n                raise ValueError(\"open_status and closed_status cannot both be False\")\n\n        return self.client.call('Account', call, mask=mask, iter=True)", "response": "List all tickets.\n\n        :param boolean open_status: include open tickets\n        :param boolean closed_status: include closed tickets"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget details about a ticket.", "response": "def get_ticket(self, ticket_id):\n        \"\"\"Get details about a ticket.\n\n        :param integer ticket_id: the ticket ID\n        :returns: dict -- information about the specified ticket\n\n        \"\"\"\n        mask = \"\"\"mask[id, title, assignedUser[firstName, lastName],status,\n                  createDate,lastEditDate,updates[entry,editor],updateCount, priority]\"\"\"\n        return self.ticket.getObject(id=ticket_id, mask=mask)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_ticket(self, title=None, body=None, subject=None, priority=None):\n        current_user = self.account.getCurrentUser()\n        new_ticket = {\n            'subjectId': subject,\n            'contents': body,\n            'assignedUserId': current_user['id'],\n            'title': title,\n        }\n        if priority is not None:\n            new_ticket['priority'] = int(priority)\n\n        created_ticket = self.ticket.createStandardTicket(new_ticket, body)\n        return created_ticket", "response": "Create a new ticket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_ticket(self, ticket_id=None, body=None):\n        return self.ticket.addUpdate({'entry': body}, id=ticket_id)", "response": "Update a ticket.\n\n        :param integer ticket_id: the id of the ticket to update\n        :param string body: entry to update in the ticket"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upload_attachment(self, ticket_id=None, file_path=None, file_name=None):\n        file_content = None\n        with open(file_path, 'rb') as attached_file:\n            file_content = attached_file.read()\n\n        file_object = {\n            \"filename\": file_name,\n            \"data\": file_content\n        }\n\n        return self.ticket.addAttachedFile(file_object, id=ticket_id)", "response": "Uploads an attachment to a ticket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attach_hardware(self, ticket_id=None, hardware_id=None):\n        return self.ticket.addAttachedHardware(hardware_id, id=ticket_id)", "response": "Attach hardware to a ticket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attach_virtual_server(self, ticket_id=None, virtual_id=None):\n        return self.ticket.addAttachedVirtualGuest(virtual_id, id=ticket_id)", "response": "Attach a virtual server to a ticket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetaching hardware from a ticket.", "response": "def detach_hardware(self, ticket_id=None, hardware_id=None):\n        \"\"\"Detach hardware from a ticket.\n\n        :param ticket_id: the id of the ticket to detach from\n        :param hardware_id: the id of the hardware to detach\n\n        :returns: bool -- Whether the detachment was successful\n        \"\"\"\n        return self.ticket.removeAttachedHardware(hardware_id, id=ticket_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetaches a virtual server from a ticket.", "response": "def detach_virtual_server(self, ticket_id=None, virtual_id=None):\n        \"\"\"Detach a virtual server from a ticket.\n\n        :param ticket_id: the id of the ticket to detach from\n        :param virtual_id: the id of the virtual server to detach\n\n        :returns: bool -- Whether the detachment was successful\n        \"\"\"\n        return self.ticket.removeAttachedVirtualGuest(virtual_id, id=ticket_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, zonefile, dry_run):\n\n    manager = SoftLayer.DNSManager(env.client)\n    with open(zonefile) as zone_f:\n        zone_contents = zone_f.read()\n\n    zone, records, bad_lines = parse_zone_details(zone_contents)\n\n    env.out(\"Parsed: zone=%s\" % zone)\n    for record in records:\n        env.out(\"Parsed: %s\" % RECORD_FMT.format(**record))\n\n    for line in bad_lines:\n        env.out(\"Unparsed: %s\" % line)\n\n    if dry_run:\n        return\n\n    # Find zone id or create the zone if it doesn't exist\n    try:\n        zone_id = helpers.resolve_id(manager.resolve_ids, zone,\n                                     name='zone')\n    except exceptions.CLIAbort:\n        zone_id = manager.create_zone(zone)['id']\n        env.out(click.style(\"Created: %s\" % zone, fg='green'))\n\n    # Attempt to create each record\n    for record in records:\n        try:\n            manager.create_record(zone_id,\n                                  record['record'],\n                                  record['type'],\n                                  record['data'],\n                                  record['ttl'])\n\n            env.out(click.style(\"Created: %s\" % RECORD_FMT.format(**record),\n                                fg='green'))\n        except SoftLayer.SoftLayerAPIError as ex:\n            env.out(click.style(\"Failed: %s\" % RECORD_FMT.format(**record),\n                                fg='red'))\n            env.out(click.style(str(ex), fg='red'))\n\n    env.out(click.style(\"Finished\", fg='green'))", "response": "Import zone based off a BIND zone file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a zone file into python data - structures.", "response": "def parse_zone_details(zone_contents):\n    \"\"\"Parses a zone file into python data-structures.\"\"\"\n    records = []\n    bad_lines = []\n    zone_lines = [line.strip() for line in zone_contents.split('\\n')]\n\n    zone_search = re.search(r'^\\$ORIGIN (?P<zone>.*)\\.', zone_lines[0])\n    zone = zone_search.group('zone')\n\n    for line in zone_lines[1:]:\n        record_search = re.search(RECORD_REGEX, line)\n        if record_search is None:\n            bad_lines.append(line)\n            continue\n\n        name = record_search.group('domain')\n        # The API requires we send a host, although bind allows a blank\n        # entry. @ is the same thing as blank\n        if name is None:\n            name = \"@\"\n\n        ttl = record_search.group('ttl')\n        # we don't do anything with the class\n        # domain_class = domainSearch.group('class')\n        record_type = record_search.group('type').upper()\n        data = record_search.group('data')\n\n        # the dns class doesn't support weighted MX records yet, so we chomp\n        # that part out.\n        if record_type == \"MX\":\n            record_search = re.search(r'(?P<weight>\\d+)\\s+(?P<data>.*)', data)\n            data = record_search.group('data')\n\n        # This will skip the SOA record bit. And any domain that gets\n        # parsed oddly.\n        if record_type == 'IN':\n            bad_lines.append(line)\n            continue\n\n        records.append({\n            'record': name,\n            'type': record_type,\n            'data': data,\n            'ttl': ttl,\n        })\n\n    return zone, records, bad_lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist object storage accounts.", "response": "def cli(env):\n    \"\"\"List object storage accounts.\"\"\"\n\n    mgr = SoftLayer.ObjectStorageManager(env.client)\n    accounts = mgr.list_accounts()\n    table = formatting.Table(['id', 'name', 'apiType'])\n    table.sortby = 'id'\n    api_type = None\n    for account in accounts:\n        if 'vendorName' in account and account['vendorName'] == 'Swift':\n            api_type = 'Swift'\n        elif 'Cleversafe' in account['serviceResource']['name']:\n            api_type = 'S3'\n\n        table.add_row([\n            account['id'],\n            account['username'],\n            api_type,\n        ])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, billing_id, datacenter):\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    if not formatting.confirm(\"This action will incur charges on your \"\n                              \"account. Continue?\"):\n        raise exceptions.CLIAbort('Aborted.')\n    mgr.add_local_lb(billing_id, datacenter=datacenter)\n    env.fout(\"Load balancer is being created!\")", "response": "Adds a load balancer given the id returned from create - options."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresets all connections on a certain service group.", "response": "def cli(env, identifier):\n    \"\"\"Reset connections on a certain service group.\"\"\"\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    loadbal_id, group_id = loadbal.parse_id(identifier)\n\n    mgr.reset_service_group(loadbal_id, group_id)\n    env.fout('Load balancer service group connections are being reset!')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an attachment to an existing ticket.", "response": "def cli(env, identifier, path, name):\n    \"\"\"Adds an attachment to an existing ticket.\"\"\"\n    mgr = SoftLayer.TicketManager(env.client)\n\n    ticket_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'ticket')\n\n    if path is None:\n        raise exceptions.ArgumentError(\"Missing argument --path\")\n\n    if not os.path.exists(path):\n        raise exceptions.ArgumentError(\"%s not exist\" % path)\n\n    if name is None:\n        name = os.path.basename(path)\n\n    attached_file = mgr.upload_attachment(ticket_id=ticket_id,\n                                          file_path=path,\n                                          file_name=name)\n    env.fout(\"File attached: \\n%s\" % attached_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_standard_package(self, server_id, is_virt=True):\n\n        firewall_port_speed = self._get_fwl_port_speed(server_id, is_virt)\n\n        _value = \"%s%s\" % (firewall_port_speed, \"Mbps Hardware Firewall\")\n        _filter = {'items': {'description': utils.query_filter(_value)}}\n\n        return self.prod_pkg.getItems(id=0, filter=_filter)", "response": "Retrieves the standard firewall package for the virtual server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the dedicated firewall package.", "response": "def get_dedicated_package(self, ha_enabled=False):\n        \"\"\"Retrieves the dedicated firewall package.\n\n        :param bool ha_enabled: True if HA is to be enabled on the firewall\n                                False for No HA\n        :returns: A dictionary containing the dedicated virtual server firewall\n                  package\n        \"\"\"\n\n        fwl_filter = 'Hardware Firewall (Dedicated)'\n        ha_fwl_filter = 'Hardware Firewall (High Availability)'\n        _filter = utils.NestedDict({})\n        if ha_enabled:\n            _filter['items']['description'] = utils.query_filter(ha_fwl_filter)\n        else:\n            _filter['items']['description'] = utils.query_filter(fwl_filter)\n\n        return self.prod_pkg.getItems(id=0, filter=_filter.to_dict())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancel_firewall(self, firewall_id, dedicated=False):\n\n        fwl_billing = self._get_fwl_billing_item(firewall_id, dedicated)\n        billing_item_service = self.client['Billing_Item']\n        return billing_item_service.cancelService(id=fwl_billing['id'])", "response": "Cancels the specified firewall.\n\n        :param int firewall_id: Firewall ID to be cancelled.\n        :param bool dedicated: If true, the firewall instance is dedicated,\n                               otherwise, the firewall instance is shared."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a firewall for the specified virtual server.", "response": "def add_standard_firewall(self, server_id, is_virt=True):\n        \"\"\"Creates a firewall for the specified virtual/hardware server.\n\n        :param int server_id: The ID of the server to create the firewall for\n        :param bool is_virt: If true, will create the firewall for a virtual\n                             server, otherwise for a hardware server.\n        :returns: A dictionary containing the standard virtual server firewall\n                  order\n        \"\"\"\n\n        package = self.get_standard_package(server_id, is_virt)\n        if is_virt:\n            product_order = {\n                'complexType': 'SoftLayer_Container_Product_Order_Network_'\n                               'Protection_Firewall',\n                'quantity': 1,\n                'packageId': 0,\n                'virtualGuests': [{'id': server_id}],\n                'prices': [{'id': package[0]['prices'][0]['id']}]\n            }\n        else:\n            product_order = {\n                'complexType': 'SoftLayer_Container_Product_Order_Network_'\n                               'Protection_Firewall',\n                'quantity': 1,\n                'packageId': 0,\n                'hardware': [{'id': server_id}],\n                'prices': [{'id': package[0]['prices'][0]['id']}]\n            }\n        return self.client['Product_Order'].placeOrder(product_order)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_vlan_firewall(self, vlan_id, ha_enabled=False):\n\n        package = self.get_dedicated_package(ha_enabled)\n        product_order = {\n            'complexType': 'SoftLayer_Container_Product_Order_Network_'\n                           'Protection_Firewall_Dedicated',\n            'quantity': 1,\n            'packageId': 0,\n            'vlanId': vlan_id,\n            'prices': [{'id': package[0]['prices'][0]['id']}]\n        }\n        return self.client['Product_Order'].placeOrder(product_order)", "response": "Creates a firewall for the specified VLAN."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the billing item of the firewall.", "response": "def _get_fwl_billing_item(self, firewall_id, dedicated=False):\n        \"\"\"Retrieves the billing item of the firewall.\n\n        :param int firewall_id: Firewall ID to get the billing item for\n        :param bool dedicated: whether the firewall is dedicated or standard\n        :returns: A dictionary of the firewall billing item.\n        \"\"\"\n\n        mask = 'mask[id,billingItem[id]]'\n        if dedicated:\n            firewall_service = self.client['Network_Vlan_Firewall']\n        else:\n            firewall_service = self.client['Network_Component_Firewall']\n        firewall = firewall_service.getObject(id=firewall_id, mask=mask)\n        if firewall is None:\n            raise exceptions.SoftLayerError(\n                \"Unable to find firewall %d\" % firewall_id)\n        if firewall.get('billingItem') is None:\n            raise exceptions.SoftLayerError(\n                \"Unable to find billing item for firewall %d\" % firewall_id)\n\n        return firewall['billingItem']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_fwl_port_speed(self, server_id, is_virt=True):\n\n        fwl_port_speed = 0\n        if is_virt:\n            mask = ('primaryNetworkComponent[maxSpeed]')\n            svc = self.client['Virtual_Guest']\n            primary = svc.getObject(mask=mask, id=server_id)\n            fwl_port_speed = primary['primaryNetworkComponent']['maxSpeed']\n        else:\n            mask = ('id,maxSpeed,networkComponentGroup.networkComponents')\n            svc = self.client['Hardware_Server']\n            network_components = svc.getFrontendNetworkComponents(\n                mask=mask, id=server_id)\n            grouped = [interface['networkComponentGroup']['networkComponents']\n                       for interface in network_components\n                       if 'networkComponentGroup' in interface]\n            ungrouped = [interface\n                         for interface in network_components\n                         if 'networkComponentGroup' not in interface]\n\n            # For each group, sum the maxSpeeds of each compoment in the\n            # group. Put the sum for each in a new list\n            group_speeds = []\n            for group in grouped:\n                group_speed = 0\n                for interface in group:\n                    group_speed += interface['maxSpeed']\n                group_speeds.append(group_speed)\n\n            # The max speed of all groups is the max of the list\n            max_grouped_speed = max(group_speeds)\n\n            max_ungrouped = 0\n            for interface in ungrouped:\n                max_ungrouped = max(max_ungrouped, interface['maxSpeed'])\n\n            fwl_port_speed = max(max_grouped_speed, max_ungrouped)\n\n        return fwl_port_speed", "response": "Determines the appropriate speed for a firewall."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_firewalls(self):\n\n        mask = ('firewallNetworkComponents,'\n                'networkVlanFirewall,'\n                'dedicatedFirewallFlag,'\n                'firewallGuestNetworkComponents,'\n                'firewallInterfaces,'\n                'firewallRules,'\n                'highAvailabilityFirewallFlag')\n\n        return [firewall\n                for firewall in self.account.getNetworkVlans(mask=mask)\n                if has_firewall(firewall)]", "response": "Returns a list of all firewalls on the current account."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the rules of a standard firewall.", "response": "def get_standard_fwl_rules(self, firewall_id):\n        \"\"\"Get the rules of a standard firewall.\n\n        :param integer firewall_id: the instance ID of the standard firewall\n        :returns: A list of the rules.\n        \"\"\"\n\n        svc = self.client['Network_Component_Firewall']\n        return svc.getRules(id=firewall_id, mask=RULE_MASK)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nediting the rules for dedicated firewall.", "response": "def edit_dedicated_fwl_rules(self, firewall_id, rules):\n        \"\"\"Edit the rules for dedicated firewall.\n\n        :param integer firewall_id: the instance ID of the dedicated firewall\n        :param list rules: the rules to be pushed on the firewall as defined by\n                           SoftLayer_Network_Firewall_Update_Request_Rule\n        \"\"\"\n\n        mask = ('mask[networkVlan[firewallInterfaces'\n                '[firewallContextAccessControlLists]]]')\n        svc = self.client['Network_Vlan_Firewall']\n        fwl = svc.getObject(id=firewall_id, mask=mask)\n        network_vlan = fwl['networkVlan']\n\n        for fwl1 in network_vlan['firewallInterfaces']:\n            if fwl1['name'] == 'inside':\n                continue\n\n            for control_list in fwl1['firewallContextAccessControlLists']:\n                if control_list['direction'] == 'out':\n                    continue\n                fwl_ctx_acl_id = control_list['id']\n\n        template = {'firewallContextAccessControlListId': fwl_ctx_acl_id,\n                    'rules': rules}\n\n        svc = self.client['Network_Firewall_Update_Request']\n        return svc.createObject(template)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nedits the rules for the standard firewall.", "response": "def edit_standard_fwl_rules(self, firewall_id, rules):\n        \"\"\"Edit the rules for standard firewall.\n\n        :param integer firewall_id: the instance ID of the standard firewall\n        :param dict rules: the rules to be pushed on the firewall\n        \"\"\"\n\n        rule_svc = self.client['Network_Firewall_Update_Request']\n        template = {'networkComponentFirewallId': firewall_id, 'rules': rules}\n\n        return rule_svc.createObject(template)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_request(self, request):\n        request.headers['authenticate'] = {\n            'complexType': 'PortalLoginToken',\n            'userId': self.user_id,\n            'authToken': self.auth_token,\n        }\n        return request", "response": "Sets token - based auth headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting token - based auth headers.", "response": "def get_request(self, request):\n        \"\"\"Sets token-based auth headers.\"\"\"\n        request.headers['authenticate'] = {\n            'username': self.username,\n            'apiKey': self.api_key,\n        }\n        return request"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_request(self, request):\n        request.transport_user = self.username\n        request.transport_password = self.api_key\n        return request", "response": "Sets token - based auth headers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nediting details of an image.", "response": "def cli(env, identifier, name, note, tag):\n    \"\"\"Edit details of an image.\"\"\"\n\n    image_mgr = SoftLayer.ImageManager(env.client)\n    data = {}\n    if name:\n        data['name'] = name\n    if note:\n        data['note'] = note\n    if tag:\n        data['tag'] = tag\n    image_id = helpers.resolve_id(image_mgr.resolve_ids, identifier, 'image')\n    if not image_mgr.edit(image_id, **data):\n        raise exceptions.CLIAbort(\"Failed to Edit Image\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattaching devices to a ticket.", "response": "def cli(env, identifier, hardware_identifier, virtual_identifier):\n    \"\"\"Attach devices to a ticket.\"\"\"\n    ticket_mgr = SoftLayer.TicketManager(env.client)\n\n    if hardware_identifier and virtual_identifier:\n        raise exceptions.ArgumentError(\"Cannot attach hardware and a virtual server at the same time\")\n\n    if hardware_identifier:\n        hardware_mgr = SoftLayer.HardwareManager(env.client)\n        hardware_id = helpers.resolve_id(hardware_mgr.resolve_ids, hardware_identifier, 'hardware')\n        ticket_mgr.attach_hardware(identifier, hardware_id)\n    elif virtual_identifier:\n        vs_mgr = SoftLayer.VSManager(env.client)\n        vs_id = helpers.resolve_id(vs_mgr.resolve_ids, virtual_identifier, 'VS')\n        ticket_mgr.attach_virtual_server(identifier, vs_id)\n    else:\n        raise exceptions.ArgumentError(\"Must have a hardware or virtual server identifier to attach\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_network(self, kind, router=True, vlans=True, vlan_ids=True):\n        network = {}\n        macs = self.get('%s_mac' % kind)\n        network['mac_addresses'] = macs\n\n        if len(macs) == 0:\n            return network\n\n        if router:\n            network['router'] = self.get('router', macs[0])\n\n        if vlans:\n            network['vlans'] = self.get('vlans', macs[0])\n\n        if vlan_ids:\n            network['vlan_ids'] = self.get('vlan_ids', macs[0])\n\n        return network", "response": "Wrapper for getting details about networks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrestore block volume using a given snapshot", "response": "def cli(env, volume_id, snapshot_id):\n    \"\"\"Restore block volume using a given snapshot\"\"\"\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    success = block_manager.restore_from_snapshot(volume_id, snapshot_id)\n\n    if success:\n        click.echo('Block volume %s is being restored using snapshot %s'\n                   % (volume_id, snapshot_id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd and upload SSL certificate details.", "response": "def cli(env, crt, csr, icc, key, notes):\n    \"\"\"Add and upload SSL certificate details.\"\"\"\n\n    template = {\n        'intermediateCertificate': '',\n        'certificateSigningRequest': '',\n        'notes': notes,\n    }\n    template['certificate'] = open(crt).read()\n    template['privateKey'] = open(key).read()\n    if csr:\n        body = open(csr).read()\n        template['certificateSigningRequest'] = body\n\n    if icc:\n        body = open(icc).read()\n        template['intermediateCertificate'] = body\n\n    manager = SoftLayer.SSLManager(env.client)\n    manager.add_certificate(template)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, identifier, price=False, guests=False):\n    dhost = SoftLayer.DedicatedHostManager(env.client)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    result = dhost.get_host(identifier)\n    result = utils.NestedDict(result)\n\n    table.add_row(['id', result['id']])\n    table.add_row(['name', result['name']])\n    table.add_row(['cpu count', result['cpuCount']])\n    table.add_row(['memory capacity', result['memoryCapacity']])\n    table.add_row(['disk capacity', result['diskCapacity']])\n    table.add_row(['create date', result['createDate']])\n    table.add_row(['modify date', result['modifyDate']])\n    table.add_row(['router id', result['backendRouter']['id']])\n    table.add_row(['router hostname', result['backendRouter']['hostname']])\n    table.add_row(['owner', formatting.FormattedItem(\n        utils.lookup(result, 'billingItem', 'orderItem', 'order', 'userRecord', 'username') or formatting.blank(),)])\n\n    if price:\n        total_price = utils.lookup(result,\n                                   'billingItem',\n                                   'nextInvoiceTotalRecurringAmount') or 0\n        total_price += sum(p['nextInvoiceTotalRecurringAmount']\n                           for p\n                           in utils.lookup(result,\n                                           'billingItem',\n                                           'children') or [])\n        table.add_row(['price_rate', total_price])\n\n    table.add_row(['guest count', result['guestCount']])\n    if guests:\n        guest_table = formatting.Table(['id', 'hostname', 'domain', 'uuid'])\n        for guest in result['guests']:\n            guest_table.add_row([\n                guest['id'], guest['hostname'], guest['domain'], guest['uuid']])\n        table.add_row(['guests', guest_table])\n\n    table.add_row(['datacenter', result['datacenter']['name']])\n\n    env.fout(table)", "response": "Get details for a virtual server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport an image to object storage.", "response": "def cli(env, identifier, uri, ibm_api_key):\n    \"\"\"Export an image to object storage.\n\n    The URI for an object storage object (.vhd/.iso file) of the format:\n    swift://<objectStorageAccount>@<cluster>/<container>/<objectPath>\n    or cos://<regionName>/<bucketName>/<objectPath> if using IBM Cloud\n    Object Storage\n    \"\"\"\n\n    image_mgr = SoftLayer.ImageManager(env.client)\n    image_id = helpers.resolve_id(image_mgr.resolve_ids, identifier, 'image')\n    result = image_mgr.export_image_to_uri(image_id, uri, ibm_api_key)\n\n    if not result:\n        raise exceptions.CLIAbort(\"Failed to export Image\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, limit, closed=False, get_all=False):\n\n    manager = AccountManager(env.client)\n    invoices = manager.get_invoices(limit, closed, get_all)\n\n    table = formatting.Table([\n        \"Id\", \"Created\", \"Type\", \"Status\", \"Starting Balance\", \"Ending Balance\", \"Invoice Amount\", \"Items\"\n    ])\n    table.align['Starting Balance'] = 'l'\n    table.align['Ending Balance'] = 'l'\n    table.align['Invoice Amount'] = 'l'\n    table.align['Items'] = 'l'\n    if isinstance(invoices, dict):\n        invoices = [invoices]\n    for invoice in invoices:\n        table.add_row([\n            invoice.get('id'),\n            utils.clean_time(invoice.get('createDate'), out_format=\"%Y-%m-%d\"),\n            invoice.get('typeCode'),\n            invoice.get('statusCode'),\n            invoice.get('startingBalance'),\n            invoice.get('endingBalance'),\n            invoice.get('invoiceTotalAmount'),\n            invoice.get('itemCount')\n        ])\n    env.fout(table)", "response": "List all invoices and all that mess"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate memory argument. Returns the memory value in megabytes.", "response": "def convert(self, value, param, ctx):  # pylint: disable=inconsistent-return-statements\n        \"\"\"Validate memory argument. Returns the memory value in megabytes.\"\"\"\n        matches = MEMORY_RE.match(value.lower())\n        if matches is None:\n            self.fail('%s is not a valid value for memory amount' % value, param, ctx)\n        amount_str, unit = matches.groups()\n        amount = int(amount_str)\n        if unit in [None, 'm', 'mb']:\n            # Assume the user intends gigabytes if they specify a number < 1024\n            if amount < 1024:\n                return amount * 1024\n            else:\n                if amount % 1024 != 0:\n                    self.fail('%s is not an integer that is divisable by 1024' % value, param, ctx)\n                return amount\n        elif unit in ['g', 'gb']:\n            return amount * 1024"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists NAS account credentials.", "response": "def cli(env, identifier):\n    \"\"\"List NAS account credentials.\"\"\"\n\n    nw_mgr = SoftLayer.NetworkManager(env.client)\n    result = nw_mgr.get_nas_credentials(identifier)\n    table = formatting.Table(['username', 'password'])\n    table.add_row([result.get('username', 'None'),\n                   result.get('password', 'None')])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_rules_table(rules):\n    table = formatting.Table(['#', 'action', 'protocol', 'src_ip', 'src_mask',\n                              'dest', 'dest_mask'])\n    table.sortby = '#'\n    for rule in rules:\n        table.add_row([\n            rule['orderValue'],\n            rule['action'],\n            rule['protocol'],\n            rule['sourceIpAddress'],\n            utils.lookup(rule, 'sourceIpSubnetMask'),\n            '%s:%s-%s' % (rule['destinationIpAddress'],\n                          rule['destinationPortRangeStart'],\n                          rule['destinationPortRangeEnd']),\n            utils.lookup(rule, 'destinationIpSubnetMask')])\n    return table", "response": "Helper to format the rules into a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_key(self, key, label, notes=None):\n        order = {\n            'key': key,\n            'label': label,\n            'notes': notes,\n        }\n\n        return self.sshkey.createObject(order)", "response": "Adds a new SSH key to the account."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nedits information about an SSH key.", "response": "def edit_key(self, key_id, label=None, notes=None):\n        \"\"\"Edits information about an SSH key.\n\n        :param int key_id: The ID of the key to edit\n        :param string label: The new label for the key\n        :param string notes: Notes to set or change on the key\n        :returns: A Boolean indicating success or failure\n        \"\"\"\n        data = {}\n\n        if label:\n            data['label'] = label\n\n        if notes:\n            data['notes'] = notes\n\n        return self.sshkey.editObject(data, id=key_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_keys(self, label=None):\n        _filter = utils.NestedDict({})\n        if label:\n            _filter['sshKeys']['label'] = utils.query_filter(label)\n\n        return self.client['Account'].getSshKeys(filter=_filter.to_dict())", "response": "Lists all SSH keys on the account."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_ids_from_label(self, label):\n        keys = self.list_keys()\n        results = []\n        for key in keys:\n            if key['label'] == label:\n                results.append(key['id'])\n        return results", "response": "Return a list of sshkey IDs which match the given label."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading SSL certificate and key file.", "response": "def cli(env, identifier):\n    \"\"\"Download SSL certificate and key file.\"\"\"\n\n    manager = SoftLayer.SSLManager(env.client)\n    certificate = manager.get_certificate(identifier)\n\n    write_cert(certificate['commonName'] + '.crt', certificate['certificate'])\n    write_cert(certificate['commonName'] + '.key', certificate['privateKey'])\n\n    if 'intermediateCertificate' in certificate:\n        write_cert(certificate['commonName'] + '.icc',\n                   certificate['intermediateCertificate'])\n\n    if 'certificateSigningRequest' in certificate:\n        write_cert(certificate['commonName'] + '.csr',\n                   certificate['certificateSigningRequest'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, identifier, name, all, note):\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n\n    capture = vsi.capture(vs_id, name, all, note)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['vs_id', capture['guestId']])\n    table.add_row(['date', capture['createDate'][:10]])\n    table.add_row(['time', capture['createDate'][11:19]])\n    table.add_row(['transaction', formatting.transaction_status(capture)])\n    table.add_row(['transaction_id', capture['id']])\n    table.add_row(['all_disks', all])\n    env.fout(table)", "response": "Capture one or all disks from a virtual server to a SoftLayer image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_event_logs(self, request_filter=None, log_limit=20, iterator=True):\n        if iterator:\n            # Call iter_call directly as this returns the actual generator\n            return self.client.iter_call('Event_Log', 'getAllObjects', filter=request_filter, limit=log_limit)\n        return self.client.call('Event_Log', 'getAllObjects', filter=request_filter, limit=log_limit)", "response": "Returns a list of event logs in a given order."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_filter(date_min=None, date_max=None, obj_event=None, obj_id=None, obj_type=None, utc_offset=None):\n\n        if not any([date_min, date_max, obj_event, obj_id, obj_type]):\n            return {}\n\n        request_filter = {}\n\n        if date_min and date_max:\n            request_filter['eventCreateDate'] = utils.event_log_filter_between_date(date_min, date_max, utc_offset)\n        else:\n            if date_min:\n                request_filter['eventCreateDate'] = utils.event_log_filter_greater_than_date(date_min, utc_offset)\n            elif date_max:\n                request_filter['eventCreateDate'] = utils.event_log_filter_less_than_date(date_max, utc_offset)\n\n        if obj_event:\n            request_filter['eventName'] = {'operation': obj_event}\n\n        if obj_id:\n            request_filter['objectId'] = {'operation': obj_id}\n\n        if obj_type:\n            request_filter['objectName'] = {'operation': obj_type}\n\n        return request_filter", "response": "Builds a query filter that can be passed into EventLogManager. get_event_logs\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env, zone, data, record, ttl, type):\n\n    manager = SoftLayer.DNSManager(env.client)\n    table = formatting.Table(['id', 'record', 'type', 'ttl', 'data'])\n\n    table.align['ttl'] = 'l'\n    table.align['record'] = 'r'\n    table.align['data'] = 'l'\n\n    zone_id = helpers.resolve_id(manager.resolve_ids, zone, name='zone')\n\n    records = manager.get_records(zone_id,\n                                  record_type=type,\n                                  host=record,\n                                  ttl=ttl,\n                                  data=data)\n\n    for the_record in records:\n        table.add_row([\n            the_record['id'],\n            the_record['host'],\n            the_record['type'].upper(),\n            the_record['ttl'],\n            the_record['data']\n        ])\n\n    env.fout(table)", "response": "List all records in a zone."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist Subject IDs for ticket creation.", "response": "def cli(env):\n    \"\"\"List Subject IDs for ticket creation.\"\"\"\n    ticket_mgr = SoftLayer.TicketManager(env.client)\n\n    table = formatting.Table(['id', 'subject'])\n    for subject in ticket_mgr.list_subjects():\n        table.add_row([subject['id'], subject['name']])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a placement group.", "response": "def cli(env, **args):\n    \"\"\"Create a placement group.\"\"\"\n    manager = PlacementManager(env.client)\n    backend_router_id = helpers.resolve_id(manager.get_backend_router_id_from_hostname,\n                                           args.get('backend_router'),\n                                           'backendRouter')\n    rule_id = helpers.resolve_id(manager.get_rule_id_from_name, args.get('rule'), 'Rule')\n    placement_object = {\n        'name': args.get('name'),\n        'backendRouterId': backend_router_id,\n        'ruleId': rule_id\n    }\n\n    result = manager.create(placement_object)\n    click.secho(\"Successfully created placement group: ID: %s, Name: %s\" % (result['id'], result['name']), fg='green')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting a table of basic user information", "response": "def basic_info(user, keys):\n    \"\"\"Prints a table of basic user information\"\"\"\n\n    table = formatting.KeyValueTable(['Title', 'Basic Information'])\n    table.align['Title'] = 'r'\n    table.align['Basic Information'] = 'l'\n\n    table.add_row(['Id', user.get('id', '-')])\n    table.add_row(['Username', user.get('username', '-')])\n    if keys:\n        for key in user.get('apiAuthenticationKeys'):\n            table.add_row(['APIKEY', key.get('authenticationKey')])\n    table.add_row(['Name', \"%s %s\" % (user.get('firstName', '-'), user.get('lastName', '-'))])\n    table.add_row(['Email', user.get('email')])\n    table.add_row(['OpenID', user.get('openIdConnectUserName')])\n    address = \"%s %s %s %s %s %s\" % (\n        user.get('address1'), user.get('address2'), user.get('city'), user.get('state'),\n        user.get('country'), user.get('postalCode'))\n    table.add_row(['Address', address])\n    table.add_row(['Company', user.get('companyName')])\n    table.add_row(['Created', user.get('createDate')])\n    table.add_row(['Phone Number', user.get('officePhone')])\n    if user.get('parentId', False):\n        table.add_row(['Parent User', utils.lookup(user, 'parent', 'username')])\n    table.add_row(['Status', utils.lookup(user, 'userStatus', 'name')])\n    table.add_row(['PPTP VPN', user.get('pptpVpnAllowedFlag', 'No')])\n    table.add_row(['SSL VPN', user.get('sslVpnAllowedFlag', 'No')])\n    for login in user.get('unsuccessfulLogins', {}):\n        login_string = \"%s From: %s\" % (login.get('createDate'), login.get('ipAddress'))\n        table.add_row(['Last Failed Login', login_string])\n        break\n    for login in user.get('successfulLogins', {}):\n        login_string = \"%s From: %s\" % (login.get('createDate'), login.get('ipAddress'))\n        table.add_row(['Last Login', login_string])\n        break\n\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint out a users permissions", "response": "def print_permissions(permissions):\n    \"\"\"Prints out a users permissions\"\"\"\n\n    table = formatting.Table(['keyName', 'Description'])\n    for perm in permissions:\n        table.add_row([perm['keyName'], perm['name']])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_access(access, title):\n\n    columns = ['id', 'hostname', 'Primary Public IP', 'Primary Private IP', 'Created']\n    table = formatting.Table(columns, title)\n\n    for host in access:\n        host_id = host.get('id')\n        host_fqdn = host.get('fullyQualifiedDomainName', '-')\n        host_primary = host.get('primaryIpAddress')\n        host_private = host.get('primaryBackendIpAddress')\n        host_created = host.get('provisionDate')\n        table.add_row([host_id, host_fqdn, host_primary, host_private, host_created])\n    return table", "response": "Prints out the hardware or virtual guests that a user can access"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting out the dedicated hosts a user can access", "response": "def print_dedicated_access(access):\n    \"\"\"Prints out the dedicated hosts a user can access\"\"\"\n\n    table = formatting.Table(['id', 'Name', 'Cpus', 'Memory', 'Disk', 'Created'], 'Dedicated Access')\n    for host in access:\n        host_id = host.get('id')\n        host_fqdn = host.get('name')\n        host_cpu = host.get('cpuCount')\n        host_mem = host.get('memoryCapacity')\n        host_disk = host.get('diskCapacity')\n        host_created = host.get('createDate')\n        table.add_row([host_id, host_fqdn, host_cpu, host_mem, host_disk, host_created])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_logins(logins):\n    table = formatting.Table(['Date', 'IP Address', 'Successufl Login?'])\n    for login in logins:\n        table.add_row([login.get('createDate'), login.get('ipAddress'), login.get('successFlag')])\n    return table", "response": "Prints out the login history for a user"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints out the event log for a user", "response": "def print_events(events):\n    \"\"\"Prints out the event log for a user\"\"\"\n    columns = ['Date', 'Type', 'IP Address', 'label', 'username']\n    table = formatting.Table(columns)\n    for event in events:\n        table.add_row([event.get('eventCreateDate'), event.get('eventName'),\n                       event.get('ipAddress'), event.get('label'), event.get('username')])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, identifier):\n\n    mgr = SoftLayer.UserManager(env.client)\n\n    user_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'username')\n\n    user_template = {'userStatusId': 1021}\n\n    result = mgr.edit_user(user_id, user_template)\n    if result:\n        click.secho(\"%s deleted successfully\" % identifier, fg='green')\n    else:\n        click.secho(\"Failed to delete %s\" % identifier, fg='red')", "response": "Delete a user s content."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an origin pull mapping.", "response": "def cli(env, account_id, content_url, type, cname):\n    \"\"\"Create an origin pull mapping.\"\"\"\n\n    manager = SoftLayer.CDNManager(env.client)\n    manager.add_origin(account_id, type, content_url, cname)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_to_template(filename, args, exclude=None):\n    exclude = exclude or []\n    exclude.append('config')\n    exclude.append('really')\n    exclude.append('format')\n    exclude.append('debug')\n\n    with open(filename, \"w\") as template_file:\n        for k, val in args.items():\n            if val and k not in exclude:\n                if isinstance(val, tuple):\n                    val = ','.join(val)\n                if isinstance(val, list):\n                    val = ','.join(val)\n                template_file.write('%s=%s\\n' % (k, val))", "response": "Exports given options to the given filename in INI format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, mask=None):\n        if mask is None:\n            mask = \"mask[id, name, createDate, rule, guestCount, backendRouter[id, hostname]]\"\n        groups = self.client.call('Account', 'getPlacementGroups', mask=mask, iter=True)\n        return groups", "response": "List existing placement groups."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_object(self, group_id, mask=None):\n        if mask is None:\n            mask = \"mask[id, name, createDate, rule, backendRouter[id, hostname],\" \\\n                   \"guests[activeTransaction[id,transactionStatus[name,friendlyName]]]]\"\n        return self.client.call('SoftLayer_Virtual_PlacementGroup', 'getObject', id=group_id, mask=mask)", "response": "Returns a PlacementGroup Object\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the rule that matches name.", "response": "def get_rule_id_from_name(self, name):\n        \"\"\"Finds the rule that matches name.\n\n        SoftLayer_Virtual_PlacementGroup_Rule.getAllObjects doesn't support objectFilters.\n        \"\"\"\n        results = self.client.call('SoftLayer_Virtual_PlacementGroup_Rule', 'getAllObjects')\n        return [result['id'] for result in results if result['keyName'] == name.upper()]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_backend_router_id_from_hostname(self, hostname):\n        results = self.client.call('SoftLayer_Network_Pod', 'getAllObjects')\n        return [result['backendRouterId'] for result in results if result['backendRouterName'] == hostname.lower()]", "response": "Finds the backend router Id that matches the given hostname"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting placement group ids which match the given name.", "response": "def _get_id_from_name(self, name):\n        \"\"\"List placement group ids which match the given name.\"\"\"\n        _filter = {\n            'placementGroups': {\n                'name': {'operation': name}\n            }\n        }\n        mask = \"mask[id, name]\"\n        results = self.client.call('Account', 'getPlacementGroups', filter=_filter, mask=mask)\n        return [result['id'] for result in results]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env):\n    manager = CapacityManager(env.client)\n    items = manager.get_create_options()\n\n    items.sort(key=lambda term: int(term['capacity']))\n    table = formatting.Table([\"KeyName\", \"Description\", \"Term\", \"Default Hourly Price Per Instance\"],\n                             title=\"Reserved Capacity Options\")\n    table.align[\"Hourly Price\"] = \"l\"\n    table.align[\"Description\"] = \"l\"\n    table.align[\"KeyName\"] = \"l\"\n    for item in items:\n        table.add_row([\n            item['keyName'], item['description'], item['capacity'], get_price(item)\n        ])\n    env.fout(table)\n\n    regions = manager.get_available_routers()\n    location_table = formatting.Table(['Location', 'POD', 'BackendRouterId'], 'Orderable Locations')\n    for region in regions:\n        for location in region['locations']:\n            for pod in location['location']['pods']:\n                location_table.add_row([region['keyname'], pod['backendRouterName'], pod['backendRouterId']])\n    env.fout(location_table)", "response": "List options for creating Reserved Capacity"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the price with the default locationGroupId", "response": "def get_price(item):\n    \"\"\"Finds the price with the default locationGroupId\"\"\"\n    the_price = \"No Default Pricing\"\n    for price in item.get('prices', []):\n        if not price.get('locationGroupId'):\n            the_price = \"%0.4f\" % float(price['hourlyRecurringFee'])\n    return the_price"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, keyword, package_type):\n    manager = ordering.OrderingManager(env.client)\n    table = formatting.Table(COLUMNS)\n\n    _filter = {'type': {'keyName': {'operation': '!= BLUEMIX_SERVICE'}}}\n    if keyword:\n        _filter['name'] = {'operation': '*= %s' % keyword}\n    if package_type:\n        _filter['type'] = {'keyName': {'operation': package_type}}\n\n    packages = manager.list_packages(filter=_filter)\n\n    for package in packages:\n        table.add_row([\n            package['id'],\n            package['name'],\n            package['keyName'],\n            package['type']['keyName']\n        ])\n    env.fout(table)", "response": "List packages that can be ordered via the placeOrder API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\norder or create a dedicated server.", "response": "def cli(env, **args):\n    \"\"\"Order/create a dedicated server.\"\"\"\n    mgr = SoftLayer.HardwareManager(env.client)\n\n    # Get the SSH keys\n    ssh_keys = []\n    for key in args.get('key'):\n        resolver = SoftLayer.SshKeyManager(env.client).resolve_ids\n        key_id = helpers.resolve_id(resolver, key, 'SshKey')\n        ssh_keys.append(key_id)\n\n    order = {\n        'hostname': args['hostname'],\n        'domain': args['domain'],\n        'size': args['size'],\n        'location': args.get('datacenter'),\n        'ssh_keys': ssh_keys,\n        'post_uri': args.get('postinstall'),\n        'os': args['os'],\n        'hourly': args.get('billing') == 'hourly',\n        'port_speed': args.get('port_speed'),\n        'no_public': args.get('no_public') or False,\n        'extras': args.get('extra'),\n    }\n\n    # Do not create hardware server with --test or --export\n    do_create = not (args['export'] or args['test'])\n\n    output = None\n    if args.get('test'):\n        result = mgr.verify_order(**order)\n\n        table = formatting.Table(['Item', 'cost'])\n        table.align['Item'] = 'r'\n        table.align['cost'] = 'r'\n\n        total = 0.0\n        for price in result['prices']:\n            total += float(price.get('recurringFee', 0.0))\n            rate = \"%.2f\" % float(price['recurringFee'])\n\n            table.add_row([price['item']['description'], rate])\n\n        table.add_row(['Total monthly cost', \"%.2f\" % total])\n        output = []\n        output.append(table)\n        output.append(formatting.FormattedItem(\n            '',\n            ' -- ! Prices reflected here are retail and do not '\n            'take account level discounts and are not guaranteed.'))\n\n    if args['export']:\n        export_file = args.pop('export')\n        template.export_to_template(export_file, args,\n                                    exclude=['wait', 'test'])\n        env.fout('Successfully exported options to a template file.')\n        return\n\n    if do_create:\n        if not (env.skip_confirmations or formatting.confirm(\n                \"This action will incur charges on your account. \"\n                \"Continue?\")):\n            raise exceptions.CLIAbort('Aborting dedicated server order.')\n\n        result = mgr.place_order(**order)\n\n        table = formatting.KeyValueTable(['name', 'value'])\n        table.align['name'] = 'r'\n        table.align['value'] = 'l'\n        table.add_row(['id', result['orderId']])\n        table.add_row(['created', result['orderDate']])\n        output = table\n\n    env.fout(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env):\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    load_balancers = mgr.get_local_lbs()\n\n    table = formatting.Table(['ID',\n                              'VIP Address',\n                              'Location',\n                              'SSL Offload',\n                              'Connections/second',\n                              'Type'])\n\n    table.align['Connections/second'] = 'r'\n\n    for load_balancer in load_balancers:\n        ssl_support = 'Not Supported'\n        if load_balancer['sslEnabledFlag']:\n            if load_balancer['sslActiveFlag']:\n                ssl_support = 'On'\n            else:\n                ssl_support = 'Off'\n        lb_type = 'Standard'\n        if load_balancer['dedicatedFlag']:\n            lb_type = 'Dedicated'\n        elif load_balancer['highAvailabilityFlag']:\n            lb_type = 'HA'\n        table.add_row([\n            'local:%s' % load_balancer['id'],\n            load_balancer['ipAddress']['ipAddress'],\n            load_balancer['loadBalancerHardware'][0]['datacenter']['name'],\n            ssl_support,\n            load_balancer['connectionLimit'],\n            lb_type\n        ])\n\n    env.fout(table)", "response": "List active load balancers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate_host_templates(host_templates,\n                            hardware_ids=None,\n                            virtual_guest_ids=None,\n                            ip_address_ids=None,\n                            subnet_ids=None):\n    \"\"\"Populate the given host_templates array with the IDs provided\n\n    :param host_templates: The array to which host templates will be added\n    :param hardware_ids: A List of SoftLayer_Hardware ids\n    :param virtual_guest_ids: A List of SoftLayer_Virtual_Guest ids\n    :param ip_address_ids: A List of SoftLayer_Network_Subnet_IpAddress ids\n    :param subnet_ids: A List of SoftLayer_Network_Subnet ids\n    \"\"\"\n    if hardware_ids is not None:\n        for hardware_id in hardware_ids:\n            host_templates.append({\n                'objectType': 'SoftLayer_Hardware',\n                'id': hardware_id\n            })\n\n    if virtual_guest_ids is not None:\n        for virtual_guest_id in virtual_guest_ids:\n            host_templates.append({\n                'objectType': 'SoftLayer_Virtual_Guest',\n                'id': virtual_guest_id\n            })\n\n    if ip_address_ids is not None:\n        for ip_address_id in ip_address_ids:\n            host_templates.append({\n                'objectType': 'SoftLayer_Network_Subnet_IpAddress',\n                'id': ip_address_id\n            })\n\n    if subnet_ids is not None:\n        for subnet_id in subnet_ids:\n            host_templates.append({\n                'objectType': 'SoftLayer_Network_Subnet',\n                'id': subnet_id\n            })", "response": "Populate the given host_templates array with the IDs provided."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a product package based on type of storage.", "response": "def get_package(manager, category_code):\n    \"\"\"Returns a product package based on type of storage.\n\n    :param manager: The storage manager which calls this function.\n    :param category_code: Category code of product package.\n    :return: Returns a packaged based on type of storage.\n    \"\"\"\n\n    _filter = utils.NestedDict({})\n    _filter['categories']['categoryCode'] = (\n        utils.query_filter(category_code))\n    _filter['statusCode'] = (utils.query_filter('ACTIVE'))\n\n    packages = manager.client.call(\n        'Product_Package', 'getAllObjects',\n        filter=_filter.to_dict(),\n        mask='id,name,items[prices[categories],attributes]'\n    )\n    if len(packages) == 0:\n        raise ValueError('No packages were found for %s' % category_code)\n    if len(packages) > 1:\n        raise ValueError('More than one package was found for %s'\n                         % category_code)\n\n    return packages[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the id of the location in the datacenter which is specified by location.", "response": "def get_location_id(manager, location):\n    \"\"\"Returns location id\n\n    :param manager: The storage manager which calls this function.\n    :param location: Datacenter short name\n    :return: Returns location id\n    \"\"\"\n    loc_svc = manager.client['Location_Datacenter']\n    datacenters = loc_svc.getDatacenters(mask='mask[longName,id,name]')\n    for datacenter in datacenters:\n        if datacenter['name'] == location:\n            location = datacenter['id']\n            return location\n    raise ValueError('Invalid datacenter name specified.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the price in the given package that has the given category", "response": "def find_price_by_category(package, price_category):\n    \"\"\"Find the price in the given package that has the specified category\n\n    :param package: The AsAService, Enterprise, or Performance product package\n    :param price_category: The price category code to search for\n    :return: Returns the price for the given category, or an error if not found\n    \"\"\"\n    for item in package['items']:\n        price_id = _find_price_id(item['prices'], price_category)\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price with the category, %s\" % price_category)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the space price for the given category size and tier_level.", "response": "def find_ent_space_price(package, category, size, tier_level):\n    \"\"\"Find the space price for the given category, size, and tier\n\n    :param package: The Enterprise (Endurance) product package\n    :param category: The category of space (endurance, replication, snapshot)\n    :param size: The size for which a price is desired\n    :param tier_level: The endurance tier for which a price is desired\n    :return: Returns the matching price, or an error if not found\n    \"\"\"\n    if category == 'snapshot':\n        category_code = 'storage_snapshot_space'\n    elif category == 'replication':\n        category_code = 'performance_storage_replication'\n    else:  # category == 'endurance'\n        category_code = 'performance_storage_space'\n\n    level = ENDURANCE_TIERS.get(tier_level)\n\n    for item in package['items']:\n        if int(item['capacity']) != size:\n            continue\n        price_id = _find_price_id(item['prices'], category_code, 'STORAGE_TIER_LEVEL', level)\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for %s storage space\" % category)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the price in the given package with the specified tier level", "response": "def find_ent_endurance_tier_price(package, tier_level):\n    \"\"\"Find the price in the given package with the specified tier level\n\n    :param package: The Enterprise (Endurance) product package\n    :param tier_level: The endurance tier for which a price is desired\n    :return: Returns the price for the given tier, or an error if not found\n    \"\"\"\n    for item in package['items']:\n        for attribute in item.get('attributes', []):\n            if int(attribute['value']) == ENDURANCE_TIERS.get(tier_level):\n                break\n        else:\n            continue\n\n        price_id = _find_price_id(item['prices'], 'storage_tier_level')\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for endurance tier level\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the tier for the given endurance volume", "response": "def find_endurance_tier_iops_per_gb(volume):\n    \"\"\"Find the tier for the given endurance volume (IOPS per GB)\n\n    :param volume: The volume for which the tier level is desired\n    :return: Returns a float value indicating the IOPS per GB for the volume\n    \"\"\"\n    tier = volume['storageTierLevel']\n    iops_per_gb = 0.25\n\n    if tier == \"LOW_INTENSITY_TIER\":\n        iops_per_gb = 0.25\n    elif tier == \"READHEAVY_TIER\":\n        iops_per_gb = 2\n    elif tier == \"WRITEHEAVY_TIER\":\n        iops_per_gb = 4\n    elif tier == \"10_IOPS_PER_GB\":\n        iops_per_gb = 10\n    else:\n        raise ValueError(\"Could not find tier IOPS per GB for this volume\")\n\n    return iops_per_gb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_perf_space_price(package, size):\n    for item in package['items']:\n        if int(item['capacity']) != size:\n            continue\n\n        price_id = _find_price_id(item['prices'], 'performance_storage_space')\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find performance space price for this volume\")", "response": "Find the price in the given package with the specified size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_perf_iops_price(package, size, iops):\n    for item in package['items']:\n        if int(item['capacity']) != int(iops):\n            continue\n\n        price_id = _find_price_id(item['prices'], 'performance_storage_iops', 'STORAGE_SPACE', size)\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for iops for the given volume\")", "response": "Find the price in the given package with the specified size and IOPS"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the SaaS endurance storage space price for the size and tier", "response": "def find_saas_endurance_space_price(package, size, tier_level):\n    \"\"\"Find the SaaS endurance storage space price for the size and tier\n\n    :param package: The Storage As A Service product package\n    :param size: The volume size for which a price is desired\n    :param tier_level: The endurance tier for which a price is desired\n    :return: Returns the price for the size and tier, or an error if not found\n    \"\"\"\n    if tier_level != 0.25:\n        tier_level = int(tier_level)\n    key_name = 'STORAGE_SPACE_FOR_{0}_IOPS_PER_GB'.format(tier_level)\n    key_name = key_name.replace(\".\", \"_\")\n    for item in package['items']:\n        if key_name not in item['keyName']:\n            continue\n\n        if 'capacityMinimum' not in item or 'capacityMaximum' not in item:\n            continue\n\n        capacity_minimum = int(item['capacityMinimum'])\n        capacity_maximum = int(item['capacityMaximum'])\n        if size < capacity_minimum or size > capacity_maximum:\n            continue\n\n        price_id = _find_price_id(item['prices'], 'performance_storage_space')\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for endurance storage space\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the SaaS storage tier level price for the given tier level", "response": "def find_saas_endurance_tier_price(package, tier_level):\n    \"\"\"Find the SaaS storage tier level price for the specified tier level\n\n    :param package: The Storage As A Service product package\n    :param tier_level: The endurance tier for which a price is desired\n    :return: Returns the price for the given tier, or an error if not found\n    \"\"\"\n    target_capacity = ENDURANCE_TIERS.get(tier_level)\n    for item in package['items']:\n        if 'itemCategory' not in item\\\n                or 'categoryCode' not in item['itemCategory']\\\n                or item['itemCategory']['categoryCode']\\\n                != 'storage_tier_level':\n            continue\n\n        if int(item['capacity']) != target_capacity:\n            continue\n\n        price_id = _find_price_id(item['prices'], 'storage_tier_level')\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for endurance tier level\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the SaaS performance storage space price for the given size", "response": "def find_saas_perform_space_price(package, size):\n    \"\"\"Find the SaaS performance storage space price for the given size\n\n    :param package: The Storage As A Service product package\n    :param size: The volume size for which a price is desired\n    :return: Returns the price for the size and tier, or an error if not found\n    \"\"\"\n    for item in package['items']:\n        if 'itemCategory' not in item\\\n                or 'categoryCode' not in item['itemCategory']\\\n                or item['itemCategory']['categoryCode']\\\n                != 'performance_storage_space':\n            continue\n\n        if 'capacityMinimum' not in item or 'capacityMaximum' not in item:\n            continue\n\n        capacity_minimum = int(item['capacityMinimum'])\n        capacity_maximum = int(item['capacityMaximum'])\n        if size < capacity_minimum or size > capacity_maximum:\n            continue\n\n        key_name = '{0}_{1}_GBS'.format(capacity_minimum, capacity_maximum)\n        if item['keyName'] != key_name:\n            continue\n        price_id = _find_price_id(item['prices'], 'performance_storage_space')\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for performance storage space\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_saas_perform_iops_price(package, size, iops):\n    for item in package['items']:\n        if 'itemCategory' not in item\\\n                or 'categoryCode' not in item['itemCategory']\\\n                or item['itemCategory']['categoryCode']\\\n                != 'performance_storage_iops':\n            continue\n\n        if 'capacityMinimum' not in item or 'capacityMaximum' not in item:\n            continue\n\n        capacity_minimum = int(item['capacityMinimum'])\n        capacity_maximum = int(item['capacityMaximum'])\n        if iops < capacity_minimum or iops > capacity_maximum:\n            continue\n\n        price_id = _find_price_id(item['prices'], 'performance_storage_iops', 'STORAGE_SPACE', size)\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for iops for the given volume\")", "response": "Find the SaaS IOPS price for the specified size and iops"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the price in the SaaS package for the desired snapshot space size tier and IOPS", "response": "def find_saas_snapshot_space_price(package, size, tier=None, iops=None):\n    \"\"\"Find the price in the SaaS package for the desired snapshot space size\n\n    :param package: The product package of the endurance storage type\n    :param size: The snapshot space size for which a price is desired\n    :param tier: The tier of the volume for which space is being ordered\n    :param iops: The IOPS of the volume for which space is being ordered\n    :return: Returns the price for the given size, or an error if not found\n    \"\"\"\n    if tier is not None:\n        target_value = ENDURANCE_TIERS.get(tier)\n        target_restriction_type = 'STORAGE_TIER_LEVEL'\n    else:\n        target_value = iops\n        target_restriction_type = 'IOPS'\n\n    for item in package['items']:\n        if int(item['capacity']) != size:\n            continue\n\n        price_id = _find_price_id(item['prices'], 'storage_snapshot_space', target_restriction_type, target_value)\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for snapshot space\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_saas_replication_price(package, tier=None, iops=None):\n    if tier is not None:\n        target_value = ENDURANCE_TIERS.get(tier)\n        target_item_keyname = 'REPLICATION_FOR_TIERBASED_PERFORMANCE'\n        target_restriction_type = 'STORAGE_TIER_LEVEL'\n    else:\n        target_value = iops\n        target_item_keyname = 'REPLICATION_FOR_IOPSBASED_PERFORMANCE'\n        target_restriction_type = 'IOPS'\n\n    for item in package['items']:\n        if item['keyName'] != target_item_keyname:\n            continue\n\n        price_id = _find_price_id(\n            item['prices'],\n            'performance_storage_replication',\n            target_restriction_type,\n            target_value\n        )\n        if price_id:\n            return price_id\n\n    raise ValueError(\"Could not find price for replicant volume\")", "response": "Find the price in the given package for the desired replicant volume."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the snapshot schedule ID for the given volume and keyname", "response": "def find_snapshot_schedule_id(volume, snapshot_schedule_keyname):\n    \"\"\"Find the snapshot schedule ID for the given volume and keyname\n\n    :param volume: The volume for which the snapshot ID is desired\n    :param snapshot_schedule_keyname: The keyname of the snapshot schedule\n    :return: Returns an int value indicating the volume's snapshot schedule ID\n    \"\"\"\n    for schedule in volume['schedules']:\n        if 'type' in schedule and 'keyname' in schedule['type']:\n            if schedule['type']['keyname'] == snapshot_schedule_keyname:\n                return schedule['id']\n\n    raise ValueError(\"The given snapshot schedule ID was not found for \"\n                     \"the given storage volume\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare_snapshot_order_object(manager, volume, capacity, tier, upgrade):\n    # Ensure the storage volume has not been cancelled\n    if 'billingItem' not in volume:\n        raise exceptions.SoftLayerError(\n            'This volume has been cancelled; unable to order snapshot space')\n\n    # Determine and validate the storage volume's billing item category\n    billing_item_category_code = volume['billingItem']['categoryCode']\n    if billing_item_category_code == 'storage_as_a_service':\n        order_type_is_saas = True\n    elif billing_item_category_code == 'storage_service_enterprise':\n        order_type_is_saas = False\n    else:\n        raise exceptions.SoftLayerError(\n            \"Snapshot space cannot be ordered for a primary volume with a \"\n            \"billing item category code of '%s'\" % billing_item_category_code)\n\n    # Use the volume's billing item category code to get the product package\n    package = get_package(manager, billing_item_category_code)\n\n    # Find prices based on the volume's type and billing item category\n    if order_type_is_saas:  # 'storage_as_a_service' package\n        volume_storage_type = volume['storageType']['keyName']\n        if 'ENDURANCE' in volume_storage_type:\n            if tier is None:\n                tier = find_endurance_tier_iops_per_gb(volume)\n            prices = [find_saas_snapshot_space_price(\n                package, capacity, tier=tier)]\n        elif 'PERFORMANCE' in volume_storage_type:\n            if not _staas_version_is_v2_or_above(volume):\n                raise exceptions.SoftLayerError(\n                    \"Snapshot space cannot be ordered for this performance \"\n                    \"volume since it does not support Encryption at Rest.\")\n            iops = int(volume['provisionedIops'])\n            prices = [find_saas_snapshot_space_price(\n                package, capacity, iops=iops)]\n        else:\n            raise exceptions.SoftLayerError(\n                \"Storage volume does not have a valid storage type \"\n                \"(with an appropriate keyName to indicate the \"\n                \"volume is a PERFORMANCE or an ENDURANCE volume)\")\n    else:  # 'storage_service_enterprise' package\n        if tier is None:\n            tier = find_endurance_tier_iops_per_gb(volume)\n        prices = [find_ent_space_price(package, 'snapshot', capacity, tier)]\n\n    # Currently, these types are valid for snapshot space orders, whether\n    # the base volume's order container was Enterprise or AsAService\n    if upgrade:\n        complex_type = 'SoftLayer_Container_Product_Order_'\\\n                       'Network_Storage_Enterprise_SnapshotSpace_Upgrade'\n    else:\n        complex_type = 'SoftLayer_Container_Product_Order_'\\\n                       'Network_Storage_Enterprise_SnapshotSpace'\n\n    # Determine if hourly billing should be used\n    hourly_billing_flag = utils.lookup(volume, 'billingItem', 'hourlyFlag')\n    if hourly_billing_flag is None:\n        hourly_billing_flag = False\n\n    # Build and return the order object\n    snapshot_space_order = {\n        'complexType': complex_type,\n        'packageId': package['id'],\n        'prices': prices,\n        'quantity': 1,\n        'location': volume['billingItem']['location']['id'],\n        'volumeId': volume['id'],\n        'useHourlyPricing': hourly_billing_flag\n    }\n\n    return snapshot_space_order", "response": "Prepares the snapshot space order object for the snapshot space service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare_volume_order_object(manager, storage_type, location, size,\n                                iops, tier, snapshot_size, service_offering,\n                                volume_type, hourly_billing_flag=False):\n    \"\"\"Prepare the order object which is submitted to the placeOrder() method\n\n    :param manager: The File or Block manager calling this function\n    :param storage_type: \"performance\" or \"endurance\"\n    :param location: Requested datacenter location name for the ordered volume\n    :param size: Desired size of the volume, in GB\n    :param iops: Number of IOPs for a \"Performance\" volume order\n    :param tier: Tier level to use for an \"Endurance\" volume order\n    :param snapshot_size: The size of snapshot space for the volume (optional)\n    :param service_offering: Requested offering package to use for the order\n    :param volume_type: The type of the volume to order ('file' or 'block')\n    :param hourly_billing_flag: Billing type, monthly (False) or hourly (True)\n    :return: Returns the order object for the\n             Product_Order service's placeOrder() method\n    \"\"\"\n    # Ensure the volume storage type is valid\n    if storage_type != 'performance' and storage_type != 'endurance':\n        raise exceptions.SoftLayerError(\n            \"Volume storage type must be either performance or endurance\")\n\n    # Find the ID for the requested location\n    try:\n        location_id = get_location_id(manager, location)\n    except ValueError:\n        raise exceptions.SoftLayerError(\n            \"Invalid datacenter name specified. \"\n            \"Please provide the lower case short name (e.g.: dal09)\")\n\n    # Determine the category code to use for the order (and product package)\n    order_type_is_saas, order_category_code = _get_order_type_and_category(\n        service_offering,\n        storage_type,\n        volume_type\n    )\n\n    # Get the product package for the given category code\n    package = get_package(manager, order_category_code)\n\n    # Based on the storage type and product package, build up the complex type\n    # and array of price codes to include in the order object\n    base_type_name = 'SoftLayer_Container_Product_Order_Network_'\n    if order_type_is_saas:\n        complex_type = base_type_name + 'Storage_AsAService'\n        if storage_type == 'performance':\n            prices = [\n                find_price_by_category(package, order_category_code),\n                find_price_by_category(package, 'storage_' + volume_type),\n                find_saas_perform_space_price(package, size),\n                find_saas_perform_iops_price(package, size, iops)\n            ]\n            if snapshot_size is not None:\n                prices.append(find_saas_snapshot_space_price(\n                    package, snapshot_size, iops=iops))\n        else:  # storage_type == 'endurance'\n            prices = [\n                find_price_by_category(package, order_category_code),\n                find_price_by_category(package, 'storage_' + volume_type),\n                find_saas_endurance_space_price(package, size, tier),\n                find_saas_endurance_tier_price(package, tier)\n            ]\n            if snapshot_size is not None:\n                prices.append(find_saas_snapshot_space_price(\n                    package, snapshot_size, tier=tier))\n    else:  # offering package is enterprise or performance\n        if storage_type == 'performance':\n            if volume_type == 'block':\n                complex_type = base_type_name + 'PerformanceStorage_Iscsi'\n            else:\n                complex_type = base_type_name + 'PerformanceStorage_Nfs'\n            prices = [\n                find_price_by_category(package, order_category_code),\n                find_perf_space_price(package, size),\n                find_perf_iops_price(package, size, iops),\n            ]\n        else:  # storage_type == 'endurance'\n            complex_type = base_type_name + 'Storage_Enterprise'\n            prices = [\n                find_price_by_category(package, order_category_code),\n                find_price_by_category(package, 'storage_' + volume_type),\n                find_ent_space_price(package, 'endurance', size, tier),\n                find_ent_endurance_tier_price(package, tier),\n            ]\n            if snapshot_size is not None:\n                prices.append(find_ent_space_price(\n                    package, 'snapshot', snapshot_size, tier))\n\n    # Build and return the order object\n    order = {\n        'complexType': complex_type,\n        'packageId': package['id'],\n        'prices': prices,\n        'quantity': 1,\n        'location': location_id,\n        'useHourlyPricing': hourly_billing_flag\n    }\n\n    if order_type_is_saas:\n        order['volumeSize'] = size\n        if storage_type == 'performance':\n            order['iops'] = iops\n\n    return order", "response": "Prepares the order object for the given storage type location size tier snapshot size service offering and volume type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare_replicant_order_object(manager, snapshot_schedule, location,\n                                   tier, volume, volume_type):\n    \"\"\"Prepare the order object which is submitted to the placeOrder() method\n\n    :param manager: The File or Block manager calling this function\n    :param snapshot_schedule: The primary volume's snapshot\n                              schedule to use for replication\n    :param location: The location for the ordered replicant volume\n    :param tier: The tier (IOPS per GB) of the primary volume\n    :param volume: The primary volume as a SoftLayer_Network_Storage object\n    :param volume_type: The type of the primary volume ('file' or 'block')\n    :return: Returns the order object for the\n             Product_Order service's placeOrder() method\n    \"\"\"\n    # Ensure the primary volume and snapshot space are not set for cancellation\n    if 'billingItem' not in volume\\\n            or volume['billingItem']['cancellationDate'] != '':\n        raise exceptions.SoftLayerError(\n            'This volume is set for cancellation; '\n            'unable to order replicant volume')\n\n    for child in volume['billingItem']['activeChildren']:\n        if child['categoryCode'] == 'storage_snapshot_space'\\\n                and child['cancellationDate'] != '':\n            raise exceptions.SoftLayerError(\n                'The snapshot space for this volume is set for '\n                'cancellation; unable to order replicant volume')\n\n    # Find the ID for the requested location\n    try:\n        location_id = get_location_id(manager, location)\n    except ValueError:\n        raise exceptions.SoftLayerError(\n            \"Invalid datacenter name specified. \"\n            \"Please provide the lower case short name (e.g.: dal09)\")\n\n    # Get sizes and properties needed for the order\n    volume_size = int(volume['capacityGb'])\n\n    billing_item_category_code = volume['billingItem']['categoryCode']\n    if billing_item_category_code == 'storage_as_a_service':\n        order_type_is_saas = True\n    elif billing_item_category_code == 'storage_service_enterprise':\n        order_type_is_saas = False\n    else:\n        raise exceptions.SoftLayerError(\n            \"A replicant volume cannot be ordered for a primary volume with a \"\n            \"billing item category code of '%s'\" % billing_item_category_code)\n\n    if 'snapshotCapacityGb' in volume:\n        snapshot_size = int(volume['snapshotCapacityGb'])\n    else:\n        raise exceptions.SoftLayerError(\n            \"Snapshot capacity not found for the given primary volume\")\n\n    snapshot_schedule_id = find_snapshot_schedule_id(\n        volume,\n        'SNAPSHOT_' + snapshot_schedule\n    )\n\n    # Use the volume's billing item category code to get the product package\n    package = get_package(manager, billing_item_category_code)\n\n    # Find prices based on the primary volume's type and billing item category\n    if order_type_is_saas:  # 'storage_as_a_service' package\n        complex_type = 'SoftLayer_Container_Product_Order_'\\\n                       'Network_Storage_AsAService'\n        volume_storage_type = volume['storageType']['keyName']\n        if 'ENDURANCE' in volume_storage_type:\n            volume_is_performance = False\n            if tier is None:\n                tier = find_endurance_tier_iops_per_gb(volume)\n            prices = [\n                find_price_by_category(package, billing_item_category_code),\n                find_price_by_category(package, 'storage_' + volume_type),\n                find_saas_endurance_space_price(package, volume_size, tier),\n                find_saas_endurance_tier_price(package, tier),\n                find_saas_snapshot_space_price(\n                    package, snapshot_size, tier=tier),\n                find_saas_replication_price(package, tier=tier)\n            ]\n        elif 'PERFORMANCE' in volume_storage_type:\n            if not _staas_version_is_v2_or_above(volume):\n                raise exceptions.SoftLayerError(\n                    \"A replica volume cannot be ordered for this performance \"\n                    \"volume since it does not support Encryption at Rest.\")\n            volume_is_performance = True\n            iops = int(volume['provisionedIops'])\n            prices = [\n                find_price_by_category(package, billing_item_category_code),\n                find_price_by_category(package, 'storage_' + volume_type),\n                find_saas_perform_space_price(package, volume_size),\n                find_saas_perform_iops_price(package, volume_size, iops),\n                find_saas_snapshot_space_price(\n                    package, snapshot_size, iops=iops),\n                find_saas_replication_price(package, iops=iops)\n            ]\n        else:\n            raise exceptions.SoftLayerError(\n                \"Storage volume does not have a valid storage type \"\n                \"(with an appropriate keyName to indicate the \"\n                \"volume is a PERFORMANCE or an ENDURANCE volume)\")\n    else:  # 'storage_service_enterprise' package\n        complex_type = 'SoftLayer_Container_Product_Order_'\\\n                       'Network_Storage_Enterprise'\n        volume_is_performance = False\n        if tier is None:\n            tier = find_endurance_tier_iops_per_gb(volume)\n        prices = [\n            find_price_by_category(package, billing_item_category_code),\n            find_price_by_category(package, 'storage_' + volume_type),\n            find_ent_space_price(package, 'endurance', volume_size, tier),\n            find_ent_endurance_tier_price(package, tier),\n            find_ent_space_price(package, 'snapshot', snapshot_size, tier),\n            find_ent_space_price(package, 'replication', volume_size, tier)\n        ]\n\n    # Determine if hourly billing should be used\n    hourly_billing_flag = utils.lookup(volume, 'billingItem', 'hourlyFlag')\n    if hourly_billing_flag is None:\n        hourly_billing_flag = False\n\n    # Build and return the order object\n    replicant_order = {\n        'complexType': complex_type,\n        'packageId': package['id'],\n        'prices': prices,\n        'quantity': 1,\n        'location': location_id,\n        'originVolumeId': volume['id'],\n        'originVolumeScheduleId': snapshot_schedule_id,\n        'useHourlyPricing': hourly_billing_flag\n    }\n\n    if order_type_is_saas:\n        replicant_order['volumeSize'] = volume_size\n        if volume_is_performance:\n            replicant_order['iops'] = iops\n\n    return replicant_order", "response": "Prepares the replicant order object for the given resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_duplicate_order_object(manager, origin_volume, iops, tier,\n                                   duplicate_size, duplicate_snapshot_size,\n                                   volume_type, hourly_billing_flag=False):\n    \"\"\"Prepare the duplicate order to submit to SoftLayer_Product::placeOrder()\n\n    :param manager: The File or Block manager calling this function\n    :param origin_volume: The origin volume which is being duplicated\n    :param iops: The IOPS for the duplicate volume (performance)\n    :param tier: The tier level for the duplicate volume (endurance)\n    :param duplicate_size: The requested size for the duplicate volume\n    :param duplicate_snapshot_size: The size for the duplicate snapshot space\n    :param volume_type: The type of the origin volume ('file' or 'block')\n    :param hourly_billing_flag: Billing type, monthly (False) or hourly (True)\n    :return: Returns the order object to be passed to the\n             placeOrder() method of the Product_Order service\n    \"\"\"\n\n    # Verify that the origin volume has not been cancelled\n    if 'billingItem' not in origin_volume:\n        raise exceptions.SoftLayerError(\n            \"The origin volume has been cancelled; \"\n            \"unable to order duplicate volume\")\n\n    # Verify that the origin volume has snapshot space (needed for duplication)\n    if isinstance(utils.lookup(origin_volume, 'snapshotCapacityGb'), str):\n        origin_snapshot_size = int(origin_volume['snapshotCapacityGb'])\n    else:\n        raise exceptions.SoftLayerError(\n            \"Snapshot space not found for the origin volume. \"\n            \"Origin snapshot space is needed for duplication.\")\n\n    # Obtain the datacenter location ID for the duplicate\n    if isinstance(utils.lookup(origin_volume, 'billingItem',\n                               'location', 'id'), int):\n        location_id = origin_volume['billingItem']['location']['id']\n    else:\n        raise exceptions.SoftLayerError(\n            \"Cannot find origin volume's location\")\n\n    # Ensure the origin volume is STaaS v2 or higher\n    # and supports Encryption at Rest\n    if not _staas_version_is_v2_or_above(origin_volume):\n        raise exceptions.SoftLayerError(\n            \"This volume cannot be duplicated since it \"\n            \"does not support Encryption at Rest.\")\n\n    # If no specific snapshot space was requested for the duplicate,\n    # use the origin snapshot space size\n    if duplicate_snapshot_size is None:\n        duplicate_snapshot_size = origin_snapshot_size\n\n    # Use the origin volume size if no size was specified for the duplicate\n    if duplicate_size is None:\n        duplicate_size = origin_volume['capacityGb']\n\n    # Get the appropriate package for the order\n    # ('storage_as_a_service' is currently used for duplicate volumes)\n    package = get_package(manager, 'storage_as_a_service')\n\n    # Determine the IOPS or tier level for the duplicate volume, along with\n    # the type and prices for the order\n    origin_storage_type = origin_volume['storageType']['keyName']\n    if 'PERFORMANCE' in origin_storage_type:\n        volume_is_performance = True\n        if iops is None:\n            iops = int(origin_volume.get('provisionedIops', 0))\n            if iops <= 0:\n                raise exceptions.SoftLayerError(\"Cannot find origin volume's provisioned IOPS\")\n        # Set up the price array for the order\n        prices = [\n            find_price_by_category(package, 'storage_as_a_service'),\n            find_price_by_category(package, 'storage_' + volume_type),\n            find_saas_perform_space_price(package, duplicate_size),\n            find_saas_perform_iops_price(package, duplicate_size, iops),\n        ]\n        # Add the price code for snapshot space as well, unless 0 GB was given\n        if duplicate_snapshot_size > 0:\n            prices.append(find_saas_snapshot_space_price(\n                package, duplicate_snapshot_size, iops=iops))\n\n    elif 'ENDURANCE' in origin_storage_type:\n        volume_is_performance = False\n        if tier is None:\n            tier = find_endurance_tier_iops_per_gb(origin_volume)\n        # Set up the price array for the order\n        prices = [\n            find_price_by_category(package, 'storage_as_a_service'),\n            find_price_by_category(package, 'storage_' + volume_type),\n            find_saas_endurance_space_price(package, duplicate_size, tier),\n            find_saas_endurance_tier_price(package, tier),\n        ]\n        # Add the price code for snapshot space as well, unless 0 GB was given\n        if duplicate_snapshot_size > 0:\n            prices.append(find_saas_snapshot_space_price(\n                package, duplicate_snapshot_size, tier=tier))\n\n    else:\n        raise exceptions.SoftLayerError(\n            \"Origin volume does not have a valid storage type \"\n            \"(with an appropriate keyName to indicate the \"\n            \"volume is a PERFORMANCE or an ENDURANCE volume)\")\n\n    duplicate_order = {\n        'complexType': 'SoftLayer_Container_Product_Order_'\n                       'Network_Storage_AsAService',\n        'packageId': package['id'],\n        'prices': prices,\n        'volumeSize': duplicate_size,\n        'quantity': 1,\n        'location': location_id,\n        'duplicateOriginVolumeId': origin_volume['id'],\n        'useHourlyPricing': hourly_billing_flag\n    }\n\n    if volume_is_performance:\n        duplicate_order['iops'] = iops\n\n    return duplicate_order", "response": "Prepares the duplicate order object for the given origin volume."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing the modification order to submit to the Product_Order service.", "response": "def prepare_modify_order_object(manager, volume, new_iops, new_tier, new_size):\n    \"\"\"Prepare the modification order to submit to SoftLayer_Product::placeOrder()\n\n    :param manager: The File or Block manager calling this function\n    :param volume: The volume which is being modified\n    :param new_iops: The new IOPS for the volume (performance)\n    :param new_tier: The new tier level for the volume (endurance)\n    :param new_size: The requested new size for the volume\n    :return: Returns the order object to be passed to the placeOrder() method of the Product_Order service\n    \"\"\"\n\n    # Verify that the origin volume has not been cancelled\n    if 'billingItem' not in volume:\n        raise exceptions.SoftLayerError(\"The volume has been cancelled; unable to modify volume.\")\n\n    # Ensure the origin volume is STaaS v2 or higher and supports Encryption at Rest\n    if not _staas_version_is_v2_or_above(volume):\n        raise exceptions.SoftLayerError(\"This volume cannot be modified since it does not support Encryption at Rest.\")\n\n    # Get the appropriate package for the order ('storage_as_a_service' is currently used for modifying volumes)\n    package = get_package(manager, 'storage_as_a_service')\n\n    # Based on volume storage type, ensure at least one volume property is being modified,\n    # use current values if some are not specified, and lookup price codes for the order\n    volume_storage_type = volume['storageType']['keyName']\n    if 'PERFORMANCE' in volume_storage_type:\n        volume_is_performance = True\n        if new_size is None and new_iops is None:\n            raise exceptions.SoftLayerError(\"A size or IOPS value must be given to modify this performance volume.\")\n\n        if new_size is None:\n            new_size = volume['capacityGb']\n        elif new_iops is None:\n            new_iops = int(volume.get('provisionedIops', 0))\n            if new_iops <= 0:\n                raise exceptions.SoftLayerError(\"Cannot find volume's provisioned IOPS.\")\n\n        # Set up the prices array for the order\n        prices = [\n            find_price_by_category(package, 'storage_as_a_service'),\n            find_saas_perform_space_price(package, new_size),\n            find_saas_perform_iops_price(package, new_size, new_iops),\n        ]\n\n    elif 'ENDURANCE' in volume_storage_type:\n        volume_is_performance = False\n        if new_size is None and new_tier is None:\n            raise exceptions.SoftLayerError(\"A size or tier value must be given to modify this endurance volume.\")\n\n        if new_size is None:\n            new_size = volume['capacityGb']\n        elif new_tier is None:\n            new_tier = find_endurance_tier_iops_per_gb(volume)\n\n        # Set up the prices array for the order\n        prices = [\n            find_price_by_category(package, 'storage_as_a_service'),\n            find_saas_endurance_space_price(package, new_size, new_tier),\n            find_saas_endurance_tier_price(package, new_tier),\n        ]\n\n    else:\n        raise exceptions.SoftLayerError(\"Volume does not have a valid storage type (with an appropriate \"\n                                        \"keyName to indicate the volume is a PERFORMANCE or an ENDURANCE volume).\")\n\n    modify_order = {\n        'complexType': 'SoftLayer_Container_Product_Order_Network_Storage_AsAService_Upgrade',\n        'packageId': package['id'],\n        'prices': prices,\n        'volume': {'id': volume['id']},\n        'volumeSize': new_size\n    }\n\n    if volume_is_performance:\n        modify_order['iops'] = new_iops\n\n    return modify_order"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, snapshot_id):\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    deleted = block_manager.delete_snapshot(snapshot_id)\n\n    if deleted:\n        click.echo('Snapshot %s deleted' % snapshot_id)", "response": "Deletes a snapshot on a given volume"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a release. Example: $ fab release:3.0.0", "response": "def release(version, force=False):\n    \"\"\"Perform a release. Example:\n\n    $ fab release:3.0.0\n\n    \"\"\"\n    if version.startswith(\"v\"):\n        abort(\"Version should not start with 'v'\")\n    version_str = \"v%s\" % version\n\n    clean()\n\n    local(\"pip install wheel\")\n\n    puts(\" * Uploading to PyPI\")\n    upload()\n\n    puts(\" * Tagging Version %s\" % version_str)\n    force_option = 'f' if force else ''\n    local(\"git tag -%sam \\\"%s\\\" %s\" % (force_option, version_str, version_str))\n\n    puts(\" * Pushing Tag to upstream\")\n    local(\"git push upstream %s\" % version_str)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, volume_id, reason, immediate):\n\n    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n\n    if not (env.skip_confirmations or formatting.no_going_back(volume_id)):\n        raise exceptions.CLIAbort('Aborted')\n\n    cancelled = file_storage_manager.cancel_snapshot_space(\n        volume_id, reason, immediate)\n\n    if cancelled:\n        if immediate:\n            click.echo('File volume with id %s has been marked'\n                       ' for immediate snapshot cancellation' % volume_id)\n        else:\n            click.echo('File volume with id %s has been marked'\n                       ' for snapshot cancellation' % volume_id)\n    else:\n        click.echo('Unable to cancel snapshot space for file volume %s'\n                   % volume_id)", "response": "Cancel existing snapshot space for a given volume."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay details for a specified block storage volume.", "response": "def cli(env, volume_id):\n    \"\"\"Display details for a specified volume.\"\"\"\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    block_volume = block_manager.get_block_volume_details(volume_id)\n    block_volume = utils.NestedDict(block_volume)\n\n    table = formatting.KeyValueTable(['Name', 'Value'])\n    table.align['Name'] = 'r'\n    table.align['Value'] = 'l'\n\n    storage_type = block_volume['storageType']['keyName'].split('_').pop(0)\n    table.add_row(['ID', block_volume['id']])\n    table.add_row(['Username', block_volume['username']])\n    table.add_row(['Type', storage_type])\n    table.add_row(['Capacity (GB)', \"%iGB\" % block_volume['capacityGb']])\n    table.add_row(['LUN Id', \"%s\" % block_volume['lunId']])\n\n    if block_volume.get('provisionedIops'):\n        table.add_row(['IOPs', float(block_volume['provisionedIops'])])\n\n    if block_volume.get('storageTierLevel'):\n        table.add_row([\n            'Endurance Tier',\n            block_volume['storageTierLevel'],\n        ])\n\n    table.add_row([\n        'Data Center',\n        block_volume['serviceResource']['datacenter']['name'],\n    ])\n    table.add_row([\n        'Target IP',\n        block_volume['serviceResourceBackendIpAddress'],\n    ])\n\n    if block_volume['snapshotCapacityGb']:\n        table.add_row([\n            'Snapshot Capacity (GB)',\n            block_volume['snapshotCapacityGb'],\n        ])\n        if 'snapshotSizeBytes' in block_volume['parentVolume']:\n            table.add_row([\n                'Snapshot Used (Bytes)',\n                block_volume['parentVolume']['snapshotSizeBytes'],\n            ])\n\n    table.add_row(['# of Active Transactions', \"%i\"\n                   % block_volume['activeTransactionCount']])\n\n    if block_volume['activeTransactions']:\n        for trans in block_volume['activeTransactions']:\n            if 'transactionStatus' in trans and 'friendlyName' in trans['transactionStatus']:\n                table.add_row(['Ongoing Transaction', trans['transactionStatus']['friendlyName']])\n\n    table.add_row(['Replicant Count', \"%u\" % block_volume.get('replicationPartnerCount', 0)])\n\n    if block_volume['replicationPartnerCount'] > 0:\n        # This if/else temporarily handles a bug in which the SL API\n        # returns a string or object for 'replicationStatus'; it seems that\n        # the type is string for File volumes and object for Block volumes\n        if 'message' in block_volume['replicationStatus']:\n            table.add_row(['Replication Status', \"%s\"\n                           % block_volume['replicationStatus']['message']])\n        else:\n            table.add_row(['Replication Status', \"%s\"\n                           % block_volume['replicationStatus']])\n\n        replicant_list = []\n        for replicant in block_volume['replicationPartners']:\n            replicant_table = formatting.Table(['Replicant ID',\n                                                replicant['id']])\n            replicant_table.add_row([\n                'Volume Name',\n                utils.lookup(replicant, 'username')])\n            replicant_table.add_row([\n                'Target IP',\n                utils.lookup(replicant, 'serviceResourceBackendIpAddress')])\n            replicant_table.add_row([\n                'Data Center',\n                utils.lookup(replicant,\n                             'serviceResource', 'datacenter', 'name')])\n            replicant_table.add_row([\n                'Schedule',\n                utils.lookup(replicant,\n                             'replicationSchedule', 'type', 'keyname')])\n            replicant_list.append(replicant_table)\n        table.add_row(['Replicant Volumes', replicant_list])\n\n    if block_volume.get('originalVolumeSize'):\n        original_volume_info = formatting.Table(['Property', 'Value'])\n        original_volume_info.add_row(['Original Volume Size', block_volume['originalVolumeSize']])\n        if block_volume.get('originalVolumeName'):\n            original_volume_info.add_row(['Original Volume Name', block_volume['originalVolumeName']])\n        if block_volume.get('originalSnapshotName'):\n            original_volume_info.add_row(['Original Snapshot Name', block_volume['originalSnapshotName']])\n        table.add_row(['Original Volume Properties', original_volume_info])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_by_request_id(env, request_id):\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    logs = mgr.get_event_logs_by_request_id(request_id)\n\n    table = formatting.Table(COLUMNS)\n    table.align['metadata'] = \"l\"\n\n    for log in logs:\n        metadata = json.dumps(json.loads(log['metaData']), indent=4, sort_keys=True)\n\n        table.add_row([log['eventName'], log['label'], log['eventCreateDate'], metadata])\n\n    env.fout(table)", "response": "Search for event logs by request id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a support ticket.", "response": "def cli(env, title, subject_id, body, hardware_identifier, virtual_identifier, priority):\n    \"\"\"Create a support ticket.\"\"\"\n    ticket_mgr = SoftLayer.TicketManager(env.client)\n\n    if body is None:\n        body = click.edit('\\n\\n' + ticket.TEMPLATE_MSG)\n    created_ticket = ticket_mgr.create_ticket(\n        title=title,\n        body=body,\n        subject=subject_id,\n        priority=priority)\n\n    if hardware_identifier:\n        hardware_mgr = SoftLayer.HardwareManager(env.client)\n        hardware_id = helpers.resolve_id(hardware_mgr.resolve_ids, hardware_identifier, 'hardware')\n        ticket_mgr.attach_hardware(created_ticket['id'], hardware_id)\n\n    if virtual_identifier:\n        vs_mgr = SoftLayer.VSManager(env.client)\n        vs_id = helpers.resolve_id(vs_mgr.resolve_ids, virtual_identifier, 'VS')\n        ticket_mgr.attach_virtual_server(created_ticket['id'], vs_id)\n\n    env.fout(ticket.get_ticket_results(ticket_mgr, created_ticket['id']))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, volume_id):\n    file_manager = SoftLayer.FileStorageManager(env.client)\n    file_volume = file_manager.get_file_volume_details(volume_id)\n    file_volume = utils.NestedDict(file_volume)\n\n    table = formatting.KeyValueTable(['Name', 'Value'])\n    table.align['Name'] = 'r'\n    table.align['Value'] = 'l'\n\n    storage_type = file_volume['storageType']['keyName'].split('_').pop(0)\n    table.add_row(['ID', file_volume['id']])\n    table.add_row(['Username', file_volume['username']])\n    table.add_row(['Type', storage_type])\n    table.add_row(['Capacity (GB)', \"%iGB\" % file_volume['capacityGb']])\n\n    used_space = int(file_volume['bytesUsed'])\\\n        if file_volume['bytesUsed'] else 0\n    if used_space < (1 << 10):\n        table.add_row(['Used Space', \"%dB\" % used_space])\n    elif used_space < (1 << 20):\n        table.add_row(['Used Space', \"%dKB\" % (used_space / (1 << 10))])\n    elif used_space < (1 << 30):\n        table.add_row(['Used Space', \"%dMB\" % (used_space / (1 << 20))])\n    else:\n        table.add_row(['Used Space', \"%dGB\" % (used_space / (1 << 30))])\n\n    if file_volume.get('provisionedIops'):\n        table.add_row(['IOPs', float(file_volume['provisionedIops'])])\n\n    if file_volume.get('storageTierLevel'):\n        table.add_row([\n            'Endurance Tier',\n            file_volume['storageTierLevel'],\n        ])\n\n    table.add_row([\n        'Data Center',\n        file_volume['serviceResource']['datacenter']['name'],\n    ])\n    table.add_row([\n        'Target IP',\n        file_volume['serviceResourceBackendIpAddress'],\n    ])\n\n    if file_volume['fileNetworkMountAddress']:\n        table.add_row([\n            'Mount Address',\n            file_volume['fileNetworkMountAddress'],\n        ])\n\n    if file_volume['snapshotCapacityGb']:\n        table.add_row([\n            'Snapshot Capacity (GB)',\n            file_volume['snapshotCapacityGb'],\n        ])\n        if 'snapshotSizeBytes' in file_volume['parentVolume']:\n            table.add_row([\n                'Snapshot Used (Bytes)',\n                file_volume['parentVolume']['snapshotSizeBytes'],\n            ])\n\n    table.add_row(['# of Active Transactions', \"%i\"\n                   % file_volume['activeTransactionCount']])\n\n    if file_volume['activeTransactions']:\n        for trans in file_volume['activeTransactions']:\n            if 'transactionStatus' in trans and 'friendlyName' in trans['transactionStatus']:\n                table.add_row(['Ongoing Transaction', trans['transactionStatus']['friendlyName']])\n\n    table.add_row(['Replicant Count', \"%u\" % file_volume.get('replicationPartnerCount', 0)])\n\n    if file_volume['replicationPartnerCount'] > 0:\n        # This if/else temporarily handles a bug in which the SL API\n        # returns a string or object for 'replicationStatus'; it seems that\n        # the type is string for File volumes and object for Block volumes\n        if 'message' in file_volume['replicationStatus']:\n            table.add_row(['Replication Status', \"%s\"\n                           % file_volume['replicationStatus']['message']])\n        else:\n            table.add_row(['Replication Status', \"%s\"\n                           % file_volume['replicationStatus']])\n\n        replicant_list = []\n        for replicant in file_volume['replicationPartners']:\n            replicant_table = formatting.Table(['Replicant ID',\n                                                replicant['id']])\n            replicant_table.add_row([\n                'Volume Name',\n                utils.lookup(replicant, 'username')])\n            replicant_table.add_row([\n                'Target IP',\n                utils.lookup(replicant, 'serviceResourceBackendIpAddress')])\n            replicant_table.add_row([\n                'Data Center',\n                utils.lookup(replicant,\n                             'serviceResource', 'datacenter', 'name')])\n            replicant_table.add_row([\n                'Schedule',\n                utils.lookup(replicant,\n                             'replicationSchedule', 'type', 'keyname')])\n            replicant_list.append(replicant_table)\n        table.add_row(['Replicant Volumes', replicant_list])\n\n    if file_volume.get('originalVolumeSize'):\n        original_volume_info = formatting.Table(['Property', 'Value'])\n        original_volume_info.add_row(['Original Volume Size', file_volume['originalVolumeSize']])\n        if file_volume.get('originalVolumeName'):\n            original_volume_info.add_row(['Original Volume Name', file_volume['originalVolumeName']])\n        if file_volume.get('originalSnapshotName'):\n            original_volume_info.add_row(['Original Snapshot Name', file_volume['originalSnapshotName']])\n        table.add_row(['Original Volume Properties', original_volume_info])\n\n    env.fout(table)", "response": "Display details for a specified file storage volume."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, volume_id, replicant_id):\n    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n\n    success = file_storage_manager.failback_from_replicant(\n        volume_id,\n        replicant_id\n    )\n\n    if success:\n        click.echo(\"Failback from replicant is now in progress.\")\n    else:\n        click.echo(\"Failback operation could not be initiated.\")", "response": "Failback a file volume from the given replicant volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, volume_id, snapshot_id):\n    file_manager = SoftLayer.FileStorageManager(env.client)\n    success = file_manager.restore_from_snapshot(volume_id, snapshot_id)\n\n    if success:\n        click.echo('File volume %s is being restored using snapshot %s'\n                   % (volume_id, snapshot_id))", "response": "Restore a file volume using a given snapshot."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env, storage_type, size, iops, tier,\n        location, snapshot_size, service_offering, billing):\n    \"\"\"Order a file storage volume.\n\n    Valid size and iops options can be found here:\n    https://console.bluemix.net/docs/infrastructure/FileStorage/index.html#provisioning\n    \"\"\"\n    file_manager = SoftLayer.FileStorageManager(env.client)\n    storage_type = storage_type.lower()\n\n    hourly_billing_flag = False\n    if billing.lower() == \"hourly\":\n        hourly_billing_flag = True\n\n    if service_offering != 'storage_as_a_service':\n        click.secho('{} is a legacy storage offering'.format(service_offering), fg='red')\n        if hourly_billing_flag:\n            raise exceptions.CLIAbort(\n                'Hourly billing is only available for the storage_as_a_service service offering'\n            )\n\n    if storage_type == 'performance':\n        if iops is None:\n            raise exceptions.CLIAbort('Option --iops required with Performance')\n\n        if service_offering == 'performance' and snapshot_size is not None:\n            raise exceptions.CLIAbort(\n                '--snapshot-size is not available for performance service offerings. '\n                'Use --service-offering storage_as_a_service'\n            )\n\n        try:\n            order = file_manager.order_file_volume(\n                storage_type=storage_type,\n                location=location,\n                size=size,\n                iops=iops,\n                snapshot_size=snapshot_size,\n                service_offering=service_offering,\n                hourly_billing_flag=hourly_billing_flag\n            )\n        except ValueError as ex:\n            raise exceptions.ArgumentError(str(ex))\n\n    if storage_type == 'endurance':\n        if tier is None:\n            raise exceptions.CLIAbort(\n                'Option --tier required with Endurance in IOPS/GB [0.25,2,4,10]'\n            )\n\n        try:\n            order = file_manager.order_file_volume(\n                storage_type=storage_type,\n                location=location,\n                size=size,\n                tier_level=float(tier),\n                snapshot_size=snapshot_size,\n                service_offering=service_offering,\n                hourly_billing_flag=hourly_billing_flag\n            )\n        except ValueError as ex:\n            raise exceptions.ArgumentError(str(ex))\n\n    if 'placedOrder' in order.keys():\n        click.echo(\"Order #{0} placed successfully!\".format(\n            order['placedOrder']['id']))\n        for item in order['placedOrder']['items']:\n            click.echo(\" > %s\" % item['description'])\n    else:\n        click.echo(\"Order could not be placed! Please verify your options and try again.\")", "response": "Order a file storage volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, **args):\n    create_args = _parse_create_args(env.client, args)\n\n    create_args['primary_disk'] = args.get('primary_disk')\n    manager = CapacityManager(env.client)\n    capacity_id = args.get('capacity_id')\n    test = args.get('test')\n\n    result = manager.create_guest(capacity_id, test, create_args)\n\n    env.fout(_build_receipt(result, test))", "response": "Allows for creating a virtual guest in a reserved capacity."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env, identifier, out_file):\n\n    mgr = SoftLayer.SshKeyManager(env.client)\n\n    key_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'SshKey')\n\n    key = mgr.get_key(key_id)\n\n    if out_file:\n        with open(path.expanduser(out_file), 'w') as pub_file:\n            pub_file.write(key['key'])\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.add_row(['id', key['id']])\n    table.add_row(['label', key.get('label')])\n    table.add_row(['notes', key.get('notes', '-')])\n    env.fout(table)", "response": "Prints out an SSH key to the screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_endpoints(self):\n        _filter = {\n            'hubNetworkStorage': {'vendorName': {'operation': 'Swift'}},\n        }\n        endpoints = []\n        network_storage = self.client.call('Account',\n                                           'getHubNetworkStorage',\n                                           mask=ENDPOINT_MASK,\n                                           limit=1,\n                                           filter=_filter)\n        if network_storage:\n            for node in network_storage['storageNodes']:\n                endpoints.append({\n                    'datacenter': node['datacenter'],\n                    'public': node['frontendIpAddress'],\n                    'private': node['backendIpAddress'],\n                })\n\n        return endpoints", "response": "Lists the known object storage endpoints."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the object storage credential.", "response": "def delete_credential(self, identifier, credential_id=None):\n        \"\"\"Delete the object storage credential.\n\n        :param int id: The object storage account identifier.\n        :param int credential_id: The credential id to be deleted.\n\n        \"\"\"\n        credential = {\n            'id': credential_id\n        }\n\n        return self.client.call('SoftLayer_Network_Storage_Hub_Cleversafe_Account', 'credentialDelete',\n                                credential, id=identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, user, template):\n    mgr = SoftLayer.UserManager(env.client)\n    user_id = helpers.resolve_id(mgr.resolve_ids, user, 'username')\n\n    user_template = {}\n    if template is not None:\n        try:\n            template_object = json.loads(template)\n            for key in template_object:\n                user_template[key] = template_object[key]\n        except ValueError as ex:\n            raise exceptions.ArgumentError(\"Unable to parse --template. %s\" % ex)\n\n    result = mgr.edit_user(user_id, user_template)\n    if result:\n        click.secho(\"%s updated successfully\" % (user), fg='green')\n    else:\n        click.secho(\"Failed to update %s\" % (user), fg='red')", "response": "Edit a Users details."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, ip_address):\n\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    addr_info = mgr.ip_lookup(ip_address)\n\n    if not addr_info:\n        raise exceptions.CLIAbort('Not found')\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['id', addr_info['id']])\n    table.add_row(['ip', addr_info['ipAddress']])\n\n    subnet_table = formatting.KeyValueTable(['name', 'value'])\n    subnet_table.align['name'] = 'r'\n    subnet_table.align['value'] = 'l'\n    subnet_table.add_row(['id', addr_info['subnet']['id']])\n    subnet_table.add_row(['identifier',\n                          '%s/%s' % (addr_info['subnet']['networkIdentifier'],\n                                     str(addr_info['subnet']['cidr']))])\n    subnet_table.add_row(['netmask', addr_info['subnet']['netmask']])\n    if addr_info['subnet'].get('gateway'):\n        subnet_table.add_row(['gateway', addr_info['subnet']['gateway']])\n    subnet_table.add_row(['type', addr_info['subnet'].get('subnetType')])\n\n    table.add_row(['subnet', subnet_table])\n\n    if addr_info.get('virtualGuest') or addr_info.get('hardware'):\n        device_table = formatting.KeyValueTable(['name', 'value'])\n        device_table.align['name'] = 'r'\n        device_table.align['value'] = 'l'\n        if addr_info.get('virtualGuest'):\n            device = addr_info['virtualGuest']\n            device_type = 'vs'\n        else:\n            device = addr_info['hardware']\n            device_type = 'server'\n        device_table.add_row(['id', device['id']])\n        device_table.add_row(['name', device['fullyQualifiedDomainName']])\n        device_table.add_row(['type', device_type])\n        table.add_row(['device', device_table])\n    env.fout(table)", "response": "Find an IP address and display its subnet and device info."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env, name, note, os_code, uri, ibm_api_key, root_key_crn, wrapped_dek,\n        cloud_init, byol, is_encrypted):\n    \"\"\"Import an image.\n\n    The URI for an object storage object (.vhd/.iso file) of the format:\n    swift://<objectStorageAccount>@<cluster>/<container>/<objectPath>\n    or cos://<regionName>/<bucketName>/<objectPath> if using IBM Cloud\n    Object Storage\n    \"\"\"\n\n    image_mgr = SoftLayer.ImageManager(env.client)\n    result = image_mgr.import_image_from_uri(\n        name=name,\n        note=note,\n        os_code=os_code,\n        uri=uri,\n        ibm_api_key=ibm_api_key,\n        root_key_crn=root_key_crn,\n        wrapped_dek=wrapped_dek,\n        cloud_init=cloud_init,\n        byol=byol,\n        is_encrypted=is_encrypted\n    )\n\n    if not result:\n        raise exceptions.CLIAbort(\"Failed to import Image\")\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n    table.add_row(['name', result['name']])\n    table.add_row(['id', result['id']])\n    table.add_row(['created', result['createDate']])\n    table.add_row(['guid', result['globalIdentifier']])\n    env.fout(table)", "response": "Import an image.\n\n    The URI for an object storage object (.vhd/.iso file) of the format:\n    swift://<objectStorageAccount>@<cluster>/<container>/<objectPath>\n    or cos://<regionName>/<bucketName>/<objectPath> if using IBM Cloud\n    Object Storage"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating tunnel context properties.", "response": "def cli(env, context_id, friendly_name, remote_peer, preshared_key,\n        phase1_auth, phase1_crypto, phase1_dh, phase1_key_ttl, phase2_auth,\n        phase2_crypto, phase2_dh, phase2_forward_secrecy, phase2_key_ttl):\n    \"\"\"Update tunnel context properties.\n\n    Updates are made atomically, so either all are accepted or none are.\n\n    Key life values must be in the range 120-172800.\n\n    Phase 2 perfect forward secrecy must be in the range 0-1.\n\n    A separate configuration request should be made to realize changes on\n    network devices.\n    \"\"\"\n    manager = SoftLayer.IPSECManager(env.client)\n    succeeded = manager.update_tunnel_context(\n        context_id,\n        friendly_name=friendly_name,\n        remote_peer=remote_peer,\n        preshared_key=preshared_key,\n        phase1_auth=phase1_auth,\n        phase1_crypto=phase1_crypto,\n        phase1_dh=phase1_dh,\n        phase1_key_ttl=phase1_key_ttl,\n        phase2_auth=phase2_auth,\n        phase2_crypto=phase2_crypto,\n        phase2_dh=phase2_dh,\n        phase2_forward_secrecy=phase2_forward_secrecy,\n        phase2_key_ttl=phase2_key_ttl\n    )\n    if succeeded:\n        env.out('Updated context #{}'.format(context_id))\n    else:\n        raise CLIHalt('Failed to update context #{}'.format(context_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_internal_subnet(self, context_id, subnet_id):\n        return self.context.addPrivateSubnetToNetworkTunnel(subnet_id,\n                                                            id=context_id)", "response": "Add an internal subnet to a tunnel context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a remote subnet to a tunnel context.", "response": "def add_remote_subnet(self, context_id, subnet_id):\n        \"\"\"Adds a remote subnet to a tunnel context.\n\n        :param int context_id: The id-value representing the context instance.\n        :param int subnet_id: The id-value representing the remote subnet.\n        :return bool: True if remote subnet addition was successful.\n        \"\"\"\n        return self.context.addCustomerSubnetToNetworkTunnel(subnet_id,\n                                                             id=context_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_service_subnet(self, context_id, subnet_id):\n        return self.context.addServiceSubnetToNetworkTunnel(subnet_id,\n                                                            id=context_id)", "response": "Adds a service subnet to a tunnel context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_remote_subnet(self, account_id, identifier, cidr):\n        return self.remote_subnet.createObject({\n            'accountId': account_id,\n            'cidr': cidr,\n            'networkIdentifier': identifier\n        })", "response": "Creates a remote subnet on the given account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_translation(self, context_id, static_ip, remote_ip, notes):\n        return self.context.createAddressTranslation({\n            'customerIpAddress': remote_ip,\n            'internalIpAddress': static_ip,\n            'notes': notes\n        }, id=context_id)", "response": "Creates an address translation entry on a tunnel context."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the network tunnel context instance.", "response": "def get_tunnel_context(self, context_id, **kwargs):\n        \"\"\"Retrieves the network tunnel context instance.\n\n        :param int context_id: The id-value representing the context instance.\n        :return dict: Mapping of properties for the tunnel context.\n        :raise SoftLayerAPIError: If a context cannot be found.\n        \"\"\"\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n        _filter['networkTunnelContexts']['id'] = utils.query_filter(context_id)\n\n        kwargs['filter'] = _filter.to_dict()\n        contexts = self.account.getNetworkTunnelContexts(**kwargs)\n        if len(contexts) == 0:\n            raise SoftLayerAPIError('SoftLayer_Exception_ObjectNotFound',\n                                    'Unable to find object with id of \\'{}\\''\n                                    .format(context_id))\n        return contexts[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_translation(self, context_id, translation_id):\n        translation = next((x for x in self.get_translations(context_id)\n                            if x['id'] == translation_id), None)\n        if translation is None:\n            raise SoftLayerAPIError('SoftLayer_Exception_ObjectNotFound',\n                                    'Unable to find object with id of \\'{}\\''\n                                    .format(translation_id))\n        return translation", "response": "Retrieves a translation entry for the given id values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve all the translations associated with the given tunnel context.", "response": "def get_translations(self, context_id):\n        \"\"\"Retrieves all translation entries for a tunnel context.\n\n        :param int context_id: The id-value representing the context instance.\n        :return list(dict): Translations associated with the given context\n        \"\"\"\n        _mask = ('[mask[addressTranslations[customerIpAddressRecord,'\n                 'internalIpAddressRecord]]]')\n        context = self.get_tunnel_context(context_id, mask=_mask)\n        # Pull the internal and remote IP addresses into the translation\n        for translation in context.get('addressTranslations', []):\n            remote_ip = translation.get('customerIpAddressRecord', {})\n            internal_ip = translation.get('internalIpAddressRecord', {})\n            translation['customerIpAddress'] = remote_ip.get('ipAddress', '')\n            translation['internalIpAddress'] = internal_ip.get('ipAddress', '')\n            translation.pop('customerIpAddressRecord', None)\n            translation.pop('internalIpAddressRecord', None)\n        return context['addressTranslations']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove an internal subnet from a tunnel context.", "response": "def remove_internal_subnet(self, context_id, subnet_id):\n        \"\"\"Remove an internal subnet from a tunnel context.\n\n        :param int context_id: The id-value representing the context instance.\n        :param int subnet_id: The id-value representing the internal subnet.\n        :return bool: True if internal subnet removal was successful.\n        \"\"\"\n        return self.context.removePrivateSubnetFromNetworkTunnel(subnet_id,\n                                                                 id=context_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_remote_subnet(self, context_id, subnet_id):\n        return self.context.removeCustomerSubnetFromNetworkTunnel(subnet_id,\n                                                                  id=context_id)", "response": "Removes a remote subnet from a tunnel context."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a service subnet from a tunnel context.", "response": "def remove_service_subnet(self, context_id, subnet_id):\n        \"\"\"Removes a service subnet from a tunnel context.\n\n        :param int context_id: The id-value representing the context instance.\n        :param int subnet_id: The id-value representing the service subnet.\n        :return bool: True if service subnet removal was successful.\n        \"\"\"\n        return self.context.removeServiceSubnetFromNetworkTunnel(subnet_id,\n                                                                 id=context_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_translation(self, context_id, translation_id):\n        return self.context.deleteAddressTranslation(translation_id,\n                                                     id=context_id)", "response": "Removes a translation entry from a tunnel context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate an address translation entry with the given values.", "response": "def update_translation(self, context_id, translation_id, static_ip=None,\n                           remote_ip=None, notes=None):\n        \"\"\"Updates an address translation entry using the given values.\n\n        :param int context_id: The id-value representing the context instance.\n        :param dict template: A key-value mapping of translation properties.\n        :param string static_ip: The static IP address value to update.\n        :param string remote_ip: The remote IP address value to update.\n        :param string notes: The notes value to update.\n        :return bool: True if the update was successful.\n        \"\"\"\n        translation = self.get_translation(context_id, translation_id)\n\n        if static_ip is not None:\n            translation['internalIpAddress'] = static_ip\n            translation.pop('internalIpAddressId', None)\n        if remote_ip is not None:\n            translation['customerIpAddress'] = remote_ip\n            translation.pop('customerIpAddressId', None)\n        if notes is not None:\n            translation['notes'] = notes\n        self.context.editAddressTranslation(translation, id=context_id)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_tunnel_context(self, context_id, friendly_name=None,\n                              remote_peer=None, preshared_key=None,\n                              phase1_auth=None, phase1_crypto=None,\n                              phase1_dh=None, phase1_key_ttl=None,\n                              phase2_auth=None, phase2_crypto=None,\n                              phase2_dh=None, phase2_forward_secrecy=None,\n                              phase2_key_ttl=None):\n        \"\"\"Updates a tunnel context using the given values.\n\n        :param string context_id: The id-value representing the context.\n        :param string friendly_name: The friendly name value to update.\n        :param string remote_peer: The remote peer IP address value to update.\n        :param string preshared_key: The preshared key value to update.\n        :param string phase1_auth: The phase 1 authentication value to update.\n        :param string phase1_crypto: The phase 1 encryption value to update.\n        :param string phase1_dh: The phase 1 diffie hellman group value\n               to update.\n        :param string phase1_key_ttl: The phase 1 key life value to update.\n        :param string phase2_auth: The phase 2 authentication value to update.\n        :param string phase2_crypto: The phase 2 encryption value to update.\n        :param string phase2_df: The phase 2 diffie hellman group value\n               to update.\n        :param string phase2_forward_secriecy: The phase 2 perfect forward\n               secrecy value to update.\n        :param string phase2_key_ttl: The phase 2 key life value to update.\n        :return bool: True if the update was successful.\n        \"\"\"\n        context = self.get_tunnel_context(context_id)\n\n        if friendly_name is not None:\n            context['friendlyName'] = friendly_name\n        if remote_peer is not None:\n            context['customerPeerIpAddress'] = remote_peer\n        if preshared_key is not None:\n            context['presharedKey'] = preshared_key\n        if phase1_auth is not None:\n            context['phaseOneAuthentication'] = phase1_auth\n        if phase1_crypto is not None:\n            context['phaseOneEncryption'] = phase1_crypto\n        if phase1_dh is not None:\n            context['phaseOneDiffieHellmanGroup'] = phase1_dh\n        if phase1_key_ttl is not None:\n            context['phaseOneKeylife'] = phase1_key_ttl\n        if phase2_auth is not None:\n            context['phaseTwoAuthentication'] = phase2_auth\n        if phase2_crypto is not None:\n            context['phaseTwoEncryption'] = phase2_crypto\n        if phase2_dh is not None:\n            context['phaseTwoDiffieHellmanGroup'] = phase2_dh\n        if phase2_forward_secrecy is not None:\n            context['phaseTwoPerfectForwardSecrecy'] = phase2_forward_secrecy\n        if phase2_key_ttl is not None:\n            context['phaseTwoKeylife'] = phase2_key_ttl\n        return self.context.editObject(context, id=context_id)", "response": "Updates the tunnel context with the given values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(env, identifier):\n\n    mgr = SoftLayer.ObjectStorageManager(env.client)\n    credential = mgr.create_credential(identifier)\n    table = formatting.Table(['id', 'password', 'username', 'type_name'])\n    table.sortby = 'id'\n    table.add_row([\n        credential['id'],\n        credential['password'],\n        credential['username'],\n        credential['type']['name']\n    ])\n\n    env.fout(table)", "response": "Create credentials for an IBM Cloud Object Storage Account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a subnet to an IPSEC tunnel context.", "response": "def cli(env, context_id, subnet_id, subnet_type, network_identifier):\n    \"\"\"Add a subnet to an IPSEC tunnel context.\n\n    A subnet id may be specified to link to the existing tunnel context.\n\n    Otherwise, a network identifier in CIDR notation should be specified,\n    indicating that a subnet resource should first be created before associating\n    it with the tunnel context. Note that this is only supported for remote\n    subnets, which are also deleted upon failure to attach to a context.\n\n    A separate configuration request should be made to realize changes on\n    network devices.\n    \"\"\"\n    create_remote = False\n    if subnet_id is None:\n        if network_identifier is None:\n            raise ArgumentError('Either a network identifier or subnet id '\n                                'must be provided.')\n        if subnet_type != 'remote':\n            raise ArgumentError('Unable to create {} subnets'\n                                .format(subnet_type))\n        create_remote = True\n\n    manager = SoftLayer.IPSECManager(env.client)\n    context = manager.get_tunnel_context(context_id)\n\n    if create_remote:\n        subnet = manager.create_remote_subnet(context['accountId'],\n                                              identifier=network_identifier[0],\n                                              cidr=network_identifier[1])\n        subnet_id = subnet['id']\n        env.out('Created subnet {}/{} #{}'\n                .format(network_identifier[0],\n                        network_identifier[1],\n                        subnet_id))\n\n    succeeded = False\n    if subnet_type == 'internal':\n        succeeded = manager.add_internal_subnet(context_id, subnet_id)\n    elif subnet_type == 'remote':\n        succeeded = manager.add_remote_subnet(context_id, subnet_id)\n    elif subnet_type == 'service':\n        succeeded = manager.add_service_subnet(context_id, subnet_id)\n\n    if succeeded:\n        env.out('Added {} subnet #{}'.format(subnet_type, subnet_id))\n    else:\n        raise CLIHalt('Failed to add {} subnet #{}'\n                      .format(subnet_type, subnet_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, ack_all):\n\n    manager = AccountManager(env.client)\n    events = manager.get_upcoming_events()\n\n    if ack_all:\n        for event in events:\n            result = manager.ack_event(event['id'])\n            event['acknowledgedFlag'] = result\n    env.fout(event_table(events))", "response": "Summary and acknowledgement of upcoming and ongoing maintenance events"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats a table for events.", "response": "def event_table(events):\n    \"\"\"Formats a table for events\"\"\"\n    table = formatting.Table([\n        \"Id\",\n        \"Start Date\",\n        \"End Date\",\n        \"Subject\",\n        \"Status\",\n        \"Acknowledged\",\n        \"Updates\",\n        \"Impacted Resources\"\n    ], title=\"Upcoming Events\")\n    table.align['Subject'] = 'l'\n    table.align['Impacted Resources'] = 'l'\n    for event in events:\n        table.add_row([\n            event.get('id'),\n            utils.clean_time(event.get('startDate')),\n            utils.clean_time(event.get('endDate')),\n            # Some subjects can have \\r\\n for some reason.\n            utils.clean_splitlines(event.get('subject')),\n            utils.lookup(event, 'statusCode', 'name'),\n            event.get('acknowledgedFlag'),\n            event.get('updateCount'),\n            event.get('impactedResourceCount')\n        ])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange a password for a volume s access.", "response": "def cli(env, access_id, password):\n    \"\"\"Changes a password for a volume's access.\n\n    access id is the allowed_host_id from slcli block access-list\n    \"\"\"\n\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n\n    result = block_manager.set_credential_password(access_id=access_id, password=password)\n\n    if result:\n        click.echo('Password updated for %s' % access_id)\n    else:\n        click.echo('FAILED updating password for %s' % access_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetails a CDN Account.", "response": "def cli(env, account_id):\n    \"\"\"Detail a CDN Account.\"\"\"\n\n    manager = SoftLayer.CDNManager(env.client)\n    account = manager.get_account(account_id)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['id', account['id']])\n    table.add_row(['account_name', account['cdnAccountName']])\n    table.add_row(['type', account['cdnSolutionName']])\n    table.add_row(['status', account['status']['name']])\n    table.add_row(['created', account['createDate']])\n    table.add_row(['notes',\n                   account.get('cdnAccountNote', formatting.blank())])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lookup(dic, key, *keys):\n    if keys:\n        return lookup(dic.get(key, {}), keys[0], *keys[1:])\n    return dic.get(key)", "response": "A generic dictionary access helper."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_filter(query):\n    try:\n        return {'operation': int(query)}\n    except ValueError:\n        pass\n\n    if isinstance(query, string_types):\n        query = query.strip()\n        for operation in KNOWN_OPERATIONS:\n            if query.startswith(operation):\n                query = \"%s %s\" % (operation, query[len(operation):].strip())\n                return {'operation': query}\n        if query.startswith('*') and query.endswith('*'):\n            query = \"*= %s\" % query.strip('*')\n        elif query.startswith('*'):\n            query = \"$= %s\" % query.strip('*')\n        elif query.endswith('*'):\n            query = \"^= %s\" % query.strip('*')\n        else:\n            query = \"_= %s\" % query\n\n    return {'operation': query}", "response": "Translate a query - style string to a filter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying filters given start and end date.", "response": "def query_filter_date(start, end):\n    \"\"\"Query filters given start and end date.\n\n    :param start:YY-MM-DD\n    :param end: YY-MM-DD\n    \"\"\"\n    sdate = datetime.datetime.strptime(start, \"%Y-%m-%d\")\n    edate = datetime.datetime.strptime(end, \"%Y-%m-%d\")\n    startdate = \"%s/%s/%s\" % (sdate.month, sdate.day, sdate.year)\n    enddate = \"%s/%s/%s\" % (edate.month, edate.day, edate.year)\n    return {\n        'operation': 'betweenDate',\n        'options': [\n            {'name': 'startDate', 'value': [startdate + ' 0:0:0']},\n            {'name': 'endDate', 'value': [enddate + ' 0:0:0']}\n        ]\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_event_log_date(date_string, utc):\n    user_date_format = \"%m/%d/%Y\"\n\n    user_date = datetime.datetime.strptime(date_string, user_date_format)\n    dirty_time = user_date.isoformat()\n\n    if utc is None:\n        utc = \"+0000\"\n\n    iso_time_zone = utc[:3] + ':' + utc[3:]\n    cleaned_time = \"{}.000000{}\".format(dirty_time, iso_time_zone)\n\n    return cleaned_time", "response": "Formats the given date in the format that the SoftLayer_EventLog object likes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery filter that SoftLayer_EventLog likes at least one date between two given dates.", "response": "def event_log_filter_between_date(start, end, utc):\n    \"\"\"betweenDate Query filter that SoftLayer_EventLog likes\n\n    :param string start: lower bound date in mm/dd/yyyy format\n    :param string end: upper bound date in mm/dd/yyyy format\n    :param string utc: utc offset. Defaults to '+0000'\n    \"\"\"\n    return {\n        'operation': 'betweenDate',\n        'options': [\n            {'name': 'startDate', 'value': [format_event_log_date(start, utc)]},\n            {'name': 'endDate', 'value': [format_event_log_date(end, utc)]}\n        ]\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves IDs given a list of functions.", "response": "def resolve_ids(identifier, resolvers):\n    \"\"\"Resolves IDs given a list of functions.\n\n    :param string identifier: identifier string\n    :param list resolvers: a list of functions\n    :returns list:\n    \"\"\"\n\n    # Before doing anything, let's see if this is an integer\n    try:\n        return [int(identifier)]\n    except ValueError:\n        pass  # It was worth a shot\n\n    # This looks like a globalIdentifier (UUID)\n    if len(identifier) == 36 and UUID_RE.match(identifier):\n        return [identifier]\n\n    for resolver in resolvers:\n        ids = resolver(identifier)\n        if ids:\n            return ids\n\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_ready(instance, pending=False):\n\n    last_reload = lookup(instance, 'lastOperatingSystemReload', 'id')\n    active_transaction = lookup(instance, 'activeTransaction', 'id')\n\n    reloading = all((\n        active_transaction,\n        last_reload,\n        last_reload == active_transaction,\n    ))\n    outstanding = False\n    if pending:\n        outstanding = active_transaction\n    if instance.get('provisionDate') and not reloading and not outstanding:\n        return True\n    return False", "response": "Returns True if the instance is ready to be used."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_time(sltime, in_format='%Y-%m-%dT%H:%M:%S%z', out_format='%Y-%m-%d %H:%M'):\n    try:\n        clean = datetime.datetime.strptime(sltime, in_format)\n        return clean.strftime(out_format)\n    # The %z option only exists with py3.6+\n    except ValueError:\n        return sltime", "response": "Easy way to format time strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self):\n        return {key: val.to_dict() if isinstance(val, NestedDict) else val\n                for key, val in self.items()}", "response": "Converts a NestedDict instance into a real dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, identifier):\n\n    mgr = SoftLayer.FirewallManager(env.client)\n    firewall_type, firewall_id = firewall.parse_id(identifier)\n\n    if not (env.skip_confirmations or\n            formatting.confirm(\"This action will cancel a firewall from your \"\n                               \"account. Continue?\")):\n        raise exceptions.CLIAbort('Aborted.')\n\n    if firewall_type in ['vs', 'server']:\n        mgr.cancel_firewall(firewall_id, dedicated=False)\n    elif firewall_type == 'vlan':\n        mgr.cancel_firewall(firewall_id, dedicated=True)\n    else:\n        raise exceptions.CLIAbort('Unknown firewall type: %s' % firewall_type)\n\n    env.fout('Firewall with id %s is being cancelled!' % identifier)", "response": "Cancels a single firewall."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, label, in_file, key, note):\n\n    if in_file is None and key is None:\n        raise exceptions.ArgumentError(\n            'Either [-f | --in-file] or [-k | --key] arguments are required to add a key'\n        )\n\n    if in_file and key:\n        raise exceptions.ArgumentError(\n            '[-f | --in-file] is not allowed with [-k | --key]'\n        )\n\n    if key:\n        key_text = key\n    else:\n        key_file = open(path.expanduser(in_file), 'rU')\n        key_text = key_file.read().strip()\n        key_file.close()\n\n    mgr = SoftLayer.SshKeyManager(env.client)\n    result = mgr.add_key(key_text, label, note)\n\n    env.fout(\"SSH key added: %s\" % result.get('fingerprint'))", "response": "Add a new SSH key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env):\n\n    mgr = SoftLayer.NetworkManager(env.client)\n    result = mgr.get_rwhois()\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n    table.add_row(['Name', result['firstName'] + ' ' + result['lastName']])\n    table.add_row(['Company', result['companyName']])\n    table.add_row(['Abuse Email', result['abuseEmail']])\n    table.add_row(['Address 1', result['address1']])\n    if result.get('address2'):\n        table.add_row(['Address 2', result['address2']])\n    table.add_row(['City', result['city']])\n    table.add_row(['State', result.get('state', '-')])\n    table.add_row(['Postal Code', result.get('postalCode', '-')])\n    table.add_row(['Country', result['country']])\n    table.add_row(['Private Residence', result['privateResidenceFlag']])\n\n    env.fout(table)", "response": "Display the RWhois information for your account."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an address translation to an IPSEC tunnel context.", "response": "def cli(env, context_id, static_ip, remote_ip, note):\n    \"\"\"Add an address translation to an IPSEC tunnel context.\n\n    A separate configuration request should be made to realize changes on\n    network devices.\n    \"\"\"\n    manager = SoftLayer.IPSECManager(env.client)\n    # ensure context can be retrieved by given id\n    manager.get_tunnel_context(context_id)\n\n    translation = manager.create_translation(context_id,\n                                             static_ip=static_ip,\n                                             remote_ip=remote_ip,\n                                             notes=note)\n    env.out('Created translation from {} to {} #{}'\n            .format(static_ip, remote_ip, translation['id']))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, identifier, a_record, aaaa_record, ptr, ttl):\n\n    items = ['id',\n             'globalIdentifier',\n             'fullyQualifiedDomainName',\n             'hostname',\n             'domain',\n             'primaryBackendIpAddress',\n             'primaryIpAddress',\n             '''primaryNetworkComponent[\n                id, primaryIpAddress,\n                primaryVersion6IpAddressRecord[ipAddress]\n             ]''']\n    mask = \"mask[%s]\" % ','.join(items)\n    dns = SoftLayer.DNSManager(env.client)\n    vsi = SoftLayer.VSManager(env.client)\n\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    instance = vsi.get_instance(vs_id, mask=mask)\n    zone_id = helpers.resolve_id(dns.resolve_ids,\n                                 instance['domain'],\n                                 name='zone')\n\n    def sync_a_record():\n        \"\"\"Sync A record.\"\"\"\n        records = dns.get_records(zone_id,\n                                  host=instance['hostname'],\n                                  record_type='a')\n        if not records:\n            # don't have a record, lets add one to the base zone\n            dns.create_record(zone['id'],\n                              instance['hostname'],\n                              'a',\n                              instance['primaryIpAddress'],\n                              ttl=ttl)\n        else:\n            if len(records) != 1:\n                raise exceptions.CLIAbort(\"Aborting A record sync, found \"\n                                          \"%d A record exists!\" % len(records))\n            rec = records[0]\n            rec['data'] = instance['primaryIpAddress']\n            rec['ttl'] = ttl\n            dns.edit_record(rec)\n\n    def sync_aaaa_record():\n        \"\"\"Sync AAAA record.\"\"\"\n        records = dns.get_records(zone_id,\n                                  host=instance['hostname'],\n                                  record_type='aaaa')\n        try:\n            # done this way to stay within 80 character lines\n            component = instance['primaryNetworkComponent']\n            record = component['primaryVersion6IpAddressRecord']\n            ip_address = record['ipAddress']\n        except KeyError:\n            raise exceptions.CLIAbort(\"%s does not have an ipv6 address\"\n                                      % instance['fullyQualifiedDomainName'])\n\n        if not records:\n            # don't have a record, lets add one to the base zone\n            dns.create_record(zone['id'],\n                              instance['hostname'],\n                              'aaaa',\n                              ip_address,\n                              ttl=ttl)\n        else:\n            if len(records) != 1:\n                raise exceptions.CLIAbort(\"Aborting A record sync, found \"\n                                          \"%d A record exists!\" % len(records))\n            rec = records[0]\n            rec['data'] = ip_address\n            rec['ttl'] = ttl\n            dns.edit_record(rec)\n\n    def sync_ptr_record():\n        \"\"\"Sync PTR record.\"\"\"\n        host_rec = instance['primaryIpAddress'].split('.')[-1]\n        ptr_domains = (env.client['Virtual_Guest']\n                       .getReverseDomainRecords(id=instance['id'])[0])\n        edit_ptr = None\n        for ptr in ptr_domains['resourceRecords']:\n            if ptr['host'] == host_rec:\n                ptr['ttl'] = ttl\n                edit_ptr = ptr\n                break\n\n        if edit_ptr:\n            edit_ptr['data'] = instance['fullyQualifiedDomainName']\n            dns.edit_record(edit_ptr)\n        else:\n            dns.create_record(ptr_domains['id'],\n                              host_rec,\n                              'ptr',\n                              instance['fullyQualifiedDomainName'],\n                              ttl=ttl)\n\n    if not instance['primaryIpAddress']:\n        raise exceptions.CLIAbort('No primary IP address associated with '\n                                  'this VS')\n\n    zone = dns.get_zone(zone_id)\n\n    go_for_it = env.skip_confirmations or formatting.confirm(\n        \"Attempt to update DNS records for %s\"\n        % instance['fullyQualifiedDomainName'])\n\n    if not go_for_it:\n        raise exceptions.CLIAbort(\"Aborting DNS sync\")\n\n    both = False\n    if not ptr and not a_record and not aaaa_record:\n        both = True\n\n    if both or a_record:\n        sync_a_record()\n\n    if both or ptr:\n        sync_ptr_record()\n\n    if aaaa_record:\n        sync_aaaa_record()", "response": "Syncs A record and AAAA record."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisables snapshots on the specified schedule for a given volume.", "response": "def cli(env, volume_id, schedule_type):\n    \"\"\"Disables snapshots on the specified schedule for a given volume\"\"\"\n\n    if (schedule_type not in ['INTERVAL', 'HOURLY', 'DAILY', 'WEEKLY']):\n        raise exceptions.CLIAbort(\n            '--schedule-type must be INTERVAL, HOURLY, DAILY, or WEEKLY')\n\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    disabled = block_manager.disable_snapshots(volume_id, schedule_type)\n\n    if disabled:\n        click.echo('%s snapshots have been disabled for volume %s'\n                   % (schedule_type, volume_id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting health check types.", "response": "def cli(env):\n    \"\"\"List health check types.\"\"\"\n\n    mgr = SoftLayer.LoadBalancerManager(env.client)\n\n    hc_types = mgr.get_hc_types()\n    table = formatting.KeyValueTable(['ID', 'Name'])\n    table.align['ID'] = 'l'\n    table.align['Name'] = 'l'\n    table.sortby = 'ID'\n    for hc_type in hc_types:\n        table.add_row([hc_type['id'], hc_type['name']])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlimiting the number of objects storage accounts for this IBM Cloud Object Storage account.", "response": "def cli(env, identifier):\n    \"\"\"Credential limits for this IBM Cloud Object Storage account.\"\"\"\n\n    mgr = SoftLayer.ObjectStorageManager(env.client)\n    credential_limit = mgr.limit_credential(identifier)\n    table = formatting.Table(['limit'])\n    table.add_row([\n        credential_limit,\n    ])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, context_id, include):\n    mask = _get_tunnel_context_mask(('at' in include),\n                                    ('is' in include),\n                                    ('rs' in include),\n                                    ('sr' in include),\n                                    ('ss' in include))\n    manager = SoftLayer.IPSECManager(env.client)\n    context = manager.get_tunnel_context(context_id, mask=mask)\n\n    env.out('Context Details:')\n    env.fout(_get_context_table(context))\n\n    for relation in include:\n        if relation == 'at':\n            env.out('Address Translations:')\n            env.fout(_get_address_translations_table(\n                context.get('addressTranslations', [])))\n        elif relation == 'is':\n            env.out('Internal Subnets:')\n            env.fout(_get_subnets_table(context.get('internalSubnets', [])))\n        elif relation == 'rs':\n            env.out('Remote Subnets:')\n            env.fout(_get_subnets_table(context.get('customerSubnets', [])))\n        elif relation == 'sr':\n            env.out('Static Subnets:')\n            env.fout(_get_subnets_table(context.get('staticRouteSubnets', [])))\n        elif relation == 'ss':\n            env.out('Service Subnets:')\n            env.fout(_get_subnets_table(context.get('serviceSubnets', [])))", "response": "List IPSEC VPN tunnel context details."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_address_translations_table(address_translations):\n    table = formatting.Table(['id',\n                              'static IP address',\n                              'static IP address id',\n                              'remote IP address',\n                              'remote IP address id',\n                              'note'])\n    for address_translation in address_translations:\n        table.add_row([address_translation.get('id', ''),\n                       address_translation.get('internalIpAddressRecord', {})\n                       .get('ipAddress', ''),\n                       address_translation.get('internalIpAddressId', ''),\n                       address_translation.get('customerIpAddressRecord', {})\n                       .get('ipAddress', ''),\n                       address_translation.get('customerIpAddressId', ''),\n                       address_translation.get('notes', '')])\n    return table", "response": "Yields a formatted table to print address translations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding a formatted table to print subnet details.", "response": "def _get_subnets_table(subnets):\n    \"\"\"Yields a formatted table to print subnet details.\n\n    :param List[dict] subnets: List of subnets.\n    :return Table: Formatted for subnet output.\n    \"\"\"\n    table = formatting.Table(['id',\n                              'network identifier',\n                              'cidr',\n                              'note'])\n    for subnet in subnets:\n        table.add_row([subnet.get('id', ''),\n                       subnet.get('networkIdentifier', ''),\n                       subnet.get('cidr', ''),\n                       subnet.get('note', '')])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_tunnel_context_mask(address_translations=False,\n                             internal_subnets=False,\n                             remote_subnets=False,\n                             static_subnets=False,\n                             service_subnets=False):\n    \"\"\"Yields a mask object for a tunnel context.\n\n    All exposed properties on the tunnel context service are included in\n    the constructed mask. Additional joins may be requested.\n\n    :param bool address_translations: Whether to join the context's address\n           translation entries.\n    :param bool internal_subnets: Whether to join the context's internal\n           subnet associations.\n    :param bool remote_subnets: Whether to join the context's remote subnet\n           associations.\n    :param bool static_subnets: Whether to join the context's statically\n           routed subnet associations.\n    :param bool service_subnets: Whether to join the SoftLayer service\n           network subnets.\n    :return string: Encoding for the requested mask object.\n    \"\"\"\n    entries = ['id',\n               'accountId',\n               'advancedConfigurationFlag',\n               'createDate',\n               'customerPeerIpAddress',\n               'modifyDate',\n               'name',\n               'friendlyName',\n               'internalPeerIpAddress',\n               'phaseOneAuthentication',\n               'phaseOneDiffieHellmanGroup',\n               'phaseOneEncryption',\n               'phaseOneKeylife',\n               'phaseTwoAuthentication',\n               'phaseTwoDiffieHellmanGroup',\n               'phaseTwoEncryption',\n               'phaseTwoKeylife',\n               'phaseTwoPerfectForwardSecrecy',\n               'presharedKey']\n    if address_translations:\n        entries.append('addressTranslations[internalIpAddressRecord[ipAddress],'\n                       'customerIpAddressRecord[ipAddress]]')\n    if internal_subnets:\n        entries.append('internalSubnets')\n    if remote_subnets:\n        entries.append('customerSubnets')\n    if static_subnets:\n        entries.append('staticRouteSubnets')\n    if service_subnets:\n        entries.append('serviceSubnets')\n    return '[mask[{}]]'.format(','.join(entries))", "response": "Returns a mask object for tunnel context services."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_context_table(context):\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    table.add_row(['id', context.get('id', '')])\n    table.add_row(['name', context.get('name', '')])\n    table.add_row(['friendly name', context.get('friendlyName', '')])\n    table.add_row(['internal peer IP address',\n                   context.get('internalPeerIpAddress', '')])\n    table.add_row(['remote peer IP address',\n                   context.get('customerPeerIpAddress', '')])\n    table.add_row(['advanced configuration flag',\n                   context.get('advancedConfigurationFlag', '')])\n    table.add_row(['preshared key', context.get('presharedKey', '')])\n    table.add_row(['phase 1 authentication',\n                   context.get('phaseOneAuthentication', '')])\n    table.add_row(['phase 1 diffie hellman group',\n                   context.get('phaseOneDiffieHellmanGroup', '')])\n    table.add_row(['phase 1 encryption', context.get('phaseOneEncryption', '')])\n    table.add_row(['phase 1 key life', context.get('phaseOneKeylife', '')])\n    table.add_row(['phase 2 authentication',\n                   context.get('phaseTwoAuthentication', '')])\n    table.add_row(['phase 2 diffie hellman group',\n                   context.get('phaseTwoDiffieHellmanGroup', '')])\n    table.add_row(['phase 2 encryption', context.get('phaseTwoEncryption', '')])\n    table.add_row(['phase 2 key life', context.get('phaseTwoKeylife', '')])\n    table.add_row(['phase 2 perfect forward secrecy',\n                   context.get('phaseTwoPerfectForwardSecrecy', '')])\n    table.add_row(['created', context.get('createDate')])\n    table.add_row(['modified', context.get('modifyDate')])\n    return table", "response": "Returns a formatted table to print tunnel context details."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, volume_id, replicant_id, immediate):\n    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n\n    success = file_storage_manager.failover_to_replicant(\n        volume_id,\n        replicant_id,\n        immediate\n    )\n\n    if success:\n        click.echo(\"Failover to replicant is now in progress.\")\n    else:\n        click.echo(\"Failover operation could not be initiated.\")", "response": "Failover a file volume to a given replicant volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting number of file storage volumes per datacenter.", "response": "def cli(env, sortby, datacenter):\n    \"\"\"List number of file storage volumes per datacenter.\"\"\"\n    file_manager = SoftLayer.FileStorageManager(env.client)\n    mask = \"mask[serviceResource[datacenter[name]],\"\\\n           \"replicationPartners[serviceResource[datacenter[name]]]]\"\n    file_volumes = file_manager.list_file_volumes(datacenter=datacenter,\n                                                  mask=mask)\n\n    datacenters = dict()\n    for volume in file_volumes:\n        service_resource = volume['serviceResource']\n        if 'datacenter' in service_resource:\n            datacenter_name = service_resource['datacenter']['name']\n            if datacenter_name not in datacenters.keys():\n                datacenters[datacenter_name] = 1\n            else:\n                datacenters[datacenter_name] += 1\n\n    table = formatting.KeyValueTable(DEFAULT_COLUMNS)\n    table.sortby = sortby\n    for datacenter_name in datacenters:\n        table.add_row([datacenter_name, datacenters[datacenter_name]])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env):\n\n    hardware_manager = hardware.HardwareManager(env.client)\n    options = hardware_manager.get_create_options()\n\n    tables = []\n\n    # Datacenters\n    dc_table = formatting.Table(['datacenter', 'value'])\n    dc_table.sortby = 'value'\n    for location in options['locations']:\n        dc_table.add_row([location['name'], location['key']])\n    tables.append(dc_table)\n\n    # Presets\n    preset_table = formatting.Table(['size', 'value'])\n    preset_table.sortby = 'value'\n    for size in options['sizes']:\n        preset_table.add_row([size['name'], size['key']])\n    tables.append(preset_table)\n\n    # Operating systems\n    os_table = formatting.Table(['operating_system', 'value'])\n    os_table.sortby = 'value'\n    for operating_system in options['operating_systems']:\n        os_table.add_row([operating_system['name'], operating_system['key']])\n    tables.append(os_table)\n\n    # Port speed\n    port_speed_table = formatting.Table(['port_speed', 'value'])\n    port_speed_table.sortby = 'value'\n    for speed in options['port_speeds']:\n        port_speed_table.add_row([speed['name'], speed['key']])\n    tables.append(port_speed_table)\n\n    # Extras\n    extras_table = formatting.Table(['extras', 'value'])\n    extras_table.sortby = 'value'\n    for extra in options['extras']:\n        extras_table.add_row([extra['name'], extra['key']])\n    tables.append(extras_table)\n\n    env.fout(formatting.listing(tables, separator='\\n'))", "response": "Server order options for a given chassis."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_id(input_id):\n    key_value = input_id.split(':')\n\n    if len(key_value) != 2:\n        raise exceptions.CLIAbort(\n            'Invalid ID %s: ID should be of the form xxx:yyy' % input_id)\n    return key_value[0], int(key_value[1])", "response": "Helper package to retrieve the actual IDs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_filters(_filters):\n    root = utils.NestedDict({})\n    for _filter in _filters:\n        operation = None\n        for operation, token in SPLIT_TOKENS:\n            # split \"some.key=value\" into [\"some.key\", \"value\"]\n            top_parts = _filter.split(token, 1)\n            if len(top_parts) == 2:\n                break\n        else:\n            raise exceptions.CLIAbort('Failed to find valid operation for: %s'\n                                      % _filter)\n\n        key, value = top_parts\n        current = root\n        # split \"some.key\" into [\"some\", \"key\"]\n        parts = [part.strip() for part in key.split('.')]\n\n        # Actually drill down and add the filter\n        for part in parts[:-1]:\n            current = current[part]\n\n        if operation == 'eq':\n            current[parts[-1]] = utils.query_filter(value.strip())\n        elif operation == 'in':\n            current[parts[-1]] = {\n                'operation': 'in',\n                'options': [{\n                    'name': 'data',\n                    'value': [p.strip() for p in value.split(',')],\n                }],\n            }\n\n    return root.to_dict()", "response": "Builds the filters from the list of filters passed into the CLI."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, service, method, parameters, _id, _filters, mask, limit, offset,\n        output_python=False):\n    \"\"\"Call arbitrary API endpoints with the given SERVICE and METHOD.\n\n    Example::\n\n        slcli call-api Account getObject\n        slcli call-api Account getVirtualGuests --limit=10 --mask=id,hostname\n        slcli call-api Virtual_Guest getObject --id=12345\n        slcli call-api Metric_Tracking_Object getBandwidthData --id=1234 \\\\\n            \"2015-01-01 00:00:00\" \"2015-01-1 12:00:00\" public\n        slcli call-api Account getVirtualGuests \\\\\n            -f 'virtualGuests.datacenter.name=dal05' \\\\\n            -f 'virtualGuests.maxCpu=4' \\\\\n            --mask=id,hostname,datacenter.name,maxCpu\n        slcli call-api Account getVirtualGuests \\\\\n            -f 'virtualGuests.datacenter.name IN dal05,sng01'\n    \"\"\"\n\n    args = [service, method] + list(parameters)\n    kwargs = {\n        'id': _id,\n        'filter': _build_filters(_filters),\n        'mask': mask,\n        'limit': limit,\n        'offset': offset,\n    }\n\n    if output_python:\n        env.out(_build_python_example(args, kwargs))\n    else:\n        result = env.client.call(*args, **kwargs)\n        env.fout(formatting.iter_to_table(result))", "response": "Get a set of virtual servers for a given service and method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cli(env, volume_id, sortby, columns):\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    snapshots = block_manager.get_block_volume_snapshot_list(\n        volume_id,\n        mask=columns.mask()\n    )\n\n    table = formatting.Table(columns.columns)\n    table.sortby = sortby\n\n    for snapshot in snapshots:\n        table.add_row([value or formatting.blank()\n                       for value in columns.row(snapshot)])\n\n    env.fout(table)", "response": "List block storage snapshots."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, identifier, passwords, price):\n\n    hardware = SoftLayer.HardwareManager(env.client)\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    hardware_id = helpers.resolve_id(hardware.resolve_ids, identifier, 'hardware')\n    result = hardware.get_hardware(hardware_id)\n    result = utils.NestedDict(result)\n\n    operating_system = utils.lookup(result, 'operatingSystem', 'softwareLicense', 'softwareDescription') or {}\n    memory = formatting.gb(result.get('memoryCapacity', 0))\n    owner = None\n    if utils.lookup(result, 'billingItem') != []:\n        owner = utils.lookup(result, 'billingItem', 'orderItem', 'order', 'userRecord', 'username')\n\n    table.add_row(['id', result['id']])\n    table.add_row(['guid', result['globalIdentifier'] or formatting.blank()])\n    table.add_row(['hostname', result['hostname']])\n    table.add_row(['domain', result['domain']])\n    table.add_row(['fqdn', result['fullyQualifiedDomainName']])\n    table.add_row(['status', result['hardwareStatus']['status']])\n    table.add_row(['datacenter', result['datacenter']['name'] or formatting.blank()])\n    table.add_row(['cores', result['processorPhysicalCoreAmount']])\n    table.add_row(['memory', memory])\n    table.add_row(['public_ip', result['primaryIpAddress'] or formatting.blank()])\n    table.add_row(['private_ip', result['primaryBackendIpAddress'] or formatting.blank()])\n    table.add_row(['ipmi_ip', result['networkManagementIpAddress'] or formatting.blank()])\n    table.add_row(['os', operating_system.get('name') or formatting.blank()])\n    table.add_row(['os_version', operating_system.get('version') or formatting.blank()])\n    table.add_row(['created', result['provisionDate'] or formatting.blank()])\n    table.add_row(['owner', owner or formatting.blank()])\n\n    vlan_table = formatting.Table(['type', 'number', 'id'])\n    for vlan in result['networkVlans']:\n        vlan_table.add_row([vlan['networkSpace'], vlan['vlanNumber'], vlan['id']])\n\n    table.add_row(['vlans', vlan_table])\n\n    if result.get('notes'):\n        table.add_row(['notes', result['notes']])\n\n    if price:\n        total_price = utils.lookup(result, 'billingItem', 'nextInvoiceTotalRecurringAmount') or 0\n\n        price_table = formatting.Table(['Item', 'Recurring Price'])\n        price_table.add_row(['Total', total_price])\n\n        for item in utils.lookup(result, 'billingItem', 'children') or []:\n            price_table.add_row([item['description'], item['nextInvoiceTotalRecurringAmount']])\n\n        table.add_row(['prices', price_table])\n\n    if passwords:\n        pass_table = formatting.Table(['username', 'password'])\n        for item in result['operatingSystem']['passwords']:\n            pass_table.add_row([item['username'], item['password']])\n        table.add_row(['users', pass_table])\n\n        pass_table = formatting.Table(['ipmi_username', 'password'])\n        for item in result['remoteManagementAccounts']:\n            pass_table.add_row([item['username'], item['password']])\n        table.add_row(['remote users', pass_table])\n\n    table.add_row(['tags', formatting.tags(result['tagReferences'])])\n\n    env.fout(table)", "response": "Get details for a hardware device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env):\n\n    vsi = SoftLayer.VSManager(env.client)\n    result = vsi.get_create_options()\n\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    # Datacenters\n    datacenters = [dc['template']['datacenter']['name']\n                   for dc in result['datacenters']]\n    datacenters = sorted(datacenters)\n\n    table.add_row(['datacenter',\n                   formatting.listing(datacenters, separator='\\n')])\n\n    def _add_flavor_rows(flavor_key, flavor_label, flavor_options):\n        flavors = []\n\n        for flavor_option in flavor_options:\n            flavor_key_name = utils.lookup(flavor_option, 'flavor', 'keyName')\n            if not flavor_key_name.startswith(flavor_key):\n                continue\n\n            flavors.append(flavor_key_name)\n\n        if len(flavors) > 0:\n            table.add_row(['flavors (%s)' % flavor_label,\n                           formatting.listing(flavors, separator='\\n')])\n\n    if result.get('flavors', None):\n        _add_flavor_rows('B1', 'balanced', result['flavors'])\n        _add_flavor_rows('BL1', 'balanced local - hdd', result['flavors'])\n        _add_flavor_rows('BL2', 'balanced local - ssd', result['flavors'])\n        _add_flavor_rows('C1', 'compute', result['flavors'])\n        _add_flavor_rows('M1', 'memory', result['flavors'])\n        _add_flavor_rows('AC', 'GPU', result['flavors'])\n\n    # CPUs\n    standard_cpus = [int(x['template']['startCpus']) for x in result['processors']\n                     if not x['template'].get('dedicatedAccountHostOnlyFlag',\n                                              False)\n                     and not x['template'].get('dedicatedHost', None)]\n    ded_cpus = [int(x['template']['startCpus']) for x in result['processors']\n                if x['template'].get('dedicatedAccountHostOnlyFlag', False)]\n    ded_host_cpus = [int(x['template']['startCpus']) for x in result['processors']\n                     if x['template'].get('dedicatedHost', None)]\n\n    standard_cpus = sorted(standard_cpus)\n    table.add_row(['cpus (standard)', formatting.listing(standard_cpus, separator=',')])\n    ded_cpus = sorted(ded_cpus)\n    table.add_row(['cpus (dedicated)', formatting.listing(ded_cpus, separator=',')])\n    ded_host_cpus = sorted(ded_host_cpus)\n    table.add_row(['cpus (dedicated host)', formatting.listing(ded_host_cpus, separator=',')])\n\n    # Memory\n    memory = [int(m['template']['maxMemory']) for m in result['memory']\n              if not m['itemPrice'].get('dedicatedHostInstanceFlag', False)]\n    ded_host_memory = [int(m['template']['maxMemory']) for m in result['memory']\n                       if m['itemPrice'].get('dedicatedHostInstanceFlag', False)]\n\n    memory = sorted(memory)\n    table.add_row(['memory',\n                   formatting.listing(memory, separator=',')])\n\n    ded_host_memory = sorted(ded_host_memory)\n    table.add_row(['memory (dedicated host)',\n                   formatting.listing(ded_host_memory, separator=',')])\n\n    # Operating Systems\n    op_sys = [o['template']['operatingSystemReferenceCode'] for o in\n              result['operatingSystems']]\n\n    op_sys = sorted(op_sys)\n    os_summary = set()\n\n    for operating_system in op_sys:\n        os_summary.add(operating_system[0:operating_system.find('_')])\n\n    for summary in sorted(os_summary):\n        table.add_row([\n            'os (%s)' % summary,\n            os.linesep.join(sorted([x for x in op_sys\n                                    if x[0:len(summary)] == summary]))\n        ])\n\n    # Disk\n    local_disks = [x for x in result['blockDevices']\n                   if x['template'].get('localDiskFlag', False)\n                   and not x['itemPrice'].get('dedicatedHostInstanceFlag',\n                                              False)]\n\n    ded_host_local_disks = [x for x in result['blockDevices']\n                            if x['template'].get('localDiskFlag', False)\n                            and x['itemPrice'].get('dedicatedHostInstanceFlag',\n                                                   False)]\n\n    san_disks = [x for x in result['blockDevices']\n                 if not x['template'].get('localDiskFlag', False)]\n\n    def add_block_rows(disks, name):\n        \"\"\"Add block rows to the table.\"\"\"\n        simple = {}\n        for disk in disks:\n            block = disk['template']['blockDevices'][0]\n            bid = block['device']\n\n            if bid not in simple:\n                simple[bid] = []\n\n            simple[bid].append(str(block['diskImage']['capacity']))\n\n        for label in sorted(simple):\n            table.add_row(['%s disk(%s)' % (name, label),\n                           formatting.listing(simple[label],\n                                              separator=',')])\n\n    add_block_rows(san_disks, 'san')\n    add_block_rows(local_disks, 'local')\n    add_block_rows(ded_host_local_disks, 'local (dedicated host)')\n\n    # Network\n    speeds = []\n    ded_host_speeds = []\n    for option in result['networkComponents']:\n        template = option.get('template', None)\n        price = option.get('itemPrice', None)\n\n        if not template or not price \\\n                or not template.get('networkComponents', None):\n            continue\n\n        if not template['networkComponents'][0] \\\n                or not template['networkComponents'][0].get('maxSpeed', None):\n            continue\n\n        max_speed = str(template['networkComponents'][0]['maxSpeed'])\n        if price.get('dedicatedHostInstanceFlag', False) \\\n                and max_speed not in ded_host_speeds:\n            ded_host_speeds.append(max_speed)\n        elif max_speed not in speeds:\n            speeds.append(max_speed)\n\n    speeds = sorted(speeds)\n    table.add_row(['nic', formatting.listing(speeds, separator=',')])\n\n    ded_host_speeds = sorted(ded_host_speeds)\n    table.add_row(['nic (dedicated host)',\n                   formatting.listing(ded_host_speeds, separator=',')])\n\n    env.fout(table)", "response": "Virtual server order options."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplace or verify an order. This CLI command is used for placing/verifying an order of the specified package in the given location (denoted by a datacenter's long name). Orders made via the CLI can then be converted to be made programmatically by calling SoftLayer.OrderingManager.place_order() with the same keynames. Packages for ordering can be retrieved from `slcli order package-list` Presets for ordering can be retrieved from `slcli order preset-list` (not all packages have presets) Items can be retrieved from `slcli order item-list`. In order to find required items for the order, use `slcli order category-list`, and then provide the --category option for each category code in `slcli order item-list`. Example:: # Order an hourly VSI with 4 CPU, 16 GB RAM, 100 GB SAN disk, # Ubuntu 16.04, and 1 Gbps public & private uplink in dal13 slcli order place --billing hourly CLOUD_SERVER DALLAS13 \\\\ GUEST_CORES_4 \\\\ RAM_16_GB \\\\ REBOOT_REMOTE_CONSOLE \\\\ 1_GBPS_PUBLIC_PRIVATE_NETWORK_UPLINKS \\\\ BANDWIDTH_0_GB_2 \\\\ 1_IP_ADDRESS \\\\ GUEST_DISK_100_GB_SAN \\\\ OS_UBUNTU_16_04_LTS_XENIAL_XERUS_MINIMAL_64_BIT_FOR_VSI \\\\ MONITORING_HOST_PING \\\\ NOTIFICATION_EMAIL_AND_TICKET \\\\ AUTOMATED_NOTIFICATION \\\\ UNLIMITED_SSL_VPN_USERS_1_PPTP_VPN_USER_PER_ACCOUNT \\\\ NESSUS_VULNERABILITY_ASSESSMENT_REPORTING \\\\ --extras '{\"virtualGuests\": [{\"hostname\": \"test\", \"domain\": \"softlayer.com\"}]}' \\\\ --complex-type SoftLayer_Container_Product_Order_Virtual_Guest", "response": "def cli(env, package_keyname, location, preset, verify, billing, complex_type,\n        quantity, extras, order_items):\n    \"\"\"Place or verify an order.\n\n    This CLI command is used for placing/verifying an order of the specified package in\n    the given location (denoted by a datacenter's long name). Orders made via the CLI\n    can then be converted to be made programmatically by calling\n    SoftLayer.OrderingManager.place_order() with the same keynames.\n\n    Packages for ordering can be retrieved from `slcli order package-list`\n    Presets for ordering can be retrieved from `slcli order preset-list` (not all packages\n    have presets)\n\n    Items can be retrieved from `slcli order item-list`. In order to find required\n    items for the order, use `slcli order category-list`, and then provide the\n    --category option for each category code in `slcli order item-list`.\n\n\n    Example::\n\n        # Order an hourly VSI with 4 CPU, 16 GB RAM, 100 GB SAN disk,\n        # Ubuntu 16.04, and 1 Gbps public & private uplink in dal13\n        slcli order place --billing hourly CLOUD_SERVER DALLAS13 \\\\\n            GUEST_CORES_4 \\\\\n            RAM_16_GB \\\\\n            REBOOT_REMOTE_CONSOLE \\\\\n            1_GBPS_PUBLIC_PRIVATE_NETWORK_UPLINKS \\\\\n            BANDWIDTH_0_GB_2 \\\\\n            1_IP_ADDRESS \\\\\n            GUEST_DISK_100_GB_SAN \\\\\n            OS_UBUNTU_16_04_LTS_XENIAL_XERUS_MINIMAL_64_BIT_FOR_VSI \\\\\n            MONITORING_HOST_PING \\\\\n            NOTIFICATION_EMAIL_AND_TICKET \\\\\n            AUTOMATED_NOTIFICATION \\\\\n            UNLIMITED_SSL_VPN_USERS_1_PPTP_VPN_USER_PER_ACCOUNT \\\\\n            NESSUS_VULNERABILITY_ASSESSMENT_REPORTING \\\\\n            --extras '{\"virtualGuests\": [{\"hostname\": \"test\", \"domain\": \"softlayer.com\"}]}' \\\\\n            --complex-type SoftLayer_Container_Product_Order_Virtual_Guest\n\n    \"\"\"\n    manager = ordering.OrderingManager(env.client)\n\n    if extras:\n        try:\n            extras = json.loads(extras)\n        except ValueError as err:\n            raise exceptions.CLIAbort(\"There was an error when parsing the --extras value: {}\".format(err))\n\n    args = (package_keyname, location, order_items)\n    kwargs = {'preset_keyname': preset,\n              'extras': extras,\n              'quantity': quantity,\n              'complex_type': complex_type,\n              'hourly': bool(billing == 'hourly')}\n\n    if verify:\n        table = formatting.Table(COLUMNS)\n        order_to_place = manager.verify_order(*args, **kwargs)\n        for price in order_to_place['orderContainers'][0]['prices']:\n            cost_key = 'hourlyRecurringFee' if billing == 'hourly' else 'recurringFee'\n            table.add_row([\n                price['item']['keyName'],\n                price['item']['description'],\n                price[cost_key] if cost_key in price else formatting.blank()\n            ])\n\n    else:\n        if not (env.skip_confirmations or formatting.confirm(\n                \"This action will incur charges on your account. Continue?\")):\n            raise exceptions.CLIAbort(\"Aborting order.\")\n\n        order = manager.place_order(*args, **kwargs)\n\n        table = formatting.KeyValueTable(['name', 'value'])\n        table.align['name'] = 'r'\n        table.align['value'] = 'l'\n        table.add_row(['id', order['orderId']])\n        table.add_row(['created', order['orderDate']])\n        table.add_row(['status', order['placedOrder']['status']])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, identifier, label, note):\n\n    mgr = SoftLayer.SshKeyManager(env.client)\n\n    key_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'SshKey')\n\n    if not mgr.edit_key(key_id, label=label, notes=note):\n        raise exceptions.CLIAbort('Failed to edit SSH key')", "response": "Edits an SSH key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, securitygroup_id):\n    mgr = SoftLayer.NetworkManager(env.client)\n    if not mgr.delete_securitygroup(securitygroup_id):\n        raise exceptions.CLIAbort(\"Failed to delete security group\")", "response": "Deletes the given security group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, ipv6, test):\n\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    version = 4\n    if ipv6:\n        version = 6\n\n    if not (test or env.skip_confirmations):\n        if not formatting.confirm(\"This action will incur charges on your \"\n                                  \"account. Continue?\"):\n            raise exceptions.CLIAbort('Cancelling order.')\n\n    result = mgr.add_global_ip(version=version, test_order=test)\n\n    table = formatting.Table(['item', 'cost'])\n    table.align['Item'] = 'r'\n    table.align['cost'] = 'r'\n\n    total = 0.0\n    for price in result['orderDetails']['prices']:\n        total += float(price.get('recurringFee', 0.0))\n        rate = \"%.2f\" % float(price['recurringFee'])\n\n        table.add_row([price['item']['description'], rate])\n\n    table.add_row(['Total monthly cost', \"%.2f\" % total])\n    env.fout(table)", "response": "Creates a global IP."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving a translation entry from an IPSEC tunnel context.", "response": "def cli(env, context_id, translation_id):\n    \"\"\"Remove a translation entry from an IPSEC tunnel context.\n\n    A separate configuration request should be made to realize changes on\n    network devices.\n    \"\"\"\n    manager = SoftLayer.IPSECManager(env.client)\n    # ensure translation can be retrieved by given id\n    manager.get_translation(context_id, translation_id)\n\n    succeeded = manager.remove_translation(context_id, translation_id)\n    if succeeded:\n        env.out('Removed translation #{}'.format(translation_id))\n    else:\n        raise CLIHalt('Failed to remove translation #{}'.format(translation_id))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(env, volume_id, schedule_type):\n\n    if (schedule_type not in ['INTERVAL', 'HOURLY', 'DAILY', 'WEEKLY']):\n        raise exceptions.CLIAbort(\n            '--schedule_type must be INTERVAL, HOURLY, DAILY, or WEEKLY')\n\n    file_manager = SoftLayer.FileStorageManager(env.client)\n    disabled = file_manager.disable_snapshots(volume_id, schedule_type)\n\n    if disabled:\n        click.echo('%s snapshots have been disabled for volume %s'\n                   % (schedule_type, volume_id))", "response": "Disables snapshots on the specified schedule for a given volume."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(env, name, description):\n    mgr = SoftLayer.NetworkManager(env.client)\n\n    result = mgr.create_securitygroup(name, description)\n    table = formatting.KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n    table.add_row(['id', result['id']])\n    table.add_row(['name',\n                   result.get('name') or formatting.blank()])\n    table.add_row(['description',\n                   result.get('description') or formatting.blank()])\n    table.add_row(['created', result['createDate']])\n\n    env.fout(table)", "response": "Create a security group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving some data will format it for console output.", "response": "def format_output(data, fmt='table'):  # pylint: disable=R0911,R0912\n    \"\"\"Given some data, will format it for console output.\n\n    :param data: One of: String, Table, FormattedItem, List, Tuple,\n                 SequentialOutput\n    :param string fmt (optional): One of: table, raw, json, python\n    \"\"\"\n    if isinstance(data, utils.string_types):\n        if fmt in ('json', 'jsonraw'):\n            return json.dumps(data)\n        return data\n\n    # responds to .prettytable()\n    if hasattr(data, 'prettytable'):\n        if fmt == 'table':\n            return str(format_prettytable(data))\n        elif fmt == 'raw':\n            return str(format_no_tty(data))\n\n    # responds to .to_python()\n    if hasattr(data, 'to_python'):\n        if fmt == 'json':\n            return json.dumps(\n                format_output(data, fmt='python'),\n                indent=4,\n                cls=CLIJSONEncoder)\n        elif fmt == 'jsonraw':\n            return json.dumps(format_output(data, fmt='python'),\n                              cls=CLIJSONEncoder)\n        elif fmt == 'python':\n            return data.to_python()\n\n    # responds to .formatted\n    if hasattr(data, 'formatted'):\n        if fmt == 'table':\n            return data.formatted\n\n    # responds to .separator\n    if hasattr(data, 'separator'):\n        output = [format_output(d, fmt=fmt) for d in data if d]\n        return str(SequentialOutput(data.separator, output))\n\n    # is iterable\n    if isinstance(data, list) or isinstance(data, tuple):\n        output = [format_output(d, fmt=fmt) for d in data]\n        if fmt == 'python':\n            return output\n        return format_output(listing(output, separator=os.linesep))\n\n    # fallback, convert this odd object to a string\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a SoftLayer. CLI. formatting. Table instance to a prettytable.", "response": "def format_prettytable(table):\n    \"\"\"Converts SoftLayer.CLI.formatting.Table instance to a prettytable.\"\"\"\n    for i, row in enumerate(table.rows):\n        for j, item in enumerate(row):\n            table.rows[i][j] = format_output(item)\n\n    ptable = table.prettytable()\n    ptable.hrules = prettytable.FRAME\n    ptable.horizontal_char = '.'\n    ptable.vertical_char = ':'\n    ptable.junction_char = ':'\n    return ptable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a SoftLayer. CLI. formatting. Table instance to a prettytable.", "response": "def format_no_tty(table):\n    \"\"\"Converts SoftLayer.CLI.formatting.Table instance to a prettytable.\"\"\"\n\n    for i, row in enumerate(table.rows):\n        for j, item in enumerate(row):\n            table.rows[i][j] = format_output(item, fmt='raw')\n    ptable = table.prettytable()\n\n    for col in table.columns:\n        ptable.align[col] = 'l'\n\n    ptable.hrules = prettytable.NONE\n    ptable.border = False\n    ptable.header = False\n    ptable.left_padding_width = 0\n    ptable.right_padding_width = 2\n    return ptable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a FormattedItem describing the given transaction.", "response": "def transaction_status(transaction):\n    \"\"\"Returns a FormattedItem describing the given transaction.\n\n        :param item: An object capable of having an active transaction\n    \"\"\"\n    if not transaction or not transaction.get('transactionStatus'):\n        return blank()\n\n    return FormattedItem(\n        transaction['transactionStatus'].get('name'),\n        transaction['transactionStatus'].get('friendlyName'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a formatted list of tags.", "response": "def tags(tag_references):\n    \"\"\"Returns a formatted list of tags.\"\"\"\n    if not tag_references:\n        return blank()\n\n    tag_row = []\n    for tag_detail in tag_references:\n        tag = utils.lookup(tag_detail, 'tag', 'name')\n        if tag is not None:\n            tag_row.append(tag)\n\n    return listing(tag_row, separator=', ')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef confirm(prompt_str, default=False):\n    if default:\n        default_str = 'y'\n        prompt = '%s [Y/n]' % prompt_str\n    else:\n        default_str = 'n'\n        prompt = '%s [y/N]' % prompt_str\n\n    ans = click.prompt(prompt, default=default_str, show_default=False)\n    if ans.lower() in ('y', 'yes', 'yeah', 'yup', 'yolo'):\n        return True\n\n    return False", "response": "Show a confirmation prompt to a command - line user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef no_going_back(confirmation):\n    if not confirmation:\n        confirmation = 'yes'\n\n    prompt = ('This action cannot be undone! Type \"%s\" or press Enter '\n              'to abort' % confirmation)\n\n    ans = click.prompt(prompt, default='', show_default=False)\n    if ans.lower() == str(confirmation):\n        return True\n\n    return False", "response": "Show a confirmation to a user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_to_table(value):\n    if isinstance(value, list):\n        return _format_list(value)\n    if isinstance(value, dict):\n        return _format_dict(value)\n    return value", "response": "Convert raw API responses to response tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _format_dict(result):\n\n    table = KeyValueTable(['name', 'value'])\n    table.align['name'] = 'r'\n    table.align['value'] = 'l'\n\n    for key, value in result.items():\n        value = iter_to_table(value)\n        table.add_row([key, value])\n\n    return table", "response": "Format dictionary responses into key - value table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _format_list(result):\n\n    if not result:\n        return result\n\n    if isinstance(result[0], dict):\n        return _format_list_objects(result)\n\n    table = Table(['value'])\n    for item in result:\n        table.add_row([iter_to_table(item)])\n    return table", "response": "Format list responses into a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _format_list_objects(result):\n\n    all_keys = set()\n    for item in result:\n        all_keys = all_keys.union(item.keys())\n\n    all_keys = sorted(all_keys)\n    table = Table(all_keys)\n\n    for item in result:\n        values = []\n        for key in all_keys:\n            value = iter_to_table(item.get(key))\n            values.append(value)\n\n        table.add_row(values)\n\n    return table", "response": "Format list of objects into a table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencoding object if it implements to_python method.", "response": "def default(self, obj):\n        \"\"\"Encode object if it implements to_python().\"\"\"\n        if hasattr(obj, 'to_python'):\n            return obj.to_python()\n        return super(CLIJSONEncoder, self).default(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding this Table object to standard Python types.", "response": "def to_python(self):\n        \"\"\"Decode this Table object to standard Python types.\"\"\"\n        # Adding rows\n        items = []\n        for row in self.rows:\n            formatted_row = [_format_python_value(v) for v in row]\n            items.append(dict(zip(self.columns, formatted_row)))\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prettytable(self):\n        table = prettytable.PrettyTable(self.columns)\n\n        if self.sortby:\n            if self.sortby in self.columns:\n                table.sortby = self.sortby\n            else:\n                msg = \"Column (%s) doesn't exist to sort by\" % self.sortby\n                raise exceptions.CLIAbort(msg)\n        for a_col, alignment in self.align.items():\n            table.align[a_col] = alignment\n\n        if self.title:\n            table.title = self.title\n        # Adding rows\n        for row in self.rows:\n            table.add_row(row)\n        return table", "response": "Returns a prettytable instance with the contents of this object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_python(self):\n        mapping = {}\n        for row in self.rows:\n            mapping[row[0]] = _format_python_value(row[1])\n        return mapping", "response": "Decode this KeyValueTable object to standard Python types."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a user Users.", "response": "def cli(env, username, email, password, from_user, template, api_key):\n    \"\"\"Creates a user Users.\n\n    :Example: slcli user create my@email.com -e my@email.com -p generate -a\n    -t '{\"firstName\": \"Test\", \"lastName\": \"Testerson\"}'\n\n    Remember to set the permissions and access for this new user.\n    \"\"\"\n\n    mgr = SoftLayer.UserManager(env.client)\n    user_mask = (\"mask[id, firstName, lastName, email, companyName, address1, city, country, postalCode, \"\n                 \"state, userStatusId, timezoneId]\")\n    from_user_id = None\n    if from_user is None:\n        user_template = mgr.get_current_user(objectmask=user_mask)\n        from_user_id = user_template['id']\n    else:\n        from_user_id = helpers.resolve_id(mgr.resolve_ids, from_user, 'username')\n        user_template = mgr.get_user(from_user_id, objectmask=user_mask)\n    # If we send the ID back to the API, an exception will be thrown\n    del user_template['id']\n\n    if template is not None:\n        try:\n            template_object = json.loads(template)\n            for key in template_object:\n                user_template[key] = template_object[key]\n        except ValueError as ex:\n            raise exceptions.ArgumentError(\"Unable to parse --template. %s\" % ex)\n\n    user_template['username'] = username\n    if password == 'generate':\n        password = generate_password()\n\n    user_template['email'] = email\n\n    if not env.skip_confirmations:\n        table = formatting.KeyValueTable(['name', 'value'])\n        for key in user_template:\n            table.add_row([key, user_template[key]])\n        table.add_row(['password', password])\n        click.secho(\"You are about to create the following user...\", fg='green')\n        env.fout(table)\n        if not formatting.confirm(\"Do you wish to continue?\"):\n            raise exceptions.CLIAbort(\"Canceling creation!\")\n\n    result = mgr.create_user(user_template, password)\n    new_api_key = None\n    if api_key:\n        click.secho(\"Adding API key...\", fg='green')\n        new_api_key = mgr.add_api_authentication_key(result['id'])\n\n    table = formatting.Table(['Username', 'Email', 'Password', 'API Key'])\n    table.add_row([result['username'], result['email'], password, new_api_key])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_password():\n    if sys.version_info > (3, 6):\n        import secrets  # pylint: disable=import-error\n        alphabet = string.ascii_letters + string.digits\n        password = ''.join(secrets.choice(alphabet) for i in range(20))\n        special = ''.join(secrets.choice(string.punctuation) for i in range(3))\n        return password + special\n    else:\n        raise ImportError(\"Generating passwords require python 3.6 or higher\")", "response": "Returns a 23 character random string with 3 special characters at the end"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(env, volume_id, notes):\n    block_manager = SoftLayer.BlockStorageManager(env.client)\n    snapshot = block_manager.create_snapshot(volume_id, notes=notes)\n\n    if 'id' in snapshot:\n        click.echo('New snapshot created with id: %s' % snapshot['id'])\n    else:\n        click.echo('Error occurred while creating snapshot.\\n'\n                   'Ensure volume is not failed over or in another '\n                   'state which prevents taking snapshots.')", "response": "Creates a snapshot on a given volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget Event Log Types", "response": "def cli(env):\n    \"\"\"Get Event Log Types\"\"\"\n    mgr = SoftLayer.EventLogManager(env.client)\n\n    event_log_types = mgr.get_event_log_types()\n\n    table = formatting.Table(COLUMNS)\n\n    for event_log_type in event_log_types:\n        table.add_row([event_log_type])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a price id attached to the given key_name.", "response": "def _get_extra_price_id(items, key_name, hourly, location):\n    \"\"\"Returns a price id attached to item with the given key_name.\"\"\"\n\n    for item in items:\n        if utils.lookup(item, 'keyName') != key_name:\n            continue\n\n        for price in item['prices']:\n            if not _matches_billing(price, hourly):\n                continue\n\n            if not _matches_location(price, location):\n                continue\n\n            return price['id']\n\n    raise SoftLayer.SoftLayerError(\n        \"Could not find valid price for extra option, '%s'\" % key_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_default_price_id(items, option, hourly, location):\n\n    for item in items:\n        if utils.lookup(item, 'itemCategory', 'categoryCode') != option:\n            continue\n\n        for price in item['prices']:\n            if all([float(price.get('hourlyRecurringFee', 0)) == 0.0,\n                    float(price.get('recurringFee', 0)) == 0.0,\n                    _matches_billing(price, hourly),\n                    _matches_location(price, location)]):\n                return price['id']\n\n    raise SoftLayer.SoftLayerError(\n        \"Could not find valid price for '%s' option\" % option)", "response": "Returns a free price id given an option."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_bandwidth_price_id(items,\n                            hourly=True,\n                            no_public=False,\n                            location=None):\n    \"\"\"Choose a valid price id for bandwidth.\"\"\"\n\n    # Prefer pay-for-use data transfer with hourly\n    for item in items:\n\n        capacity = float(item.get('capacity', 0))\n        # Hourly and private only do pay-as-you-go bandwidth\n        if any([utils.lookup(item,\n                             'itemCategory',\n                             'categoryCode') != 'bandwidth',\n                (hourly or no_public) and capacity != 0.0,\n                not (hourly or no_public) and capacity == 0.0]):\n            continue\n\n        for price in item['prices']:\n            if not _matches_billing(price, hourly):\n                continue\n            if not _matches_location(price, location):\n                continue\n\n            return price['id']\n\n    raise SoftLayer.SoftLayerError(\n        \"Could not find valid price for bandwidth option\")", "response": "Choose a valid price id for bandwidth."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_os_price_id(items, os, location):\n\n    for item in items:\n        if any([utils.lookup(item,\n                             'itemCategory',\n                             'categoryCode') != 'os',\n                utils.lookup(item,\n                             'softwareDescription',\n                             'referenceCode') != os]):\n            continue\n\n        for price in item['prices']:\n            if not _matches_location(price, location):\n                continue\n\n            return price['id']\n\n    raise SoftLayer.SoftLayerError(\"Could not find valid price for os: '%s'\" %\n                                   os)", "response": "Returns the price id matching the given os and location."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_port_speed_price_id(items, port_speed, no_public, location):\n\n    for item in items:\n        if utils.lookup(item,\n                        'itemCategory',\n                        'categoryCode') != 'port_speed':\n            continue\n\n        # Check for correct capacity and if the item matches private only\n        if any([int(utils.lookup(item, 'capacity')) != port_speed,\n                _is_private_port_speed_item(item) != no_public,\n                not _is_bonded(item)]):\n            continue\n\n        for price in item['prices']:\n            if not _matches_location(price, location):\n                continue\n\n            return price['id']\n\n    raise SoftLayer.SoftLayerError(\n        \"Could not find valid price for port speed: '%s'\" % port_speed)", "response": "Returns a valid price id for a given port speed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _matches_billing(price, hourly):\n    return any([hourly and price.get('hourlyRecurringFee') is not None,\n                not hourly and price.get('recurringFee') is not None])", "response": "Return True if the price object is hourly and or monthly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the price object matches the location.", "response": "def _matches_location(price, location):\n    \"\"\"Return True if the price object matches the location.\"\"\"\n    # the price has no location restriction\n    if not price.get('locationGroupId'):\n        return True\n\n    # Check to see if any of the location groups match the location group\n    # of this price object\n    for group in location['location']['location']['priceGroups']:\n        if group['id'] == price['locationGroupId']:\n            return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_location(package, location):\n    for region in package['regions']:\n        if region['location']['location']['name'] == location:\n            return region\n\n    raise SoftLayer.SoftLayerError(\"Could not find valid location for: '%s'\" % location)", "response": "Get the longer key with a short location name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the preset id given the keyName of the preset.", "response": "def _get_preset_id(package, size):\n    \"\"\"Get the preset id given the keyName of the preset.\"\"\"\n    for preset in package['activePresets'] + package['accountRestrictedActivePresets']:\n        if preset['keyName'] == size or preset['id'] == size:\n            return preset['id']\n\n    raise SoftLayer.SoftLayerError(\"Could not find valid size for: '%s'\" % size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncancels the specified dedicated server.", "response": "def cancel_hardware(self, hardware_id, reason='unneeded', comment='', immediate=False):\n        \"\"\"Cancels the specified dedicated server.\n\n        Example::\n\n            # Cancels hardware id 1234\n            result = mgr.cancel_hardware(hardware_id=1234)\n\n        :param int hardware_id: The ID of the hardware to be cancelled.\n        :param string reason: The reason code for the cancellation. This should come from\n                              :func:`get_cancellation_reasons`.\n        :param string comment: An optional comment to include with the cancellation.\n        :param bool immediate: If set to True, will automatically update the cancelation ticket to request\n                               the resource be reclaimed asap. This request still has to be reviewed by a human\n        :returns: True on success or an exception\n        \"\"\"\n\n        # Get cancel reason\n        reasons = self.get_cancellation_reasons()\n        cancel_reason = reasons.get(reason, reasons['unneeded'])\n        ticket_mgr = SoftLayer.TicketManager(self.client)\n        mask = 'mask[id, hourlyBillingFlag, billingItem[id], openCancellationTicket[id], activeTransaction]'\n        hw_billing = self.get_hardware(hardware_id, mask=mask)\n\n        if 'activeTransaction' in hw_billing:\n            raise SoftLayer.SoftLayerError(\"Unable to cancel hardware with running transaction\")\n\n        if 'billingItem' not in hw_billing:\n            raise SoftLayer.SoftLayerError(\"Ticket #%s already exists for this server\" %\n                                           hw_billing['openCancellationTicket']['id'])\n\n        billing_id = hw_billing['billingItem']['id']\n\n        if immediate and not hw_billing['hourlyBillingFlag']:\n            LOGGER.warning(\"Immediate cancelation of montly servers is not guaranteed.\"\n                           \"Please check the cancelation ticket for updates.\")\n\n            result = self.client.call('Billing_Item', 'cancelItem',\n                                      False, False, cancel_reason, comment, id=billing_id)\n            hw_billing = self.get_hardware(hardware_id, mask=mask)\n            ticket_number = hw_billing['openCancellationTicket']['id']\n            cancel_message = \"Please reclaim this server ASAP, it is no longer needed. Thankyou.\"\n            ticket_mgr.update_ticket(ticket_number, cancel_message)\n            LOGGER.info(\"Cancelation ticket #%s has been updated requesting immediate reclaim\", ticket_number)\n        else:\n            result = self.client.call('Billing_Item', 'cancelItem',\n                                      immediate, False, cancel_reason, comment, id=billing_id)\n            hw_billing = self.get_hardware(hardware_id, mask=mask)\n            ticket_number = hw_billing['openCancellationTicket']['id']\n            LOGGER.info(\"Cancelation ticket #%s has been created\", ticket_number)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all hardware servers and bare metal computing instances.", "response": "def list_hardware(self, tags=None, cpus=None, memory=None, hostname=None,\n                      domain=None, datacenter=None, nic_speed=None,\n                      public_ip=None, private_ip=None, **kwargs):\n        \"\"\"List all hardware (servers and bare metal computing instances).\n\n        :param list tags: filter based on tags\n        :param integer cpus: filter based on number of CPUS\n        :param integer memory: filter based on amount of memory in gigabytes\n        :param string hostname: filter based on hostname\n        :param string domain: filter based on domain\n        :param string datacenter: filter based on datacenter\n        :param integer nic_speed: filter based on network speed (in MBPS)\n        :param string public_ip: filter based on public ip address\n        :param string private_ip: filter based on private ip address\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        :returns: Returns a list of dictionaries representing the matching\n                  hardware. This list will contain both dedicated servers and\n                  bare metal computing instances\n\n       Example::\n\n            # Using a custom object-mask. Will get ONLY what is specified\n            # These will stem from the SoftLayer_Hardware_Server datatype\n            object_mask = \"mask[hostname,monitoringRobot[robotStatus]]\"\n            result = mgr.list_hardware(mask=object_mask)\n        \"\"\"\n        if 'mask' not in kwargs:\n            hw_items = [\n                'id',\n                'hostname',\n                'domain',\n                'hardwareStatusId',\n                'globalIdentifier',\n                'fullyQualifiedDomainName',\n                'processorPhysicalCoreAmount',\n                'memoryCapacity',\n                'primaryBackendIpAddress',\n                'primaryIpAddress',\n                'datacenter',\n            ]\n            server_items = [\n                'activeTransaction[id, transactionStatus[friendlyName,name]]',\n            ]\n\n            kwargs['mask'] = ('[mask[%s],'\n                              ' mask(SoftLayer_Hardware_Server)[%s]]'\n                              % (','.join(hw_items), ','.join(server_items)))\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n        if tags:\n            _filter['hardware']['tagReferences']['tag']['name'] = {\n                'operation': 'in',\n                'options': [{'name': 'data', 'value': tags}],\n            }\n\n        if cpus:\n            _filter['hardware']['processorPhysicalCoreAmount'] = (\n                utils.query_filter(cpus))\n\n        if memory:\n            _filter['hardware']['memoryCapacity'] = utils.query_filter(memory)\n\n        if hostname:\n            _filter['hardware']['hostname'] = utils.query_filter(hostname)\n\n        if domain:\n            _filter['hardware']['domain'] = utils.query_filter(domain)\n\n        if datacenter:\n            _filter['hardware']['datacenter']['name'] = (\n                utils.query_filter(datacenter))\n\n        if nic_speed:\n            _filter['hardware']['networkComponents']['maxSpeed'] = (\n                utils.query_filter(nic_speed))\n\n        if public_ip:\n            _filter['hardware']['primaryIpAddress'] = (\n                utils.query_filter(public_ip))\n\n        if private_ip:\n            _filter['hardware']['primaryBackendIpAddress'] = (\n                utils.query_filter(private_ip))\n\n        kwargs['filter'] = _filter.to_dict()\n        kwargs['iter'] = True\n        return self.client.call('Account', 'getHardware', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting details about a hardware device.", "response": "def get_hardware(self, hardware_id, **kwargs):\n        \"\"\"Get details about a hardware device.\n\n        :param integer id: the hardware ID\n        :returns: A dictionary containing a large amount of information about\n                  the specified server.\n\n        Example::\n\n            object_mask = \"mask[id,networkVlans[vlanNumber]]\"\n            # Object masks are optional\n            result = mgr.get_hardware(hardware_id=1234,mask=object_mask)\n        \"\"\"\n\n        if 'mask' not in kwargs:\n            kwargs['mask'] = (\n                'id,'\n                'globalIdentifier,'\n                'fullyQualifiedDomainName,'\n                'hostname,'\n                'domain,'\n                'provisionDate,'\n                'hardwareStatus,'\n                'processorPhysicalCoreAmount,'\n                'memoryCapacity,'\n                'notes,'\n                'privateNetworkOnlyFlag,'\n                'primaryBackendIpAddress,'\n                'primaryIpAddress,'\n                'networkManagementIpAddress,'\n                'userData,'\n                'datacenter,'\n                '''networkComponents[id, status, speed, maxSpeed, name,\n                   ipmiMacAddress, ipmiIpAddress, macAddress, primaryIpAddress,\n                   port, primarySubnet[id, netmask, broadcastAddress,\n                                       networkIdentifier, gateway]],'''\n                'hardwareChassis[id,name],'\n                'activeTransaction[id, transactionStatus[friendlyName,name]],'\n                '''operatingSystem[\n                    softwareLicense[softwareDescription[manufacturer,\n                                                        name,\n                                                        version,\n                                                        referenceCode]],\n                    passwords[username,password]],'''\n                '''softwareComponents[\n                    softwareLicense[softwareDescription[manufacturer,\n                                                        name,\n                                                        version,\n                                                        referenceCode]],\n                    passwords[username,password]],'''\n                'billingItem['\n                'id,nextInvoiceTotalRecurringAmount,'\n                'children[nextInvoiceTotalRecurringAmount],'\n                'orderItem.order.userRecord[username]'\n                '],'\n                'hourlyBillingFlag,'\n                'tagReferences[id,tag[name,id]],'\n                'networkVlans[id,vlanNumber,networkSpace],'\n                'remoteManagementAccounts[username,password]'\n            )\n\n        return self.hardware.getObject(id=hardware_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming an OS reload of a server with its current configuration.", "response": "def reload(self, hardware_id, post_uri=None, ssh_keys=None):\n        \"\"\"Perform an OS reload of a server with its current configuration.\n\n        :param integer hardware_id: the instance ID to reload\n        :param string post_uri: The URI of the post-install script to run\n                                after reload\n        :param list ssh_keys: The SSH keys to add to the root user\n        \"\"\"\n\n        config = {}\n\n        if post_uri:\n            config['customProvisionScriptUri'] = post_uri\n\n        if ssh_keys:\n            config['sshKeyIds'] = [key_id for key_id in ssh_keys]\n\n        return self.hardware.reloadOperatingSystem('FORCE', config,\n                                                   id=hardware_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_port_speed(self, hardware_id, public, speed):\n        if public:\n            return self.client.call('Hardware_Server',\n                                    'setPublicNetworkInterfaceSpeed',\n                                    speed, id=hardware_id)\n        else:\n            return self.client.call('Hardware_Server',\n                                    'setPrivateNetworkInterfaceSpeed',\n                                    speed, id=hardware_id)", "response": "Allows you to change the port speed of a server s NICs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef place_order(self, **kwargs):\n        create_options = self._generate_create_dict(**kwargs)\n        return self.client['Product_Order'].placeOrder(create_options)", "response": "Places an order for a piece of hardware."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify an order for a piece of hardware.", "response": "def verify_order(self, **kwargs):\n        \"\"\"Verifies an order for a piece of hardware.\n\n        See :func:`place_order` for a list of available options.\n        \"\"\"\n        create_options = self._generate_create_dict(**kwargs)\n        return self.client['Product_Order'].verifyOrder(create_options)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_create_options(self):\n\n        package = self._get_package()\n\n        # Locations\n        locations = []\n        for region in package['regions']:\n            locations.append({\n                'name': region['location']['location']['longName'],\n                'key': region['location']['location']['name'],\n            })\n\n        # Sizes\n        sizes = []\n        for preset in package['activePresets'] + package['accountRestrictedActivePresets']:\n            sizes.append({\n                'name': preset['description'],\n                'key': preset['keyName']\n            })\n\n        # Operating systems\n        operating_systems = []\n        for item in package['items']:\n            if item['itemCategory']['categoryCode'] == 'os':\n                operating_systems.append({\n                    'name': item['softwareDescription']['longDescription'],\n                    'key': item['softwareDescription']['referenceCode'],\n                })\n\n        # Port speeds\n        port_speeds = []\n        for item in package['items']:\n            if all([item['itemCategory']['categoryCode'] == 'port_speed',\n                    # Hide private options\n                    not _is_private_port_speed_item(item),\n                    # Hide unbonded options\n                    _is_bonded(item)]):\n                port_speeds.append({\n                    'name': item['description'],\n                    'key': item['capacity'],\n                })\n\n        # Extras\n        extras = []\n        for item in package['items']:\n            if item['itemCategory']['categoryCode'] in EXTRA_CATEGORIES:\n                extras.append({\n                    'name': item['description'],\n                    'key': item['keyName']\n                })\n\n        return {\n            'locations': locations,\n            'sizes': sizes,\n            'operating_systems': operating_systems,\n            'port_speeds': port_speeds,\n            'extras': extras,\n        }", "response": "Returns valid options for ordering hardware."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the package related to simple hardware ordering.", "response": "def _get_package(self):\n        \"\"\"Get the package related to simple hardware ordering.\"\"\"\n        mask = '''\n            items[\n                keyName,\n                capacity,\n                description,\n                attributes[id,attributeTypeKeyName],\n                itemCategory[id,categoryCode],\n                softwareDescription[id,referenceCode,longDescription],\n                prices\n            ],\n            activePresets,\n            accountRestrictedActivePresets,\n            regions[location[location[priceGroups]]]\n            '''\n\n        package_keyname = 'BARE_METAL_SERVER'\n        package = self.ordering_manager.get_package_by_key(package_keyname, mask=mask)\n        return package"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntranslate arguments into a dictionary for creating a server.", "response": "def _generate_create_dict(self,\n                              size=None,\n                              hostname=None,\n                              domain=None,\n                              location=None,\n                              os=None,\n                              port_speed=None,\n                              ssh_keys=None,\n                              post_uri=None,\n                              hourly=True,\n                              no_public=False,\n                              extras=None):\n        \"\"\"Translates arguments into a dictionary for creating a server.\"\"\"\n\n        extras = extras or []\n\n        package = self._get_package()\n        location = _get_location(package, location)\n\n        prices = []\n        for category in ['pri_ip_addresses',\n                         'vpn_management',\n                         'remote_management']:\n            prices.append(_get_default_price_id(package['items'],\n                                                option=category,\n                                                hourly=hourly,\n                                                location=location))\n\n        prices.append(_get_os_price_id(package['items'], os,\n                                       location=location))\n        prices.append(_get_bandwidth_price_id(package['items'],\n                                              hourly=hourly,\n                                              no_public=no_public,\n                                              location=location))\n        prices.append(_get_port_speed_price_id(package['items'],\n                                               port_speed,\n                                               no_public,\n                                               location=location))\n\n        for extra in extras:\n            prices.append(_get_extra_price_id(package['items'],\n                                              extra, hourly,\n                                              location=location))\n\n        hardware = {\n            'hostname': hostname,\n            'domain': domain,\n        }\n\n        order = {\n            'hardware': [hardware],\n            'location': location['keyname'],\n            'prices': [{'id': price} for price in prices],\n            'packageId': package['id'],\n            'presetId': _get_preset_id(package, size),\n            'useHourlyPricing': hourly,\n        }\n\n        if post_uri:\n            order['provisionScripts'] = [post_uri]\n\n        if ssh_keys:\n            order['sshKeys'] = [{'sshKeyIds': ssh_keys}]\n\n        return order"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of matching hardware IDs for a given hostname.", "response": "def _get_ids_from_hostname(self, hostname):\n        \"\"\"Returns list of matching hardware IDs for a given hostname.\"\"\"\n        results = self.list_hardware(hostname=hostname, mask=\"id\")\n        return [result['id'] for result in results]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns list of matching hardware IDs for a given ip address.", "response": "def _get_ids_from_ip(self, ip):  # pylint: disable=inconsistent-return-statements\n        \"\"\"Returns list of matching hardware IDs for a given ip address.\"\"\"\n        try:\n            # Does it look like an ip address?\n            socket.inet_aton(ip)\n        except socket.error:\n            return []\n\n        # Find the server via ip address. First try public ip, then private\n        results = self.list_hardware(public_ip=ip, mask=\"id\")\n        if results:\n            return [result['id'] for result in results]\n\n        results = self.list_hardware(private_ip=ip, mask=\"id\")\n        if results:\n            return [result['id'] for result in results]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef edit(self, hardware_id, userdata=None, hostname=None, domain=None,\n             notes=None, tags=None):\n        \"\"\"Edit hostname, domain name, notes, user data of the hardware.\n\n        Parameters set to None will be ignored and not attempted to be updated.\n\n        :param integer hardware_id: the instance ID to edit\n        :param string userdata: user data on the hardware to edit.\n                                If none exist it will be created\n        :param string hostname: valid hostname\n        :param string domain: valid domain name\n        :param string notes: notes about this particular hardware\n        :param string tags: tags to set on the hardware as a comma separated\n                            list. Use the empty string to remove all tags.\n\n        Example::\n\n            # Change the hostname on instance 12345 to 'something'\n            result = mgr.edit(hardware_id=12345 , hostname=\"something\")\n            #result will be True or an Exception\n        \"\"\"\n\n        obj = {}\n        if userdata:\n            self.hardware.setUserMetadata([userdata], id=hardware_id)\n\n        if tags is not None:\n            self.hardware.setTags(tags, id=hardware_id)\n\n        if hostname:\n            obj['hostname'] = hostname\n\n        if domain:\n            obj['domain'] = domain\n\n        if notes:\n            obj['notes'] = notes\n\n        if not obj:\n            return True\n\n        return self.hardware.editObject(obj, id=hardware_id)", "response": "Edit the hostname domain name and notes of a specific instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the firmware of the specified hardware.", "response": "def update_firmware(self,\n                        hardware_id,\n                        ipmi=True,\n                        raid_controller=True,\n                        bios=True,\n                        hard_drive=True):\n        \"\"\"Update hardware firmware.\n\n        This will cause the server to be unavailable for ~20 minutes.\n\n        :param int hardware_id: The ID of the hardware to have its firmware\n                                updated.\n        :param bool ipmi: Update the ipmi firmware.\n        :param bool raid_controller: Update the raid controller firmware.\n        :param bool bios: Update the bios firmware.\n        :param bool hard_drive: Update the hard drive firmware.\n\n        Example::\n\n            # Check the servers active transactions to see progress\n            result = mgr.update_firmware(hardware_id=1234)\n        \"\"\"\n\n        return self.hardware.createFirmwareUpdateTransaction(\n            bool(ipmi), bool(raid_controller), bool(bios), bool(hard_drive), id=hardware_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reflash_firmware(self,\n                         hardware_id,\n                         ipmi=True,\n                         raid_controller=True,\n                         bios=True):\n        \"\"\"Reflash hardware firmware.\n\n        This will cause the server to be unavailable for ~60 minutes.\n        The firmware will not be upgraded but rather reflashed to the version installed.\n\n        :param int hardware_id: The ID of the hardware to have its firmware\n                                reflashed.\n        :param bool ipmi: Reflash the ipmi firmware.\n        :param bool raid_controller: Reflash the raid controller firmware.\n        :param bool bios: Reflash the bios firmware.\n\n        Example::\n\n            # Check the servers active transactions to see progress\n            result = mgr.reflash_firmware(hardware_id=1234)\n        \"\"\"\n\n        return self.hardware.createFirmwareReflashTransaction(\n            bool(ipmi), bool(raid_controller), bool(bios), id=hardware_id)", "response": "Reflash the hardware firmware."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for_ready(self, instance_id, limit=14400, delay=10, pending=False):\n        now = time.time()\n        until = now + limit\n        mask = \"mask[id, lastOperatingSystemReload[id], activeTransaction, provisionDate]\"\n        instance = self.get_hardware(instance_id, mask=mask)\n        while now <= until:\n            if utils.is_ready(instance, pending):\n                return True\n            transaction = utils.lookup(instance, 'activeTransaction', 'transactionStatus', 'friendlyName')\n            snooze = min(delay, until - now)\n            LOGGER.info(\"%s - %d not ready. Auto retry in %ds\", transaction, instance_id, snooze)\n            time.sleep(snooze)\n            instance = self.get_hardware(instance_id, mask=mask)\n            now = time.time()\n\n        LOGGER.info(\"Waiting for %d expired.\", instance_id)\n        return False", "response": "Wait until a server is ready."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, identifier, immediate, comment, reason):\n\n    mgr = SoftLayer.HardwareManager(env.client)\n    hw_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'hardware')\n\n    if not (env.skip_confirmations or formatting.no_going_back(hw_id)):\n        raise exceptions.CLIAbort('Aborted')\n\n    mgr.cancel_hardware(hw_id, reason, comment, immediate)", "response": "Cancel a dedicated server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(env, identifier, postinstall, key, image):\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    keys = []\n    if key:\n        for single_key in key:\n            resolver = SoftLayer.SshKeyManager(env.client).resolve_ids\n            key_id = helpers.resolve_id(resolver, single_key, 'SshKey')\n            keys.append(key_id)\n    if not (env.skip_confirmations or formatting.no_going_back(vs_id)):\n        raise exceptions.CLIAbort('Aborted')\n\n    vsi.reload_instance(vs_id,\n                        post_uri=postinstall,\n                        ssh_keys=keys,\n                        image_id=image)", "response": "Reload operating system on a virtual server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreserve Capacity Group details. Will show which guests are assigned to a reservation.", "response": "def cli(env, identifier, columns):\n    \"\"\"Reserved Capacity Group details. Will show which guests are assigned to a reservation.\"\"\"\n\n    manager = CapacityManager(env.client)\n    mask = \"\"\"mask[instances[id,createDate,guestId,billingItem[id, description, recurringFee, category[name]],\n              guest[modifyDate,id, primaryBackendIpAddress, primaryIpAddress,domain, hostname]]]\"\"\"\n    result = manager.get_object(identifier, mask)\n\n    try:\n        flavor = result['instances'][0]['billingItem']['description']\n    except KeyError:\n        flavor = \"Pending Approval...\"\n\n    table = formatting.Table(columns.columns, title=\"%s - %s\" % (result.get('name'), flavor))\n    # RCI = Reserved Capacity Instance\n    for rci in result['instances']:\n        guest = rci.get('guest', None)\n        if guest is not None:\n            table.add_row([value or formatting.blank() for value in columns.row(guest)])\n        else:\n            table.add_row(['-' for value in columns.columns])\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_client_settings_args(**kwargs):\n    timeout = kwargs.get('timeout')\n    if timeout is not None:\n        timeout = float(timeout)\n\n    return {\n        'endpoint_url': kwargs.get('endpoint_url'),\n        'timeout': timeout,\n        'proxy': kwargs.get('proxy'),\n        'username': kwargs.get('username'),\n        'api_key': kwargs.get('api_key'),\n    }", "response": "Retrieve client settings from user - supplied arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves client settings from environment settings.", "response": "def get_client_settings_env(**_):\n    \"\"\"Retrieve client settings from environment settings.\n\n        :param \\\\*\\\\*kwargs: Arguments that are passed into the client instance\n    \"\"\"\n\n    return {\n        'proxy': os.environ.get('https_proxy'),\n        'username': os.environ.get('SL_USERNAME'),\n        'api_key': os.environ.get('SL_API_KEY'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_client_settings_config_file(**kwargs):  # pylint: disable=inconsistent-return-statements\n    config_files = ['/etc/softlayer.conf', '~/.softlayer']\n    if kwargs.get('config_file'):\n        config_files.append(kwargs.get('config_file'))\n    config_files = [os.path.expanduser(f) for f in config_files]\n    config = utils.configparser.RawConfigParser({\n        'username': '',\n        'api_key': '',\n        'endpoint_url': '',\n        'timeout': '0',\n        'proxy': '',\n    })\n    config.read(config_files)\n\n    if config.has_section('softlayer'):\n        return {\n            'endpoint_url': config.get('softlayer', 'endpoint_url'),\n            'timeout': config.getfloat('softlayer', 'timeout'),\n            'proxy': config.get('softlayer', 'proxy'),\n            'username': config.get('softlayer', 'username'),\n            'api_key': config.get('softlayer', 'api_key'),\n        }", "response": "Retrieve client settings from the possible config file locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_client_settings(**kwargs):\n    all_settings = {}\n    for setting_method in SETTING_RESOLVERS:\n        settings = setting_method(**kwargs)\n        if settings:\n            settings.update((k, v) for k, v in all_settings.items() if v)\n            all_settings = settings\n\n    return all_settings", "response": "Parse client settings.\n\n    Parses settings from various input methods, preferring earlier values\n    to later ones. The settings currently come from explicit user arguments,\n    environmental variables and config files.\n\n        :param \\\\*\\\\*kwargs: Arguments that are passed into the client instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist the categories of a package.", "response": "def cli(env, package_keyname, required):\n    \"\"\"List the categories of a package.\n\n    ::\n\n        # List the categories of Bare Metal servers\n        slcli order category-list BARE_METAL_SERVER\n\n        # List the required categories for Bare Metal servers\n        slcli order category-list BARE_METAL_SERVER --required\n\n    \"\"\"\n    client = env.client\n    manager = ordering.OrderingManager(client)\n    table = formatting.Table(COLUMNS)\n\n    categories = manager.list_categories(package_keyname)\n\n    if required:\n        categories = [cat for cat in categories if cat['isRequired']]\n\n    for cat in categories:\n        table.add_row([\n            cat['itemCategory']['name'],\n            cat['itemCategory']['categoryCode'],\n            'Y' if cat['isRequired'] else 'N'\n        ])\n\n    env.fout(table)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all certificates. :param string method: The type of certificates to list. Options are 'all', 'expired', and 'valid'. :returns: A list of dictionaries representing the requested SSL certs. Example:: # Get all valid SSL certs certs = mgr.list_certs(method='valid') print certs", "response": "def list_certs(self, method='all'):\n        \"\"\"List all certificates.\n\n        :param string method: The type of certificates to list. Options are\n                              'all', 'expired', and 'valid'.\n        :returns: A list of dictionaries representing the requested SSL certs.\n\n        Example::\n\n            # Get all valid SSL certs\n            certs = mgr.list_certs(method='valid')\n            print certs\n\n        \"\"\"\n        ssl = self.client['Account']\n        methods = {\n            'all': 'getSecurityCertificates',\n            'expired': 'getExpiredSecurityCertificates',\n            'valid': 'getValidSecurityCertificates'\n        }\n\n        mask = \"mask[id, commonName, validityDays, notes]\"\n        func = getattr(ssl, methods[method])\n        return func(mask=mask)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a bandwidth report for every virtual sever server and bandwidth pool.", "response": "def cli(env, start, end, sortby):\n    \"\"\"Bandwidth report for every pool/server.\n\n    This reports on the total data transfered for each virtual sever, hardware\n    server and bandwidth pool.\n    \"\"\"\n\n    env.err('Generating bandwidth report for %s to %s' % (start, end))\n\n    table = formatting.Table([\n        'type',\n        'hostname',\n        'public_in',\n        'public_out',\n        'private_in',\n        'private_out',\n        'pool',\n    ])\n    table.sortby = sortby\n\n    def f_type(key, results):\n        \"Filter metric data by type\"\n        return (result['counter'] for result in results\n                if result['type'] == key)\n\n    try:\n        for item in itertools.chain(_get_pooled_bandwidth(env, start, end),\n                                    _get_virtual_bandwidth(env, start, end),\n                                    _get_hardware_bandwidth(env, start, end)):\n            pub_in = int(sum(f_type('publicIn_net_octet', item['data'])))\n            pub_out = int(sum(f_type('publicOut_net_octet', item['data'])))\n            pri_in = int(sum(f_type('privateIn_net_octet', item['data'])))\n            pri_out = int(sum(f_type('privateOut_net_octet', item['data'])))\n            table.add_row([\n                item['type'],\n                item['name'],\n                formatting.b_to_gb(pub_in),\n                formatting.b_to_gb(pub_out),\n                formatting.b_to_gb(pri_in),\n                formatting.b_to_gb(pri_out),\n                item.get('pool') or formatting.blank(),\n            ])\n    except KeyboardInterrupt:\n        env.err(\"Printing collected results and then aborting.\")\n\n    env.out(env.fmt(table))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_file_volumes(self, datacenter=None, username=None,\n                          storage_type=None, **kwargs):\n        \"\"\"Returns a list of file volumes.\n\n        :param datacenter: Datacenter short name (e.g.: dal09)\n        :param username: Name of volume.\n        :param storage_type: Type of volume: Endurance or Performance\n        :param kwargs:\n        :return: Returns a list of file volumes.\n        \"\"\"\n        if 'mask' not in kwargs:\n            items = [\n                'id',\n                'username',\n                'capacityGb',\n                'bytesUsed',\n                'serviceResource.datacenter[name]',\n                'serviceResourceBackendIpAddress',\n                'activeTransactionCount',\n                'fileNetworkMountAddress',\n                'replicationPartnerCount'\n            ]\n            kwargs['mask'] = ','.join(items)\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n\n        _filter['nasNetworkStorage']['serviceResource']['type']['type'] = \\\n            (utils.query_filter('!~ NAS'))\n\n        _filter['nasNetworkStorage']['storageType']['keyName'] = (\n            utils.query_filter('*FILE_STORAGE*'))\n        if storage_type:\n            _filter['nasNetworkStorage']['storageType']['keyName'] = (\n                utils.query_filter('%s_FILE_STORAGE*' % storage_type.upper()))\n\n        if datacenter:\n            _filter['nasNetworkStorage']['serviceResource']['datacenter'][\n                'name'] = (utils.query_filter(datacenter))\n\n        if username:\n            _filter['nasNetworkStorage']['username'] = \\\n                (utils.query_filter(username))\n\n        kwargs['filter'] = _filter.to_dict()\n        return self.client.call('Account', 'getNasNetworkStorage', **kwargs)", "response": "Returns a list of file volumes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns details about the specified file volume.", "response": "def get_file_volume_details(self, volume_id, **kwargs):\n        \"\"\"Returns details about the specified volume.\n\n        :param volume_id: ID of volume.\n        :param kwargs:\n        :return: Returns details about the specified volume.\n        \"\"\"\n\n        if 'mask' not in kwargs:\n            items = [\n                'id',\n                'username',\n                'password',\n                'capacityGb',\n                'bytesUsed',\n                'snapshotCapacityGb',\n                'parentVolume.snapshotSizeBytes',\n                'storageType.keyName',\n                'serviceResource.datacenter[name]',\n                'serviceResourceBackendIpAddress',\n                'fileNetworkMountAddress',\n                'storageTierLevel',\n                'provisionedIops',\n                'lunId',\n                'originalVolumeName',\n                'originalSnapshotName',\n                'originalVolumeSize',\n                'activeTransactionCount',\n                'activeTransactions.transactionStatus[friendlyName]',\n                'replicationPartnerCount',\n                'replicationStatus',\n                'replicationPartners[id,username,'\n                'serviceResourceBackendIpAddress,'\n                'serviceResource[datacenter[name]],'\n                'replicationSchedule[type[keyname]]]',\n            ]\n            kwargs['mask'] = ','.join(items)\n        return self.client.call('Network_Storage', 'getObject',\n                                id=volume_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef order_replicant_volume(self, volume_id, snapshot_schedule,\n                               location, tier=None):\n        \"\"\"Places an order for a replicant file volume.\n\n        :param volume_id: The ID of the primary volume to be replicated\n        :param snapshot_schedule: The primary volume's snapshot\n                                  schedule to use for replication\n        :param location: The location for the ordered replicant volume\n        :param tier: The tier (IOPS per GB) of the primary volume\n        :return: Returns a SoftLayer_Container_Product_Order_Receipt\n        \"\"\"\n\n        file_mask = 'billingItem[activeChildren,hourlyFlag],'\\\n                    'storageTierLevel,osType,staasVersion,'\\\n                    'hasEncryptionAtRest,snapshotCapacityGb,schedules,'\\\n                    'intervalSchedule,hourlySchedule,dailySchedule,'\\\n                    'weeklySchedule,storageType[keyName],provisionedIops'\n        file_volume = self.get_file_volume_details(volume_id,\n                                                   mask=file_mask)\n\n        order = storage_utils.prepare_replicant_order_object(\n            self, snapshot_schedule, location, tier, file_volume, 'file'\n        )\n\n        return self.client.call('Product_Order', 'placeOrder', order)", "response": "Places an order for a replicant file volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplacing an order for a duplicate file volume.", "response": "def order_duplicate_volume(self, origin_volume_id, origin_snapshot_id=None,\n                               duplicate_size=None, duplicate_iops=None,\n                               duplicate_tier_level=None,\n                               duplicate_snapshot_size=None,\n                               hourly_billing_flag=False):\n        \"\"\"Places an order for a duplicate file volume.\n\n        :param origin_volume_id: The ID of the origin volume to be duplicated\n        :param origin_snapshot_id: Origin snapshot ID to use for duplication\n        :param duplicate_size: Size/capacity for the duplicate volume\n        :param duplicate_iops: The IOPS per GB for the duplicate volume\n        :param duplicate_tier_level: Tier level for the duplicate volume\n        :param duplicate_snapshot_size: Snapshot space size for the duplicate\n        :param hourly_billing_flag: Billing type, monthly (False)\n            or hourly (True), default to monthly.\n        :return: Returns a SoftLayer_Container_Product_Order_Receipt\n        \"\"\"\n\n        file_mask = 'id,billingItem[location,hourlyFlag],snapshotCapacityGb,'\\\n                    'storageType[keyName],capacityGb,originalVolumeSize,'\\\n                    'provisionedIops,storageTierLevel,'\\\n                    'staasVersion,hasEncryptionAtRest'\n        origin_volume = self.get_file_volume_details(origin_volume_id,\n                                                     mask=file_mask)\n\n        order = storage_utils.prepare_duplicate_order_object(\n            self, origin_volume, duplicate_iops, duplicate_tier_level,\n            duplicate_size, duplicate_snapshot_size, 'file',\n            hourly_billing_flag\n        )\n\n        if origin_snapshot_id is not None:\n            order['duplicateOriginSnapshotId'] = origin_snapshot_id\n\n        return self.client.call('Product_Order', 'placeOrder', order)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef order_modified_volume(self, volume_id, new_size=None, new_iops=None, new_tier_level=None):\n\n        mask_items = [\n            'id',\n            'billingItem',\n            'storageType[keyName]',\n            'capacityGb',\n            'provisionedIops',\n            'storageTierLevel',\n            'staasVersion',\n            'hasEncryptionAtRest',\n        ]\n        file_mask = ','.join(mask_items)\n        volume = self.get_file_volume_details(volume_id, mask=file_mask)\n\n        order = storage_utils.prepare_modify_order_object(\n            self, volume, new_iops, new_tier_level, new_size\n        )\n\n        return self.client.call('Product_Order', 'placeOrder', order)", "response": "Places an order for modifying an existing file volume."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplacing an order for snapshot space for the given file volume.", "response": "def order_snapshot_space(self, volume_id, capacity, tier, upgrade, **kwargs):\n        \"\"\"Orders snapshot space for the given file volume.\n\n        :param integer volume_id: The ID of the volume\n        :param integer capacity: The capacity to order, in GB\n        :param float tier: The tier level of the file volume, in IOPS per GB\n        :param boolean upgrade: Flag to indicate if this order is an upgrade\n        :return: Returns a SoftLayer_Container_Product_Order_Receipt\n        \"\"\"\n        file_mask = 'id,billingItem[location,hourlyFlag],'\\\n            'storageType[keyName],storageTierLevel,provisionedIops,'\\\n            'staasVersion,hasEncryptionAtRest'\n        file_volume = self.get_file_volume_details(volume_id,\n                                                   mask=file_mask,\n                                                   **kwargs)\n\n        order = storage_utils.prepare_snapshot_order_object(\n            self, file_volume, capacity, tier, upgrade)\n\n        return self.client.call('Product_Order', 'placeOrder', order)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(env, zone):\n\n    manager = SoftLayer.DNSManager(env.client)\n    zone_id = helpers.resolve_id(manager.resolve_ids, zone, name='zone')\n    env.fout(manager.dump_zone(zone_id))", "response": "Print zone in BIND format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rescue(env, identifier):\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    if not (env.skip_confirmations or\n            formatting.confirm(\"This action will reboot this VSI. Continue?\")):\n        raise exceptions.CLIAbort('Aborted')\n\n    vsi.rescue(vs_id)", "response": "Reboot into a rescue image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npower off an active virtual server.", "response": "def power_off(env, identifier, hard):\n    \"\"\"Power off an active virtual server.\"\"\"\n\n    virtual_guest = env.client['Virtual_Guest']\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    if not (env.skip_confirmations or\n            formatting.confirm('This will power off the VS with id %s. '\n                               'Continue?' % vs_id)):\n        raise exceptions.CLIAbort('Aborted.')\n\n    if hard:\n        virtual_guest.powerOff(id=vs_id)\n    else:\n        virtual_guest.powerOffSoft(id=vs_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npowers on a virtual server.", "response": "def power_on(env, identifier):\n    \"\"\"Power on a virtual server.\"\"\"\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    env.client['Virtual_Guest'].powerOn(id=vs_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npauses an active virtual server.", "response": "def pause(env, identifier):\n    \"\"\"Pauses an active virtual server.\"\"\"\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n\n    if not (env.skip_confirmations or\n            formatting.confirm('This will pause the VS with id %s. Continue?'\n                               % vs_id)):\n        raise exceptions.CLIAbort('Aborted.')\n\n    env.client['Virtual_Guest'].pause(id=vs_id)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resume(env, identifier):\n\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    env.client['Virtual_Guest'].resume(id=vs_id)", "response": "Resumes a paused virtual server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting some various bits of information about an account", "response": "def cli(env):\n    \"\"\"Prints some various bits of information about an account\"\"\"\n\n    manager = AccountManager(env.client)\n    summary = manager.get_summary()\n    env.fout(get_snapshot_table(summary))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a table for printing account summary data", "response": "def get_snapshot_table(account):\n    \"\"\"Generates a table for printing account summary data\"\"\"\n    table = formatting.KeyValueTable([\"Name\", \"Value\"], title=\"Account Snapshot\")\n    table.align['Name'] = 'r'\n    table.align['Value'] = 'l'\n    table.add_row(['Company Name', account.get('companyName', '-')])\n    table.add_row(['Balance', utils.lookup(account, 'pendingInvoice', 'startingBalance')])\n    table.add_row(['Upcoming Invoice', utils.lookup(account, 'pendingInvoice', 'invoiceTotalAmount')])\n    table.add_row(['Image Templates', account.get('blockDeviceTemplateGroupCount', '-')])\n    table.add_row(['Dedicated Hosts', account.get('dedicatedHostCount', '-')])\n    table.add_row(['Hardware', account.get('hardwareCount', '-')])\n    table.add_row(['Virtual Guests', account.get('virtualGuestCount', '-')])\n    table.add_row(['Domains', account.get('domainCount', '-')])\n    table.add_row(['Network Storage Volumes', account.get('networkStorageCount', '-')])\n    table.add_row(['Open Tickets', account.get('openTicketCount', '-')])\n    table.add_row(['Network Vlans', account.get('networkVlanCount', '-')])\n    table.add_row(['Subnets', account.get('subnetCount', '-')])\n    table.add_row(['Users', account.get('userCount', '-')])\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(env, prop):\n\n    try:\n        if prop == 'network':\n            env.fout(get_network())\n            return\n\n        meta_prop = META_MAPPING.get(prop) or prop\n        env.fout(SoftLayer.MetadataManager().get(meta_prop))\n    except SoftLayer.TransportError:\n        raise exceptions.CLIAbort(\n            'Cannot connect to the backend service address. Make sure '\n            'this command is being ran from a device on the backend '\n            'network.')", "response": "Find details about this machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of tables with public and private network details.", "response": "def get_network():\n    \"\"\"Returns a list of tables with public and private network details.\"\"\"\n    meta = SoftLayer.MetadataManager()\n    network_tables = []\n    for network_func in [meta.public_network, meta.private_network]:\n        network = network_func()\n\n        table = formatting.KeyValueTable(['name', 'value'])\n        table.align['name'] = 'r'\n        table.align['value'] = 'l'\n        table.add_row(['mac addresses',\n                       formatting.listing(network['mac_addresses'],\n                                          separator=',')])\n        table.add_row(['router', network['router']])\n        table.add_row(['vlans',\n                       formatting.listing(network['vlans'], separator=',')])\n        table.add_row(['vlan ids',\n                       formatting.listing(network['vlan_ids'], separator=',')])\n        network_tables.append(table)\n\n    return network_tables"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_formatter(columns):\n\n    column_map = dict((column.name, column) for column in columns)\n\n    def validate(ctx, param, value):\n        \"\"\"Click validation function.\"\"\"\n        if value == '':\n            raise click.BadParameter('At least one column is required.')\n\n        formatter = ColumnFormatter()\n        for column in [col.strip() for col in value.split(',')]:\n            if column in column_map:\n                formatter.add_column(column_map[column])\n            else:\n                formatter.add_column(Column(column, column.split('.')))\n\n        return formatter\n\n    return validate", "response": "This function returns a callback to use with click options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_column(self, column):\n        self.columns.append(column.name)\n        self.column_funcs.append(column.path)\n\n        if column.mask is not None:\n            self.mask_parts.add(column.mask)", "response": "Add a new column along with a formatting function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a formatted row for the given data.", "response": "def row(self, data):\n        \"\"\"Return a formatted row for the given data.\"\"\"\n        for column in self.column_funcs:\n            if callable(column):\n                yield column(data)\n            else:\n                yield utils.lookup(data, *column)"}
