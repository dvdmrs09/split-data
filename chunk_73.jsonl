{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisengaging axes primarily in order to reduce heat consumption.", "response": "async def disengage_axes(request):\n    \"\"\"\n    Disengage axes (turn off power) primarily in order to reduce heat\n    consumption.\n    :param request: Must contain an \"axes\" field with a list of axes\n        to disengage ([\"x\", \"y\", \"z\", \"a\", \"b\", \"c\"])\n    :return: message and status code\n    \"\"\"\n    hw = hw_from_req(request)\n    data = await request.text()\n    axes = json.loads(data).get('axes')\n    invalid_axes = [ax for ax in axes if ax.lower() not in 'xyzabc']\n    if invalid_axes:\n        message = \"Invalid axes: {}\".format(', '.join(invalid_axes))\n        status = 400\n    else:\n        await hw.disengage_axes([ax.upper() for ax in axes])\n        message = \"Disengaged axes: {}\".format(', '.join(axes))\n        status = 200\n    return web.json_response({\"message\": message}, status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove the robot to the specified position as provided by the control. info endpoint response", "response": "async def move(request):\n    \"\"\"\n    Moves the robot to the specified position as provided by the `control.info`\n    endpoint response\n\n    Post body must include the following keys:\n    - 'target': either 'mount' or 'pipette'\n    - 'point': a tuple of 3 floats for x, y, z\n    - 'mount': must be 'left' or 'right'\n\n    If 'target' is 'pipette', body must also contain:\n    - 'model': must be a valid pipette model (as defined in `pipette_config`)\n    \"\"\"\n    hw = hw_from_req(request)\n    req = await request.text()\n    data = json.loads(req)\n\n    target, point, mount, model, message, error = _validate_move_data(data)\n    if error:\n        status = 400\n    else:\n        status = 200\n        if ff.use_protocol_api_v2():\n            await hw.cache_instruments()\n            if target == 'mount':\n                critical_point = CriticalPoint.MOUNT\n            else:\n                critical_point = None\n            mount = Mount[mount.upper()]\n            target = Point(*point)\n            await hw.home_z()\n            pos = await hw.gantry_position(mount, critical_point)\n            await hw.move_to(mount, target._replace(z=pos.z),\n                             critical_point=critical_point)\n            await hw.move_to(mount, target,\n                             critical_point=critical_point)\n            pos = await hw.gantry_position(mount)\n            message = 'Move complete. New position: {}'.format(pos)\n        else:\n            if target == 'mount':\n                message = _move_mount(hw, mount, point)\n            elif target == 'pipette':\n                message = _move_pipette(hw, mount, model, point)\n\n    return web.json_response({\"message\": message}, status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves a mount in the Z axis and the gantry moves in X Y and Z height.", "response": "def _move_mount(robot, mount, point):\n    \"\"\"\n    The carriage moves the mount in the Z axis, and the gantry moves in X and Y\n\n    Mount movements do not have the same protections calculated in to an\n    existing `move` command like Pipette does, so the safest thing is to home\n    the Z axis, then move in X and Y, then move down to the specified Z height\n    \"\"\"\n    carriage = robot._actuators[mount]['carriage']\n\n    # Home both carriages, to prevent collisions and to ensure that the other\n    # mount doesn't block the one being moved (mount moves are primarily for\n    # changing pipettes, so we don't want the other pipette blocking access)\n    robot.poses = carriage.home(robot.poses)\n    other_mount = 'left' if mount == 'right' else 'right'\n    robot.poses = robot._actuators[other_mount]['carriage'].home(robot.poses)\n\n    robot.gantry.move(\n        robot.poses, x=point[0], y=point[1])\n    robot.poses = carriage.move(\n        robot.poses, z=point[2])\n\n    # These x and y values are hard to interpret because of some internals of\n    # pose tracker. It's mostly z that matters for this operation anyway\n    x, y, _ = tuple(\n        pose_tracker.absolute(\n            robot.poses, robot._actuators[mount]['carriage']))\n    _, _, z = tuple(\n        pose_tracker.absolute(\n            robot.poses, robot.gantry))\n    new_position = (x, y, z)\n    return \"Move complete. New position: {}\".format(new_position)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets request will return a list of available SSIDs", "response": "async def list_networks(request: web.Request) -> web.Response:\n    \"\"\"\n    Get request will return a list of discovered ssids:\n\n    GET /wifi/list\n\n    200 OK\n    { \"list\": [\n        {\n           ssid: string // e.g. \"linksys\", name to connect to\n           signal: int // e.g. 100; arbitrary signal strength, more is better\n           active: boolean // e.g. true; whether there is a connection active\n           security: str // e.g. \"WPA2 802.1X\" raw nmcli security type output\n           securityType: str // e.g. \"wpa-eap\"; see below\n        }\n      ]\n    }\n\n    The securityType field contains a value suitable for passing to the\n    securityType argument of /configure, or 'unsupported'. The security\n    field is mostly useful for debugging if you are unable to connect to\n    the network even though you think you are using the correct security\n    type.\n    \"\"\"\n    try:\n        networks = await nmcli.available_ssids()\n    except RuntimeError as e:\n        return web.json_response({'message': ' '.join(e.args)}, status=500)\n    else:\n        return web.json_response({'list': networks}, status=200)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck for args that are not required for the EAP method.", "response": "def _eap_check_no_extra_args(\n        config: Dict[str, Any], options: Any):\n    # options is an Any because the type annotation for EAP_CONFIG_SHAPE itself\n    # can\u2019t quite express the type properly because of the inference from the\n    # dict annotation.\n    \"\"\"Check for args that are not required for this method (to aid debugging)\n    ``config`` should be the user config.\n    ``options`` should be the options submember for the eap method.\n\n    Before this method is called, the validity of the 'eapType' key should be\n    established.\n    \"\"\"\n    arg_names = [k for k in config.keys() if k != 'eapType']\n    valid_names = [o['name'] for o in options]\n    for an in arg_names:\n        if an not in valid_names:\n            raise ConfigureArgsError(\n                'Option {} is not valid for EAP method {}'\n                .format(an, config['eapType']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _eap_check_option_ok(opt: Dict[str, str], config: Dict[str, Any]):\n    if opt['name'] not in config:\n        if opt['required']:\n            raise ConfigureArgsError(\n                'Required argument {} for eap method {} not present'\n                .format(opt['displayName'], config['eapType']))\n        else:\n            return\n    name = opt['name']\n    o_type = opt['type']\n    arg = config[name]\n    if name in config:\n        if o_type in ('string', 'password') and not isinstance(arg, str):\n            raise ConfigureArgsError('Option {} should be a str'\n                                     .format(name))\n        elif o_type == 'file' and not isinstance(arg, str):\n            raise ConfigureArgsError('Option {} must be a str'\n                                     .format(name))", "response": "Check that a given EAP option is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the eap specific args and replace values where needed.", "response": "def _eap_check_config(eap_config: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\" Check the eap specific args, and replace values where needed.\n\n    Similar to _check_configure_args but for only EAP.\n    \"\"\"\n    eap_type = eap_config.get('eapType')\n    for method in EAP_CONFIG_SHAPE['options']:\n        if method['name'] == eap_type:\n            options = method['options']\n            break\n    else:\n        raise ConfigureArgsError('EAP method {} is not valid'.format(eap_type))\n\n    _eap_check_no_extra_args(eap_config, options)\n\n    for opt in options:  # type: ignore\n        # Ignoring most types to do with EAP_CONFIG_SHAPE because of issues\n        # wth type inference for dict comprehensions\n        _eap_check_option_ok(opt, eap_config)\n        if opt['type'] == 'file' and opt['name'] in eap_config:\n            # Special work for file: rewrite from key id to path\n            eap_config[opt['name']] = _get_key_file(eap_config[opt['name']])\n    return eap_config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake sure that the security_type is known, or throw.", "response": "def _deduce_security(kwargs) -> nmcli.SECURITY_TYPES:\n    \"\"\" Make sure that the security_type is known, or throw. \"\"\"\n    # Security should be one of our valid strings\n    sec_translation = {\n        'wpa-psk': nmcli.SECURITY_TYPES.WPA_PSK,\n        'none': nmcli.SECURITY_TYPES.NONE,\n        'wpa-eap': nmcli.SECURITY_TYPES.WPA_EAP,\n    }\n    if not kwargs.get('securityType'):\n        if kwargs.get('psk') and kwargs.get('eapConfig'):\n            raise ConfigureArgsError(\n                'Cannot deduce security type: psk and eap both passed')\n        elif kwargs.get('psk'):\n            kwargs['securityType'] = 'wpa-psk'\n        elif kwargs.get('eapConfig'):\n            kwargs['securityType'] = 'wpa-eap'\n        else:\n            kwargs['securityType'] = 'none'\n    try:\n        return sec_translation[kwargs['securityType']]\n    except KeyError:\n        raise ConfigureArgsError('securityType must be one of {}'\n                                 .format(','.join(sec_translation.keys())))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_configure_args(configure_args: Dict[str, Any]) -> Dict[str, Any]:\n    # SSID must always be present\n    if not configure_args.get('ssid')\\\n       or not isinstance(configure_args['ssid'], str):\n        raise ConfigureArgsError(\"SSID must be specified\")\n    # If specified, hidden must be a bool\n    if not configure_args.get('hidden'):\n        configure_args['hidden'] = False\n    elif not isinstance(configure_args['hidden'], bool):\n        raise ConfigureArgsError('If specified, hidden must be a bool')\n\n    configure_args['securityType'] = _deduce_security(configure_args)\n\n    # If we have wpa2-personal, we need a psk\n    if configure_args['securityType'] == nmcli.SECURITY_TYPES.WPA_PSK:\n        if not configure_args.get('psk'):\n            raise ConfigureArgsError(\n                'If securityType is wpa-psk, psk must be specified')\n        return configure_args\n\n    # If we have wpa2-enterprise, we need eap config, and we need to check\n    # it\n    if configure_args['securityType'] == nmcli.SECURITY_TYPES.WPA_EAP:\n        if not configure_args.get('eapConfig'):\n            raise ConfigureArgsError(\n                'If securityType is wpa-eap, eapConfig must be specified')\n        configure_args['eapConfig']\\\n            = _eap_check_config(configure_args['eapConfig'])\n        return configure_args\n\n    # If we\u2019re still here we have no security and we\u2019re done\n    return configure_args", "response": "Checks the arguments passed to configure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def configure(request: web.Request) -> web.Response:\n    try:\n        body = await request.json()\n    except json.JSONDecodeError as e:\n        log.debug(\"Error: JSONDecodeError in /wifi/configure: {}\".format(e))\n        return web.json_response({'message': e.msg}, status=400)\n\n    try:\n        configure_kwargs = _check_configure_args(body)\n    except ConfigureArgsError as e:\n        return web.json_response({'message': e.msg}, status=400)\n\n    try:\n        ok, message = await nmcli.configure(**configure_kwargs)\n        log.debug(\"Wifi configure result: {}\".format(message))\n    except ValueError as ve:\n        return web.json_response({'message': str(ve)}, status=400)\n    except TypeError as te:\n        # Indicates an unexpected kwarg; check is done here to avoid keeping\n        # the _check_configure_args signature up to date with nmcli.configure\n        return web.json_response({'message': str(te)}, status=400)\n\n    if ok:\n        return web.json_response({'message': message,\n                                  'ssid': configure_kwargs['ssid']},\n                                 status=201)\n    else:\n        return web.json_response({'message': message}, status=401)", "response": "Configure the current network with WPA - EAP."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def status(request: web.Request) -> web.Response:\n    connectivity = {'status': 'none', 'interfaces': {}}\n    try:\n        connectivity['status'] = await nmcli.is_connected()\n        connectivity['interfaces'] = {\n            i.value: await nmcli.iface_info(i) for i in nmcli.NETWORK_IFACES\n        }\n        log.debug(\"Connectivity: {}\".format(connectivity['status']))\n        log.debug(\"Interfaces: {}\".format(connectivity['interfaces']))\n        status = 200\n    except subprocess.CalledProcessError as e:\n        log.error(\"CalledProcessError: {}\".format(e.stdout))\n        status = 500\n    except FileNotFoundError as e:\n        log.error(\"FileNotFoundError: {}\".format(e))\n        status = 500\n\n    return web.json_response(connectivity, status=status)", "response": "Returns the status of the current machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def add_key(request: web.Request) -> web.Response:\n    keys_dir = CONFIG['wifi_keys_dir']\n    if not request.can_read_body:\n        return web.json_response({'message': \"Must upload key file\"},\n                                 status=400)\n    data = await request.post()\n    keyfile = data.get('key')\n    if not keyfile:\n        return web.json_response(\n            {'message': \"No key 'key' in request\"}, status=400)\n    hasher = hashlib.sha256()\n    key_contents = keyfile.file.read()\n    hasher.update(key_contents)\n    key_hash = hasher.hexdigest()\n    if key_hash in os.listdir(keys_dir):\n        files = os.listdir(os.path.join(keys_dir, key_hash))\n        if files:\n            return web.json_response(\n                {'message': 'Key file already present',\n                 'uri': '/wifi/keys/{}'.format(key_hash),\n                 'id': key_hash,\n                 'name': files[0]},\n                status=200)\n        else:\n            log.warning(\n                \"Key directory with nothing in it: {}\"\n                .format(key_hash))\n            os.rmdir(os.path.join(keys_dir, key_hash))\n    key_hash_path = os.path.join(keys_dir, key_hash)\n    os.mkdir(key_hash_path)\n    with open(os.path.join(key_hash_path,\n                           os.path.basename(keyfile.filename)), 'wb') as f:\n        f.write(key_contents)\n    return web.json_response(\n        {'uri': '/wifi/keys/{}'.format(key_hash),\n         'id': key_hash,\n         'name': os.path.basename(keyfile.filename)},\n        status=201)", "response": "Add a key file to the system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def list_keys(request: web.Request) -> web.Response:\n    keys_dir = CONFIG['wifi_keys_dir']\n    keys: List[Dict[str, str]] = []\n    # TODO(mc, 2018-10-24): add last modified info to keys for sort purposes\n    for path in os.listdir(keys_dir):\n        full_path = os.path.join(keys_dir, path)\n        if os.path.isdir(full_path):\n            in_path = os.listdir(full_path)\n            if len(in_path) > 1:\n                log.warning(\"Garbage in key dir for key {}\".format(path))\n            keys.append(\n                {'uri': '/wifi/keys/{}'.format(path),\n                 'id': path,\n                 'name': os.path.basename(in_path[0])})\n        else:\n            log.warning(\"Garbage in wifi keys dir: {}\".format(full_path))\n    return web.json_response({'keys': keys}, status=200)", "response": "List the keys installed in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def remove_key(request: web.Request) -> web.Response:\n    keys_dir = CONFIG['wifi_keys_dir']\n    available_keys = os.listdir(keys_dir)\n    requested_hash = request.match_info['key_uuid']\n    if requested_hash not in available_keys:\n        return web.json_response(\n            {'message': 'No such key file {}'\n             .format(requested_hash)},\n            status=404)\n    key_path = os.path.join(keys_dir, requested_hash)\n    name = os.listdir(key_path)[0]\n    shutil.rmtree(key_path)\n    return web.json_response(\n        {'message': 'Key file {} deleted'.format(name)},\n        status=200)", "response": "Remove a key.\n\n    ```\n    DELETE /wifi/keys/:id\n\n    -> 200 OK\n    {message: 'Removed key keyfile.pem'}\n    ```"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets request returns the available configuration options for WPA - EAP secured networks", "response": "async def eap_options(request: web.Request) -> web.Response:\n    \"\"\" Get request returns the available configuration options for WPA-EAP.\n\n    Because the options for connecting to WPA-EAP secured networks are quite\n    complex, to avoid duplicating logic this endpoint returns a json object\n    describing the structure of arguments and options for the eap_config arg to\n    /wifi/configure.\n\n    The object is shaped like this:\n    {\n        options: [ // Supported EAP methods and their options. One of these\n                   // method names must be passed in the eapConfig dict\n            {\n                name: str // i.e. TTLS-EAPMSCHAPv2. Should be in the eapType\n                          // key of eapConfig when sent to /configure.\n                options: [\n                    {\n                     name: str // i.e. \"username\"\n                     displayName: str // i.e. \"Username\"\n                     required: bool,\n                     type: str\n                   }\n                ]\n            }\n        ]\n    }\n\n\n    The ``type`` keys denote the semantic kind of the argument. Valid types\n    are:\n\n    password: This is some kind of password. It may be a psk for the network,\n              an Active Directory password, or the passphrase for a private key\n    string:   A generic string; perhaps a username, or a subject-matches\n              domain name for server validation\n    file:     A file that the user must provide. This should be the id of a\n              file previously uploaded via POST /wifi/keys.\n\n\n    Although the arguments are described hierarchically, they should be\n    specified in eap_config as a flat dict. For instance, a /configure\n    invocation for TTLS/EAP-TLS might look like\n\n    ```\n    POST\n    {\n        ssid: \"my-ssid\",\n        securityType: \"wpa-eap\",\n        hidden: false,\n        eapConfig : {\n            eapType: \"TTLS/EAP-TLS\",  // One of the method options\n            identity: \"alice@example.com\", // And then its arguments\n            anonymousIdentity: \"anonymous@example.com\",\n            password: \"testing123\",\n            caCert: \"12d1f180f081b\",\n            phase2CaCert: \"12d1f180f081b\",\n            phase2ClientCert: \"009909fd9fa\",\n            phase2PrivateKey: \"081009fbcbc\"\n            phase2PrivateKeyPassword: \"testing321\"\n        }\n    }\n    ```\n    \"\"\"\n\n    return web.json_response(EAP_CONFIG_SHAPE, status=200)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimplementing the publish method for the base class.", "response": "def do_publish(broker, cmd, f, when, res, meta, *args, **kwargs):\n    \"\"\" Implement the publish so it can be called outside the decorator \"\"\"\n    publish_command = functools.partial(\n        broker.publish,\n        topic=command_types.COMMAND)\n    call_args = _get_args(f, args, kwargs)\n    if when == 'before':\n        broker.logger.info(\"{}: {}\".format(\n            f.__qualname__,\n            {k: v for k, v in call_args.items() if str(k) != 'self'}))\n    command_args = dict(\n        zip(\n            reversed(inspect.getfullargspec(cmd).args),\n            reversed(inspect.getfullargspec(cmd).defaults\n                     or [])))\n\n    # TODO (artyom, 20170927): we are doing this to be able to use\n    # the decorator in Instrument class methods, in which case\n    # self is effectively an instrument.\n    # To narrow the scope of this hack, we are checking if the\n    # command is expecting instrument first.\n    if 'instrument' in inspect.getfullargspec(cmd).args:\n        # We are also checking if call arguments have 'self' and\n        # don't have instruments specified, in which case\n        # instruments should take precedence.\n        if 'self' in call_args and 'instrument' not in call_args:\n            call_args['instrument'] = call_args['self']\n\n    command_args.update({\n        key: call_args[key]\n        for key in\n        (set(inspect.getfullargspec(cmd).args)\n         & call_args.keys())\n    })\n\n    if meta:\n        command_args['meta'] = meta\n\n    payload = cmd(**command_args)\n\n    message = {**payload, '$': when}\n    if when == 'after':\n        message['return'] = res\n    publish_command(\n        message={**payload, '$': when})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread position from driver into a tuple and map 3 - rd value to the axis of a pipette currently used", "response": "def position(axis, hardware, cp=None):\n    \"\"\"\n    Read position from driver into a tuple and map 3-rd value\n    to the axis of a pipette currently used\n    \"\"\"\n\n    if not ff.use_protocol_api_v2():\n        p = hardware._driver.position\n        return (p['X'], p['Y'], p[axis])\n    else:\n        p = hardware.gantry_position(axis, critical_point=cp)\n        return (p.x, p.y, p.z)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the machine hostname and restarts avahi if needed.", "response": "async def setup_hostname() -> str:\n    \"\"\"\n    Intended to be run when the server starts. Sets the machine hostname.\n\n    The machine hostname is set from the systemd-generated machine-id, which\n    changes at every boot.\n\n    Once the hostname is set, we restart avahi.\n\n    This is a separate task from establishing and changing the opentrons\n    machine name, which is UTF-8 and stored in /etc/machine-info as the\n    PRETTY_HOSTNAME and used in the avahi service name.\n\n    :returns: the hostname\n    \"\"\"\n    machine_id = open('/etc/machine-id').read().strip()\n    hostname = machine_id[:6]\n    with open('/etc/hostname', 'w') as ehn:\n        ehn.write(f'{hostname}\\n')\n\n    # First, we run hostnamed which will set the transient hostname\n    # and loaded static hostname from the value we just wrote to\n    # /etc/hostname\n    LOG.debug(\"Setting hostname\")\n    proc = await asyncio.create_subprocess_exec(\n        'hostname', '-F', '/etc/hostname',\n        stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n    stdout, stderr = await proc.communicate()\n    ret = proc.returncode\n    if ret != 0:\n        LOG.error(\n            f'Error starting hostname: {ret} '\n            f'stdout: {stdout} stderr: {stderr}')\n        raise RuntimeError(\"Couldn't run hostname\")\n\n    # Then, with the hostname set, we can restart avahi\n    LOG.debug(\"Restarting avahi\")\n    proc = await asyncio.create_subprocess_exec(\n        'systemctl', 'restart', 'avahi-daemon',\n        stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n    stdout, stderr = await proc.communicate()\n    ret = proc.returncode\n    if ret != 0:\n        LOG.error(\n            f'Error restarting avahi-daemon: {ret} '\n            f'stdout: {stdout} stderr: {stderr}')\n        raise RuntimeError(\"Error restarting avahi\")\n    LOG.debug(\"Updated hostname and restarted avahi OK\")\n    return hostname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the pretty hostname.", "response": "def _update_pretty_hostname(new_val: str):\n    \"\"\" Write a new value for the pretty hostname.\n\n    :raises OSError: If the new value could not be written.\n    \"\"\"\n    try:\n        with open('/etc/machine-info') as emi:\n            contents = emi.read()\n    except OSError:\n        LOG.exception(\"Couldn't read /etc/machine-info\")\n        contents = ''\n    new_contents = ''\n    for line in contents.split('\\n'):\n        if not line.startswith('PRETTY_HOSTNAME'):\n            new_contents += f'{line}\\n'\n    new_contents += f'PRETTY_HOSTNAME={new_val}\\n'\n    with open('/etc/machine-info', 'w') as emi:\n        emi.write(new_contents)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_name(default: str = 'no name set'):\n    try:\n        with open('/etc/machine-info') as emi:\n            contents = emi.read()\n    except OSError:\n        LOG.exception(\n            \"Couldn't read /etc/machine-info\")\n        contents = ''\n    for line in contents.split('\\n'):\n        if line.startswith('PRETTY_HOSTNAME='):\n            return '='.join(line.split('=')[1:])\n    LOG.warning(f\"No PRETTY_HOSTNAME in {contents}, defaulting to {default}\")\n    try:\n        _update_pretty_hostname(default)\n    except OSError:\n        LOG.exception(\"Could not write new pretty hostname!\")\n    return default", "response": "Get the currently - configured name of the machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def set_name(name: str = None) -> str:\n\n    if name:\n        checked_name = name\n        try:\n            _update_pretty_hostname(checked_name)\n        except OSError:\n            LOG.exception(\"Could not set pretty hostname\")\n            checked_name = get_name()\n    else:\n        checked_name = get_name()\n\n    async with _BUS_LOCK:\n        await asyncio.get_event_loop().run_in_executor(\n            None, _set_name_future, checked_name)\n    return checked_name", "response": "Set the name of the node in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def set_name_endpoint(request: web.Request) -> web.Response:\n\n    def build_400(msg: str) -> web.Response:\n        return web.json_response(\n            data={'message': msg},\n            status=400)\n\n    body = await request.json()\n    if 'name' not in body or not isinstance(body['name'], str):\n        return build_400('Body has no \"name\" key with a string')\n\n    new_name = await set_name(body['name'])\n    request.app[DEVICE_NAME_VARNAME] = new_name\n\n    return web.json_response(data={'name': new_name},\n                             status=200)", "response": "Set the name of the robot."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the name of the robot.", "response": "async def get_name_endpoint(request: web.Request) -> web.Response:\n    \"\"\" Get the name of the robot.\n\n    This information is also accessible in /server/update/health, but this\n    endpoint provides symmetry with POST /server/name.\n\n    GET /server/name -> 200 OK, {'name': robot name}\n    \"\"\"\n    return web.json_response(\n        data={'name': request.app[DEVICE_NAME_VARNAME]},\n        status=200)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all custom container files.", "response": "def get_custom_container_files():\n    \"\"\"\n    Traverses environment.get_path('CONTAINERS_DIR') to retrieve\n    all .json files\n    \"\"\"\n\n    def is_special_file(name):\n        return name.startswith('.')\n\n    res = []\n\n    top = infer_config_base_dir()/'containers'\n    for root, dirnames, files in os.walk(top):\n        for name in filter(is_special_file, dirnames):\n            dirnames.remove(name)\n\n        res.extend(\n            [\n                os.path.join(root, name) for name in files\n                if not is_special_file(name) and name.endswith('.json')\n            ])\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves the current location by the given point.", "response": "def move(self, point: Point) -> 'Location':\n        \"\"\"\n        Alter the point stored in the location while preserving the labware.\n\n        This returns a new Location and does not alter the current one. It\n        should be used like\n\n        .. code-block:: python\n\n            >>> loc = Location(Point(1, 1, 1), 'Hi')\n            >>> new_loc = loc.move(Point(1, 1, 1))\n            >>> assert loc_2.point == Point(2, 2, 2)  # True\n            >>> assert loc.point == Point(1, 1, 1)  # True\n\n        \"\"\"\n        return self._replace(point=self.point + point)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a weird container from the old json file", "response": "def _load_weird_container(container_name):\n    \"\"\" Load a container from persisted containers, whatever that is \"\"\"\n    # First must populate \"get persisted container\" list\n    old_container_loading.load_all_containers_from_disk()\n    # Load container from old json file\n    container = old_container_loading.get_persisted_container(\n        container_name)\n    # Rotate coordinates to fit the new deck map\n    rotated_container = database_migration.rotate_container_for_alpha(\n        container)\n    # Save to the new database\n    database.save_new_container(rotated_container, container_name)\n    return container"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a container from a variety of methods.", "response": "def _setup_container(container_name):\n    \"\"\" Try and find a container in a variety of methods \"\"\"\n    for meth in (database.load_container,\n                 load_new_labware,\n                 _load_weird_container):\n        log.debug(\n            f\"Trying to load container {container_name} via {meth.__name__}\")\n        try:\n            container = meth(container_name)\n            if meth == _load_weird_container:\n                container.properties['type'] = container_name\n            log.info(f\"Loaded {container_name} from {meth.__name__}\")\n            break\n        except (ValueError, KeyError):\n            log.info(f\"{container_name} not in {meth.__name__}\")\n    else:\n        raise KeyError(f\"Unknown labware {container_name}\")\n\n    container_x, container_y, container_z = container._coordinates\n\n    # infer z from height\n    if container_z == 0 and 'height' in container[0].properties:\n        container_z = container[0].properties['height']\n\n    from opentrons.util.vector import Vector\n    container._coordinates = Vector(\n        container_x,\n        container_y,\n        container_z)\n\n    return container"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclearing the tip length of all the pipette members.", "response": "def clear_tips(self):\n        \"\"\"\n        If reset is called with a tip attached, the tip must be removed\n        before the poses and _instruments members are cleared. If the tip is\n        not removed, the effective length of the pipette remains increased by\n        the length of the tip, and subsequent `_add_tip` calls will increase\n        the length in addition to this. This should be fixed by changing pose\n        tracking to that it tracks the tip as a separate node rather than\n        adding and subtracting the tip length to the pipette length.\n        \"\"\"\n        for instrument in self._instruments.values():\n            if instrument.tip_attached:\n                instrument._remove_tip(instrument._tip_length)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cache_instrument_models(self):\n        log.debug(\"Updating instrument model cache\")\n        for mount in self.model_by_mount.keys():\n            model_value = self._driver.read_pipette_model(mount)\n            if model_value:\n                id_response = self._driver.read_pipette_id(mount)\n            else:\n                id_response = None\n            self.model_by_mount[mount] = {\n                'model': model_value,\n                'id': id_response\n            }\n            log.debug(\"{}: {} [{}]\".format(\n                mount,\n                self.model_by_mount[mount]['model'],\n                self.model_by_mount[mount]['id']))", "response": "Queries Smoothie for the model and ID strings of attached pipettes and saves them in the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef identify(self, seconds):\n        from time import sleep\n        for i in range(seconds):\n            self.turn_off_button_light()\n            sleep(0.25)\n            self.turn_on_button_light()\n            sleep(0.25)", "response": "Identify a robot by flashing the light around the frame button for 10s\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an instrument to a pipette.", "response": "def add_instrument(self, mount, instrument):\n        \"\"\"\n        Adds instrument to a robot.\n\n        Parameters\n        ----------\n        mount : str\n            Specifies which axis the instruments is attached to.\n            Valid options are \"left\" or \"right\".\n        instrument : Instrument\n            An instance of a :class:`Pipette` to attached to the axis.\n\n        Notes\n        -----\n        A canonical way to add to add a Pipette to a robot is:\n\n        ::\n\n            from opentrons import instruments\n            m300 = instruments.P300_Multi(mount='left')\n\n        This will create a pipette and call :func:`add_instrument`\n        to attach the instrument.\n        \"\"\"\n        if mount in self._instruments:\n            prev_instr = self._instruments[mount]\n            raise RuntimeError('Instrument {0} already on {1} mount'.format(\n                prev_instr.name, mount))\n        self._instruments[mount] = instrument\n\n        instrument.instrument_actuator = self._actuators[mount]['plunger']\n        instrument.instrument_mover = self._actuators[mount]['carriage']\n\n        # instrument_offset is the distance found (with tip-probe) between\n        # the pipette's expected position, and the actual position\n        # this is expected to be no greater than ~3mm\n        # Z is not included, because Z offsets found during tip-probe are used\n        # to determined the tip's length\n        cx, cy, _ = self.config.instrument_offset[mount][instrument.type]\n\n        # model_offset is the expected position of the pipette, determined\n        # by designed dimensions of that model (eg: p10-multi vs p300-single)\n        mx, my, mz = instrument.model_offset\n\n        # combine each offset to get the pipette's position relative to gantry\n        _x, _y, _z = (\n            mx + cx,\n            my + cy,\n            mz\n        )\n        # if it's the left mount, apply the offset from right pipette\n        if mount == 'left':\n            _x, _y, _z = (\n               _x + self.config.mount_offset[0],\n               _y + self.config.mount_offset[1],\n               _z + self.config.mount_offset[2]\n            )\n        self.poses = pose_tracker.add(\n            self.poses,\n            instrument,\n            parent=mount,\n            point=(_x, _y, _z)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, port=None, options=None):\n\n        self._driver.connect(port=port)\n        self.fw_version = self._driver.get_fw_version()\n\n        # the below call to `cache_instrument_models` is relied upon by\n        # `Session._simulate()`, which calls `robot.connect()` after exec'ing a\n        # protocol. That protocol could very well have different pipettes than\n        # what are physically attached to the robot\n        self.cache_instrument_models()", "response": "Connects the robot to a serial port."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhomes the robot s head and plunger motors.", "response": "def home(self, *args, **kwargs):\n        \"\"\"\n        Home robot's head and plunger motors.\n        \"\"\"\n\n        # Home gantry first to avoid colliding with labware\n        # and to make sure tips are not in the liquid while\n        # homing plungers. Z/A axis will automatically home before X/Y\n        self.poses = self.gantry.home(self.poses)\n        # Then plungers\n        self.poses = self._actuators['left']['plunger'].home(self.poses)\n        self.poses = self._actuators['right']['plunger'].home(self.poses)\n\n        # next move should not use any previously used instrument or labware\n        # to prevent robot.move_to() from using risky path optimization\n        self._previous_instrument = None\n        self._prev_container = None\n\n        # explicitly update carriage Mover positions in pose tree\n        # because their Mover.home() commands aren't used here\n        for a in self._actuators.values():\n            self.poses = a['carriage'].update_pose_from_driver(self.poses)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the speeds of the robot to 400 mm / sec or the maximum speed of that axis.", "response": "def head_speed(\n            self, combined_speed=None,\n            x=None, y=None, z=None, a=None, b=None, c=None):\n        \"\"\"\n        Set the speeds (mm/sec) of the robot\n\n        Parameters\n        ----------\n        combined_speed : number specifying a combined-axes speed\n        <axis> : key/value pair, specifying the maximum speed of that axis\n\n        Examples\n        ---------\n\n        >>> from opentrons import robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> robot.head_speed(combined_speed=400) # doctest: +SKIP\n        #  sets the head speed to 400 mm/sec or the axis max per axis\n        >>> robot.head_speed(x=400, y=200) # doctest: +SKIP\n        # sets max speeds of X and Y\n        \"\"\"\n        user_set_speeds = {'x': x, 'y': y, 'z': z, 'a': a, 'b': b, 'c': c}\n        axis_max_speeds = {\n            axis: value\n            for axis, value in user_set_speeds.items()\n            if value\n        }\n        if axis_max_speeds:\n            self._driver.set_axis_max_speed(axis_max_speeds)\n        if combined_speed:\n            self._driver.set_speed(combined_speed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move_to(\n            self,\n            location,\n            instrument,\n            strategy='arc',\n            **kwargs):\n        \"\"\"\n        Move an instrument to a coordinate, container or a coordinate within\n        a container.\n\n        Parameters\n        ----------\n        location : one of the following:\n            1. :class:`Placeable` (i.e. Container, Deck, Slot, Well) \u2014\u00a0will\n            move to the origin of a container.\n            2. :class:`Vector` move to the given coordinate in Deck coordinate\n            system.\n            3. (:class:`Placeable`, :class:`Vector`) move to a given coordinate\n            within object's coordinate system.\n\n        instrument :\n            Instrument to move relative to. If ``None``, move relative to the\n            center of a gantry.\n\n        strategy : {'arc', 'direct'}\n            ``arc`` : move to the point using arc trajectory\n            avoiding obstacles.\n\n            ``direct`` : move to the point in a straight line.\n        \"\"\"\n\n        placeable, coordinates = containers.unpack_location(location)\n\n        # because the top position is what is tracked,\n        # this checks if coordinates doesn't equal top\n        offset = subtract(coordinates, placeable.top()[1])\n\n        if isinstance(placeable, containers.WellSeries):\n            placeable = placeable[0]\n\n        target = add(\n            pose_tracker.absolute(\n                self.poses,\n                placeable\n            ),\n            offset.coordinates\n        )\n\n        if self._previous_instrument:\n            if self._previous_instrument != instrument:\n                self._previous_instrument.retract()\n                # because we're switching pipettes, this ensures a large (safe)\n                # Z arc height will be used for the new pipette\n                self._prev_container = None\n\n        self._previous_instrument = instrument\n\n        if strategy == 'arc':\n            arc_coords = self._create_arc(instrument, target, placeable)\n            for coord in arc_coords:\n                self.poses = instrument._move(\n                    self.poses,\n                    **coord)\n\n        elif strategy == 'direct':\n            position = {'x': target[0], 'y': target[1], 'z': target[2]}\n            self.poses = instrument._move(\n                self.poses,\n                **position)\n        else:\n            raise RuntimeError(\n                'Unknown move strategy: {}'.format(strategy))", "response": "Move an object to a given location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_arc(self, inst, destination, placeable=None):\n        this_container = None\n        if isinstance(placeable, containers.Well):\n            this_container = placeable.get_parent()\n        elif isinstance(placeable, containers.WellSeries):\n            this_container = placeable.get_parent()\n        elif isinstance(placeable, containers.Container):\n            this_container = placeable\n\n        if this_container and self._prev_container == this_container:\n            # movements that stay within the same container do not need to\n            # avoid other containers on the deck, so the travel height of\n            # arced movements can be relative to just that one container\n            arc_top = self.max_placeable_height_on_deck(this_container)\n            arc_top += TIP_CLEARANCE_LABWARE\n        elif self._use_safest_height:\n            # bring the pipettes up as high as possible while calibrating\n            arc_top = inst._max_deck_height()\n        else:\n            # bring pipette up above the tallest container currently on deck\n            arc_top = self.max_deck_height() + TIP_CLEARANCE_DECK\n\n        self._prev_container = this_container\n\n        # if instrument is currently taller than arc_top, don't move down\n        _, _, pip_z = pose_tracker.absolute(self.poses, inst)\n\n        arc_top = max(arc_top, destination[2], pip_z)\n        arc_top = min(arc_top, inst._max_deck_height())\n\n        strategy = [\n            {'z': arc_top},\n            {'x': destination[0], 'y': destination[1]},\n            {'z': destination[2]}\n        ]\n\n        return strategy", "response": "Create an arc from the given location and a list of coordinates to arrive to the destination coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disconnect(self):\n        if self._driver:\n            self._driver.disconnect()\n\n        self.axis_homed = {\n            'x': False, 'y': False, 'z': False, 'a': False, 'b': False}", "response": "Disconnects from the robot."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the row and column offsets for the current locale.", "response": "def get_slot_offsets(self):\n        \"\"\"\n        col_offset\n        - from bottom left corner of 1 to bottom corner of 2\n\n        row_offset\n        - from bottom left corner of 1 to bottom corner of 4\n\n        TODO: figure out actual X and Y offsets (from origin)\n        \"\"\"\n        SLOT_OFFSETS = {\n            'slots': {\n                'col_offset': 132.50,\n                'row_offset': 90.5\n            }\n        }\n        slot_settings = SLOT_OFFSETS.get(self.get_deck_slot_types())\n        row_offset = slot_settings.get('row_offset')\n        col_offset = slot_settings.get('col_offset')\n        return (row_offset, col_offset)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of all the instruments in the system", "response": "def get_instruments(self, name=None):\n        \"\"\"\n        :returns: sorted list of (mount, instrument)\n        \"\"\"\n        if name:\n            return self.get_instruments_by_name(name)\n\n        return sorted(\n            self._instruments.items(), key=lambda s: s[0].lower())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_container_to_pose_tracker(self, location, container: Container):\n        self.poses = pose_tracker.add(\n            self.poses,\n            container,\n            container.parent,\n            pose_tracker.Point(*container._coordinates))\n\n        for well in container:\n            center_x, center_y, center_z = well.top()[1]\n            offset_x, offset_y, offset_z = well._coordinates\n            center_z = 0\n            self.poses = pose_tracker.add(\n                self.poses,\n                well,\n                container,\n                pose_tracker.Point(\n                    center_x + offset_x,\n                    center_y + offset_y,\n                    center_z + offset_z\n                )\n            )", "response": "Adds container and child wells to pose tracker. Sets self. pens to pose_tracker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget model names of attached pipettes", "response": "def get_attached_pipettes(self):\n        \"\"\"\n        Gets model names of attached pipettes\n\n        :return: :dict with keys 'left' and 'right' and a model string for each\n            mount, or 'uncommissioned' if no model string available\n        \"\"\"\n        left_data = {\n                'mount_axis': 'z',\n                'plunger_axis': 'b',\n                'model': self.model_by_mount['left']['model'],\n                'id': self.model_by_mount['left']['id']\n            }\n        left_model = left_data.get('model')\n\n        if left_model:\n            tip_length = pipette_config.load(\n                left_model, left_data['id']).tip_length\n            left_data.update({'tip_length': tip_length})\n\n        right_data = {\n                'mount_axis': 'a',\n                'plunger_axis': 'c',\n                'model': self.model_by_mount['right']['model'],\n                'id': self.model_by_mount['right']['id']\n            }\n        right_model = right_data.get('model')\n        if right_model:\n            tip_length = pipette_config.load(\n                right_model, right_data['id']).tip_length\n            right_data.update({'tip_length': tip_length})\n        return {\n            'left': left_data,\n            'right': right_data\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalibrates a container using the bottom of the first well with respect to instrument.", "response": "def calibrate_container_with_instrument(self,\n                                            container: Container,\n                                            instrument,\n                                            save: bool\n                                            ):\n        '''Calibrates a container using the bottom of the first well'''\n        well = container[0]\n\n        # Get the relative position of well with respect to instrument\n        delta = pose_tracker.change_base(\n            self.poses,\n            src=instrument,\n            dst=well\n        )\n\n        if fflags.calibrate_to_bottom():\n\n            delta_x = delta[0]\n            delta_y = delta[1]\n            if 'tiprack' in container.get_type():\n                delta_z = delta[2]\n            else:\n                delta_z = delta[2] + well.z_size()\n        else:\n            delta_x = delta[0]\n            delta_y = delta[1]\n            delta_z = delta[2]\n\n        self.poses = self._calibrate_container_with_delta(\n            self.poses,\n            container,\n            delta_x,\n            delta_y,\n            delta_z,\n            save\n        )\n\n        self.max_deck_height.cache_clear()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef max_placeable_height_on_deck(self, placeable):\n        offset = placeable.top()[1]\n        placeable_coordinate = add(\n            pose_tracker.absolute(\n                self.poses,\n                placeable\n            ),\n            offset.coordinates\n        )\n        placeable_tallest_point = pose_tracker.max_z(self.poses, placeable)\n        return placeable_coordinate[2] + placeable_tallest_point", "response": "Calculates the maximum height of a deck from a placeable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting the write process.", "response": "def _begin_write(session: UpdateSession,\n                 loop: asyncio.AbstractEventLoop,\n                 rootfs_file_path: str):\n    \"\"\" Start the write process. \"\"\"\n    session.set_progress(0)\n    session.set_stage(Stages.WRITING)\n    write_future = asyncio.ensure_future(loop.run_in_executor(\n        None, file_actions.write_update, rootfs_file_path,\n        session.set_progress))\n\n    def write_done(fut):\n        exc = fut.exception()\n        if exc:\n            session.set_error(getattr(exc, 'short', str(type(exc))),\n                              str(exc))\n        else:\n            session.set_stage(Stages.DONE)\n\n    write_future.add_done_callback(write_done)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the validation process.", "response": "def _begin_validation(\n        session: UpdateSession,\n        config: config.Config,\n        loop: asyncio.AbstractEventLoop,\n        downloaded_update_path: str)\\\n        -> asyncio.futures.Future:\n    \"\"\" Start the validation process. \"\"\"\n    session.set_stage(Stages.VALIDATING)\n    cert_path = config.update_cert_path\\\n        if config.signature_required else None\n\n    validation_future \\\n        = asyncio.ensure_future(loop.run_in_executor(\n            None, file_actions.validate_update,\n            downloaded_update_path, session.set_progress, cert_path))\n\n    def validation_done(fut):\n        exc = fut.exception()\n        if exc:\n            session.set_error(getattr(exc, 'short', str(type(exc))),\n                              str(exc))\n        else:\n            rootfs_file = fut.result()\n            loop.call_soon_threadsafe(_begin_write,\n                                      session,\n                                      loop,\n                                      rootfs_file)\n    validation_future.add_done_callback(validation_done)\n    return validation_future"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def file_upload(\n        request: web.Request, session: UpdateSession) -> web.Response:\n    \"\"\" Serves /update/:session/file\n\n    Requires multipart (encoding doesn't matter) with a file field in the\n    body called 'ot2-system.zip'.\n    \"\"\"\n    if session.stage != Stages.AWAITING_FILE:\n        return web.json_response(\n            data={'error': 'file-already-uploaded',\n                  'message': 'A file has already been sent for this update'},\n            status=409)\n    reader = await request.multipart()\n    async for part in reader:\n        if part.name != 'ot2-system.zip':\n            LOG.warning(\n                f\"Unknown field name {part.name} in file_upload, ignoring\")\n            await part.release()\n        else:\n            await _save_file(part, session.download_path)\n\n    _begin_validation(\n        session,\n        config.config_from_request(request),\n        asyncio.get_event_loop(),\n        os.path.join(session.download_path, 'ot2-system.zip'))\n\n    return web.json_response(data=session.state,\n                             status=201)", "response": "Serves / update/:session file\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncommits the current state of the current update.", "response": "async def commit(\n        request: web.Request, session: UpdateSession) -> web.Response:\n    \"\"\" Serves /update/:session/commit \"\"\"\n    if session.stage != Stages.DONE:\n        return web.json_response(\n            data={'error': 'not-ready',\n                  'message': f'System is not ready to commit the update '\n                  f'(currently {session.stage.value.short})'},\n            status=409)\n    async with request.app[RESTART_LOCK_NAME]:\n        file_actions.commit_update()\n        session.set_stage(Stages.READY_FOR_RESTART)\n\n    return web.json_response(\n        data=session.state,\n        status=200)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride of build_py. get_data_files that includes out of tree configs.", "response": "def _get_data_files(self):\n        \"\"\"\n        Override of build_py.get_data_files that includes out of tree configs.\n        These are currently hardcoded to include everything in\n         ../shared-data/robot-data, which will move to\n        opentrons/config/shared-data\n        \"\"\"\n        files = super()._get_data_files()\n        # We don\u2019t really want to duplicate logic used in the original\n        # implementation, but we can back out what it did with commonpath -\n        # should be something ending in opentrons\n        build_base = os.path.commonpath([f[2] for f in files])\n        # We want a list of paths to only files relative to ../shared-data\n        to_include = get_shared_data_files()\n        destination = os.path.join(build_base, DEST_BASE_PATH)\n        # And finally, tell the system about our files\n        files.append(('opentrons', SHARED_DATA_PATH,\n                      destination, to_include))\n        return files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a list of tuples of form depth text return a dictionary representing a command tree.", "response": "def from_list(commands):\n    \"\"\"\n    Given a list of tuples of form (depth, text)\n    that represents a DFS traversal of a command tree,\n    returns a dictionary representing command tree.\n    \"\"\"\n    def subtrees(commands, level):\n        if not commands:\n            return\n\n        acc = []\n        parent, *commands = commands\n\n        for command in commands:\n            if command['level'] > level:\n                acc.append(command)\n            else:\n                yield (parent, acc)\n                parent = command\n                acc.clear()\n        yield (parent, acc)\n\n    def walk(commands, level=0):\n        return [\n            {\n                'description': key['description'],\n                'children': walk(subtree, level + 1),\n                'id': key['id']\n            }\n            for key, subtree in subtrees(commands, level)\n        ]\n\n    return walk(commands)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calibrate(self):\n        '''\n        Calibration involves probing for top plate to get the plate height\n        '''\n        if self._driver and self._driver.is_connected():\n            self._driver.probe_plate()\n            # return if successful or not?\n            self._engaged = False", "response": "Calibration involves probing for the top plate and checking if the plate is already connected."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect to the serial port of the MagDeck.", "response": "def connect(self):\n        '''\n        Connect to the serial port\n        '''\n        if self._port:\n            self._driver = MagDeckDriver()\n            self._driver.connect(self._port)\n            self._device_info = self._driver.get_device_info()\n        else:\n            # Sanity check: Should never happen, because connect should\n            # never be called without a port on Module\n            raise MissingDevicePortError(\n                \"MagDeck couldnt connect to port {}\".format(self._port)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the lowest frame in the traceback that matches the given protocol name.", "response": "def _find_protocol_error(tb, proto_name):\n    \"\"\"Return the FrameInfo for the lowest frame in the traceback from the\n    protocol.\n    \"\"\"\n    tb_info = traceback.extract_tb(tb)\n    for frame in reversed(tb_info):\n        if frame.filename == proto_name:\n            return frame\n    else:\n        raise KeyError"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_protocol(protocol_code: Any = None,\n                 protocol_json: Dict[Any, Any] = None,\n                 simulate: bool = False,\n                 context: ProtocolContext = None):\n    \"\"\" Create a ProtocolRunner instance from one of a variety of protocol\n    sources.\n\n    :param protocol_bytes: If the protocol is a Python protocol, pass the\n    file contents here.\n    :param protocol_json: If the protocol is a json file, pass the contents\n    here.\n    :param simulate: True to simulate; False to execute. If this is not an\n    OT2, ``simulate`` will be forced ``True``.\n    :param context: The context to use. If ``None``, create a new\n    ProtocolContext.\n    \"\"\"\n    if not config.IS_ROBOT:\n        simulate = True # noqa - will be used later\n    if None is context and simulate:\n        true_context = ProtocolContext()\n        true_context.home()\n        MODULE_LOG.info(\"Generating blank protocol context for simulate\")\n    elif context:\n        true_context = context\n    else:\n        raise RuntimeError(\n            'Will not automatically generate hardware controller')\n    if None is not protocol_code:\n        _run_python(protocol_code, true_context)\n    elif None is not protocol_json:\n        protocol_version = get_protocol_schema_version(protocol_json)\n        if protocol_version > 3:\n            raise RuntimeError(\n                f'JSON Protocol version {protocol_version} is not yet ' +\n                'supported in this version of the API')\n\n        validate_protocol(protocol_json)\n\n        if protocol_version >= 3:\n            ins = execute_v3.load_pipettes_from_json(\n                true_context, protocol_json)\n            lw = execute_v3.load_labware_from_json_defs(\n                true_context, protocol_json)\n            execute_v3.dispatch_json(true_context, protocol_json, ins, lw)\n        else:\n            ins = execute_v1.load_pipettes_from_json(\n                true_context, protocol_json)\n            lw = execute_v1.load_labware_from_json_loadnames(\n                true_context, protocol_json)\n            execute_v1.dispatch_json(true_context, protocol_json, ins, lw)\n    else:\n        raise RuntimeError(\"run_protocol must have either code or json\")", "response": "Runs a protocol from a Python file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ports_by_name(device_name):\n    '''Returns all serial devices with a given name'''\n    filtered_devices = filter(\n        lambda device: device_name in device[1],\n        list_ports.comports()\n    )\n    device_ports = [device[0] for device in filtered_devices]\n    return device_ports", "response": "Returns all serial devices with a given name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serial_with_temp_timeout(serial_connection, timeout):\n    '''Implements a temporary timeout for a serial connection'''\n    saved_timeout = serial_connection.timeout\n    if timeout is not None:\n        serial_connection.timeout = timeout\n    yield serial_connection\n    serial_connection.timeout = saved_timeout", "response": "Implements a temporary timeout for a serial connection"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_to_device_and_return(cmd, ack, device_connection):\n    '''Writes to a serial device.\n    - Formats command\n    - Wait for ack return\n    - return parsed response'''\n    log.debug('Write -> {}'.format(cmd.encode()))\n    device_connection.write(cmd.encode())\n    response = device_connection.read_until(ack.encode())\n    log.debug('Read <- {}'.format(response))\n    if ack.encode() not in response:\n        raise SerialNoResponse(\n            'No response from serial port after {} second(s)'.format(\n                device_connection.timeout))\n    clean_response = _parse_serial_response(response, ack.encode())\n    if clean_response:\n        return clean_response.decode()\n    return ''", "response": "Writes to a serial device and waits for ack and returns parsed response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _attempt_command_recovery(command, ack, serial_conn):\n    '''Recovery after following a failed write_and_return() atempt'''\n    with serial_with_temp_timeout(serial_conn, RECOVERY_TIMEOUT) as device:\n        response = _write_to_device_and_return(command, ack, device)\n    if response is None:\n        log.debug(\"No valid response during _attempt_command_recovery\")\n        raise RuntimeError(\n            \"Recovery attempted - no valid serial response \"\n            \"for command: {} in {} seconds\".format(\n                command.encode(), RECOVERY_TIMEOUT))\n    return response", "response": "Attempts to send a command to the device and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a command and return the response", "response": "def write_and_return(\n        command, ack, serial_connection, timeout=DEFAULT_WRITE_TIMEOUT):\n    '''Write a command and return the response'''\n    clear_buffer(serial_connection)\n    with serial_with_temp_timeout(\n            serial_connection, timeout) as device_connection:\n        response = _write_to_device_and_return(command, ack, device_connection)\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(device_name=None, port=None, baudrate=115200):\n    '''\n    Creates a serial connection\n    :param device_name: defaults to 'Smoothieboard'\n    :param baudrate: integer frequency for serial communication\n    :return: serial.Serial connection\n    '''\n    if not port:\n        port = get_ports_by_name(device_name=device_name)[0]\n    log.debug(\"Device name: {}, Port: {}\".format(device_name, port))\n    return _connect(port_name=port, baudrate=baudrate)", "response": "Creates a serial connection to the specified device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the advanced settings for the current page.", "response": "async def get_advanced_settings(request: web.Request) -> web.Response:\n    \"\"\"\n    Handles a GET request and returns a json body with the key \"settings\" and a\n    value that is a list of objects where each object has keys \"id\", \"title\",\n    \"description\", and \"value\"\n    \"\"\"\n    res = _get_adv_settings()\n    return web.json_response(res)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a POST request with a json body that has keys id and value and saves the value of value in the opentrons. config. advanced_settings. settings.", "response": "async def set_advanced_setting(request: web.Request) -> web.Response:\n    \"\"\"\n    Handles a POST request with a json body that has keys \"id\" and \"value\",\n    where the value of \"id\" must correspond to an id field of a setting in\n    `opentrons.config.advanced_settings.settings`. Saves the value of \"value\"\n    for the setting that matches the supplied id.\n    \"\"\"\n    data = await request.json()\n    key = data.get('id')\n    value = data.get('value')\n    if key and key in advs.settings_by_id.keys():\n        advs.set_adv_setting(key, value)\n        res = _get_adv_settings()\n        status = 200\n    else:\n        res = {'message': 'ID {} not found in settings list'.format(key)}\n        status = 400\n    return web.json_response(res, status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the user s configuration.", "response": "async def reset(request: web.Request) -> web.Response:\n    \"\"\" Execute a reset of the requested parts of the user configuration.\n    \"\"\"\n    data = await request.json()\n    ok, bad_key = _check_reset(data)\n    if not ok:\n        return web.json_response(\n            {'message': '{} is not a valid reset option'\n             .format(bad_key)},\n            status=400)\n    log.info(\"Reset requested for {}\".format(', '.join(data.keys())))\n    if data.get('tipProbe'):\n        config = rc.load()\n        if ff.use_protocol_api_v2():\n            config = config._replace(\n                instrument_offset=rc.build_fallback_instrument_offset({}))\n        else:\n            config.tip_length.clear()\n        rc.save_robot_settings(config)\n    if data.get('labwareCalibration'):\n        if ff.use_protocol_api_v2():\n            labware.clear_calibrations()\n        else:\n            db.reset()\n\n    if data.get('bootScripts'):\n        if IS_ROBOT:\n            if os.path.exists('/data/boot.d'):\n                shutil.rmtree('/data/boot.d')\n        else:\n            log.debug('Not on pi, not removing /data/boot.d')\n    return web.json_response({}, status=200)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def available_resets(request: web.Request) -> web.Response:\n    return web.json_response({'options': _settings_reset_options}, status=200)", "response": "Indicate what parts of the user configuration are available for reset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def modify_pipette_settings(request: web.Request) -> web.Response:\n    pipette_id = request.match_info['id']\n    whole_config = pc.load_config_dict(pipette_id)\n    config_match = pc.list_mutable_configs(pipette_id)\n    data = await request.json()\n    if not data.get('fields'):\n        return web.json_response(config_match, status=200)\n\n    for key, value in data['fields'].items():\n        if value:\n            config = value['value']\n            default = config_match[key]\n            if config < default['min'] or config > default['max']:\n                return web.json_response(\n                    {'message': '{} out of range with {}'.format(key, config)},\n                    status=412)\n    pc.save_overrides(pipette_id, data['fields'], whole_config.get('model'))\n    updated_configs = {'fields': pc.list_mutable_configs(pipette_id)}\n    return web.json_response(updated_configs, status=200)", "response": "Modify the settings of a pipette."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def available_ssids() -> List[Dict[str, Any]]:\n    fields = ['ssid', 'signal', 'active', 'security']\n    cmd = ['--terse',\n           '--fields',\n           ','.join(fields),\n           'device',\n           'wifi',\n           'list']\n    out, err = await _call(cmd)\n    if err:\n        raise RuntimeError(err)\n    output = _dict_from_terse_tabular(\n        fields, out,\n        transformers={'signal': lambda s: int(s) if s.isdigit() else None,\n                      'active': lambda a: a.lower() == 'yes',\n                      'ssid': lambda s: s if s != '--' else None})\n\n    return [_add_security_type_to_scan(nw) for nw in output if nw['ssid']]", "response": "List the available SSIDs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def connections(\n        for_type: Optional[CONNECTION_TYPES] = None) -> List[Dict[str, str]]:\n    \"\"\" Return the list of configured connections.\n\n    This is all connections that nmcli knows about and manages.\n    Each connection is a dict containing some basic information - the\n    information retrievable from nmcli connection show. Further information\n    should be queried on a connection by connection basis.\n\n    If for_type is not None, it should be a str containing an element of\n    CONNECTION_TYPES, and results will be limited to that connection type.\n    \"\"\"\n    fields = ['name', 'type', 'active']\n    res, _ = await _call(['-t', '-f', ','.join(fields), 'connection', 'show'])\n    found = _dict_from_terse_tabular(\n        fields,\n        res,\n        # \u2019ethernet\u2019 or \u2019wireless\u2019 from \u2019802-11-wireless\u2019 or \u2019802-4-ethernet\u2019\n        # and bools from \u2019yes\u2019 or \u2019no\n        transformers={'type': lambda s: s.split('-')[-1],\n                      'active': lambda s: s.lower() == 'yes'}\n    )\n    if for_type is not None:\n        should_return = []\n        for c in found:\n            if c['type'] == for_type.value:\n                should_return.append(c)\n        return should_return\n    else:\n        return found", "response": "Return the list of configured connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a connection exists for the given SSID.", "response": "async def connection_exists(ssid: str) -> Optional[str]:\n    \"\"\" If there is already a connection for this ssid, return the name of\n    the connection; if there is not, return None.\n    \"\"\"\n    nmcli_conns = await connections()\n    for wifi in [c['name']\n                 for c in nmcli_conns if c['type'] == 'wireless']:\n        res, _ = await _call(['-t', '-f', '802-11-wireless.ssid',\n                              '-m', 'tabular',\n                              'connection', 'show', wifi])\n        if res == ssid:\n            return wifi\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove all connections of con_type but the specified one.", "response": "async def _trim_old_connections(\n        new_name: str, con_type: CONNECTION_TYPES) -> Tuple[bool, str]:\n    \"\"\" Delete all connections of con_type but the one specified.\n    \"\"\"\n    existing_cons = await connections(for_type=con_type)\n    not_us = [c['name'] for c in existing_cons if c['name'] != new_name]\n    ok = True\n    res = []\n    for c in not_us:\n        this_ok, remove_res = await remove(name=c)\n        ok = ok and this_ok\n        if not this_ok:\n            # This is not a failure case for connection, and indeed the new\n            # connection is already established, so just warn about it\n            log.warning(\"Could not remove wifi connection {}: {}\"\n                        .format(c, remove_res))\n            res.append(remove_res)\n        else:\n            log.debug(\"Removed old wifi connection {}\".format(c))\n    return ok, ';'.join(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_eap_args(eap_args: Dict[str, str]) -> List[str]:\n    args = ['wifi-sec.key-mgmt', 'wpa-eap']\n    eap_type = EAP_TYPES.by_qualified_name(eap_args['eapType'])\n    type_args = eap_type.args()\n    args += ['802-1x.eap', eap_type.outer.value.name]\n    if eap_type.inner:\n        args += ['802-1x.phase2-autheap', eap_type.inner.value.name]\n    for ta in type_args:\n        if ta['name'] in eap_args:\n            if ta['type'] == 'file':\n                # Keyfiles must be prepended with file:// so nm-cli\n                # knows that we\u2019re not giving it DER-encoded blobs\n                _make_host_symlink_if_necessary()\n                path = _rewrite_key_path_to_host_path(eap_args[ta['name']])\n                val = 'file://' + path\n            else:\n                val = eap_args[ta['name']]\n            args += ['802-1x.' + ta['nmName'], val]\n    return args", "response": "Add configuration options suitable for WPA - EAP configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_con_add_cmd(ssid: str, security_type: SECURITY_TYPES,\n                       psk: Optional[str], hidden: bool,\n                       eap_args: Optional[Dict[str, Any]]) -> List[str]:\n    \"\"\" Build the nmcli connection add command to configure the new network.\n\n    The parameters are the same as configure but without the defaults; this\n    should be called only by configure.\n    \"\"\"\n    configure_cmd = ['connection', 'add',\n                     'save', 'yes',\n                     'autoconnect', 'yes',\n                     'ifname', 'wlan0',\n                     'type', 'wifi',\n                     'con-name', ssid,\n                     'wifi.ssid', ssid]\n    if hidden:\n        configure_cmd += ['wifi.hidden', 'true']\n    if security_type == SECURITY_TYPES.WPA_PSK:\n        configure_cmd += ['wifi-sec.key-mgmt', security_type.value]\n        if psk is None:\n            raise ValueError('wpa-psk security type requires psk')\n        configure_cmd += ['wifi-sec.psk', psk]\n    elif security_type == SECURITY_TYPES.WPA_EAP:\n        if eap_args is None:\n            raise ValueError('wpa-eap security type requires eap_args')\n        configure_cmd += _add_eap_args(eap_args)\n    elif security_type == SECURITY_TYPES.NONE:\n        pass\n    else:\n        raise ValueError(\"Bad security_type {}\".format(security_type))\n\n    return configure_cmd", "response": "Builds the nmcli connection add command to configure the new network."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures a connection with autoconnect.", "response": "async def configure(ssid: str,\n                    securityType: SECURITY_TYPES,\n                    psk: Optional[str] = None,\n                    hidden: bool = False,\n                    eapConfig: Optional[Dict[str, Any]] = None,\n                    upRetries: int = 2) -> Tuple[bool, str]:\n    \"\"\" Configure a connection but do not bring it up (though it is configured\n    for autoconnect).\n\n    Returns (success, message) where ``success`` is a ``bool`` and ``message``\n    is a ``str``.\n\n    Only anticipated failures are treated that way - for instance, an ssid\n    that doesn't exist will get a False and a message; a system where nmcli\n    is not found will raise a CalledProcessError.\n\n    Input checks should be conducted before calling this function; any issues\n    with arguments that make it to this function will probably surface\n    themselves as TypeErrors and ValueErrors rather than anything more\n    structured.\n\n    The ssid and security_type arguments are mandatory; the others have\n    different requirements depending on the security type.\n    \"\"\"\n    already = await connection_exists(ssid)\n    if already:\n        # TODO(seth, 8/29/2018): We may need to do connection modifies\n        # here for EAP configuration if e.g. we\u2019re passing a keyfile in a\n        # different http request\n        _1, _2 = await _call(['connection', 'delete', already])\n\n    configure_cmd = _build_con_add_cmd(\n        ssid, securityType, psk, hidden, eapConfig)\n    res, err = await _call(configure_cmd)\n\n    # nmcli connection add returns a string that looks like\n    # \"Connection \u2019connection-name\u2019 (connection-uuid) successfully added.\"\n    # This unfortunately doesn\u2019t respect the --terse flag, so we need to\n    # regex out the name or the uuid to use later in connection up; the\n    # uuid is slightly more regular, so that\u2019s what we use.\n    uuid_matches = re.search(  # noqa\n        \"Connection '(.*)'[\\s]+\\(([\\w\\d-]+)\\) successfully\", res)  # noqa\n    if not uuid_matches:\n        return False, err.split('\\r')[-1]\n    name = uuid_matches.group(1)\n    uuid = uuid_matches.group(2)\n    for _ in range(upRetries):\n        res, err = await _call(['connection', 'up', 'uuid', uuid])\n        if 'Connection successfully activated' in res:\n            # If we successfully added the connection, remove other wifi\n            # connections so they don\u2019t accumulate over time\n\n            _3, _4 = await _trim_old_connections(name,\n                                                 CONNECTION_TYPES.WIRELESS)\n            return True, res\n    else:\n        return False, err.split('\\r')[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def remove(ssid: str = None, name: str = None) -> Tuple[bool, str]:\n    if None is not ssid:\n        name = await connection_exists(ssid)\n    if None is not name:\n        res, err = await _call(['connection', 'delete', name])\n        if 'successfully deleted' in res:\n            return True, res\n        else:\n            return False, err\n    else:\n        return False, 'No connection for ssid {}'.format(ssid)", "response": "Remove a network from the NMCLI."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the basic network configuration of an interface.", "response": "async def iface_info(which_iface: NETWORK_IFACES) -> Dict[str, Optional[str]]:\n    \"\"\" Get the basic network configuration of an interface.\n\n    Returns a dict containing the info:\n    {\n      'ipAddress': 'xx.xx.xx.xx/yy' (ip4 addr with subnet as CIDR) or None\n      'macAddress': 'aa:bb:cc:dd:ee:ff' or None\n      'gatewayAddress: 'zz.zz.zz.zz' or None\n    }\n\n    which_iface should be a string in IFACE_NAMES.\n    \"\"\"\n    # example device info lines\n    #  GENERAL.HWADDR:B8:27:EB:24:D1:D0\n    #  IP4.ADDRESS[1]:10.10.2.221/22\n    # capture the field name (without the number in brackets) and the value\n    # using regex instead of split because there may be \":\" in the value\n    _DEV_INFO_LINE_RE = re.compile(r'([\\w.]+)(?:\\[\\d+])?:(.*)')\n    # example device info: 30 (disconnected)\n    # capture the string without the number\n    _IFACE_STATE_RE = re.compile(r'\\d+ \\((.+)\\)')\n\n    info: Dict[str, Optional[str]] = {'ipAddress': None,\n                                      'macAddress': None,\n                                      'gatewayAddress': None,\n                                      'state': None,\n                                      'type': None}\n    fields = ['GENERAL.HWADDR', 'IP4.ADDRESS',\n              'IP4.GATEWAY', 'GENERAL.TYPE', 'GENERAL.STATE']\n    # Note on this specific command: Most nmcli commands default to a tabular\n    # output mode, where if there are multiple things to pull a couple specific\n    # fields from it you\u2019ll get a table where rows are, say, connections, and\n    # columns are field name. However, specifically \u2018con show <con-name>\u2018 and\n    # \u2018dev show <dev-name>\u2019 default to a multiline representation, and even if\n    # explicitly ask for it to be tabular, it\u2019s not quite the same as the other\n    # commands. So we have to special-case the parsing.\n    res, err = await _call(['--mode', 'multiline',\n                            '--escape', 'no',\n                            '--terse', '--fields', ','.join(fields),\n                            'dev', 'show', which_iface.value])\n\n    field_map = {}\n    for line in res.split('\\n'):\n        # pull the key (without brackets) and the value out of the line\n        match = _DEV_INFO_LINE_RE.fullmatch(line)\n        if match is None:\n            raise ValueError(\n                \"Bad nmcli result; out: {}; err: {}\".format(res, err))\n        key, val = match.groups()\n        # nmcli can put \"--\" instead of \"\" for None\n        field_map[key] = None if val == '--' else val\n\n    info['macAddress'] = field_map.get('GENERAL.HWADDR')\n    info['ipAddress'] = field_map.get('IP4.ADDRESS')\n    info['gatewayAddress'] = field_map.get('IP4.GATEWAY')\n    info['type'] = field_map.get('GENERAL.TYPE')\n    state_val = field_map.get('GENERAL.STATE')\n\n    if state_val:\n        state_match = _IFACE_STATE_RE.fullmatch(state_val)\n        if state_match:\n            info['state'] = state_match.group(1)\n\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _call(cmd: List[str]) -> Tuple[str, str]:\n    to_exec = [quote(c) for c in ['nmcli'] + cmd]\n    cmd_str = ' '.join(to_exec)\n    # We have to use a shell invocation here because nmcli will not accept\n    # secrets specified on the command line unless it\u2019s in a shell. The other\n    # option is editing the connection configuration file in /etc/ afterwards\n    # (or using d-bus and pretending to be an auth agent)\n    proc = await as_subprocess.create_subprocess_shell(\n        cmd_str,\n        stdout=as_subprocess.PIPE, stderr=as_subprocess.PIPE)\n    out, err = await proc.communicate()\n    out_str, err_str = out.decode().strip(), err.decode().strip()\n    sanitized = sanitize_args(to_exec)\n    log.debug('{}: stdout={}'.format(' '.join(sanitized), out_str))\n    if err_str:\n        log.info('{}: stderr={}'.format(' '.join(sanitized), err_str))\n    return out_str, err_str", "response": "Runs the command in a subprocess and returns the captured stdout and stderr."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters the command so that it no longer contains passwords.", "response": "def sanitize_args(cmd: List[str]) -> List[str]:\n    \"\"\" Filter the command so that it no longer contains passwords\n    \"\"\"\n    sanitized = []\n    for idx, fieldname in enumerate(cmd):\n        def _is_password(cmdstr):\n            return 'wifi-sec.psk' in cmdstr\\\n                or 'password' in cmdstr.lower()\n        if idx > 0 and _is_password(cmd[idx-1]):\n            sanitized.append('****')\n        else:\n            sanitized.append(fieldname)\n    return sanitized"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing NMCLI terse tabular output into a list of Python dicts.", "response": "def _dict_from_terse_tabular(\n        names: List[str],\n        inp: str,\n        transformers: Dict[str, Callable[[str], Any]] = {})\\\n        -> List[Dict[str, Any]]:\n    \"\"\" Parse NMCLI terse tabular output into a list of Python dict.\n\n    ``names`` is a list of strings of field names to apply to the input data,\n    which is assumed to be colon separated.\n\n    ``inp`` is the input as a string (i.e. already decode()d) from nmcli\n\n    ``transformers`` is a dict mapping field names to callables of the form\n    f: str -> any. If a fieldname is in transformers, that callable will be\n    invoked on the field matching the name and the result stored.\n\n    The return value is a list with one element per valid line of input, where\n    each element is a dict with keys taken from names and values from the input\n    \"\"\"\n    res = []\n    for n in names:\n        if n not in transformers:\n            transformers[n] = lambda s: s\n    for line in inp.split('\\n'):\n        if len(line) < 3:\n            continue\n        fields = line.split(':')\n        res.append(dict([\n            (elem[0], transformers[elem[0]](elem[1]))\n            for elem in zip(names, fields)]))\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Pipette(self, *args, **kwargs):\n        return inst.Pipette(self.robot, *args, **kwargs)", "response": "Returns a Pipette instance for this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield the list of items that can be used to plan a transfer.", "response": "def _plan_transfer(self):\n        \"\"\"\n        * **Source/ Dest:** Multiple sources to multiple destinations.\n                            Src & dest should be equal length\n\n        * **Volume:** Single volume or List of volumes is acceptable. This list\n                      should be same length as sources/destinations\n\n        * **Behavior with transfer options:**\n\n            - New_tip: can be either NEVER or ONCE or ALWAYS\n            - Air_gap: if specified, will be performed after every aspirate\n            - Blow_out: can be performed after each dispense (after mix, before\n                        touch_tip) at the location specified. If there is\n                        liquid present in the tip (as in the case of nonzero\n                        disposal volume), blow_out will be performed at either\n                        user-defined location or (default) trash.\n                        If no liquid is supposed to be present in the tip after\n                        dispense, blow_out will be performed at dispense well\n                        location (if blow out strategy is DEST_IF_EMPTY)\n            - Touch_tip: can be performed after each aspirate and/or after\n                         each dispense\n            - Mix: can be performed before aspirate and/or after dispense\n                   if there is no disposal volume (i.e. can be performed\n                   only when the tip is supposed to be empty)\n\n            Considering all options, the sequence of actions is:\n            *New Tip -> Mix -> Aspirate (with disposal volume) -> Air gap ->\n            -> Touch tip -> Dispense air gap -> Dispense -> Mix if empty ->\n            -> Blow out -> Touch tip -> Drop tip*\n        \"\"\"\n        plan_iter = zip(self._volumes, self._sources, self._dests)\n        for step_vol, src, dest in plan_iter:\n            if self._strategy.new_tip == types.TransferTipPolicy.ALWAYS:\n                yield self._format_dict('pick_up_tip', kwargs=self._tip_opts)\n            max_vol = self._instr.max_volume - \\\n                self._strategy.disposal_volume - self._strategy.air_gap\n            xferred_vol = 0\n            while xferred_vol < step_vol:\n                # TODO: account for unequal length sources, dests\n                # TODO: ensure last transfer is > min_vol\n                vol = min(max_vol, step_vol - xferred_vol)\n                yield from self._aspirate_actions(vol, src)\n                yield from self._dispense_actions(vol, dest)\n                xferred_vol += vol\n            yield from self._new_tip_action()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset(self):\n        self.tip_attached = False\n        self.placeables = []\n        self.previous_placeable = None\n        self.current_volume = 0\n        self.reset_tip_tracking()", "response": "Resets the state of this pipette to the initial state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_tip_rack(self):\n        return (self.tip_racks is not None\n                and isinstance(self.tip_racks, list)\n                and len(self.tip_racks) > 0)", "response": "Returns True if this Pipette was instantiated with tip_racks\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset_tip_tracking(self):\n        self.current_tip(None)\n        self.tip_rack_iter = iter([])\n\n        if self.has_tip_rack():\n            iterables = self.tip_racks\n\n            if self.channels > 1:\n                iterables = [c for rack in self.tip_racks for c in rack.cols]\n            else:\n                iterables = [w for rack in self.tip_racks for w in rack]\n\n            if self.starting_tip:\n                iterables = iterables[iterables.index(self.starting_tip):]\n\n            self.tip_rack_iter = itertools.chain(iterables)", "response": "Resets the pipette tip tracking and refilling the tip racks"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a placeable to the list of placeables.", "response": "def _associate_placeable(self, location):\n        \"\"\"\n        Saves a reference to a placeable\n        \"\"\"\n        if not location:\n            return\n\n        placeable, _ = unpack_location(location)\n        self.previous_placeable = placeable\n        if not self.placeables or (placeable != self.placeables[-1]):\n            self.placeables.append(placeable)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove this. pipette to a specific location.", "response": "def move_to(self, location, strategy=None):\n        \"\"\"\n        Move this :any:`Pipette` to a :any:`Placeable` on the :any:`Deck`\n\n        Notes\n        -----\n        Until obstacle-avoidance algorithms are in place,\n        :any:`Robot` and :any:`Pipette` :meth:`move_to` use either an\n        \"arc\" or \"direct\"\n\n        Parameters\n        ----------\n        location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`)\n            The destination to arrive at\n\n        strategy : \"arc\" or \"direct\"\n            \"arc\" strategies (default) will pick the head up on Z axis, then\n            over to the XY destination, then finally down to the Z destination.\n            \"direct\" strategies will simply move in a straight line from\n            the current position\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n        \"\"\"\n        if not location:\n            return self\n\n        placeable, _ = unpack_location(location)\n\n        if strategy is None:\n            # if no strategy is specified, default to type 'arc'\n            strategy = 'arc'\n            # unless we are still within the same Well, then move directly\n            if placeable == self.previous_placeable:\n                strategy = 'direct'\n\n        self._associate_placeable(placeable)\n\n        self.robot.move_to(\n            location,\n            instrument=self,\n            strategy=strategy)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aspirate(self, volume=None, location=None, rate=1.0):\n        if not self.tip_attached:\n            log.warning(\"Cannot aspirate without a tip attached.\")\n\n        # Note: volume positional argument may not be passed. if it isn't then\n        # assume the first positional argument is the location\n        if not helpers.is_number(volume):\n            if volume and not location:\n                location = volume\n            volume = self.max_volume - self.current_volume\n\n        display_location = location if location else self.previous_placeable\n\n        do_publish(self.broker, commands.aspirate, self.aspirate, 'before',\n                   None, None, self, volume, display_location, rate)\n\n        # if volume is specified as 0uL, then do nothing\n        if volume != 0:\n            if self.current_volume + volume > self.max_volume:\n                raise RuntimeWarning(\n                    'Pipette with max volume of {0} cannot hold volume {1}'\n                    .format(\n                        self.max_volume,\n                        self.current_volume + volume)\n                )\n\n            self._position_for_aspirate(location)\n\n            mm_position = self._aspirate_plunger_position(\n                self.current_volume + volume)\n            speed = self.speeds['aspirate'] * rate\n            self.instrument_actuator.push_speed()\n            self.instrument_actuator.set_speed(speed)\n            self.instrument_actuator.set_active_current(self._plunger_current)\n            self.robot.poses = self.instrument_actuator.move(\n                self.robot.poses,\n                x=mm_position\n            )\n            self.instrument_actuator.pop_speed()\n            self.current_volume += volume  # update after actual aspirate\n\n        do_publish(self.broker, commands.aspirate, self.aspirate, 'after',\n                   self, None, self, volume, display_location, rate)\n\n        return self", "response": "Aspirate a volume of liquid using this pipette at a specific location."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dispense(self,\n                 volume=None,\n                 location=None,\n                 rate=1.0):\n        \"\"\"\n        Dispense a volume of liquid (in microliters/uL) using this pipette\n\n        Notes\n        -----\n        If only a volume is passed, the pipette will dispense\n        from it's current position. If only a location is passed,\n        `dispense` will default to it's `current_volume`\n\n        The location may be a Well, or a specific position in relation to a\n        Well, such as `Well.top()`. If a Well is specified without calling a\n        a position method (such as .top or .bottom), this method will default\n        to the bottom of the well.\n\n        Parameters\n        ----------\n        volume : int or float\n            The number of microliters to dispense\n            (Default: self.current_volume)\n        location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`)\n            The :any:`Placeable` (:any:`Well`) to perform the dispense.\n            Can also be a tuple with first item :any:`Placeable`,\n            second item relative :any:`Vector`\n        rate : float\n            Set plunger speed for this dispense, where\n            speed = rate * dispense_speed (see :meth:`set_speed`)\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> plate = labware.load('96-flat', '3') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left') # doctest: +SKIP\n        # fill the pipette with liquid (200uL)\n        >>> p300.aspirate(plate[0]) # doctest: +SKIP\n        # dispense 50uL to a Well\n        >>> p300.dispense(50, plate[0]) # doctest: +SKIP\n        # dispense 50uL to the center of a well\n        >>> relative_vector = plate[1].center() # doctest: +SKIP\n        >>> p300.dispense(50, (plate[1], relative_vector)) # doctest: +SKIP\n        # dispense 20uL in place, at half the speed\n        >>> p300.dispense(20, rate=0.5) # doctest: +SKIP\n        # dispense the pipette's remaining volume (80uL) to a Well\n        >>> p300.dispense(plate[2]) # doctest: +SKIP\n        \"\"\"\n        if not self.tip_attached:\n            log.warning(\"Cannot dispense without a tip attached.\")\n\n        # Note: volume positional argument may not be passed. if it isn't then\n        # assume the first positional argument is the location\n        if not helpers.is_number(volume):\n            if volume and not location:\n                location = volume\n            volume = self.current_volume\n\n        # Ensure we don't dispense more than the current volume\n        volume = min(self.current_volume, volume)\n\n        display_location = location if location else self.previous_placeable\n\n        do_publish(self.broker, commands.dispense, self.dispense, 'before',\n                   None, None, self, volume, display_location, rate)\n\n        # if volume is specified as 0uL, then do nothing\n        if volume != 0:\n            self._position_for_dispense(location)\n\n            mm_position = self._dispense_plunger_position(\n                self.current_volume - volume)\n            speed = self.speeds['dispense'] * rate\n\n            self.instrument_actuator.push_speed()\n            self.instrument_actuator.set_speed(speed)\n            self.instrument_actuator.set_active_current(self._plunger_current)\n            self.robot.poses = self.instrument_actuator.move(\n                self.robot.poses,\n                x=mm_position\n            )\n            self.instrument_actuator.pop_speed()\n            self.current_volume -= volume  # update after actual dispense\n\n        do_publish(self.broker, commands.dispense, self.dispense, 'after',\n                   self, None, self, volume, display_location, rate)\n\n        return self", "response": "Dispense a volume of liquid using this pipette."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npositions this. any. Pipette for an aspiration and the given location.", "response": "def _position_for_aspirate(self, location=None, clearance=1.0):\n        \"\"\"\n        Position this :any:`Pipette` for an aspiration,\n        given it's current state\n        \"\"\"\n\n        placeable = None\n        if location:\n            placeable, _ = unpack_location(location)\n            # go to top of source, if not already there\n            if placeable != self.previous_placeable:\n                self.move_to(placeable.top())\n        else:\n            placeable = self.previous_placeable\n\n        # if pipette is currently empty, ensure the plunger is at \"bottom\"\n        if self.current_volume == 0:\n            pos, _, _ = pose_tracker.absolute(\n                self.robot.poses, self.instrument_actuator)\n            if pos != self._get_plunger_position('bottom'):\n                # move to top of well to avoid touching liquid\n                if placeable:\n                    self.move_to(placeable.top())\n                self.instrument_actuator.set_active_current(\n                    self._plunger_current)\n                self.robot.poses = self.instrument_actuator.move(\n                    self.robot.poses,\n                    x=self._get_plunger_position('bottom')\n                )\n\n        # then go inside the location\n        if location:\n            if isinstance(location, Placeable):\n                location = location.bottom(min(location.z_size(), clearance))\n            self.move_to(location, strategy='direct')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npositions this. Pipette for an dispense.", "response": "def _position_for_dispense(self, location=None, clearance=0.5):\n        \"\"\"\n        Position this :any:`Pipette` for an dispense\n        \"\"\"\n\n        if location:\n            if isinstance(location, Placeable):\n                location = location.bottom(min(location.z_size(), clearance))\n            self.move_to(location)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef retract(self, safety_margin=10):\n        '''\n        Move the pipette's mount upwards and away from the deck\n\n        Parameters\n        ----------\n        safety_margin: int\n            Distance in millimeters awey from the limit switch,\n            used during the mount's `fast_home()` method\n        '''\n        self.previous_placeable = None  # it is no longer inside a placeable\n        self.robot.poses = self.instrument_mover.fast_home(\n            self.robot.poses, safety_margin)\n        return self", "response": "Move the pipette s mount upwards and away from the deck s placeable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmixes a volume of liquid (in microliters/uL) using this pipette Notes ----- If no `location` is passed, the pipette will mix from it's current position. If no `volume` is passed, `mix` will default to it's `max_volume` Parameters ---------- repetitions: int How many times the pipette should mix (Default: 1) volume : int or float The number of microliters to mix (Default: self.max_volume) location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`) The :any:`Placeable` (:any:`Well`) to perform the mix. Can also be a tuple with first item :any:`Placeable`, second item relative :any:`Vector` rate : float Set plunger speed for this mix, where speed = rate * (aspirate_speed or dispense_speed) (see :meth:`set_speed`) Returns ------- This instance of :class:`Pipette`. Examples -------- .. >>> from opentrons import instruments, labware, robot # doctest: +SKIP >>> robot.reset() # doctest: +SKIP >>> plate = labware.load('96-flat', '4') # doctest: +SKIP >>> p300 = instruments.P300_Single(mount='left') # doctest: +SKIP # mix 50uL in a Well, three times >>> p300.mix(3, 50, plate[0]) # doctest: +SKIP # mix 3x with the pipette's max volume, from current position >>> p300.mix(3) # doctest: +SKIP", "response": "def mix(self,\n            repetitions=1,\n            volume=None,\n            location=None,\n            rate=1.0):\n        \"\"\"\n        Mix a volume of liquid (in microliters/uL) using this pipette\n\n        Notes\n        -----\n        If no `location` is passed, the pipette will mix\n        from it's current position. If no `volume` is passed,\n        `mix` will default to it's `max_volume`\n\n        Parameters\n        ----------\n        repetitions: int\n            How many times the pipette should mix (Default: 1)\n\n        volume : int or float\n            The number of microliters to mix (Default: self.max_volume)\n\n        location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`)\n            The :any:`Placeable` (:any:`Well`) to perform the mix.\n            Can also be a tuple with first item :any:`Placeable`,\n            second item relative :any:`Vector`\n\n        rate : float\n            Set plunger speed for this mix, where\n            speed = rate * (aspirate_speed or dispense_speed)\n            (see :meth:`set_speed`)\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> plate = labware.load('96-flat', '4') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left') # doctest: +SKIP\n        # mix 50uL in a Well, three times\n        >>> p300.mix(3, 50, plate[0]) # doctest: +SKIP\n        # mix 3x with the pipette's max volume, from current position\n        >>> p300.mix(3) # doctest: +SKIP\n        \"\"\"\n        if not self.tip_attached:\n            log.warning(\"Cannot mix without a tip attached.\")\n\n        if volume is None:\n            volume = self.max_volume\n\n        if not location and self.previous_placeable:\n            location = self.previous_placeable\n\n        do_publish(self.broker, commands.mix, self.mix, 'before',\n                   self, None, None, repetitions, volume, location, rate)\n\n        self.aspirate(location=location, volume=volume, rate=rate)\n        for i in range(repetitions - 1):\n            self.dispense(volume, rate=rate)\n            self.aspirate(volume, rate=rate)\n        self.dispense(volume, rate=rate)\n\n        do_publish(self.broker, commands.mix, self.mix, 'after',\n                   self, None, self, repetitions, volume, location, rate)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nforces any remaining liquid dispense by moving this pipette s plunger to the calibrated blow_out position.", "response": "def blow_out(self, location=None):\n        \"\"\"\n        Force any remaining liquid to dispense, by moving\n        this pipette's plunger to the calibrated `blow_out` position\n\n        Notes\n        -----\n        If no `location` is passed, the pipette will blow_out\n        from it's current position.\n\n        Parameters\n        ----------\n        location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`)\n            The :any:`Placeable` (:any:`Well`) to perform the blow_out.\n            Can also be a tuple with first item :any:`Placeable`,\n            second item relative :any:`Vector`\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left') # doctest: +SKIP\n        >>> p300.aspirate(50).dispense().blow_out() # doctest: +SKIP\n        \"\"\"\n        if not self.tip_attached:\n            log.warning(\"Cannot 'blow out' without a tip attached.\")\n\n        self.move_to(location)\n        self.instrument_actuator.set_active_current(self._plunger_current)\n        self.robot.poses = self.instrument_actuator.move(\n            self.robot.poses,\n            x=self._get_plunger_position('blow_out')\n        )\n        self.current_volume = 0\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntouching the pipette tip to the sides of a well.", "response": "def touch_tip(self, location=None, radius=1.0, v_offset=-1.0, speed=60.0):\n        \"\"\"\n        Touch the :any:`Pipette` tip to the sides of a well,\n        with the intent of removing left-over droplets\n\n        Notes\n        -----\n        If no `location` is passed, the pipette will touch_tip\n        from it's current position.\n\n        Parameters\n        ----------\n        location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`)\n            The :any:`Placeable` (:any:`Well`) to perform the touch_tip.\n            Can also be a tuple with first item :any:`Placeable`,\n            second item relative :any:`Vector`\n\n        radius : float\n            Radius is a floating point describing the percentage of a well's\n            radius. When radius=1.0, :any:`touch_tip()` will move to 100% of\n            the wells radius. When radius=0.5, :any:`touch_tip()` will move to\n            50% of the wells radius.\n            Default: 1.0 (100%)\n\n        speed: float\n            The speed for touch tip motion, in mm/s.\n            Default: 60.0 mm/s, Max: 80.0 mm/s, Min: 20.0 mm/s\n\n        v_offset: float\n            The offset in mm from the top of the well to touch tip.\n            Default: -1.0 mm\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> plate = labware.load('96-flat', '8') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left') # doctest: +SKIP\n        >>> p300.aspirate(50, plate[0]) # doctest: +SKIP\n        >>> p300.dispense(plate[1]).touch_tip() # doctest: +SKIP\n        \"\"\"\n        if not self.tip_attached:\n            log.warning(\"Cannot touch tip without a tip attached.\")\n        if speed > 80.0:\n            log.warning(\"Touch tip speeds greater than 80mm/s not allowed\")\n            speed = 80.0\n        if speed < 20.0:\n            log.warning(\"Touch tip speeds greater than 80mm/s not allowed\")\n            speed = 20.0\n\n        if helpers.is_number(location):\n            # Deprecated syntax\n            log.warning(\"Please use the `v_offset` named parameter\")\n            v_offset = location\n            location = None\n\n        # if no location specified, use the previously\n        # associated placeable to get Well dimensions\n        if location is None:\n            location = self.previous_placeable\n\n        do_publish(self.broker, commands.touch_tip, self.touch_tip, 'before',\n                   None, None, self, location, radius, v_offset, speed)\n\n        # move to location if we're not already there\n        if location != self.previous_placeable:\n            self.move_to(location)\n\n        v_offset = (0, 0, v_offset)\n\n        well_edges = [\n            location.from_center(x=radius, y=0, z=1),       # right edge\n            location.from_center(x=radius * -1, y=0, z=1),  # left edge\n            location.from_center(x=0, y=radius, z=1),       # back edge\n            location.from_center(x=0, y=radius * -1, z=1)   # front edge\n        ]\n\n        # Apply vertical offset to well edges\n        well_edges = map(lambda x: x + v_offset, well_edges)\n\n        self.robot.gantry.push_speed()\n        self.robot.gantry.set_speed(speed)\n        [self.move_to((location, e), strategy='direct') for e in well_edges]\n        self.robot.gantry.pop_speed()\n\n        do_publish(self.broker, commands.touch_tip, self.touch_tip, 'after',\n                   self, None, self, location, radius, v_offset, speed)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef air_gap(self, volume=None, height=None):\n        if not self.tip_attached:\n            log.warning(\"Cannot perform air_gap without a tip attached.\")\n\n        if height is None:\n            height = 5\n\n        do_publish(self.broker, commands.air_gap, self.air_gap, 'before',\n                   self, None, self, volume, height)\n\n        # if volumes is specified as 0uL, do nothing\n        if volume != 0:\n            location = self.previous_placeable.top(height)\n            # \"move_to\" separate from aspirate command\n            # so \"_position_for_aspirate\" isn't executed\n            self.move_to(location)\n            self.aspirate(volume)\n\n        do_publish(self.broker, commands.air_gap, self.air_gap, 'after',\n                   self, None, self, volume, height)\n\n        return self", "response": "Pulls air into the current tip and moves it to the next placeable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the current tip to the pipette s originating tip rack.", "response": "def return_tip(self, home_after=True):\n        \"\"\"\n        Drop the pipette's current tip to it's originating tip rack\n\n        Notes\n        -----\n        This method requires one or more tip-rack :any:`Container`\n        to be in this Pipette's `tip_racks` list (see :any:`Pipette`)\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> tiprack = labware.load('GEB-tiprack-300', '2') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left',\n        ...     tip_racks=[tiprack, tiprack2]) # doctest: +SKIP\n        >>> p300.pick_up_tip() # doctest: +SKIP\n        >>> p300.aspirate(50, plate[0]) # doctest: +SKIP\n        >>> p300.dispense(plate[1]) # doctest: +SKIP\n        >>> p300.return_tip() # doctest: +SKIP\n        \"\"\"\n        if not self.tip_attached:\n            log.warning(\"Cannot return tip without tip attached.\")\n\n        if not self.current_tip():\n            self.robot.add_warning(\n                'Pipette has no tip to return, dropping in place')\n\n        self.drop_tip(self.current_tip(), home_after=home_after)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npicking up a tip for the Pipette.", "response": "def pick_up_tip(self, location=None, presses=None, increment=None):\n        \"\"\"\n        Pick up a tip for the Pipette to run liquid-handling commands with\n\n        Notes\n        -----\n        A tip can be manually set by passing a `location`. If no location\n        is passed, the Pipette will pick up the next available tip in\n        it's `tip_racks` list (see :any:`Pipette`)\n\n        Parameters\n        ----------\n        location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`)\n            The :any:`Placeable` (:any:`Well`) to perform the pick_up_tip.\n            Can also be a tuple with first item :any:`Placeable`,\n            second item relative :any:`Vector`\n        presses : :any:int\n            The number of times to lower and then raise the pipette when\n            picking up a tip, to ensure a good seal (0 [zero] will result in\n            the pipette hovering over the tip but not picking it up--generally\n            not desireable, but could be used for dry-run). Default: 3 presses\n        increment: :int\n            The additional distance to travel on each successive press (e.g.:\n            if presses=3 and increment=1, then the first press will travel down\n            into the tip by 3.5mm, the second by 4.5mm, and the third by 5.5mm.\n            Default: 1mm\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> tiprack = labware.load('GEB-tiprack-300', '2') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left',\n        ...     tip_racks=[tiprack]) # doctest: +SKIP\n        >>> p300.pick_up_tip(tiprack[0]) # doctest: +SKIP\n        >>> p300.return_tip() # doctest: +SKIP\n        # `pick_up_tip` will automatically go to tiprack[1]\n        >>> p300.pick_up_tip() # doctest: +SKIP\n        >>> p300.return_tip() # doctest: +SKIP\n        \"\"\"\n        if self.tip_attached:\n            log.warning(\"There is already a tip attached to this pipette.\")\n\n        if not location:\n            location = self.get_next_tip()\n        self.current_tip(None)\n        if location:\n            placeable, _ = unpack_location(location)\n            self.current_tip(placeable)\n\n        presses = (self._pick_up_presses\n                   if not helpers.is_number(presses)\n                   else presses)\n        increment = (self._pick_up_increment\n                     if not helpers.is_number(increment)\n                     else increment)\n\n        def _pick_up_tip(\n                self, location, presses, increment):\n            self.instrument_actuator.set_active_current(self._plunger_current)\n            self.robot.poses = self.instrument_actuator.move(\n                self.robot.poses,\n                x=self._get_plunger_position('bottom')\n            )\n            self.current_volume = 0\n            self.move_to(self.current_tip().top(0))\n\n            for i in range(int(presses)):\n                # move nozzle down into the tip\n                self.instrument_mover.push_speed()\n\n                self.instrument_mover.push_active_current()\n                self.instrument_mover.set_active_current(self._pick_up_current)\n                self.instrument_mover.set_speed(self._pick_up_speed)\n                dist = (-1 * self._pick_up_distance) + (-1 * increment * i)\n                self.move_to(\n                    self.current_tip().top(dist),\n                    strategy='direct')\n                # move nozzle back up\n                self.instrument_mover.pop_active_current()\n                self.instrument_mover.pop_speed()\n                self.move_to(\n                    self.current_tip().top(0),\n                    strategy='direct')\n            self._add_tip(\n                length=self._tip_length\n            )\n            # neighboring tips tend to get stuck in the space between\n            # the volume chamber and the drop-tip sleeve on p1000.\n            # This extra shake ensures those tips are removed\n            if 'needs-pickup-shake' in self.quirks:\n                self._shake_off_tips(location)\n                self._shake_off_tips(location)\n            self.previous_placeable = None  # no longer inside a placeable\n            self.robot.poses = self.instrument_mover.fast_home(\n                self.robot.poses, self._pick_up_distance)\n\n            return self\n        do_publish(self.broker, commands.pick_up_tip, self.pick_up_tip,\n                   'before', None, None, self, location, presses, increment)\n        _pick_up_tip(\n            self, location=location, presses=presses, increment=increment)\n        do_publish(self.broker, commands.pick_up_tip, self.pick_up_tip,\n                   'after', self, None, self, location, presses, increment)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndropping the tip from the pipette.", "response": "def drop_tip(self, location=None, home_after=True):\n        \"\"\"\n        Drop the pipette's current tip\n\n        Notes\n        -----\n        If no location is passed, the pipette defaults to its `trash_container`\n        (see :any:`Pipette`)\n\n        Parameters\n        ----------\n        location : :any:`Placeable` or tuple(:any:`Placeable`, :any:`Vector`)\n            The :any:`Placeable` (:any:`Well`) to perform the drop_tip.\n            Can also be a tuple with first item :any:`Placeable`,\n            second item relative :any:`Vector`\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> tiprack = labware.load('tiprack-200ul', 'C2') # doctest: +SKIP\n        >>> trash = labware.load('point', 'A3') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left') # doctest: +SKIP\n        >>> p300.pick_up_tip(tiprack[0]) # doctest: +SKIP\n        # drops the tip in the fixed trash\n        >>> p300.drop_tip() # doctest: +SKIP\n        >>> p300.pick_up_tip(tiprack[1]) # doctest: +SKIP\n        # drops the tip back at its tip rack\n        >>> p300.drop_tip(tiprack[1]) # doctest: +SKIP\n        \"\"\"\n        if not self.tip_attached:\n            log.warning(\"Cannot drop tip without a tip attached.\")\n\n        if not location and self.trash_container:\n            location = self.trash_container\n\n        if isinstance(location, Placeable):\n            # give space for the drop-tip mechanism\n            # @TODO (Laura & Andy 2018261)\n            # When container typing is implemented, make sure that\n            # when returning to a tiprack, tips are dropped within the rack\n            if 'rack' in location.get_parent().get_type():\n                half_tip_length = self._tip_length / 2\n                location = location.top(-half_tip_length)\n            elif 'trash' in location.get_parent().get_type():\n                loc, coords = location.top()\n                location = (loc, coords + (0, self.model_offset[1], 0))\n            else:\n                location = location.top()\n\n        def _drop_tip(location, instrument=self):\n            if location:\n                self.move_to(location)\n\n            pos_bottom = self._get_plunger_position('bottom')\n            pos_drop_tip = self._get_plunger_position('drop_tip')\n\n            self.instrument_actuator.set_active_current(self._plunger_current)\n            self.robot.poses = self.instrument_actuator.move(\n                self.robot.poses,\n                x=pos_bottom\n            )\n            self.instrument_actuator.set_active_current(self._drop_tip_current)\n            self.instrument_actuator.push_speed()\n            self.instrument_actuator.set_speed(self._drop_tip_speed)\n            self.robot.poses = self.instrument_actuator.move(\n                self.robot.poses,\n                x=pos_drop_tip\n            )\n            self.instrument_actuator.pop_speed()\n            self._shake_off_tips(location)\n            if home_after:\n                self._home_after_drop_tip()\n\n            self.current_volume = 0\n            self.current_tip(None)\n            self._remove_tip(\n                length=self._tip_length\n            )\n\n        do_publish(self.broker, commands.drop_tip, self.drop_tip,\n                   'before', None, None, self, location)\n        _drop_tip(location)\n        do_publish(self.broker, commands.drop_tip, self.drop_tip,\n                   'after', self, None, self, location)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhomes the pipette s plunger axis during a protocol run", "response": "def home(self):\n        \"\"\"\n        Home the pipette's plunger axis during a protocol run\n\n        Notes\n        -----\n        `Pipette.home()` homes the `Robot`\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='right') # doctest: +SKIP\n        >>> p300.home() # doctest: +SKIP\n        \"\"\"\n        def _home(mount):\n            self.current_volume = 0\n            self.instrument_actuator.set_active_current(self._plunger_current)\n            self.robot.poses = self.instrument_actuator.home(\n                self.robot.poses)\n            self.robot.poses = self.instrument_mover.home(self.robot.poses)\n            self.previous_placeable = None  # no longer inside a placeable\n        do_publish(self.broker, commands.home, _home,\n                   'before', None, None, self.mount)\n        _home(self.mount)\n        do_publish(self.broker, commands.home, _home,\n                   'after', self, None, self.mount)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef distribute(self, volume, source, dest, *args, **kwargs):\n        # Note: currently it varies whether the pipette should have a tip on\n        # or not depending on the parameters for this call, so we cannot\n        # create a very reliable assertion on tip status\n\n        args = [volume, source, dest, *args]\n        kwargs['mode'] = 'distribute'\n        kwargs['mix_after'] = (0, 0)\n        if 'disposal_vol' not in kwargs:\n            kwargs['disposal_vol'] = self.min_volume\n        return self.transfer(*args, **kwargs)", "response": "Distribute a single liquid from one source location to another."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconsolidate a volume from one liquid to another.", "response": "def consolidate(self, volume, source, dest, *args, **kwargs):\n        \"\"\"\n        Consolidate will move a volume of liquid from a list of sources\n        to a single target location. See :any:`Transfer` for details\n        and a full list of optional arguments.\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ..\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> plate = labware.load('96-flat', 'A3') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='left') # doctest: +SKIP\n        >>> p300.consolidate(50, plate.cols[0], plate[1]) # doctest: +SKIP\n        \"\"\"\n\n        kwargs['mode'] = 'consolidate'\n        kwargs['mix_before'] = (0, 0)\n        kwargs['air_gap'] = 0\n        kwargs['disposal_vol'] = 0\n        args = [volume, source, dest, *args]\n        return self.transfer(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransfer will move a volume of liquid from a source location(s) to a dest location(s). It is a higher-level command, incorporating other :any:`Pipette` commands, like :any:`aspirate` and :any:`dispense`, designed to make protocol writing easier at the cost of specificity. Parameters ---------- volumes : number, list, or tuple The amount of volume to remove from each `sources` :any:`Placeable` and add to each `targets` :any:`Placeable`. If `volumes` is a list, each volume will be used for the sources/targets at the matching index. If `volumes` is a tuple with two elements, like `(20, 100)`, then a list of volumes will be generated with a linear gradient between the two volumes in the tuple. source : Placeable or list Single :any:`Placeable` or list of :any:`Placeable`s, from where liquid will be :any:`aspirate`ed from. dest : Placeable or list Single :any:`Placeable` or list of :any:`Placeable`s, where liquid will be :any:`dispense`ed to. new_tip : str The number of clean tips this transfer command will use. If 'never', no tips will be picked up nor dropped. If 'once', a single tip will be used for all commands. If 'always', a new tip will be used for each transfer. Default is 'once'. trash : boolean If `False` (default behavior) tips will be returned to their tip rack. If `True` and a trash container has been attached to this `Pipette`, then the tip will be sent to the trash container. touch_tip : boolean If `True`, a :any:`touch_tip` will occur following each :any:`aspirate` and :any:`dispense`. If set to `False` (default), no :any:`touch_tip` will occur. blow_out : boolean If `True`, a :any:`blow_out` will occur following each :any:`dispense`, but only if the pipette has no liquid left in it. If set to `False` (default), no :any:`blow_out` will occur. mix_before : tuple Specify the number of repetitions volume to mix, and a :any:`mix` will proceed each :any:`aspirate` during the transfer and dispense. The tuple's values is interpreted as (repetitions, volume). mix_after : tuple Specify the number of repetitions volume to mix, and a :any:`mix` will following each :any:`dispense` during the transfer or consolidate. The tuple's values is interpreted as (repetitions, volume). carryover : boolean If `True` (default), any `volumes` that exceed the maximum volume of this `Pipette` will be split into multiple smaller volumes. repeat : boolean (Only applicable to :any:`distribute` and :any:`consolidate`)If `True` (default), sequential :any:`aspirate` volumes will be combined into one tip for the purpose of saving time. If `False`, all volumes will be transferred seperately. gradient : lambda Function for calculated the curve used for gradient volumes. When `volumes` is a tuple of length 2, it's values are used to create a list of gradient volumes. The default curve for this gradient is linear (lambda x: x), however a method can be passed with the `gradient` keyword argument to create a custom curve. Returns ------- This instance of :class:`Pipette`. Examples -------- ... >>> from opentrons import instruments, labware, robot # doctest: +SKIP >>> robot.reset() # doctest: +SKIP >>> plate = labware.load('96-flat', '5') # doctest: +SKIP >>> p300 = instruments.P300_Single(mount='right') # doctest: +SKIP >>> p300.transfer(50, plate[0], plate[1]) # doctest: +SKIP", "response": "def transfer(self, volume, source, dest, **kwargs):\n        \"\"\"\n        Transfer will move a volume of liquid from a source location(s)\n        to a dest location(s). It is a higher-level command, incorporating\n        other :any:`Pipette` commands, like :any:`aspirate` and\n        :any:`dispense`, designed to make protocol writing easier at the\n        cost of specificity.\n\n        Parameters\n        ----------\n        volumes : number, list, or tuple\n            The amount of volume to remove from each `sources` :any:`Placeable`\n            and add to each `targets` :any:`Placeable`. If `volumes` is a list,\n            each volume will be used for the sources/targets at the\n            matching index. If `volumes` is a tuple with two elements,\n            like `(20, 100)`, then a list of volumes will be generated with\n            a linear gradient between the two volumes in the tuple.\n\n        source : Placeable or list\n            Single :any:`Placeable` or list of :any:`Placeable`s, from where\n            liquid will be :any:`aspirate`ed from.\n\n        dest : Placeable or list\n            Single :any:`Placeable` or list of :any:`Placeable`s, where\n            liquid will be :any:`dispense`ed to.\n\n        new_tip : str\n            The number of clean tips this transfer command will use. If\n            'never', no tips will be picked up nor dropped. If 'once', a\n            single tip will be used for all commands. If 'always', a new tip\n            will be used for each transfer. Default is 'once'.\n\n        trash : boolean\n            If `False` (default behavior) tips will be returned to their\n            tip rack. If `True` and a trash container has been attached\n            to this `Pipette`, then the tip will be sent to the trash\n            container.\n\n        touch_tip : boolean\n            If `True`, a :any:`touch_tip` will occur following each\n            :any:`aspirate` and :any:`dispense`. If set to `False` (default),\n            no :any:`touch_tip` will occur.\n\n        blow_out : boolean\n            If `True`, a :any:`blow_out` will occur following each\n            :any:`dispense`, but only if the pipette has no liquid left in it.\n            If set to `False` (default), no :any:`blow_out` will occur.\n\n        mix_before : tuple\n            Specify the number of repetitions volume to mix, and a :any:`mix`\n            will proceed each :any:`aspirate` during the transfer and dispense.\n            The tuple's values is interpreted as (repetitions, volume).\n\n        mix_after : tuple\n            Specify the number of repetitions volume to mix, and a :any:`mix`\n            will following each :any:`dispense` during the transfer or\n            consolidate. The tuple's values is interpreted as\n            (repetitions, volume).\n\n        carryover : boolean\n            If `True` (default), any `volumes` that exceed the maximum volume\n            of this `Pipette` will be split into multiple smaller volumes.\n\n        repeat : boolean\n            (Only applicable to :any:`distribute` and :any:`consolidate`)If\n            `True` (default), sequential :any:`aspirate` volumes will be\n            combined into one tip for the purpose of saving time. If `False`,\n            all volumes will be transferred seperately.\n\n        gradient : lambda\n            Function for calculated the curve used for gradient volumes.\n            When `volumes` is a tuple of length 2, it's values are used\n            to create a list of gradient volumes. The default curve for\n            this gradient is linear (lambda x: x), however a method can\n            be passed with the `gradient` keyword argument to create a\n            custom curve.\n\n        Returns\n        -------\n\n        This instance of :class:`Pipette`.\n\n        Examples\n        --------\n        ...\n        >>> from opentrons import instruments, labware, robot # doctest: +SKIP\n        >>> robot.reset() # doctest: +SKIP\n        >>> plate = labware.load('96-flat', '5') # doctest: +SKIP\n        >>> p300 = instruments.P300_Single(mount='right') # doctest: +SKIP\n        >>> p300.transfer(50, plate[0], plate[1]) # doctest: +SKIP\n        \"\"\"\n        # Note: currently it varies whether the pipette should have a tip on\n        # or not depending on the parameters for this call, so we cannot\n        # create a very reliable assertion on tip status\n\n        kwargs['mode'] = kwargs.get('mode', 'transfer')\n\n        touch_tip = kwargs.get('touch_tip', False)\n        if touch_tip is True:\n            touch_tip = -1\n        kwargs['touch_tip'] = touch_tip\n\n        tip_options = {\n            'once': 1,\n            'never': 0,\n            'always': float('inf')\n        }\n        tip_option = kwargs.get('new_tip', 'once')\n        tips = tip_options.get(tip_option)\n        if tips is None:\n            raise ValueError('Unknown \"new_tip\" option: {}'.format(tip_option))\n\n        plan = self._create_transfer_plan(volume, source, dest, **kwargs)\n        self._run_transfer_plan(tips, plan, **kwargs)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npauses and resume the current thread.", "response": "def delay(self, seconds=0, minutes=0):\n        \"\"\"\n        Parameters\n        ----------\n\n        seconds: float\n            The number of seconds to freeze in place.\n        \"\"\"\n\n        minutes += int(seconds / 60)\n        seconds = seconds % 60\n        seconds += float(minutes * 60)\n\n        self.robot.pause()\n        if not self.robot.is_simulating():\n            _sleep(seconds)\n        self.robot.resume()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the values for the plunger pipette.", "response": "def calibrate_plunger(\n            self,\n            top=None,\n            bottom=None,\n            blow_out=None,\n            drop_tip=None):\n        \"\"\"Set calibration values for the pipette plunger.\n\n        This can be called multiple times as the user sets each value,\n        or you can set them all at once.\n\n        Parameters\n        ----------\n\n        top : int\n           Touching but not engaging the plunger.\n\n        bottom: int\n            Must be above the pipette's physical hard-stop, while still\n            leaving enough room for 'blow_out'\n\n        blow_out : int\n            Plunger has been pushed down enough to expell all liquids.\n\n        drop_tip : int\n            This position that causes the tip to be released from the\n            pipette.\n\n        \"\"\"\n        if top is not None:\n            self.plunger_positions['top'] = top\n        if bottom is not None:\n            self.plunger_positions['bottom'] = bottom\n        if blow_out is not None:\n            self.plunger_positions['blow_out'] = blow_out\n        if drop_tip is not None:\n            self.plunger_positions['drop_tip'] = drop_tip\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_plunger_position(self, position):\n        try:\n            value = self.plunger_positions[position]\n            if helpers.is_number(value):\n                return value\n            else:\n                raise RuntimeError(\n                    'Plunger position \"{}\" not yet calibrated'.format(\n                        position))\n        except KeyError:\n            raise RuntimeError(\n                'Plunger position \"{}\" does not exist'.format(\n                    position))", "response": "Returns the calibrated coordinate of a given plunger position Raises exception if the position is not calibrated yet"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the position of the plunger in the pipette motor in mm.", "response": "def _aspirate_plunger_position(self, ul):\n        \"\"\"Calculate axis position for a given liquid volume.\n\n        Translates the passed liquid volume to absolute coordinates\n        on the axis associated with this pipette.\n\n        Calibration of the pipette motor's ul-to-mm conversion is required\n        \"\"\"\n        millimeters = ul / self._ul_per_mm(ul, 'aspirate')\n        destination_mm = self._get_plunger_position('bottom') + millimeters\n        return round(destination_mm, 6)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the dispense position for a given liquid volume.", "response": "def _dispense_plunger_position(self, ul):\n        \"\"\"Calculate axis position for a given liquid volume.\n\n        Translates the passed liquid volume to absolute coordinates\n        on the axis associated with this pipette.\n\n        Calibration of the pipette motor's ul-to-mm conversion is required\n        \"\"\"\n        millimeters = ul / self._ul_per_mm(ul, 'dispense')\n        destination_mm = self._get_plunger_position('bottom') + millimeters\n        return round(destination_mm, 6)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ul_per_mm(self, ul: float, func: str) -> float:\n        sequence = self.ul_per_mm[func]\n        return pipette_config.piecewise_volume_conversion(ul, sequence)", "response": "converts microliters to mm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the plunger percentage for a given volume.", "response": "def _volume_percentage(self, volume):\n        \"\"\"Returns the plunger percentage for a given volume.\n\n        We use this to calculate what actual position the plunger axis\n        needs to be at in order to achieve the correct volume of liquid.\n        \"\"\"\n        if volume < 0:\n            raise RuntimeError(\n                \"Volume must be a positive number, got {}.\".format(volume))\n        if volume > self.max_volume:\n            raise RuntimeError(\n                \"{0}\u00b5l exceeds pipette's maximum volume ({1}ul).\".format(\n                    volume, self.max_volume))\n        if volume < self.min_volume:\n            self.robot.add_warning(\n                \"{0}\u00b5l is less than pipette's min_volume ({1}ul).\".format(\n                    volume, self.min_volume))\n\n        return volume / self.max_volume"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_tip_during_transfer(self, tips, **kwargs):\n        if self.has_tip_rack() and tips > 0 and not self.current_tip():\n            self.pick_up_tip()", "response": "Adds a tip when running a transfer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform an aspirate when running a transfer and optionally a touch_tip afterwards.", "response": "def _aspirate_during_transfer(self, vol, loc, **kwargs):\n        \"\"\"\n        Performs an :any:`aspirate` when running a :any:`transfer`, and\n        optionally a :any:`touch_tip` afterwards.\n        \"\"\"\n        rate = kwargs.get('rate', 1)\n        mix_before = kwargs.get('mix', kwargs.get('mix_before', (0, 0)))\n        air_gap = kwargs.get('air_gap', 0)\n        touch_tip = kwargs.get('touch_tip', False)\n\n        well, _ = unpack_location(loc)\n\n        if self.current_volume == 0:\n            self._mix_during_transfer(mix_before, well, **kwargs)\n        self.aspirate(vol, loc, rate=rate)\n        if air_gap:\n            self.air_gap(air_gap)\n        if touch_tip or touch_tip is 0:  # noqa(pyflakes)\n            self.touch_tip(touch_tip)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a dispense when running a transfer and optionally a mix or touch_tip.", "response": "def _dispense_during_transfer(self, vol, loc, **kwargs):\n        \"\"\"\n        Performs a :any:`dispense` when running a :any:`transfer`, and\n        optionally a :any:`mix`, :any:`touch_tip`, and/or\n        :any:`blow_out` afterwards.\n        \"\"\"\n        mix_after = kwargs.get('mix_after', (0, 0))\n        rate = kwargs.get('rate', 1)\n        air_gap = kwargs.get('air_gap', 0)\n\n        well, _ = unpack_location(loc)\n\n        if air_gap:\n            self.dispense(air_gap, well.top(5), rate=rate)\n        self.dispense(vol, loc, rate=rate)\n        self._mix_during_transfer(mix_after, well, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a drop_tip or return_tip when the ith entry is reached.", "response": "def _drop_tip_during_transfer(self, tips, i, total, **kwargs):\n        \"\"\"\n        Performs a :any:`drop_tip` or :any:`return_tip` when\n        running a :any:`transfer`, :any:`distribute`, or :any:`consolidate`.\n        \"\"\"\n        trash = kwargs.get('trash', True)\n        if tips > 1 or (i + 1 == total and tips > 0):\n            if trash and self.trash_container:\n                self.drop_tip()\n            else:\n                self.return_tip()\n            tips -= 1\n        return tips"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_speed(self, aspirate=None, dispense=None):\n        if aspirate:\n            self.speeds['aspirate'] = aspirate\n        if dispense:\n            self.speeds['dispense'] = dispense\n        return self", "response": "Sets the speed of the Pipette."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the flow rate for the current pipette.", "response": "def set_flow_rate(self, aspirate=None, dispense=None):\n        \"\"\"\n        Set the speed (uL/second) the :any:`Pipette` plunger will move\n        during :meth:`aspirate` and :meth:`dispense`. The speed is set using\n        nominal max volumes for any given pipette model.\n        Parameters\n        ----------\n        aspirate: int\n            The speed in microliters-per-second, at which the plunger will\n            move while performing an aspirate\n\n        dispense: int\n            The speed in microliters-per-second, at which the plunger will\n            move while performing an dispense\n        \"\"\"\n        ul = self.max_volume\n        if aspirate:\n            # Set the ul_per_mm for the pipette\n            ul_per_mm = self._ul_per_mm(ul, 'aspirate')\n            self.set_speed(\n                aspirate=round(aspirate / ul_per_mm, 6))\n        if dispense:\n            # Set the ul_per_mm for the pipette\n            ul_per_mm = self._ul_per_mm(ul, 'dispense')\n            self.set_speed(\n                dispense=round(dispense / ul_per_mm, 6))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the current amperage of the pipette mount.", "response": "def set_pick_up_current(self, amperes):\n        \"\"\"\n        Set the current (amperes) the pipette mount's motor will use while\n        picking up a tip.\n\n        Parameters\n        ----------\n        amperes: float (0.0 - 2.0)\n            The amperage of the motor while creating a seal with tips.\n        \"\"\"\n        if amperes >= 0 and amperes <= 2.0:\n            self._pick_up_current = amperes\n        else:\n            raise ValueError(\n                'Amperes must be a floating point between 0.0 and 2.0')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef engage(self, height):\n        if height > MAX_ENGAGE_HEIGHT or height < 0:\n            raise ValueError('Invalid engage height. Should be 0 to {}'.format(\n                MAX_ENGAGE_HEIGHT))\n        self._driver.move(height)\n        self._engaged = True", "response": "Move the magnet to a specific height in mm from home position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive two points p1 and target, this returns a list of incremental positions or relative steps", "response": "def break_down_travel(p1, target, increment=5, mode='absolute'):\n    \"\"\"\n    given two points p1 and target, this returns a list of\n    incremental positions or relative steps\n    \"\"\"\n\n    heading = target - p1\n    if mode == 'relative':\n        heading = target\n    length = heading.length()\n\n    length_steps = length / increment\n    length_remainder = length % increment\n\n    vector_step = Vector(0, 0, 0)\n    if length_steps > 0:\n        vector_step = heading / length_steps\n    vector_remainder = vector_step * (length_remainder / increment)\n\n    res = []\n    if mode == 'absolute':\n        for i in range(int(length_steps)):\n            p1 = p1 + vector_step\n            res.append(p1)\n        p1 = p1 + vector_remainder\n        res.append(p1)\n    else:\n        for i in range(int(length_steps)):\n            res.append(vector_step)\n        res.append(vector_remainder)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _expand_for_carryover(max_vol, plan, **kwargs):\n    max_vol = float(max_vol)\n    carryover = kwargs.get('carryover', True)\n    if not carryover:\n        return plan\n    new_transfer_plan = []\n    for p in plan:\n        source = p['aspirate']['location']\n        target = p['dispense']['location']\n        volume = float(p['aspirate']['volume'])\n        while volume > max_vol * 2:\n            new_transfer_plan.append({\n                'aspirate': {'location': source, 'volume': max_vol},\n                'dispense': {'location': target, 'volume': max_vol}\n            })\n            volume -= max_vol\n\n        if volume > max_vol:\n            volume /= 2\n            new_transfer_plan.append({\n                'aspirate': {'location': source, 'volume': float(volume)},\n                'dispense': {'location': target, 'volume': float(volume)}\n            })\n        new_transfer_plan.append({\n            'aspirate': {'location': source, 'volume': float(volume)},\n            'dispense': {'location': target, 'volume': float(volume)}\n        })\n    return new_transfer_plan", "response": "Expands a list of transfers into a single list of ones with maximum volume."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncombining target volumes into one aspirate", "response": "def _compress_for_repeater(max_vol, plan, **kwargs):\n    \"\"\"\n    Reduce size of transfer plan, if mode is distribute or consolidate\n    \"\"\"\n    max_vol = float(max_vol)\n    mode = kwargs.get('mode', 'transfer')\n    if mode == 'distribute':   # combine target volumes into single aspirate\n        return _compress_for_distribute(max_vol, plan, **kwargs)\n    if mode == 'consolidate':  # combine target volumes into multiple aspirates\n        return _compress_for_consolidate(max_vol, plan, **kwargs)\n    else:\n        return plan"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compress_for_distribute(max_vol, plan, **kwargs):\n    source = None\n    new_source = None\n    a_vol = 0\n    temp_dispenses = []\n    new_transfer_plan = []\n    disposal_vol = kwargs.get('disposal_vol', 0)\n    max_vol = max_vol - disposal_vol\n\n    def _append_dispenses():\n        nonlocal a_vol, temp_dispenses, new_transfer_plan, source\n        if not temp_dispenses:\n            return\n        added_volume = 0\n        if len(temp_dispenses) > 1:\n            added_volume = disposal_vol\n        new_transfer_plan.append({\n            'aspirate': {\n                'location': source,\n                'volume': a_vol + added_volume\n            }\n        })\n        for d in temp_dispenses:\n            new_transfer_plan.append({\n                'dispense': {\n                    'location': d['location'],\n                    'volume': d['volume']\n                }\n            })\n        a_vol = 0\n        temp_dispenses = []\n\n    for p in plan:\n        this_vol = p['aspirate']['volume']\n        new_source = p['aspirate']['location']\n        if (new_source is not source) or (this_vol + a_vol > max_vol):\n            _append_dispenses()\n        source = new_source\n        a_vol += this_vol\n        temp_dispenses.append(p['dispense'])\n    _append_dispenses()\n    return new_transfer_plan", "response": "Returns a new copy of the given plan with distribuciones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compress_for_consolidate(max_vol, plan, **kwargs):\n    target = None\n    new_target = None\n    d_vol = 0\n    temp_aspirates = []\n    new_transfer_plan = []\n\n    def _append_aspirates():\n        nonlocal d_vol, temp_aspirates, new_transfer_plan, target\n        if not temp_aspirates:\n            return\n        for a in temp_aspirates:\n            new_transfer_plan.append({\n                'aspirate': {\n                    'location': a['location'], 'volume': a['volume']\n                }\n            })\n        new_transfer_plan.append({\n            'dispense': {\n                'location': target, 'volume': d_vol\n            }\n        })\n        d_vol = 0\n        temp_aspirates = []\n\n    for i, p in enumerate(plan):\n        this_vol = p['aspirate']['volume']\n        new_target = p['dispense']['location']\n        if (new_target is not target) or (this_vol + d_vol > max_vol):\n            _append_aspirates()\n        target = new_target\n        d_vol += this_vol\n        temp_aspirates.append(p['aspirate'])\n    _append_aspirates()\n    return new_transfer_plan", "response": "Combine as many aspirates as can fit within the maximum volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict of alladv settings keyed by setting ID.", "response": "def get_all_adv_settings() -> Dict[str, Dict[str, Union[str, bool, None]]]:\n    \"\"\"\n    :return: a dict of settings keyed by setting ID, where each value is a\n        dict with keys \"id\", \"title\", \"description\", and \"value\"\n    \"\"\"\n    settings_file = CONFIG['feature_flags_file']\n\n    values, _ = _read_settings_file(settings_file)\n\n    return {\n        key: {**settings_by_id[key].__dict__,\n              'value': value}\n        for key, value in values.items()\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_settings_file(settings_file: 'Path') -> SettingsData:\n    # Read settings from persistent file\n    data = _read_json_file(settings_file)\n    settings, version = _migrate(data)\n\n    if (data.get('_version') != version):\n        _write_settings_file(settings, version, settings_file)\n\n    return settings, version", "response": "Read the settings file and return the settings and version"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _migrate0to1(previous: Mapping[str, Any]) -> SettingsMap:\n    next: SettingsMap = {}\n\n    for s in settings:\n        id = s.id\n        old_id = s.old_id\n\n        if previous.get(id) is True:\n            next[id] = True\n        elif previous.get(old_id) is True:\n            next[id] = True\n        else:\n            next[id] = None\n\n    return next", "response": "Migrate to version 1 of the feature flags file. Replaces old IDs with new IDs and sets any False values to None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _migrate(data: Mapping[str, Any]) -> SettingsData:\n    next = dict(data)\n    version = next.pop('_version', 0)\n    target_version = len(_MIGRATIONS)\n    migrations = _MIGRATIONS[version:]\n\n    if len(migrations) > 0:\n        log.info(\n            \"Migrating advanced settings from version {} to {}\"\n            .format(version, target_version))\n\n    for m in migrations:\n        next = m(next)\n\n    return next, target_version", "response": "Migrate the settings from the latest version to the latest version."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ensure_values(data: Mapping[str, Any]) -> Tuple[Dict[str, Any], bool]:\n    to_return = {}\n    should_write = False\n    for keyname, typekind, default in REQUIRED_DATA:\n        if keyname not in data:\n            LOG.debug(f\"Defaulted config value {keyname} to {default}\")\n            to_return[keyname] = default\n            should_write = True\n        elif not isinstance(data[keyname], typekind):\n            LOG.warning(\n                f\"Config value {keyname} was {type(data[keyname])} not\"\n                f\" {typekind}, defaulted to {default}\")\n            to_return[keyname] = default\n            should_write = True\n        else:\n            to_return[keyname] = data[keyname]\n    return to_return, should_write", "response": "Make sure we have appropriate keys and say if we should write"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_from_path(path: str) -> Config:\n    data = _ensure_load(path)\n    if not data:\n        data = {}\n    values, should_write = _ensure_values(data)\n    values.update({'path': path})\n    config = Config(**values)\n    if config.signature_required:\n        if not os.path.exists(config.update_cert_path):\n            LOG.warning(\n                f\"No signing cert is present in {config.update_cert_path}, \"\n                \"code signature checking disabled\")\n            config = config._replace(signature_required=False)\n            config = config._replace(update_cert_path=DEFAULT_CERT_PATH)\n    if should_write:\n        save_to_path(path, config)\n    return config", "response": "Load a config from a file and ensure its structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the valid path from args then env then default", "response": "def _get_path(args_path: Optional[str]) -> str:\n    \"\"\" Find the valid path from args then env then default \"\"\"\n    env_path = os.getenv(PATH_ENVIRONMENT_VARIABLE)\n    for path, source in ((args_path, 'arg'),\n                         (env_path, 'env')):\n        if not path:\n            LOG.debug(f\"config.load: skipping {source} (path None)\")\n            continue\n        else:\n            LOG.debug(f\"config.load: using config path {path} from {source}\")\n            return path\n    return DEFAULT_PATH"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the config file to a specific path", "response": "def save_to_path(path: str, config: Config) -> None:\n    \"\"\"\n    Save the config file to a specific path (not what's in the config)\n    \"\"\"\n    LOG.debug(f\"Saving config to {path}\")\n    with open(path, 'w') as cf:\n        cf.write(json.dumps({k: v for k, v in config._asdict().items()\n                             if k != 'path'}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_stage(self, stage: Stages):\n        assert stage in Stages\n        LOG.info(f'Update session: stage {self._stage.name}->{stage.name}')\n        self._stage = stage", "response": "Set the stage and lookup message"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_error(self, error_shortmsg: str, error_longmsg: str):\n        LOG.error(f\"Update session: error in stage {self._stage.name}: \"\n                  f\"{error_shortmsg}: {error_longmsg}\")\n        self._error = Value(error_shortmsg, error_longmsg)\n        self.set_stage(Stages.ERROR)", "response": "Set the error value and add a message"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef message(self) -> str:\n        if self.is_error:\n            assert self._error\n            return self._error.human\n        else:\n            return self._stage.value.human", "response": "The human readable message of the current stage."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a flattened list of DFS traversal of subtree from object that contains descendant object and it s depth", "response": "def descendants(state, obj, level=0):\n    \"\"\" Returns a flattened list tuples of DFS traversal of subtree\n    from object that contains descendant object and it's depth \"\"\"\n    return sum([\n        [(child, level)] + descendants(state, child, level + 1)\n        for child in state[obj].children\n    ], [])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the base of a state.", "response": "def change_base(state, point=Point(0, 0, 0), src=ROOT, dst=ROOT):\n    \"\"\"\n    Transforms point from source coordinate system to destination.\n    Point(0, 0, 0) means the origin of the source.\n    \"\"\"\n    def fold(objects):\n        return functools.reduce(\n            lambda a, b: a.dot(b),\n            [state[key].transform for key in objects],\n            np.identity(4)\n        )\n\n    up, down = ascend(state, src), list(reversed(ascend(state, dst)))\n\n    # Find common prefix. Last item is common root\n    root = [n1 for n1, n2 in zip(reversed(up), down) if n1 is n2].pop()\n\n    # Nodes up to root, EXCLUDING root\n    up = list(itertools.takewhile(lambda node: node is not root, up))\n\n    # Nodes down from root, EXCLUDING root\n    down = list(itertools.dropwhile(lambda node: node is not root, down))[1:]\n\n    # Point in root's coordinate system\n    point_in_root = inv(fold(up)).dot((*point, 1))\n\n    # Return point in destination's coordinate system\n    return fold(down).dot(point_in_root)[:-1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_instrument_config(\n        instrument, measured_center) -> Tuple[Point, float]:\n    \"\"\"\n    Update config and pose tree with instrument's x and y offsets\n    and tip length based on delta between probe center and measured_center,\n    persist updated config and return it\n    \"\"\"\n    from copy import deepcopy\n    from opentrons.trackers.pose_tracker import update\n\n    robot = instrument.robot\n    config = robot.config\n    instrument_offset = deepcopy(config.instrument_offset)\n\n    dx, dy, dz = array(measured_center) - config.tip_probe.center\n\n    log.debug(\"This is measured probe center dx {}\".format(Point(dx, dy, dz)))\n    # any Z offset will adjust the tip length, so instruments have Z=0 offset\n    old_x, old_y, _ = instrument_offset[instrument.mount][instrument.type]\n    instrument_offset[instrument.mount][instrument.type] = \\\n        (old_x - dx, old_y - dy, 0.0)\n    tip_length = deepcopy(config.tip_length)\n    tip_length[instrument.name] = tip_length[instrument.name] + dz\n\n    config = config \\\n        ._replace(instrument_offset=instrument_offset) \\\n        ._replace(tip_length=tip_length)\n    robot.config = config\n    log.debug(\"Updating config for {} instrument\".format(instrument.mount))\n    robot_configs.save_robot_settings(config)\n\n    new_coordinates = change_base(\n        robot.poses,\n        src=instrument,\n        dst=instrument.instrument_mover) - Point(dx, dy, 0.0)\n    robot.poses = update(robot.poses, instrument, new_coordinates)\n\n    return robot.config", "response": "Update config and pose tree with instrument s x and y offsets and tip length based on delta between probe center and measured_center."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number in the expected string N : 12. 3 where N is the axis and 12. 3 is a floating point value for the axis position", "response": "def _parse_number_from_substring(smoothie_substring):\n    '''\n    Returns the number in the expected string \"N:12.3\", where \"N\" is the\n    axis, and \"12.3\" is a floating point value for the axis' position\n    '''\n    try:\n        return round(\n            float(smoothie_substring.split(':')[1]),\n            GCODE_ROUNDING_PRECISION\n        )\n    except (ValueError, IndexError, TypeError, AttributeError):\n        log.exception('Unexpected argument to _parse_number_from_substring:')\n        raise ParseError(\n            'Unexpected argument to _parse_number_from_substring: {}'.format(\n                smoothie_substring))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_axis_from_substring(smoothie_substring):\n    '''\n    Returns the axis in the expected string \"N:12.3\", where \"N\" is the\n    axis, and \"12.3\" is a floating point value for the axis' position\n    '''\n    try:\n        return smoothie_substring.split(':')[0].title()  # upper 1st letter\n    except (ValueError, IndexError, TypeError, AttributeError):\n        log.exception('Unexpected argument to _parse_axis_from_substring:')\n        raise ParseError(\n            'Unexpected argument to _parse_axis_from_substring: {}'.format(\n                smoothie_substring))", "response": "Parses a string that contains the axis in the expected string N : 12. 3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the Smoothieware response to a G28. 6 command ( homing - status ) and returns a dict containing the keys axis and values.", "response": "def _parse_homing_status_values(raw_homing_status_values):\n    '''\n        Parse the Smoothieware response to a G28.6 command (homing-status)\n        A \"1\" means it has been homed, and \"0\" means it has not been homed\n\n        Example response after homing just X axis:\n        \"X:1 Y:0 Z:0 A:0 B:0 C:0\"\n\n        returns: dict\n            Key is axis, value is True if the axis needs to be homed\n    '''\n    if not raw_homing_status_values or \\\n            not isinstance(raw_homing_status_values, str):\n        raise ParseError(\n            'Unexpected argument to _parse_homing_status_values: {}'.format(\n                raw_homing_status_values))\n    parsed_values = raw_homing_status_values.strip().split(' ')\n    res = {\n        _parse_axis_from_substring(s): bool(_parse_number_from_substring(s))\n        for s in parsed_values\n    }\n    if len(list(AXES) & res.keys()) != 6:\n        raise ParseError(\n            'Unexpected argument to _parse_homing_status_values: {}'.format(\n                raw_homing_status_values))\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_pipette_id(self, mount) -> Optional[str]:\n        '''\n        Reads in an attached pipette's ID\n        The ID is unique to this pipette, and is a string of unknown length\n\n        :param mount: string with value 'left' or 'right'\n        :return id string, or None\n        '''\n        res: Optional[str] = None\n        if self.simulating:\n            res = '1234567890'\n        else:\n            try:\n                res = self._read_from_pipette(\n                    GCODES['READ_INSTRUMENT_ID'], mount)\n            except UnicodeDecodeError:\n                log.exception(\"Failed to decode pipette ID string:\")\n                res = None\n        return res", "response": "Reads in an attached pipette s ID and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_pipette_model(self, mount) -> Optional[str]:\n        '''\n        Reads an attached pipette's MODEL\n        The MODEL is a unique string for this model of pipette\n\n        :param mount: string with value 'left' or 'right'\n        :return model string, or None\n        '''\n        if self.simulating:\n            res = None\n        else:\n            res = self._read_from_pipette(\n                GCODES['READ_INSTRUMENT_MODEL'], mount)\n            if res and '_v' not in res:\n                # Backward compatibility for pipettes programmed with model\n                # strings that did not include the _v# designation\n                res = res + '_v1'\n            elif res and '_v13' in res:\n                # Backward compatibility for pipettes programmed with model\n                # strings that did not include the \".\" to seperate version\n                # major and minor values\n                res = res.replace('_v13', '_v1.3')\n\n        return res", "response": "Reads an attached pipette s MODEL and returns the MODEL string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_fw_version(self):\n        '''\n        Queries Smoothieware for it's build version, and returns\n        the parsed response.\n\n        returns: str\n            Current version of attached Smoothi-driver. Versions are derived\n            from git branch-hash (eg: edge-66ec883NOMSD)\n\n        Example Smoothieware response:\n\n        Build version: edge-66ec883NOMSD, Build date: Jan 28 2018 15:26:57, MCU: LPC1769, System Clock: 120MHz  # NOQA\n          CNC Build   NOMSD Build\n        6 axis\n        '''\n        version = 'Virtual Smoothie'\n        if not self.simulating:\n            version = self._send_command('version')\n            version = version.split(',')[0].split(':')[-1].strip()\n            version = version.replace('NOMSD', '')\n        return version", "response": "Queries Smoothieware for it s build version and returns\n            the parsed response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of the current position of the current node.", "response": "def position(self):\n        \"\"\"\n        Instead of sending M114.2 we are storing target values in\n        self._position since movement and home commands are blocking and\n        assumed to go the correct place.\n\n        Cases where Smoothie would not be in the correct place (such as if a\n        belt slips) would not be corrected by getting position with M114.2\n        because Smoothie would also not be aware of slippage.\n        \"\"\"\n        return {k.upper(): v for k, v in self._position.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_homed_flags(self, flags=None):\n        '''\n        Returns Smoothieware's current homing-status, which is a dictionary\n        of boolean values for each axis (XYZABC). If an axis is False, then it\n        still needs to be homed, and it's coordinate cannot be trusted.\n        Smoothieware sets it's internal homing flags for all axes to False when\n        it has yet to home since booting/restarting, or an endstop/homing error\n\n        returns: dict\n            {\n                'X': False,\n                'Y': True,\n                'Z': False,\n                'A': True,\n                'B': False,\n                'C': True\n            }\n        '''\n        if flags and isinstance(flags, dict):\n            self.homed_flags.update(flags)\n\n        elif self.simulating:\n            self.homed_flags.update({ax: False for ax in AXES})\n\n        elif self.is_connected():\n\n            def _recursive_update_homed_flags(retries):\n                try:\n                    res = self._send_command(GCODES['HOMING_STATUS'])\n                    flags = _parse_homing_status_values(res)\n                    self.homed_flags.update(flags)\n                except ParseError as e:\n                    retries -= 1\n                    if retries <= 0:\n                        raise e\n                    if not self.simulating:\n                        sleep(DEFAULT_STABILIZE_DELAY)\n                    return _recursive_update_homed_flags(retries)\n\n            _recursive_update_homed_flags(DEFAULT_COMMAND_RETRIES)", "response": "Updates the internal homing - flags of the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_speed(self, value):\n        ''' set total axes movement speed in mm/second'''\n        self._combined_speed = float(value)\n        speed_per_min = int(self._combined_speed * SEC_PER_MIN)\n        command = GCODES['SET_SPEED'] + str(speed_per_min)\n        log.debug(\"set_speed: {}\".format(command))\n        self._send_command(command)", "response": "set total axes movement speed in mm / second"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the acceleration of the current locale for the current locale.", "response": "def set_acceleration(self, settings):\n        '''\n        Sets the acceleration (mm/sec^2) that a given axis will move\n\n        settings\n            Dict with axes as valies (e.g.: 'X', 'Y', 'Z', 'A', 'B', or 'C')\n            and floating point number for mm-per-second-squared (mm/sec^2)\n        '''\n        self._acceleration.update(settings)\n        values = ['{}{}'.format(axis.upper(), value)\n                  for axis, value in sorted(settings.items())]\n        command = '{} {}'.format(\n            GCODES['ACCELERATION'],\n            ' '.join(values)\n        )\n        log.debug(\"set_acceleration: {}\".format(command))\n        self._send_command(command)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_active_current(self, settings):\n        '''\n        Sets the amperage of each motor for when it is activated by driver.\n        Values are initialized from the `robot_config.high_current` values,\n        and can then be changed through this method by other parts of the API.\n\n        For example, `Pipette` setting the active-current of it's pipette,\n        depending on what model pipette it is, and what action it is performing\n\n        settings\n            Dict with axes as valies (e.g.: 'X', 'Y', 'Z', 'A', 'B', or 'C')\n            and floating point number for current (generally between 0.1 and 2)\n        '''\n        self._active_current_settings['now'].update(settings)\n\n        # if an axis specified in the `settings` is currently active,\n        # reset it's current to the new active-current value\n        active_axes_to_update = {\n            axis: amperage\n            for axis, amperage in self._active_current_settings['now'].items()\n            if self._active_axes.get(axis) is True\n            if self.current[axis] != amperage\n        }\n        if active_axes_to_update:\n            self._save_current(active_axes_to_update, axes_active=True)", "response": "Sets the active - current of each motor for when it is activated by driver."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_dwelling_current(self, settings):\n        '''\n        Sets the amperage of each motor for when it is dwelling.\n        Values are initialized from the `robot_config.log_current` values,\n        and can then be changed through this method by other parts of the API.\n\n        For example, `Pipette` setting the dwelling-current of it's pipette,\n        depending on what model pipette it is.\n\n        settings\n            Dict with axes as valies (e.g.: 'X', 'Y', 'Z', 'A', 'B', or 'C')\n            and floating point number for current (generally between 0.1 and 2)\n        '''\n        self._dwelling_current_settings['now'].update(settings)\n\n        # if an axis specified in the `settings` is currently dwelling,\n        # reset it's current to the new dwelling-current value\n        dwelling_axes_to_update = {\n            axis: amps\n            for axis, amps in self._dwelling_current_settings['now'].items()\n            if self._active_axes.get(axis) is False\n            if self.current[axis] != amps\n        }\n        if dwelling_axes_to_update:\n            self._save_current(dwelling_axes_to_update, axes_active=False)", "response": "Sets the current value of each log entry in the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the current setting for the current locale.", "response": "def _save_current(self, settings, axes_active=True):\n        '''\n        Sets the current in Amperes (A) by axis. Currents are limited to be\n        between 0.0-2.0 amps per axis motor.\n\n        Note: this method does not send gcode commands, but instead stores the\n        desired current setting. A seperate call to _generate_current_command()\n        will return a gcode command that can be used to set Smoothie's current\n\n        settings\n            Dict with axes as valies (e.g.: 'X', 'Y', 'Z', 'A', 'B', or 'C')\n            and floating point number for current (generally between 0.1 and 2)\n        '''\n        self._active_axes.update({\n            ax: axes_active\n            for ax in settings.keys()\n        })\n        self._current_settings['now'].update(settings)\n        log.debug(\"_save_current: {}\".format(self.current))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_current_command(self):\n        '''\n        Returns a constructed GCode string that contains this driver's\n        axis-current settings, plus a small delay to wait for those settings\n        to take effect.\n        '''\n        values = ['{}{}'.format(axis, value)\n                  for axis, value in sorted(self.current.items())]\n        current_cmd = '{} {}'.format(\n            GCODES['SET_CURRENT'],\n            ' '.join(values)\n        )\n        command = '{currents} {code}P{seconds}'.format(\n            currents=current_cmd,\n            code=GCODES['DWELL'],\n            seconds=CURRENT_CHANGE_DELAY\n        )\n        log.debug(\"_generate_current_command: {}\".format(command))\n        return command", "response": "Generates the current command for this driver."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisengages the stepper - motor s 36v output to motor", "response": "def disengage_axis(self, axes):\n        '''\n        Disable the stepper-motor-driver's 36v output to motor\n        This is a safe GCODE to send to Smoothieware, as it will automatically\n        re-engage the motor if it receives a home or move command\n\n        axes\n            String containing the axes to be disengaged\n            (e.g.: 'XY' or 'ZA' or 'XYZABC')\n        '''\n        axes = ''.join(set(axes.upper()) & set(AXES))\n        if axes:\n            log.debug(\"disengage_axis: {}\".format(axes))\n            self._send_command(GCODES['DISENGAGE_MOTOR'] + axes)\n            for axis in axes:\n                self.engaged_axes[axis] = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dwell_axes(self, axes):\n        '''\n        Sets motors to low current, for when they are not moving.\n\n        Dwell for XYZA axes is only called after HOMING\n        Dwell for BC axes is called after both HOMING and MOVING\n\n        axes:\n            String containing the axes to set to low current (eg: 'XYZABC')\n        '''\n        axes = ''.join(set(axes) & set(AXES) - set(DISABLE_AXES))\n        dwelling_currents = {\n            ax: self._dwelling_current_settings['now'][ax]\n            for ax in axes\n            if self._active_axes[ax] is True\n        }\n        if dwelling_currents:\n            self._save_current(dwelling_currents, axes_active=False)", "response": "Sets motors to low current for when they are not moving."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef activate_axes(self, axes):\n        '''\n        Sets motors to a high current, for when they are moving\n        and/or must hold position\n\n        Activating XYZABC axes before both HOMING and MOVING\n\n        axes:\n            String containing the axes to set to high current (eg: 'XYZABC')\n        '''\n        axes = ''.join(set(axes) & set(AXES) - set(DISABLE_AXES))\n        active_currents = {\n            ax: self._active_current_settings['now'][ax]\n            for ax in axes\n            if self._active_axes[ax] is False\n        }\n        if active_currents:\n            self._save_current(active_currents, axes_active=True)", "response": "Activates the specified axes before both HOMING and MOVING."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send_command(self, command, timeout=DEFAULT_SMOOTHIE_TIMEOUT):\n        if self.simulating:\n            return\n\n        command_line = command + ' ' + SMOOTHIE_COMMAND_TERMINATOR\n        ret_code = self._recursive_write_and_return(\n            command_line, timeout, DEFAULT_COMMAND_RETRIES)\n\n        ret_code = self._remove_unwanted_characters(command_line, ret_code)\n\n        # Smoothieware returns error state if a switch was hit while moving\n        if (ERROR_KEYWORD in ret_code.lower()) or \\\n                (ALARM_KEYWORD in ret_code.lower()):\n            self._reset_from_error()\n            error_axis = ret_code.strip()[-1]\n            if GCODES['HOME'] not in command and error_axis in 'XYZABC':\n                self.home(error_axis)\n            raise SmoothieError(ret_code)\n\n        return ret_code.strip()", "response": "Sends a GCODE command to the robot and waits until the command is done."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_from_pipette(self, gcode, mount) -> Optional[str]:\n        '''\n        Read from an attached pipette's internal memory. The gcode used\n        determines which portion of memory is read and returned.\n\n        All motors must be disengaged to consistently read over I2C lines\n\n        gcode:\n            String (str) containing a GCode\n            either 'READ_INSTRUMENT_ID' or 'READ_INSTRUMENT_MODEL'\n        mount:\n            String (str) with value 'left' or 'right'\n        '''\n        allowed_mounts = {'left': 'L', 'right': 'R'}\n        mount = allowed_mounts.get(mount)\n        if not mount:\n            raise ValueError('Unexpected mount: {}'.format(mount))\n        try:\n            # EMI interference from both plunger motors has been found to\n            # prevent the I2C lines from communicating between Smoothieware and\n            # pipette's onboard EEPROM. To avoid, turn off both plunger motors\n            self.disengage_axis('BC')\n            self.delay(CURRENT_CHANGE_DELAY)\n            # request from Smoothieware the information from that pipette\n            res = self._send_command(gcode + mount)\n            if res:\n                res = _parse_instrument_data(res)\n                assert mount in res\n                # data is read/written as strings of HEX characters\n                # to avoid firmware weirdness in how it parses GCode arguments\n                return _byte_array_to_ascii_string(res[mount])\n        except (ParseError, AssertionError, SmoothieError):\n            pass\n        return None", "response": "Read from an attached pipette s internal memory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_to_pipette(self, gcode, mount, data_string):\n        '''\n        Write to an attached pipette's internal memory. The gcode used\n        determines which portion of memory is written to.\n\n        NOTE: To enable write-access to the pipette, it's button must be held\n\n        gcode:\n            String (str) containing a GCode\n            either 'WRITE_INSTRUMENT_ID' or 'WRITE_INSTRUMENT_MODEL'\n        mount:\n            String (str) with value 'left' or 'right'\n        data_string:\n            String (str) that is of unkown length\n        '''\n        allowed_mounts = {'left': 'L', 'right': 'R'}\n        mount = allowed_mounts.get(mount)\n        if not mount:\n            raise ValueError('Unexpected mount: {}'.format(mount))\n        if not isinstance(data_string, str):\n            raise ValueError(\n                'Expected {0}, not {1}'.format(str, type(data_string)))\n        # EMI interference from both plunger motors has been found to\n        # prevent the I2C lines from communicating between Smoothieware and\n        # pipette's onboard EEPROM. To avoid, turn off both plunger motors\n        self.disengage_axis('BC')\n        self.delay(CURRENT_CHANGE_DELAY)\n        # data is read/written as strings of HEX characters\n        # to avoid firmware weirdness in how it parses GCode arguments\n        byte_string = _byte_array_to_hex_string(\n            bytearray(data_string.encode()))\n        command = gcode + mount + byte_string\n        log.debug(\"_write_to_pipette: {}\".format(command))\n        self._send_command(command)", "response": "Write to an attached pipette s internal memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove to the target Smoothieware coordinate along any of the size of the size of the size of the internal Arc.", "response": "def move(self, target, home_flagged_axes=False):\n        '''\n        Move to the `target` Smoothieware coordinate, along any of the size\n        axes, XYZABC.\n\n        target: dict\n            dict setting the coordinate that Smoothieware will be at when\n            `move()` returns. `target` keys are the axis in upper-case, and the\n            values are the coordinate in millimeters (float)\n\n        home_flagged_axes: boolean (default=False)\n            If set to `True`, each axis included within the target coordinate\n            may be homed before moving, determined by Smoothieware's internal\n            homing-status flags (`True` means it has already homed). All axes'\n            flags are set to `False` by Smoothieware under three conditions:\n            1) Smoothieware boots or resets, 2) if a HALT gcode or signal\n            is sent, or 3) a homing/limitswitch error occured.\n        '''\n        from numpy import isclose\n\n        self.run_flag.wait()\n\n        def valid_movement(coords, axis):\n            return not (\n                (axis in DISABLE_AXES) or\n                (coords is None) or\n                isclose(coords, self.position[axis])\n            )\n\n        def create_coords_list(coords_dict):\n            return [\n                axis + str(round(coords, GCODE_ROUNDING_PRECISION))\n                for axis, coords in sorted(coords_dict.items())\n                if valid_movement(coords, axis)\n            ]\n\n        backlash_target = target.copy()\n        backlash_target.update({\n            axis: value + PLUNGER_BACKLASH_MM\n            for axis, value in sorted(target.items())\n            if axis in 'BC' and self.position[axis] < value\n        })\n\n        target_coords = create_coords_list(target)\n        backlash_coords = create_coords_list(backlash_target)\n\n        if target_coords:\n            non_moving_axes = ''.join([\n                ax\n                for ax in AXES\n                if ax not in target.keys()\n            ])\n            self.dwell_axes(non_moving_axes)\n            self.activate_axes(target.keys())\n\n            # include the current-setting gcodes within the moving gcode string\n            # to reduce latency, since we're setting current so much\n            command = self._generate_current_command()\n\n            if backlash_coords != target_coords:\n                command += ' ' + GCODES['MOVE'] + ''.join(backlash_coords)\n            command += ' ' + GCODES['MOVE'] + ''.join(target_coords)\n\n            try:\n                for axis in target.keys():\n                    self.engaged_axes[axis] = True\n                if home_flagged_axes:\n                    self.home_flagged_axes(''.join(list(target.keys())))\n                log.debug(\"move: {}\".format(command))\n                # TODO (andy) a movement's timeout should be calculated by\n                # how long the movement is expected to take. A default timeout\n                # of 30 seconds prevents any movements that take longer\n                self._send_command(command, timeout=DEFAULT_MOVEMENT_TIMEOUT)\n            finally:\n                # dwell pipette motors because they get hot\n                plunger_axis_moved = ''.join(set('BC') & set(target.keys()))\n                if plunger_axis_moved:\n                    self.dwell_axes(plunger_axis_moved)\n                    self._set_saved_current()\n\n            self._update_position(target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fast_home(self, axis, safety_margin):\n        ''' home after a controlled motor stall\n\n        Given a known distance we have just stalled along an axis, move\n        that distance away from the homing switch. Then finish with home.\n        '''\n        # move some mm distance away from the target axes endstop switch(es)\n        destination = {\n            ax: self.homed_position.get(ax) - abs(safety_margin)\n            for ax in axis.upper()\n        }\n\n        # there is a chance the axis will hit it's home switch too soon\n        # if this happens, catch the error and continue with homing afterwards\n        try:\n            self.move(destination)\n        except SmoothieError:\n            pass\n\n        # then home once we're closer to the endstop(s)\n        disabled = ''.join([ax for ax in AXES if ax not in axis.upper()])\n        return self.home(axis=axis, disabled=disabled)", "response": "home after a controlled motor stall with homing switch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unstick_axes(self, axes, distance=None, speed=None):\n        '''\n        The plunger axes on OT2 can build up static friction over time and\n        when it's cold. To get over this, the robot can move that plunger at\n        normal current and a very slow speed to increase the torque, removing\n        the static friction\n\n        axes:\n            String containing each axis to be moved. Ex: 'BC' or 'ZABC'\n\n        distance:\n            Distance to travel in millimeters (default is 1mm)\n\n        speed:\n            Millimeters-per-second to travel to travel at (default is 1mm/sec)\n        '''\n        for ax in axes:\n            if ax not in AXES:\n                raise ValueError('Unknown axes: {}'.format(axes))\n\n        if distance is None:\n            distance = UNSTICK_DISTANCE\n        if speed is None:\n            speed = UNSTICK_SPEED\n\n        self.push_active_current()\n        self.set_active_current({\n            ax: self._config.high_current[ax]\n            for ax in axes\n            })\n        self.push_axis_max_speed()\n        self.set_axis_max_speed({ax: speed for ax in axes})\n\n        # only need to request switch state once\n        state_of_switches = {ax: False for ax in AXES}\n        if not self.simulating:\n            state_of_switches = self.switch_state\n\n        # incase axes is pressing endstop, home it slowly instead of moving\n        homing_axes = ''.join([ax for ax in axes if state_of_switches[ax]])\n        moving_axes = {\n            ax: self.position[ax] - distance  # retract\n            for ax in axes\n            if (not state_of_switches[ax]) and (ax not in homing_axes)\n        }\n\n        try:\n            if moving_axes:\n                self.move(moving_axes)\n            if homing_axes:\n                self.home(homing_axes)\n        finally:\n            self.pop_active_current()\n            self.pop_axis_max_speed()", "response": "This method is used to build up static friction over time and the robot can move that plunger at a specific time and the torque is at a very slow speed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kill(self):\n        log.debug(\"kill\")\n        self._smoothie_hard_halt()\n        self._reset_from_error()\n        self._setup()", "response": "Kills the Smoothie driver."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of axes to check this method will home each axis if they are not already homed.", "response": "def home_flagged_axes(self, axes_string):\n        '''\n        Given a list of axes to check, this method will home each axis if\n        Smoothieware's internal flag sets it as needing to be homed\n        '''\n        axes_that_need_to_home = [\n            axis\n            for axis, already_homed in self.homed_flags.items()\n            if (not already_homed) and (axis in axes_string)\n        ]\n        if axes_that_need_to_home:\n            axes_string = ''.join(axes_that_need_to_home)\n            self.home(axes_string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def update_firmware(self,\n                              filename: str,\n                              loop: asyncio.AbstractEventLoop = None,\n                              explicit_modeset: bool = True) -> str:\n        \"\"\"\n        Program the smoothie board with a given hex file.\n\n        If explicit_modeset is True (default), explicitly place the smoothie in\n        programming mode.\n\n        If explicit_modeset is False, assume the smoothie is already in\n        programming mode.\n        \"\"\"\n        # ensure there is a reference to the port\n        if self.simulating:\n            return 'Did nothing (simulating)'\n\n        smoothie_update._ensure_programmer_executable()\n\n        if not self.is_connected():\n            self._connect_to_port()\n\n        # get port name\n        port = self._connection.port\n\n        if explicit_modeset:\n            # set smoothieware into programming mode\n            self._smoothie_programming_mode()\n            # close the port so other application can access it\n            self._connection.close()\n\n        # run lpc21isp, THIS WILL TAKE AROUND 1 MINUTE TO COMPLETE\n        update_cmd = 'lpc21isp -wipe -donotstart {0} {1} {2} 12000'.format(\n            filename, port, self._config.serial_speed)\n        kwargs: Dict[str, Any] = {'stdout': asyncio.subprocess.PIPE}\n        if loop:\n            kwargs['loop'] = loop\n        proc = await asyncio.create_subprocess_shell(\n            update_cmd, **kwargs)\n        rd: bytes = await proc.stdout.read()  # type: ignore\n        res = rd.decode().strip()\n        await proc.communicate()\n\n        # re-open the port\n        self._connection.open()\n        # reset smoothieware\n        self._smoothie_reset()\n        # run setup gcodes\n        self._setup()\n\n        return res", "response": "Update the firmware of a given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _connect(self):\n        if self._poller:\n            self._poller.join()\n        self._driver.connect(self._port)\n        self._device_info = self._driver.get_device_info()\n        self._poller = Poller(self._driver)\n        self._poller.start()", "response": "Connect to the TempDeck and connect to the Port"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(robot, container_name, slot, label=None, share=False):\n\n    # OT-One users specify columns in the A1, B3 fashion\n    # below methods help convert to the 1, 2, etc integer names\n    def is_ot_one_slot_name(s):\n        return isinstance(s, str) and len(s) == 2 and s[0] in 'ABCD'\n\n    def convert_ot_one_slot_names(s):\n        col = 'ABCD'.index(slot[0])\n        row = int(slot[1]) - 1\n        slot_number = col + (row * robot.get_max_robot_cols()) + 1\n        log.warning('Changing deprecated slot name \"{}\" to \"{}\"'.format(\n            slot, slot_number))\n        return slot_number\n\n    if isinstance(slot, str):\n        # convert to integer\n        try:\n            slot = int(slot)\n        except (ValueError, TypeError):\n            if is_ot_one_slot_name(slot):\n                slot = convert_ot_one_slot_names(slot)\n\n    if helpers.is_number(slot):\n        # test that it is within correct range\n        if not (1 <= slot <= len(robot.deck)):\n            raise ValueError('Unknown slot: {}'.format(slot))\n        slot = str(slot)\n\n    return robot.add_container(container_name, slot, label, share)", "response": "Loads a new container into the current hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(name, grid, spacing, diameter, depth, volume=0):\n    columns, rows = grid\n    col_spacing, row_spacing = spacing\n    custom_container = Container()\n    properties = {\n        'type': 'custom',\n        'diameter': diameter,\n        'height': depth,\n        'total-liquid-volume': volume\n    }\n\n    for c in range(columns):\n        for r in range(rows):\n            well = Well(properties=properties)\n            well_name = chr(r + ord('A')) + str(1 + c)\n            coordinates = (c * col_spacing, (rows - r - 1) * row_spacing, 0)\n            custom_container.add(well, well_name, coordinates)\n    database.save_new_container(custom_container, name)\n    return database.load_container(name)", "response": "Creates a new regularly - spaced well with the given grid and spacing."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_new_labware(container_name):\n    defn = new_labware.load_definition_by_name(container_name)\n    labware_id = defn['otId']\n    saved_offset = _look_up_offsets(labware_id)\n    container = Container()\n    log.info(f\"Container name {container_name}\")\n    container.properties['type'] = container_name\n    container.properties['otId'] = labware_id\n    format = defn['parameters']['format']\n\n    container._coordinates = Vector(defn['cornerOffsetFromSlot'])\n    for well_name in itertools.chain(*defn['ordering']):\n        well_obj, well_pos = _load_new_well(\n            defn['wells'][well_name], saved_offset, format)\n        container.add(well_obj, well_name, well_pos)\n    return container", "response": "Load a labware in the new schema into a placeable."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a JSON object containing the hierarchy of the Android hierarchy.", "response": "def get_android_hierarchy(d):\n    \"\"\"\n    Returns:\n        JSON object\n    \"\"\"\n    page_xml = d.dump_hierarchy(compressed=False, pretty=False).encode('utf-8')\n    dom = xml.dom.minidom.parseString(page_xml)\n    root = dom.documentElement\n\n    def travel(node):\n        # print(node)\n        if node.attributes is None:\n            return\n        json_node = parse_uiautomator_node(node)\n        json_node['id'] = str(uuid.uuid4())\n        if node.childNodes:\n            children = []\n            for n in node.childNodes:\n                sub_hierarchy = travel(n)\n                if sub_hierarchy:\n                    children.append(sub_hierarchy)\n            json_node['children'] = children\n        return json_node\n\n    return travel(root)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run(self, device_url, code):\n        try:\n\n            print(\"DEBUG: run code\\n%s\" % code)\n            env = os.environ.copy()\n            env['UIAUTOMATOR_DEBUG'] = 'true'\n            if device_url and device_url != 'default':\n                env['ATX_CONNECT_URL'] = tostr(device_url)\n            start_time = time.time()\n\n            self.proc = subprocess.Popen([sys.executable, \"-u\"],\n                                         env=env, stdout=PIPE, stderr=subprocess.STDOUT, stdin=PIPE)\n            self.proc.stdin.write(code)\n            self.proc.stdin.close()\n\n            for line in iter(self.proc.stdout.readline, b''):\n                print(\"recv subprocess:\", repr(line))\n                if line is None:\n                    break\n                gqueue.put((self, {\"buffer\": line.decode('utf-8')}))\n            print(\"Wait exit\")\n            exit_code = self.proc.wait()\n            duration = time.time() - start_time\n            ret = {\n                \"buffer\": \"\",\n                \"result\": {\"exitCode\": exit_code, \"duration\": int(duration)*1000}\n            }\n            gqueue.put((self, ret))\n            time.sleep(3)  # wait until write done\n        except Exception:\n            traceback.print_exc()", "response": "Run code in a thread and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a HOCON config into a JSON string representation.", "response": "def to_json(cls, config, compact=False, indent=2, level=0):\n        \"\"\"Convert HOCON input into a JSON output\n\n        :return: JSON string representation\n        :type return: basestring\n        \"\"\"\n        lines = \"\"\n        if isinstance(config, ConfigTree):\n            if len(config) == 0:\n                lines += '{}'\n            else:\n                lines += '{\\n'\n                bet_lines = []\n                for key, item in config.items():\n                    bet_lines.append('{indent}\"{key}\": {value}'.format(\n                        indent=''.rjust((level + 1) * indent, ' '),\n                        key=key.strip('\"'),  # for dotted keys enclosed with \"\" to not be interpreted as nested key\n                        value=cls.to_json(item, compact, indent, level + 1))\n                    )\n                lines += ',\\n'.join(bet_lines)\n                lines += '\\n{indent}}}'.format(indent=''.rjust(level * indent, ' '))\n        elif isinstance(config, list):\n            if len(config) == 0:\n                lines += '[]'\n            else:\n                lines += '[\\n'\n                bet_lines = []\n                for item in config:\n                    bet_lines.append('{indent}{value}'.format(\n                        indent=''.rjust((level + 1) * indent, ' '),\n                        value=cls.to_json(item, compact, indent, level + 1))\n                    )\n                lines += ',\\n'.join(bet_lines)\n                lines += '\\n{indent}]'.format(indent=''.rjust(level * indent, ' '))\n        elif isinstance(config, basestring):\n            lines = json.dumps(config)\n        elif config is None or isinstance(config, NoneValue):\n            lines = 'null'\n        elif config is True:\n            lines = 'true'\n        elif config is False:\n            lines = 'false'\n        else:\n            lines = str(config)\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_hocon(cls, config, compact=False, indent=2, level=0):\n        lines = \"\"\n        if isinstance(config, ConfigTree):\n            if len(config) == 0:\n                lines += '{}'\n            else:\n                if level > 0:  # don't display { at root level\n                    lines += '{\\n'\n                bet_lines = []\n\n                for key, item in config.items():\n                    if compact:\n                        full_key = key\n                        while isinstance(item, ConfigTree) and len(item) == 1:\n                            key, item = next(iter(item.items()))\n                            full_key += '.' + key\n                    else:\n                        full_key = key\n\n                    bet_lines.append('{indent}{key}{assign_sign} {value}'.format(\n                        indent=''.rjust(level * indent, ' '),\n                        key=full_key,\n                        assign_sign='' if isinstance(item, dict) else ' =',\n                        value=cls.to_hocon(item, compact, indent, level + 1))\n                    )\n                lines += '\\n'.join(bet_lines)\n\n                if level > 0:  # don't display { at root level\n                    lines += '\\n{indent}}}'.format(indent=''.rjust((level - 1) * indent, ' '))\n        elif isinstance(config, list):\n            if len(config) == 0:\n                lines += '[]'\n            else:\n                lines += '[\\n'\n                bet_lines = []\n                for item in config:\n                    bet_lines.append('{indent}{value}'.format(indent=''.rjust(level * indent, ' '),\n                                                              value=cls.to_hocon(item, compact, indent, level + 1)))\n                lines += '\\n'.join(bet_lines)\n                lines += '\\n{indent}]'.format(indent=''.rjust((level - 1) * indent, ' '))\n        elif isinstance(config, basestring):\n            if '\\n' in config:\n                lines = '\"\"\"{value}\"\"\"'.format(value=config)  # multilines\n            else:\n                lines = '\"{value}\"'.format(value=config.replace('\\n', '\\\\n').replace('\"', '\\\\\"'))\n        elif isinstance(config, ConfigValues):\n            lines = ''.join(cls.to_hocon(o, compact, indent, level) for o in config.tokens)\n        elif isinstance(config, ConfigSubstitution):\n            lines = '${'\n            if config.optional:\n                lines += '?'\n            lines += config.variable + '}' + config.ws\n        elif isinstance(config, ConfigQuotedString):\n            if '\\n' in config.value:\n                lines = '\"\"\"{value}\"\"\"'.format(value=config.value)  # multilines\n            else:\n                lines = '\"{value}\"'.format(value=config.value.replace('\\n', '\\\\n').replace('\"', '\\\\\"'))\n        elif config is None or isinstance(config, NoneValue):\n            lines = 'null'\n        elif config is True:\n            lines = 'true'\n        elif config is False:\n            lines = 'false'\n        else:\n            lines = str(config)\n        return lines", "response": "Convert a HOCON config into a JSON string representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_yaml(cls, config, compact=False, indent=2, level=0):\n        lines = \"\"\n        if isinstance(config, ConfigTree):\n            if len(config) > 0:\n                if level > 0:\n                    lines += '\\n'\n                bet_lines = []\n                for key, item in config.items():\n                    bet_lines.append('{indent}{key}: {value}'.format(\n                        indent=''.rjust(level * indent, ' '),\n                        key=key.strip('\"'),  # for dotted keys enclosed with \"\" to not be interpreted as nested key,\n                        value=cls.to_yaml(item, compact, indent, level + 1))\n                    )\n                lines += '\\n'.join(bet_lines)\n        elif isinstance(config, list):\n            config_list = [line for line in config if line is not None]\n            if len(config_list) == 0:\n                lines += '[]'\n            else:\n                lines += '\\n'\n                bet_lines = []\n                for item in config_list:\n                    bet_lines.append('{indent}- {value}'.format(indent=''.rjust(level * indent, ' '),\n                                                                value=cls.to_yaml(item, compact, indent, level + 1)))\n                lines += '\\n'.join(bet_lines)\n        elif isinstance(config, basestring):\n            # if it contains a \\n then it's multiline\n            lines = config.split('\\n')\n            if len(lines) == 1:\n                lines = config\n            else:\n                lines = '|\\n' + '\\n'.join([line.rjust(level * indent, ' ') for line in lines])\n        elif config is None or isinstance(config, NoneValue):\n            lines = 'null'\n        elif config is True:\n            lines = 'true'\n        elif config is False:\n            lines = 'false'\n        else:\n            lines = str(config)\n        return lines", "response": "Convert a HOCON config into a YAML string representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_properties(cls, config, compact=False, indent=2, key_stack=[]):\n\n        def escape_value(value):\n            return value.replace('=', '\\\\=').replace('!', '\\\\!').replace('#', '\\\\#').replace('\\n', '\\\\\\n')\n\n        stripped_key_stack = [key.strip('\"') for key in key_stack]\n        lines = []\n        if isinstance(config, ConfigTree):\n            for key, item in config.items():\n                if item is not None:\n                    lines.append(cls.to_properties(item, compact, indent, stripped_key_stack + [key]))\n        elif isinstance(config, list):\n            for index, item in enumerate(config):\n                if item is not None:\n                    lines.append(cls.to_properties(item, compact, indent, stripped_key_stack + [str(index)]))\n        elif isinstance(config, basestring):\n            lines.append('.'.join(stripped_key_stack) + ' = ' + escape_value(config))\n        elif config is True:\n            lines.append('.'.join(stripped_key_stack) + ' = true')\n        elif config is False:\n            lines.append('.'.join(stripped_key_stack) + ' = false')\n        elif config is None or isinstance(config, NoneValue):\n            pass\n        else:\n            lines.append('.'.join(stripped_key_stack) + ' = ' + str(config))\n        return '\\n'.join([line for line in lines if len(line) > 0])", "response": "Convert a HOCON config into a. properties string representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_from_file(cls, input_file=None, output_file=None, output_format='json', indent=2, compact=False):\n\n        if input_file is None:\n            content = sys.stdin.read()\n            config = ConfigFactory.parse_string(content)\n        else:\n            config = ConfigFactory.parse_file(input_file)\n\n        res = cls.convert(config, output_format, indent, compact)\n        if output_file is None:\n            print(res)\n        else:\n            with open(output_file, \"w\") as fd:\n                fd.write(res)", "response": "Convert from file to json properties or yaml string representation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a file into a list of Config objects.", "response": "def parse_file(cls, filename, encoding='utf-8', required=True, resolve=True, unresolved_value=DEFAULT_SUBSTITUTION):\n        \"\"\"Parse file\n\n        :param filename: filename\n        :type filename: basestring\n        :param encoding: file encoding\n        :type encoding: basestring\n        :param required: If true, raises an exception if can't load file\n        :type required: boolean\n        :param resolve: if true, resolve substitutions\n        :type resolve: boolean\n        :param unresolved_value: assigned value value to unresolved substitution.\n        If overriden with a default value, it will replace all unresolved value to the default value.\n        If it is set to to pyhocon.STR_SUBSTITUTION then it will replace the value by its substitution expression (e.g., ${x})\n        :type unresolved_value: boolean\n        :return: Config object\n        :type return: Config\n        \"\"\"\n        try:\n            with codecs.open(filename, 'r', encoding=encoding) as fd:\n                content = fd.read()\n                return cls.parse_string(content, os.path.dirname(filename), resolve, unresolved_value)\n        except IOError as e:\n            if required:\n                raise e\n            logger.warn('Cannot include file %s. File does not exist or cannot be read.', filename)\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a URL into a Config object.", "response": "def parse_URL(cls, url, timeout=None, resolve=True, required=False, unresolved_value=DEFAULT_SUBSTITUTION):\n        \"\"\"Parse URL\n\n        :param url: url to parse\n        :type url: basestring\n        :param resolve: if true, resolve substitutions\n        :type resolve: boolean\n        :param unresolved_value: assigned value value to unresolved substitution.\n        If overriden with a default value, it will replace all unresolved value to the default value.\n        If it is set to to pyhocon.STR_SUBSTITUTION then it will replace the value by its substitution expression (e.g., ${x})\n        :type unresolved_value: boolean\n        :return: Config object or []\n        :type return: Config or list\n        \"\"\"\n        socket_timeout = socket._GLOBAL_DEFAULT_TIMEOUT if timeout is None else timeout\n\n        try:\n            with contextlib.closing(urlopen(url, timeout=socket_timeout)) as fd:\n                content = fd.read() if use_urllib2 else fd.read().decode('utf-8')\n                return cls.parse_string(content, os.path.dirname(url), resolve, unresolved_value)\n        except (HTTPError, URLError) as e:\n            logger.warn('Cannot include url %s. Resource is inaccessible.', url)\n            if required:\n                raise e\n            else:\n                return []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_string(cls, content, basedir=None, resolve=True, unresolved_value=DEFAULT_SUBSTITUTION):\n        return ConfigParser().parse(content, basedir, resolve, unresolved_value)", "response": "Parse a string containing a configuration key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_dict(cls, dictionary, root=False):\n\n        def create_tree(value):\n            if isinstance(value, dict):\n                res = ConfigTree(root=root)\n                for key, child_value in value.items():\n                    res.put(key, create_tree(child_value))\n                return res\n            if isinstance(value, list):\n                return [create_tree(v) for v in value]\n            else:\n                return value\n\n        return create_tree(dictionary)", "response": "Convert a dictionary into a ConfigTree object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a HOCON content into a ConfigTree or a list of ConfigTree objects", "response": "def parse(cls, content, basedir=None, resolve=True, unresolved_value=DEFAULT_SUBSTITUTION):\n        \"\"\"parse a HOCON content\n\n        :param content: HOCON content to parse\n        :type content: basestring\n        :param resolve: if true, resolve substitutions\n        :type resolve: boolean\n        :param unresolved_value: assigned value value to unresolved substitution.\n        If overriden with a default value, it will replace all unresolved value to the default value.\n        If it is set to to pyhocon.STR_SUBSTITUTION then it will replace the value by its substitution expression (e.g., ${x})\n        :type unresolved_value: boolean\n        :return: a ConfigTree or a list\n        \"\"\"\n\n        unescape_pattern = re.compile(r'\\\\.')\n\n        def replace_escape_sequence(match):\n            value = match.group(0)\n            return cls.REPLACEMENTS.get(value, value)\n\n        def norm_string(value):\n            return unescape_pattern.sub(replace_escape_sequence, value)\n\n        def unescape_string(tokens):\n            return ConfigUnquotedString(norm_string(tokens[0]))\n\n        def parse_multi_string(tokens):\n            # remove the first and last 3 \"\n            return tokens[0][3: -3]\n\n        def convert_number(tokens):\n            n = tokens[0]\n            try:\n                return int(n, 10)\n            except ValueError:\n                return float(n)\n\n        def convert_period(tokens):\n\n            period_value = int(tokens.value)\n            period_identifier = tokens.unit\n\n            period_unit = next((single_unit for single_unit, values\n                                in cls.get_supported_period_type_map().items()\n                                if period_identifier in values))\n\n            return period(period_value, period_unit)\n\n        # ${path} or ${?path} for optional substitution\n        SUBSTITUTION_PATTERN = r\"\\$\\{(?P<optional>\\?)?(?P<variable>[^}]+)\\}(?P<ws>[ \\t]*)\"\n\n        def create_substitution(instring, loc, token):\n            # remove the ${ and }\n            match = re.match(SUBSTITUTION_PATTERN, token[0])\n            variable = match.group('variable')\n            ws = match.group('ws')\n            optional = match.group('optional') == '?'\n            substitution = ConfigSubstitution(variable, optional, ws, instring, loc)\n            return substitution\n\n        # ${path} or ${?path} for optional substitution\n        STRING_PATTERN = '\"(?P<value>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"(?P<ws>[ \\t]*)'\n\n        def create_quoted_string(instring, loc, token):\n            # remove the ${ and }\n            match = re.match(STRING_PATTERN, token[0])\n            value = norm_string(match.group('value'))\n            ws = match.group('ws')\n            return ConfigQuotedString(value, ws, instring, loc)\n\n        def include_config(instring, loc, token):\n            url = None\n            file = None\n            required = False\n\n            if token[0] == 'required':\n                required = True\n                final_tokens = token[1:]\n            else:\n                final_tokens = token\n\n            if len(final_tokens) == 1:  # include \"test\"\n                value = final_tokens[0].value if isinstance(final_tokens[0], ConfigQuotedString) else final_tokens[0]\n                if value.startswith(\"http://\") or value.startswith(\"https://\") or value.startswith(\"file://\"):\n                    url = value\n                else:\n                    file = value\n            elif len(final_tokens) == 2:  # include url(\"test\") or file(\"test\")\n                value = final_tokens[1].value if isinstance(token[1], ConfigQuotedString) else final_tokens[1]\n                if final_tokens[0] == 'url':\n                    url = value\n                else:\n                    file = value\n\n            if url is not None:\n                logger.debug('Loading config from url %s', url)\n                obj = ConfigFactory.parse_URL(\n                    url,\n                    resolve=False,\n                    required=required,\n                    unresolved_value=NO_SUBSTITUTION\n                )\n            elif file is not None:\n                path = file if basedir is None else os.path.join(basedir, file)\n                logger.debug('Loading config from file %s', path)\n                obj = ConfigFactory.parse_file(\n                    path,\n                    resolve=False,\n                    required=required,\n                    unresolved_value=NO_SUBSTITUTION\n                )\n            else:\n                raise ConfigException('No file or URL specified at: {loc}: {instring}', loc=loc, instring=instring)\n\n            return ConfigInclude(obj if isinstance(obj, list) else obj.items())\n\n        @contextlib.contextmanager\n        def set_default_white_spaces():\n            default = ParserElement.DEFAULT_WHITE_CHARS\n            ParserElement.setDefaultWhitespaceChars(' \\t')\n            yield\n            ParserElement.setDefaultWhitespaceChars(default)\n\n        with set_default_white_spaces():\n            assign_expr = Forward()\n            true_expr = Keyword(\"true\", caseless=True).setParseAction(replaceWith(True))\n            false_expr = Keyword(\"false\", caseless=True).setParseAction(replaceWith(False))\n            null_expr = Keyword(\"null\", caseless=True).setParseAction(replaceWith(NoneValue()))\n            key = QuotedString('\"', escChar='\\\\', unquoteResults=False) | Word(alphanums + alphas8bit + '._- /')\n\n            eol = Word('\\n\\r').suppress()\n            eol_comma = Word('\\n\\r,').suppress()\n            comment = (Literal('#') | Literal('//')) - SkipTo(eol | StringEnd())\n            comment_eol = Suppress(Optional(eol_comma) + comment)\n            comment_no_comma_eol = (comment | eol).suppress()\n            number_expr = Regex(r'[+-]?(\\d*\\.\\d+|\\d+(\\.\\d+)?)([eE][+\\-]?\\d+)?(?=$|[ \\t]*([\\$\\}\\],#\\n\\r]|//))',\n                                re.DOTALL).setParseAction(convert_number)\n\n            period_types = itertools.chain.from_iterable(cls.get_supported_period_type_map().values())\n            period_expr = Regex(r'(?P<value>\\d+)\\s*(?P<unit>' + '|'.join(period_types) + ')$'\n                                ).setParseAction(convert_period)\n\n            # multi line string using \"\"\"\n            # Using fix described in http://pyparsing.wikispaces.com/share/view/3778969\n            multiline_string = Regex('\"\"\".*?\"*\"\"\"', re.DOTALL | re.UNICODE).setParseAction(parse_multi_string)\n            # single quoted line string\n            quoted_string = Regex(r'\"(?:[^\"\\\\\\n]|\\\\.)*\"[ \\t]*', re.UNICODE).setParseAction(create_quoted_string)\n            # unquoted string that takes the rest of the line until an optional comment\n            # we support .properties multiline support which is like this:\n            # line1  \\\n            # line2 \\\n            # so a backslash precedes the \\n\n            unquoted_string = Regex(r'(?:[^^`+?!@*&\"\\[\\{\\s\\]\\}#,=\\$\\\\]|\\\\.)+[ \\t]*', re.UNICODE).setParseAction(unescape_string)\n            substitution_expr = Regex(r'[ \\t]*\\$\\{[^\\}]+\\}[ \\t]*').setParseAction(create_substitution)\n            string_expr = multiline_string | quoted_string | unquoted_string\n\n            value_expr = period_expr | number_expr | true_expr | false_expr | null_expr | string_expr\n\n            include_content = (quoted_string | ((Keyword('url') | Keyword('file')) - Literal('(').suppress() - quoted_string - Literal(')').suppress()))\n            include_expr = (\n                Keyword(\"include\", caseless=True).suppress() + (\n                    include_content | (\n                        Keyword(\"required\") - Literal('(').suppress() - include_content - Literal(')').suppress()\n                    )\n                )\n            ).setParseAction(include_config)\n\n            root_dict_expr = Forward()\n            dict_expr = Forward()\n            list_expr = Forward()\n            multi_value_expr = ZeroOrMore(comment_eol | include_expr | substitution_expr | dict_expr | list_expr | value_expr | (Literal(\n                '\\\\') - eol).suppress())\n            # for a dictionary : or = is optional\n            # last zeroOrMore is because we can have t = {a:4} {b: 6} {c: 7} which is dictionary concatenation\n            inside_dict_expr = ConfigTreeParser(ZeroOrMore(comment_eol | include_expr | assign_expr | eol_comma))\n            inside_root_dict_expr = ConfigTreeParser(ZeroOrMore(comment_eol | include_expr | assign_expr | eol_comma), root=True)\n            dict_expr << Suppress('{') - inside_dict_expr - Suppress('}')\n            root_dict_expr << Suppress('{') - inside_root_dict_expr - Suppress('}')\n            list_entry = ConcatenatedValueParser(multi_value_expr)\n            list_expr << Suppress('[') - ListParser(list_entry - ZeroOrMore(eol_comma - list_entry)) - Suppress(']')\n\n            # special case when we have a value assignment where the string can potentially be the remainder of the line\n            assign_expr << Group(\n                key - ZeroOrMore(comment_no_comma_eol) - (dict_expr | (Literal('=') | Literal(':') | Literal('+=')) - ZeroOrMore(\n                    comment_no_comma_eol) - ConcatenatedValueParser(multi_value_expr))\n            )\n\n            # the file can be { ... } where {} can be omitted or []\n            config_expr = ZeroOrMore(comment_eol | eol) + (list_expr | root_dict_expr | inside_root_dict_expr) + ZeroOrMore(\n                comment_eol | eol_comma)\n            config = config_expr.parseString(content, parseAll=True)[0]\n\n            if resolve:\n                allow_unresolved = resolve and unresolved_value is not DEFAULT_SUBSTITUTION and unresolved_value is not MANDATORY_SUBSTITUTION\n                has_unresolved = cls.resolve_substitutions(config, allow_unresolved)\n                if has_unresolved and unresolved_value is MANDATORY_SUBSTITUTION:\n                    raise ConfigSubstitutionException('resolve cannot be set to True and unresolved_value to MANDATORY_SUBSTITUTION')\n\n            if unresolved_value is not NO_SUBSTITUTION and unresolved_value is not DEFAULT_SUBSTITUTION:\n                cls.unresolve_substitutions_to_value(config, unresolved_value)\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _resolve_variable(cls, config, substitution):\n        variable = substitution.variable\n        try:\n            return True, config.get(variable)\n        except ConfigMissingException:\n            # default to environment variable\n            value = os.environ.get(variable)\n\n            if value is None:\n                if substitution.optional:\n                    return False, None\n                else:\n                    raise ConfigSubstitutionException(\n                        \"Cannot resolve variable ${{{variable}}} (line: {line}, col: {col})\".format(\n                            variable=variable,\n                            line=lineno(substitution.loc, substitution.instring),\n                            col=col(substitution.loc, substitution.instring)))\n            elif isinstance(value, ConfigList) or isinstance(value, ConfigTree):\n                raise ConfigSubstitutionException(\n                    \"Cannot substitute variable ${{{variable}}} because it does not point to a \"\n                    \"string, int, float, boolean or null {type} (line:{line}, col: {col})\".format(\n                        variable=variable,\n                        type=value.__class__.__name__,\n                        line=lineno(substitution.loc, substitution.instring),\n                        col=col(substitution.loc, substitution.instring)))\n            return True, value", "response": "Resolve a variable in the config and substitute it with the value of the variable in the substitution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_substitutions(cls, item):\n        if isinstance(item, ConfigValues):\n            return item.get_substitutions()\n\n        substitutions = []\n        elements = []\n        if isinstance(item, ConfigTree):\n            elements = item.values()\n        elif isinstance(item, list):\n            elements = item\n\n        for child in elements:\n            substitutions += cls._find_substitutions(child)\n        return substitutions", "response": "Convert HOCON input into a JSON string representation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef postParse(self, instring, loc, token_list):\n        cleaned_token_list = [token for tokens in (token.tokens if isinstance(token, ConfigInclude) else [token]\n                                                   for token in token_list if token != '')\n                              for token in tokens]\n        config_list = ConfigList(cleaned_token_list)\n        return [config_list]", "response": "Create a list from the tokens\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef postParse(self, instring, loc, token_list):\n        config_tree = ConfigTree(root=self.root)\n        for element in token_list:\n            expanded_tokens = element.tokens if isinstance(element, ConfigInclude) else [element]\n\n            for tokens in expanded_tokens:\n                # key, value1 (optional), ...\n                key = tokens[0].strip()\n                operator = '='\n                if len(tokens) == 3 and tokens[1].strip() in [':', '=', '+=']:\n                    operator = tokens[1].strip()\n                    values = tokens[2:]\n                elif len(tokens) == 2:\n                    values = tokens[1:]\n                else:\n                    raise ParseSyntaxException(\"Unknown tokens {tokens} received\".format(tokens=tokens))\n                # empty string\n                if len(values) == 0:\n                    config_tree.put(key, '')\n                else:\n                    value = values[0]\n                    if isinstance(value, list) and operator == \"+=\":\n                        value = ConfigValues([ConfigSubstitution(key, True, '', False, loc), value], False, loc)\n                        config_tree.put(key, value, False)\n                    elif isinstance(value, unicode) and operator == \"+=\":\n                        value = ConfigValues([ConfigSubstitution(key, True, '', True, loc), ' ' + value], True, loc)\n                        config_tree.put(key, value, False)\n                    elif isinstance(value, list):\n                        config_tree.put(key, value, False)\n                    else:\n                        existing_value = config_tree.get(key, None)\n                        if isinstance(value, ConfigTree) and not isinstance(existing_value, list):\n                            # Only Tree has to be merged with tree\n                            config_tree.put(key, value, True)\n                        elif isinstance(value, ConfigValues):\n                            conf_value = value\n                            value.parent = config_tree\n                            value.key = key\n                            if isinstance(existing_value, list) or isinstance(existing_value, ConfigTree):\n                                config_tree.put(key, conf_value, True)\n                            else:\n                                config_tree.put(key, conf_value, False)\n                        else:\n                            config_tree.put(key, value, False)\n        return config_tree", "response": "Create a ConfigTree from a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge config b into a.", "response": "def merge_configs(a, b, copy_trees=False):\n        \"\"\"Merge config b into a\n\n        :param a: target config\n        :type a: ConfigTree\n        :param b: source config\n        :type b: ConfigTree\n        :return: merged config a\n        \"\"\"\n        for key, value in b.items():\n            # if key is in both a and b and both values are dictionary then merge it otherwise override it\n            if key in a and isinstance(a[key], ConfigTree) and isinstance(b[key], ConfigTree):\n                if copy_trees:\n                    a[key] = a[key].copy()\n                ConfigTree.merge_configs(a[key], b[key], copy_trees=copy_trees)\n            else:\n                if isinstance(value, ConfigValues):\n                    value.parent = a\n                    value.key = key\n                    if key in a:\n                        value.overriden_value = a[key]\n                a[key] = value\n                if a.root:\n                    if b.root:\n                        a.history[key] = a.history.get(key, []) + b.history.get(key, [value])\n                    else:\n                        a.history[key] = a.history.get(key, []) + [value]\n\n        return a"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a key into a list of path elements.", "response": "def parse_key(string):\n        \"\"\"\n        Split a key into path elements:\n        - a.b.c => a, b, c\n        - a.\"b.c\" => a, QuotedKey(\"b.c\") if . is any of the special characters: $}[]:=+#`^?!@*&.\n        - \"a\" => a\n        - a.b.\"c\" => a, b, c (special case)\n        :param str:\n        :return:\n        \"\"\"\n        special_characters = '$}[]:=+#`^?!@*&.'\n        tokens = re.findall(r'\"[^\"]+\"|[^{special_characters}]+'.format(special_characters=re.escape(special_characters)), string)\n\n        def contains_special_character(token):\n            return any((c in special_characters) for c in token)\n\n        return [token if contains_special_character(token) else token.strip('\"') for token in tokens]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nputting a value in the tree ( dot separated )", "response": "def put(self, key, value, append=False):\n        \"\"\"Put a value in the tree (dot separated)\n\n        :param key: key to use (dot separated). E.g., a.b.c\n        :type key: basestring\n        :param value: value to put\n        \"\"\"\n        self._put(ConfigTree.parse_key(key), value, append)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, key, default=UndefinedKey):\n        return self._get(ConfigTree.parse_key(key), 0, default)", "response": "Get a value from the tree at the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn string representation of value found at key", "response": "def get_string(self, key, default=UndefinedKey):\n        \"\"\"Return string representation of value found at key\n\n        :param key: key to use (dot separated). E.g., a.b.c\n        :type key: basestring\n        :param default: default value if key not found\n        :type default: basestring\n        :return: string value\n        :type return: basestring\n        \"\"\"\n        value = self.get(key, default)\n        if value is None:\n            return None\n\n        string_value = unicode(value)\n        if isinstance(value, bool):\n            string_value = string_value.lower()\n        return string_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove specified key and returns the corresponding value.", "response": "def pop(self, key, default=UndefinedKey):\n        \"\"\"Remove specified key and return the corresponding value.\n        If key is not found, default is returned if given, otherwise ConfigMissingException is raised\n\n        This method assumes the user wants to remove the last value in the chain so it parses via parse_key\n        and pops the last value out of the dict.\n\n        :param key: key to use (dot separated). E.g., a.b.c\n        :type key: basestring\n        :param default: default value if key not found\n        :type default: object\n        :param default: default value if key not found\n        :return: value in the tree located at key\n        \"\"\"\n        if default != UndefinedKey and key not in self:\n            return default\n\n        value = self.get(key, UndefinedKey)\n        lst = ConfigTree.parse_key(key)\n        parent = self.KEY_SEP.join(lst[0:-1])\n        child = lst[-1]\n\n        if parent:\n            self.get(parent).__delitem__(child)\n        else:\n            self.__delitem__(child)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_int(self, key, default=UndefinedKey):\n        value = self.get(key, default)\n        try:\n            return int(value) if value is not None else None\n        except (TypeError, ValueError):\n            raise ConfigException(\n                u\"{key} has type '{type}' rather than 'int'\".format(key=key, type=type(value).__name__))", "response": "Return int representation of value found at key\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_float(self, key, default=UndefinedKey):\n        value = self.get(key, default)\n        try:\n            return float(value) if value is not None else None\n        except (TypeError, ValueError):\n            raise ConfigException(\n                u\"{key} has type '{type}' rather than 'float'\".format(key=key, type=type(value).__name__))", "response": "Return float representation of value found at key\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a boolean value from the configuration file at key .", "response": "def get_bool(self, key, default=UndefinedKey):\n        \"\"\"Return boolean representation of value found at key\n\n        :param key: key to use (dot separated). E.g., a.b.c\n        :type key: basestring\n        :param default: default value if key not found\n        :type default: bool\n        :return: boolean value\n        :type return: bool\n        \"\"\"\n\n        # String conversions as per API-recommendations:\n        # https://github.com/typesafehub/config/blob/master/HOCON.md#automatic-type-conversions\n        bool_conversions = {\n            None: None,\n            'true': True, 'yes': True, 'on': True,\n            'false': False, 'no': False, 'off': False\n        }\n        string_value = self.get_string(key, default)\n        if string_value is not None:\n            string_value = string_value.lower()\n        try:\n            return bool_conversions[string_value]\n        except KeyError:\n            raise ConfigException(\n                u\"{key} does not translate to a Boolean value\".format(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_list(self, key, default=UndefinedKey):\n        value = self.get(key, default)\n        if isinstance(value, list):\n            return value\n        elif isinstance(value, ConfigTree):\n            lst = []\n            for k, v in sorted(value.items(), key=lambda kv: kv[0]):\n                if re.match('^[1-9][0-9]*$|0', k):\n                    lst.append(v)\n                else:\n                    raise ConfigException(u\"{key} does not translate to a list\".format(key=key))\n            return lst\n        elif value is None:\n            return None\n        else:\n            raise ConfigException(\n                u\"{key} has type '{type}' rather than 'list'\".format(key=key, type=type(value).__name__))", "response": "Return list representation of value found at key\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(self, key, default=UndefinedKey):\n        value = self.get(key, default)\n        if isinstance(value, dict):\n            return value\n        elif value is None:\n            return None\n        else:\n            raise ConfigException(\n                u\"{key} has type '{type}' rather than 'config'\".format(key=key, type=type(value).__name__))", "response": "Return tree config representation of value found at key\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new config with fallback on config", "response": "def with_fallback(self, config, resolve=True):\n        \"\"\"\n        return a new config with fallback on config\n        :param config: config or filename of the config to fallback on\n        :param resolve: resolve substitutions\n        :return: new config with fallback on config\n        \"\"\"\n        if isinstance(config, ConfigTree):\n            result = ConfigTree.merge_configs(copy.deepcopy(config), copy.deepcopy(self))\n        else:\n            from . import ConfigFactory\n            result = ConfigTree.merge_configs(ConfigFactory.parse_file(config, resolve=False), copy.deepcopy(self))\n\n        if resolve:\n            from . import ConfigParser\n            ConfigParser.resolve_substitutions(result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a deep copy of this config as a plain OrderedDict", "response": "def as_plain_ordered_dict(self):\n        \"\"\"return a deep copy of this config as a plain OrderedDict\n\n        The config tree should be fully resolved.\n\n        This is useful to get an object with no special semantics such as path expansion for the keys.\n        In particular this means that keys that contain dots are not surrounded with '\"' in the plain OrderedDict.\n\n        :return: this config as an OrderedDict\n        :type return: OrderedDict\n        \"\"\"\n        def plain_value(v):\n            if isinstance(v, list):\n                return [plain_value(e) for e in v]\n            elif isinstance(v, ConfigTree):\n                return v.as_plain_ordered_dict()\n            else:\n                if isinstance(v, ConfigValues):\n                    raise ConfigException(\"The config tree contains unresolved elements\")\n                return v\n\n        return OrderedDict((key.strip('\"'), plain_value(value)) for key, value in self.items())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlabel a single matplotlib line at position x", "response": "def labelLine(line, x, label=None, align=True, **kwargs):\n    '''Label a single matplotlib line at position x\n\n    Parameters\n    ----------\n    line : matplotlib.lines.Line\n       The line holding the label\n    x : number\n       The location in data unit of the label\n    label : string, optional\n       The label to set. This is inferred from the line by default\n    kwargs : dict, optional\n       Optional arguments passed to ax.text\n    '''\n    ax = line.axes\n    xdata = line.get_xdata()\n    ydata = line.get_ydata()\n\n    order = np.argsort(xdata)\n    xdata = xdata[order]\n    ydata = ydata[order]\n\n    # Convert datetime objects to floats\n    if isinstance(x, datetime):\n        x = date2num(x)\n\n    xmin, xmax = xdata[0], xdata[-1]\n    if (x < xmin) or (x > xmax):\n        raise Exception('x label location is outside data range!')\n\n    # Find corresponding y co-ordinate and angle of the\n    ip = 1\n    for i in range(len(xdata)):\n        if x < xdata[i]:\n            ip = i\n            break\n\n    y = ydata[ip-1] + (ydata[ip]-ydata[ip-1]) * \\\n        (x-xdata[ip-1])/(xdata[ip]-xdata[ip-1])\n\n    if not label:\n        label = line.get_label()\n\n    if align:\n        # Compute the slope\n        dx = xdata[ip] - xdata[ip-1]\n        dy = ydata[ip] - ydata[ip-1]\n        ang = degrees(atan2(dy, dx))\n\n        # Transform to screen co-ordinates\n        pt = np.array([x, y]).reshape((1, 2))\n        trans_angle = ax.transData.transform_angles(np.array((ang, )), pt)[0]\n\n    else:\n        trans_angle = 0\n\n    # Set a bunch of keyword arguments\n    if 'color' not in kwargs:\n        kwargs['color'] = line.get_color()\n\n    if ('horizontalalignment' not in kwargs) and ('ha' not in kwargs):\n        kwargs['ha'] = 'center'\n\n    if ('verticalalignment' not in kwargs) and ('va' not in kwargs):\n        kwargs['va'] = 'center'\n\n    if 'backgroundcolor' not in kwargs:\n        kwargs['backgroundcolor'] = ax.get_facecolor()\n\n    if 'clip_on' not in kwargs:\n        kwargs['clip_on'] = True\n\n    if 'zorder' not in kwargs:\n        kwargs['zorder'] = 2.5\n\n    ax.text(x, y, label, rotation=trans_angle, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlabeling all lines with their respective legends.", "response": "def labelLines(lines, align=True, xvals=None, **kwargs):\n    '''Label all lines with their respective legends.\n\n    Parameters\n    ----------\n    lines : list of matplotlib lines\n       The lines to label\n    align : boolean, optional\n       If True, the label will be aligned with the slope of the line\n       at the location of the label. If False, they will be horizontal.\n    xvals : (xfirst, xlast) or array of float, optional\n       The location of the labels. If a tuple, the labels will be\n       evenly spaced between xfirst and xlast (in the axis units).\n    kwargs : dict, optional\n       Optional arguments passed to ax.text\n    '''\n    ax = lines[0].axes\n    labLines = []\n    labels = []\n\n    # Take only the lines which have labels other than the default ones\n    for line in lines:\n        label = line.get_label()\n        if \"_line\" not in label:\n            labLines.append(line)\n            labels.append(label)\n\n    if xvals is None:\n        xvals = ax.get_xlim() # set axis limits as annotation limits, xvals now a tuple\n    if type(xvals) == tuple:\n        xmin, xmax = xvals\n        xscale = ax.get_xscale()\n        if xscale == \"log\":\n            xvals = np.logspace(np.log10(xmin), np.log10(xmax), len(labLines)+2)[1:-1]\n        else:\n            xvals = np.linspace(xmin, xmax, len(labLines)+2)[1:-1]\n\n    for line, x, label in zip(labLines, xvals, labels):\n        labelLine(line, x, label, align, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_param(self, param, default=None):\n        if param in self.config:\n            return self.config[param]\n        elif default is not None:\n            return default\n        else:\n            raise MissingParameter('ppolicy', param)", "response": "Get a parameter from the config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_as_dict(self, raw=True, vars=None):\n    # Load INI file into a dict\n    result = {}\n    for section in self.sections():\n        if section not in result:\n            result[section] = {}\n        for option in self.options(section):\n            value = self.get(section, option, raw=raw, vars=vars)\n            try:\n                value = cherrypy.lib.reprconf.unrepr(value)\n            except Exception:\n                x = sys.exc_info()[1]\n                msg = (\"Config error in section: %r, option: %r, \"\n                       \"value: %r. Config values must be valid Python.\" %\n                       (section, option, value))\n                raise ValueError(msg, x.__class__.__name__, x.args)\n            result[section][option] = value\n    return result", "response": "Convert an INI file into a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auth(self, username, password):\n        if username not in self.users:\n            return False\n        elif self.users[username][self.pwd_attr] == password:\n            return True\n        return False", "response": "Check authentication against backend"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a user to the backend", "response": "def add_user(self, attrs):\n        \"\"\" Add a user to the backend\n\n        :param attrs: attributes of the user\n        :type attrs: dict ({<attr>: <value>})\n\n        .. warning:: raise UserAlreadyExists if user already exists\n        \"\"\"\n        username = attrs[self.key]\n        if username in self.users:\n            raise UserAlreadyExists(username, self.backend_name)\n        self.users[username] = attrs\n        self.users[username]['groups'] = set([])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef del_user(self, username):\n        self._check_fix_users(username)\n        try:\n            del self.users[username]\n        except Exception as e:\n            raise UserDoesntExist(username, self.backend_name)", "response": "Delete a user from the backend"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset a list of attributes for a given user", "response": "def set_attrs(self, username, attrs):\n        \"\"\" set a list of attributes for a given user\n\n        :param username: 'key' attribute of the user\n        :type username: string\n        :param attrs: attributes of the user\n        :type attrs: dict ({<attr>: <value>})\n        \"\"\"\n        self._check_fix_users(username)\n        for attr in attrs:\n            self.users[username][attr] = attrs[attr]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_to_groups(self, username, groups):\n        self._check_fix_users(username)\n        current_groups = self.users[username]['groups']\n        new_groups = current_groups | set(groups)\n        self.users[username]['groups'] = new_groups", "response": "Add a user to a list of groups"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search(self, searchstring):\n        ret = {}\n        for user in self.users:\n            match = False\n            for attr in self.search_attrs:\n                if attr not in self.users[user]:\n                    pass\n                elif re.search(searchstring + '.*', self.users[user][attr]):\n                    match = True\n            if match:\n                ret[user] = self.users[user]\n        return ret", "response": "Search backend for users\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a user s attributes", "response": "def get_user(self, username):\n        \"\"\" Get a user's attributes\n\n        :param username: 'key' attribute of the user\n        :type username: string\n        :rtype: dict ( {<attr>: <value>} )\n\n        .. warning:: raise UserDoesntExist if user doesn't exist\n        \"\"\"\n        try:\n            return self.users[username]\n        except Exception as e:\n            raise UserDoesntExist(username, self.backend_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_groups(self, username):\n        try:\n            return self.users[username]['groups']\n        except Exception as e:\n            raise UserDoesntExist(username, self.backend_name)", "response": "Get a user s groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_loglevel(level):\n    if level == 'debug':\n        return logging.DEBUG\n    elif level == 'notice':\n        return logging.INFO\n    elif level == 'info':\n        return logging.INFO\n    elif level == 'warning' or level == 'warn':\n        return logging.WARNING\n    elif level == 'error' or level == 'err':\n        return logging.ERROR\n    elif level == 'critical' or level == 'crit':\n        return logging.CRITICAL\n    elif level == 'alert':\n        return logging.CRITICAL\n    elif level == 'emergency' or level == 'emerg':\n        return logging.CRITICAL\n    else:\n        return logging.INFO", "response": "returns a logging level object corresponding to a given syslog log level"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_format_keys(self, fmt_string):\n\n        class AccessSaver:\n            def __init__(self):\n                self.keys = []\n\n            def __getitem__(self, key):\n                self.keys.append(key)\n\n        a = AccessSaver()\n        fmt_string % a\n\n        return a.keys", "response": "Extract the keys of a format string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _normalize_group_attrs(self, attrs):\n        for key in self.group_attrs_keys:\n            if key not in attrs:\n                raise MissingGroupAttr(key)\n            if type(attrs[key]) is list and len(attrs[key]) == 1:\n                attrs[key] = attrs[key][0]\n            if type(attrs[key]) is list and len(attrs[key]) != 1:\n                raise MultivaluedGroupAttr(key)", "response": "Normalizes the attributes used to set groups\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes an ldap client and initialize the ldap client", "response": "def _connect(self):\n        \"\"\"Initialize an ldap client\"\"\"\n        ldap_client = ldap.initialize(self.uri)\n        ldap.set_option(ldap.OPT_REFERRALS, 0)\n        ldap.set_option(ldap.OPT_TIMEOUT, self.timeout)\n        if self.starttls == 'on':\n            ldap.set_option(ldap.OPT_X_TLS_DEMAND, True)\n        else:\n            ldap.set_option(ldap.OPT_X_TLS_DEMAND, False)\n        # set the CA file if declared and if necessary\n        if self.ca and self.checkcert == 'on':\n            # check if the CA file actually exists\n            if os.path.isfile(self.ca):\n                ldap.set_option(ldap.OPT_X_TLS_CACERTFILE, self.ca)\n            else:\n                raise CaFileDontExist(self.ca)\n        if self.checkcert == 'off':\n            # this is dark magic\n            # remove any of these two lines and it doesn't work\n            ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_NEVER)\n            ldap_client.set_option(\n                ldap.OPT_X_TLS_REQUIRE_CERT,\n                ldap.OPT_X_TLS_NEVER\n                )\n        else:\n            # this is even darker magic\n            ldap_client.set_option(\n                ldap.OPT_X_TLS_REQUIRE_CERT,\n                ldap.OPT_X_TLS_DEMAND\n                )\n            # it doesn't make sense to set it to never\n            # (== don't check certifate)\n            # but it only works with this option...\n            # ... and it checks the certificat\n            # (I've lost my sanity over this)\n            ldap.set_option(\n                ldap.OPT_X_TLS_REQUIRE_CERT,\n                ldap.OPT_X_TLS_NEVER\n                )\n        if self.starttls == 'on':\n            try:\n                ldap_client.start_tls_s()\n            except Exception as e:\n                self._exception_handler(e)\n        return ldap_client"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bind(self):\n        ldap_client = self._connect()\n        try:\n            ldap_client.simple_bind_s(self.binddn, self.bindpassword)\n        except Exception as e:\n            ldap_client.unbind_s()\n            self._exception_handler(e)\n        return ldap_client", "response": "bind to the ldap with the technical account"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_user(self, username, attrs=ALL_ATTRS):\n\n        username = ldap.filter.escape_filter_chars(username)\n        user_filter = self.user_filter_tmpl % {\n            'username': self._uni(username)\n        }\n        r = self._search(self._byte_p2(user_filter), attrs, self.userdn)\n\n        if len(r) == 0:\n            return None\n\n        # if NO_ATTR, only return the DN\n        if attrs == NO_ATTR:\n            dn_entry = r[0][0]\n        # in other cases, return everything (dn + attributes)\n        else:\n            dn_entry = r[0]\n        return dn_entry", "response": "Get a user from the ldap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef auth(self, username, password):\n\n        binddn = self._get_user(self._byte_p2(username), NO_ATTR)\n        if binddn is not None:\n            ldap_client = self._connect()\n            try:\n                ldap_client.simple_bind_s(\n                        self._byte_p2(binddn),\n                        self._byte_p2(password)\n                        )\n            except ldap.INVALID_CREDENTIALS:\n                ldap_client.unbind_s()\n                return False\n            ldap_client.unbind_s()\n            return True\n        else:\n            return False", "response": "Authentication of a user"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_user(self, attrs):\n        ldap_client = self._bind()\n        # encoding crap\n        attrs_srt = self.attrs_pretreatment(attrs)\n\n        attrs_srt[self._byte_p2('objectClass')] = self.objectclasses\n        # construct is DN\n        dn = \\\n            self._byte_p2(self.dn_user_attr) + \\\n            self._byte_p2('=') + \\\n            self._byte_p2(ldap.dn.escape_dn_chars(\n                        attrs[self.dn_user_attr]\n                    )\n                ) + \\\n            self._byte_p2(',') + \\\n            self._byte_p2(self.userdn)\n        # gen the ldif first add_s and add the user\n        ldif = modlist.addModlist(attrs_srt)\n        try:\n            ldap_client.add_s(dn, ldif)\n        except ldap.ALREADY_EXISTS as e:\n            raise UserAlreadyExists(attrs[self.key], self.backend_name)\n        except Exception as e:\n            ldap_client.unbind_s()\n            self._exception_handler(e)\n        ldap_client.unbind_s()", "response": "add a user to the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef del_from_groups(self, username, groups):\n        # it follows the same logic than add_to_groups\n        # but with MOD_DELETE\n        ldap_client = self._bind()\n        tmp = self._get_user(self._byte_p2(username), ALL_ATTRS)\n        if tmp is None:\n            raise UserDoesntExist(username, self.backend_name)\n        dn = tmp[0]\n        attrs = tmp[1]\n        attrs['dn'] = dn\n        self._normalize_group_attrs(attrs)\n        dn = self._byte_p2(tmp[0])\n        for group in groups:\n            group = self._byte_p2(group)\n            for attr in self.group_attrs:\n                content = self._byte_p2(self.group_attrs[attr] % attrs)\n                ldif = [(ldap.MOD_DELETE, attr, self._byte_p3(content))]\n                try:\n                    ldap_client.modify_s(group, ldif)\n                except ldap.NO_SUCH_ATTRIBUTE as e:\n                    self._logger(\n                        severity=logging.INFO,\n                        msg=\"%(backend)s: user '%(user)s'\"\n                        \" wasn't member of group '%(group)s'\"\n                        \" (attribute '%(attr)s')\" % {\n                            'user': username,\n                            'group': self._uni(group),\n                            'attr': attr,\n                            'backend': self.backend_name\n                            }\n                    )\n                except Exception as e:\n                    ldap_client.unbind_s()\n                    self._exception_handler(e)\n        ldap_client.unbind_s()", "response": "Delete user from groups"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches users for the user with the given searchstring.", "response": "def search(self, searchstring):\n        \"\"\"Search users\"\"\"\n        # escape special char to avoid injection\n        searchstring = ldap.filter.escape_filter_chars(\n            self._byte_p2(searchstring)\n        )\n        # fill the search string template\n        searchfilter = self.search_filter_tmpl % {\n            'searchstring': searchstring\n        }\n\n        ret = {}\n        # search an process the result a little\n        for u in self._search(searchfilter, DISPLAYED_ATTRS, self.userdn):\n            attrs = {}\n            attrs_tmp = u[1]\n            for attr in attrs_tmp:\n                value_tmp = attrs_tmp[attr]\n                if len(value_tmp) == 1:\n                    attrs[attr] = value_tmp[0]\n                else:\n                    attrs[attr] = value_tmp\n\n            if self.key in attrs:\n                ret[attrs[self.key]] = attrs\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all groups of a user", "response": "def get_groups(self, username):\n        \"\"\"Get all groups of a user\"\"\"\n        username = ldap.filter.escape_filter_chars(self._byte_p2(username))\n        userdn = self._get_user(username, NO_ATTR)\n\n        searchfilter = self.group_filter_tmpl % {\n            'userdn': userdn,\n            'username': username\n        }\n\n        groups = self._search(searchfilter, NO_ATTR, self.groupdn)\n        ret = []\n        for entry in groups:\n            ret.append(self._uni(entry[0]))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _merge_groups(self, backends_list):\n        ret = {}\n        for backends in backends_list:\n            for b in backends:\n                if b not in ret:\n                    ret[b] = set([])\n                for group in backends[b]:\n                    ret[b].add(group)\n        for b in ret:\n            ret[b] = list(ret[b])\n            ret[b].sort()\n        return ret", "response": "merge a list of backend groups into a single backend"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _flatten(self, roles=None, groups=None):\n        if roles is None:\n            roles_in = copy.deepcopy(self.roles_raw)\n        else:\n            roles_in = roles\n        for roleid in roles_in:\n            role = roles_in[roleid]\n            if groups is not None:\n                role['backends_groups'] = self._merge_groups(\n                    [role['backends_groups'], groups],\n                    )\n            if 'subroles' in role:\n                self._flatten(\n                    role['subroles'],\n                    role['backends_groups'],\n                    )\n                del role['subroles']\n\n            self.flatten[roleid] = role", "response": "flatten a ( semi ) nest roles structure"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntesting if roleid1 is contained inside roleid2", "response": "def _is_parent(self, roleid1, roleid2):\n        \"\"\"Test if roleid1 is contained inside roleid2\"\"\"\n\n        role2 = copy.deepcopy(self.flatten[roleid2])\n        role1 = copy.deepcopy(self.flatten[roleid1])\n\n        if role1 == role2:\n            return False\n\n        # Check if role1 is contained by role2\n        for b1 in role1['backends_groups']:\n            if b1 not in role2['backends_groups']:\n                return False\n            for group in role1['backends_groups'][b1]:\n                if group not in role2['backends_groups'][b1]:\n                    return False\n\n        # If role2 is inside role1, roles are equal, throw exception\n        for b2 in role2['backends_groups']:\n            if b2 not in role1['backends_groups']:\n                return True\n            for group in role2['backends_groups'][b2]:\n                if group not in role1['backends_groups'][b2]:\n                    return True\n        raise DumplicateRoleContent(roleid1, roleid2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnesting the roles creates the hierarchy", "response": "def _nest(self):\n        \"\"\"nests the roles (creates roles hierarchy)\"\"\"\n        self._flatten()\n        parent_roles = {}\n        for roleid in self.flatten:\n            role = copy.deepcopy(self.flatten[roleid])\n\n            # Display name is mandatory\n            if 'display_name' not in role:\n                raise MissingKey('display_name', role, self.role_file)\n\n            if 'description' not in role:\n                raise MissingKey('description', role, self.role_file)\n\n            # Backend is mandatory\n            if 'backends_groups' not in role:\n                raise MissingKey('backends_groups', role, self.role_file)\n\n            # Create the list of backends\n            for backend in role['backends_groups']:\n                self.backends.add(backend)\n\n            if roleid not in self.graph:\n                self.graph[roleid] = {\n                    'parent_roles': set([]),\n                    'sub_roles': set([])\n                    }\n\n        # Create the nested groups\n        for roleid in self.flatten:\n            role = copy.deepcopy(self.flatten[roleid])\n            # create reverse groups 2 roles\n            for b in role['backends_groups']:\n                for g in role['backends_groups'][b]:\n                    if b not in self.group2roles:\n                        self.group2roles[b] = {}\n                    if g not in self.group2roles[b]:\n                        self.group2roles[b][g] = set([])\n                    self.group2roles[b][g].add(roleid)\n\n            parent_roles[roleid] = []\n            for roleid2 in self.flatten:\n                role2 = copy.deepcopy(self.flatten[roleid2])\n                if self._is_parent(roleid, roleid2):\n                    parent_roles[roleid].append(roleid2)\n                    self.graph[roleid2]['parent_roles'].add(roleid)\n                    self.graph[roleid]['sub_roles'].add(roleid2)\n\n        for r in parent_roles:\n            for p in parent_roles[r]:\n                for p2 in parent_roles[r]:\n                    if p != p2 and p in parent_roles[p2]:\n                        parent_roles[r].remove(p)\n\n        def nest(p):\n            ret = copy.deepcopy(self.flatten[p])\n            ret['subroles'] = {}\n            if len(parent_roles[p]) == 0:\n                return ret\n            else:\n                for i in parent_roles[p]:\n                    sub = nest(i)\n                    ret['subroles'][i] = sub\n                return ret\n\n        for p in parent_roles.keys():\n            if p in parent_roles:\n                self.roles[p] = nest(p)\n\n        for roleid in self.roles:\n            role = self.roles[roleid]\n            # Create the list of roles which are ldapcherry admins\n            if 'LC_admins' in role and role['LC_admins']:\n                self.admin_roles.append(roleid)\n                self._set_admin(role)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget groups to remove from list of roles to remove and current roles", "response": "def get_groups_to_remove(self, current_roles, roles_to_remove):\n        \"\"\"get groups to remove from list of\n        roles to remove and current roles\n        \"\"\"\n        current_roles = set(current_roles)\n\n        ret = {}\n        roles_to_remove = set(roles_to_remove)\n        tmp = set([])\n        # get sub roles of the role to remove that the user belongs to\n        # if we remove a role, there is no reason to keep the sub roles\n        for r in roles_to_remove:\n            for sr in self._get_subroles(r):\n                if sr not in roles_to_remove and sr in current_roles:\n                    tmp.add(sr)\n\n        roles_to_remove = roles_to_remove.union(tmp)\n        roles = current_roles.difference(set(roles_to_remove))\n        groups_roles = self._get_groups(roles)\n        groups_roles_to_remove = self._get_groups(roles_to_remove)\n\n        # if groups belongs to roles the user keeps, don't remove it\n        for b in groups_roles_to_remove:\n            if b in groups_roles:\n                groups_roles_to_remove[b] = \\\n                    groups_roles_to_remove[b].difference(groups_roles[b])\n        return groups_roles_to_remove"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_roles(self, groups):\n        roles = set([])\n        parentroles = set([])\n        notroles = set([])\n        tmp = set([])\n        usedgroups = {}\n        unusedgroups = {}\n        ret = {}\n        # determine roles membership\n        for role in self.roles:\n            if self._check_member(\n                    role, groups, notroles,\n                    tmp, parentroles, usedgroups):\n                roles.add(role)\n        # determine standalone groups not matching any roles\n        for b in groups:\n            for g in groups[b]:\n                if b not in usedgroups or g not in usedgroups[b]:\n                    if b not in unusedgroups:\n                        unusedgroups[b] = set([])\n                    unusedgroups[b].add(g)\n\n        ret['roles'] = roles\n        ret['unusedgroups'] = unusedgroups\n        return ret", "response": "get list of roles and list of standalone groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_display_name(self, role):\n        if role not in self.flatten:\n            raise MissingRole(role)\n        return self.flatten[role]['display_name']", "response": "get the display name of a role"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the list of groups from role", "response": "def get_groups(self, roles):\n        \"\"\"get the list of groups from role\"\"\"\n        ret = {}\n        for role in roles:\n            if role not in self.flatten:\n                raise MissingRole(role)\n            for b in self.flatten[role]['backends_groups']:\n                if b not in ret:\n                    ret[b] = []\n                ret[b] = ret[b] + self.flatten[role]['backends_groups'][b]\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining from a list of roles if is ldapcherry administrator", "response": "def is_admin(self, roles):\n        \"\"\"determine from a list of roles if is ldapcherry administrator\"\"\"\n        for r in roles:\n            if r in self.admin_roles:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the value of a configuration parameter key from configfile", "response": "def _get_param(self, section, key, config, default=None):\n        \"\"\" Get configuration parameter \"key\" from config\n        @str section: the section of the config file\n        @str key: the key to get\n        @dict config: the configuration (dictionnary)\n        @str default: the default value if parameter \"key\" is not present\n        @rtype: str (value of config['key'] if present default otherwith\n        \"\"\"\n        if section in config and key in config[section]:\n            return config[section][key]\n        if default is not None:\n            return default\n        else:\n            raise MissingParameter(section, key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_groups(self, username):\n        ret = {}\n        for b in self.backends:\n            ret[b] = self.backends[b].get_groups(username)\n        cherrypy.log.error(\n            msg=\"user '\" + username + \"' groups: \" + str(ret),\n            severity=logging.DEBUG,\n        )\n        return ret", "response": "Get groups of a user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the roles of a user", "response": "def _get_roles(self, username):\n        \"\"\" Get roles of a user\n        @str username: name of the user\n        @rtype: dict, format { 'roles': [<list of roles>],\n            'unusedgroups': [<list of groups not matching roles>] }\n        \"\"\"\n        groups = self._get_groups(username)\n        user_roles = self.roles.get_roles(groups)\n        cherrypy.log.error(\n            msg=\"user '\" + username + \"' roles: \" + str(user_roles),\n            severity=logging.DEBUG,\n        )\n        return user_roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a user is an ldapcherry administrator", "response": "def _is_admin(self, username):\n        \"\"\" Check if a user is an ldapcherry administrator\n        @str username: name of the user\n        @rtype: bool, True if administrator, False otherwise\n        \"\"\"\n        roles = self._get_roles(username)\n        return self.roles.is_admin(roles['roles'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_backends(self):\n        backends = self.backends_params.keys()\n        for b in self.roles.get_backends():\n            if b not in backends:\n                raise MissingBackend(b, 'role')\n        for b in self.attributes.get_backends():\n            if b not in backends:\n                raise MissingBackend(b, 'attribute')", "response": "Check that every backend in roles and attributes are declared in main configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes all backends in ldapcherry - related modules", "response": "def _init_backends(self, config):\n        \"\"\" Init all backends\n        @dict: configuration of ldapcherry\n        \"\"\"\n        self.backends_params = {}\n        self.backends = {}\n        self.backends_display_names = {}\n        for entry in config['backends']:\n            # split at the first dot\n            backend, sep, param = entry.partition('.')\n            value = config['backends'][entry]\n            if backend not in self.backends_params:\n                self.backends_params[backend] = {}\n            self.backends_params[backend][param] = value\n        for backend in self.backends_params:\n            # get the backend display_name\n            try:\n                self.backends_display_names[backend] = \\\n                    self.backends_params[backend]['display_name']\n            except Exception as e:\n                self.backends_display_names[backend] = backend\n                self.backends_params[backend]['display_name'] = backend\n            params = self.backends_params[backend]\n            # Loading the backend module\n            try:\n                module = params['module']\n            except Exception as e:\n                raise MissingParameter('backends', backend + '.module')\n            try:\n                bc = __import__(module, globals(), locals(), ['Backend'], 0)\n            except Exception as e:\n                self._handle_exception(e)\n                raise BackendModuleLoadingFail(module)\n            try:\n                attrslist = self.attributes.get_backend_attributes(backend)\n                key = self.attributes.get_backend_key(backend)\n                self.backends[backend] = bc.Backend(\n                    params,\n                    cherrypy.log.error,\n                    backend,\n                    attrslist,\n                    key,\n                    )\n            except MissingParameter as e:\n                raise\n            except Exception as e:\n                self._handle_exception(e)\n                raise BackendModuleInitFail(module)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the LDAP Cherry Authentication object", "response": "def _init_auth(self, config):\n        \"\"\" Init authentication\n        @dict: configuration of ldapcherry\n        \"\"\"\n        self.auth_mode = self._get_param('auth', 'auth.mode', config)\n        if self.auth_mode in ['and', 'or', 'none']:\n            pass\n        elif self.auth_mode == 'custom':\n            # load custom auth module\n            auth_module = self._get_param('auth', 'auth.module', config)\n            auth = __import__(auth_module, globals(), locals(), ['Auth'], 0)\n            self.auth = auth.Auth(config['auth'], cherrypy.log)\n        else:\n            raise WrongParamValue(\n                'auth.mode',\n                'auth',\n                ['and', 'or', 'none', 'custom'],\n                )\n\n        self.roles_file = self._get_param('roles', 'roles.file', config)\n        cherrypy.log.error(\n            msg=\"loading roles file '%(file)s'\" % {'file': self.roles_file},\n            severity=logging.DEBUG\n        )\n        self.roles = Roles(self.roles_file)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_access_log(self, config, level):\n        access_handler = self._get_param(\n            'global',\n            'log.access_handler',\n            config,\n            'syslog',\n            )\n\n        # log format for syslog\n        syslog_formatter = logging.Formatter(\n            \"ldapcherry[%(process)d]: %(message)s\"\n            )\n\n        # replace access log handler by a syslog handler\n        if access_handler == 'syslog':\n            cherrypy.log.access_log.handlers = []\n            handler = logging.handlers.SysLogHandler(\n                address='/dev/log',\n                facility='user',\n                )\n            handler.setFormatter(syslog_formatter)\n            cherrypy.log.access_log.addHandler(handler)\n\n        # if stdout, open a logger on stdout\n        elif access_handler == 'stdout':\n            cherrypy.log.access_log.handlers = []\n            handler = logging.StreamHandler(sys.stdout)\n            formatter = logging.Formatter(\n                'ldapcherry.access - %(levelname)s - %(message)s'\n            )\n            handler.setFormatter(formatter)\n            cherrypy.log.access_log.addHandler(handler)\n        # if file, we keep the default\n        elif access_handler == 'file':\n            pass\n\n        # replace access log handler by a null handler\n        elif access_handler == 'none':\n            cherrypy.log.access_log.handlers = []\n            handler = logging.NullHandler()\n            cherrypy.log.access_log.addHandler(handler)\n\n        # set log level\n        cherrypy.log.access_log.setLevel(level)", "response": "Configure access logs based on the config and level"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring error logs based on the error_handler and level.", "response": "def _set_error_log(self, config, level, debug=False):\n        \"\"\" Configure error logs\n        \"\"\"\n        error_handler = self._get_param(\n            'global',\n            'log.error_handler',\n            config,\n            'syslog'\n            )\n\n        # log format for syslog\n        syslog_formatter = logging.Formatter(\n            \"ldapcherry[%(process)d]: %(message)s\",\n            )\n\n        # replacing the error handler by a syslog handler\n        if error_handler == 'syslog':\n            cherrypy.log.error_log.handlers = []\n\n            # redefining log.error method because cherrypy does weird\n            # things like adding the date inside the message\n            # or adding space even if context is empty\n            # (by the way, what's the use of \"context\"?)\n            cherrypy.log.error = syslog_error\n\n            handler = logging.handlers.SysLogHandler(\n                address='/dev/log',\n                facility='user',\n                )\n            handler.setFormatter(syslog_formatter)\n            cherrypy.log.error_log.addHandler(handler)\n\n        # if stdout, open a logger on stdout\n        elif error_handler == 'stdout':\n            cherrypy.log.error_log.handlers = []\n            handler = logging.StreamHandler(sys.stdout)\n            formatter = logging.Formatter(\n                'ldapcherry.app    - %(levelname)s - %(message)s'\n            )\n            handler.setFormatter(formatter)\n            cherrypy.log.error_log.addHandler(handler)\n        # if file, we keep the default\n        elif error_handler == 'file':\n            pass\n\n        # replacing the error handler by a null handler\n        elif error_handler == 'none':\n            cherrypy.log.error_log.handlers = []\n            handler = logging.NullHandler()\n            cherrypy.log.error_log.addHandler(handler)\n\n        # set log level\n        cherrypy.log.error_log.setLevel(level)\n\n        if debug:\n            cherrypy.log.error_log.handlers = []\n            handler = logging.StreamHandler(sys.stderr)\n            handler.setLevel(logging.DEBUG)\n            cherrypy.log.error_log.addHandler(handler)\n            cherrypy.log.error_log.setLevel(logging.DEBUG)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nauthenticate a user and password", "response": "def _auth(self, user, password):\n        \"\"\" authenticate a user\n        @str user: login of the user\n        @str password: password of the user\n        @rtype: dict, {'connected': <boolean, True if connection succeded>,\n            'isadmin': <True if user is ldapcherry administrator>}\n        \"\"\"\n        if self.auth_mode == 'none':\n            return {'connected': True, 'isadmin': True}\n        elif self.auth_mode == 'and':\n            ret1 = True\n            for b in self.backends:\n                ret1 = self.backends[b].auth(user, password) and ret1\n        elif self.auth_mode == 'or':\n            ret1 = False\n            for b in self.backends:\n                ret1 = self.backends[b].auth(user, password) or ret1\n        elif self.auth_mode == 'custom':\n            ret1 = self.auth.auth(user, password)\n        else:\n            raise Exception()\n        if not ret1:\n            return {'connected': False, 'isadmin': False}\n        else:\n            isadmin = self._is_admin(user)\n            return {'connected': True, 'isadmin': isadmin}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads templates from directory and load each template", "response": "def _load_templates(self, config):\n        \"\"\" load templates\n        @dict: configuration of ldapcherry\n        \"\"\"\n        # definition of the template directory\n        self.template_dir = self._get_param(\n            'resources',\n            'templates.dir',\n            config\n            )\n        cherrypy.log.error(\n            msg=\"loading templates from dir '%(dir)s'\" %\n                {'dir': self.template_dir},\n            severity=logging.DEBUG\n        )\n        # preload templates\n        self.temp_lookup = lookup.TemplateLookup(\n            directories=self.template_dir, input_encoding='utf-8',\n            default_filters=['unicode', 'h']\n            )\n        # load each template\n        self.temp = {}\n        for t in ('index.tmpl', 'error.tmpl', 'login.tmpl', '404.tmpl',\n                  'searchadmin.tmpl', 'searchuser.tmpl', 'adduser.tmpl',\n                  'roles.tmpl', 'groups.tmpl', 'form.tmpl', 'selfmodify.tmpl',\n                  'modify.tmpl', 'service_unavailable.tmpl'\n                  ):\n            self.temp[t] = self.temp_lookup.get_template(t)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reload(self, config=None, debug=False):\n        try:\n            # log configuration handling\n            # get log level\n            # (if not in configuration file, log level is set to debug)\n            level = get_loglevel(\n                self._get_param(\n                    'global',\n                    'log.level',\n                    config,\n                    'debug',\n                    )\n                )\n            # configure access log\n            self._set_access_log(config, level)\n            # configure error log\n            self._set_error_log(config, level, debug)\n\n            # load template files\n            self._load_templates(config)\n\n            # loading the auth configuration\n            self._init_auth(config)\n\n            # Loading the attributes\n            self.attributes_file = \\\n                self._get_param('attributes', 'attributes.file', config)\n            cherrypy.log.error(\n                msg=\"loading attributes file '%(file)s'\" %\n                    {'file': self.attributes_file},\n                severity=logging.DEBUG\n            )\n\n            self.notifications = {}\n\n            self.attributes = Attributes(self.attributes_file)\n\n            cherrypy.log.error(\n                msg=\"init directories backends\",\n                severity=logging.DEBUG\n            )\n            self._init_backends(config)\n            self._check_backends()\n\n            # loading the ppolicy\n            self._init_ppolicy(config)\n\n            # loading custom javascript\n            self._init_custom_js(config)\n\n            cherrypy.log.error(\n                msg=\"application started\",\n                severity=logging.INFO\n            )\n\n        except Exception as e:\n            self._handle_exception(e)\n            cherrypy.log.error(\n                msg=\"application failed to start\",\n                severity=logging.ERROR\n            )\n            exit(1)", "response": "reload the ldapcherry - related object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_notification(self, message):\n        sess = cherrypy.session\n        username = sess.get(SESSION_KEY, None)\n        if username not in self.notifications:\n            self.notifications[username] = []\n        self.notifications[username].append(message)", "response": "add a notification to the notification queue"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nempty and return list of message notification", "response": "def _empty_notification(self):\n        \"\"\" empty and return list of message notification\n        \"\"\"\n        sess = cherrypy.session\n        username = sess.get(SESSION_KEY, None)\n        if username in self.notifications:\n            ret = self.notifications[username]\n        else:\n            ret = []\n        self.notifications[username] = []\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge attributes from one backend search to the attributes dict", "response": "def _merge_user_attrs(self, attrs_backend, attrs_out, backend_name):\n        \"\"\" merge attributes from one backend search to the attributes dict\n        output\n\n        \"\"\"\n        for attr in attrs_backend:\n            if attr in self.attributes.backend_attributes[backend_name]:\n                attrid = self.attributes.backend_attributes[backend_name][attr]\n                if attrid not in attrs_out:\n                    attrs_out[attrid] = attrs_backend[attr]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching users and return a dict of user - id - > user - id - > attribute - > value", "response": "def _search(self, searchstring):\n        \"\"\" search users\n        @str searchstring: search string\n        @rtype: dict, {<user>: {<attr>: <value>}}\n        \"\"\"\n        if searchstring is None:\n            return {}\n        ret = {}\n        for b in self.backends:\n            tmp = self.backends[b].search(searchstring)\n            for u in tmp:\n                if u not in ret:\n                    ret[u] = {}\n                self._merge_user_attrs(tmp[u], ret[u], b)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets user attributes @str username: user to get @rtype: dict, {<attr>: <value>}", "response": "def _get_user(self, username):\n        \"\"\" get user attributes\n        @str username: user to get\n        @rtype: dict, {<attr>: <value>}\n        \"\"\"\n        if username is None:\n            return {}\n        ret = {}\n        for b in self.backends:\n            try:\n                tmp = self.backends[b].get_user(username)\n            except UserDoesntExist as e:\n                self._handle_exception(e)\n                tmp = {}\n            self._merge_user_attrs(tmp, ret, b)\n\n        cherrypy.log.error(\n            msg=\"user '\" + username + \"' attributes \" + str(ret),\n            severity=logging.DEBUG\n        )\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_params(self, params):\n        ret = {'attrs': {}, 'roles': {}, 'groups': {}}\n        for p in params:\n            # each form attributes is prefixed with type, ex: attr.uidNumber\n            # separate this prefix from the attribute name\n            p_type, sep, param = p.partition('.')\n            if p_type == 'attr':\n                ret['attrs'][param] = params[p]\n            elif p_type == 'role':\n                ret['roles'][param] = params[p]\n            elif p_type == 'group':\n                # with groups there is a second prefix\n                # corresponding to the backend\n                backend, sep, value = param.partition('.')\n                if backend not in ret['groups']:\n                    ret['groups'][backend] = []\n                ret['groups'][backend].append(value)\n        return ret", "response": "parse the parameters in the user attributes and roles and groups"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef selfmodify(self, **params):\n        self._check_auth(must_admin=False)\n        is_admin = self._check_admin()\n        sess = cherrypy.session\n        user = sess.get(SESSION_KEY, None)\n        if self.auth_mode == 'none':\n            return self.temp['error.tmpl'].render(\n                is_admin=is_admin,\n                alert='warning',\n                message=\"Not accessible with authentication disabled.\"\n                )\n        if cherrypy.request.method.upper() == 'POST':\n            params = self._parse_params(params)\n            self._selfmodify(params)\n            self._add_notification(\n                \"Self modification done\"\n            )\n        user_attrs = self._get_user(user)\n\n        try:\n            if user_attrs == {}:\n                return self.temp['error.tmpl'].render(\n                    is_admin=is_admin,\n                    alert='warning',\n                    message=\"User doesn't exist\"\n                    )\n\n            form = self.temp['form.tmpl'].render(\n                attributes=self.attributes.get_selfattributes(),\n                values=user_attrs,\n                modify=True,\n                autofill=False\n                )\n            return self.temp['selfmodify.tmpl'].render(\n                form=form,\n                is_admin=is_admin,\n                notifications=self._empty_notification(),\n                )\n        except NameError:\n            raise TemplateRenderError(\n                    exceptions.text_error_template().render()\n                    )", "response": "self modify user page"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(configfile=None, daemonize=False, environment=None,\n          fastcgi=False, scgi=False, pidfile=None,\n          cgi=False, debug=False):\n    \"\"\"Subscribe all engine plugins and start the engine.\"\"\"\n    sys.path = [''] + sys.path\n\n    # monkey patching cherrypy to disable config interpolation\n    def new_as_dict(self, raw=True, vars=None):\n        \"\"\"Convert an INI file to a dictionary\"\"\"\n        # Load INI file into a dict\n        result = {}\n        for section in self.sections():\n            if section not in result:\n                result[section] = {}\n            for option in self.options(section):\n                value = self.get(section, option, raw=raw, vars=vars)\n                try:\n                    value = cherrypy.lib.reprconf.unrepr(value)\n                except Exception:\n                    x = sys.exc_info()[1]\n                    msg = (\"Config error in section: %r, option: %r, \"\n                           \"value: %r. Config values must be valid Python.\" %\n                           (section, option, value))\n                    raise ValueError(msg, x.__class__.__name__, x.args)\n                result[section][option] = value\n        return result\n    cherrypy.lib.reprconf.Parser.as_dict = new_as_dict\n\n    instance = LdapCherry()\n    app = cherrypy.tree.mount(instance, '/', configfile)\n    cherrypy.config.update(configfile)\n    instance.reload(app.config, debug)\n\n    engine = cherrypy.engine\n\n    # Turn off autoreload\n    cherrypy.config.update({'engine.autoreload.on': False})\n\n    if environment is not None:\n        cherrypy.config.update({'environment': environment})\n\n    # Only daemonize if asked to.\n    if daemonize:\n        # Don't print anything to stdout/sterr.\n        cherrypy.config.update({'log.screen': False})\n        plugins.Daemonizer(engine).subscribe()\n\n    if pidfile:\n        plugins.PIDFile(engine, pidfile).subscribe()\n\n    if hasattr(engine, \"signal_handler\"):\n        engine.signal_handler.subscribe()\n    if hasattr(engine, \"console_control_handler\"):\n        engine.console_control_handler.subscribe()\n\n    if (fastcgi and (scgi or cgi)) or (scgi and cgi):\n        cherrypy.log.error(\"You may only specify one of the cgi, fastcgi, and \"\n                           \"scgi options.\", 'ENGINE')\n        sys.exit(1)\n    elif fastcgi or scgi or cgi:\n        # Turn off the default HTTP server (which is subscribed by default).\n        cherrypy.server.unsubscribe()\n\n        addr = cherrypy.server.bind_addr\n        if fastcgi:\n            f = servers.FlupFCGIServer(application=cherrypy.tree,\n                                       bindAddress=addr)\n        elif scgi:\n            f = servers.FlupSCGIServer(application=cherrypy.tree,\n                                       bindAddress=addr)\n        else:\n            f = servers.FlupCGIServer(application=cherrypy.tree,\n                                      bindAddress=addr)\n        s = servers.ServerAdapter(engine, httpserver=f, bind_addr=addr)\n        s.subscribe()\n\n    # Always start the engine; this will start all other services\n    try:\n        engine.start()\n    except Exception as e:\n        # Assume the error has been logged already via bus.log.\n        sys.exit(1)\n    else:\n        engine.block()", "response": "Start the LDAP Cherry engine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering the HTML output of the content of the current job.", "response": "def render_to_response(self, context, *args, **kwargs):\n        \"\"\"\n        Build dictionary of content\n        \"\"\"\n        preview_outputs = []\n        file_outputs = []\n        bast_ctx = context\n        added = set()\n\n        for output_group, output_files in context['job_info']['file_groups'].items():\n            for output_file_content in output_files:\n                if output_group:\n                    bast_ctx.update({\n                        'job_info': context['job_info'],\n                        'output_group': output_group,\n                        'output_file_content': output_file_content,\n                    })\n                    preview = render_to_string('wooey/preview/%s.html' % output_group, bast_ctx)\n                    preview_outputs.append(preview)\n\n        for file_info in context['job_info']['all_files']:\n            if file_info and file_info.get('name') not in added:\n                row_ctx = dict(\n                    file=file_info,\n                    **context\n                )\n                table_row = render_to_string('wooey/jobs/results/table_row.html', row_ctx)\n                file_outputs.append(table_row)\n                added.add(file_info.get('name'))\n\n        return JsonResponse({\n            'status': context['job_info']['status'].lower(),\n            'command': context['job_info']['job'].command,\n            'stdout': context['job_info']['job'].get_stdout(),\n            'stderr': context['job_info']['job'].get_stderr(),\n            'preview_outputs_html': preview_outputs,\n            'file_outputs_html': file_outputs,\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toggle_favorite(request):\n    if not request.is_ajax():\n        return HttpResponseForbidden()\n\n    try:\n        app, model, pk = request.POST['app'], request.POST['model'], int(request.POST['pk'])\n\n    except ValueError:\n        return HttpResponseBadRequest()\n\n    try:\n        ctype = ContentType.objects.get(app_label=app, model=model)\n        obj = ctype.get_object_for_this_type(id=pk)\n\n    except Favorite.DoesNotExist:\n        return HttpResponseNotFound()\n\n    try:\n        fave = Favorite.objects.get(content_type=ctype, object_id=obj.id, user=request.user)\n\n    except Favorite.DoesNotExist:\n        # Does not exist, so create it\n        fave = Favorite(content_object=obj, user=request.user)\n        fave.save()\n        is_favorite = True\n\n    else:\n        # Exists, so delete it\n        fave.delete()\n        is_favorite = False\n\n    # Return the current total number for UI updates\n    favorites_count = Favorite.objects.filter(content_type=ctype, user=request.user).count()\n\n    return JsonResponse({\n        'app': app,\n        'model': model,\n        'pk': pk,\n        'is_favorite': is_favorite,\n        'count': favorites_count,\n    })", "response": "Add or remove an object to the user s favorites. Checks for existance and adds if not else removes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a app - model - id string for a given object", "response": "def app_model_id(obj):\n    \"\"\"\n    Returns a app-model-id string for a given object\n    :param obj:\n    :return:\n    \"\"\"\n    ct = ContentType.objects.get_for_model(obj)\n\n    return '%s-%s-%s' % (ct.app_label, ct.model, obj.id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_field(param, initial=None):\n        form_field = param.form_field\n        widget_data_dict = {}\n        appender_data_dict = {}\n        widget_init = {}\n        WOOEY_CHOICE_LIMIT = 'data-wooey-choice-limit'\n        choices = json.loads(param.choices)\n        field_kwargs = {\n            'label': param.script_param.replace('_', ' ').title(),\n            'required': param.required,\n            'help_text': param.param_help,\n        }\n        multiple_choices = param.multiple_choice\n        choice_limit = param.max_choices\n        if initial is None and param.default is not None:\n            initial = param.default\n        if choices:\n            form_field = 'MultipleChoiceField' if multiple_choices else 'ChoiceField'\n            base_choices = [(None, '----')] if not param.required and not multiple_choices else []\n            field_kwargs['choices'] = base_choices+[(str(i), str(i).title()) for i in choices]\n\n        custom_widget = param.custom_widget\n        if custom_widget:\n            if custom_widget.widget_class:\n                widget_class = import_string(custom_widget.widget_class)\n                field_kwargs['widget'] = widget_class\n            widget_init['attrs'] = custom_widget.widget_attributes\n\n        if form_field == 'FileField':\n            if param.is_output:\n                form_field = 'CharField'\n                if initial:\n                    if not isinstance(initial, (list, tuple)):\n                        initial = [initial]\n                    initial = [os.path.split(i.name)[1] for i in initial]\n            elif initial is not None and list(filter(None, initial)):  # for python3, we need to evaluate the filter object\n                if isinstance(initial, (list, tuple)):\n                    _initial = []\n                    for value in initial:\n                        if not hasattr(value, 'path'):\n                            with utils.get_storage_object(value, close=False) as so:\n                                _initial.append(so)\n                        else:\n                            _initial.append(value)\n                    initial = _initial\n                else:\n                    if not hasattr(initial, 'path'):\n                        with utils.get_storage_object(initial, close=False) as so:\n                            initial = so\n                    else:\n                        initial = initial\n                if not field_kwargs.get('widget'):\n                    field_kwargs['widget'] = forms.ClearableFileInput\n        if not multiple_choices and isinstance(initial, list):\n            initial = initial[0]\n        field_kwargs['initial'] = initial\n        field = getattr(forms, form_field)\n\n        if 'widget' in field_kwargs:\n            field_kwargs['widget'] = field_kwargs['widget'](**widget_init)\n\n        field = field(**field_kwargs)\n\n        if form_field != 'MultipleChoiceField' and multiple_choices:\n            field.widget.render = mutli_render(field.widget.render, appender_data_dict=appender_data_dict)\n            field.widget.value_from_datadict = multi_value_from_datadict(field.widget.value_from_datadict)\n            field.clean = multi_value_clean(field.clean)\n            if choice_limit > 0:\n                appender_data_dict[WOOEY_CHOICE_LIMIT] = choice_limit\n        elif multiple_choices and choice_limit>0:\n            widget_data_dict[WOOEY_CHOICE_LIMIT] = choice_limit\n\n        field.widget.attrs.update(widget_data_dict)\n        return field", "response": "Returns a Wooey field class based on the parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize a query string into a list of keywords discarding spaces and grouping quoted words together.", "response": "def normalize_query(query_string,\n                    findterms=re.compile(r'\"([^\"]+)\"|(\\S+)').findall,\n                    normspace=re.compile(r'\\s{2,}').sub):\n    \"\"\"\n    Split the query string into individual keywords, discarding spaces\n    and grouping quoted words together.\n\n    >>> normalize_query('  some random  words \"with   quotes  \" and   spaces')\n    ['some', 'random', 'words', 'with quotes', 'and', 'spaces']\n    \"\"\"\n\n    return [normspace(' ', (t[0] or t[1]).strip()) for t in findterms(query_string)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving model and set initial state.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        Saves model and set initial state.\n        \"\"\"\n        super(ModelDiffMixin, self).save(*args, **kwargs)\n        self.__initial = self._dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_text_assets(self, text_image_file, text_file):\n\n        text_pixels = self.load_image(text_image_file, False)\n        with open(text_file, 'r') as f:\n            loaded_text = f.read()\n        self._text_dict = {}\n        for index, s in enumerate(loaded_text):\n            start = index * 40\n            end = start + 40\n            char = text_pixels[start:end]\n            self._text_dict[s] = char", "response": "Internal. Loads the text from the image file and sets the _text_dict attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_fb_device(self):\n\n        device = None\n\n        for fb in glob.glob('/sys/class/graphics/fb*'):\n            name_file = os.path.join(fb, 'name')\n            if os.path.isfile(name_file):\n                with open(name_file, 'r') as f:\n                    name = f.read()\n                if name.strip() == self.SENSE_HAT_FB_NAME:\n                    fb_device = fb.replace(os.path.dirname(fb), '/dev')\n                    if os.path.exists(fb_device):\n                        device = fb_device\n                        break\n\n        return device", "response": "Internal. Finds the correct frame buffer device for the sense HAT\n        and returns its / dev name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_rotation(self, r=0, redraw=True):\n\n        if r in self._pix_map.keys():\n            if redraw:\n                pixel_list = self.get_pixels()\n            self._rotation = r\n            if redraw:\n                self.set_pixels(pixel_list)\n        else:\n            raise ValueError('Rotation must be 0, 90, 180 or 270 degrees')", "response": "Sets the LED matrix rotation for viewing. 0 is with the Pi HDMI port facing downwards 0 is with the Pi HDMI port facing upsideways down or sideways 0 is with the Pi HDMI port facing upsideways down or sideways 0 is with the Pi HDMI port facing downwards 0 is with the Pi HDMI port facing upwards"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flip_h(self, redraw=True):\n\n        pixel_list = self.get_pixels()\n        flipped = []\n        for i in range(8):\n            offset = i * 8\n            flipped.extend(reversed(pixel_list[offset:offset + 8]))\n        if redraw:\n            self.set_pixels(flipped)\n        return flipped", "response": "Flip LED matrix horizontal"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the pixels in the LED matrix.", "response": "def set_pixels(self, pixel_list):\n        \"\"\"\n        Accepts a list containing 64 smaller lists of [R,G,B] pixels and\n        updates the LED matrix. R,G,B elements must intergers between 0\n        and 255\n        \"\"\"\n\n        if len(pixel_list) != 64:\n            raise ValueError('Pixel lists must have 64 elements')\n\n        for index, pix in enumerate(pixel_list):\n            if len(pix) != 3:\n                raise ValueError('Pixel at index %d is invalid. Pixels must contain 3 elements: Red, Green and Blue' % index)\n\n            for element in pix:\n                if element > 255 or element < 0:\n                    raise ValueError('Pixel at index %d is invalid. Pixel elements must be between 0 and 255' % index)\n\n        with open(self._fb_device, 'wb') as f:\n            map = self._pix_map[self._rotation]\n            for index, pix in enumerate(pixel_list):\n                # Two bytes per pixel in fb memory, 16 bit RGB565\n                f.seek(map[index // 8][index % 8] * 2)  # row, column\n                f.write(self._pack_bin(pix))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of R G B pixels representing what is currently displayed on the LED matrix.", "response": "def get_pixels(self):\n        \"\"\"\n        Returns a list containing 64 smaller lists of [R,G,B] pixels\n        representing what is currently displayed on the LED matrix\n        \"\"\"\n\n        pixel_list = []\n        with open(self._fb_device, 'rb') as f:\n            map = self._pix_map[self._rotation]\n            for row in range(8):\n                for col in range(8):\n                    # Two bytes per pixel in fb memory, 16 bit RGB565\n                    f.seek(map[row][col] * 2)  # row, column\n                    pixel_list.append(self._unpack_bin(f.read(2)))\n        return pixel_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_pixel(self, x, y, *args):\n\n        pixel_error = 'Pixel arguments must be given as (r, g, b) or r, g, b'\n\n        if len(args) == 1:\n            pixel = args[0]\n            if len(pixel) != 3:\n                raise ValueError(pixel_error)\n        elif len(args) == 3:\n            pixel = args\n        else:\n            raise ValueError(pixel_error)\n\n        if x > 7 or x < 0:\n            raise ValueError('X position must be between 0 and 7')\n\n        if y > 7 or y < 0:\n            raise ValueError('Y position must be between 0 and 7')\n\n        for element in pixel:\n            if element > 255 or element < 0:\n                raise ValueError('Pixel elements must be between 0 and 255')\n\n        with open(self._fb_device, 'wb') as f:\n            map = self._pix_map[self._rotation]\n            # Two bytes per pixel in fb memory, 16 bit RGB565\n            f.seek(map[y][x] * 2)  # row, column\n            f.write(self._pack_bin(pixel))", "response": "Updates the LED matrix with the specified pixel."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of R G B values representing the pixel specified by x y on the LED matrix.", "response": "def get_pixel(self, x, y):\n        \"\"\"\n        Returns a list of [R,G,B] representing the pixel specified by x and y\n        on the LED matrix. Top left = 0,0 Bottom right = 7,7\n        \"\"\"\n\n        if x > 7 or x < 0:\n            raise ValueError('X position must be between 0 and 7')\n\n        if y > 7 or y < 0:\n            raise ValueError('Y position must be between 0 and 7')\n\n        pix = None\n\n        with open(self._fb_device, 'rb') as f:\n            map = self._pix_map[self._rotation]\n            # Two bytes per pixel in fb memory, 16 bit RGB565\n            f.seek(map[y][x] * 2)  # row, column\n            pix = self._unpack_bin(f.read(2))\n\n        return pix"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload an 8 x 8 image file and updates the LED matrix with the image data.", "response": "def load_image(self, file_path, redraw=True):\n        \"\"\"\n        Accepts a path to an 8 x 8 image file and updates the LED matrix with\n        the image\n        \"\"\"\n\n        if not os.path.exists(file_path):\n            raise IOError('%s not found' % file_path)\n\n        img = Image.open(file_path).convert('RGB')\n        pixel_list = list(map(list, img.getdata()))\n\n        if redraw:\n            self.set_pixels(pixel_list)\n\n        return pixel_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclears the LED matrix with a single colour.", "response": "def clear(self, *args):\n        \"\"\"\n        Clears the LED matrix with a single colour, default is black / off\n\n        e.g. ap.clear()\n        or\n        ap.clear(r, g, b)\n        or\n        colour = (r, g, b)\n        ap.clear(colour)\n        \"\"\"\n\n        black = (0, 0, 0)  # default\n\n        if len(args) == 0:\n            colour = black\n        elif len(args) == 1:\n            colour = args[0]\n        elif len(args) == 3:\n            colour = args\n        else:\n            raise ValueError('Pixel arguments must be given as (r, g, b) or r, g, b')\n\n        self.set_pixels([colour] * 64)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying a string of text across the LED matrix.", "response": "def show_message(\n            self,\n            text_string,\n            scroll_speed=.1,\n            text_colour=[255, 255, 255],\n            back_colour=[0, 0, 0]\n        ):\n        \"\"\"\n        Scrolls a string of text across the LED matrix using the specified\n        speed and colours\n        \"\"\"\n\n        # We must rotate the pixel map left through 90 degrees when drawing\n        # text, see _load_text_assets\n        previous_rotation = self._rotation\n        self._rotation -= 90\n        if self._rotation < 0:\n            self._rotation = 270\n        dummy_colour = [None, None, None]\n        string_padding = [dummy_colour] * 64\n        letter_padding = [dummy_colour] * 8\n        # Build pixels from dictionary\n        scroll_pixels = []\n        scroll_pixels.extend(string_padding)\n        for s in text_string:\n            scroll_pixels.extend(self._trim_whitespace(self._get_char_pixels(s)))\n            scroll_pixels.extend(letter_padding)\n        scroll_pixels.extend(string_padding)\n        # Recolour pixels as necessary\n        coloured_pixels = [\n            text_colour if pixel == [255, 255, 255] else back_colour\n            for pixel in scroll_pixels\n        ]\n        # Shift right by 8 pixels per frame to scroll\n        scroll_length = len(coloured_pixels) // 8\n        for i in range(scroll_length - 8):\n            start = i * 8\n            end = start + 64\n            self.set_pixels(coloured_pixels[start:end])\n            time.sleep(scroll_speed)\n        self._rotation = previous_rotation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a single text character on the LED matrix using the specified colours.", "response": "def show_letter(\n            self,\n            s,\n            text_colour=[255, 255, 255],\n            back_colour=[0, 0, 0]\n        ):\n        \"\"\"\n        Displays a single text character on the LED matrix using the specified\n        colours\n        \"\"\"\n\n        if len(s) > 1:\n            raise ValueError('Only one character may be passed into this method')\n        # We must rotate the pixel map left through 90 degrees when drawing\n        # text, see _load_text_assets\n        previous_rotation = self._rotation\n        self._rotation -= 90\n        if self._rotation < 0:\n            self._rotation = 270\n        dummy_colour = [None, None, None]\n        pixel_list = [dummy_colour] * 8\n        pixel_list.extend(self._get_char_pixels(s))\n        pixel_list.extend([dummy_colour] * 16)\n        coloured_pixels = [\n            text_colour if pixel == [255, 255, 255] else back_colour\n            for pixel in pixel_list\n        ]\n        self.set_pixels(coloured_pixels)\n        self._rotation = previous_rotation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset the LED matrix gamma correction to default", "response": "def gamma_reset(self):\n        \"\"\"\n        Resets the LED matrix gamma correction to default\n        \"\"\"\n\n        with open(self._fb_device) as f:\n            fcntl.ioctl(f, self.SENSE_HAT_FB_FBIORESET_GAMMA, self.SENSE_HAT_FB_GAMMA_DEFAULT)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _init_humidity(self):\n\n        if not self._humidity_init:\n            self._humidity_init = self._humidity.humidityInit()\n            if not self._humidity_init:\n                raise OSError('Humidity Init Failed')", "response": "Internal. Initialises the humidity sensor via RTIMU\n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_humidity(self):\n\n        self._init_humidity()  # Ensure humidity sensor is initialised\n        humidity = 0\n        data = self._humidity.humidityRead()\n        if (data[0]):  # Humidity valid\n            humidity = data[1]\n        return humidity", "response": "Returns the percentage of relative humidity of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_temperature_from_humidity(self):\n\n        self._init_humidity()  # Ensure humidity sensor is initialised\n        temp = 0\n        data = self._humidity.humidityRead()\n        if (data[2]):  # Temp valid\n            temp = data[3]\n        return temp", "response": "Returns the temperature in Celsius from the humidity sensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the temperature in Celsius from the pressure sensor", "response": "def get_temperature_from_pressure(self):\n        \"\"\"\n        Returns the temperature in Celsius from the pressure sensor\n        \"\"\"\n\n        self._init_pressure()  # Ensure pressure sensor is initialised\n        temp = 0\n        data = self._pressure.pressureRead()\n        if (data[2]):  # Temp valid\n            temp = data[3]\n        return temp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_pressure(self):\n\n        self._init_pressure()  # Ensure pressure sensor is initialised\n        pressure = 0\n        data = self._pressure.pressureRead()\n        if (data[0]):  # Pressure valid\n            pressure = data[1]\n        return pressure", "response": "Returns the pressure in Millibars\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the internal state of the IMU.", "response": "def set_imu_config(self, compass_enabled, gyro_enabled, accel_enabled):\n        \"\"\"\n        Enables and disables the gyroscope, accelerometer and/or magnetometer\n        input to the orientation functions\n        \"\"\"\n\n        # If the consuming code always calls this just before reading the IMU\n        # the IMU consistently fails to read. So prevent unnecessary calls to\n        # IMU config functions using state variables\n\n        self._init_imu()  # Ensure imu is initialised\n\n        if (not isinstance(compass_enabled, bool)\n        or not isinstance(gyro_enabled, bool)\n        or not isinstance(accel_enabled, bool)):\n            raise TypeError('All set_imu_config parameters must be of boolean type')\n\n        if self._compass_enabled != compass_enabled:\n            self._compass_enabled = compass_enabled\n            self._imu.setCompassEnable(self._compass_enabled)\n\n        if self._gyro_enabled != gyro_enabled:\n            self._gyro_enabled = gyro_enabled\n            self._imu.setGyroEnable(self._gyro_enabled)\n\n        if self._accel_enabled != accel_enabled:\n            self._accel_enabled = accel_enabled\n            self._imu.setAccelEnable(self._accel_enabled)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary object to represent the current orientation in degrees using the aircraft principal axes of pitch roll and yaw.", "response": "def get_orientation_radians(self):\n        \"\"\"\n        Returns a dictionary object to represent the current orientation in\n        radians using the aircraft principal axes of pitch, roll and yaw\n        \"\"\"\n\n        raw = self._get_raw_data('fusionPoseValid', 'fusionPose')\n\n        if raw is not None:\n            raw['roll'] = raw.pop('x')\n            raw['pitch'] = raw.pop('y')\n            raw['yaw'] = raw.pop('z')\n            self._last_orientation = raw\n\n        return deepcopy(self._last_orientation)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary object to represent the current orientation in degrees 0 to 360", "response": "def get_orientation_degrees(self):\n        \"\"\"\n        Returns a dictionary object to represent the current orientation\n        in degrees, 0 to 360, using the aircraft principal axes of\n        pitch, roll and yaw\n        \"\"\"\n\n        orientation = self.get_orientation_radians()\n        for key, val in orientation.items():\n            deg = math.degrees(val)  # Result is -180 to +180\n            orientation[key] = deg + 360 if deg < 0 else deg\n        return orientation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the direction of North from the magnetometer in degrees Returns None if no compass is set", "response": "def get_compass(self):\n        \"\"\"\n        Gets the direction of North from the magnetometer in degrees\n        \"\"\"\n\n        self.set_imu_config(True, False, False)\n        orientation = self.get_orientation_degrees()\n        if type(orientation) is dict and 'yaw' in orientation.keys():\n            return orientation['yaw']\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the compass raw data in uT", "response": "def get_compass_raw(self):\n        \"\"\"\n        Magnetometer x y z raw data in uT (micro teslas)\n        \"\"\"\n\n        raw = self._get_raw_data('compassValid', 'compass')\n\n        if raw is not None:\n            self._last_compass_raw = raw\n\n        return deepcopy(self._last_compass_raw)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the gyroscope x y z raw data in radians per second", "response": "def get_gyroscope_raw(self):\n        \"\"\"\n        Gyroscope x y z raw data in radians per second\n        \"\"\"\n\n        raw = self._get_raw_data('gyroValid', 'gyro')\n\n        if raw is not None:\n            self._last_gyro_raw = raw\n\n        return deepcopy(self._last_gyro_raw)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the accelerometer raw data in Gs", "response": "def get_accelerometer_raw(self):\n        \"\"\"\n        Accelerometer x y z raw data in Gs\n        \"\"\"\n\n        raw = self._get_raw_data('accelValid', 'accel')\n\n        if raw is not None:\n            self._last_accel_raw = raw\n\n        return deepcopy(self._last_accel_raw)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _stick_device(self):\n        for evdev in glob.glob('/sys/class/input/event*'):\n            try:\n                with io.open(os.path.join(evdev, 'device', 'name'), 'r') as f:\n                    if f.read().strip() == self.SENSE_HAT_EVDEV_NAME:\n                        return os.path.join('/dev', 'input', os.path.basename(evdev))\n            except IOError as e:\n                if e.errno != errno.ENOENT:\n                    raise\n        raise RuntimeError('unable to locate SenseHAT joystick device')", "response": "Returns the path to the file that represents the SenseHAT joystick device."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a single event from the joystick blocking until one is available. Returns None if a non - key event was read or an InputEvent tuple describing the event otherwise.", "response": "def _read(self):\n        \"\"\"\n        Reads a single event from the joystick, blocking until one is\n        available. Returns `None` if a non-key event was read, or an\n        `InputEvent` tuple describing the event otherwise.\n        \"\"\"\n        event = self._stick_file.read(self.EVENT_SIZE)\n        (tv_sec, tv_usec, type, code, value) = struct.unpack(self.EVENT_FORMAT, event)\n        if type == self.EV_KEY:\n            return InputEvent(\n                timestamp=tv_sec + (tv_usec / 1000000),\n                direction={\n                    self.KEY_UP:    DIRECTION_UP,\n                    self.KEY_DOWN:  DIRECTION_DOWN,\n                    self.KEY_LEFT:  DIRECTION_LEFT,\n                    self.KEY_RIGHT: DIRECTION_RIGHT,\n                    self.KEY_ENTER: DIRECTION_MIDDLE,\n                    }[code],\n                action={\n                    self.STATE_PRESS:   ACTION_PRESSED,\n                    self.STATE_RELEASE: ACTION_RELEASED,\n                    self.STATE_HOLD:    ACTION_HELD,\n                    }[value])\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _wait(self, timeout=None):\n        r, w, x = select.select([self._stick_file], [], [], timeout)\n        return bool(r)", "response": "Waits for an event to be available from the\n        joystick. Returns True if the event is available and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting until a joystick event becomes available.", "response": "def wait_for_event(self, emptybuffer=False):\n        \"\"\"\n        Waits until a joystick event becomes available.  Returns the event, as\n        an `InputEvent` tuple.\n\n        If *emptybuffer* is `True` (it defaults to `False`), any pending\n        events will be thrown away first. This is most useful if you are only\n        interested in \"pressed\" events.\n        \"\"\"\n        if emptybuffer:\n            while self._wait(0):\n                self._read()\n        while self._wait():\n            event = self._read()\n            if event:\n                return event"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all joystick events that have occurred since the last call to get_events.", "response": "def get_events(self):\n        \"\"\"\n        Returns a list of all joystick events that have occurred since the last\n        call to `get_events`. The list contains events in the order that they\n        occurred. If no events have occurred in the intervening time, the\n        result is an empty list.\n        \"\"\"\n        result = []\n        while self._wait(0):\n            event = self._read()\n            if event:\n                result.append(event)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters an event handler.", "response": "def on(self, event, handler=None):\n        \"\"\"Register an event handler.\n\n        :param event: The event name. Can be ``'connect'``, ``'message'`` or\n                      ``'disconnect'``.\n        :param handler: The function that should be invoked to handle the\n                        event. When this parameter is not given, the method\n                        acts as a decorator for the handler function.\n\n        Example usage::\n\n            # as a decorator:\n            @eio.on('connect')\n            def connect_handler():\n                print('Connection request')\n\n            # as a method:\n            def message_handler(msg):\n                print('Received message: ', msg)\n                eio.send('response')\n            eio.on('message', message_handler)\n        \"\"\"\n        if event not in self.event_names:\n            raise ValueError('Invalid event')\n\n        def set_handler(handler):\n            self.handlers[event] = handler\n            return handler\n\n        if handler is None:\n            return set_handler\n        set_handler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_background_task(self, target, *args, **kwargs):\n        th = threading.Thread(target=target, args=args, kwargs=kwargs)\n        th.start()\n        return th", "response": "Start a background task."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_queue(self, *args, **kwargs):\n        q = queue.Queue(*args, **kwargs)\n        q.Empty = queue.Empty\n        return q", "response": "Create a queue object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _connect_websocket(self, url, headers, engineio_path):\n        if websocket is None:  # pragma: no cover\n            # not installed\n            self.logger.warning('websocket-client package not installed, only '\n                                'polling transport is available')\n            return False\n        websocket_url = self._get_engineio_url(url, engineio_path, 'websocket')\n        if self.sid:\n            self.logger.info(\n                'Attempting WebSocket upgrade to ' + websocket_url)\n            upgrade = True\n            websocket_url += '&sid=' + self.sid\n        else:\n            upgrade = False\n            self.base_url = websocket_url\n            self.logger.info(\n                'Attempting WebSocket connection to ' + websocket_url)\n\n        # get the cookies from the long-polling connection so that they can\n        # also be sent the the WebSocket route\n        cookies = None\n        if self.http:\n            cookies = '; '.join([\"{}={}\".format(cookie.name, cookie.value)\n                                 for cookie in self.http.cookies])\n        try:\n            ws = websocket.create_connection(\n                websocket_url + self._get_url_timestamp(), header=headers,\n                cookie=cookies)\n        except ConnectionError:\n            if upgrade:\n                self.logger.warning(\n                    'WebSocket upgrade failed: connection error')\n                return False\n            else:\n                raise exceptions.ConnectionError('Connection error')\n        if upgrade:\n            p = packet.Packet(packet.PING,\n                              data=six.text_type('probe')).encode()\n            try:\n                ws.send(p)\n            except Exception as e:  # pragma: no cover\n                self.logger.warning(\n                    'WebSocket upgrade failed: unexpected send exception: %s',\n                    str(e))\n                return False\n            try:\n                p = ws.recv()\n            except Exception as e:  # pragma: no cover\n                self.logger.warning(\n                    'WebSocket upgrade failed: unexpected recv exception: %s',\n                    str(e))\n                return False\n            pkt = packet.Packet(encoded_packet=p)\n            if pkt.packet_type != packet.PONG or pkt.data != 'probe':\n                self.logger.warning(\n                    'WebSocket upgrade failed: no PONG packet')\n                return False\n            p = packet.Packet(packet.UPGRADE).encode()\n            try:\n                ws.send(p)\n            except Exception as e:  # pragma: no cover\n                self.logger.warning(\n                    'WebSocket upgrade failed: unexpected send exception: %s',\n                    str(e))\n                return False\n            self.current_transport = 'websocket'\n            self.logger.info('WebSocket upgrade was successful')\n        else:\n            try:\n                p = ws.recv()\n            except Exception as e:  # pragma: no cover\n                raise exceptions.ConnectionError(\n                    'Unexpected recv exception: ' + str(e))\n            open_packet = packet.Packet(encoded_packet=p)\n            if open_packet.packet_type != packet.OPEN:\n                raise exceptions.ConnectionError('no OPEN packet')\n            self.logger.info(\n                'WebSocket connection accepted with ' + str(open_packet.data))\n            self.sid = open_packet.data['sid']\n            self.upgrades = open_packet.data['upgrades']\n            self.ping_interval = open_packet.data['pingInterval'] / 1000.0\n            self.ping_timeout = open_packet.data['pingTimeout'] / 1000.0\n            self.current_transport = 'websocket'\n\n            self.state = 'connected'\n            connected_clients.append(self)\n            self._trigger_event('connect', run_async=False)\n        self.ws = ws\n\n        # start background tasks associated with this client\n        self.ping_loop_task = self.start_background_task(self._ping_loop)\n        self.write_loop_task = self.start_background_task(self._write_loop)\n        self.read_loop_task = self.start_background_task(\n            self._read_loop_websocket)\n        return True", "response": "Establish or upgrade to a WebSocket connection with the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _trigger_event(self, event, *args, **kwargs):\n        run_async = kwargs.pop('run_async', False)\n        if event in self.handlers:\n            if run_async:\n                return self.start_background_task(self.handlers[event], *args)\n            else:\n                try:\n                    return self.handlers[event](*args)\n                except:\n                    self.logger.exception(event + ' handler error')", "response": "Invoke an event handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the Engine. IO connection URL.", "response": "def _get_engineio_url(self, url, engineio_path, transport):\n        \"\"\"Generate the Engine.IO connection URL.\"\"\"\n        engineio_path = engineio_path.strip('/')\n        parsed_url = urllib.parse.urlparse(url)\n\n        if transport == 'polling':\n            scheme = 'http'\n        elif transport == 'websocket':\n            scheme = 'ws'\n        else:  # pragma: no cover\n            raise ValueError('invalid transport')\n        if parsed_url.scheme in ['https', 'wss']:\n            scheme += 's'\n\n        return ('{scheme}://{netloc}/{path}/?{query}'\n                '{sep}transport={transport}&EIO=3').format(\n                    scheme=scheme, netloc=parsed_url.netloc,\n                    path=engineio_path, query=parsed_url.query,\n                    sep='&' if parsed_url.query else '',\n                    transport=transport)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ping_loop(self):\n        self.pong_received = True\n        self.ping_loop_event.clear()\n        while self.state == 'connected':\n            if not self.pong_received:\n                self.logger.info(\n                    'PONG response has not been received, aborting')\n                if self.ws:\n                    self.ws.close()\n                self.queue.put(None)\n                break\n            self.pong_received = False\n            self._send_packet(packet.Packet(packet.PING))\n            self.ping_loop_event.wait(timeout=self.ping_interval)\n        self.logger.info('Exiting ping task')", "response": "This method sends a PING to the server at the requested interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread packets by polling the Engine. IO server.", "response": "def _read_loop_polling(self):\n        \"\"\"Read packets by polling the Engine.IO server.\"\"\"\n        while self.state == 'connected':\n            self.logger.info(\n                'Sending polling GET request to ' + self.base_url)\n            r = self._send_request(\n                'GET', self.base_url + self._get_url_timestamp())\n            if r is None:\n                self.logger.warning(\n                    'Connection refused by the server, aborting')\n                self.queue.put(None)\n                break\n            if r.status_code != 200:\n                self.logger.warning('Unexpected status code %s in server '\n                                    'response, aborting', r.status_code)\n                self.queue.put(None)\n                break\n            try:\n                p = payload.Payload(encoded_payload=r.content)\n            except ValueError:\n                self.logger.warning(\n                    'Unexpected packet from server, aborting')\n                self.queue.put(None)\n                break\n            for pkt in p.packets:\n                self._receive_packet(pkt)\n\n        self.logger.info('Waiting for write loop task to end')\n        self.write_loop_task.join()\n        self.logger.info('Waiting for ping loop task to end')\n        self.ping_loop_event.set()\n        self.ping_loop_task.join()\n        if self.state == 'connected':\n            self._trigger_event('disconnect', run_async=False)\n            try:\n                connected_clients.remove(self)\n            except ValueError:  # pragma: no cover\n                pass\n            self._reset()\n        self.logger.info('Exiting read loop task')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread packets from the Engine. IO WebSocket connection.", "response": "def _read_loop_websocket(self):\n        \"\"\"Read packets from the Engine.IO WebSocket connection.\"\"\"\n        while self.state == 'connected':\n            p = None\n            try:\n                p = self.ws.recv()\n            except websocket.WebSocketConnectionClosedException:\n                self.logger.warning(\n                    'WebSocket connection was closed, aborting')\n                self.queue.put(None)\n                break\n            except Exception as e:\n                self.logger.info(\n                    'Unexpected error \"%s\", aborting', str(e))\n                self.queue.put(None)\n                break\n            if isinstance(p, six.text_type):  # pragma: no cover\n                p = p.encode('utf-8')\n            pkt = packet.Packet(encoded_packet=p)\n            self._receive_packet(pkt)\n\n        self.logger.info('Waiting for write loop task to end')\n        self.write_loop_task.join()\n        self.logger.info('Waiting for ping loop task to end')\n        self.ping_loop_event.set()\n        self.ping_loop_task.join()\n        if self.state == 'connected':\n            self._trigger_event('disconnect', run_async=False)\n            try:\n                connected_clients.remove(self)\n            except ValueError:  # pragma: no cover\n                pass\n            self._reset()\n        self.logger.info('Exiting read loop task')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_loop(self):\n        while self.state == 'connected':\n            # to simplify the timeout handling, use the maximum of the\n            # ping interval and ping timeout as timeout, with an extra 5\n            # seconds grace period\n            timeout = max(self.ping_interval, self.ping_timeout) + 5\n            packets = None\n            try:\n                packets = [self.queue.get(timeout=timeout)]\n            except self.queue.Empty:\n                self.logger.error('packet queue is empty, aborting')\n                self._reset()\n                break\n            if packets == [None]:\n                self.queue.task_done()\n                packets = []\n            else:\n                while True:\n                    try:\n                        packets.append(self.queue.get(block=False))\n                    except self.queue.Empty:\n                        break\n                    if packets[-1] is None:\n                        packets = packets[:-1]\n                        self.queue.task_done()\n                        break\n            if not packets:\n                # empty packet list returned -> connection closed\n                break\n            if self.current_transport == 'polling':\n                p = payload.Payload(packets=packets)\n                r = self._send_request(\n                    'POST', self.base_url, body=p.encode(),\n                    headers={'Content-Type': 'application/octet-stream'})\n                for pkt in packets:\n                    self.queue.task_done()\n                if r is None:\n                    self.logger.warning(\n                        'Connection refused by the server, aborting')\n                    break\n                if r.status_code != 200:\n                    self.logger.warning('Unexpected status code %s in server '\n                                        'response, aborting', r.status_code)\n                    self._reset()\n                    break\n            else:\n                # websocket\n                try:\n                    for pkt in packets:\n                        self.ws.send(pkt.encode())\n                        self.queue.task_done()\n                except websocket.WebSocketConnectionClosedException:\n                    self.logger.warning(\n                        'WebSocket connection was closed, aborting')\n                    break\n        self.logger.info('Exiting write loop task')", "response": "This loop sends packages to the server as they are send queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisconnect a client. :param sid: The session id of the client to close. If this parameter is not given, then all clients are closed.", "response": "def disconnect(self, sid=None):\n        \"\"\"Disconnect a client.\n\n        :param sid: The session id of the client to close. If this parameter\n                    is not given, then all clients are closed.\n        \"\"\"\n        if sid is not None:\n            try:\n                socket = self._get_socket(sid)\n            except KeyError:  # pragma: no cover\n                # the socket was already closed or gone\n                pass\n            else:\n                socket.close()\n                del self.sockets[sid]\n        else:\n            for client in six.itervalues(self.sockets):\n                client.close()\n            self.sockets = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles an HTTP request from the client. This is the entry point of the Engine.IO application, using the same interface as a WSGI application. For the typical usage, this function is invoked by the :class:`Middleware` instance, but it can be invoked directly when the middleware is not used. :param environ: The WSGI environment. :param start_response: The WSGI ``start_response`` function. This function returns the HTTP response body to deliver to the client as a byte sequence.", "response": "def handle_request(self, environ, start_response):\n        \"\"\"Handle an HTTP request from the client.\n\n        This is the entry point of the Engine.IO application, using the same\n        interface as a WSGI application. For the typical usage, this function\n        is invoked by the :class:`Middleware` instance, but it can be invoked\n        directly when the middleware is not used.\n\n        :param environ: The WSGI environment.\n        :param start_response: The WSGI ``start_response`` function.\n\n        This function returns the HTTP response body to deliver to the client\n        as a byte sequence.\n        \"\"\"\n        method = environ['REQUEST_METHOD']\n        query = urllib.parse.parse_qs(environ.get('QUERY_STRING', ''))\n        if 'j' in query:\n            self.logger.warning('JSONP requests are not supported')\n            r = self._bad_request()\n        else:\n            sid = query['sid'][0] if 'sid' in query else None\n            b64 = False\n            if 'b64' in query:\n                if query['b64'][0] == \"1\" or query['b64'][0].lower() == \"true\":\n                    b64 = True\n            if method == 'GET':\n                if sid is None:\n                    transport = query.get('transport', ['polling'])[0]\n                    if transport != 'polling' and transport != 'websocket':\n                        self.logger.warning('Invalid transport %s', transport)\n                        r = self._bad_request()\n                    else:\n                        r = self._handle_connect(environ, start_response,\n                                                 transport, b64)\n                else:\n                    if sid not in self.sockets:\n                        self.logger.warning('Invalid session %s', sid)\n                        r = self._bad_request()\n                    else:\n                        socket = self._get_socket(sid)\n                        try:\n                            packets = socket.handle_get_request(\n                                environ, start_response)\n                            if isinstance(packets, list):\n                                r = self._ok(packets, b64=b64)\n                            else:\n                                r = packets\n                        except exceptions.EngineIOError:\n                            if sid in self.sockets:  # pragma: no cover\n                                self.disconnect(sid)\n                            r = self._bad_request()\n                        if sid in self.sockets and self.sockets[sid].closed:\n                            del self.sockets[sid]\n            elif method == 'POST':\n                if sid is None or sid not in self.sockets:\n                    self.logger.warning('Invalid session %s', sid)\n                    r = self._bad_request()\n                else:\n                    socket = self._get_socket(sid)\n                    try:\n                        socket.handle_post_request(environ)\n                        r = self._ok()\n                    except exceptions.EngineIOError:\n                        if sid in self.sockets:  # pragma: no cover\n                            self.disconnect(sid)\n                        r = self._bad_request()\n                    except:  # pragma: no cover\n                        # for any other unexpected errors, we log the error\n                        # and keep going\n                        self.logger.exception('post request handler error')\n                        r = self._ok()\n            elif method == 'OPTIONS':\n                r = self._ok()\n            else:\n                self.logger.warning('Method %s not supported', method)\n                r = self._method_not_found()\n        if not isinstance(r, dict):\n            return r or []\n        if self.http_compression and \\\n                len(r['response']) >= self.compression_threshold:\n            encodings = [e.split(';')[0].strip() for e in\n                         environ.get('HTTP_ACCEPT_ENCODING', '').split(',')]\n            for encoding in encodings:\n                if encoding in self.compression_methods:\n                    r['response'] = \\\n                        getattr(self, '_' + encoding)(r['response'])\n                    r['headers'] += [('Content-Encoding', encoding)]\n                    break\n        cors_headers = self._cors_headers(environ)\n        start_response(r['status'], r['headers'] + cors_headers)\n        return [r['response']]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_background_task(self, target, *args, **kwargs):\n        th = self._async['thread'](target=target, args=args, kwargs=kwargs)\n        th.start()\n        return th", "response": "Start a background task using the appropriate async model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle a client connection request.", "response": "def _handle_connect(self, environ, start_response, transport, b64=False):\n        \"\"\"Handle a client connection request.\"\"\"\n        if self.start_service_task:\n            # start the service task to monitor connected clients\n            self.start_service_task = False\n            self.start_background_task(self._service_task)\n\n        sid = self._generate_id()\n        s = socket.Socket(self, sid)\n        self.sockets[sid] = s\n\n        pkt = packet.Packet(\n            packet.OPEN, {'sid': sid,\n                          'upgrades': self._upgrades(sid, transport),\n                          'pingTimeout': int(self.ping_timeout * 1000),\n                          'pingInterval': int(self.ping_interval * 1000)})\n        s.send(pkt)\n\n        ret = self._trigger_event('connect', sid, environ, run_async=False)\n        if ret is False:\n            del self.sockets[sid]\n            self.logger.warning('Application rejected connection')\n            return self._unauthorized()\n\n        if transport == 'websocket':\n            ret = s.handle_get_request(environ, start_response)\n            if s.closed:\n                # websocket connection ended, so we are done\n                del self.sockets[sid]\n            return ret\n        else:\n            s.connected = True\n            headers = None\n            if self.cookie:\n                headers = [('Set-Cookie', self.cookie + '=' + sid)]\n            try:\n                return self._ok(s.poll(), headers=headers, b64=b64)\n            except exceptions.QueueEmpty:\n                return self._bad_request()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the list of possible upgrades for a client connection.", "response": "def _upgrades(self, sid, transport):\n        \"\"\"Return the list of possible upgrades for a client connection.\"\"\"\n        if not self.allow_upgrades or self._get_socket(sid).upgraded or \\\n                self._async['websocket'] is None or transport == 'websocket':\n            return []\n        return ['websocket']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the socket object for a given session.", "response": "def _get_socket(self, sid):\n        \"\"\"Return the socket object for a given session.\"\"\"\n        try:\n            s = self.sockets[sid]\n        except KeyError:\n            raise KeyError('Session not found')\n        if s.closed:\n            del self.sockets[sid]\n            raise KeyError('Session is disconnected')\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ok(self, packets=None, headers=None, b64=False):\n        if packets is not None:\n            if headers is None:\n                headers = []\n            if b64:\n                headers += [('Content-Type', 'text/plain; charset=UTF-8')]\n            else:\n                headers += [('Content-Type', 'application/octet-stream')]\n            return {'status': '200 OK',\n                    'headers': headers,\n                    'response': payload.Payload(packets=packets).encode(b64)}\n        else:\n            return {'status': '200 OK',\n                    'headers': [('Content-Type', 'text/plain')],\n                    'response': b'OK'}", "response": "Generate a successful HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cors_headers(self, environ):\n        if isinstance(self.cors_allowed_origins, six.string_types):\n            if self.cors_allowed_origins == '*':\n                allowed_origins = None\n            else:\n                allowed_origins = [self.cors_allowed_origins]\n        else:\n            allowed_origins = self.cors_allowed_origins\n        if allowed_origins is not None and \\\n                environ.get('HTTP_ORIGIN', '') not in allowed_origins:\n            return []\n        if 'HTTP_ORIGIN' in environ:\n            headers = [('Access-Control-Allow-Origin', environ['HTTP_ORIGIN'])]\n        else:\n            headers = [('Access-Control-Allow-Origin', '*')]\n        if environ['REQUEST_METHOD'] == 'OPTIONS':\n            headers += [('Access-Control-Allow-Methods', 'OPTIONS, GET, POST')]\n        if 'HTTP_ACCESS_CONTROL_REQUEST_HEADERS' in environ:\n            headers += [('Access-Control-Allow-Headers',\n                         environ['HTTP_ACCESS_CONTROL_REQUEST_HEADERS'])]\n        if self.cors_credentials:\n            headers += [('Access-Control-Allow-Credentials', 'true')]\n        return headers", "response": "Return the cross - origin - resource - sharing headers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _gzip(self, response):\n        bytesio = six.BytesIO()\n        with gzip.GzipFile(fileobj=bytesio, mode='w') as gz:\n            gz.write(response)\n        return bytesio.getvalue()", "response": "Apply gzip compression to a response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _service_task(self):  # pragma: no cover\n        while True:\n            if len(self.sockets) == 0:\n                # nothing to do\n                self.sleep(self.ping_timeout)\n                continue\n\n            # go through the entire client list in a ping interval cycle\n            sleep_interval = self.ping_timeout / len(self.sockets)\n\n            try:\n                # iterate over the current clients\n                for s in self.sockets.copy().values():\n                    if not s.closing and not s.closed:\n                        s.check_ping_timeout()\n                    self.sleep(sleep_interval)\n            except (SystemExit, KeyboardInterrupt):\n                self.logger.info('service task canceled')\n                break\n            except:\n                # an unexpected exception has occurred, log it and continue\n                self.logger.exception('service task exception')", "response": "Monitor connected clients and clean up those that time out."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        uwsgi.disconnect()\n        if self._req_ctx is None:\n            # better kill it here in case wait() is not called again\n            self._select_greenlet.kill()\n            self._event.set()", "response": "Disconnects uWSGI from the client."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _send(self, msg):\n        if isinstance(msg, six.binary_type):\n            method = uwsgi.websocket_send_binary\n        else:\n            method = uwsgi.websocket_send\n        if self._req_ctx is not None:\n            method(msg, request_context=self._req_ctx)\n        else:\n            method(msg)", "response": "Transmits a message either in binary or UTF - 8 text mode and depending on its type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decode_received(self, msg):\n        if not isinstance(msg, six.binary_type):\n            # already decoded - do nothing\n            return msg\n        # only decode from utf-8 if message is not binary data\n        type = six.byte2int(msg[0:1])\n        if type >= 48:  # no binary\n            return msg.decode('utf-8')\n        # binary message, don't try to decode\n        return msg", "response": "Returns either bytes or str depending on message type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nqueue a message for sending.", "response": "def send(self, msg):\n        \"\"\"Queues a message for sending. Real transmission is done in\n        wait method.\n        Sends directly if uWSGI version is new enough.\"\"\"\n        if self._req_ctx is not None:\n            self._send(msg)\n        else:\n            self._send_queue.put(msg)\n            self._event.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait(self):\n        while True:\n            if self._req_ctx is not None:\n                try:\n                    msg = uwsgi.websocket_recv(request_context=self._req_ctx)\n                except IOError:  # connection closed\n                    return None\n                return self._decode_received(msg)\n            else:\n                # we wake up at least every 3 seconds to let uWSGI\n                # do its ping/ponging\n                event_set = self._event.wait(timeout=3)\n                if event_set:\n                    self._event.clear()\n                    # maybe there is something to send\n                    msgs = []\n                    while True:\n                        try:\n                            msgs.append(self._send_queue.get(block=False))\n                        except gevent.queue.Empty:\n                            break\n                    for msg in msgs:\n                        self._send(msg)\n                # maybe there is something to receive, if not, at least\n                # ensure uWSGI does its ping/ponging\n                try:\n                    msg = uwsgi.websocket_recv_nb()\n                except IOError:  # connection closed\n                    self._select_greenlet.kill()\n                    return None\n                if msg:  # message available\n                    return self._decode_received(msg)", "response": "Waits and returns received messages."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_route(app, engineio_server, engineio_endpoint):\n    app.add_route(engineio_server.handle_request, engineio_endpoint,\n                  methods=['GET', 'POST', 'OPTIONS'])\n    try:\n        app.enable_websocket()\n    except AttributeError:\n        # ignore, this version does not support websocket\n        pass", "response": "This function creates the engine. io endpoint as a route for the engine. io application."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef translate_request(request):\n    class AwaitablePayload(object):\n        def __init__(self, payload):\n            self.payload = payload or b''\n\n        async def read(self, length=None):\n            if length is None:\n                r = self.payload\n                self.payload = b''\n            else:\n                r = self.payload[:length]\n                self.payload = self.payload[length:]\n            return r\n\n    uri_parts = urlsplit(request.url)\n    environ = {\n        'wsgi.input': AwaitablePayload(request.body),\n        'wsgi.errors': sys.stderr,\n        'wsgi.version': (1, 0),\n        'wsgi.async': True,\n        'wsgi.multithread': False,\n        'wsgi.multiprocess': False,\n        'wsgi.run_once': False,\n        'SERVER_SOFTWARE': 'sanic',\n        'REQUEST_METHOD': request.method,\n        'QUERY_STRING': uri_parts.query or '',\n        'RAW_URI': request.url,\n        'SERVER_PROTOCOL': 'HTTP/' + request.version,\n        'REMOTE_ADDR': '127.0.0.1',\n        'REMOTE_PORT': '0',\n        'SERVER_NAME': 'sanic',\n        'SERVER_PORT': '0',\n        'sanic.request': request\n    }\n\n    for hdr_name, hdr_value in request.headers.items():\n        hdr_name = hdr_name.upper()\n        if hdr_name == 'CONTENT-TYPE':\n            environ['CONTENT_TYPE'] = hdr_value\n            continue\n        elif hdr_name == 'CONTENT-LENGTH':\n            environ['CONTENT_LENGTH'] = hdr_value\n            continue\n\n        key = 'HTTP_%s' % hdr_name.replace('-', '_')\n        if key in environ:\n            hdr_value = '%s,%s' % (environ[key], hdr_value)\n\n        environ[key] = hdr_value\n\n    environ['wsgi.url_scheme'] = environ.get('HTTP_X_FORWARDED_PROTO', 'http')\n\n    path_info = uri_parts.path\n\n    environ['PATH_INFO'] = path_info\n    environ['SCRIPT_NAME'] = ''\n\n    return environ", "response": "This function takes the arguments passed to the request handler and returns a WSGI compatible dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_response(status, headers, payload, environ):\n    headers_dict = {}\n    content_type = None\n    for h in headers:\n        if h[0].lower() == 'content-type':\n            content_type = h[1]\n        else:\n            headers_dict[h[0]] = h[1]\n    return HTTPResponse(body_bytes=payload, content_type=content_type,\n                        status=int(status.split()[0]), headers=headers_dict)", "response": "This function generates an appropriate response object for this async\n    mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attach(self, app, engineio_path='engine.io'):\n        engineio_path = engineio_path.strip('/')\n        self._async['create_route'](app, self, '/{}/'.format(engineio_path))", "response": "Attach the Engine. IO server to an application."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a message to a client.", "response": "async def send(self, sid, data, binary=None):\n        \"\"\"Send a message to a client.\n\n        :param sid: The session id of the recipient client.\n        :param data: The data to send to the client. Data can be of type\n                     ``str``, ``bytes``, ``list`` or ``dict``. If a ``list``\n                     or ``dict``, the data will be serialized as JSON.\n        :param binary: ``True`` to send packet as binary, ``False`` to send\n                       as text. If not given, unicode (Python 2) and str\n                       (Python 3) are sent as text, and str (Python 2) and\n                       bytes (Python 3) are sent as binary.\n\n        Note: this method is a coroutine.\n        \"\"\"\n        try:\n            socket = self._get_socket(sid)\n        except KeyError:\n            # the socket is not available\n            self.logger.warning('Cannot send to sid %s', sid)\n            return\n        await socket.send(packet.Packet(packet.MESSAGE, data=data,\n                                        binary=binary))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstores the user session for a client.", "response": "async def save_session(self, sid, session):\n        \"\"\"Store the user session for a client.\n\n        :param sid: The session id of the client.\n        :param session: The session dictionary.\n        \"\"\"\n        socket = self._get_socket(sid)\n        socket.session = session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the user session dictionary for a client with context manager syntax.", "response": "def session(self, sid):\n        \"\"\"Return the user session for a client with context manager syntax.\n\n        :param sid: The session id of the client.\n\n        This is a context manager that returns the user session dictionary for\n        the client. Any changes that are made to this dictionary inside the\n        context manager block are saved back to the session. Example usage::\n\n            @eio.on('connect')\n            def on_connect(sid, environ):\n                username = authenticate_user(environ)\n                if not username:\n                    return False\n                with eio.session(sid) as session:\n                    session['username'] = username\n\n            @eio.on('message')\n            def on_message(sid, msg):\n                async with eio.session(sid) as session:\n                    print('received message from ', session['username'])\n        \"\"\"\n        class _session_context_manager(object):\n            def __init__(self, server, sid):\n                self.server = server\n                self.sid = sid\n                self.session = None\n\n            async def __aenter__(self):\n                self.session = await self.server.get_session(sid)\n                return self.session\n\n            async def __aexit__(self, *args):\n                await self.server.save_session(sid, self.session)\n\n        return _session_context_manager(self, sid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_background_task(self, target, *args, **kwargs):\n        return asyncio.ensure_future(target(*args, **kwargs))", "response": "Start a background task using the appropriate async model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _trigger_event(self, event, *args, **kwargs):\n        run_async = kwargs.pop('run_async', False)\n        ret = None\n        if event in self.handlers:\n            if asyncio.iscoroutinefunction(self.handlers[event]) is True:\n                if run_async:\n                    return self.start_background_task(self.handlers[event],\n                                                      *args)\n                else:\n                    try:\n                        ret = await self.handlers[event](*args)\n                    except asyncio.CancelledError:  # pragma: no cover\n                        pass\n                    except:\n                        self.logger.exception(event + ' async handler error')\n                        if event == 'connect':\n                            # if connect handler raised error we reject the\n                            # connection\n                            return False\n            else:\n                if run_async:\n                    async def async_handler():\n                        return self.handlers[event](*args)\n\n                    return self.start_background_task(async_handler)\n                else:\n                    try:\n                        ret = self.handlers[event](*args)\n                    except:\n                        self.logger.exception(event + ' handler error')\n                        if event == 'connect':\n                            # if connect handler raised error we reject the\n                            # connection\n                            return False\n        return ret", "response": "Invoke an event handler."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _service_task(self):  # pragma: no cover\n        while True:\n            if len(self.sockets) == 0:\n                # nothing to do\n                await self.sleep(self.ping_timeout)\n                continue\n\n            # go through the entire client list in a ping interval cycle\n            sleep_interval = self.ping_timeout / len(self.sockets)\n\n            try:\n                # iterate over the current clients\n                for socket in self.sockets.copy().values():\n                    if not socket.closing and not socket.closed:\n                        await socket.check_ping_timeout()\n                    await self.sleep(sleep_interval)\n            except (SystemExit, KeyboardInterrupt, asyncio.CancelledError):\n                self.logger.info('service task canceled')\n                break\n            except:\n                if asyncio.get_event_loop().is_closed():\n                    self.logger.info('event loop is closed, exiting service '\n                                     'task')\n                    break\n\n                # an unexpected exception has occurred, log it and continue\n                self.logger.exception('service task exception')", "response": "Monitor connected clients and clean up those that time out."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef poll(self):\n        queue_empty = self.server.get_queue_empty_exception()\n        try:\n            packets = [self.queue.get(timeout=self.server.ping_timeout)]\n            self.queue.task_done()\n        except queue_empty:\n            raise exceptions.QueueEmpty()\n        if packets == [None]:\n            return []\n        while True:\n            try:\n                packets.append(self.queue.get(block=False))\n                self.queue.task_done()\n            except queue_empty:\n                break\n        return packets", "response": "Wait for packets to send to the client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_get_request(self, environ, start_response):\n        connections = [\n            s.strip()\n            for s in environ.get('HTTP_CONNECTION', '').lower().split(',')]\n        transport = environ.get('HTTP_UPGRADE', '').lower()\n        if 'upgrade' in connections and transport in self.upgrade_protocols:\n            self.server.logger.info('%s: Received request to upgrade to %s',\n                                    self.sid, transport)\n            return getattr(self, '_upgrade_' + transport)(environ,\n                                                          start_response)\n        try:\n            packets = self.poll()\n        except exceptions.QueueEmpty:\n            exc = sys.exc_info()\n            self.close(wait=False)\n            six.reraise(*exc)\n        return packets", "response": "Handle a long - polling GET request from the client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclosing the socket connection.", "response": "def close(self, wait=True, abort=False):\n        \"\"\"Close the socket connection.\"\"\"\n        if not self.closed and not self.closing:\n            self.closing = True\n            self.server._trigger_event('disconnect', self.sid, run_async=False)\n            if not abort:\n                self.send(packet.Packet(packet.CLOSE))\n            self.closed = True\n            self.queue.put(None)\n            if wait:\n                self.queue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _websocket_handler(self, ws):\n        # try to set a socket timeout matching the configured ping interval\n        for attr in ['_sock', 'socket']:  # pragma: no cover\n            if hasattr(ws, attr) and hasattr(getattr(ws, attr), 'settimeout'):\n                getattr(ws, attr).settimeout(self.server.ping_timeout)\n\n        if self.connected:\n            # the socket was already connected, so this is an upgrade\n            self.upgrading = True  # hold packet sends during the upgrade\n\n            pkt = ws.wait()\n            decoded_pkt = packet.Packet(encoded_packet=pkt)\n            if decoded_pkt.packet_type != packet.PING or \\\n                    decoded_pkt.data != 'probe':\n                self.server.logger.info(\n                    '%s: Failed websocket upgrade, no PING packet', self.sid)\n                return []\n            ws.send(packet.Packet(\n                packet.PONG,\n                data=six.text_type('probe')).encode(always_bytes=False))\n            self.queue.put(packet.Packet(packet.NOOP))  # end poll\n\n            pkt = ws.wait()\n            decoded_pkt = packet.Packet(encoded_packet=pkt)\n            if decoded_pkt.packet_type != packet.UPGRADE:\n                self.upgraded = False\n                self.server.logger.info(\n                    ('%s: Failed websocket upgrade, expected UPGRADE packet, '\n                     'received %s instead.'),\n                    self.sid, pkt)\n                return []\n            self.upgraded = True\n\n            # flush any packets that were sent during the upgrade\n            for pkt in self.packet_backlog:\n                self.queue.put(pkt)\n            self.packet_backlog = []\n            self.upgrading = False\n        else:\n            self.connected = True\n            self.upgraded = True\n\n        # start separate writer thread\n        def writer():\n            while True:\n                packets = None\n                try:\n                    packets = self.poll()\n                except exceptions.QueueEmpty:\n                    break\n                if not packets:\n                    # empty packet list returned -> connection closed\n                    break\n                try:\n                    for pkt in packets:\n                        ws.send(pkt.encode(always_bytes=False))\n                except:\n                    break\n        writer_task = self.server.start_background_task(writer)\n\n        self.server.logger.info(\n            '%s: Upgrade to websocket successful', self.sid)\n\n        while True:\n            p = None\n            try:\n                p = ws.wait()\n            except Exception as e:\n                # if the socket is already closed, we can assume this is a\n                # downstream error of that\n                if not self.closed:  # pragma: no cover\n                    self.server.logger.info(\n                        '%s: Unexpected error \"%s\", closing connection',\n                        self.sid, str(e))\n                break\n            if p is None:\n                # connection closed by client\n                break\n            if isinstance(p, six.text_type):  # pragma: no cover\n                p = p.encode('utf-8')\n            pkt = packet.Packet(encoded_packet=p)\n            try:\n                self.receive(pkt)\n            except exceptions.UnknownPacketError:  # pragma: no cover\n                pass\n            except exceptions.SocketIsClosedError:  # pragma: no cover\n                self.server.logger.info('Receive error -- socket is closed')\n                break\n            except:  # pragma: no cover\n                # if we get an unexpected exception we log the error and exit\n                # the connection properly\n                self.server.logger.exception('Unknown receive error')\n                break\n\n        self.queue.put(None)  # unlock the writer task so that it can exit\n        writer_task.join()\n        self.close(wait=False, abort=True)\n\n        return []", "response": "Handler for the Engine. IO transport."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def connect(self, url, headers={}, transports=None,\n                      engineio_path='engine.io'):\n        \"\"\"Connect to an Engine.IO server.\n\n        :param url: The URL of the Engine.IO server. It can include custom\n                    query string parameters if required by the server.\n        :param headers: A dictionary with custom headers to send with the\n                        connection request.\n        :param transports: The list of allowed transports. Valid transports\n                           are ``'polling'`` and ``'websocket'``. If not\n                           given, the polling transport is connected first,\n                           then an upgrade to websocket is attempted.\n        :param engineio_path: The endpoint where the Engine.IO server is\n                              installed. The default value is appropriate for\n                              most cases.\n\n        Note: this method is a coroutine.\n\n        Example usage::\n\n            eio = engineio.Client()\n            await eio.connect('http://localhost:5000')\n        \"\"\"\n        if self.state != 'disconnected':\n            raise ValueError('Client is not in a disconnected state')\n        valid_transports = ['polling', 'websocket']\n        if transports is not None:\n            if isinstance(transports, six.text_type):\n                transports = [transports]\n            transports = [transport for transport in transports\n                          if transport in valid_transports]\n            if not transports:\n                raise ValueError('No valid transports provided')\n        self.transports = transports or valid_transports\n        self.queue = self.create_queue()\n        return await getattr(self, '_connect_' + self.transports[0])(\n            url, headers, engineio_path)", "response": "Connect to an Engine. IO server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to a client.", "response": "async def send(self, data, binary=None):\n        \"\"\"Send a message to a client.\n\n        :param data: The data to send to the client. Data can be of type\n                     ``str``, ``bytes``, ``list`` or ``dict``. If a ``list``\n                     or ``dict``, the data will be serialized as JSON.\n        :param binary: ``True`` to send packet as binary, ``False`` to send\n                       as text. If not given, unicode (Python 2) and str\n                       (Python 3) are sent as text, and str (Python 2) and\n                       bytes (Python 3) are sent as binary.\n\n        Note: this method is a coroutine.\n        \"\"\"\n        await self._send_packet(packet.Packet(packet.MESSAGE, data=data,\n                                              binary=binary))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisconnect from the server.", "response": "async def disconnect(self, abort=False):\n        \"\"\"Disconnect from the server.\n\n        :param abort: If set to ``True``, do not wait for background tasks\n                      associated with the connection to end.\n\n        Note: this method is a coroutine.\n        \"\"\"\n        if self.state == 'connected':\n            await self._send_packet(packet.Packet(packet.CLOSE))\n            await self.queue.put(None)\n            self.state = 'disconnecting'\n            await self._trigger_event('disconnect', run_async=False)\n            if self.current_transport == 'websocket':\n                await self.ws.close()\n            if not abort:\n                await self.read_loop_task\n            self.state = 'disconnected'\n            try:\n                client.connected_clients.remove(self)\n            except ValueError:  # pragma: no cover\n                pass\n        self._reset()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a queue object.", "response": "def create_queue(self):\n        \"\"\"Create a queue object.\"\"\"\n        q = asyncio.Queue()\n        q.Empty = asyncio.QueueEmpty\n        return q"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _connect_polling(self, url, headers, engineio_path):\n        if aiohttp is None:  # pragma: no cover\n            self.logger.error('aiohttp not installed -- cannot make HTTP '\n                              'requests!')\n            return\n        self.base_url = self._get_engineio_url(url, engineio_path, 'polling')\n        self.logger.info('Attempting polling connection to ' + self.base_url)\n        r = await self._send_request(\n            'GET', self.base_url + self._get_url_timestamp(), headers=headers)\n        if r is None:\n            self._reset()\n            raise exceptions.ConnectionError(\n                'Connection refused by the server')\n        if r.status != 200:\n            raise exceptions.ConnectionError(\n                'Unexpected status code {} in server response'.format(\n                    r.status))\n        try:\n            p = payload.Payload(encoded_payload=await r.read())\n        except ValueError:\n            six.raise_from(exceptions.ConnectionError(\n                'Unexpected response from server'), None)\n        open_packet = p.packets[0]\n        if open_packet.packet_type != packet.OPEN:\n            raise exceptions.ConnectionError(\n                'OPEN packet not returned by server')\n        self.logger.info(\n            'Polling connection accepted with ' + str(open_packet.data))\n        self.sid = open_packet.data['sid']\n        self.upgrades = open_packet.data['upgrades']\n        self.ping_interval = open_packet.data['pingInterval'] / 1000.0\n        self.ping_timeout = open_packet.data['pingTimeout'] / 1000.0\n        self.current_transport = 'polling'\n        self.base_url += '&sid=' + self.sid\n\n        self.state = 'connected'\n        client.connected_clients.append(self)\n        await self._trigger_event('connect', run_async=False)\n\n        for pkt in p.packets[1:]:\n            await self._receive_packet(pkt)\n\n        if 'websocket' in self.upgrades and 'websocket' in self.transports:\n            # attempt to upgrade to websocket\n            if await self._connect_websocket(url, headers, engineio_path):\n                # upgrade to websocket succeeded, we're done here\n                return\n\n        self.ping_loop_task = self.start_background_task(self._ping_loop)\n        self.write_loop_task = self.start_background_task(self._write_loop)\n        self.read_loop_task = self.start_background_task(\n            self._read_loop_polling)", "response": "Establish a long - polling connection to the Engine. IO server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _connect_websocket(self, url, headers, engineio_path):\n        if websockets is None:  # pragma: no cover\n            self.logger.error('websockets package not installed')\n            return False\n        websocket_url = self._get_engineio_url(url, engineio_path,\n                                               'websocket')\n        if self.sid:\n            self.logger.info(\n                'Attempting WebSocket upgrade to ' + websocket_url)\n            upgrade = True\n            websocket_url += '&sid=' + self.sid\n        else:\n            upgrade = False\n            self.base_url = websocket_url\n            self.logger.info(\n                'Attempting WebSocket connection to ' + websocket_url)\n\n        # get the cookies from the long-polling connection so that they can\n        # also be sent the the WebSocket route\n        cookies = None\n        if self.http:\n            cookies = '; '.join([\"{}={}\".format(cookie.key, cookie.value)\n                                 for cookie in self.http._cookie_jar])\n            headers = headers.copy()\n            headers['Cookie'] = cookies\n\n        try:\n            ws = await websockets.connect(\n                websocket_url + self._get_url_timestamp(),\n                extra_headers=headers)\n        except (websockets.exceptions.InvalidURI,\n                websockets.exceptions.InvalidHandshake):\n            if upgrade:\n                self.logger.warning(\n                    'WebSocket upgrade failed: connection error')\n                return False\n            else:\n                raise exceptions.ConnectionError('Connection error')\n        if upgrade:\n            p = packet.Packet(packet.PING, data='probe').encode(\n                always_bytes=False)\n            try:\n                await ws.send(p)\n            except Exception as e:  # pragma: no cover\n                self.logger.warning(\n                    'WebSocket upgrade failed: unexpected send exception: %s',\n                    str(e))\n                return False\n            try:\n                p = await ws.recv()\n            except Exception as e:  # pragma: no cover\n                self.logger.warning(\n                    'WebSocket upgrade failed: unexpected recv exception: %s',\n                    str(e))\n                return False\n            pkt = packet.Packet(encoded_packet=p)\n            if pkt.packet_type != packet.PONG or pkt.data != 'probe':\n                self.logger.warning(\n                    'WebSocket upgrade failed: no PONG packet')\n                return False\n            p = packet.Packet(packet.UPGRADE).encode(always_bytes=False)\n            try:\n                await ws.send(p)\n            except Exception as e:  # pragma: no cover\n                self.logger.warning(\n                    'WebSocket upgrade failed: unexpected send exception: %s',\n                    str(e))\n                return False\n            self.current_transport = 'websocket'\n            if self.http:  # pragma: no cover\n                await self.http.close()\n            self.logger.info('WebSocket upgrade was successful')\n        else:\n            try:\n                p = await ws.recv()\n            except Exception as e:  # pragma: no cover\n                raise exceptions.ConnectionError(\n                    'Unexpected recv exception: ' + str(e))\n            open_packet = packet.Packet(encoded_packet=p)\n            if open_packet.packet_type != packet.OPEN:\n                raise exceptions.ConnectionError('no OPEN packet')\n            self.logger.info(\n                'WebSocket connection accepted with ' + str(open_packet.data))\n            self.sid = open_packet.data['sid']\n            self.upgrades = open_packet.data['upgrades']\n            self.ping_interval = open_packet.data['pingInterval'] / 1000.0\n            self.ping_timeout = open_packet.data['pingTimeout'] / 1000.0\n            self.current_transport = 'websocket'\n\n            self.state = 'connected'\n            client.connected_clients.append(self)\n            await self._trigger_event('connect', run_async=False)\n\n        self.ws = ws\n        self.ping_loop_task = self.start_background_task(self._ping_loop)\n        self.write_loop_task = self.start_background_task(self._write_loop)\n        self.read_loop_task = self.start_background_task(\n            self._read_loop_websocket)\n        return True", "response": "Establish or upgrade to a WebSocket connection with the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _receive_packet(self, pkt):\n        packet_name = packet.packet_names[pkt.packet_type] \\\n            if pkt.packet_type < len(packet.packet_names) else 'UNKNOWN'\n        self.logger.info(\n            'Received packet %s data %s', packet_name,\n            pkt.data if not isinstance(pkt.data, bytes) else '<binary>')\n        if pkt.packet_type == packet.MESSAGE:\n            await self._trigger_event('message', pkt.data, run_async=True)\n        elif pkt.packet_type == packet.PONG:\n            self.pong_received = True\n        elif pkt.packet_type == packet.NOOP:\n            pass\n        else:\n            self.logger.error('Received unexpected packet of type %s',\n                              pkt.packet_type)", "response": "Handle incoming packets from the server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nqueuing a packet to be sent to the server.", "response": "async def _send_packet(self, pkt):\n        \"\"\"Queue a packet to be sent to the server.\"\"\"\n        if self.state != 'connected':\n            return\n        await self.queue.put(pkt)\n        self.logger.info(\n            'Sending packet %s data %s',\n            packet.packet_names[pkt.packet_type],\n            pkt.data if not isinstance(pkt.data, bytes) else '<binary>')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes the payload for transmission.", "response": "def encode(self, b64=False):\n        \"\"\"Encode the payload for transmission.\"\"\"\n        encoded_payload = b''\n        for pkt in self.packets:\n            encoded_packet = pkt.encode(b64=b64)\n            packet_len = len(encoded_packet)\n            if b64:\n                encoded_payload += str(packet_len).encode('utf-8') + b':' + \\\n                    encoded_packet\n            else:\n                binary_len = b''\n                while packet_len != 0:\n                    binary_len = six.int2byte(packet_len % 10) + binary_len\n                    packet_len = int(packet_len / 10)\n                if not pkt.binary:\n                    encoded_payload += b'\\0'\n                else:\n                    encoded_payload += b'\\1'\n                encoded_payload += binary_len + b'\\xff' + encoded_packet\n        return encoded_payload"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a transmitted payload.", "response": "def decode(self, encoded_payload):\n        \"\"\"Decode a transmitted payload.\"\"\"\n        self.packets = []\n        while encoded_payload:\n            if six.byte2int(encoded_payload[0:1]) <= 1:\n                packet_len = 0\n                i = 1\n                while six.byte2int(encoded_payload[i:i + 1]) != 255:\n                    packet_len = packet_len * 10 + six.byte2int(\n                        encoded_payload[i:i + 1])\n                    i += 1\n                self.packets.append(packet.Packet(\n                    encoded_packet=encoded_payload[i + 1:i + 1 + packet_len]))\n            else:\n                i = encoded_payload.find(b':')\n                if i == -1:\n                    raise ValueError('invalid payload')\n\n                # extracting the packet out of the payload is extremely\n                # inefficient, because the payload needs to be treated as\n                # binary, but the non-binary packets have to be parsed as\n                # unicode. Luckily this complication only applies to long\n                # polling, as the websocket transport sends packets\n                # individually wrapped.\n                packet_len = int(encoded_payload[0:i])\n                pkt = encoded_payload.decode('utf-8', errors='ignore')[\n                    i + 1: i + 1 + packet_len].encode('utf-8')\n                self.packets.append(packet.Packet(encoded_packet=pkt))\n\n                # the engine.io protocol sends the packet length in\n                # utf-8 characters, but we need it in bytes to be able to\n                # jump to the next packet in the payload\n                packet_len = len(pkt)\n            encoded_payload = encoded_payload[i + 1 + packet_len:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_response(status, headers, payload, environ):\n    tornado_handler = environ['tornado.handler']\n    tornado_handler.set_status(int(status.split()[0]))\n    for header, value in headers:\n        tornado_handler.set_header(header, value)\n    tornado_handler.write(payload)\n    tornado_handler.finish()", "response": "This function generates an appropriate response object for this async\n    mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_route(app, engineio_server, engineio_endpoint):\n    app.router.add_get(engineio_endpoint, engineio_server.handle_request)\n    app.router.add_post(engineio_endpoint, engineio_server.handle_request)\n    app.router.add_route('OPTIONS', engineio_endpoint,\n                         engineio_server.handle_request)", "response": "This function creates the engine. io endpoint as a route for the engine. io application."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_response(status, headers, payload, environ):\n    return Response(body=payload, status=int(status.split()[0]),\n                    headers=headers)", "response": "This function generates an appropriate response object for this async\n    mode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode(self, b64=False, always_bytes=True):\n        if self.binary and not b64:\n            encoded_packet = six.int2byte(self.packet_type)\n        else:\n            encoded_packet = six.text_type(self.packet_type)\n            if self.binary and b64:\n                encoded_packet = 'b' + encoded_packet\n        if self.binary:\n            if b64:\n                encoded_packet += base64.b64encode(self.data).decode('utf-8')\n            else:\n                encoded_packet += self.data\n        elif isinstance(self.data, six.string_types):\n            encoded_packet += self.data\n        elif isinstance(self.data, dict) or isinstance(self.data, list):\n            encoded_packet += self.json.dumps(self.data,\n                                              separators=(',', ':'))\n        elif self.data is not None:\n            encoded_packet += str(self.data)\n        if always_bytes and not isinstance(encoded_packet, binary_types):\n            encoded_packet = encoded_packet.encode('utf-8')\n        return encoded_packet", "response": "Encode the packet for transmission."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode a transmitted package.", "response": "def decode(self, encoded_packet):\n        \"\"\"Decode a transmitted package.\"\"\"\n        b64 = False\n        if not isinstance(encoded_packet, binary_types):\n            encoded_packet = encoded_packet.encode('utf-8')\n        elif not isinstance(encoded_packet, bytes):\n            encoded_packet = bytes(encoded_packet)\n        self.packet_type = six.byte2int(encoded_packet[0:1])\n        if self.packet_type == 98:  # 'b' --> binary base64 encoded packet\n            self.binary = True\n            encoded_packet = encoded_packet[1:]\n            self.packet_type = six.byte2int(encoded_packet[0:1])\n            self.packet_type -= 48\n            b64 = True\n        elif self.packet_type >= 48:\n            self.packet_type -= 48\n            self.binary = False\n        else:\n            self.binary = True\n        self.data = None\n        if len(encoded_packet) > 1:\n            if self.binary:\n                if b64:\n                    self.data = base64.b64decode(encoded_packet[1:])\n                else:\n                    self.data = encoded_packet[1:]\n            else:\n                try:\n                    self.data = self.json.loads(\n                        encoded_packet[1:].decode('utf-8'))\n                    if isinstance(self.data, int):\n                        # do not allow integer payloads, see\n                        # github.com/miguelgrinberg/python-engineio/issues/75\n                        # for background on this decision\n                        raise ValueError\n                except ValueError:\n                    self.data = encoded_packet[1:].decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait for packets to send to the client.", "response": "async def poll(self):\n        \"\"\"Wait for packets to send to the client.\"\"\"\n        try:\n            packets = [await asyncio.wait_for(self.queue.get(),\n                                              self.server.ping_timeout)]\n            self.queue.task_done()\n        except (asyncio.TimeoutError, asyncio.CancelledError):\n            raise exceptions.QueueEmpty()\n        if packets == [None]:\n            return []\n        try:\n            packets.append(self.queue.get_nowait())\n            self.queue.task_done()\n        except asyncio.QueueEmpty:\n            pass\n        return packets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def receive(self, pkt):\n        self.server.logger.info('%s: Received packet %s data %s',\n                                self.sid, packet.packet_names[pkt.packet_type],\n                                pkt.data if not isinstance(pkt.data, bytes)\n                                else '<binary>')\n        if pkt.packet_type == packet.PING:\n            self.last_ping = time.time()\n            await self.send(packet.Packet(packet.PONG, pkt.data))\n        elif pkt.packet_type == packet.MESSAGE:\n            await self.server._trigger_event(\n                'message', self.sid, pkt.data,\n                run_async=self.server.async_handlers)\n        elif pkt.packet_type == packet.UPGRADE:\n            await self.send(packet.Packet(packet.NOOP))\n        elif pkt.packet_type == packet.CLOSE:\n            await self.close(wait=False, abort=True)\n        else:\n            raise exceptions.UnknownPacketError()", "response": "Receive packet from the client."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def check_ping_timeout(self):\n        if self.closed:\n            raise exceptions.SocketIsClosedError()\n        if time.time() - self.last_ping > self.server.ping_interval + 5:\n            self.server.logger.info('%s: Client is gone, closing socket',\n                                    self.sid)\n            # Passing abort=False here will cause close() to write a\n            # CLOSE packet. This has the effect of updating half-open sockets\n            # to their correct state of disconnected\n            await self.close(wait=False, abort=False)\n            return False\n        return True", "response": "Check if the client is still sending pings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def send(self, pkt):\n        if not await self.check_ping_timeout():\n            return\n        if self.upgrading:\n            self.packet_backlog.append(pkt)\n        else:\n            await self.queue.put(pkt)\n        self.server.logger.info('%s: Sending packet %s data %s',\n                                self.sid, packet.packet_names[pkt.packet_type],\n                                pkt.data if not isinstance(pkt.data, bytes)\n                                else '<binary>')", "response": "Send a packet to the client."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling a long - polling POST request from the client.", "response": "async def handle_post_request(self, environ):\n        \"\"\"Handle a long-polling POST request from the client.\"\"\"\n        length = int(environ.get('CONTENT_LENGTH', '0'))\n        if length > self.server.max_http_buffer_size:\n            raise exceptions.ContentTooLongError()\n        else:\n            body = await environ['wsgi.input'].read(length)\n            p = payload.Payload(encoded_payload=body)\n            for pkt in p.packets:\n                await self.receive(pkt)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupgrading the connection from polling to websocket.", "response": "async def _upgrade_websocket(self, environ):\n        \"\"\"Upgrade the connection from polling to websocket.\"\"\"\n        if self.upgraded:\n            raise IOError('Socket has been upgraded already')\n        if self.server._async['websocket'] is None:\n            # the selected async mode does not support websocket\n            return self.server._bad_request()\n        ws = self.server._async['websocket'](self._websocket_handler)\n        return await ws(environ)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _websocket_handler(self, ws):\n        if self.connected:\n            # the socket was already connected, so this is an upgrade\n            self.upgrading = True  # hold packet sends during the upgrade\n\n            try:\n                pkt = await ws.wait()\n            except IOError:  # pragma: no cover\n                return\n            decoded_pkt = packet.Packet(encoded_packet=pkt)\n            if decoded_pkt.packet_type != packet.PING or \\\n                    decoded_pkt.data != 'probe':\n                self.server.logger.info(\n                    '%s: Failed websocket upgrade, no PING packet', self.sid)\n                return\n            await ws.send(packet.Packet(\n                packet.PONG,\n                data=six.text_type('probe')).encode(always_bytes=False))\n            await self.queue.put(packet.Packet(packet.NOOP))  # end poll\n\n            try:\n                pkt = await ws.wait()\n            except IOError:  # pragma: no cover\n                return\n            decoded_pkt = packet.Packet(encoded_packet=pkt)\n            if decoded_pkt.packet_type != packet.UPGRADE:\n                self.upgraded = False\n                self.server.logger.info(\n                    ('%s: Failed websocket upgrade, expected UPGRADE packet, '\n                     'received %s instead.'),\n                    self.sid, pkt)\n                return\n            self.upgraded = True\n\n            # flush any packets that were sent during the upgrade\n            for pkt in self.packet_backlog:\n                await self.queue.put(pkt)\n            self.packet_backlog = []\n            self.upgrading = False\n        else:\n            self.connected = True\n            self.upgraded = True\n\n        # start separate writer thread\n        async def writer():\n            while True:\n                packets = None\n                try:\n                    packets = await self.poll()\n                except exceptions.QueueEmpty:\n                    break\n                if not packets:\n                    # empty packet list returned -> connection closed\n                    break\n                try:\n                    for pkt in packets:\n                        await ws.send(pkt.encode(always_bytes=False))\n                except:\n                    break\n        writer_task = asyncio.ensure_future(writer())\n\n        self.server.logger.info(\n            '%s: Upgrade to websocket successful', self.sid)\n\n        while True:\n            p = None\n            wait_task = asyncio.ensure_future(ws.wait())\n            try:\n                p = await asyncio.wait_for(wait_task, self.server.ping_timeout)\n            except asyncio.CancelledError:  # pragma: no cover\n                # there is a bug (https://bugs.python.org/issue30508) in\n                # asyncio that causes a \"Task exception never retrieved\" error\n                # to appear when wait_task raises an exception before it gets\n                # cancelled. Calling wait_task.exception() prevents the error\n                # from being issued in Python 3.6, but causes other errors in\n                # other versions, so we run it with all errors suppressed and\n                # hope for the best.\n                try:\n                    wait_task.exception()\n                except:\n                    pass\n                break\n            except:\n                break\n            if p is None:\n                # connection closed by client\n                break\n            if isinstance(p, six.text_type):  # pragma: no cover\n                p = p.encode('utf-8')\n            pkt = packet.Packet(encoded_packet=p)\n            try:\n                await self.receive(pkt)\n            except exceptions.UnknownPacketError:  # pragma: no cover\n                pass\n            except exceptions.SocketIsClosedError:  # pragma: no cover\n                self.server.logger.info('Receive error -- socket is closed')\n                break\n            except:  # pragma: no cover\n                # if we get an unexpected exception we log the error and exit\n                # the connection properly\n                self.server.logger.exception('Unknown receive error')\n\n        await self.queue.put(None)  # unlock the writer task so it can exit\n        await asyncio.wait_for(writer_task, timeout=None)\n        await self.close(wait=False, abort=True)", "response": "Handler for WebSocket transport."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decodeRPCErrorMsg(e):\n    found = re.search(\n        (\n            \"(10 assert_exception: Assert Exception\\n|\"\n            \"3030000 tx_missing_posting_auth)\"\n            \".*: (.*)\\n\"\n        ),\n        str(e),\n        flags=re.M,\n    )\n    if found:\n        return found.group(2).strip()\n    else:\n        return str(e)", "response": "Helper function to decode the raised Exception and give it a\nCTYPE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransferring an asset to another account.", "response": "def transfer(self, to, amount, asset, memo=\"\", account=None, **kwargs):\n        \"\"\" Transfer an asset to another account.\n\n            :param str to: Recipient\n            :param float amount: Amount to transfer\n            :param str asset: Asset to transfer\n            :param str memo: (optional) Memo, may begin with `#` for encrypted\n                messaging\n            :param str account: (optional) the source account for the transfer\n                if not ``default_account``\n        \"\"\"\n        from .memo import Memo\n\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n\n        account = Account(account, blockchain_instance=self)\n        amount = Amount(amount, asset, blockchain_instance=self)\n        to = Account(to, blockchain_instance=self)\n\n        memoObj = Memo(from_account=account, to_account=to, blockchain_instance=self)\n\n        op = operations.Transfer(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"from\": account[\"id\"],\n                \"to\": to[\"id\"],\n                \"amount\": {\"amount\": int(amount), \"asset_id\": amount.asset[\"id\"]},\n                \"memo\": memoObj.encrypt(memo),\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account, \"active\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupgrading an account to Lifetime membership", "response": "def upgrade_account(self, account=None, **kwargs):\n        \"\"\" Upgrade an account to Lifetime membership\n\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)\n        \"\"\"\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        op = operations.Account_upgrade(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"account_to_upgrade\": account[\"id\"],\n                \"upgrade_to_lifetime_member\": True,\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nallows additional access to an account by some other public key or account.", "response": "def allow(\n        self,\n        foreign,\n        weight=None,\n        permission=\"active\",\n        account=None,\n        threshold=None,\n        **kwargs\n    ):\n        \"\"\" Give additional access to an account by some other public\n            key or account.\n\n            :param str foreign: The foreign account that will obtain access\n            :param int weight: (optional) The weight to use. If not\n                define, the threshold will be used. If the weight is\n                smaller than the threshold, additional signatures will\n                be required. (defaults to threshold)\n            :param str permission: (optional) The actual permission to\n                modify (defaults to ``active``)\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)\n            :param int threshold: The threshold that needs to be reached\n                by signatures to be able to interact\n        \"\"\"\n        from copy import deepcopy\n\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n\n        if permission not in [\"owner\", \"active\"]:\n            raise ValueError(\"Permission needs to be either 'owner', or 'active\")\n        account = Account(account, blockchain_instance=self)\n\n        if not weight:\n            weight = account[permission][\"weight_threshold\"]\n\n        authority = deepcopy(account[permission])\n        try:\n            pubkey = PublicKey(foreign, prefix=self.prefix)\n            authority[\"key_auths\"].append([str(pubkey), weight])\n        except Exception:\n            try:\n                foreign_account = Account(foreign, blockchain_instance=self)\n                authority[\"account_auths\"].append([foreign_account[\"id\"], weight])\n            except Exception:\n                raise ValueError(\"Unknown foreign account or invalid public key\")\n        if threshold:\n            authority[\"weight_threshold\"] = threshold\n            self._test_weights_treshold(authority)\n\n        op = operations.Account_update(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"account\": account[\"id\"],\n                permission: authority,\n                \"extensions\": {},\n                \"prefix\": self.prefix,\n            }\n        )\n        if permission == \"owner\":\n            return self.finalizeOp(op, account[\"name\"], \"owner\", **kwargs)\n        else:\n            return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_memo_key(self, key, account=None, **kwargs):\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n\n        PublicKey(key, prefix=self.prefix)\n\n        account = Account(account, blockchain_instance=self)\n        account[\"options\"][\"memo_key\"] = key\n        op = operations.Account_update(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"account\": account[\"id\"],\n                \"new_options\": account[\"options\"],\n                \"extensions\": {},\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)", "response": "Update an account s memo public key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef approveproposal(self, proposal_ids, account=None, approver=None, **kwargs):\n        from .proposal import Proposal\n\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        is_key = approver and approver[:3] == self.prefix\n        if not approver and not is_key:\n            approver = account\n        elif approver and not is_key:\n            approver = Account(approver, blockchain_instance=self)\n        else:\n            approver = PublicKey(approver)\n\n        if not isinstance(proposal_ids, (list, set, tuple)):\n            proposal_ids = {proposal_ids}\n\n        op = []\n        for proposal_id in proposal_ids:\n            proposal = Proposal(proposal_id, blockchain_instance=self)\n            update_dict = {\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"fee_paying_account\": account[\"id\"],\n                \"proposal\": proposal[\"id\"],\n                \"prefix\": self.prefix,\n            }\n            if is_key:\n                update_dict.update({\"key_approvals_to_add\": [str(approver)]})\n            else:\n                update_dict.update({\"active_approvals_to_add\": [approver[\"id\"]]})\n            op.append(operations.Proposal_update(**update_dict))\n        if is_key:\n            self.txbuffer.appendSigner(approver, \"active\")\n            return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)\n        return self.finalizeOp(op, approver, \"active\", **kwargs)", "response": "Approve a list of proposals"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disapproveproposal(self, proposal_ids, account=None, approver=None, **kwargs):\n        from .proposal import Proposal\n\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        if not approver:\n            approver = account\n        else:\n            approver = Account(approver, blockchain_instance=self)\n\n        if not isinstance(proposal_ids, (list, set, tuple)):\n            proposal_ids = {proposal_ids}\n\n        op = []\n        for proposal_id in proposal_ids:\n            proposal = Proposal(proposal_id, blockchain_instance=self)\n            op.append(\n                operations.Proposal_update(\n                    **{\n                        \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                        \"fee_paying_account\": account[\"id\"],\n                        \"proposal\": proposal[\"id\"],\n                        \"active_approvals_to_remove\": [approver[\"id\"]],\n                        \"prefix\": self.prefix,\n                    }\n                )\n            )\n        return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)", "response": "Disapprove a list of proposals"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef approveworker(self, workers, account=None, **kwargs):\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        options = account[\"options\"]\n\n        if not isinstance(workers, (list, set, tuple)):\n            workers = {workers}\n\n        for worker in workers:\n            worker = Worker(worker, blockchain_instance=self)\n            options[\"votes\"].append(worker[\"vote_for\"])\n        options[\"votes\"] = list(set(options[\"votes\"]))\n        options[\"voting_account\"] = \"1.2.5\"  # Account(\"proxy-to-self\")[\"id\"]\n\n        op = operations.Account_update(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"account\": account[\"id\"],\n                \"new_options\": options,\n                \"extensions\": {},\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)", "response": "Approve a worker in a specific account"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_proxy(self, proxy_account, account=None, **kwargs):\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        proxy = Account(proxy_account, blockchain_instance=self)\n        options = account[\"options\"]\n        options[\"voting_account\"] = proxy[\"id\"]\n\n        op = operations.Account_update(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"account\": account[\"id\"],\n                \"new_options\": options,\n                \"extensions\": {},\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)", "response": "Set a specific proxy for an account."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cancel(self, orderNumbers, account=None, **kwargs):\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, full=False, blockchain_instance=self)\n\n        if not isinstance(orderNumbers, (list, set, tuple)):\n            orderNumbers = {orderNumbers}\n\n        op = []\n        for order in orderNumbers:\n            op.append(\n                operations.Limit_order_cancel(\n                    **{\n                        \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                        \"fee_paying_account\": account[\"id\"],\n                        \"order\": order,\n                        \"extensions\": [],\n                        \"prefix\": self.prefix,\n                    }\n                )\n            )\n        return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)", "response": "Cancels an order in a given market. Requires the orderNumbers and account parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vesting_balance_withdraw(self, vesting_id, amount=None, account=None, **kwargs):\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n\n        if not amount:\n            obj = Vesting(vesting_id, blockchain_instance=self)\n            amount = obj.claimable\n\n        op = operations.Vesting_balance_withdraw(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"vesting_balance\": vesting_id,\n                \"owner\": account[\"id\"],\n                \"amount\": {\"amount\": int(amount), \"asset_id\": amount[\"asset\"][\"id\"]},\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\")", "response": "Withdraw vesting balance\n\n            :param str vesting_id: Id of the vesting object\n            :param bitshares.amount.Amount Amount: to withdraw (\"all\" if not\n                provided\")\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publish_price_feed(\n        self, symbol, settlement_price, cer=None, mssr=110, mcr=200, account=None\n    ):\n        \"\"\" Publish a price feed for a market-pegged asset\n\n            :param str symbol: Symbol of the asset to publish feed for\n            :param bitshares.price.Price settlement_price: Price for settlement\n            :param bitshares.price.Price cer: Core exchange Rate (default\n                ``settlement_price + 5%``)\n            :param float mssr: Percentage for max short squeeze ratio (default:\n                110%)\n            :param float mcr: Percentage for maintenance collateral ratio\n                (default: 200%)\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)\n\n            .. note:: The ``account`` needs to be allowed to produce a\n                      price feed for ``symbol``. For witness produced\n                      feeds this means ``account`` is a witness account!\n        \"\"\"\n        assert mcr > 100\n        assert mssr > 100\n        assert isinstance(\n            settlement_price, Price\n        ), \"settlement_price needs to be instance of `bitshares.price.Price`!\"\n        assert cer is None or isinstance(\n            cer, Price\n        ), \"cer needs to be instance of `bitshares.price.Price`!\"\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        asset = Asset(symbol, blockchain_instance=self, full=True)\n        backing_asset = asset[\"bitasset_data\"][\"options\"][\"short_backing_asset\"]\n        assert (\n            asset[\"id\"] == settlement_price[\"base\"][\"asset\"][\"id\"]\n            or asset[\"id\"] == settlement_price[\"quote\"][\"asset\"][\"id\"]\n        ), \"Price needs to contain the asset of the symbol you'd like to produce a feed for!\"\n        assert asset.is_bitasset, \"Symbol needs to be a bitasset!\"\n        assert (\n            settlement_price[\"base\"][\"asset\"][\"id\"] == backing_asset\n            or settlement_price[\"quote\"][\"asset\"][\"id\"] == backing_asset\n        ), \"The Price needs to be relative to the backing collateral!\"\n\n        settlement_price = settlement_price.as_base(symbol)\n\n        if cer:\n            cer = cer.as_base(symbol)\n            if cer[\"quote\"][\"asset\"][\"id\"] != \"1.3.0\":\n                raise ValueError(\"CER must be defined against core asset '1.3.0'\")\n        else:\n            if settlement_price[\"quote\"][\"asset\"][\"id\"] != \"1.3.0\":\n                raise ValueError(\n                    \"CER must be manually provided because it relates to core asset '1.3.0'\"\n                )\n            cer = settlement_price.as_quote(symbol) * 0.95\n\n        op = operations.Asset_publish_feed(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"publisher\": account[\"id\"],\n                \"asset_id\": asset[\"id\"],\n                \"feed\": {\n                    \"settlement_price\": settlement_price.as_base(symbol).json(),\n                    \"core_exchange_rate\": cer.as_base(symbol).json(),\n                    \"maximum_short_squeeze_ratio\": int(mssr * 10),\n                    \"maintenance_collateral_ratio\": int(mcr * 10),\n                },\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\")", "response": "Publish a price feed for a market -pegged asset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the Core Exchange Rate of an asset", "response": "def update_cer(self, symbol, cer, account=None):\n        \"\"\" Update the Core Exchange Rate (CER) of an asset\n\n            :param str symbol: Symbol of the asset to publish feed for\n            :param bitshares.price.Price cer: Core exchange Rate\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)\n\n        \"\"\"\n        assert isinstance(\n            cer, Price\n        ), \"cer needs to be instance of `bitshares.price.Price`!\"\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        asset = Asset(symbol, blockchain_instance=self, full=True)\n        assert (\n            asset[\"id\"] == cer[\"base\"][\"asset\"][\"id\"]\n            or asset[\"id\"] == cer[\"quote\"][\"asset\"][\"id\"]\n        ), \"Price needs to contain the asset of the symbol you'd like to produce a feed for!\"\n\n        cer = cer.as_base(symbol)\n        if cer[\"quote\"][\"asset\"][\"id\"] != \"1.3.0\":\n            raise ValueError(\"CER must be defined against core asset '1.3.0'\")\n\n        options = asset[\"options\"]\n        options.update({\"core_exchange_rate\": cer.as_base(symbol).json()})\n        op = operations.Asset_update(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"issuer\": account[\"id\"],\n                \"asset_to_update\": asset[\"id\"],\n                \"new_options\": options,\n                \"extensions\": [],\n                \"prefix\": self.prefix,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupgrade a witness account", "response": "def update_witness(self, witness_identifier, url=None, key=None, **kwargs):\n        \"\"\" Upgrade a witness account\n\n            :param str witness_identifier: Identifier for the witness\n            :param str url: New URL for the witness\n            :param str key: Public Key for the signing\n        \"\"\"\n        witness = Witness(witness_identifier)\n        account = witness.account\n        op = operations.Witness_update(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"prefix\": self.prefix,\n                \"witness\": witness[\"id\"],\n                \"witness_account\": account[\"id\"],\n                \"new_url\": url,\n                \"new_signing_key\": key,\n            }\n        )\n        return self.finalizeOp(op, account[\"name\"], \"active\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreserving an amount of this shares.", "response": "def reserve(self, amount, account=None, **kwargs):\n        \"\"\" Reserve/Burn an amount of this shares\n\n            This removes the shares from the supply\n\n            :param bitshares.amount.Amount amount: The amount to be burned.\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)\n        \"\"\"\n        assert isinstance(amount, Amount)\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        op = operations.Asset_reserve(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"payer\": account[\"id\"],\n                \"amount_to_reserve\": {\n                    \"amount\": int(amount),\n                    \"asset_id\": amount[\"asset\"][\"id\"],\n                },\n                \"extensions\": [],\n            }\n        )\n        return self.finalizeOp(op, account, \"active\", **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_worker(\n        self,\n        name,\n        daily_pay,\n        end,\n        url=\"\",\n        begin=None,\n        payment_type=\"vesting\",\n        pay_vesting_period_days=0,\n        account=None,\n        **kwargs\n    ):\n        \"\"\" Create a worker\n\n            This removes the shares from the supply\n\n            **Required**\n\n            :param str name: Name of the worke\n            :param bitshares.amount.Amount daily_pay: The amount to be paid\n                daily\n            :param datetime end: Date/time of end of the worker\n\n            **Optional**\n\n            :param str url: URL to read more about the worker\n            :param datetime begin: Date/time of begin of the worker\n            :param string payment_type: [\"burn\", \"refund\", \"vesting\"] (default:\n                \"vesting\")\n            :param int pay_vesting_period_days: Days of vesting (default: 0)\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)\n        \"\"\"\n        from bitsharesbase.transactions import timeformat\n\n        assert isinstance(daily_pay, Amount)\n        assert daily_pay[\"asset\"][\"id\"] == \"1.3.0\"\n        if not begin:\n            begin = datetime.utcnow() + timedelta(seconds=30)\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n\n        if payment_type == \"refund\":\n            initializer = [0, {}]\n        elif payment_type == \"vesting\":\n            initializer = [1, {\"pay_vesting_period_days\": pay_vesting_period_days}]\n        elif payment_type == \"burn\":\n            initializer = [2, {}]\n        else:\n            raise ValueError('payment_type not in [\"burn\", \"refund\", \"vesting\"]')\n\n        op = operations.Worker_create(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"owner\": account[\"id\"],\n                \"work_begin_date\": begin.strftime(timeformat),\n                \"work_end_date\": end.strftime(timeformat),\n                \"daily_pay\": int(daily_pay),\n                \"name\": name,\n                \"url\": url,\n                \"initializer\": initializer,\n            }\n        )\n        return self.finalizeOp(op, account, \"active\", **kwargs)", "response": "Create a worker for the given worke."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fund_fee_pool(self, symbol, amount, account=None, **kwargs):\n        assert isinstance(amount, float)\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        amount = Amount(amount, \"1.3.0\", blockchain_instance=self)\n        account = Account(account, blockchain_instance=self)\n        asset = Asset(symbol, blockchain_instance=self)\n        op = operations.Asset_fund_fee_pool(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"from_account\": account[\"id\"],\n                \"asset_id\": asset[\"id\"],\n                \"amount\": int(amount),\n                \"extensions\": [],\n            }\n        )\n        return self.finalizeOp(op, account, \"active\", **kwargs)", "response": "Fund the fee pool of an asset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_committee_member(self, url=\"\", account=None, **kwargs):\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n\n        op = operations.Committee_member_create(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"committee_member_account\": account[\"id\"],\n                \"url\": url,\n            }\n        )\n        return self.finalizeOp(op, account, \"active\", **kwargs)", "response": "Create a committee member in a worker."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef account_whitelist(\n        self,\n        account_to_whitelist,\n        lists=[\"white\"],  # set of 'white' and/or 'black'\n        account=None,\n        **kwargs\n    ):\n        \"\"\" Account whitelisting\n\n            :param str account_to_whitelist: The account we want to add\n                to either the white- or the blacklist\n            :param set lists: (defaults to ``('white')``). Lists the\n                user should be added to. Either empty set, 'black',\n                'white' or both.\n            :param str account: (optional) the account to allow access\n                to (defaults to ``default_account``)\n        \"\"\"\n        if not account:\n            if \"default_account\" in self.config:\n                account = self.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, blockchain_instance=self)\n        account_to_list = Account(account_to_whitelist, blockchain_instance=self)\n\n        if not isinstance(lists, (set, list)):\n            raise ValueError('\"lists\" must be of instance list()')\n\n        new_listing = operations.Account_whitelist.no_listing\n        if \"white\" in lists:\n            new_listing += operations.Account_whitelist.white_listed\n        if \"black\" in lists:\n            new_listing += operations.Account_whitelist.black_listed\n\n        op = operations.Account_whitelist(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"authorizing_account\": account[\"id\"],\n                \"account_to_list\": account_to_list[\"id\"],\n                \"new_listing\": new_listing,\n            }\n        )\n        return self.finalizeOp(op, account, \"active\", **kwargs)", "response": "Adds the user to the whitelist of the specified set of modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset_subscriptions(self, accounts=[], markets=[], objects=[]):\n        self.websocket.reset_subscriptions(\n            accounts, self.get_market_ids(markets), objects\n        )", "response": "Change the subscriptions of a running Notify instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of all fees that apply through the network.", "response": "def returnFees(self):\n        \"\"\" Returns a dictionary of all fees that apply through the\n            network\n\n            Example output:\n\n            .. code-block:: js\n\n                {'proposal_create': {'fee': 400000.0},\n                'asset_publish_feed': {'fee': 1000.0}, 'account_create':\n                {'basic_fee': 950000.0, 'price_per_kbyte': 20000.0,\n                'premium_fee': 40000000.0}, 'custom': {'fee': 20000.0},\n                'asset_fund_fee_pool': {'fee': 20000.0},\n                'override_transfer': {'fee': 400000.0}, 'fill_order':\n                {}, 'asset_update': {'price_per_kbyte': 20000.0, 'fee':\n                200000.0}, 'asset_update_feed_producers': {'fee':\n                10000000.0}, 'assert': {'fee': 20000.0},\n                'committee_member_create': {'fee': 100000000.0}}\n\n        \"\"\"\n        from bitsharesbase.operations import operations\n\n        r = {}\n        obj, base = self.blockchain.rpc.get_objects([\"2.0.0\", \"1.3.0\"])\n        fees = obj[\"parameters\"][\"current_fees\"][\"parameters\"]\n        scale = float(obj[\"parameters\"][\"current_fees\"][\"scale\"])\n        for f in fees:\n            op_name = \"unknown %d\" % f[0]\n            for name in operations:\n                if operations[name] == f[0]:\n                    op_name = name\n            fs = f[1]\n            for _type in fs:\n                fs[_type] = float(fs[_type]) * scale / 1e4 / 10 ** base[\"precision\"]\n            r[op_name] = fs\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_debt_positions(self, account=None):\n        if not account:\n            if \"default_account\" in self.blockchain.config:\n                account = self.blockchain.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, full=True, blockchain_instance=self.blockchain)\n\n        r = {}\n        for debt in account.get(\"call_orders\"):\n            base = Asset(\n                debt[\"call_price\"][\"base\"][\"asset_id\"],\n                full=True,\n                blockchain_instance=self.blockchain,\n            )\n            quote = Asset(\n                debt[\"call_price\"][\"quote\"][\"asset_id\"],\n                full=True,\n                blockchain_instance=self.blockchain,\n            )\n            if not quote.is_bitasset:\n                continue\n            quote.ensure_full()\n            bitasset = quote[\"bitasset_data\"]\n            settlement_price = Price(\n                bitasset[\"current_feed\"][\"settlement_price\"],\n                blockchain_instance=self.blockchain,\n            )\n            if not settlement_price:\n                continue\n            call_price = Price(debt[\"call_price\"], blockchain_instance=self.blockchain)\n            collateral_amount = Amount({\"amount\": debt[\"collateral\"], \"asset\": base})\n            debt_amount = Amount({\"amount\": debt[\"debt\"], \"asset\": quote})\n            r[quote[\"symbol\"]] = {\n                \"collateral\": collateral_amount,\n                \"debt\": debt_amount,\n                \"call_price\": call_price,\n                \"settlement_price\": settlement_price,\n                \"ratio\": float(collateral_amount)\n                / float(debt_amount)\n                * float(settlement_price),\n            }\n        return r", "response": "Returns a dict of assets with amounts and call price for each asset in the account"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close_debt_position(self, symbol, account=None):\n        if not account:\n            if \"default_account\" in self.blockchain.config:\n                account = self.blockchain.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, full=True, blockchain_instance=self.blockchain)\n        debts = self.list_debt_positions(account)\n        if symbol not in debts:\n            raise ValueError(\"No call position open for %s\" % symbol)\n        debt = debts[symbol]\n        asset = debt[\"debt\"][\"asset\"]\n        collateral_asset = debt[\"collateral\"][\"asset\"]\n        op = operations.Call_order_update(\n            **{\n                \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n                \"delta_debt\": {\n                    \"amount\": int(-float(debt[\"debt\"]) * 10 ** asset[\"precision\"]),\n                    \"asset_id\": asset[\"id\"],\n                },\n                \"delta_collateral\": {\n                    \"amount\": int(\n                        -float(debt[\"collateral\"]) * 10 ** collateral_asset[\"precision\"]\n                    ),\n                    \"asset_id\": collateral_asset[\"id\"],\n                },\n                \"funding_account\": account[\"id\"],\n                \"extensions\": [],\n            }\n        )\n        return self.blockchain.finalizeOp(op, account[\"name\"], \"active\")", "response": "Closes a debt position and reclaim the collateral asset"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adjust_debt(\n        self,\n        delta,\n        new_collateral_ratio=None,\n        account=None,\n        target_collateral_ratio=None,\n    ):\n        \"\"\" Adjust the amount of debt for an asset\n\n            :param Amount delta: Delta amount of the debt (-10 means reduce\n                debt by 10, +10 means borrow another 10)\n                :param float new_collateral_ratio: collateral ratio to maintain\n                (optional, by default tries to maintain old ratio)\n            :param float target_collateral_ratio: Tag the call order so that in\n                case of margin call, only enough debt is covered to get back to\n                this ratio\n            :raises ValueError: if symbol is not a bitasset\n            :raises ValueError: if collateral ratio is smaller than maintenance\n                collateral ratio\n            :raises ValueError: if required amounts of collateral are not available\n        \"\"\"\n        if not account:\n            if \"default_account\" in self.blockchain.config:\n                account = self.blockchain.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, full=True, blockchain_instance=self.blockchain)\n\n        # We sell quote and pay with base\n        symbol = delta[\"symbol\"]\n        asset = Asset(symbol, full=True, blockchain_instance=self.blockchain)\n        if not asset.is_bitasset:\n            raise ValueError(\"%s is not a bitasset!\" % symbol)\n        bitasset = asset[\"bitasset_data\"]\n\n        # Check minimum collateral ratio\n        backing_asset_id = bitasset[\"options\"][\"short_backing_asset\"]\n        current_debts = self.list_debt_positions(account)\n        if not new_collateral_ratio and symbol not in current_debts:\n            new_collateral_ratio = (\n                bitasset[\"current_feed\"][\"maintenance_collateral_ratio\"] / 1000\n            )\n        elif not new_collateral_ratio and symbol in current_debts:\n            new_collateral_ratio = current_debts[symbol][\"ratio\"]\n\n        # Derive Amount of Collateral\n        collateral_asset = Asset(backing_asset_id, blockchain_instance=self.blockchain)\n        settlement_price = Price(\n            bitasset[\"current_feed\"][\"settlement_price\"],\n            blockchain_instance=self.blockchain,\n        )\n\n        if symbol in current_debts:\n            amount_of_collateral = (\n                (float(current_debts[symbol][\"debt\"]) + float(delta[\"amount\"]))\n                * new_collateral_ratio\n                / float(settlement_price)\n            )\n            amount_of_collateral -= float(current_debts[symbol][\"collateral\"])\n        else:\n            amount_of_collateral = (\n                float(delta[\"amount\"]) * new_collateral_ratio / float(settlement_price)\n            )\n\n        # Verify that enough funds are available\n        fundsNeeded = amount_of_collateral + float(\n            self.returnFees()[\"call_order_update\"][\"fee\"]\n        )\n        fundsHave = account.balance(collateral_asset[\"symbol\"]) or 0\n        if fundsHave <= fundsNeeded:\n            raise ValueError(\n                \"Not enough funds available. Need %f %s, but only %f %s are available\"\n                % (\n                    fundsNeeded,\n                    collateral_asset[\"symbol\"],\n                    fundsHave,\n                    collateral_asset[\"symbol\"],\n                )\n            )\n\n        payload = {\n            \"fee\": {\"amount\": 0, \"asset_id\": \"1.3.0\"},\n            \"delta_debt\": {\n                \"amount\": int(float(delta) * 10 ** asset[\"precision\"]),\n                \"asset_id\": asset[\"id\"],\n            },\n            \"delta_collateral\": {\n                \"amount\": int(\n                    float(amount_of_collateral) * 10 ** collateral_asset[\"precision\"]\n                ),\n                \"asset_id\": collateral_asset[\"id\"],\n            },\n            \"funding_account\": account[\"id\"],\n            \"extensions\": {},\n        }\n        # Extension\n        if target_collateral_ratio:\n            payload[\"extensions\"].update(\n                dict(target_collateral_ratio=int(target_collateral_ratio * 100))\n            )\n\n        op = operations.Call_order_update(**payload)\n        return self.blockchain.finalizeOp(op, account[\"name\"], \"active\")", "response": "Adjust the amount of debt for an asset based on the amount of collateral"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadjust the collataral ratio of a specific asset", "response": "def adjust_collateral_ratio(\n        self, symbol, new_collateral_ratio, account=None, target_collateral_ratio=None\n    ):\n        \"\"\" Adjust the collataral ratio of a debt position\n\n            :param Asset amount: Amount to borrow (denoted in 'asset')\n            :param float new_collateral_ratio: desired collateral ratio\n            :param float target_collateral_ratio: Tag the call order so that in\n                case of margin call, only enough debt is covered to get back to\n                this ratio\n            :raises ValueError: if symbol is not a bitasset\n            :raises ValueError: if collateral ratio is smaller than maintenance collateral ratio\n            :raises ValueError: if required amounts of collateral are not available\n        \"\"\"\n        if not account:\n            if \"default_account\" in self.blockchain.config:\n                account = self.blockchain.config[\"default_account\"]\n        if not account:\n            raise ValueError(\"You need to provide an account\")\n        account = Account(account, full=True, blockchain_instance=self.blockchain)\n        current_debts = self.list_debt_positions(account)\n        if symbol not in current_debts:\n            raise ValueError(\n                \"No Call position available to adjust! Please borrow first!\"\n            )\n        return self.adjust_debt(\n            Amount(0, symbol),\n            new_collateral_ratio,\n            account,\n            target_collateral_ratio=target_collateral_ratio,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nborrowing bitassets from a CFD at a given collateral ratio.", "response": "def borrow(\n        self, amount, collateral_ratio=None, account=None, target_collateral_ratio=None\n    ):\n        \"\"\" Borrow bitassets/smartcoins from the network by putting up\n            collateral in a CFD at a given collateral ratio.\n\n            :param float amount: Amount to borrow (denoted in 'asset')\n            :param float collateral_ratio: Collateral ratio to borrow at\n            :param float target_collateral_ratio: Tag the call order so that in\n                case of margin call, only enough debt is covered to get back to\n                this ratio\n            :raises ValueError: if symbol is not a bitasset\n            :raises ValueError: if collateral ratio is smaller than maintenance collateral ratio\n            :raises ValueError: if required amounts of collateral are not available\n\n        \"\"\"\n        return self.adjust_debt(\n            amount,\n            collateral_ratio,\n            account,\n            target_collateral_ratio=target_collateral_ratio,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert an operation id into a corresponding string", "response": "def getOperationNameForId(i):\n    \"\"\" Convert an operation id into the corresponding string\n    \"\"\"\n    for key in operations:\n        if int(operations[key]) is int(i):\n            return key\n    return \"Unknown Operation ID %d\" % i"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nidentifies the connected network. This call returns a dictionary with keys chain_id core_symbol and prefix", "response": "def get_network(self):\n        \"\"\" Identify the connected network. This call returns a\n            dictionary with keys chain_id, core_symbol and prefix\n        \"\"\"\n        props = self.get_chain_properties()\n        chain_id = props[\"chain_id\"]\n        for k, v in known_chains.items():\n            if v[\"chain_id\"] == chain_id:\n                return v\n        raise exceptions.UnknownNetworkException(\n            \"Connecting to unknown network (chain_id: {})!\".format(props[\"chain_id\"])\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_account(self, name, **kwargs):\n        if len(name.split(\".\")) == 3:\n            return self.get_objects([name])[0]\n        else:\n            return self.get_account_by_name(name, **kwargs)", "response": "Get full account details from account name or id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_asset(self, name, **kwargs):\n        if len(name.split(\".\")) == 3:\n            return self.get_objects([name], **kwargs)[0]\n        else:\n            return self.lookup_asset_symbols([name], **kwargs)[0]", "response": "Get full asset from name of id\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_notice(self, notice):\n        id = notice[\"id\"]\n\n        _a, _b, _ = id.split(\".\")\n\n        if id in self.subscription_objects:\n            self.on_object(notice)\n\n        elif \".\".join([_a, _b, \"x\"]) in self.subscription_objects:\n            self.on_object(notice)\n\n        elif id[:4] == \"2.6.\":\n            # Treat account updates separately\n            self.on_account(notice)", "response": "This method is called when a notice is received from the system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self, *args, **kwargs):\n        self.run_event.set()\n        self.ws.close()\n\n        if self.keepalive and self.keepalive.is_alive():\n            self.keepalive.join()", "response": "Closes the websocket connection and waits for the ping thread to close\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rpcexec(self, payload):\n        log.debug(json.dumps(payload))\n        self.ws.send(json.dumps(payload, ensure_ascii=False).encode(\"utf8\"))", "response": "Execute a call by sending the payload to the server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nput the texts in items in an HTML table.", "response": "def text2html_table(items):\n    \"Put the texts in `items` in an HTML table.\"\n    html_code = f\"\"\"<table border=\"1\" class=\"dataframe\">\\n\"\"\"\n    html_code += f\"\"\"  <thead>\\n    <tr style=\"text-align: left;\">\\n\"\"\"\n    for i in items[0]: html_code += f\"      <th>{i}</th>\\n\"\n    html_code += f\"    </tr>\\n  </thead>\\n  <tbody>\\n\"\n    for line in items[1:]:\n        html_code += \"    <tr>\\n\"\n        for i in line: html_code += f\"      <td>{i}</td>\\n\"\n        html_code += \"    </tr>\\n\"\n    html_code += \"  </tbody>\\n</table><p>\"\n    return html_code"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring logging for the default node.", "response": "def configure(logdir, flush_secs=2):\n    \"\"\" Configure logging: a file will be written to logdir, and flushed\n    every flush_secs.\n    \"\"\"\n    global _default_logger\n    if _default_logger is not None:\n        raise ValueError('default logger already configured')\n    _default_logger = Logger(logdir, flush_secs=flush_secs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog a new value for given name on given step.", "response": "def log_value(self, name, value, step=None):\n        \"\"\"Log new value for given name on given step.\n\n        Args:\n            name (str): name of the variable (it will be converted to a valid\n                tensorflow summary name).\n            value (float): this is a real number to be logged as a scalar.\n            step (int): non-negative integer used for visualization: you can\n                log several different variables on one step, but should not log\n                different values of the same variable on the same step (this is\n                not checked).\n        \"\"\"\n        if isinstance(value, six.string_types):\n            raise TypeError('\"value\" should be a number, got {}'\n                            .format(type(value)))\n        value = float(value)\n\n        self._check_step(step)\n        tf_name = self._ensure_tf_name(name)\n\n        summary = self._scalar_summary(tf_name, value, step)\n        self._log_summary(tf_name, summary, value, step=step)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log_histogram(self, name, value, step=None):\n        if isinstance(value, six.string_types):\n            raise TypeError('\"value\" should be a number, got {}'\n                            .format(type(value)))\n\n        self._check_step(step)\n        tf_name = self._ensure_tf_name(name)\n\n        summary = self._histogram_summary(tf_name, value, step=step)\n        self._log_summary(tf_name, summary, value, step=step)", "response": "Log a histogram for given name on given step."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog new images for given name on given step.", "response": "def log_images(self, name, images, step=None):\n        \"\"\"Log new images for given name on given step.\n\n        Args:\n            name (str): name of the variable (it will be converted to a valid\n                tensorflow summary name).\n            images (list): list of images to visualize\n            step (int): non-negative integer used for visualization\n        \"\"\"\n        if isinstance(images, six.string_types):\n            raise TypeError('\"images\" should be a list of ndarrays, got {}'\n                            .format(type(images)))\n\n        self._check_step(step)\n        tf_name = self._ensure_tf_name(name)\n\n        summary = self._image_summary(tf_name, images, step=step)\n        self._log_summary(tf_name, summary, images, step=step)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a summary for a list of images.", "response": "def _image_summary(self, tf_name, images, step=None):\n        \"\"\"\n        Log a list of images.\n\n        References:\n            https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/tensorboard/logger.py#L22\n\n        Example:\n            >>> tf_name = 'foo'\n            >>> value = ([0, 1, 2, 3, 4, 5], [1, 20, 10, 22, 11])\n            >>> self = Logger(None, is_dummy=True)\n            >>> images = [np.random.rand(10, 10), np.random.rand(10, 10)]\n            >>> summary = self._image_summary(tf_name, images, step=None)\n            >>> assert len(summary.value) == 2\n            >>> assert summary.value[0].image.width == 10\n        \"\"\"\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=\"png\")\n\n            # Create an Image object\n            img_sum = summary_pb2.Summary.Image(\n                encoded_image_string=s.getvalue(),\n                height=img.shape[0],\n                width=img.shape[1]\n            )\n            # Create a Summary value\n            img_value = summary_pb2.Summary.Value(tag='{}/{}'.format(tf_name, i),\n                                                  image=img_sum)\n            img_summaries.append(img_value)\n            summary = summary_pb2.Summary()\n            summary.value.add(tag=tf_name, image=img_sum)\n\n        summary = summary_pb2.Summary(value=img_summaries)\n        return summary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction that creates a histogram summary for a single variable.", "response": "def _histogram_summary(self, tf_name, value, step=None):\n        \"\"\"\n        Args:\n            tf_name (str): name of tensorflow variable\n            value (tuple or list): either a tuple of bin_edges and bincounts or\n                a list of values to summarize in a histogram.\n\n        References:\n            https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/tensorboard/logger.py#L45\n\n        Example:\n            >>> tf_name = 'foo'\n            >>> value = ([0, 1, 2, 3, 4, 5], [1, 20, 10, 22, 11])\n            >>> self = Logger(None, is_dummy=True)\n            >>> summary = self._histogram_summary(tf_name, value, step=None)\n            >>> assert summary.value[0].histo.max == 5\n\n        Example:\n            >>> tf_name = 'foo'\n            >>> value = [0.72,  0.18,  0.34,  0.66,  0.11,  0.70,  0.23]\n            >>> self = Logger(None, is_dummy=True)\n            >>> summary = self._histogram_summary(tf_name, value, step=None)\n            >>> assert summary.value[0].histo.num == 7.0\n        \"\"\"\n        if isinstance(value, tuple):\n            bin_edges, bincounts = value\n            assert len(bin_edges) == len(bincounts) + 1, (\n                'must have one more edge than count')\n            hist = summary_pb2.HistogramProto()\n            hist.min = float(min(bin_edges))\n            hist.max = float(max(bin_edges))\n        else:\n            values = np.array(value)\n\n            bincounts, bin_edges = np.histogram(values)\n\n            hist = summary_pb2.HistogramProto()\n            hist.min = float(np.min(values))\n            hist.max = float(np.max(values))\n            hist.num = int(np.prod(values.shape))\n            hist.sum = float(np.sum(values))\n            hist.sum_squares = float(np.sum(values**2))\n\n        # Add bin edges and counts\n        for edge in bin_edges[1:]:\n            hist.bucket_limit.append(edge)\n        for v in bincounts:\n            hist.bucket.append(v)\n\n        summary = summary_pb2.Summary()\n        summary.value.add(tag=tf_name, histo=hist)\n        return summary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a parameter value to a string suitable for use in Jinja2 tool.", "response": "def jinja_filter_param_value_str(value, str_quote_style=\"\", bool_is_str=False):\n    \"\"\" Convert a parameter value to string suitable to be passed to an EDA tool\n\n    Rules:\n    - Booleans are represented as 0/1 or \"true\"/\"false\" depending on the\n      bool_is_str argument\n    - Strings are either passed through or enclosed in the characters specified\n      in str_quote_style (e.g. '\"' or '\\\\\"')\n    - Everything else (including int, float, etc.) are converted using the str()\n      function.\n    \"\"\"\n    if (type(value) == bool) and not bool_is_str:\n        if (value) == True:\n            return '1'\n        else:\n            return '0'\n    elif type(value) == str or ((type(value) == bool) and bool_is_str):\n        return str_quote_style + str(value) + str_quote_style\n    else:\n        return str(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders a Jinja2 template for the backend", "response": "def render_template(self, template_file, target_file, template_vars = {}):\n        \"\"\"\n        Render a Jinja2 template for the backend\n\n        The template file is expected in the directory templates/BACKEND_NAME.\n        \"\"\"\n        template_dir = str(self.__class__.__name__).lower()\n        template = self.jinja_env.get_template(os.path.join(template_dir, template_file))\n        file_path = os.path.join(self.work_root, target_file)\n        with open(file_path, 'w') as f:\n            f.write(template.render(template_vars))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery the number of target items in a sequence", "response": "def query_target_count(self, target):\n    \"\"\"Return the target count\"\"\"\n    reply = NVCtrlQueryTargetCountReplyRequest(display=self.display,\n                                               opcode=self.display.get_extension_major(extname),\n                                               target_type=target.type())\n    return int(reply._data.get('count'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query_int_attribute(self, target, display_mask, attr):\n    reply = NVCtrlQueryAttributeReplyRequest(display=self.display,\n                                             opcode=self.display.get_extension_major(extname),\n                                             target_id=target.id(),\n                                             target_type=target.type(),\n                                             display_mask=display_mask,\n                                             attr=attr)\n    if not reply._data.get('flags'):\n        return None\n    return int(reply._data.get('value'))", "response": "Query the value of an integer attribute"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_int_attribute(self, target, display_mask, attr, value):\n    reply = NVCtrlSetAttributeAndGetStatusReplyRequest(display=self.display,\n                                                       opcode=self.display.get_extension_major(extname),\n                                                       target_id=target.id(),\n                                                       target_type=target.type(),\n                                                       display_mask=display_mask,\n                                                       attr=attr,\n                                                       value=value)\n    return reply._data.get('flags') != 0", "response": "Set the value of an integer attribute"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query_string_attribute(self, target, display_mask, attr):\n    reply = NVCtrlQueryStringAttributeReplyRequest(display=self.display,\n                                                   opcode=self.display.get_extension_major(extname),\n                                                   target_id=target.id(),\n                                                   target_type=target.type(),\n                                                   display_mask=display_mask,\n                                                   attr=attr)\n    if not reply._data.get('flags'):\n        return None\n    return str(reply._data.get('string')).strip('\\0')", "response": "Query the value of a string attribute"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query_binary_data(self, target, display_mask, attr):\n    reply = NVCtrlQueryBinaryDataReplyRequest(display=self.display,\n                                              opcode=self.display.get_extension_major(extname),\n                                              target_id=target.id(),\n                                              target_type=target.type(),\n                                              display_mask=display_mask,\n                                              attr=attr)\n    if not reply._data.get('flags'):\n        return None\n    return reply._data.get('data')", "response": "Query binary data from the server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a display number from a string", "response": "def _displaystr2num(st):\n    \"\"\"Return a display number from a string\"\"\"\n    num = None\n    for s, n in [('DFP-', 16), ('TV-', 8), ('CRT-', 0)]:\n        if st.startswith(s):\n            try:\n                curnum = int(st[len(s):])\n                if 0 <= curnum <= 7:\n                    num = n + curnum\n                    break\n            except Exception:\n                pass\n    if num is not None:\n        return num\n    else:\n        raise ValueError('Unrecognised display name: ' + st)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a resource object of the specified type and id.", "response": "def create_resource_object(self, type, id):\n        \"\"\"Create a resource object of type for the integer id. type\n        should be one of the following strings:\n\n        resource\n        drawable\n        window\n        pixmap\n        fontable\n        font\n        gc\n        colormap\n        cursor\n\n        This function can be used when a resource ID has been fetched\n        e.g. from an resource or a command line argument. Resource\n        objects should never be created by instantiating the appropriate\n        class directly, since any X extensions dynamically added by the\n        library will not be available.\n        \"\"\"\n        return self.display.resource_classes[type](self.display, id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an X extension module method to the extension module.", "response": "def extension_add_method(self, object, name, function):\n        \"\"\"extension_add_method(object, name, function)\n\n        Add an X extension module method.  OBJECT is the type of\n        object to add the function to, a string from this list:\n\n            display\n            resource\n            drawable\n            window\n            pixmap\n            fontable\n            font\n            gc\n            colormap\n            cursor\n\n        NAME is the name of the method, a string.  FUNCTION is a\n        normal function whose first argument is a 'self'.\n        \"\"\"\n\n        if object == 'display':\n            if hasattr(self, name):\n                raise AssertionError('attempting to replace display method: %s' % name)\n\n            self.display_extension_methods[name] = function\n\n        else:\n            class_list = (object, ) + _resource_hierarchy.get(object, ())\n            for class_name in class_list:\n                cls = _resource_baseclasses[class_name]\n                if hasattr(cls, name):\n                    raise AssertionError('attempting to replace %s method: %s' % (class_name, name))\n\n                method = create_unbound_method(function, cls)\n\n                # Maybe should check extension overrides too\n                try:\n                    self.class_extension_dicts[class_name][name] = method\n                except KeyError:\n                    self.class_extension_dicts[class_name] = { name: method }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extension_add_event(self, code, evt, name = None):\n\n        newevt = type(evt.__name__, evt.__bases__,\n                      evt.__dict__.copy())\n        newevt._code = code\n\n        self.display.add_extension_event(code, newevt)\n\n        if name is None:\n            name = evt.__name__\n\n        setattr(self.extension_event, name, code)", "response": "Add an extension event to the display."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keycode_to_keysym(self, keycode, index):\n        try:\n            return self._keymap_codes[keycode][index]\n        except IndexError:\n            return X.NoSymbol", "response": "Convert a keycode to a keysym looking in the keymap."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlook up all the keycodes that are bound to keysym. A list of tuples is returned sorted primarily on the lowest index and secondarily on the lowest keycode.", "response": "def keysym_to_keycodes(self, keysym):\n        \"\"\"Look up all the keycodes that is bound to keysym. A list of\n        tuples (keycode, index) is returned, sorted primarily on the\n        lowest index and secondarily on the lowest keycode.\"\"\"\n        try:\n            # Copy the map list, reversing the arguments\n            return map(lambda x: (x[1], x[0]), self._keymap_syms[keysym])\n        except KeyError:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef refresh_keyboard_mapping(self, evt):\n        if isinstance(evt, event.MappingNotify):\n            if evt.request == X.MappingKeyboard:\n                self._update_keymap(evt.first_keycode, evt.count)\n        else:\n            raise TypeError('expected a MappingNotify event')", "response": "This method is called once when a MappingNotify event is received to update the keyboard mapping cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_keymap(self, first_keycode, count):\n\n        # Delete all sym->code maps for the changed codes\n\n        lastcode = first_keycode + count\n        for keysym, codes in self._keymap_syms.items():\n            i = 0\n            while i < len(codes):\n                code = codes[i][1]\n                if code >= first_keycode and code < lastcode:\n                    del codes[i]\n                else:\n                    i = i + 1\n\n        # Get the new keyboard mapping\n        keysyms = self.get_keyboard_mapping(first_keycode, count)\n\n        # Replace code->sym map with the new map\n        self._keymap_codes[first_keycode:lastcode] = keysyms\n\n        # Update sym->code map\n        code = first_keycode\n        for syms in keysyms:\n            index = 0\n            for sym in syms:\n                if sym != X.NoSymbol:\n                    if sym in self._keymap_syms:\n                        symcodes = self._keymap_syms[sym]\n                        symcodes.append((index, code))\n                        symcodes.sort()\n                    else:\n                        self._keymap_syms[sym] = [(index, code)]\n\n                index = index + 1\n            code = code + 1", "response": "Internal function to update the keymap cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lookup_string(self, keysym):\n        s = self.keysym_translations.get(keysym)\n        if s is not None:\n            return s\n\n        import Xlib.XK\n        return Xlib.XK.keysym_to_string(keysym)", "response": "Return a string corresponding to KEYSYM or None if no such translation is found."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rebind_string(self, keysym, newstring):\n        if newstring is None:\n            try:\n                del self.keysym_translations[keysym]\n            except KeyError:\n                pass\n        else:\n            self.keysym_translations[keysym] = newstring", "response": "Change the translation of KEYSYM to NEWSTRING."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intern_atom(self, name, only_if_exists = 0):\n        r = request.InternAtom(display = self.display,\n                               name = name,\n                               only_if_exists = only_if_exists)\n        return r.atom", "response": "Intern the string name returning its atom number. If only_if_exists is true the atom number will not be returned. If only_if_exists is false the atom number will not be returned."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooking up the name of atom returning it as a string. Will raise a BadAtom if the atom does not exist. Will raise a BadAtom if the atom does not exist. Will raise a BadAtom if the atom does not exist.", "response": "def get_atom_name(self, atom):\n        \"\"\"Look up the name of atom, returning it as a string. Will raise\n        BadAtom if atom does not exist.\"\"\"\n        r = request.GetAtomName(display = self.display,\n                                atom = atom)\n        return r.name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the window that owns the selection. Can raise BadAtom.", "response": "def get_selection_owner(self, selection):\n        \"\"\"Return the window that owns selection (an atom), or X.NONE if\n        there is no owner for the selection. Can raise BadAtom.\"\"\"\n        r = request.GetSelectionOwner(display = self.display,\n                                      selection = selection)\n        return r.owner"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_event(self, destination, event, event_mask = 0, propagate = 0,\n                   onerror = None):\n        \"\"\"Send a synthetic event to the window destination which can be\n        a window object, or X.PointerWindow or X.InputFocus. event is the\n        event object to send, instantiated from one of the classes in\n        protocol.events. See XSendEvent(3X11) for details.\n\n        There is also a Window.send_event() method.\"\"\"\n        request.SendEvent(display = self.display,\n                          onerror = onerror,\n                          propagate = propagate,\n                          destination = destination,\n                          event_mask = event_mask,\n                          event = event)", "response": "Send an event to the window destination."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrelease a grabbed pointer and any queued events. See XUngrabPointer ( 3X11 ).", "response": "def ungrab_pointer(self, time, onerror = None):\n        \"\"\"Release a grabbed pointer and any queued events. See\n        XUngrabPointer(3X11).\"\"\"\n        request.UngrabPointer(display = self.display,\n                              onerror = onerror,\n                              time = time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_active_pointer_grab(self, event_mask, cursor, time, onerror = None):\n        request.ChangeActivePointerGrab(display = self.display,\n                                        onerror = onerror,\n                                        cursor = cursor,\n                                        time = time,\n                                        event_mask = event_mask)", "response": "Change the dynamic parameters of a pointer grab. See\n        XChangeActivePointerGrab ( 3X11 )."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ungrab_keyboard(self, time, onerror = None):\n        request.UngrabKeyboard(display = self.display,\n                               onerror = onerror,\n                               time = time)", "response": "Ungrab a grabbed keyboard and any queued events. See\n        XUngrabKeyboard ( 3X11 )."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef allow_events(self, mode, time, onerror = None):\n        request.AllowEvents(display = self.display,\n                            onerror = onerror,\n                            mode = mode,\n                            time = time)", "response": "Release some queued events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling processing of requests on all other client connections until the server is ungrabbed.", "response": "def grab_server(self, onerror = None):\n        \"\"\"Disable processing of requests on all other client connections\n        until the server is ungrabbed. Server grabbing should be avoided\n        as much as possible.\"\"\"\n        request.GrabServer(display = self.display,\n                           onerror = onerror)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrelease the server if it was previously grabbed by this client.", "response": "def ungrab_server(self, onerror = None):\n        \"\"\"Release the server if it was previously grabbed by this client.\"\"\"\n        request.UngrabServer(display = self.display,\n                             onerror = onerror)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef warp_pointer(self, x, y, src_window = X.NONE, src_x = 0, src_y = 0,\n                     src_width = 0, src_height = 0, onerror = None):\n        \"\"\"Move the pointer relative its current position by the offsets\n        (x, y). However, if src_window is a window the pointer is only\n        moved if the specified rectangle in src_window contains it. If\n        src_width is 0 it will be replaced with the width of src_window -\n        src_x. src_height is treated in a similar way.\n\n        To move the pointer to absolute coordinates, use Window.warp_pointer().\"\"\"\n        request.WarpPointer(display = self.display,\n                            onerror = onerror,\n                            src_window = src_window,\n                            dst_window = X.NONE,\n                            src_x = src_x,\n                            src_y = src_y,\n                            src_width = src_width,\n                            src_height = src_height,\n                            dst_x = x,\n                            dst_y = y)", "response": "Move the pointer relative to the current position by the offsets\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting input focus to focus which should be a window or X. PointerRoot or X. NONE.", "response": "def set_input_focus(self, focus, revert_to, time, onerror = None):\n        \"\"\"Set input focus to focus, which should be a window,\n        X.PointerRoot or X.NONE. revert_to specifies where the focus\n        reverts to if the focused window becomes not visible, and should\n        be X.RevertToParent, RevertToPointerRoot, or RevertToNone. See\n        XSetInputFocus(3X11) for details.\n\n        There is also a Window.set_input_focus().\"\"\"\n        request.SetInputFocus(display = self.display,\n                              onerror = onerror,\n                              revert_to = revert_to,\n                              focus = focus,\n                              time = time)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query_keymap(self):\n        r = request.QueryKeymap(display = self.display)\n        return r.map", "response": "Return a bit vector for the logical state of the keyboard."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef open_font(self, name):\n        fid = self.display.allocate_resource_id()\n        ec = error.CatchError(error.BadName)\n\n        request.OpenFont(display = self.display,\n                         onerror = ec,\n                         fid = fid,\n                         name = name)\n        self.sync()\n\n        if ec.get_error():\n            self.display.free_resource_id(fid)\n            return None\n        else:\n            cls = self.display.get_resource_class('font', fontable.Font)\n            return cls(self.display, fid, owner = 1)", "response": "Open the font identifed by the pattern name and return its\n        font object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of font names matching pattern. No more than max_names will be returned.", "response": "def list_fonts(self, pattern, max_names):\n        \"\"\"Return a list of font names matching pattern. No more than\n        max_names will be returned.\"\"\"\n        r = request.ListFonts(display = self.display,\n                              max_names = max_names,\n                              pattern = pattern)\n        return r.fonts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_fonts_with_info(self, pattern, max_names):\n        return request.ListFontsWithInfo(display = self.display,\n                                         max_names = max_names,\n                                         pattern = pattern)", "response": "Return a list of fonts matching pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_font_path(self, path, onerror = None):\n        request.SetFontPath(display = self.display,\n                            onerror = onerror,\n                            path = path)", "response": "Set the font path to path which should be a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_font_path(self):\n        r = request.GetFontPath(display = self.display)\n        return r.paths", "response": "Return the current font path as a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nasking the server if it supports the extension name.", "response": "def query_extension(self, name):\n        \"\"\"Ask the server if it supports the extension name. If it is\n        supported an object with the following attributes is returned:\n\n        major_opcode\n            The major opcode that the requests of this extension uses.\n        first_event\n            The base event code if the extension have additional events, or 0.\n        first_error\n            The base error code if the extension have additional errors, or 0.\n\n        If the extension is not supported, None is returned.\"\"\"\n        r = request.QueryExtension(display = self.display,\n                                   name = name)\n        if r.present:\n            return r\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all the extensions provided by the server.", "response": "def list_extensions(self):\n        \"\"\"Return a list of all the extensions provided by the server.\"\"\"\n        r = request.ListExtensions(display = self.display)\n        return r.names"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies the keyboard mapping starting with first_keycode.", "response": "def change_keyboard_mapping(self, first_keycode, keysyms, onerror = None):\n        \"\"\"Modify the keyboard mapping, starting with first_keycode.\n        keysyms is a list of tuples of keysyms. keysyms[n][i] will be\n        assigned to keycode first_keycode+n at index i.\"\"\"\n        request.ChangeKeyboardMapping(display = self.display,\n                                      onerror = onerror,\n                                      first_keycode = first_keycode,\n                                      keysyms = keysyms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_keyboard_mapping(self, first_keycode, count):\n        r = request.GetKeyboardMapping(display = self.display,\n                                       first_keycode = first_keycode,\n                                       count = count)\n        return r.keysyms", "response": "Return the current keyboard mapping as a list of tuples starting at first_keycount and no more than count."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef change_keyboard_control(self, onerror = None, **keys):\n        request.ChangeKeyboardControl(display = self.display,\n                                      onerror = onerror,\n                                      attrs = keys)", "response": "Change the state of the keyboard control."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nring the bell at the volume percent.", "response": "def bell(self, percent = 0, onerror = None):\n        \"\"\"Ring the bell at the volume percent which is relative the base\n        volume. See XBell(3X11).\"\"\"\n        request.Bell(display = self.display,\n                     onerror = onerror,\n                     percent = percent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_pointer_control(self, accel = None, threshold = None, onerror = None):\n        if accel is None:\n            do_accel = 0\n            accel_num = 0\n            accel_denum = 0\n        else:\n            do_accel = 1\n            accel_num, accel_denum = accel\n\n        if threshold is None:\n            do_threshold = 0\n        else:\n            do_threshold = 1\n\n        request.ChangePointerControl(display = self.display,\n                                     onerror = onerror,\n                                     do_accel = do_accel,\n                                     do_thresh = do_threshold,\n                                     accel_num = accel_num,\n                                     accel_denum = accel_denum,\n                                     threshold = threshold)", "response": "Change the pointer control of the current resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the screen saver for this application.", "response": "def set_screen_saver(self, timeout, interval, prefer_blank, allow_exposures, onerror = None):\n        \"\"\"See XSetScreenSaver(3X11).\"\"\"\n        request.SetScreenSaver(display = self.display,\n                               onerror = onerror,\n                               timeout = timeout,\n                               interval = interval,\n                               prefer_blank = prefer_blank,\n                               allow_exposures = allow_exposures)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_hosts(self, mode, host_family, host, onerror = None):\n        request.ChangeHosts(display = self.display,\n                            onerror = onerror,\n                            mode = mode,\n                            host_family = host_family,\n                            host = host)", "response": "Change the hosts of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling use of access control lists at connection setup if mode is X. EnableAccess disable access control lists at connection setup if mode is X. DisableAccess.", "response": "def set_access_control(self, mode, onerror = None):\n        \"\"\"Enable use of access control lists at connection setup if mode\n        is X.EnableAccess, disable if it is X.DisableAccess.\"\"\"\n        request.SetAccessControl(display = self.display,\n                                 onerror = onerror,\n                                 mode = mode)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncontrolling what will happen with the client s resources at connection close. The default is DestroyAll. The other values are X. RetainPermanent and X. RetainTemporary.", "response": "def set_close_down_mode(self, mode, onerror = None):\n        \"\"\"Control what will happen with the client's resources at\n        connection close. The default is X.DestroyAll, the other values\n        are X.RetainPermanent and X.RetainTemporary.\"\"\"\n        request.SetCloseDownMode(display = self.display,\n                                 onerror = onerror,\n                                 mode = mode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nforcing the screen saver to be used.", "response": "def force_screen_saver(self, mode, onerror = None):\n        \"\"\"If mode is X.ScreenSaverActive the screen saver is activated.\n        If it is X.ScreenSaverReset, the screen saver is deactivated as\n        if device input had been received.\"\"\"\n        request.ForceScreenSaver(display = self.display,\n                                 onerror = onerror,\n                                 mode = mode)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_pointer_mapping(self, map):\n        r = request.SetPointerMapping(display = self.display,\n                                      map = map)\n        return r.status", "response": "Sets the mapping of the pointer buttons."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pointer_mapping(self):\n        r = request.GetPointerMapping(display = self.display)\n        return r.map", "response": "Return a list of the pointer button mappings. Entry N in the the\n        list sets the logical button number for the physical button N + 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_modifier_mapping(self, keycodes):\n        r = request.SetModifierMapping(display = self.display,\n                                       keycodes = keycodes)\n        return r.status", "response": "Set the keycodes for the eight modifiers X. Shift X. Lock X. Control X. Mod1 X. Mod2 X. Mod3 X. Mod4 and X. Mod5."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_modifier_mapping(self):\n        r = request.GetModifierMapping(display = self.display)\n        return r.keycodes", "response": "Return a list of eight lists one for each modifier. The list\n        can be indexed using ShiftMapIndex Mod1MapIndex and so on. The list\n        can be indexed using Mod1MapIndex."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef no_operation(self, onerror = None):\n        request.NoOperation(display = self.display,\n                            onerror = onerror)", "response": "Do nothing but send a request to the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nallocating a new X resource id number.", "response": "def allocate_resource_id(self):\n        \"\"\"id = d.allocate_resource_id()\n\n        Allocate a new X resource id number ID.\n\n        Raises ResourceIDError if there are no free resource ids.\n        \"\"\"\n\n        self.resource_id_lock.acquire()\n        try:\n            i = self.last_resource_id\n            while i in self.resource_ids:\n                i = i + 1\n                if i > self.info.resource_id_mask:\n                    i = 0\n                if i == self.last_resource_id:\n                    raise error.ResourceIDError('out of resource ids')\n\n            self.resource_ids[i] = None\n            self.last_resource_id = i\n            return self.info.resource_id_base | i\n        finally:\n            self.resource_id_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef free_resource_id(self, rid):\n\n        self.resource_id_lock.acquire()\n        try:\n            i = rid & self.info.resource_id_mask\n\n            # Attempting to free a resource id outside our range\n            if rid - i != self.info.resource_id_base:\n                return None\n\n            try:\n                del self.resource_ids[i]\n            except KeyError:\n                pass\n        finally:\n            self.resource_id_lock.release()", "response": "Free a resource id which is not in use by us."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_response(self, request):\n\n        if request == -1:\n            return self.parse_connection_setup()\n\n        # Parse ordinary server response\n        gotreq = 0\n        while 1:\n            if self.data_recv:\n                # Check the first byte to find out what kind of response it is\n                rtype = byte2int(self.data_recv)\n\n            # Are we're waiting for additional data for the current packet?\n            if self.recv_packet_len:\n                if len(self.data_recv) < self.recv_packet_len:\n                    return gotreq\n\n                if rtype == 1:\n                    gotreq = self.parse_request_response(request) or gotreq\n                    continue\n                elif rtype & 0x7f == ge.GenericEventCode:\n                    self.parse_event_response(rtype)\n                    continue\n                else:\n                    raise AssertionError(rtype)\n\n            # Every response is at least 32 bytes long, so don't bother\n            # until we have received that much\n            if len(self.data_recv) < 32:\n                return gotreq\n\n            # Error response\n            if rtype == 0:\n                gotreq = self.parse_error_response(request) or gotreq\n\n            # Request response or generic event.\n            elif rtype == 1 or rtype & 0x7f == ge.GenericEventCode:\n                # Set reply length, and loop around to see if\n                # we have got the full response\n                rlen = int(struct.unpack('=L', self.data_recv[4:8])[0])\n                self.recv_packet_len = 32 + rlen * 4\n\n            # Else non-generic event\n            else:\n                self.parse_event_response(rtype)", "response": "Internal method to parse the response from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_display(display):\n\n    modname = _display_mods.get(platform, _default_display_mod)\n    mod = _relative_import(modname)\n    return mod.get_display(display)", "response": "Parse a display name protocol host dno screen number and return the display object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a socket object connected to the display specified by DNAME PROTOCOL HOST and DNO.", "response": "def get_socket(dname, protocol, host, dno):\n    \"\"\"socket = get_socket(dname, protocol, host, dno)\n\n    Connect to the display specified by DNAME, PROTOCOL, HOST and DNO, which\n    are the corresponding values from a previous call to get_display().\n\n    Return SOCKET, a new socket object connected to the X server.\n    \"\"\"\n\n    modname = _socket_mods.get(platform, _default_socket_mod)\n    mod = _relative_import(modname)\n    return mod.get_socket(dname, protocol, host, dno)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting authentication data for the given SOCK.", "response": "def get_auth(sock, dname, protocol, host, dno):\n    \"\"\"auth_name, auth_data = get_auth(sock, dname, protocol, host, dno)\n\n    Return authentication data for the display on the other side of\n    SOCK, which was opened with DNAME, HOST and DNO, using PROTOCOL.\n\n    Return AUTH_NAME and AUTH_DATA, two strings to be used in the\n    connection setup request.\n    \"\"\"\n\n    modname = _auth_mods.get(platform, _default_auth_mod)\n    mod = _relative_import(modname)\n    return mod.get_auth(sock, dname, protocol, host, dno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef redirect_subwindows(self, update, onerror = None):\n    RedirectSubwindows(display = self.display,\n                       onerror = onerror,\n                       opcode = self.display.get_extension_major(extname),\n                       window = self,\n                       update = update,\n                       )", "response": "Redirect the hierarchies starting at all current and future\n    children to this window to off - screen storage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop redirecting this window hierarchy.", "response": "def unredirect_window(self, update, onerror = None):\n    \"\"\"Stop redirecting this window hierarchy.\n    \"\"\"\n    UnredirectWindow(display = self.display,\n                     onerror = onerror,\n                     opcode = self.display.get_extension_major(extname),\n                     window = self,\n                     update = update,\n                     )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unredirect_subwindows(self, update, onerror = None):\n    RedirectWindow(display = self.display,\n                   onerror = onerror,\n                   opcode = self.display.get_extension_major(extname),\n                   window = self,\n                   update = update,\n                   )", "response": "Stop redirecting the hierarchies of children to this window."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_region_from_border_clip(self, onerror = None):\n\n    rid = self.display.allocate_resource_id()\n    CreateRegionFromBorderClip(\n        display = self.display,\n        onerror = onerror,\n        opcode = self.display.get_extension_major(extname),\n        region = rid,\n        window = self,\n        )\n\n    # FIXME: create Region object and return it\n    return rid", "response": "Create a region of the border clip of the window."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new pixmap that refers to the off - screen storage of the window.", "response": "def name_window_pixmap(self, onerror = None):\n    \"\"\"Create a new pixmap that refers to the off-screen storage of\n    the window, including its border.\n\n    This pixmap will remain allocated until freed whatever happens\n    with the window.  However, the window will get a new off-screen\n    pixmap every time it is mapped or resized, so to keep track of the\n    contents you must listen for these events and get a new pixmap\n    after them.\n    \"\"\"\n\n    pid = self.display.allocate_resource_id()\n    NameWindowPixmap(display = self.display,\n                     onerror = onerror,\n                     opcode = self.display.get_extension_major(extname),\n                     window = self,\n                     pixmap = pid,\n                     )\n\n    cls = self.display.get_resource_class('pixmap', drawable.Pixmap)\n    return cls(self.display, pid, owner = 1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_overlay_window(self):\n\n    return GetOverlayWindow(display = self.display,\n                              opcode = self.display.get_extension_major(extname),\n                              window = self)", "response": "Return the overlay window of the root window."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_events(self, event_masks):\n    '''\n    select_events(event_masks)\n\n    event_masks:\n      Sequence of (deviceid, mask) pairs, where deviceid is a numerical device\n      ID, or AllDevices or AllMasterDevices, and mask is either an unsigned\n      integer or sequence of 32 bits unsigned values\n    '''\n    return XISelectEvents(\n        display=self.display,\n        opcode=self.display.get_extension_major(extname),\n        window=self,\n        masks=event_masks,\n    )", "response": "Select events from the current window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting 8 - byte string into 16 - byte list.", "response": "def pack_value(self, val):\n        \"\"\"Convert 8-byte string into 16-byte list\"\"\"\n        if isinstance(val, bytes):\n            val = list(iterbytes(val))\n\n        slen = len(val)\n\n        if self.pad:\n            pad = b'\\0\\0' * (slen % 2)\n        else:\n            pad = b''\n\n        return struct.pack('>' + 'H' * slen, *val) + pad, slen, None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the internal structure into a binary representation.", "response": "def to_binary(self, *varargs, **keys):\n        \"\"\"data = s.to_binary(...)\n\n        Convert Python values into the binary representation.  The\n        arguments will be all value fields with names, in the order\n        given when the Struct object was instantiated.  With one\n        exception: fields with default arguments will be last.\n\n        Returns the binary representation as the string DATA.\n        \"\"\"\n        # Emulate Python function argument handling with our field names\n        names = [f.name for f in self.fields \\\n                 if isinstance(f, ValueField) and f.name]\n        field_args = dict(zip(names, varargs))\n        if set(field_args).intersection(keys):\n            dupes = \", \".join(set(field_args).intersection(keys))\n            raise TypeError(\"{0} arguments were passed both positionally and by keyword\".format(dupes))\n        field_args.update(keys)\n        for f in self.fields:\n            if f.name and (f.name not in field_args):\n                if f.default is None:\n                    raise TypeError(\"Missing required argument {0}\".format(f.name))\n                field_args[f.name] = f.default\n        # /argument handling\n\n        # First pack all varfields so their lengths and formats are\n        # available when we pack their static LengthFields and\n        # FormatFields\n\n        total_length = self.static_size\n        var_vals = {}\n        lengths = {}\n        formats = {}\n\n        for f in self.var_fields:\n            if f.keyword_args:\n                v, l, fm = f.pack_value(field_args[f.name], keys)\n            else:\n                v, l, fm = f.pack_value(field_args[f.name])\n            var_vals[f.name] = v\n            lengths[f.name] = l\n            formats[f.name] = fm\n\n            total_length += len(v)\n\n\n        # Construct item list for struct.pack call, packing all static fields.\n        pack_items = []\n\n        for f in self.static_fields:\n            if isinstance(f, LengthField):\n\n                # If this is a total length field, insert\n                # the calculated field value here\n                if isinstance(f, TotalLengthField):\n                    pack_items.append(f.calc_length(total_length))\n                else:\n                    pack_items.append(f.calc_length(lengths[f.name]))\n\n            # Format field, just insert the value we got previously\n            elif isinstance(f, FormatField):\n                pack_items.append(formats[f.name])\n\n            # A constant field, insert its value directly\n            elif isinstance(f, ConstantField):\n                pack_items.append(f.value)\n\n            # Value fields\n            else:\n                if f.structvalues == 1:\n                    # If there's a value check/convert function, call it\n                    if f.check_value is not None:\n                        pack_items.append(f.check_value(field_args[f.name]))\n                    # Else just use the argument as provided\n                    else:\n                        pack_items.append(field_args[f.name])\n\n                # Multivalue field.  Handled like single valuefield,\n                # but the value are tuple unpacked into separate arguments\n                # which are appended to pack_items\n                else:\n                    if f.check_value is not None:\n                        pack_items.extend(f.check_value(field_args[f.name]))\n                    else:\n                        pack_items.extend(field_args[f.name])\n\n        static_part = struct.pack(self.static_codes, *pack_items)\n        var_parts = [var_vals[f.name] for f in self.var_fields]\n        return static_part + b''.join(var_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npack the value into binary form.", "response": "def pack_value(self, value):\n\n        \"\"\" This function allows Struct objects to be used in List and\n        Object fields.  Each item represents the arguments to pass to\n        to_binary, either a tuple, a dictionary or a DictWrapper.\n\n        \"\"\"\n\n        if type(value) is tuple:\n            return self.to_binary(*value)\n        elif isinstance(value, dict):\n            return self.to_binary(**value)\n        elif isinstance(value, DictWrapper):\n            return self.to_binary(**value._data)\n        else:\n            raise BadDataError('%s is not a tuple or a list' % (value))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_binary(self, data, display, rawdict = 0):\n\n        \"\"\"values, remdata = s.parse_binary(data, display, rawdict = 0)\n\n        Convert a binary representation of the structure into Python values.\n\n        DATA is a string or a buffer containing the binary data.\n        DISPLAY should be a Xlib.protocol.display.Display object if\n        there are any Resource fields or Lists with ResourceObjs.\n\n        The Python values are returned as VALUES.  If RAWDICT is true,\n        a Python dictionary is returned, where the keys are field\n        names and the values are the corresponding Python value.  If\n        RAWDICT is false, a DictWrapper will be returned where all\n        fields are available as attributes.\n\n        REMDATA are the remaining binary data, unused by the Struct object.\n\n        \"\"\"\n        ret = {}\n        val = struct.unpack(self.static_codes, data[:self.static_size])\n        lengths = {}\n        formats = {}\n\n        vno = 0\n        for f in self.static_fields:\n\n            # Fields without name should be ignored.  This is typically\n            # pad and constant fields\n\n            if not f.name:\n                pass\n\n            # Store index in val for Length and Format fields, to be used\n            # when treating varfields.\n\n            elif isinstance(f, LengthField):\n                f_names = [f.name]\n                if f.other_fields:\n                    f_names.extend(f.other_fields)\n                field_val = val[vno]\n                if f.parse_value is not None:\n                    field_val = f.parse_value(field_val, display)\n                for f_name in f_names:\n                    lengths[f_name] = field_val\n\n            elif isinstance(f, FormatField):\n                formats[f.name] = val[vno]\n\n            # Treat value fields the same was as in parse_value.\n            else:\n                if f.structvalues == 1:\n                    field_val = val[vno]\n                else:\n                    field_val = val[vno:vno+f.structvalues]\n\n                if f.parse_value is not None:\n                    field_val = f.parse_value(field_val, display)\n                ret[f.name] = field_val\n\n            vno = vno + f.structvalues\n\n        data = data[self.static_size:]\n\n        # Call parse_binary_value for each var_field, passing the\n        # length and format values from the unpacked val.\n\n        for f in self.var_fields:\n            ret[f.name], data = f.parse_binary_value(data, display,\n                                                     lengths.get(f.name),\n                                                     formats.get(f.name),\n                                                    )\n\n        if not rawdict:\n            ret = DictWrapper(ret)\n        return ret, data", "response": "Parses a binary representation of the structure into a list of Python values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding an authentication entry matching FAMILY ADDRESS and DISPNO.", "response": "def get_best_auth(self, family, address, dispno,\n                      types = ( b\"MIT-MAGIC-COOKIE-1\", )):\n\n        \"\"\"Find an authentication entry matching FAMILY, ADDRESS and\n        DISPNO.\n\n        The name of the auth scheme must match one of the names in\n        TYPES.  If several entries match, the first scheme in TYPES\n        will be choosen.\n\n        If an entry is found, the tuple (name, data) is returned,\n        otherwise XNoAuthError is raised.\n        \"\"\"\n\n        num = str(dispno).encode()\n\n        matches = {}\n\n        for efam, eaddr, enum, ename, edata in self.entries:\n            if efam == family and eaddr == address and num == enum:\n                matches[ename] = edata\n\n        for t in types:\n            try:\n                return (t, matches[t])\n            except KeyError:\n                pass\n\n        raise error.XNoAuthError((family, address, dispno))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the example command and returns returncode of example", "response": "def run_example(path):\n    \"\"\" Returns returncode of example \"\"\"\n    cmd = \"{0} {1}\".format(sys.executable, path)\n    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res = proc.communicate()\n    if proc.returncode:\n        print(res[1].decode())\n    return proc.returncode"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_keysym_group(group):\n    '''Load all the keysyms in group.\n\n    Given a group name such as 'latin1' or 'katakana' load the keysyms\n    defined in module 'Xlib.keysymdef.group-name' into this XK module.'''\n    if '.' in group:\n        raise ValueError('invalid keysym group name: %s' % group)\n\n    G = globals() #Get a reference to XK.__dict__ a.k.a. globals\n\n    #Import just the keysyms module.\n    mod = __import__('Xlib.keysymdef.%s' % group, G, locals(), [group])\n\n    #Extract names of just the keysyms.\n    keysyms = [n for n in dir(mod) if n.startswith('XK_')]\n\n    #Copy the named keysyms into XK.__dict__\n    for keysym in keysyms:\n        ## k = mod.__dict__[keysym]; assert k == int(k) #probably too much.\n        G[keysym] = mod.__dict__[keysym]\n\n    #And get rid of the keysym module.\n    del mod", "response": "Load all the keysyms in group into this XK module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntranslate a keysym to a python string.", "response": "def keysym_to_string(keysym):\n    '''Translate a keysym (16 bit number) into a python string.\n\n    This will pass 0 to 0xff as well as XK_BackSpace, XK_Tab, XK_Clear,\n    XK_Return, XK_Pause, XK_Scroll_Lock, XK_Escape, XK_Delete. For other\n    values it returns None.'''\n\n    # ISO latin 1, LSB is the code\n    if keysym & 0xff00 == 0:\n        return chr(keysym & 0xff)\n\n    if keysym in [XK_BackSpace, XK_Tab, XK_Clear, XK_Return,\n                  XK_Pause, XK_Scroll_Lock, XK_Escape, XK_Delete]:\n        return chr(keysym & 0xff)\n\n    # We should be able to do these things quite automatically\n    # for latin2, latin3, etc, in Python 2.0 using the Unicode,\n    # but that will have to wait.\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _1_0set_screen_config(self, size_id, rotation, config_timestamp, timestamp=X.CurrentTime):\n    return _1_0SetScreenConfig(\n        display=self.display,\n        opcode=self.display.get_extension_major(extname),\n        drawable=self,\n        timestamp=timestamp,\n        config_timestamp=config_timestamp,\n        size_id=size_id,\n        rotation=rotation,\n        )", "response": "Sets the screen to the specified size and rotation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the screen to the specified size rate and reflection.", "response": "def set_screen_config(self, size_id, rotation, config_timestamp, rate=0, timestamp=X.CurrentTime):\n    \"\"\"Sets the screen to the specified size, rate, rotation and reflection.\n\n    rate can be 0 to have the server select an appropriate rate.\n\n    \"\"\"\n    return SetScreenConfig(\n        display=self.display,\n        opcode=self.display.get_extension_major(extname),\n        drawable=self,\n        timestamp=timestamp,\n        config_timestamp=config_timestamp,\n        size_id=size_id,\n        rotation=rotation,\n        rate=rate,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve information about the current and available configurations for this window.", "response": "def get_screen_info(self):\n    \"\"\"Retrieve information about the current and available configurations for\n    the screen associated with this window.\n\n    \"\"\"\n    return GetScreenInfo(\n        display=self.display,\n        opcode=self.display.get_extension_major(extname),\n        window=self,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_screen_size_range(self):\n    return GetScreenSizeRange(\n        display=self.display,\n        opcode=self.display.get_extension_major(extname),\n        window=self,\n        )", "response": "Retrieve the range of possible screen sizes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bin_insert(list, element):\n\n    if not list:\n        list.append(element)\n        return\n\n    lower = 0\n    upper = len(list) - 1\n\n    while lower <= upper:\n        center = (lower + upper) // 2\n        if element < list[center]:\n            upper = center - 1\n        elif element > list[center]:\n            lower = center + 1\n        elif element == list[center]:\n            return\n\n    if element < list[upper]:\n        list.insert(upper, element)\n    elif element > list[upper]:\n        list.insert(upper + 1, element)", "response": "Insert element into list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing X. display and X. resource_manager properties and return a display object name db and ARGV.", "response": "def get_display_opts(options, argv = sys.argv):\n    \"\"\"display, name, db, args = get_display_opts(options, [argv])\n\n    Parse X OPTIONS from ARGV (or sys.argv if not provided).\n\n    Connect to the display specified by a *.display resource if one is\n    set, or to the default X display otherwise.  Extract the\n    RESOURCE_MANAGER property and insert all resources from ARGV.\n\n    The four return values are:\n      DISPLAY -- the display object\n      NAME    -- the application name (the filname of ARGV[0])\n      DB      -- the created resource database\n      ARGS    -- any remaining arguments\n    \"\"\"\n\n    from Xlib import display, Xatom\n    import os\n\n    name = os.path.splitext(os.path.basename(argv[0]))[0]\n\n    optdb = ResourceDB()\n    leftargv = optdb.getopt(name, argv[1:], options)\n\n    dname = optdb.get(name + '.display', name + '.Display', None)\n    d = display.Display(dname)\n\n    rdbstring = d.screen(0).root.get_full_property(Xatom.RESOURCE_MANAGER,\n                                                   Xatom.STRING)\n    if rdbstring:\n        data = rdbstring.value\n    else:\n        data = None\n\n    db = ResourceDB(string = data)\n    db.update(optdb)\n\n    return d, name, db, leftargv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_file(self, file):\n\n        if type(file) is bytes:\n            file = open(file, 'r')\n\n        self.insert_string(file.read())", "response": "Load resources entries from FILE and insert them into the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninserts the resources entries in the string DATA into the resource table.", "response": "def insert_string(self, data):\n        \"\"\"insert_string(data)\n\n        Insert the resources entries in the string DATA into the\n        database.\n\n        \"\"\"\n\n        # First split string into lines\n        lines = data.split('\\n')\n\n        while lines:\n            line = lines[0]\n            del lines[0]\n\n            # Skip empty line\n            if not line:\n                continue\n\n            # Skip comments\n            if comment_re.match(line):\n                continue\n\n            # Handle continued lines\n            while line[-1] == '\\\\':\n                if lines:\n                    line = line[:-1] + lines[0]\n                    del lines[0]\n                else:\n                    line = line[:-1]\n                    break\n\n            # Split line into resource and value\n            m = resource_spec_re.match(line)\n\n            # Bad line, just ignore it silently\n            if not m:\n                continue\n\n            res, value = m.group(1, 2)\n\n            # Convert all escape sequences in value\n            splits = value_escape_re.split(value)\n\n            for i in range(1, len(splits), 2):\n                s = splits[i]\n                if len(s) == 3:\n                    splits[i] = chr(int(s, 8))\n                elif s == 'n':\n                    splits[i] = '\\n'\n\n            # strip the last value part to get rid of any\n            # unescaped blanks\n            splits[-1] = splits[-1].rstrip()\n\n            value = ''.join(splits)\n\n            self.insert(res, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninserting all the resources in the list RESOURCES into the database.", "response": "def insert_resources(self, resources):\n        \"\"\"insert_resources(resources)\n\n        Insert all resources entries in the list RESOURCES into the\n        database.  Each element in RESOURCES should be a tuple:\n\n          (resource, value)\n\n        Where RESOURCE is a string and VALUE can be any Python value.\n\n        \"\"\"\n\n        for res, value in resources:\n            self.insert(res, value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert(self, resource, value):\n\n        # Split res into components and bindings\n        parts = resource_parts_re.split(resource)\n\n        # If the last part is empty, this is an invalid resource\n        # which we simply ignore\n        if parts[-1] == '':\n            return\n\n        self.lock.acquire()\n\n        db = self.db\n        for i in range(1, len(parts), 2):\n\n            # Create a new mapping/value group\n            if parts[i - 1] not in db:\n                db[parts[i - 1]] = ({}, {})\n\n            # Use second mapping if a loose binding, first otherwise\n            if '*' in parts[i]:\n                db = db[parts[i - 1]][1]\n            else:\n                db = db[parts[i - 1]][0]\n\n        # Insert value into the derived db\n        if parts[-1] in db:\n            db[parts[-1]] = db[parts[-1]][:2] + (value, )\n        else:\n            db[parts[-1]] = ({}, {}, value)\n\n        self.lock.release()", "response": "Insert a resource entry into the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets - Returns the value matching the resource identified by NAME and CLASS. If no match is found default is returned.", "response": "def get(self, res, cls, default = None):\n        \"\"\"get(name, class [, default])\n\n        Return the value matching the resource identified by NAME and\n        CLASS.  If no match is found, DEFAULT is returned, or None if\n        DEFAULT isn't specified.\n\n        \"\"\"\n\n        try:\n            return self[(res, cls)]\n        except KeyError:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, db):\n\n        self.lock.acquire()\n        update_db(self.db, db.db)\n        self.lock.release()", "response": "Update this database with all resources entries in the resource\n        database DB."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef output(self):\n\n        self.lock.acquire()\n        text = output_db('', self.db)\n        self.lock.release()\n        return text", "response": "Return the resource database in text representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses X command line options inserting the recognised options into the resource database.", "response": "def getopt(self, name, argv, opts):\n        \"\"\"getopt(name, argv, opts)\n\n        Parse X command line options, inserting the recognised options\n        into the resource database.\n\n        NAME is the application name, and will be prepended to all\n        specifiers.  ARGV is the list of command line arguments,\n        typically sys.argv[1:].\n\n        OPTS is a mapping of options to resource specifiers.  The key is\n        the option flag (with leading -), and the value is an instance of\n        some Option subclass:\n\n        NoArg(specifier, value): set resource to value.\n        IsArg(specifier):        set resource to option itself\n        SepArg(specifier):       value is next argument\n        ResArg:                  resource and value in next argument\n        SkipArg:                 ignore this option and next argument\n        SkipLine:                ignore rest of arguments\n        SkipNArgs(count):        ignore this option and count arguments\n\n        The remaining, non-option, oparguments is returned.\n\n        rdb.OptionError is raised if there is an error in the argument list.\n        \"\"\"\n\n        while argv and argv[0] and argv[0][0] == '-':\n            try:\n                argv = opts[argv[0]].parse(name, self, argv)\n            except KeyError:\n                raise OptionError('unknown option: %s' % argv[0])\n            except IndexError:\n                raise OptionError('missing argument to option: %s' % argv[0])\n\n        return argv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_screen_size(self, screen_no):\n    return GetScreenSize(display=self.display,\n                         opcode=self.display.get_extension_major(extname),\n                         window=self.id,\n                         screen=screen_no,\n                         )", "response": "Returns the size of the given screen number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_query_cache_key(compiler):\n    sql, params = compiler.as_sql()\n    check_parameter_types(params)\n    cache_key = '%s:%s:%s' % (compiler.using, sql,\n                              [text_type(p) for p in params])\n    return sha1(cache_key.encode('utf-8')).hexdigest()", "response": "Generates a cache key from a SQLCompiler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_table_cache_key(db_alias, table):\n    cache_key = '%s:%s' % (db_alias, table)\n    return sha1(cache_key.encode('utf-8')).hexdigest()", "response": "Generates a cache key from a SQL table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninvalidates one or more SQL tables or models from the database.", "response": "def invalidate(*tables_or_models, **kwargs):\n    \"\"\"\n    Clears what was cached by django-cachalot implying one or more SQL tables\n    or models from ``tables_or_models``.\n    If ``tables_or_models`` is not specified, all tables found in the database\n    (including those outside Django) are invalidated.\n\n    If ``cache_alias`` is specified, it only clears the SQL queries stored\n    on this cache, otherwise queries from all caches are cleared.\n\n    If ``db_alias`` is specified, it only clears the SQL queries executed\n    on this database, otherwise queries from all databases are cleared.\n\n    :arg tables_or_models: SQL tables names, models or models lookups\n                           (or a combination)\n    :type tables_or_models: tuple of strings or models\n    :arg cache_alias: Alias from the Django ``CACHES`` setting\n    :type cache_alias: string or NoneType\n    :arg db_alias: Alias from the Django ``DATABASES`` setting\n    :type db_alias: string or NoneType\n    :returns: Nothing\n    :rtype: NoneType\n    \"\"\"\n    # TODO: Replace with positional arguments when we drop Python 2 support.\n    cache_alias = kwargs.pop('cache_alias', None)\n    db_alias = kwargs.pop('db_alias', None)\n    for k in kwargs:\n        raise TypeError(\n            \"invalidate() got an unexpected keyword argument '%s'\" % k)\n\n    send_signal = False\n    invalidated = set()\n    for cache_alias, db_alias, tables in _cache_db_tables_iterator(\n            list(_get_tables(tables_or_models)), cache_alias, db_alias):\n        cache = cachalot_caches.get_cache(cache_alias, db_alias)\n        if not isinstance(cache, AtomicCache):\n            send_signal = True\n        _invalidate_tables(cache, db_alias, tables)\n        invalidated.update(tables)\n\n    if send_signal:\n        for table in invalidated:\n            post_invalidation.send(table, db_alias=db_alias)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the timestamp of the most recent invalidation of the given tables or models.", "response": "def get_last_invalidation(*tables_or_models, **kwargs):\n    \"\"\"\n    Returns the timestamp of the most recent invalidation of the given\n    ``tables_or_models``.  If ``tables_or_models`` is not specified,\n    all tables found in the database (including those outside Django) are used.\n\n    If ``cache_alias`` is specified, it only fetches invalidations\n    in this cache, otherwise invalidations in all caches are fetched.\n\n    If ``db_alias`` is specified, it only fetches invalidations\n    for this database, otherwise invalidations for all databases are fetched.\n\n    :arg tables_or_models: SQL tables names, models or models lookups\n                           (or a combination)\n    :type tables_or_models: tuple of strings or models\n    :arg cache_alias: Alias from the Django ``CACHES`` setting\n    :type cache_alias: string or NoneType\n    :arg db_alias: Alias from the Django ``DATABASES`` setting\n    :type db_alias: string or NoneType\n    :returns: The timestamp of the most recent invalidation\n    :rtype: float\n    \"\"\"\n    # TODO: Replace with positional arguments when we drop Python 2 support.\n    cache_alias = kwargs.pop('cache_alias', None)\n    db_alias = kwargs.pop('db_alias', None)\n    for k in kwargs:\n        raise TypeError(\"get_last_invalidation() got an unexpected \"\n                        \"keyword argument '%s'\" % k)\n\n    last_invalidation = 0.0\n    for cache_alias, db_alias, tables in _cache_db_tables_iterator(\n            list(_get_tables(tables_or_models)), cache_alias, db_alias):\n        get_table_cache_key = cachalot_settings.CACHALOT_TABLE_KEYGEN\n        table_cache_keys = [get_table_cache_key(db_alias, t) for t in tables]\n        invalidations = cachalot_caches.get_cache(\n            cache_alias, db_alias).get_many(table_cache_keys).values()\n        if invalidations:\n            current_last_invalidation = max(invalidations)\n            if current_last_invalidation > last_invalidation:\n                last_invalidation = current_last_invalidation\n    return last_invalidation"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a software reset command to all servo drivers on the bus.", "response": "def software_reset(i2c=None, **kwargs):\n    \"\"\"Sends a software reset (SWRST) command to all servo drivers on the bus.\"\"\"\n    # Setup I2C interface for device 0x00 to talk to all of them.\n    if i2c is None:\n        import Adafruit_GPIO.I2C as I2C\n        i2c = I2C\n    self._device = i2c.get_i2c_device(0x00, **kwargs)\n    self._device.writeRaw8(0x06)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntranslates a REIL instruction into a SMT representation.", "response": "def translate(self, instr):\n        \"\"\"Return the SMT representation of a REIL instruction.\n        \"\"\"\n        try:\n            translator = self._instr_translators[instr.mnemonic]\n\n            return translator(*instr.operands)\n        except Exception:\n            logger.error(\"Failed to translate instruction: %s\", instr, exc_info=True)\n\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget initial name of symbol.", "response": "def get_name_init(self, name):\n        \"\"\"Get initial name of symbol.\n        \"\"\"\n        self._register_name(name)\n\n        return self._var_name_mappers[name].get_init()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_name_curr(self, name):\n        self._register_name(name)\n\n        return self._var_name_mappers[name].get_current()", "response": "Get current name of symbol."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a variable name.", "response": "def _register_name(self, name):\n        \"\"\"Get register name.\n        \"\"\"\n        if name not in self._var_name_mappers:\n            self._var_name_mappers[name] = VariableNamer(name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _translate_src_oprnd(self, operand):\n        if isinstance(operand, ReilRegisterOperand):\n            return self._translate_src_register_oprnd(operand)\n        elif isinstance(operand, ReilImmediateOperand):\n            return smtsymbol.Constant(operand.size, operand.immediate)\n        else:\n            raise Exception(\"Invalid operand type\")", "response": "Translate source operand to a SMT expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _translate_dst_oprnd(self, operand):\n        if isinstance(operand, ReilRegisterOperand):\n            return self._translate_dst_register_oprnd(operand)\n        else:\n            raise Exception(\"Invalid operand type\")", "response": "Translate destination operand to a SMT expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _translate_src_register_oprnd(self, operand):\n        reg_info = self._arch_alias_mapper.get(operand.name, None)\n\n        if reg_info:\n            var_base_name, offset = reg_info\n\n            var_size = self._arch_regs_size[var_base_name]\n        else:\n            var_base_name = operand.name\n            var_size = operand.size\n\n        var_name = self._get_var_name(var_base_name)\n        ret_val = self.make_bitvec(var_size, var_name)\n\n        if reg_info:\n            ret_val = smtfunction.extract(ret_val, offset, operand.size)\n\n        return ret_val", "response": "Translate source register operand to SMT expr."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntranslate destination register operand to SMT expr.", "response": "def _translate_dst_register_oprnd(self, operand):\n        \"\"\"Translate destination register operand to SMT expr.\n        \"\"\"\n        reg_info = self._arch_alias_mapper.get(operand.name, None)\n\n        parent_reg_constrs = []\n\n        if reg_info:\n            var_base_name, offset = reg_info\n\n            var_name_old = self._get_var_name(var_base_name, fresh=False)\n            var_name_new = self._get_var_name(var_base_name, fresh=True)\n            var_size = self._arch_regs_size[var_base_name]\n\n            ret_val_old = self.make_bitvec(var_size, var_name_old)\n            ret_val_new = self.make_bitvec(var_size, var_name_new)\n\n            ret_val = smtfunction.extract(ret_val_new, offset, operand.size)\n\n            if 0 < offset < var_size - 1:\n                lower_expr_1 = smtfunction.extract(ret_val_new, 0, offset)\n                lower_expr_2 = smtfunction.extract(ret_val_old, 0, offset)\n\n                parent_reg_constrs += [lower_expr_1 == lower_expr_2]\n\n                upper_expr_1 = smtfunction.extract(ret_val_new, offset + operand.size, var_size - offset - operand.size)\n                upper_expr_2 = smtfunction.extract(ret_val_old, offset + operand.size, var_size - offset - operand.size)\n\n                parent_reg_constrs += [upper_expr_1 == upper_expr_2]\n            elif offset == 0:\n                upper_expr_1 = smtfunction.extract(ret_val_new, offset + operand.size, var_size - offset - operand.size)\n                upper_expr_2 = smtfunction.extract(ret_val_old, offset + operand.size, var_size - offset - operand.size)\n\n                parent_reg_constrs += [upper_expr_1 == upper_expr_2]\n            elif offset == var_size-1:\n                lower_expr_1 = smtfunction.extract(ret_val_new, 0, offset)\n                lower_expr_2 = smtfunction.extract(ret_val_old, 0, offset)\n\n                parent_reg_constrs += [lower_expr_1 == lower_expr_2]\n        else:\n            var_name_new = self._get_var_name(operand.name, fresh=True)\n\n            ret_val = self.make_bitvec(operand.size, var_name_new)\n\n        return ret_val, parent_reg_constrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a formula representation of an ADD instruction.", "response": "def _translate_add(self, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return a formula representation of an ADD instruction.\n        \"\"\"\n        assert oprnd1.size and oprnd2.size and oprnd3.size\n        assert oprnd1.size == oprnd2.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op2_var = self._translate_src_oprnd(oprnd2)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        if oprnd3.size > oprnd1.size:\n            result = smtfunction.zero_extend(op1_var, oprnd3.size) + smtfunction.zero_extend(op2_var, oprnd3.size)\n        elif oprnd3.size < oprnd1.size:\n            result = smtfunction.extract(op1_var + op2_var, 0, oprnd3.size)\n        else:\n            result = op1_var + op2_var\n\n        return [op3_var == result] + op3_var_constrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _translate_div(self, oprnd1, oprnd2, oprnd3):\n        assert oprnd1.size and oprnd2.size and oprnd3.size\n        assert oprnd1.size == oprnd2.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op2_var = self._translate_src_oprnd(oprnd2)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        if oprnd3.size > oprnd1.size:\n            op1_var_zx = smtfunction.zero_extend(op1_var, oprnd3.size)\n            op2_var_zx = smtfunction.zero_extend(op2_var, oprnd3.size)\n\n            result = op1_var_zx.udiv(op2_var_zx)\n        elif oprnd3.size < oprnd1.size:\n            result = smtfunction.extract(op1_var.udiv(op2_var), 0, oprnd3.size)\n        else:\n            result = op1_var.udiv(op2_var)\n\n        return [op3_var == result] + op3_var_constrs", "response": "Return a formula representation of a DIV instruction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a formula representation of a BSH instruction.", "response": "def _translate_bsh(self, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return a formula representation of a BSH instruction.\n        \"\"\"\n        assert oprnd1.size and oprnd2.size and oprnd3.size\n        assert oprnd1.size == oprnd2.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op2_var = self._translate_src_oprnd(oprnd2)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        if oprnd3.size > oprnd1.size:\n            op1_var_zx = smtfunction.zero_extend(op1_var, oprnd3.size)\n            op2_var_zx = smtfunction.zero_extend(op2_var, oprnd3.size)\n            op2_var_neg_sx = smtfunction.sign_extend(-op2_var, oprnd3.size)\n\n            shr = smtfunction.extract(op1_var_zx >> op2_var_neg_sx, 0, op3_var.size)\n            shl = smtfunction.extract(op1_var_zx << op2_var_zx, 0, op3_var.size)\n        elif oprnd3.size < oprnd1.size:\n            shr = smtfunction.extract(op1_var >> -op2_var, 0, op3_var.size)\n            shl = smtfunction.extract(op1_var << op2_var, 0, op3_var.size)\n        else:\n            shr = op1_var >> -op2_var\n            shl = op1_var << op2_var\n\n        result = smtfunction.ite(oprnd3.size, op2_var >= 0, shl, shr)\n\n        return [op3_var == result] + op3_var_constrs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a formula representation of a LDM instruction.", "response": "def _translate_ldm(self, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return a formula representation of a LDM instruction.\n        \"\"\"\n        assert oprnd1.size and oprnd3.size\n        assert oprnd1.size == self._address_size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        exprs = []\n\n        for i in reversed(range(0, oprnd3.size, 8)):\n            exprs += [self._mem_curr[op1_var + i // 8] == smtfunction.extract(op3_var, i, 8)]\n\n        return exprs + op3_var_constrs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a formula representation of a STM instruction.", "response": "def _translate_stm(self, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return a formula representation of a STM instruction.\n        \"\"\"\n        assert oprnd1.size and oprnd3.size\n        assert oprnd3.size == self._address_size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op3_var = self._translate_src_oprnd(oprnd3)\n\n        for i in range(0, oprnd1.size, 8):\n            self._mem_curr[op3_var + i//8] = smtfunction.extract(op1_var, i, 8)\n\n        # Memory versioning.\n        self._mem_instance += 1\n\n        mem_old = self._mem_curr\n        mem_new = self.make_array(self._address_size, \"MEM_{}\".format(self._mem_instance))\n\n        self._mem_curr = mem_new\n\n        return [mem_new == mem_old]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a formula representation of a STR instruction.", "response": "def _translate_str(self, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return a formula representation of a STR instruction.\n        \"\"\"\n        assert oprnd1.size and oprnd3.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        if oprnd3.size > oprnd1.size:\n            result = smtfunction.zero_extend(op1_var, op3_var.size)\n        elif oprnd3.size < oprnd1.size:\n            result = smtfunction.extract(op1_var, 0, op3_var.size)\n        else:\n            result = op1_var\n\n        return [op3_var == result] + op3_var_constrs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a formula representation of a BISZ instruction.", "response": "def _translate_bisz(self, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return a formula representation of a BISZ instruction.\n        \"\"\"\n        assert oprnd1.size and oprnd3.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        result = smtfunction.ite(oprnd3.size, op1_var == 0x0, smtsymbol.Constant(oprnd3.size, 0x1),\n                                    smtsymbol.Constant(oprnd3.size, 0x0))\n\n        return [op3_var == result] + op3_var_constrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _translate_jcc(self, oprnd1, oprnd2, oprnd3):\n        assert oprnd1.size and oprnd3.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n\n        return [op1_var != 0x0]", "response": "Return a formula representation of a JCC instruction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a formula representation of a SEXT instruction.", "response": "def _translate_sext(self, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return a formula representation of a SEXT instruction.\n        \"\"\"\n        assert oprnd1.size and oprnd3.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        if oprnd3.size > oprnd1.size:\n            result = smtfunction.sign_extend(op1_var, op3_var.size)\n        elif oprnd3.size < oprnd1.size:\n            raise Exception(\"Operands size mismatch.\")\n        else:\n            result = op1_var\n\n        return [op3_var == result] + op3_var_constrs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _translate_sdiv(self, oprnd1, oprnd2, oprnd3):\n        assert oprnd1.size and oprnd2.size and oprnd3.size\n        assert oprnd1.size == oprnd2.size\n\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op2_var = self._translate_src_oprnd(oprnd2)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n\n        if oprnd3.size > oprnd1.size:\n            op1_var_sx = smtfunction.sign_extend(op1_var, oprnd3.size)\n            op2_var_sx = smtfunction.sign_extend(op2_var, oprnd3.size)\n\n            result = op1_var_sx // op2_var_sx\n        elif oprnd3.size < oprnd1.size:\n            result = smtfunction.extract(op1_var // op2_var, 0, oprnd3.size)\n        else:\n            result = op1_var // op2_var\n\n        return [op3_var == result] + op3_var_constrs", "response": "Return a formula representation of a DIV instruction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_section(sect_req, elffile, mem=None):\n        for nseg, section in enumerate(elffile.iter_sections()):\n            request = sect_req.get(section.name, None)\n            if request:\n                request.set_found(True)\n                request.set_sect_properties(section.header.sh_addr,\n                                            section.header.sh_size)\n                if mem:\n                    mem.add_vma(section.header.sh_addr, section.data())\n                else:\n                    request.memory.add_vma(section.header.sh_addr, section.data())", "response": "Extracts the section from an elffile."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate constraints for the MoveRegister gadgets", "response": "def _get_constrs_move_register(self, gadget):\n        \"\"\"Generate constraints for the MoveRegister gadgets:\n            dst <- src\n\n        \"\"\"\n        # *src* register has to have the same value of *dst* for all\n        # possibles assignments of *dst*.\n\n        dst = self.analyzer.get_register_expr(gadget.destination[0].name, mode=\"post\")\n        src = self.analyzer.get_register_expr(gadget.sources[0].name, mode=\"pre\")\n\n        # Check all non-modified registers don't change.\n        constrs_mod = []\n\n        for name in self._arch_info.registers_gp_base:\n            if name not in [r.name for r in gadget.modified_registers]:\n                var_initial = self.analyzer.get_register_expr(name, mode=\"pre\")\n                var_final = self.analyzer.get_register_expr(name, mode=\"post\")\n\n                constrs_mod += [var_initial != var_final]\n\n        if constrs_mod:\n            constrs_mod = [reduce(lambda c, acc: acc | c, constrs_mod[1:], constrs_mod[0])]\n\n        return [dst != src] + constrs_mod"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the constraints for the arithmetic operation gadgets.", "response": "def _get_constrs_arithmetic(self, gadget):\n        \"\"\"Generate constraints for the BinaryOperation gadgets:\n            dst <- src1 OP src2\n\n        \"\"\"\n        # *dst* register has to have the value of *src1 op src2* for all\n        # possibles assignments of *src1* and *src2*.\n\n        dst = self.analyzer.get_register_expr(gadget.destination[0].name, mode=\"post\")\n        src1 = self.analyzer.get_register_expr(gadget.sources[0].name, mode=\"pre\")\n        src2 = self.analyzer.get_register_expr(gadget.sources[1].name, mode=\"pre\")\n        op = self._arithmetic_ops[gadget.operation]\n\n        # Check all non-modified registers don't change.\n        constrs_mod = []\n\n        for name in self._arch_info.registers_gp_base:\n            if name not in [r.name for r in gadget.modified_registers]:\n                var_initial = self.analyzer.get_register_expr(name, mode=\"pre\")\n                var_final = self.analyzer.get_register_expr(name, mode=\"post\")\n\n                constrs_mod += [var_initial != var_final]\n\n        if constrs_mod:\n            constrs_mod = [reduce(lambda c, acc: acc | c, constrs_mod[1:], constrs_mod[0])]\n\n        return [dst != op(src1, src2)] + constrs_mod"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the constraints for the LoadMemory gadgets.", "response": "def _get_constrs_load_memory(self, gadget):\n        \"\"\"Generate constraints for the LoadMemory gadgets: dst_reg <- mem[src_reg + offset]\n        \"\"\"\n        dst = self.analyzer.get_register_expr(gadget.destination[0].name, mode=\"post\")\n        size = gadget.destination[0].size\n\n        if isinstance(gadget.sources[0], ReilRegisterOperand):\n            base_addr = self.analyzer.get_register_expr(gadget.sources[0].name, mode=\"pre\")\n            offset = gadget.sources[1].immediate\n\n            addr = base_addr + offset\n        else:\n            addr = gadget.sources[1].immediate\n\n        constrs = []\n\n        for i in reversed(range(0, size, 8)):\n            bytes_exprs_1 = self.analyzer.get_memory_expr(addr + i // 8, 8 // 8)\n            bytes_exprs_2 = smtfunction.extract(dst, i, 8)\n\n            constrs += [bytes_exprs_1 != bytes_exprs_2]\n\n        # Check all non-modified registers don't change.\n        constrs_mod = []\n\n        for name in self._arch_info.registers_gp_base:\n            if name not in [r.name for r in gadget.modified_registers]:\n                var_initial = self.analyzer.get_register_expr(name, mode=\"pre\")\n                var_final = self.analyzer.get_register_expr(name, mode=\"post\")\n\n                constrs_mod += [var_initial != var_final]\n\n        if constrs_mod:\n            constrs_mod = [reduce(lambda c, acc: acc | c, constrs_mod[1:], constrs_mod[0])]\n\n        return constrs + constrs_mod"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef infer_operands_size(operands):\n    size = None\n\n    for oprnd in operands:\n        if oprnd.size:\n            size = oprnd.size\n            break\n\n    if size:\n        for oprnd in operands:\n            if not oprnd.size:\n                oprnd.size = size\n    else:\n        for oprnd in operands:\n            if isinstance(oprnd, X86ImmediateOperand) and not oprnd.size:\n                oprnd.size = arch_info.architecture_size", "response": "Infer x86 instruction operand size based on other operands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_operand(string, location, tokens):\n    mod = \" \".join(tokens.get(\"modifier\", \"\"))\n\n    if \"immediate\" in tokens:\n        imm = parse_immediate(\"\".join(tokens[\"immediate\"]))\n        size = modifier_size.get(mod, None)\n\n        oprnd = X86ImmediateOperand(imm, size)\n\n    if \"register\" in tokens:\n        name = tokens[\"register\"]\n        size = arch_info.registers_size[tokens[\"register\"]]\n\n        oprnd = X86RegisterOperand(name, size)\n\n    if \"memory\" in tokens:\n        seg_reg = tokens.get(\"segment\", None)\n        base_reg = tokens.get(\"base\", None)\n        index_reg = tokens.get(\"index\", None)\n        scale_imm = int(tokens.get(\"scale\", \"0x1\"), 16)\n        displ_imm = int(\"\".join(tokens.get(\"displacement\", \"0x0\")), 16)\n\n        oprnd = X86MemoryOperand(seg_reg, base_reg, index_reg, scale_imm, displ_imm)\n\n    oprnd.modifier = mod\n\n    if not oprnd.size and oprnd.modifier:\n        oprnd.size = modifier_size[oprnd.modifier]\n\n    return oprnd", "response": "Parse an x86 instruction operand."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_instruction(string, location, tokens):\n    prefix_str = tokens.get(\"prefix\", None)\n    mnemonic_str = tokens.get(\"mnemonic\")\n    operands = [op for op in tokens.get(\"operands\", [])]\n\n    infer_operands_size(operands)\n\n    # Quick hack: Capstone returns rep instead of repe for cmps and scas\n    # instructions.\n    if prefix_str == \"rep\" and (mnemonic_str.startswith(\"cmps\") or mnemonic_str.startswith(\"scas\")):\n        prefix_str = \"repe\"\n\n    instr = X86Instruction(\n        prefix_str,\n        mnemonic_str,\n        operands,\n        arch_info.architecture_mode\n    )\n\n    return instr", "response": "Parse an x86 instruction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, instr):\n        try:\n            instr_lower = instr.lower()\n\n            if instr_lower not in self._cache:\n                instr_asm = instruction.parseString(instr_lower)[0]\n\n                self._cache[instr_lower] = instr_asm\n\n            instr_asm = copy.deepcopy(self._cache[instr_lower])\n\n            # self._check_instruction(instr_asm)\n        except Exception:\n            instr_asm = None\n\n            error_msg = \"Failed to parse instruction: %s\"\n\n            logger.error(error_msg, instr, exc_info=True)\n\n        return instr_asm", "response": "Parse an x86 instruction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disassemble(self, data, address, architecture_mode=None):\n        # TODO: Improve this code!\n        if architecture_mode is None:\n            if self._arch_mode is None:\n                architecture_mode = ARCH_ARM_MODE_THUMB\n            else:\n                architecture_mode = self._arch_mode\n\n        self._disassembler = self._available_disassemblers[architecture_mode]\n\n        disasm = self._cs_disassemble_one(data, address)\n\n        instr = self._cs_translate_insn(disasm)\n\n        if instr:\n            instr.address = address\n            instr.size = disasm.size\n            instr.bytes = data[0:disasm.size]\n        else:\n            raise DisassemblerError()\n\n        return instr", "response": "Disassemble the data into an instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisassembles the data into an instruction in string form.", "response": "def _cs_disassemble_one(self, data, address):\n        \"\"\"Disassemble the data into an instruction in string form.\n        \"\"\"\n        disasm = list(self._disassembler.disasm(bytes(data), address))\n\n        # TODO: Improve this check.\n        if len(disasm) > 0:\n            return disasm[0]\n        else:\n            cs_arm = Cs(CS_ARCH_ARM, CS_MODE_ARM)\n            cs_arm.detail = True\n            disasm = list(cs_arm.disasm(bytes(data), address))\n\n            if len(disasm) > 0:\n                return disasm[0]\n            else:\n                raise InvalidDisassemblerData(\"CAPSTONE: Unknown instruction (Addr: {:s}).\".format(hex(address)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute_lite(self, instructions, context=None):\n        if context:\n            self.__cpu.registers = dict(context)\n\n        for instr in instructions:\n            self.__execute_one(instr)\n\n        return dict(self.__cpu.registers), self.__mem", "response": "Execute a list of instructions. It does not support loops."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset the internal state of the internal state of the internal state.", "response": "def reset(self):\n        \"\"\"Reset emulator. All registers and memory are reset.\n        \"\"\"\n        self.__mem.reset()\n        self.__cpu.reset()\n        self.__tainter.reset()\n\n        # Instructions pre and post handlers.\n        self.__instr_handler_pre = None, None\n        self.__instr_handler_post = None, None\n\n        self.__set_default_handlers()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a BSH instruction.", "response": "def __execute_bsh(self, instr):\n        \"\"\"Execute BSH instruction.\n        \"\"\"\n        op0_val = self.read_operand(instr.operands[0])\n        op1_val = self.read_operand(instr.operands[1])\n\n        op1_size = instr.operands[1].size\n\n        # Check sign bit.\n        if extract_sign_bit(op1_val, op1_size) == 0:\n            op2_val = op0_val << op1_val\n        else:\n            op2_val = op0_val >> twos_complement(op1_val, op1_size)\n\n        self.write_operand(instr.operands[2], op2_val)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a BISZ instruction.", "response": "def __execute_bisz(self, instr):\n        \"\"\"Execute BISZ instruction.\n        \"\"\"\n        op0_val = self.read_operand(instr.operands[0])\n\n        op2_val = 1 if op0_val == 0 else 0\n\n        self.write_operand(instr.operands[2], op2_val)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __execute_sext(self, instr):\n        op0_size = instr.operands[0].size\n        op2_size = instr.operands[2].size\n\n        op0_val = self.read_operand(instr.operands[0])\n        op0_msb = extract_sign_bit(op0_val, op0_size)\n\n        op2_mask = (2**op2_size-1) & ~(2**op0_size-1) if op0_msb == 1 else 0x0\n\n        op2_val = op0_val | op2_mask\n\n        self.write_operand(instr.operands[2], op2_val)\n\n        return None", "response": "Execute an SEXT instruction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_path_satisfiability(code_analyzer, path, start_address):\n        start_instr_found = False\n        sat = False\n\n        # Traverse basic blocks, translate its instructions to SMT\n        # expressions and add them as assertions.\n        for bb_curr, bb_next in zip(path[:-1], path[1:]):\n            logger.info(\"BB @ {:#x}\".format(bb_curr.address))\n\n            # For each instruction...\n            for instr in bb_curr:\n                # If the start instruction have not been found, keep\n                # looking...\n                if not start_instr_found:\n                    if instr.address == start_address:\n                        start_instr_found = True\n                    else:\n                        continue\n\n                logger.info(\"{:#x} {}\".format(instr.address, instr))\n\n                # For each REIL instruction...\n                for reil_instr in instr.ir_instrs:\n                    logger.info(\"{:#x} {:02d} {}\".format(reil_instr.address >> 0x8, reil_instr.address & 0xff,\n                                                         reil_instr))\n\n                    if reil_instr.mnemonic == ReilMnemonic.JCC:\n                        # Check that the JCC is the last instruction of\n                        # the basic block (skip CALL instructions.)\n                        if instr.address + instr.size - 1 != bb_curr.end_address:\n                            logger.error(\"Unexpected JCC instruction: {:#x} {} ({})\".format(instr.address,\n                                                                                            instr,\n                                                                                            reil_instr))\n\n                            # raise Exception()\n\n                            continue\n\n                        # Make sure branch target address from current\n                        # basic block is the start address of the next.\n                        assert(bb_curr.taken_branch == bb_next.address or\n                               bb_curr.not_taken_branch == bb_next.address or\n                               bb_curr.direct_branch == bb_next.address)\n\n                        # Set branch condition accordingly.\n                        if bb_curr.taken_branch == bb_next.address:\n                            branch_var_goal = 0x1\n                        elif bb_curr.not_taken_branch == bb_next.address:\n                            branch_var_goal = 0x0\n                        else:\n                            continue\n\n                        # Add branch condition goal constraint.\n                        code_analyzer.add_constraint(code_analyzer.get_operand_expr(reil_instr.operands[0]) == branch_var_goal)\n\n                        # The JCC instruction was the last within the\n                        # current basic block. End this iteration and\n                        # start next one.\n                        break\n\n                    # Translate and add SMT expressions to the solver.\n                    code_analyzer.add_instruction(reil_instr)\n\n            sat = code_analyzer.check() == 'sat'\n\n            logger.info(\"BB @ {:#x} sat? {}\".format(bb_curr.address, sat))\n\n            if not sat:\n                break\n\n        # Return satisfiability.\n        return sat", "response": "Check satisfiability of a basic block path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_string(mnemonic):\n        strings = {\n            # Arithmetic Instructions\n            ReilMnemonic.ADD: \"add\",\n            ReilMnemonic.SUB: \"sub\",\n            ReilMnemonic.MUL: \"mul\",\n            ReilMnemonic.DIV: \"div\",\n            ReilMnemonic.MOD: \"mod\",\n            ReilMnemonic.BSH: \"bsh\",\n\n            # Bitwise Instructions\n            ReilMnemonic.AND: \"and\",\n            ReilMnemonic.OR:  \"or\",\n            ReilMnemonic.XOR: \"xor\",\n\n            # Data Transfer Instructions\n            ReilMnemonic.LDM: \"ldm\",\n            ReilMnemonic.STM: \"stm\",\n            ReilMnemonic.STR: \"str\",\n\n            # Conditional Instructions\n            ReilMnemonic.BISZ: \"bisz\",\n            ReilMnemonic.JCC:  \"jcc\",\n\n            # Other Instructions\n            ReilMnemonic.UNKN:  \"unkn\",\n            ReilMnemonic.UNDEF: \"undef\",\n            ReilMnemonic.NOP:   \"nop\",\n\n            # Extensions\n            ReilMnemonic.SEXT: \"sext\",\n            ReilMnemonic.SDIV: \"sdiv\",\n            ReilMnemonic.SMOD: \"smod\",\n            ReilMnemonic.SMUL: \"smul\",\n        }\n\n        return strings[mnemonic]", "response": "Return the string representation of the given mnemonic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_string(string):\n        mnemonics = {\n            # Arithmetic Instructions\n            \"add\": ReilMnemonic.ADD,\n            \"sub\": ReilMnemonic.SUB,\n            \"mul\": ReilMnemonic.MUL,\n            \"div\": ReilMnemonic.DIV,\n            \"mod\": ReilMnemonic.MOD,\n            \"bsh\": ReilMnemonic.BSH,\n\n            # Bitwise Instructions\n            \"and\": ReilMnemonic.AND,\n            \"or\":  ReilMnemonic.OR,\n            \"xor\": ReilMnemonic.XOR,\n\n            # Data Transfer Instructions\n            \"ldm\": ReilMnemonic.LDM,\n            \"stm\": ReilMnemonic.STM,\n            \"str\": ReilMnemonic.STR,\n\n            # Conditional Instructions\n            \"bisz\": ReilMnemonic.BISZ,\n            \"jcc\":  ReilMnemonic.JCC,\n\n            # Other Instructions\n            \"unkn\":  ReilMnemonic.UNKN,\n            \"undef\": ReilMnemonic.UNDEF,\n            \"nop\":   ReilMnemonic.NOP,\n\n            # Added Instructions\n            \"sext\": ReilMnemonic.SEXT,\n            \"sdiv\": ReilMnemonic.SDIV,\n            \"smod\": ReilMnemonic.SMOD,\n            \"smul\": ReilMnemonic.SMUL,\n        }\n\n        return mnemonics[string]", "response": "Return the mnemonic represented by the given string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __taint_move(self, instr):\n        # Get taint information.\n        op0_taint = self.get_operand_taint(instr.operands[0])\n\n        # Propagate taint.\n        self.set_operand_taint(instr.operands[2], op0_taint)", "response": "Taint registers move instruction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute an assembler instruction.", "response": "def execute(self, asm_instr):\n        \"\"\"Execute an assembler instruction.\n\n        Args:\n            asm_instr (X86Instruction): A instruction to execute.\n\n        Returns:\n            A int. The address of the next instruction to execute.\n        \"\"\"\n        # Update the instruction pointer.\n        self.ir_emulator.registers[self.ip] = asm_instr.address + asm_instr.size\n\n        # Process syscall.\n        if self.arch_info.instr_is_syscall(asm_instr):\n            raise Syscall()\n\n        # Process instruction and return next address instruction to execute.\n        return self.__execute(asm_instr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting gadgets IR instructions.", "response": "def ir_instrs(self):\n        \"\"\"Get gadgets IR instructions.\n        \"\"\"\n        ir_instrs = []\n\n        for asm_instr in self._instrs:\n            ir_instrs += asm_instr.ir_instrs\n\n        return ir_instrs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the symbol tables contained in the ELF file.", "response": "def load_symbols_elf(filename):\n    \"\"\" Load the symbol tables contained in the file\n    \"\"\"\n    f = open(filename, 'rb')\n\n    elffile = ELFFile(f)\n\n    symbols = []\n\n    for section in elffile.iter_sections():\n        if not isinstance(section, SymbolTableSection):\n            continue\n\n        if section['sh_entsize'] == 0:\n            logger.warn(\"Symbol table {} has a sh_entsize of zero.\".format(section.name))\n\n            continue\n\n        logger.info(\"Symbol table {} contains {} entries.\".format(section.name, section.num_symbols()))\n\n        for _, symbol in enumerate(section.iter_symbols()):\n            if describe_symbol_shndx(symbol['st_shndx']) != \"UND\" and \\\n                describe_symbol_type(symbol['st_info']['type']) == \"FUNC\":\n                symbols.append((symbol['st_value'], symbol['st_size'], symbol.name))\n\n    f.close()\n\n    symbols_by_addr = {\n        addr: (name, size, True) for addr, size, name in symbols\n    }\n\n    return symbols_by_addr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _classify_no_operation(self, regs_init, regs_fini, mem_fini, written_regs, read_regs):\n        # TODO: Flags should be taken into account\n\n        matches = []\n\n        # Check that registers didn't change their value.\n        regs_changed = any(regs_init[r] != regs_fini[r] for r in regs_init)\n\n        # Check that flags didn't change their value.\n        flags_changed = False\n\n        # Check that memory didn't change.\n        mem_changed = mem_fini.get_write_count() != 0\n\n        if not regs_changed and not flags_changed and not mem_changed:\n            matches.append({\n                \"op\": \"nop\",\n            })\n\n        return matches", "response": "Classify no - operation gadgets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclassifying move - register gadgets.", "response": "def _classify_move_register(self, regs_init, regs_fini, mem_fini, written_regs, read_regs):\n        \"\"\"Classify move-register gadgets.\n        \"\"\"\n        matches = []\n\n        regs_init_inv = self._invert_dictionary(regs_init)\n\n        # Check for \"dst_reg <- src_reg\" pattern.\n        for dst_reg, dst_val in regs_fini.items():\n            # Make sure the *dst* register was written.\n            if dst_reg not in written_regs:\n                continue\n\n            for src_reg in regs_init_inv.get(dst_val, []):\n                # Make sure the *src* register was read.\n                if src_reg not in read_regs:\n                    continue\n\n                # Check restrictions...\n                if self._arch_regs_size[src_reg] != self._arch_regs_size[dst_reg]:\n                    continue\n\n                if src_reg == dst_reg:\n                    continue\n\n                if regs_init[dst_reg] == regs_init[src_reg]:\n                    continue\n\n                src_reg_ir = ReilRegisterOperand(src_reg, self._arch_regs_size[src_reg])\n                dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n\n                matches.append({\n                    \"src\": [src_reg_ir],\n                    \"dst\": [dst_reg_ir]\n                })\n\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _classify_load_constant(self, regs_init, regs_fini, mem_fini, written_regs, read_regs):\n        matches = []\n\n        # Check for \"dst_reg <- constant\" pattern.\n        for dst_reg, dst_val in regs_fini.items():\n            # Make sure the *dst* register was written.\n            if dst_reg not in written_regs:\n                continue\n\n            # Check restrictions...\n            if dst_val == regs_init[dst_reg]:\n                continue\n\n            dst_val_ir = ReilImmediateOperand(dst_val, self._arch_regs_size[dst_reg])\n            dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n\n            matches.append({\n                \"src\": [dst_val_ir],\n                \"dst\": [dst_reg_ir]\n            })\n\n        return matches", "response": "Classify load - constant gadgets."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclassify load - memory gadgets.", "response": "def _classify_load_memory(self, regs_init, regs_fini, mem_fini, written_regs, read_regs):\n        \"\"\"Classify load-memory gadgets.\n        \"\"\"\n        matches = []\n\n        regs_init_inv = self._invert_dictionary(regs_init)\n\n        # Check for \"dst_reg <- mem[src_reg + offset]\" pattern.\n        for dst_reg, dst_val in regs_fini.items():\n            # Make sure the *dst* register was written.\n            if dst_reg not in written_regs:\n                continue\n\n            dst_size = self._arch_regs_size[dst_reg]\n\n            # Look for memory addresses that contain *dst_val*.\n            for src_addr in mem_fini.read_inverse(dst_val, dst_size // 8):\n\n                # Look for registers whose values are used as memory\n                # addresses.\n                for src_reg, src_val in regs_init.items():\n                    # Make sure the *src* register was read.\n                    if src_reg not in read_regs:\n                        continue\n\n                    # Check restrictions.\n                    if self._arch_regs_size[src_reg] != self._address_size:\n                        continue\n\n                    offset = (src_addr - src_val) & (2**self._address_size - 1)\n\n                    src_reg_ir = ReilRegisterOperand(src_reg, self._arch_regs_size[src_reg])\n                    src_off_ir = ReilImmediateOperand(offset, self._arch_regs_size[src_reg])\n                    dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n\n                    matches.append({\n                        \"src\": [src_reg_ir, src_off_ir],\n                        \"dst\": [dst_reg_ir]\n                    })\n\n        # Check for \"dst_reg <- mem[offset]\" pattern.\n        for dst_reg, dst_val in regs_fini.items():\n            # Make sure the *dst* register was written.\n            if dst_reg not in written_regs:\n                continue\n\n            dst_size = self._arch_regs_size[dst_reg]\n\n            for src_addr in mem_fini.read_inverse(dst_val, dst_size // 8):\n                src_reg_ir = ReilEmptyOperand()\n                src_off_ir = ReilImmediateOperand(src_addr, self._address_size)\n                dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n\n                matches.append({\n                    \"src\": [src_reg_ir, src_off_ir],\n                    \"dst\": [dst_reg_ir]\n                })\n\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _classify_store_memory(self, regs_init, regs_fini, mem_fini, written_regs, read_regs):\n        matches = []\n\n        regs_init_inv = self._invert_dictionary(regs_init)\n\n        # Check for \"mem[dst_reg + offset] <- src_reg\" pattern.\n        for src_reg, src_val in regs_init.items():\n            # Make sure the *src* register was read.\n            if src_reg not in read_regs:\n                continue\n\n            src_size = self._arch_regs_size[src_reg]\n\n            for addr in mem_fini.read_inverse(src_val, src_size // 8):\n                for dst_reg, dst_val in regs_init.items():\n                    # Make sure the *dst* register was written.\n                    if dst_reg not in read_regs:\n                        continue\n\n                    # Check restrictions.\n                    if self._arch_regs_size[dst_reg] != self._address_size:\n                        continue\n\n                    offset = (addr - dst_val) & (2**self._address_size - 1)\n\n                    src_reg_ir = ReilRegisterOperand(src_reg, self._arch_regs_size[src_reg])\n                    dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n                    dst_off_ir = ReilImmediateOperand(offset, self._arch_regs_size[dst_reg])\n\n                    matches.append({\n                        \"src\": [src_reg_ir],\n                        \"dst\": [dst_reg_ir, dst_off_ir]\n                    })\n\n        # Check for \"mem[offset] <- src_reg\" pattern.\n        for src_reg, src_val in regs_init.items():\n            # Make sure the *src* register was read.\n            if src_reg not in read_regs:\n                continue\n\n            src_size = self._arch_regs_size[src_reg]\n\n            for addr in mem_fini.read_inverse(src_val, src_size // 8):\n                offset = addr & (2**self._address_size - 1)\n\n                src_reg_ir = ReilRegisterOperand(src_reg, self._arch_regs_size[src_reg])\n                dst_reg_ir = ReilEmptyOperand()\n                dst_off_ir = ReilImmediateOperand(offset, self._address_size)\n\n                matches.append({\n                    \"src\": [src_reg_ir],\n                    \"dst\": [dst_reg_ir, dst_off_ir]\n                })\n\n        return matches", "response": "Classify the store - memory gadgets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _classify_arithmetic_load(self, regs_init, regs_fini, mem_fini, written_regs, read_regs):\n        matches = []\n\n        # Check for \"dst_reg <- dst_reg OP mem[src_reg + offset]\" pattern.\n        for op_name, op_fn in self._binary_ops.items():\n            for dst_reg, dst_val in regs_fini.items():\n                # Make sure the *dst* register was read and written.\n                if dst_reg not in written_regs or dst_reg not in read_regs:\n                    continue\n\n                dst_size = self._arch_regs_size[dst_reg]\n\n                for addr in mem_fini.get_addresses():\n                    success, val = mem_fini.try_read(addr, dst_size // 8)\n\n                    if success and dst_val == op_fn(regs_init[dst_reg], val) & (2**dst_size - 1):\n\n                        for src_reg, src_val in regs_init.items():\n                            # Make sure the *src* register was read.\n                            if src_reg not in read_regs:\n                                continue\n\n                            # Check restrictions.\n                            if self._arch_regs_size[src_reg] != self._address_size:\n                                continue\n\n                            offset = (addr - src_val) & (2**self._address_size - 1)\n\n                            src_reg_ir = ReilRegisterOperand(src_reg, self._arch_regs_size[src_reg])\n                            src_off_ir = ReilImmediateOperand(offset, self._address_size)\n                            dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n\n                            matches.append({\n                                \"src\": [dst_reg_ir, src_reg_ir, src_off_ir],\n                                \"dst\": [dst_reg_ir],\n                                \"op\":  op_name\n                            })\n\n        # Check for \"dst_reg <- dst_reg OP mem[offset]\" pattern.\n        for op_name, op_fn in self._binary_ops.items():\n            for dst_reg, dst_val in regs_fini.items():\n                # Make sure the *dst* register was read and written.\n                if dst_reg not in written_regs or dst_reg not in read_regs:\n                    continue\n\n                dst_size = self._arch_regs_size[dst_reg]\n\n                for addr in mem_fini.get_addresses():\n                    success, val = mem_fini.try_read(addr, dst_size // 8)\n\n                    if success and dst_val == op_fn(regs_init[dst_reg], val) & (2**dst_size - 1):\n                        src_reg_ir = ReilEmptyOperand()\n                        src_off_ir = ReilImmediateOperand(addr, self._address_size)\n                        dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n\n                        matches.append({\n                            \"src\": [dst_reg_ir, src_reg_ir, src_off_ir],\n                            \"dst\": [dst_reg_ir],\n                            \"op\":  op_name\n                        })\n\n        return matches", "response": "Classify arithmetic - load gadgets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclassifying arithmetic - store gadgets.", "response": "def _classify_arithmetic_store(self, regs_init, regs_fini, mem_fini, written_regs, read_regs):\n        \"\"\"Classify arithmetic-store gadgets.\n        \"\"\"\n        matches = []\n\n        # Check for \"m[dst_reg + offset] <- m[dst_reg + offset] OP src_reg\" pattern.\n        for op_name, op_fn in self._binary_ops.items():\n            for size in [8, 16, 32, 64]:\n                for addr in mem_fini.get_addresses():\n                    success_read_curr, val_curr = mem_fini.try_read(addr, size // 8)\n                    success_read_prev, val_prev = mem_fini.try_read_prev(addr, size // 8)\n\n                    if success_read_curr and success_read_prev:\n                        for src_reg, src_val in regs_init.items():\n                            # Make sure the *src* register was read.\n                            if src_reg not in read_regs:\n                                continue\n\n                            # Check restrictions.\n                            if self._arch_regs_size[src_reg] != size:\n                                continue\n\n                            if val_curr == op_fn(src_val, val_prev) & (2**size - 1):\n                                # find dst + offset\n                                for dst_reg, dst_val in regs_init.items():\n                                    # Make sure the *dst* register was written.\n                                    if dst_reg not in read_regs:\n                                        continue\n\n                                    # Check restrictions.\n                                    if self._arch_regs_size[dst_reg] != self._address_size:\n                                        continue\n\n                                    offset = (addr - dst_val) & (2**self._address_size - 1)\n\n                                    src_reg_ir = ReilRegisterOperand(src_reg, self._arch_regs_size[src_reg])\n                                    dst_reg_ir = ReilRegisterOperand(dst_reg, self._arch_regs_size[dst_reg])\n                                    dst_off_ir = ReilImmediateOperand(offset, self._address_size)\n\n                                    matches.append({\n                                        \"src\": [dst_reg_ir, dst_off_ir, src_reg_ir],\n                                        \"dst\": [dst_reg_ir, dst_off_ir],\n                                        \"op\":  op_name\n                                    })\n\n        # Check for \"m[offset] <- m[offset] OP src_reg\" pattern.\n        for op_name, op_fn in self._binary_ops.items():\n            for size in [8, 16, 32, 64]:\n                for addr in mem_fini.get_addresses():\n                    success_read_curr, val_curr = mem_fini.try_read(addr, size // 8)\n                    success_read_prev, val_prev = mem_fini.try_read_prev(addr, size // 8)\n\n                    if success_read_curr and success_read_prev:\n                        for src_reg, src_val in regs_init.items():\n                            # Make sure the *src* register was read.\n                            if src_reg not in read_regs:\n                                continue\n\n                            # Check restrictions.\n                            if self._arch_regs_size[src_reg] != size:\n                                continue\n\n                            if val_curr == op_fn(src_val, val_prev) & (2**size - 1):\n                                src_reg_ir = ReilRegisterOperand(src_reg, self._arch_regs_size[src_reg])\n                                dst_reg_ir = ReilEmptyOperand()\n                                dst_off_ir = ReilImmediateOperand(addr, self._address_size)\n\n                                matches.append({\n                                    \"src\": [dst_reg_ir, dst_off_ir, src_reg_ir],\n                                    \"dst\": [dst_reg_ir, dst_off_ir],\n                                    \"op\":  op_name\n                                })\n\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _classify(self, gadget, classifier, gadget_type, iters):\n        # Collect REIL instructions of the gadgets.\n        instrs = [ir_instr for asm_instr in gadget.instrs for ir_instr in asm_instr.ir_instrs]\n\n        # Repeat classification.\n        results = []\n\n        for _ in range(iters):\n            # Reset emulator.\n            self._ir_emulator.reset()\n\n            # Generate random values for registers.\n            regs_initial = self._init_regs_random()\n\n            # Emulate gadgets.\n            try:\n                regs_final, mem_final = self._ir_emulator.execute_lite(\n                    instrs,\n                    regs_initial\n                )\n            except:\n                # Catch emulator exceptions like ZeroDivisionError, etc.\n                results += [([], [])]\n\n                continue\n\n            # Compute values for all registers. For example, in x86, it\n            # computes 'al' from 'eax'.\n            regs_initial_full = self._compute_full_context(regs_initial)\n            regs_final_full = self._compute_full_context(regs_final)\n\n            # Get written and read registers.\n            regs_written = self._ir_emulator.written_registers\n            regs_read = self._ir_emulator.read_registers\n\n            # Compute modified registers.\n            mod_regs = self._compute_mod_regs(\n                regs_initial_full,\n                regs_final_full\n            )\n\n            # Classified gadgets based on initial and final context.\n            matches = classifier(\n                regs_initial_full,\n                regs_final_full,\n                mem_final,\n                regs_written,\n                regs_read\n            )\n\n            # Save results.\n            results += [(matches, mod_regs)]\n\n        # Analyze results and compute candidate gadgets.\n        candidates, mod_regs = self._analyze_execution_results(results)\n\n        # Create classified gadgets.\n        classified = self._create_typed_gadgets(\n            gadget,\n            candidates,\n            mod_regs,\n            gadget_type\n        )\n\n        return classified", "response": "Classify gadgets based on the given classifier and the number of iterations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_regs_random(self):\n        # Generate random values and make sure they are all different.\n        values = set()\n\n        while len(values) != len(self._arch_regs_parent):\n            values.add(random.randint(0, 2**self._arch_info.operand_size - 1))\n\n        values = list(values)\n\n        # Assign random values to registers.\n        regs = {}\n\n        for idx, reg in enumerate(self._arch_regs_parent):\n            regs[reg] = values[idx] & (2**self._arch_regs_size[reg] - 1)\n\n        return regs", "response": "Initialize the register with random values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisassembles the data into an instruction.", "response": "def disassemble(self, data, address, architecture_mode=ARCH_X86_MODE_32):\n        \"\"\"Disassemble the data into an instruction.\n        \"\"\"\n        asm, size = self._cs_disassemble_one(data, address)\n\n        instr = self._parser.parse(asm) if asm else None\n\n        if instr:\n            instr.address = address\n            instr.size = size\n            instr.bytes = data[0:size]\n        else:\n            raise DisassemblerError()\n\n        return instr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisassembling the data into an instruction in string form.", "response": "def _cs_disassemble_one(self, data, address):\n        \"\"\"Disassemble the data into an instruction in string form.\n        \"\"\"\n        asm, size = \"\", 0\n\n        disasm = list(self._disassembler.disasm_lite(bytes(data), address))\n\n        if len(disasm) > 0:\n            address, size, mnemonic, op_str = disasm[0]\n\n            asm = str(mnemonic + \" \" + op_str).strip()\n\n        # Quick fix for Capstone 'bug'.\n        if asm in [\"repne\", \"rep\", \"lock\", \"data16\"]:\n            asm, size = \"\", 0\n\n        return asm, size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve_memory_access(self, tb, x86_mem_operand):\n        size = self.__get_memory_access_size(x86_mem_operand)\n\n        addr = None\n\n        if x86_mem_operand.base:\n            addr = ReilRegisterOperand(x86_mem_operand.base, size)\n\n        if x86_mem_operand.index and x86_mem_operand.scale != 0x0:\n            index = ReilRegisterOperand(x86_mem_operand.index, size)\n            scale = ReilImmediateOperand(x86_mem_operand.scale, size)\n            scaled_index = tb.temporal(size)\n\n            tb.add(tb._builder.gen_mul(index, scale, scaled_index))\n\n            if addr:\n                tmp = tb.temporal(size)\n\n                tb.add(tb._builder.gen_add(addr, scaled_index, tmp))\n\n                addr = tmp\n            else:\n                addr = scaled_index\n\n        if x86_mem_operand.displacement != 0x0:\n            disp = ReilImmediateOperand(x86_mem_operand.displacement, size)\n\n            if addr:\n                tmp = tb.temporal(size)\n\n                tb.add(tb._builder.gen_add(addr, disp, tmp))\n\n                addr = tmp\n            else:\n                addr = disp\n        else:\n            if not addr:\n                disp = ReilImmediateOperand(x86_mem_operand.displacement, size)\n\n                addr = disp\n\n        # TODO Improve this code and add support for the rest of the segments.\n        if x86_mem_operand.segment in [\"gs\", \"fs\"]:\n            seg_base_addr_map = {\n                \"gs\": \"gs_base_addr\",\n                \"fs\": \"fs_base_addr\",\n            }\n\n            seg_base = ReilRegisterOperand(seg_base_addr_map[x86_mem_operand.segment], size)\n\n            if addr:\n                tmp = tb.temporal(size)\n\n                tb.add(tb._builder.gen_add(addr, seg_base, tmp))\n\n                addr = tmp\n            else:\n                addr = seg_base\n\n        return addr", "response": "Resolve operand memory access translation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the maximum instruction mnemonic width in a basic block.", "response": "def bb_get_instr_max_width(basic_block):\n    \"\"\"Get maximum instruction mnemonic width\n    \"\"\"\n    asm_mnemonic_max_width = 0\n\n    for instr in basic_block:\n        if len(instr.mnemonic) > asm_mnemonic_max_width:\n            asm_mnemonic_max_width = len(instr.mnemonic)\n\n    return asm_mnemonic_max_width"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the end address of the basic block.", "response": "def end_address(self):\n        \"\"\"Get basic block end address.\n        \"\"\"\n        if self._instrs is []:\n            return None\n\n        return self._instrs[-1].address + self._instrs[-1].size - 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets basic block size.", "response": "def size(self):\n        \"\"\"Get basic block size.\n        \"\"\"\n        if self._instrs is []:\n            return None\n\n        return sum([instr.size for instr in self._instrs])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets basic block branches.", "response": "def branches(self):\n        \"\"\"Get basic block branches.\n        \"\"\"\n        branches = []\n\n        if self._taken_branch:\n            branches += [(self._taken_branch, 'taken')]\n\n        if self._not_taken_branch:\n            branches += [(self._not_taken_branch, 'not-taken')]\n\n        if self._direct_branch:\n            branches += [(self._direct_branch, 'direct')]\n\n        return branches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the address of the memory in the ARM.", "response": "def _compute_memory_address(self, mem_operand):\n        \"\"\"Return operand memory access translation.\n        \"\"\"\n        base = ReilRegisterOperand(mem_operand.base_reg.name, mem_operand.size)\n\n        if mem_operand.displacement:\n            address = self.temporal(mem_operand.size)\n\n            if isinstance(mem_operand.displacement, ArmRegisterOperand):\n                disp = ReilRegisterOperand(mem_operand.displacement.name, mem_operand.size)\n            elif isinstance(mem_operand.displacement, ArmImmediateOperand):\n                disp = ReilImmediateOperand(mem_operand.displacement.immediate, mem_operand.size)\n            elif isinstance(mem_operand.displacement, ArmShiftedRegisterOperand):\n                disp = self._compute_shifted_register(mem_operand.displacement)\n            else:\n                raise Exception(\"_compute_memory_address: displacement type unknown\")\n\n            if mem_operand.index_type == ARM_MEMORY_INDEX_PRE:\n                if mem_operand.disp_minus:\n                    self.add(self._builder.gen_sub(base, disp, address))\n                else:\n                    self.add(self._builder.gen_add(base, disp, address))\n                self.add(self._builder.gen_str(address, base))\n            elif mem_operand.index_type == ARM_MEMORY_INDEX_OFFSET:\n                if mem_operand.disp_minus:\n                    self.add(self._builder.gen_sub(base, disp, address))\n                else:\n                    self.add(self._builder.gen_add(base, disp, address))\n            elif mem_operand.index_type == ARM_MEMORY_INDEX_POST:\n                self.add(self._builder.gen_str(base, address))\n                tmp = self.temporal(base.size)\n                if mem_operand.disp_minus:\n                    self.add(self._builder.gen_sub(base, disp, tmp))\n                else:\n                    self.add(self._builder.gen_add(base, disp, tmp))\n                self.add(self._builder.gen_str(tmp, base))\n            else:\n                raise Exception(\"_compute_memory_address: indexing type unknown\")\n\n        else:\n            address = base\n\n        return address"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the list of register items for the given operand.", "response": "def _compute_register_list(self, operand):\n        \"\"\"Return operand register list.\n        \"\"\"\n\n        ret = []\n        for reg_range in operand.reg_list:\n            if len(reg_range) == 1:\n                ret.append(ReilRegisterOperand(reg_range[0].name, reg_range[0].size))\n            else:\n                reg_num = int(reg_range[0].name[1:])    # Assuming the register is named with one letter + number\n                reg_end = int(reg_range[1].name[1:])\n                if reg_num > reg_end:\n                    raise NotImplementedError(\"Instruction Not Implemented: Invalid register range.\")\n                while reg_num <= reg_end:\n                    ret.append(ReilRegisterOperand(reg_range[0].name[0] + str(reg_num), reg_range[0].size))\n                    reg_num = reg_num + 1\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntranslates an instruction into an IR representation of an instruction.", "response": "def translate(self, instruction):\n        \"\"\"Return IR representation of an instruction.\n        \"\"\"\n        try:\n            trans_instrs = self.__translate(instruction)\n        except NotImplementedError:\n            unkn_instr = self._builder.gen_unkn()\n            unkn_instr.address = instruction.address << 8 | (0x0 & 0xff)\n            trans_instrs = [unkn_instr]\n\n            self._log_not_supported_instruction(instruction)\n        except Exception:\n            self._log_translation_exception(instruction)\n\n            raise\n\n        return trans_instrs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, address, size):\n        value = 0x0\n\n        for i in range(0, size):\n            value |= self._read_byte(address + i) << (i * 8)\n\n        return value", "response": "Read arbitrary size content from memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a byte from memory.", "response": "def _read_byte(self, address):\n        \"\"\"Read a byte from memory.\n        \"\"\"\n        # Initialize memory location with a random value.\n        if address not in self._memory:\n            self._memory[address] = random.randint(0x00, 0xff)\n\n        return self._memory[address]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites arbitrary size content to memory.", "response": "def write(self, address, size, value):\n        \"\"\"Write arbitrary size content to memory.\n        \"\"\"\n        for i in range(0, size):\n            self.__write_byte(address + i, (value >> (i * 8)) & 0xff)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of memory addresses that contain the specified value.", "response": "def read_inverse(self, value, size):\n        \"\"\"Return a list of memory addresses that contain the specified\n        value.\n\n        \"\"\"\n        addr_candidates = [addr for addr, val in self._memory.items() if val == (value & 0xff)]\n        addr_matches = []\n\n        for addr in addr_candidates:\n            match = True\n\n            for i in range(0, size):\n                byte_curr = (value >> (i * 8)) & 0xff\n                try:\n                    match = self._memory[addr + i] == byte_curr\n                except KeyError:\n                    match = False\n\n                if not match:\n                    break\n\n            if match:\n                addr_matches += [addr]\n\n        return addr_matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef try_read(self, address, size):\n        value = 0x0\n\n        for i in range(0, size):\n            addr = address + i\n\n            if addr in self._memory:\n                value |= self._read_byte(addr) << (i * 8)\n            else:\n                return False, None\n\n        return True, value", "response": "Try to read memory content at specified address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to read previous memory content at specified address.", "response": "def try_read_prev(self, address, size):\n        \"\"\"Try to read previous memory content at specified address.\n\n        If any location was not written before, it returns a tuple\n        (False, None). Otherwise, it returns (True, memory content).\n\n        \"\"\"\n        value = 0x0\n\n        for i in range(0, size):\n            addr = address + i\n\n            if addr in self.__memory_prev:\n                _, val_byte = self.__try_read_byte_prev(addr)\n                value |= val_byte << (i * 8)\n            else:\n                return False, None\n\n        return True, value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __try_read_byte_prev(self, address):\n        # Initialize memory location with a random value\n        if address not in self.__memory_prev:\n            return False, None\n\n        return True, self.__memory_prev[address]", "response": "Try to read the previous value for memory location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __write_byte(self, address, value):\n        # Save previous address content.\n        if address in self._memory:\n            self.__memory_prev[address] = self._memory[address]\n\n        self._memory[address] = value & 0xff", "response": "Write a byte to memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_add(src1, src2, dst):\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.ADD, src1, src2, dst)", "response": "Return an ADD instruction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a SUB instruction.", "response": "def gen_sub(src1, src2, dst):\n        \"\"\"Return a SUB instruction.\n        \"\"\"\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.SUB, src1, src2, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gen_mul(src1, src2, dst):\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.MUL, src1, src2, dst)", "response": "Return a MUL instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_div(src1, src2, dst):\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.DIV, src1, src2, dst)", "response": "Return a DIV instruction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_mod(src1, src2, dst):\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.MOD, src1, src2, dst)", "response": "Return a MOD instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a BSH instruction.", "response": "def gen_bsh(src1, src2, dst):\n        \"\"\"Return a BSH instruction.\n        \"\"\"\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.BSH, src1, src2, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an AND instruction.", "response": "def gen_and(src1, src2, dst):\n        \"\"\"Return an AND instruction.\n        \"\"\"\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.AND, src1, src2, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an OR instruction.", "response": "def gen_or(src1, src2, dst):\n        \"\"\"Return an OR instruction.\n        \"\"\"\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.OR, src1, src2, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_xor(src1, src2, dst):\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.XOR, src1, src2, dst)", "response": "Return a XOR instruction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_ldm(src, dst):\n        return ReilBuilder.build(ReilMnemonic.LDM, src, ReilEmptyOperand(), dst)", "response": "Return a LDM instruction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a STM instruction.", "response": "def gen_stm(src, dst):\n        \"\"\"Return a STM instruction.\n        \"\"\"\n        return ReilBuilder.build(ReilMnemonic.STM, src, ReilEmptyOperand(), dst)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gen_str(src, dst):\n        return ReilBuilder.build(ReilMnemonic.STR, src, ReilEmptyOperand(), dst)", "response": "Return a STR instruction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_bisz(src, dst):\n        return ReilBuilder.build(ReilMnemonic.BISZ, src, ReilEmptyOperand(), dst)", "response": "Return a BISZ instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gen_jcc(src, dst):\n        return ReilBuilder.build(ReilMnemonic.JCC, src, ReilEmptyOperand(), dst)", "response": "Return a JCC instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_unkn():\n        empty_reg = ReilEmptyOperand()\n\n        return ReilBuilder.build(ReilMnemonic.UNKN, empty_reg, empty_reg, empty_reg)", "response": "Return an UNKN instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_undef():\n        empty_reg = ReilEmptyOperand()\n\n        return ReilBuilder.build(ReilMnemonic.UNDEF, empty_reg, empty_reg, empty_reg)", "response": "Return an UNDEF instruction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a NOP instruction.", "response": "def gen_nop():\n        \"\"\"Return a NOP instruction.\n        \"\"\"\n        empty_reg = ReilEmptyOperand()\n\n        return ReilBuilder.build(ReilMnemonic.NOP, empty_reg, empty_reg, empty_reg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_sext(src, dst):\n        assert src.size <= dst.size\n\n        empty_reg = ReilEmptyOperand()\n\n        return ReilBuilder.build(ReilMnemonic.SEXT, src, empty_reg, dst)", "response": "Return a SEXT instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a SDIV instruction.", "response": "def gen_sdiv(src1, src2, dst):\n        \"\"\"Return a SDIV instruction.\n        \"\"\"\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.SDIV, src1, src2, dst)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_smod(src1, src2, dst):\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.SMOD, src1, src2, dst)", "response": "Return a SMOD instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gen_smul(src1, src2, dst):\n        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.SMUL, src1, src2, dst)", "response": "Return a SMUL instruction."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a new instruction from the specified mnemonic and operands.", "response": "def build(mnemonic, oprnd1, oprnd2, oprnd3):\n        \"\"\"Return the specified instruction.\n        \"\"\"\n        ins = ReilInstruction()\n\n        ins.mnemonic = mnemonic\n        ins.operands = [oprnd1, oprnd2, oprnd3]\n\n        return ins"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setup_arch(self, arch_mode=None):\n        # set up architecture information\n        self.arch_info = None\n\n        if self.binary.architecture == ARCH_X86:\n            self._setup_x86_arch(arch_mode)\n        else:\n            # TODO: add arch to the binary file class.\n            self._setup_arm_arch(arch_mode)", "response": "Set up architecture information."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setup_arm_arch(self, arch_mode=None):\n        if arch_mode is None:\n            arch_mode = ARCH_ARM_MODE_THUMB\n\n        self.name = \"ARM\"\n        self.arch_info = ArmArchitectureInformation(arch_mode)\n        self.disassembler = ArmDisassembler(architecture_mode=arch_mode)\n        self.ir_translator = ArmTranslator(architecture_mode=arch_mode)", "response": "Set up ARM architecture."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _setup_x86_arch(self, arch_mode=None):\n        if arch_mode is None:\n            arch_mode = self.binary.architecture_mode\n\n        # Set up architecture information\n        self.name = \"x86\"\n        self.arch_info = X86ArchitectureInformation(arch_mode)\n        self.disassembler = X86Disassembler(arch_mode)\n        self.ir_translator = X86Translator(arch_mode)", "response": "Setup the x86 architecture."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup_core_modules(self):\n        self.ir_emulator = None\n        self.smt_solver = None\n        self.smt_translator = None\n\n        if self.arch_info:\n            # Set REIL emulator.\n            self.ir_emulator = ReilEmulator(self.arch_info)\n\n            # Set SMT Solver.\n            self.smt_solver = None\n\n            if SMT_SOLVER not in (\"Z3\", \"CVC4\"):\n                raise Exception(\"{} SMT solver not supported.\".format(SMT_SOLVER))\n\n            try:\n                if SMT_SOLVER == \"Z3\":\n                    self.smt_solver = Z3Solver()\n                elif SMT_SOLVER == \"CVC4\":\n                    self.smt_solver = CVC4Solver()\n            except SmtSolverNotFound:\n                logger.warn(\"{} Solver is not installed. Run 'barf-install-solvers.sh' to install it.\".format(SMT_SOLVER))\n\n            # Set SMT translator.\n            self.smt_translator = None\n\n            if self.smt_solver:\n                self.smt_translator = SmtTranslator(self.smt_solver, self.arch_info.address_size)\n\n                self.smt_translator.set_arch_alias_mapper(self.arch_info.alias_mapper)\n                self.smt_translator.set_arch_registers_size(self.arch_info.registers_size)", "response": "Set up core modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup_analysis_modules(self):\n        # Basic block.\n        self.bb_builder = CFGRecoverer(RecursiveDescent(self.disassembler, self.text_section, self.ir_translator,\n                                                        self.arch_info))\n\n        # Code analyzer.\n        self.code_analyzer = None\n\n        if self.smt_translator:\n            self.code_analyzer = CodeAnalyzer(self.smt_solver, self.smt_translator, self.arch_info)\n\n        # Gadgets classifier.\n        self.gadget_classifier = GadgetClassifier(self.ir_emulator, self.arch_info)\n\n        # Gadgets finder.\n        self.gadget_finder = GadgetFinder(self.disassembler, self.text_section, self.ir_translator,\n                                          self.binary.architecture, self.binary.architecture_mode)\n\n        # Gadget verifier.\n        self.gadget_verifier = None\n\n        if self.code_analyzer:\n            self.gadget_verifier = GadgetVerifier(self.code_analyzer, self.arch_info)\n\n        self.emulator = Emulator(self.arch_info, self.ir_emulator, self.ir_translator, self.disassembler)", "response": "Setup the analysis modules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef open(self, filename):\n        if filename:\n            self.binary = BinaryFile(filename)\n            self.text_section = self.binary.text_section\n\n            self._load(arch_mode=self.binary.architecture_mode)", "response": "Open a file for analysis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntranslating to REIL instructions.", "response": "def load_architecture(self, name, arch_info, disassembler, translator):\n        \"\"\"Translate to REIL instructions.\n\n        Args:\n            name (str): Architecture's name.\n            arch_info (ArchitectureInformation): Architecture information object.\n            disassembler (Disassembler): Disassembler for the architecture.\n            translator (Translator): Translator for the architecture.\n        \"\"\"\n        # Set up architecture information.\n        self.name = name\n        self.arch_info = arch_info\n        self.disassembler = disassembler\n        self.ir_translator = translator\n\n        # Setup analysis modules.\n        self._setup_analysis_modules()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef translate(self, start=None, end=None, arch_mode=None):\n        start_addr = start if start else self.binary.ea_start\n        end_addr = end if end else self.binary.ea_end\n\n        self.ir_translator.reset()\n\n        for addr, asm, _ in self.disassemble(start=start_addr, end=end_addr, arch_mode=arch_mode):\n            yield addr, asm, self.ir_translator.translate(asm)", "response": "Translate to REIL instructions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisassemble native instructions. Args: start (int): Start address. end (int): End address. arch_mode (int): Architecture mode. Returns: (int, Instruction, int): A tuple of the form (address, assembler instruction, instruction size).", "response": "def disassemble(self, start=None, end=None, arch_mode=None):\n        \"\"\"Disassemble native instructions.\n\n        Args:\n            start (int): Start address.\n            end (int): End address.\n            arch_mode (int): Architecture mode.\n\n        Returns:\n            (int, Instruction, int): A tuple of the form (address, assembler instruction, instruction size).\n        \"\"\"\n        if arch_mode is None:\n            arch_mode = self.binary.architecture_mode\n\n        curr_addr = start if start else self.binary.ea_start\n        end_addr = end if end else self.binary.ea_end\n\n        while curr_addr < end_addr:\n            # Fetch the instruction.\n            encoding = self.__fetch_instr(curr_addr)\n\n            # Decode it.\n            asm_instr = self.disassembler.disassemble(encoding, curr_addr, architecture_mode=arch_mode)\n\n            if not asm_instr:\n                return\n\n            yield curr_addr, asm_instr, asm_instr.size\n\n            # update instruction pointer\n            curr_addr += asm_instr.size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrecovering CFG. Args: start (int): Start address. end (int): End address. symbols (dict): Symbol table. callback (function): A callback function which is called after each successfully recovered CFG. arch_mode (int): Architecture mode. Returns: ControlFlowGraph: A CFG.", "response": "def recover_cfg(self, start=None, end=None, symbols=None, callback=None, arch_mode=None):\n        \"\"\"Recover CFG.\n\n        Args:\n            start (int): Start address.\n            end (int): End address.\n            symbols (dict): Symbol table.\n            callback (function): A callback function which is called after each successfully recovered CFG.\n            arch_mode (int): Architecture mode.\n\n        Returns:\n            ControlFlowGraph: A CFG.\n        \"\"\"\n        # Set architecture in case it wasn't already set.\n        if arch_mode is None:\n            arch_mode = self.binary.architecture_mode\n\n        # Reload modules.\n        self._load(arch_mode=arch_mode)\n\n        # Check start address.\n        start = start if start else self.binary.entry_point\n\n        cfg, _ = self._recover_cfg(start=start, end=end, symbols=symbols, callback=callback)\n\n        return cfg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrecover CFG for all functions from an entry point and or symbol table.", "response": "def recover_cfg_all(self, entries, symbols=None, callback=None, arch_mode=None):\n        \"\"\"Recover CFG for all functions from an entry point and/or symbol table.\n\n        Args:\n            entries (list): A list of function addresses' to start the CFG recovery process.\n            symbols (dict): Symbol table.\n            callback (function): A callback function which is called after each successfully recovered CFG.\n            arch_mode (int): Architecture mode.\n\n        Returns:\n            list: A list of recovered CFGs.\n        \"\"\"\n        # Set architecture in case it wasn't already set.\n        if arch_mode is None:\n            arch_mode = self.binary.architecture_mode\n\n        # Reload modules.\n        self._load(arch_mode=arch_mode)\n\n        # Set symbols.\n        symbols = {} if not symbols else symbols\n\n        # Recover the CFGs.\n        cfgs = []\n        addrs_processed = set()\n        calls = entries\n\n        while len(calls) > 0:\n            start, calls = calls[0], calls[1:]\n\n            cfg, calls_tmp = self._recover_cfg(start=start, symbols=symbols, callback=callback)\n\n            addrs_processed.add(start)\n\n            cfgs.append(cfg)\n\n            for addr in sorted(calls_tmp):\n                if addr not in addrs_processed and addr not in calls:\n                    calls.append(addr)\n\n        return cfgs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef emulate(self, context=None, start=None, end=None, arch_mode=None, hooks=None, max_instrs=None, print_asm=False):\n        if arch_mode is not None:\n            # Reload modules.\n            self._load(arch_mode=arch_mode)\n\n        context = context if context else {}\n\n        start_addr = start if start else self.binary.ea_start\n        end_addr = end if end else self.binary.ea_end\n\n        hooks = hooks if hooks else {}\n\n        # Load registers\n        for reg, val in context.get('registers', {}).items():\n            self.ir_emulator.registers[reg] = val\n\n        # Load memory\n        # TODO Memory content should be encoded as hex strings so each\n        # entry can be of different sizes.\n        for addr, val in context.get('memory', {}).items():\n            self.ir_emulator.memory.write(addr, 4, val)\n\n        # Execute the code.\n        self.emulator.emulate(start_addr, end_addr, hooks, max_instrs, print_asm)\n\n        context_out = {\n            'registers': {},\n            'memory': {}\n        }\n\n        # save registers\n        for reg, val in self.ir_emulator.registers.items():\n            context_out['registers'][reg] = val\n\n        return context_out", "response": "Emulate native code.\n\n        Args:\n            context (dict): Processor context (register and/or memory).\n            start (int): Start address.\n            end (int): End address.\n            arch_mode (int): Architecture mode.\n            hooks (dict): Hooks by address.\n            max_instrs (int): Maximum number of instructions to execute.\n            print_asm (bool): Print asm.\n\n        Returns:\n            dict: Processor context."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_x86_candidates(self, start_address, end_address):\n        roots = []\n\n        # find gadgets tail\n        for addr in xrange(start_address, end_address + 1):\n            # TODO: Make this 'speed improvement' architecture-agnostic\n            op_codes = [\n                0xc3,     # RET\n                0xc2,     # RET imm16\n                0xeb,     # JMP rel8\n                0xe8,     # CALL rel{16,32}\n                0xe9,     # JMP rel{16,32}\n                0xff,     # JMP/CALL r/m{16,32,64}\n            ]\n\n            if self._mem[addr] not in op_codes:\n                continue\n\n            try:\n                asm_instr = self._disasm.disassemble(\n                    self._mem[addr:min(addr+16, end_address + 1)],\n                    addr\n                )\n            except:\n                asm_instr = None\n\n            if not asm_instr:\n                continue\n\n            # restarts ir register numbering\n            self._ir_trans.reset()\n\n            try:\n                ins_ir = self._ir_trans.translate(asm_instr)\n            except:\n                continue\n\n            # build gadgets\n            if ins_ir[-1] and (ins_ir[-1].mnemonic == ReilMnemonic.JCC and isinstance(ins_ir[-1].operands[2], ReilRegisterOperand)):\n\n                # add for REX.W + FF /3 call instruction\n                if ins_ir[-1].mnemonic == ReilMnemonic.JCC:\n                    # try addr - 1\n                    try:\n                        asm_instr_1 = self._disasm.disassemble(\n                            self._mem[addr-1:min(addr+15, end_address + 1)],\n                            addr\n                        )\n\n                        self._ir_trans.reset()\n\n                        ins_ir_1 = self._ir_trans.translate(asm_instr_1)\n\n                        if ins_ir_1[-1].mnemonic == ReilMnemonic.JCC:\n                            addr = addr - 1\n                            asm_instr = asm_instr_1\n                            ins_ir = ins_ir_1\n                    except:\n                            pass\n\n                asm_instr.ir_instrs = ins_ir\n\n                root = GadgetTreeNode(asm_instr)\n\n                roots.append(root)\n\n                self._build_from(addr, root, start_address, self._instrs_depth)\n\n        # filter roots with no children\n        roots = [r for r in roots if len(r.get_children()) > 0]\n\n        # build gadgets\n        root_gadgets = [self._build_gadgets(r) for r in roots]\n\n        # flatten root gadgets list\n        candidates = [item for l in root_gadgets for item in l]\n\n        return candidates", "response": "Find possible X86 gadgets."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding possible ARM gadgets.", "response": "def _find_arm_candidates(self, start_address, end_address):\n        \"\"\"Finds possible 'RET-ended' gadgets.\n        \"\"\"\n        roots = []\n        gadget_tail_addr = []\n\n        # From ROPgadget:\n        free_jump_gadgets = [\n            b\"[\\x10-\\x19\\x1e]{1}\\xff\\x2f\\xe1\",  # bx   reg\n            b\"[\\x30-\\x39\\x3e]{1}\\xff\\x2f\\xe1\",  # blx  reg\n            b\"[\\x00-\\xff]{1}\\x80\\xbd\\xe8\",      # pop {,pc}\n        ]\n\n        # find gadgets tail\n        for addr in xrange(start_address, end_address + 1):\n            # TODO: Make this 'speed improvement' architecture-agnostic\n            # TODO: Add thumb\n            # TODO: Little-Endian\n\n            # TODO: Evaluate performance\n            gad_found = False\n            for gad in free_jump_gadgets:\n                if len(re.findall(gad, self._mem[addr:min(addr+4, end_address + 1)])) > 0:     # TODO: Add thumb (+2)\n                    gad_found = True\n                    break\n            if not gad_found:\n                continue\n\n            gadget_tail_addr.append(addr)\n\n        for addr in gadget_tail_addr:\n            try:\n                asm_instr = self._disasm.disassemble(\n                    self._mem[addr:min(addr+4, end_address + 1)],   # TODO: Add thumb (+16)\n                    addr,\n                    architecture_mode=self._architecture_mode\n                )\n            except:\n                asm_instr = None\n\n            if not asm_instr:\n                continue\n\n            # restarts ir register numbering\n            self._ir_trans.reset()\n\n            try:\n                ins_ir = self._ir_trans.translate(asm_instr)\n            except:\n                continue\n\n            asm_instr.ir_instrs = ins_ir\n\n            root = GadgetTreeNode(asm_instr)\n\n            roots.append(root)\n\n            self._build_from(addr, root, start_address, self._instrs_depth)\n\n        # filter roots with no children\n        roots = [r for r in roots if len(r.get_children()) > 0]\n\n        # build gadgets\n        root_gadgets = [self._build_gadgets(r) for r in roots]\n\n        # flatten root gadgets list\n        candidates = [item for l in root_gadgets for item in l]\n\n        return candidates"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_from(self, address, root, base_address, depth=2):\n        if depth == 0:\n            return\n\n        end_addr = address\n\n        for step in range(1, self._max_bytes + 1):\n            start_addr = address - step\n\n            if start_addr < 0 or start_addr < base_address:\n                break\n\n            raw_bytes = self._mem[start_addr:end_addr]\n\n            # TODO: Improve this code.\n            if self._architecture == ARCH_ARM:\n                try:\n                    asm_instr = self._disasm.disassemble(raw_bytes, start_addr, architecture_mode=self._architecture_mode)\n                except InvalidDisassemblerData:\n                    continue\n            else:\n                try:\n                    asm_instr = self._disasm.disassemble(raw_bytes, start_addr)\n                except:\n                    asm_instr = None\n\n            if not asm_instr or asm_instr.size != step:\n                continue\n\n            try:\n                ir_instrs = self._ir_trans.translate(asm_instr)\n            except:\n                continue\n\n            if self._is_valid_ins(ir_instrs):\n                asm_instr.ir_instrs = ir_instrs\n\n                child = GadgetTreeNode(asm_instr)\n\n                root.add_child(child)\n\n                self._build_from(address - step, child, base_address, depth - 1)", "response": "Builds gadgets from the given address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a gadgets list.", "response": "def _build_gadgets(self, gadget_tree_root):\n        \"\"\"Return a gadgets list.\n        \"\"\"\n        node_list = self._build_gadgets_rec(gadget_tree_root)\n\n        return [RawGadget(n) for n in node_list]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_gadgets_rec(self, gadget_tree_root):\n        root = gadget_tree_root.get_root()\n        children = gadget_tree_root.get_children()\n\n        node_list = []\n\n        root_gadget_ins = root\n\n        if not children:\n            node_list += [[root_gadget_ins]]\n        else:\n            for child in children:\n                node_list_rec = self._build_gadgets_rec(child)\n\n                node_list += [n + [root_gadget_ins] for n in node_list_rec]\n\n        return node_list", "response": "Build a list of gadgets from a gadgets tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks for instruction validity as a gadgets.", "response": "def _is_valid_ins(self, ins_ir):\n        \"\"\"Check for instruction validity as a gadgets.\n        \"\"\"\n        invalid_instrs = [\n            ReilMnemonic.JCC,\n            ReilMnemonic.UNDEF,\n            ReilMnemonic.UNKN,\n        ]\n\n        return not any([i.mnemonic in invalid_instrs for i in ins_ir])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning REIL representation of an instruction.", "response": "def translate(self, instruction):\n        \"\"\"Return REIL representation of an instruction.\n        \"\"\"\n        try:\n            trans_instrs = self._translate(instruction)\n        except Exception:\n            self._log_translation_exception(instruction)\n\n            raise TranslationError(\"Unknown error\")\n\n        return trans_instrs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_operand_expr(self, operand, mode=\"post\"):\n        if isinstance(operand, ReilRegisterOperand):\n            if operand.name in self._arch_info.registers_all:\n                # Process architectural registers (eax, ebx, etc.)\n                expr = self.get_register_expr(operand.name, mode=mode)\n            else:\n                # Process temporal registers (t0, t1, etc.)\n                var_name = self._get_var_name(operand.name, mode)\n                expr = self._translator.make_bitvec(operand.size, var_name)\n        elif isinstance(operand, ReilImmediateOperand):\n            expr = smtsymbol.Constant(operand.size, operand.immediate)\n        else:\n            raise Exception(\"Invalid operand: %s\" % str(operand))\n\n        return expr", "response": "Return a smt expression that represents a register or temporal."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a smt bit vector that represents an architectural ( native ) register.", "response": "def get_register_expr(self, register_name, mode=\"post\"):\n        \"\"\"Return a smt bit vector that represents an architectural (native)\n        register.\n        \"\"\"\n        reg_info = self._arch_info.alias_mapper.get(register_name, None)\n\n        if reg_info:\n            var_base_name, offset = reg_info\n        else:\n            var_base_name = register_name\n\n        var_name = self._get_var_name(var_base_name, mode)\n        var_size = self._arch_info.registers_size[var_base_name]\n\n        ret_val = self._translator.make_bitvec(var_size, var_name)\n\n        if reg_info:\n            ret_val = smtfunction.extract(ret_val, offset, self._arch_info.registers_size[register_name])\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_memory_expr(self, address, size, mode=\"post\"):\n        mem = self.get_memory(mode)\n\n        return smtfunction.concat(8, *reversed([mem[address + i] for i in range(size)]))", "response": "Return a smt bit vector that represents a memory location."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a smt bit vector that represents a memory location.", "response": "def get_memory(self, mode):\n        \"\"\"Return a smt bit vector that represents a memory location.\n        \"\"\"\n        mem = {\n            \"pre\": self._translator.get_memory_init(),\n            \"post\": self._translator.get_memory_curr(),\n        }\n\n        return mem[mode]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_instruction(self, reil_instruction):\n        for expr in self._translator.translate(reil_instruction):\n            self._solver.add(expr)", "response": "Add an instruction for analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_var_name(self, register_name, mode):\n        var_name = {\n            \"pre\": self._translator.get_name_init(register_name),\n            \"post\": self._translator.get_name_curr(register_name),\n        }\n\n        return var_name[mode]", "response": "Get the variable name for a given register considering pre and post mode."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_address(self, container, start=None, end=None, find=None, avoid=None, initial_state=None):\n        # Set initial CPU state.\n        self.__set_cpu_state(initial_state)\n\n        # Convert input native addresses to reil addresses.\n        start = to_reil_address(start) if start else None\n        end = to_reil_address(end) if end else None\n        find = to_reil_address(find) if find else None\n        avoid = [to_reil_address(addr) for addr in avoid] if avoid else []\n\n        # Load instruction pointer.\n        ip = start if start else container[0].address\n\n        execution_state = Queue()\n\n        trace_current = []\n        trace_final = []\n\n        self.__fa_process_container(container, find, ip, end, avoid, initial_state, execution_state, trace_current,\n                                    trace_final)\n\n        # Only returns when all paths have been visited.\n        assert execution_state.empty()\n\n        return trace_final", "response": "Find the first instruction that matches the given start and end."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_state(self, container, start=None, end=None, avoid=None, initial_state=None, final_state=None):\n        self.__set_cpu_state(initial_state)\n\n        # Convert input native addresses to reil addresses.\n        start = to_reil_address(start) if start else None\n        end = to_reil_address(end) if end else None\n        avoid = [to_reil_address(addr) for addr in avoid] if avoid else []\n\n        # Load instruction pointer.\n        ip = start if start else container[0].address\n\n        execution_state = Queue()\n\n        trace_current = []\n        trace_final = []\n\n        self.__fs_process_container(container, final_state, ip, end, avoid, initial_state, execution_state,\n                                    trace_current, trace_final)\n\n        # Only returns when all paths have been visited.\n        assert execution_state.empty()\n\n        return trace_final", "response": "Find the state of the container."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __process_instr(self, instr, avoid, next_addr, initial_state, execution_state, trace_current):\n        # Process branch (JCC oprnd0, empty, oprnd2).\n        if instr.mnemonic == ReilMnemonic.JCC:\n            not_taken_addr = next_addr\n            address, index = split_address(instr.address)\n\n            logger.debug(\"[+] Processing branch: {:#08x}:{:02x} : {}\".format(address, index, instr))\n\n            # Process conditional branch (oprnd0 is a REGISTER).\n            if isinstance(instr.operands[0], ReilRegisterOperand):\n                next_ip = self.__process_branch_cond(instr, avoid, initial_state, execution_state, trace_current, not_taken_addr)\n\n            # Process unconditional branch (oprnd0 is an INTEGER).\n            else:\n                next_ip = self.__process_branch_uncond(instr, trace_current, not_taken_addr)\n\n        # Process the rest of the instructions.\n        else:\n            trace_current += [(instr, None)]\n\n            self.__cpu.execute(instr)\n\n            next_ip = next_addr\n\n        return next_ip", "response": "Processes a REIL instruction and returns the next address to execute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __fa_process_sequence(self, sequence, avoid, initial_state, execution_state, trace_current, next_addr):\n        # TODO: Process execution intra states.\n\n        ip = sequence.address\n        next_ip = None\n\n        while ip:\n            # Fetch next instruction in the sequence.\n            try:\n                instr = sequence.fetch(ip)\n            except ReilSequenceInvalidAddressError:\n                # At this point, ip should be a native instruction address, therefore\n                # the index should be zero.\n                assert split_address(ip)[1] == 0x0\n                next_ip = ip\n                break\n\n            try:\n                target_addr = sequence.get_next_address(ip)\n            except ReilSequenceInvalidAddressError:\n                # We reached the end of the sequence. Execution continues on the next native instruction\n                # (it's a REIL address).\n                target_addr = next_addr\n\n            next_ip = self.__process_instr(instr, avoid, target_addr, initial_state, execution_state, trace_current)\n\n            # Update instruction pointer.\n            try:\n                ip = next_ip if next_ip else sequence.get_next_address(ip)\n            except ReilSequenceInvalidAddressError:\n                break\n\n        return next_ip", "response": "Process a REIL sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses a REIL container.", "response": "def __fa_process_container(self, container, find, start, end, avoid, initial_state, execution_state, trace_current,\n                               trace_final):\n        \"\"\"Process a REIL container.\n\n        Args:\n            avoid (list): List of addresses to avoid while executing the code.\n            container (ReilContainer): REIL container to execute.\n            end (int): End address.\n            execution_state (Queue): Queue of execution states.\n            find (int): Address to find.\n            initial_state (State): Initial state.\n            start (int): Start address.\n            trace_current:\n            trace_final:\n        \"\"\"\n        ip = start\n\n        while ip:\n            # NOTE *ip* and *next_addr* variables can be, independently, either intra\n            # or inter addresses.\n\n            # Fetch next instruction.\n            try:\n                instr = container.fetch(ip)\n            except ReilContainerInvalidAddressError:\n                logger.debug(\"Exception @ {:#08x}\".format(ip))\n\n                raise ReilContainerInvalidAddressError\n\n            # Compute the address of the following instruction to the fetched one.\n            try:\n                next_addr = container.get_next_address(ip)\n            except Exception:\n                logger.debug(\"Exception @ {:#08x}\".format(ip))\n\n                # TODO Should this be considered an error?\n\n                raise ReilContainerInvalidAddressError\n\n            # Process the instruction.\n            next_ip = self.__process_instr(instr, avoid, next_addr, initial_state, execution_state, trace_current)\n\n            # # ====================================================================================================== #\n            # # NOTE This is an attempt to separate intra and inter instruction\n            # # addresses processing. Here, *ip* and *next_addr* are always inter\n            # # instruction addresses.\n            #\n            # assert split_address(ip)[1] == 0x0\n            #\n            # # Compute the address of the following instruction to the fetched one.\n            # try:\n            #     seq = container.fetch_sequence(ip)\n            # except ReilContainerInvalidAddressError:\n            #     logger.debug(\"Exception @ {:#08x}\".format(ip))\n            #\n            #     raise ReilContainerInvalidAddressError\n            #\n            # # Fetch next instruction address.\n            # try:\n            #     next_addr = container.get_next_address(ip + len(seq))\n            # except Exception:\n            #     logger.debug(\"Exception @ {:#08x}\".format(ip))\n            #\n            #     # TODO Should this be considered an error?\n            #\n            #     raise ReilContainerInvalidAddressError\n            #\n            # next_ip = self.__process_sequence(seq, avoid, initial_state, execution_state, trace_current, next_addr)\n            #\n            # if next_ip:\n            #     assert split_address(next_ip)[1] == 0x0\n            #\n            # # ====================================================================================================== #\n\n            # Check termination conditions.\n            if find and next_ip and next_ip == find:\n                logger.debug(\"[+] Find address found!\")\n\n                trace_final.append(list(trace_current))\n\n                next_ip = None\n\n            if end and next_ip and next_ip == end:\n                logger.debug(\"[+] End address found!\")\n\n                next_ip = None\n\n            # Update instruction pointer.\n            ip = next_ip if next_ip else None\n\n            while not ip:\n                if not execution_state.empty():\n                    # Pop next execution state.\n                    ip, trace_current, registers, memory = execution_state.get()\n\n                    if split_address(ip)[1] == 0x0:\n                        logger.debug(\"[+] Popping execution state @ {:#x} (INTER)\".format(ip))\n                    else:\n                        logger.debug(\"[+] Popping execution state @ {:#x} (INTRA)\".format(ip))\n\n                    # Setup cpu and memory.\n                    self.__cpu.registers = registers\n                    self.__cpu.memory = memory\n\n                    logger.debug(\"[+] Next address: {:#08x}:{:02x}\".format(ip >> 8, ip & 0xff))\n                else:\n                    logger.debug(\"[+] No more paths to explore! Exiting...\")\n\n                    break\n\n                # Check termination conditions (AGAIN...).\n                if find and ip == find:\n                    logger.debug(\"[+] Find address found!\")\n\n                    trace_final.append(list(trace_current))\n\n                    ip = None\n\n                if end and ip == end:\n                    logger.debug(\"[+] End address found!\")\n\n                    ip = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of simple paths between start and end functions.", "response": "def simple_paths_by_name(self, start_name, end_name):\n        \"\"\"Return a list of paths between start and end functions.\n        \"\"\"\n        cfg_start = self.find_function_by_name(start_name)\n        cfg_end = self.find_function_by_name(end_name)\n\n        if not cfg_start or not cfg_end:\n            raise Exception(\"Start/End function not found.\")\n\n        start_address = cfg_start.start_address\n        end_address = cfg_end.start_address\n\n        paths = networkx.all_simple_paths(self._graph, source=start_address, target=end_address)\n\n        return ([self._cfg_by_addr[addr] for addr in path] for path in paths)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simple_paths_by_address(self, start_address, end_address):\n        cfg_start = self.find_function_by_address(start_address)\n        cfg_end = self.find_function_by_address(end_address)\n\n        if not cfg_start or not cfg_end:\n            raise Exception(\"Start/End function not found.\")\n\n        start_address = cfg_start.start_address\n        end_address = cfg_end.start_address\n\n        paths = networkx.all_simple_paths(self._graph, source=start_address, target=end_address)\n\n        return ([self._cfg_by_addr[addr] for addr in path] for path in paths)", "response": "Return a list of simple paths between start and end functions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the cfg of the requested function by name.", "response": "def find_function_by_name(self, name):\n        \"\"\"Return the cfg of the requested function by name.\n        \"\"\"\n        cfg_rv = None\n        for cfg in self._cfgs:\n            if cfg.name == name:\n                cfg_rv = cfg\n                break\n        return cfg_rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_function_by_address(self, address):\n        cfg_rv = None\n        for cfg in self._cfgs:\n            if cfg.start_address == address:\n                cfg_rv = cfg\n                break\n        return cfg_rv", "response": "Return the cfg of the requested function by address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, cf, filename, format='dot'):\n        try:\n            dot_graph = Dot(**self.graph_format)\n\n            # add nodes\n            nodes = {}\n            for cfg_addr in cf._graph.node.keys():\n                nodes[cfg_addr] = self._create_node(cfg_addr, cf)\n\n                dot_graph.add_node(nodes[cfg_addr])\n\n            # add edges\n            for cfg_src_addr in cf._graph.node.keys():\n                for cfg_dst_addr in cf._edges.get(cfg_src_addr, []):\n                    edge = self._create_edge(nodes, cfg_src_addr, cfg_dst_addr)\n\n                    dot_graph.add_edge(edge)\n\n            dot_graph.write(\"{}.{}\".format(filename, format), format=format)\n        except Exception:\n            logger.error(\"Failed to save call graph: %s (%s)\", filename, format, exc_info=True)", "response": "Save basic block graph into a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of simple paths between start and end address.", "response": "def all_simple_bb_paths(self, start_address, end_address):\n        \"\"\"Return a list of path between start and end address.\n        \"\"\"\n        bb_start = self._find_basic_block(start_address)\n        bb_end = self._find_basic_block(end_address)\n\n        paths = networkx.all_simple_paths(self._graph, source=bb_start.address, target=bb_end.address)\n\n        return ([self._bb_by_addr[addr] for addr in path] for path in paths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(self, start, end, symbols=None):\n        symbols = {} if not symbols else symbols\n\n        # First pass: Recover BBs.\n        bbs = self._recover_bbs(start, end, symbols)\n\n        # Second pass: Split overlapping basic blocks introduced by back edges.\n        bbs = self._split_bbs(bbs, symbols)\n\n        # Third pass: Extract call targets for further analysis.\n        call_targets = self._extract_call_targets(bbs)\n\n        return bbs, call_targets", "response": "Build a list of basic blocks for the given start and end addresses."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a basic block graph into a file.", "response": "def save(self, cfg, filename, print_ir=False, format='dot', options=None):\n        \"\"\"Save basic block graph into a file.\n        \"\"\"\n        if options is None:\n            options = {}\n\n        try:\n            dot_graph = Dot(**self.graph_format)\n\n            # Add nodes.\n            nodes = {}\n            for bb in cfg.basic_blocks:\n                nodes[bb.address] = self._create_node(bb, cfg.name, print_ir, options)\n\n                dot_graph.add_node(nodes[bb.address])\n\n            # Add edges.\n            for bb_src in cfg.basic_blocks:\n                for bb_dst_addr, branch_type in bb_src.branches:\n                    if bb_dst_addr in nodes:\n                        edge = self._create_edge(nodes[bb_src.address], nodes[bb_dst_addr], branch_type)\n\n                        dot_graph.add_edge(edge)\n                    else:\n                        logger.warning(\"Destination basic block not found! (0x%x)\", bb_dst_addr)\n\n            # Save graph.\n            dot_graph.write(\"{}.{}\".format(filename, format), format=format)\n        except Exception:\n            logger.error(\"Failed to save basic block graph: %s (%s)\", filename, format, exc_info=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an instruction operand.", "response": "def parse_operand(string, location, tokens):\n    \"\"\"Parse instruction operand.\n    \"\"\"\n    sizes = {\n        \"dqword\":  128,\n        \"pointer\": 72,\n        \"qword\":   64,\n        \"pointer\": 40,\n        \"dword\":   32,\n        \"word\":    16,\n        \"byte\":    8,\n        \"bit\":     1,\n    }\n\n    if \"immediate\" in tokens:\n        imm_str = \"\".join(tokens[\"immediate\"])\n        base = 16 if imm_str.startswith(\"0x\") or imm_str.startswith(\"-0x\") else 10\n\n        imm = int(imm_str, base)\n\n        oprnd = ReilImmediateOperand(imm)\n\n    if \"register\" in tokens:\n        if tokens[\"register\"] in [\"e\", \"empty\"]:\n            oprnd = ReilEmptyOperand()\n\n            oprnd.size = 0\n        else:\n            name = tokens[\"register\"]\n\n            oprnd = ReilRegisterOperand(name)\n\n    if \"size\" in tokens:\n        oprnd.size = int(sizes[tokens[\"size\"]])\n\n    return [oprnd]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self, instrs):\n        instrs_reil = []\n\n        try:\n            for instr in instrs:\n                instr_lower = instr.lower()\n\n                # If the instruction to parsed is not in the cache,\n                # parse it and add it to the cache.\n                if instr_lower not in self._cache:\n                    self._cache[instr_lower] = instruction.parseString(\n                        instr_lower)[0]\n\n                # Retrieve parsed instruction from the cache and clone\n                # it.\n                instrs_reil += [copy.deepcopy(self._cache[instr_lower])]\n        except:\n            error_msg = \"Failed to parse instruction: %s\"\n\n            logger.error(error_msg, instr, exc_info=True)\n\n        return instrs_reil", "response": "Parse an IR instruction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an ARM instruction operand.", "response": "def parse_operand(string, location, tokens):\n    \"\"\"Parse an ARM instruction operand.\n    \"\"\"\n\n    if \"immediate_operand\" in tokens:\n        size = arch_info.operand_size\n        oprnd = ArmImmediateOperand(\"\".join(tokens[\"immediate_operand\"]), size)\n\n    if \"register_operand\" in tokens:\n        oprnd = process_register(tokens[\"register_operand\"])\n\n        # TODO: Figure out where to really store this flag, instead of in the register class\n        if \"wb\" in tokens[\"register_operand\"]:\n            oprnd.wb = True\n\n    if \"memory_operand\" in tokens:\n        mem_oprnd = tokens[\"memory_operand\"]\n\n        if \"offset\" in mem_oprnd:\n            index_type = ARM_MEMORY_INDEX_OFFSET\n            mem_oprnd = mem_oprnd[\"offset\"]\n        elif \"pre\" in mem_oprnd:\n            index_type = ARM_MEMORY_INDEX_PRE\n            mem_oprnd = mem_oprnd[\"pre\"]\n        elif \"post\" in mem_oprnd:\n            index_type = ARM_MEMORY_INDEX_POST\n            mem_oprnd = mem_oprnd[\"post\"]\n        else:\n            raise Exception(\"Unknown index type.\")\n\n        reg_base = process_register(mem_oprnd[\"base\"])\n        disp = mem_oprnd.get(\"disp\", None)\n        disp_minus = True if mem_oprnd.get(\"minus\") else False\n\n        if disp:\n            if \"shift\" in disp:\n                displ_imm = process_shifted_register(disp[\"shift\"])\n            elif \"reg\" in disp:\n                displ_imm = process_register(disp[\"reg\"])\n            elif \"imm\" in disp:\n                displ_imm = ArmImmediateOperand(\"\".join(disp[\"imm\"]), arch_info.operand_size)\n            else:\n                raise Exception(\"Unknown displacement type.\")\n        else:\n            displ_imm = None\n\n        size = arch_info.operand_size\n        # TODO: Add sizes for LDR/STR variations (half word, byte, double word)\n        oprnd = ArmMemoryOperand(reg_base, index_type, displ_imm, disp_minus, size)\n\n    if \"shifted_register\" in tokens:\n        oprnd = process_shifted_register(tokens[\"shifted_register\"])\n\n    if \"register_list_operand\" in tokens:\n        parsed_reg_list = tokens[\"register_list_operand\"]\n        reg_list = []\n        for reg_range in parsed_reg_list:\n            start_reg = process_register(reg_range[0])\n            if len(reg_range) > 1:\n                end_reg = process_register(reg_range[1])\n                reg_list.append([start_reg, end_reg])\n            else:\n                reg_list.append([start_reg])\n\n        oprnd = ArmRegisterListOperand(reg_list, reg_list[0][0].size)\n\n    return oprnd"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses an ARM instruction.", "response": "def parse_instruction(string, location, tokens):\n    \"\"\"Parse an ARM instruction.\n    \"\"\"\n    mnemonic_str = tokens.get(\"mnemonic\")\n    operands = [op for op in tokens.get(\"operands\", [])]\n\n    instr = ArmInstruction(\n        string,\n        mnemonic_str[\"ins\"],\n        operands,\n        arch_info.architecture_mode\n    )\n\n    if \"cc\" in mnemonic_str:\n        instr.condition_code = cc_mapper[mnemonic_str[\"cc\"]]\n\n    if \"uf\" in mnemonic_str:\n        instr.update_flags = True\n\n    if \"ldm_stm_addr_mode\" in mnemonic_str:\n        instr.ldm_stm_addr_mode = ldm_stm_am_mapper[mnemonic_str[\"ldm_stm_addr_mode\"]]\n\n    return instr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting a swagger path arguments from a given flask path.", "response": "def extract_path_arguments(path):\n  \"\"\"\n  Extracts a swagger path arguments from the given flask path.\n  This /path/<parameter> extracts [{name: 'parameter'}]\n  And this /<string(length=2):lang_code>/<string:id>/<float:probability>\n  extracts: [\n    {name: 'lang_code', dataType: 'string'},\n    {name: 'id', dataType: 'string'}\n    {name: 'probability', dataType: 'float'}]\n  \"\"\"\n  # Remove all paranteses\n  path = re.sub('\\([^\\)]*\\)', '', path)\n  args = re.findall('<([^>]+)>', path)\n\n  def split_arg(arg):\n    spl = arg.split(':')\n    if len(spl) == 1:\n      return {'name': spl[0],\n              'dataType': 'string',\n              'paramType': 'path'}\n    else:\n      return {'name': spl[1],\n              'dataType': spl[0],\n              'paramType': 'path'}\n\n  return list(map(split_arg, args))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the next scheduled time by creating a crontab object with a cron string", "response": "def get_next_scheduled_time(cron_string):\n    \"\"\"Calculate the next scheduled time by creating a crontab object\n    with a cron string\"\"\"\n    itr = croniter.croniter(cron_string, datetime.utcnow())\n    return itr.get_next(datetime)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rationalize_until(until=None):\n    if until is None:\n        until = \"+inf\"\n    elif isinstance(until, datetime):\n        until = to_unix(until)\n    elif isinstance(until, timedelta):\n        until = to_unix((datetime.utcnow() + until))\n    return until", "response": "Rationalizes the until argument used by other functions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering its own death.", "response": "def register_death(self):\n        \"\"\"Registers its own death.\"\"\"\n        self.log.info('Registering death')\n        with self.connection.pipeline() as p:\n            p.hset(self.scheduler_key, 'death', time.time())\n            p.expire(self.scheduler_key, 60)\n            p.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nacquires a lock for scheduling jobs.", "response": "def acquire_lock(self):\n        \"\"\"\n        Acquire lock before scheduling jobs to prevent another scheduler\n        from scheduling jobs at the same time.\n\n        This function returns True if a lock is acquired. False otherwise.\n        \"\"\"\n        key = '%s_lock' % self.scheduler_key\n        now = time.time()\n        expires = int(self._interval) + 10\n        self._lock_acquired = self.connection.set(\n                key, now, ex=expires, nx=True)\n        return self._lock_acquired"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _install_signal_handlers(self):\n\n        def stop(signum, frame):\n            \"\"\"\n            Register scheduler's death and exit\n            and remove previously acquired lock and exit.\n            \"\"\"\n            self.log.info('Shutting down RQ scheduler...')\n            self.register_death()\n            self.remove_lock()\n            raise SystemExit()\n\n        signal.signal(signal.SIGINT, stop)\n        signal.signal(signal.SIGTERM, stop)", "response": "Installs signal handlers for handling SIGINT and SIGTERM gracefully."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_job(self, func, args=None, kwargs=None, commit=True,\n                    result_ttl=None, ttl=None, id=None, description=None,\n                    queue_name=None, timeout=None, meta=None):\n        \"\"\"\n        Creates an RQ job and saves it to Redis.\n        \"\"\"\n        if args is None:\n            args = ()\n        if kwargs is None:\n            kwargs = {}\n        job = self.job_class.create(\n                func, args=args, connection=self.connection,\n                kwargs=kwargs, result_ttl=result_ttl, ttl=ttl, id=id,\n                description=description, timeout=timeout, meta=meta)\n        if self._queue is not None:\n            job.origin = self._queue.name\n        else:\n            job.origin = queue_name or self.queue_name\n        if commit:\n            job.save()\n        return job", "response": "Creates an RQ job and saves it to Redis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enqueue_at(self, scheduled_time, func, *args, **kwargs):\n        timeout = kwargs.pop('timeout', None)\n        job_id = kwargs.pop('job_id', None)\n        job_ttl = kwargs.pop('job_ttl', None)\n        job_result_ttl = kwargs.pop('job_result_ttl', None)\n        job_description = kwargs.pop('job_description', None)\n        meta = kwargs.pop('meta', None)\n\n        job = self._create_job(func, args=args, kwargs=kwargs, timeout=timeout,\n                               id=job_id, result_ttl=job_result_ttl, ttl=job_ttl,\n                               description=job_description, meta=meta)\n        self.connection.zadd(self.scheduled_jobs_key,\n                              {job.id: to_unix(scheduled_time)})\n        return job", "response": "Enqueue a job at a specific scheduled time."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef schedule(self, scheduled_time, func, args=None, kwargs=None,\n                 interval=None, repeat=None, result_ttl=None, ttl=None,\n                 timeout=None, id=None, description=None, queue_name=None,\n                 meta=None):\n        \"\"\"\n        Schedule a job to be periodically executed, at a certain interval.\n        \"\"\"\n        # Set result_ttl to -1 for periodic jobs, if result_ttl not specified\n        if interval is not None and result_ttl is None:\n            result_ttl = -1\n        job = self._create_job(func, args=args, kwargs=kwargs, commit=False,\n                               result_ttl=result_ttl, ttl=ttl, id=id,\n                               description=description, queue_name=queue_name,\n                               timeout=timeout, meta=meta)\n\n        if interval is not None:\n            job.meta['interval'] = int(interval)\n        if repeat is not None:\n            job.meta['repeat'] = int(repeat)\n        if repeat and interval is None:\n            raise ValueError(\"Can't repeat a job without interval argument\")\n        job.save()\n        self.connection.zadd(self.scheduled_jobs_key,\n                              {job.id: to_unix(scheduled_time)})\n        return job", "response": "Schedule a job to be executed at a certain interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncancels a job from the scheduler queue.", "response": "def cancel(self, job):\n        \"\"\"\n        Pulls a job from the scheduler queue. This function accepts either a\n        job_id or a job instance.\n        \"\"\"\n        if isinstance(job, self.job_class):\n            self.connection.zrem(self.scheduled_jobs_key, job.id)\n        else:\n            self.connection.zrem(self.scheduled_jobs_key, job)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging a job s execution time.", "response": "def change_execution_time(self, job, date_time):\n        \"\"\"\n        Change a job's execution time.\n        \"\"\"\n        with self.connection.pipeline() as pipe:\n            while 1:\n                try:\n                    pipe.watch(self.scheduled_jobs_key)\n                    if pipe.zscore(self.scheduled_jobs_key, job.id) is None:\n                        raise ValueError('Job not in scheduled jobs queue')\n                    pipe.zadd(self.scheduled_jobs_key, {job.id: to_unix(date_time)})\n                    break\n                except WatchError:\n                    # If job is still in the queue, retry otherwise job is already executed\n                    # so we raise an error\n                    if pipe.zscore(self.scheduled_jobs_key, job.id) is None:\n                        raise ValueError('Job not in scheduled jobs queue')\n                    continue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the total number of jobs scheduled for all queues.", "response": "def count(self, until=None):\n        \"\"\"\n        Returns the total number of jobs that are scheduled for all queues.\n        This function accepts datetime, timedelta instances as well as\n        integers representing epoch values.\n        \"\"\"\n\n        until = rationalize_until(until)\n        return self.connection.zcount(self.scheduled_jobs_key, 0, until)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an iterator of all jobs in the scheduler that are scheduled until the given time.", "response": "def get_jobs(self, until=None, with_times=False, offset=None, length=None):\n        \"\"\"\n        Returns a iterator of job instances that will be queued until the given\n        time. If no 'until' argument is given all jobs are returned.\n\n        If with_times is True, a list of tuples consisting of the job instance\n        and it's scheduled execution time is returned.\n\n        If offset and length are specified, a slice of the list starting at the\n        specified zero-based offset of the specified length will be returned.\n\n        If either of offset or length is specified, then both must be, or\n        an exception will be raised.\n        \"\"\"\n        def epoch_to_datetime(epoch):\n            return from_unix(float(epoch))\n\n        until = rationalize_until(until)\n        job_ids = self.connection.zrangebyscore(self.scheduled_jobs_key, 0,\n                                                until, withscores=with_times,\n                                                score_cast_func=epoch_to_datetime,\n                                                start=offset, num=length)\n        if not with_times:\n            job_ids = zip(job_ids, repeat(None))\n        for job_id, sched_time in job_ids:\n            job_id = job_id.decode('utf-8')\n            try:\n                job = self.job_class.fetch(job_id, connection=self.connection)\n            except NoSuchJobError:\n                # Delete jobs that aren't there from scheduler\n                self.cancel(job_id)\n                continue\n            if with_times:\n                yield (job, sched_time)\n            else:\n                yield job"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_jobs_to_queue(self, with_times=False):\n        return self.get_jobs(to_unix(datetime.utcnow()), with_times=with_times)", "response": "Returns a list of jobs that should be queued."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a queue to put a job into.", "response": "def get_queue_for_job(self, job):\n        \"\"\"\n        Returns a queue to put job into.\n        \"\"\"\n        if self._queue is not None:\n            return self._queue\n        key = '{0}{1}'.format(self.queue_class.redis_queue_namespace_prefix,\n                              job.origin)\n        return self.queue_class.from_queue_key(\n                key, connection=self.connection, job_class=self.job_class)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmoves a scheduled job to a queue. In addition, it also does puts the job back into the scheduler if needed.", "response": "def enqueue_job(self, job):\n        \"\"\"\n        Move a scheduled job to a queue. In addition, it also does puts the job\n        back into the scheduler if needed.\n        \"\"\"\n        self.log.debug('Pushing {0} to {1}'.format(job.id, job.origin))\n\n        interval = job.meta.get('interval', None)\n        repeat = job.meta.get('repeat', None)\n        cron_string = job.meta.get('cron_string', None)\n\n        # If job is a repeated job, decrement counter\n        if repeat:\n            job.meta['repeat'] = int(repeat) - 1\n\n        queue = self.get_queue_for_job(job)\n        queue.enqueue_job(job)\n        self.connection.zrem(self.scheduled_jobs_key, job.id)\n\n        if interval:\n            # If this is a repeat job and counter has reached 0, don't repeat\n            if repeat is not None:\n                if job.meta['repeat'] == 0:\n                    return\n            self.connection.zadd(self.scheduled_jobs_key,\n                                  {job.id: to_unix(datetime.utcnow()) + int(interval)})\n        elif cron_string:\n            # If this is a repeat job and counter has reached 0, don't repeat\n            if repeat is not None:\n                if job.meta['repeat'] == 0:\n                    return\n            self.connection.zadd(self.scheduled_jobs_key,\n                                  {job.id: to_unix(get_next_scheduled_time(cron_string))})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving scheduled jobs into queues.", "response": "def enqueue_jobs(self):\n        \"\"\"\n        Move scheduled jobs into queues.\n        \"\"\"\n        self.log.debug('Checking for scheduled jobs')\n\n        jobs = self.get_jobs_to_queue()\n        for job in jobs:\n            self.enqueue_job(job)\n\n        # Refresh scheduler key's expiry\n        self.connection.expire(self.scheduler_key, int(self._interval) + 10)\n        return jobs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, burst=False):\n\n        self.register_birth()\n        self._install_signal_handlers()\n\n        try:\n            while True:\n                self.log.debug(\"Entering run loop\")\n\n                start_time = time.time()\n                if self.acquire_lock():\n                    self.enqueue_jobs()\n                    self.remove_lock()\n\n                    if burst:\n                        self.log.info('RQ scheduler done, quitting')\n                        break\n                else:\n                    self.log.warning('Lock already taken - skipping run')\n\n                # Time has already elapsed while enqueuing jobs, so don't wait too long.\n                seconds_elapsed_since_start = time.time() - start_time\n                seconds_until_next_scheduled_run = self._interval - seconds_elapsed_since_start\n                # ensure we have a non-negative number\n                if seconds_until_next_scheduled_run > 0:\n                    self.log.debug(\"Sleeping %.2f seconds\" % seconds_until_next_scheduled_run)\n                    time.sleep(seconds_until_next_scheduled_run)\n        finally:\n            self.remove_lock()\n            self.register_death()", "response": "This method runs the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _checkDimensionsListLike(arrays):\n    dim1 = len(arrays)\n    dim2, dim3 = arrays[0].shape\n    for aa in range(1, dim1):\n        dim2_aa, dim3_aa = arrays[aa].shape\n        if (dim2_aa != dim2) or (dim3_aa != dim3):\n            raise _error.InvalidError(_MDPERR[\"obj_square\"])\n    return dim1, dim2, dim3", "response": "Check that each array in a list of arrays has the same size."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that a list - like reward input is valid.", "response": "def _checkRewardsListLike(reward, n_actions, n_states):\n    \"\"\"Check that a list-like reward input is valid.\n\n    \"\"\"\n    try:\n        lenR = len(reward)\n        if lenR == n_actions:\n            dim1, dim2, dim3 = _checkDimensionsListLike(reward)\n        elif lenR == n_states:\n            dim1 = n_actions\n            dim2 = dim3 = lenR\n        else:\n            raise _error.InvalidError(_MDPERR[\"R_shape\"])\n    except AttributeError:\n        raise _error.InvalidError(_MDPERR[\"R_shape\"])\n    return dim1, dim2, dim3"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that matrix is square.", "response": "def isSquare(matrix):\n    \"\"\"Check that ``matrix`` is square.\n\n    Returns\n    =======\n    is_square : bool\n        ``True`` if ``matrix`` is square, ``False`` otherwise.\n\n    \"\"\"\n    try:\n        try:\n            dim1, dim2 = matrix.shape\n        except AttributeError:\n            dim1, dim2 = _np.array(matrix).shape\n    except ValueError:\n        return False\n    if dim1 == dim2:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that matrix is row stochastic.", "response": "def isStochastic(matrix):\n    \"\"\"Check that ``matrix`` is row stochastic.\n\n    Returns\n    =======\n    is_stochastic : bool\n        ``True`` if ``matrix`` is row stochastic, ``False`` otherwise.\n\n    \"\"\"\n    try:\n        absdiff = (_np.abs(matrix.sum(axis=1) - _np.ones(matrix.shape[0])))\n    except AttributeError:\n        matrix = _np.array(matrix)\n        absdiff = (_np.abs(matrix.sum(axis=1) - _np.ones(matrix.shape[0])))\n    return (absdiff.max() <= 10*_np.spacing(_np.float64(1)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isNonNegative(matrix):\n    try:\n        if (matrix >= 0).all():\n            return True\n    except (NotImplementedError, AttributeError, TypeError):\n        try:\n            if (matrix.data >= 0).all():\n                return True\n        except AttributeError:\n            matrix = _np.array(matrix)\n            if (matrix.data >= 0).all():\n                return True\n    return False", "response": "Check that matrix is row non - negative."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef checkSquareStochastic(matrix):\n    if not isSquare(matrix):\n        raise _error.SquareError\n    if not isStochastic(matrix):\n        raise _error.StochasticError\n    if not isNonNegative(matrix):\n        raise _error.NonNegativeError", "response": "Checks if the matrix is square and row - stochastic and raises an error if not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if P and R define a valid Markov Decision Process.", "response": "def check(P, R):\n    \"\"\"Check if ``P`` and ``R`` define a valid Markov Decision Process (MDP).\n\n    Let ``S`` = number of states, ``A`` = number of actions.\n\n    Arguments\n    ---------\n    P : array\n        The transition matrices. It can be a three dimensional array with\n        a shape of (A, S, S). It can also be a one dimensional arraye with\n        a shape of (A, ), where each element contains a matrix of shape (S, S)\n        which can possibly be sparse.\n    R : array\n        The reward matrix. It can be a three dimensional array with a\n        shape of (S, A, A). It can also be a one dimensional array with a\n        shape of (A, ), where each element contains matrix with a shape of\n        (S, S) which can possibly be sparse. It can also be an array with\n        a shape of (S, A) which can possibly be sparse.\n\n    Notes\n    -----\n    Raises an error if ``P`` and ``R`` do not define a MDP.\n\n    Examples\n    --------\n    >>> import mdptoolbox, mdptoolbox.example\n    >>> P_valid, R_valid = mdptoolbox.example.rand(100, 5)\n    >>> mdptoolbox.util.check(P_valid, R_valid) # Nothing should happen\n    >>>\n    >>> import numpy as np\n    >>> P_invalid = np.random.rand(5, 100, 100)\n    >>> mdptoolbox.util.check(P_invalid, R_valid) # Raises an exception\n    Traceback (most recent call last):\n    ...\n    StochasticError:...\n\n    \"\"\"\n    # Checking P\n    try:\n        if P.ndim == 3:\n            aP, sP0, sP1 = P.shape\n        elif P.ndim == 1:\n            aP, sP0, sP1 = _checkDimensionsListLike(P)\n        else:\n            raise _error.InvalidError(_MDPERR[\"P_shape\"])\n    except AttributeError:\n        try:\n            aP, sP0, sP1 = _checkDimensionsListLike(P)\n        except AttributeError:\n            raise _error.InvalidError(_MDPERR[\"P_shape\"])\n    msg = \"\"\n    if aP <= 0:\n        msg = \"The number of actions in P must be greater than 0.\"\n    elif sP0 <= 0:\n        msg = \"The number of states in P must be greater than 0.\"\n    if msg:\n        raise _error.InvalidError(msg)\n    # Checking R\n    try:\n        ndimR = R.ndim\n        if ndimR == 1:\n            aR, sR0, sR1 = _checkRewardsListLike(R, aP, sP0)\n        elif ndimR == 2:\n            sR0, aR = R.shape\n            sR1 = sR0\n        elif ndimR == 3:\n            aR, sR0, sR1 = R.shape\n        else:\n            raise _error.InvalidError(_MDPERR[\"R_shape\"])\n    except AttributeError:\n        aR, sR0, sR1 = _checkRewardsListLike(R, aP, sP0)\n    msg = \"\"\n    if sR0 <= 0:\n        msg = \"The number of states in R must be greater than 0.\"\n    elif aR <= 0:\n        msg = \"The number of actions in R must be greater than 0.\"\n    elif sR0 != sR1:\n        msg = \"The matrix R must be square with respect to states.\"\n    elif sP0 != sR0:\n        msg = \"The number of states must agree in P and R.\"\n    elif aP != aR:\n        msg = \"The number of actions must agree in P and R.\"\n    if msg:\n        raise _error.InvalidError(msg)\n    # Check that the P's are square, stochastic and non-negative\n    for aa in range(aP):\n        checkSquareStochastic(P[aa])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the transition probabilities for the next state p r.", "response": "def getTransitionProbabilities(state, action):\n    \"\"\"\n    Parameters\n    ----------\n    state : tuple\n        The state\n    action : int\n        The action\n    \n    Returns\n    -------\n    s1, p, r : tuple of two lists and an int\n        s1 are the next states, p are the probabilities, and r is the reward\n    \n    \"\"\"\n    #assert isValid(state)\n    assert 0 <= action < ACTIONS\n    if not isLegal(state, action):\n        # If the action is illegal, then transition back to the same state but\n        # incur a high negative reward\n        s1 = [convertTupleToIndex(state)]\n        return(s1, [1], -10)\n    # Update the state with the action\n    state = list(state)\n    state[action] = PLAYER\n    if isWon(state, PLAYER):\n        # If the player's action is a winning move then transition to the\n        # winning state and receive a reward of 1.\n        s1 = [convertTupleToIndex(state)]\n        return(s1, [1], 1)\n    elif isDraw(state):\n        s1 = [convertTupleToIndex(state)]\n        return(s1, [1], 0)\n    # Now we search through the opponents moves, and calculate transition\n    # probabilities based on maximising the opponents chance of winning..\n    s1 = []\n    p = []\n    legal_a = getLegalActions(state)\n    for a in legal_a:\n        state[a] = OPPONENT\n        # If the opponent is going to win, we assume that the winning move will\n        # be chosen:\n        if isWon(state, OPPONENT):\n            s1 = [convertTupleToIndex(state)]\n            return(s1, [1], -1)\n        elif isDraw(state):\n            s1 = [convertTupleToIndex(state)]\n            return(s1, [1], 0)\n        # Otherwise we assume the opponent will select a move with uniform\n        # probability across potential moves:\n        s1.append(convertTupleToIndex(state))\n        p.append(1.0 / len(legal_a))\n        state[a] = 0\n    # During non-terminal play states the reward is 0.\n    return(s1, p, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntesting if a tic - tac - toe game has been won.", "response": "def isWon(state, who):\n    \"\"\"Test if a tic-tac-toe game has been won.\n    \n    Assumes that the board is in a legal state.\n    Will test if the value 1 is in any winning combination.\n    \n    \"\"\"\n    for w in WINS:\n        S = sum(1 if (w[k] == 1 and state[k] == who) else 0\n                for k in range(ACTIONS))\n        if S == 3:\n            # We have a win\n            return True\n    # There were no wins so return False\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _calculatePolicy(self):\n        cmd = '''\n              UPDATE policy\n                 SET action = (\n                     SELECT action\n                       FROM (SELECT state, action, MAX(value)\n                               FROM Q\n                              GROUP BY state) AS A\n                       WHERE policy.state = A.state\n                       GROUP BY state);'''\n        self._cur.execute(cmd)\n        self._conn.commit()", "response": "This implements argmax over the actions of Q."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calculateValue(self):\n        cmd = '''\n              UPDATE V\n                 SET value = (\n                     SELECT MAX(value)\n                       FROM Q\n                      WHERE V.state = Q.state\n                      GROUP BY state);'''\n        self._cur.execute(cmd)\n        self._conn.commit()", "response": "This is max over the actions of Q."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the policy and value vectors.", "response": "def getPolicyValue(self):\n        \"\"\"Get the policy and value vectors.\"\"\"\n        self._cur.execute(\"SELECT action FROM policy\")\n        r = self._cur.fetchall()\n        policy = [x[0] for x in r]\n        self._cur.execute(\"SELECT value FROM V\")\n        r = self._cur.fetchall()\n        value = [x[0] for x in r]\n        return policy, value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convertStateToIndex(population, fire):\n    assert 0 <= population < POPULATION_CLASSES, \"'population' must be in \" \\\n        \"(0, 1...%s)\" % str(POPULATION_CLASSES - 1)\n    assert 0 <= fire < FIRE_CLASSES, \"'fire' must be in \" \\\n        \"(0, 1...%s) \" % str(FIRE_CLASSES - 1)\n    return(population * FIRE_CLASSES + fire)", "response": "Convert state parameters to transition probability matrix index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting an index into a state parameter.", "response": "def convertIndexToState(index):\n    \"\"\"Convert transition probability matrix index to state parameters.\n    \n    Parameters\n    ----------\n    index : int\n        The index into the transition probability matrix that corresponds to\n        the state parameters.\n    \n    Returns\n    -------\n    population, fire : tuple of int\n        ``population``, the population abundance class of the threatened\n        species. ``fire``, the time in years since last fire.\n    \n    \"\"\"\n    assert index < STATES\n    population = index // FIRE_CLASSES\n    fire = index % FIRE_CLASSES\n    return(population, fire)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getTransitionProbabilities(s, x, F, a):\n    assert 0 <= x < POPULATION_CLASSES\n    assert 0 <= F < FIRE_CLASSES\n    assert 0 <= s <= 1\n    assert 0 <= a < ACTIONS \n    prob = np.zeros((STATES,))\n    r = getHabitatSuitability(F)\n    # Efect of action on time in years since fire.\n    if a == 0:\n        # Increase the time since the patch has been burned by one year.\n        # The years since fire in patch is absorbed into the last class\n        if F < FIRE_CLASSES - 1:\n            F += 1\n    elif a == 1:\n        # When the patch is burned set the years since fire to 0.\n        F = 0\n    elif a == 2:\n        pass\n    elif a == 3:\n        pass\n    # Population transitions\n    if x == 0:\n        # Demographic model probabilities\n        # population abundance class stays at 0 (extinct)\n        new_state = convertStateToIndex(0, F)\n        prob[new_state] = 1\n    elif x == POPULATION_CLASSES - 1:\n        # Population abundance class either stays at maximum or transitions\n        # down\n        x_1 = x\n        x_2 = x - 1\n        # Effect of action on the state\n        # If action 1 is taken, then the patch is burned so the population\n        # abundance moves down a class.\n        if a == 1:\n            x_1 -= 1\n            x_2 -= 1\n        elif a == 2:\n            pass\n        elif a == 3:\n            pass\n        # Demographic model probabilities\n        new_state = convertStateToIndex(x_1, F)\n        prob[new_state] = 1 - (1 - s) * (1 - r) # abundance stays the same\n        new_state = convertStateToIndex(x_2, F)\n        prob[new_state] = (1 - s) * (1 - r) # abundance goes down\n    else:\n        # Population abundance class can stay the same, transition up, or\n        # transition down.\n        x_1 = x\n        x_2 = x + 1\n        x_3 = x - 1\n        # Effect of action on the state\n        # If action 1 is taken, then the patch is burned so the population\n        # abundance moves down a class.\n        if a == 1:\n            x_1 -= 1\n            x_2 -= 1\n            # Ensure that the abundance class doesn't go to -1\n            if x_3 > 0:\n                x_3 -= 1\n        elif a == 2:\n            pass\n        elif a == 3:\n            pass\n        # Demographic model probabilities\n        new_state = convertStateToIndex(x_1, F)\n        prob[new_state] = s # abundance stays the same\n        new_state = convertStateToIndex(x_2, F)\n        prob[new_state] = (1 - s) * r # abundance goes up\n        new_state = convertStateToIndex(x_3, F)\n        # In the case when x_3 = 0 before the effect of an action is applied,\n        # then the final state is going to be the same as that for x_1, so we\n        # need to add the probabilities together.\n        prob[new_state] += (1 - s) * (1 - r) # abundance goes down\n    return(prob)", "response": "Calculates the transition probabilities for the given state and action."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getTransitionAndRewardArrays(s):\n    assert 0 <= s <= 1, \"'s' must be between 0 and 1\"\n    # The transition probability array\n    P = np.zeros((ACTIONS, STATES, STATES))\n    # The reward vector\n    R = np.zeros(STATES)\n    # Loop over all states\n    for idx in range(STATES):\n        # Get the state index as inputs to our functions\n        x, F = convertIndexToState(idx)\n        # The reward for being in this state is 1 if the population is extant\n        if x != 0:\n            R[idx] = 1\n        # Loop over all actions\n        for a in range(ACTIONS):\n            # Assign the transition probabilities for this state, action pair\n            P[a][idx] = getTransitionProbabilities(s, x, F, a)\n    return(P, R)", "response": "Generates the transition and reward arrays for a given current population."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsolve the problem as a finite horizon Markov decision process.", "response": "def solveMDP():\n    \"\"\"Solve the problem as a finite horizon Markov decision process.\n    \n    The optimal policy at each stage is found using backwards induction.\n    Possingham and Tuck report strategies for a 50 year time horizon, so the\n    number of stages for the finite horizon algorithm is set to 50. There is no\n    discount factor reported, so we set it to 0.96 rather arbitrarily.\n    \n    Returns\n    -------\n    mdp : mdptoolbox.mdp.FiniteHorizon\n        The PyMDPtoolbox object that represents a finite horizon MDP. The\n        optimal policy for each stage is accessed with mdp.policy, which is a\n        numpy array with 50 columns (one for each stage).\n    \n    \"\"\"\n    P, R = getTransitionAndRewardArrays(0.5)\n    sdp = mdp.FiniteHorizon(P, R, 0.96, 50)\n    sdp.run()\n    return(sdp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints out a policy vector as a table to console.", "response": "def printPolicy(policy):\n    \"\"\"Print out a policy vector as a table to console\n    \n    Let ``S`` = number of states.\n    \n    The output is a table that has the population class as rows, and the years\n    since a fire as the columns. The items in the table are the optimal action\n    for that population class and years since fire combination.\n    \n    Parameters\n    ----------\n    p : array\n        ``p`` is a numpy array of length ``S``.\n    \n    \"\"\"\n    p = np.array(policy).reshape(POPULATION_CLASSES, FIRE_CLASSES)\n    range_F = range(FIRE_CLASSES)\n    print(\"    \" + \" \".join(\"%2d\" % f for f in range_F))\n    print(\"    \" + \"---\" * FIRE_CLASSES)\n    for x in range(POPULATION_CLASSES):\n        print(\" %2d|\" % x + \" \".join(\"%2d\" % p[x, f] for f in range_F))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsimulates a state transition.", "response": "def simulateTransition(x, s, r, fire):\n    \"\"\"Simulate a state transition.\n    \n    Parameters\n    ----------\n    x : int\n        The current abundance class of the threatened species.\n    s : float\n        The state-independent probability of the population staying in its\n        current abundance class.\n    r : float\n        The probability the population moves up one abundance class, assuming\n        it is not staying in its current state. ``r`` depends on ``F``, the\n        time in years since the last fire.\n    fire : bool\n        True if there has been a fire in the current year, otherwise False.\n    \n    Returns\n    -------\n    x : int\n        The new abundance class of the threatened species.\n    \n    \"\"\"\n    assert 0 <= x < POPULATION_CLASSES, \"'x' must be in \" \\\n        \"{0, 1...%s}\" % POPULATION_CLASSES - 1\n    assert 0 <= s <= 1, \"'s' must be in [0; 1]\"\n    assert 0 <= r <= 1, \"'r' must be in [0; 1]\"\n    assert fire in (True, False), \"'fire' must be a boolean value\"\n    x = int(x)\n    if x == 0:\n        pass\n    elif x == POPULATION_CLASSES - 1:\n        if random.random() <= 1 - (1 - s) * (1 - r):\n            pass\n        else: # with probability (1 - s)(1 - r)\n            x -= 1\n    else:\n        if random.random() <= s:\n            pass\n        else:\n            if random.random() <= r: # with probability (1 - s)r\n                x += 1\n            else: # with probability (1 - s)(1 - r)\n                x -= 1\n    # Add the effect of a fire, making sure x doesn't go to -1\n    if fire and (x > 0):\n        x -= 1\n    return(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forest(S=3, r1=4, r2=2, p=0.1, is_sparse=False):\n    assert S > 1, \"The number of states S must be greater than 1.\"\n    assert (r1 > 0) and (r2 > 0), \"The rewards must be non-negative.\"\n    assert 0 <= p <= 1, \"The probability p must be in [0; 1].\"\n    # Definition of Transition matrix\n    if is_sparse:\n        P = []\n        rows = list(range(S)) * 2\n        cols = [0] * S + list(range(1, S)) + [S - 1]\n        vals = [p] * S + [1-p] * S\n        P.append(_sp.coo_matrix((vals, (rows, cols)), shape=(S, S)).tocsr())\n        rows = list(range(S))\n        cols = [0] * S\n        vals = [1] * S\n        P.append(_sp.coo_matrix((vals, (rows, cols)), shape=(S, S)).tocsr())\n    else:\n        P = _np.zeros((2, S, S))\n        P[0, :, :] = (1 - p) * _np.diag(_np.ones(S - 1), 1)\n        P[0, :, 0] = p\n        P[0, S - 1, S - 1] = (1 - p)\n        P[1, :, :] = _np.zeros((S, S))\n        P[1, :, 0] = 1\n    # Definition of Reward matrix\n    R = _np.zeros((S, 2))\n    R[S - 1, 0] = r1\n    R[:, 1] = _np.ones(S)\n    R[0, 1] = 0\n    R[S - 1, 1] = r2\n    return(P, R)", "response": "Generate a MDP example based on a simple forest management scenario."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating random dense transition matrix and reward matrix.", "response": "def _randDense(states, actions, mask):\n    \"\"\"Generate random dense ``P`` and ``R``. See ``rand`` for details.\n\n    \"\"\"\n    # definition of transition matrix : square stochastic matrix\n    P = _np.zeros((actions, states, states))\n    # definition of reward matrix (values between -1 and +1)\n    R = _np.zeros((actions, states, states))\n    for action in range(actions):\n        for state in range(states):\n            # create our own random mask if there is no user supplied one\n            if mask is None:\n                m = _np.random.random(states)\n                r = _np.random.random()\n                m[m <= r] = 0\n                m[m > r] = 1\n            elif mask.shape == (actions, states, states):\n                m = mask[action][state]  # mask[action, state, :]\n            else:\n                m = mask[state]\n            # Make sure that there is atleast one transition in each state\n            if m.sum() == 0:\n                m[_np.random.randint(0, states)] = 1\n            P[action][state] = m * _np.random.random(states)\n            P[action][state] = P[action][state] / P[action][state].sum()\n            R[action][state] = (m * (2 * _np.random.random(states) -\n                                _np.ones(states, dtype=int)))\n    return(P, R)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate random sparse transition matrix and reward matrix.", "response": "def _randSparse(states, actions, mask):\n    \"\"\"Generate random sparse ``P`` and ``R``. See ``rand`` for details.\n\n    \"\"\"\n    # definition of transition matrix : square stochastic matrix\n    P = [None] * actions\n    # definition of reward matrix (values between -1 and +1)\n    R = [None] * actions\n    for action in range(actions):\n        # it may be more efficient to implement this by constructing lists\n        # of rows, columns and values then creating a coo_matrix, but this\n        # works for now\n        PP = _sp.dok_matrix((states, states))\n        RR = _sp.dok_matrix((states, states))\n        for state in range(states):\n            if mask is None:\n                m = _np.random.random(states)\n                m[m <= 2/3.0] = 0\n                m[m > 2/3.0] = 1\n            elif mask.shape == (actions, states, states):\n                m = mask[action][state]  # mask[action, state, :]\n            else:\n                m = mask[state]\n            n = int(m.sum())  # m[state, :]\n            if n == 0:\n                m[_np.random.randint(0, states)] = 1\n                n = 1\n            # find the columns of the vector that have non-zero elements\n            nz = m.nonzero()\n            if len(nz) == 1:\n                cols = nz[0]\n            else:\n                cols = nz[1]\n            vals = _np.random.random(n)\n            vals = vals / vals.sum()\n            reward = 2*_np.random.random(n) - _np.ones(n)\n            PP[state, cols] = vals\n            RR[state, cols] = reward\n        # PP.tocsr() takes the same amount of time as PP.tocoo().tocsr()\n        # so constructing PP and RR as coo_matrix in the first place is\n        # probably \"better\"\n        P[action] = PP.tocsr()\n        R[action] = RR.tocsr()\n    return(P, R)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rand(S, A, is_sparse=False, mask=None):\n    # making sure the states and actions are more than one\n    assert S > 1, \"The number of states S must be greater than 1.\"\n    assert A > 1, \"The number of actions A must be greater than 1.\"\n    # if the user hasn't specified a mask, then we will make a random one now\n    if mask is not None:\n        # the mask needs to be SxS or AxSxS\n        try:\n            assert mask.shape in ((S, S), (A, S, S)), (\n                \"'mask' must have dimensions S\u00d7S or A\u00d7S\u00d7S.\"\n            )\n        except AttributeError:\n            raise TypeError(\"'mask' must be a numpy array or matrix.\")\n    # generate the transition and reward matrices based on S, A and mask\n    if is_sparse:\n        P, R = _randSparse(S, A, mask)\n    else:\n        P, R = _randDense(S, A, mask)\n    return(P, R)", "response": "Generate a random Markov Decision Process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_and_update(self):\n        data_json = self._create_json()\n        data_json.update(self.additional_params)\n        request_url = self._artifactory.drive + '/api/{uri}/{x.name}'.format(uri=self._uri, x=self)\n        r = self._session.put(\n            request_url,\n            json=data_json,\n            headers={'Content-Type': 'application/json'},\n            auth=self._auth,\n        )\n        r.raise_for_status()\n        rest_delay()\n        self.read()", "response": "Create or update the object from Artifactory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self):\n        logging.debug('Read {x.__class__.__name__} [{x.name}]'.format(x=self))\n        request_url = self._artifactory.drive + '/api/{uri}/{x.name}'.format(uri=self._uri, x=self)\n        r = self._session.get(\n            request_url,\n            auth=self._auth,\n        )\n        if 404 == r.status_code or 400 == r.status_code:\n            logging.debug('{x.__class__.__name__} [{x.name}] does not exist'.format(x=self))\n            return False\n        else:\n            logging.debug('{x.__class__.__name__} [{x.name}] exist'.format(x=self))\n            r.raise_for_status()\n            response = r.json()\n            self.raw = response\n            self._read_response(response)\n            return True", "response": "Read object from artifactory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self):\n        logging.debug('Remove {x.__class__.__name__} [{x.name}]'.format(x=self))\n        request_url = self._artifactory.drive + '/api/{uri}/{x.name}'.format(uri=self._uri, x=self)\n        r = self._session.delete(\n            request_url,\n            auth=self._auth,\n        )\n        r.raise_for_status()\n        rest_delay()", "response": "Remove object from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a JSON object from the object.", "response": "def _create_json(self):\n        \"\"\"\n        JSON Documentation: https://www.jfrog.com/confluence/display/RTF/Security+Configuration+JSON\n        \"\"\"\n        data_json = {\n            'name': self.name,\n            'email': self.email,\n            'password': self.password,\n            'admin': self.admin,\n            \"profileUpdatable\": self.profileUpdatable,\n            \"internalPasswordDisabled\": self.internalPasswordDisabled,\n            \"groups\": self._groups,\n        }\n        return data_json"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the response from the server into the object.", "response": "def _read_response(self, response):\n        \"\"\"\n        JSON Documentation: https://www.jfrog.com/confluence/display/RTF/Security+Configuration+JSON\n        \"\"\"\n        # self.password = ''  # never returned\n        self.name = response['name']\n        self.email = response['email']\n        self.admin = response['admin']\n        self.profileUpdatable = response['profileUpdatable']\n        self.internalPasswordDisabled = response['internalPasswordDisabled']\n        self._groups = response['groups'] if 'groups' in response else []\n        self._lastLoggedIn = response['lastLoggedIn'] if 'lastLoggedIn' in response else []\n        self._realm = response['realm'] if 'realm' in response else []"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a JSON object from the object.", "response": "def _create_json(self):\n        \"\"\"\n        JSON Documentation: https://www.jfrog.com/confluence/display/RTF/Security+Configuration+JSON\n        \"\"\"\n        data_json = {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"autoJoin\": self.autoJoin,\n            \"realm\": self.realm,\n        }\n        return data_json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the response from the server into the object.", "response": "def _read_response(self, response):\n        \"\"\"\n        JSON Documentation: https://www.jfrog.com/confluence/display/RTF/Security+Configuration+JSON\n        \"\"\"\n        self.name = response['name']\n        self.description = response['description']\n        self.autoJoin = response['autoJoin']\n        self.realm = response['realm']\n        self.realmAttributes = response.get('realmAttributes', None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_json(self):\n        data_json = super(GroupLDAP, self)._create_json()\n        data_json.update({\n            'realmAttributes': self.realmAttributes,\n            'external': True,\n        })\n        return data_json", "response": "Create the JSON object for the LDAP object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a JSON object for the current object.", "response": "def _create_json(self):\n        \"\"\"\n        JSON Documentation: https://www.jfrog.com/confluence/display/RTF/Repository+Configuration+JSON\n        \"\"\"\n        data_json = {\n            \"rclass\": \"local\",\n            \"key\": self.name,\n            \"description\": self.description,\n            \"packageType\": self.packageType,\n            \"notes\": \"\",\n            \"includesPattern\": \"**/*\",\n            \"excludesPattern\": \"\",\n            \"repoLayoutRef\": self.repoLayoutRef,\n            \"dockerApiVersion\": self.dockerApiVersion,\n            \"checksumPolicyType\": \"client-checksums\",\n            \"handleReleases\": True,\n            \"handleSnapshots\": True,\n            \"maxUniqueSnapshots\": 0,\n            \"snapshotVersionBehavior\": \"unique\",\n            \"suppressPomConsistencyChecks\": True,\n            \"blackedOut\": False,\n            \"propertySets\": [],\n            \"archiveBrowsingEnabled\": self.archiveBrowsingEnabled,\n            \"yumRootDepth\": 0,\n        }\n        return data_json"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the response from the API and update the instance attributes.", "response": "def _read_response(self, response):\n        \"\"\"\n        JSON Documentation: https://www.jfrog.com/confluence/display/RTF/Repository+Configuration+JSON\n        \"\"\"\n        self.name = response['key']\n        self.description = response['description']\n        self.layoutName = response['repoLayoutRef']\n        self.archiveBrowsingEnabled = response['archiveBrowsingEnabled']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a JSON object for the current object.", "response": "def _create_json(self):\n        \"\"\"\n        JSON Documentation: https://www.jfrog.com/confluence/display/RTF/Repository+Configuration+JSON\n        \"\"\"\n        data_json = {\n            \"rclass\": \"virtual\",\n            \"key\": self.name,\n            \"description\": self.description,\n            \"packageType\": self.packageType,\n            \"repositories\": self._repositories,\n            \"notes\": self.notes,\n        }\n\n        return data_json"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_response(self, response):\n        rclass = response['rclass']\n        if rclass != \"virtual\":\n            raise ArtifactoryException(\"Repositiry '{}' have '{}', but expect 'virtual'\".format(self.name, rclass))\n\n        self.name = response[\"key\"]\n        self.description = response[\"description\"]\n        self.packageType = response[\"packageType\"]\n        self._repositories = response[\"repositories\"]", "response": "Read the response from the API and update the instance attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_json(self):\n        data_json = {\n            \"name\": self.name,\n            \"includesPattern\": self.includesPattern,\n            \"excludesPattern\": self.excludesPattern,\n            \"repositories\": self._repositories,\n            \"principals\": {\n                'users': self._users,\n                'groups': self._groups,\n            }\n        }\n        return data_json", "response": "Create a JSON object for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_response(self, response):\n        self.name = response['name']\n        self.includesPattern = response['includesPattern']\n        self.excludesPattern = response['excludesPattern']\n        self._repositories = response.get('repositories', [])\n        if 'principals' in response:\n            if 'users' in response['principals']:\n                self._users = response['principals']['users']\n            if 'groups' in response['principals']:\n                self._groups = response['principals']['groups']", "response": "Read the response from the API to set attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_config(config_path=default_config_path):\n    config_path = os.path.expanduser(config_path)\n    if not os.path.isfile(config_path):\n        raise OSError(errno.ENOENT,\n                      \"Artifactory configuration file not found: '%s'\" %\n                      config_path)\n\n    p = configparser.ConfigParser()\n    p.read(config_path)\n\n    result = {}\n\n    for section in p.sections():\n        username = p.get(section, 'username') if p.has_option(section, 'username') else None\n        password = p.get(section, 'password') if p.has_option(section, 'password') else None\n        verify = p.getboolean(section, 'verify') if p.has_option(section, 'verify') else True\n        cert = p.get(section, 'cert') if p.has_option(section, 'cert') else None\n\n        result[section] = {'username': username,\n                           'password': password,\n                           'verify': verify,\n                           'cert': cert}\n        # certificate path may contain '~', and we'd better expand it properly\n        if result[section]['cert']:\n            result[section]['cert'] = \\\n                os.path.expanduser(result[section]['cert'])\n    return result", "response": "Read the configuration file and produce a dictionary of the necessary structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_base_url(config, url):\n    if not config:\n        return None\n\n    # First, try to search for the best match\n    for item in config:\n        if url.startswith(item):\n            return item\n\n    # Then search for indirect match\n    for item in config:\n        if without_http_prefix(url).startswith(without_http_prefix(item)):\n            return item", "response": "Search through config and try to find the base for url"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_config_entry(config, url):\n    if not config:\n        return None\n\n    # First, try to search for the best match\n    if url in config:\n        return config[url]\n\n    # Then search for indirect match\n    for item in config:\n        if without_http_prefix(item) == without_http_prefix(url):\n            return config[item]", "response": "Get the config entry for url"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating sha1 hash of a file", "response": "def sha1sum(filename):\n    \"\"\"\n    Calculates sha1 hash of a file\n    \"\"\"\n    sha1 = hashlib.sha1()\n    with open(filename, 'rb') as f:\n        for chunk in iter(lambda: f.read(128 * sha1.block_size), b''):\n            sha1.update(chunk)\n    return sha1.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chunks(data, size):\n    it = iter(data)\n    for i in range(0, len(data), size):\n        yield {k: data[k] for k in islice(it, size)}", "response": "Yields a list of size chunks from a list of data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode_matrix_parameters(parameters):\n    result = []\n\n    for param in iter(sorted(parameters)):\n        if isinstance(parameters[param], (list, tuple)):\n            value = (';%s=' % (param)).join(parameters[param])\n        else:\n            value = parameters[param]\n\n        result.append(\"%s=%s\" % (param, value))\n\n    return ';'.join(result)", "response": "Encodes url matrix parameters from dictionary to\n    a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting a path string into drive root and relative path.", "response": "def splitroot(self, part, sep=sep):\n        \"\"\"\n        Splits path string into drive, root and relative path\n\n        Uses '/artifactory/' as a splitting point in URI. Everything\n        before it, including '/artifactory/' itself is treated as drive.\n        The next folder is treated as root, and everything else is taken\n        for relative path.\n\n        If '/artifactory/' is not in the URI. Everything before the path\n        component is treated as drive. The first folder of the path is\n        treated as root, and everything else is taken for relative path.\n        \"\"\"\n        drv = ''\n        root = ''\n\n        base = get_global_base_url(part)\n        if base and without_http_prefix(part).startswith(without_http_prefix(base)):\n            mark = without_http_prefix(base).rstrip(sep) + sep\n            parts = part.split(mark)\n        else:\n            url = urllib3.util.parse_url(part)\n\n            if (without_http_prefix(part).strip(\"/\") == part.strip(\"/\") and\n                    url.path and not url.path.strip(\"/\").startswith(\"artifactory\")):\n                return '', '', part\n\n            if url.path is None or url.path == sep:\n                if url.scheme:\n                    return part.rstrip(sep), '', ''\n                return '', '', part\n            elif url.path.lstrip(\"/\").startswith(\"artifactory\"):\n                mark = sep + 'artifactory' + sep\n                parts = part.split(mark)\n            else:\n                drv = part.split(url.path)[0]\n                path_parts = url.path.strip(sep).split(sep)\n                root = sep + path_parts[0] + sep\n                rest = sep.join(path_parts[1:])\n                return drv, root, rest\n\n        if len(parts) >= 2:\n            drv = parts[0] + mark.rstrip(sep)\n            rest = sep + mark.join(parts[1:])\n        elif part.endswith(mark.rstrip(sep)):\n            drv = part\n            rest = ''\n        else:\n            rest = part\n\n        if not rest:\n            return drv, '', ''\n\n        if rest == sep:\n            return drv, '', ''\n\n        if rest.startswith(sep):\n            root, _, part = rest[1:].partition(sep)\n            root = sep + root + sep\n\n        return drv, root, part"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a DELETE request to url with requests. session", "response": "def rest_del(self, url, params=None, session=None, verify=True, cert=None):\n        \"\"\"\n        Perform a DELETE request to url with requests.session\n        \"\"\"\n        res = session.delete(url, params=params, verify=verify, cert=cert)\n        return res.text, res.status_code"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a chunked PUT request to url with requests. session", "response": "def rest_put_stream(self, url, stream, headers=None, session=None, verify=True, cert=None):\n        \"\"\"\n        Perform a chunked PUT request to url with requests.session\n        This is specifically to upload files.\n        \"\"\"\n        res = session.put(url, headers=headers, data=stream, verify=verify, cert=cert)\n        return res.text, res.status_code"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a chunked GET request to url with requests. session", "response": "def rest_get_stream(self, url, session=None, verify=True, cert=None):\n        \"\"\"\n        Perform a chunked GET request to url with requests.session\n        This is specifically to download files.\n        \"\"\"\n        res = session.get(url, stream=True, verify=verify, cert=cert)\n        return res.raw, res.status_code"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stat(self, pathobj):\n        jsn = self.get_stat_json(pathobj)\n\n        is_dir = False\n        if 'size' not in jsn:\n            is_dir = True\n\n        children = None\n        if 'children' in jsn:\n            children = [child['uri'][1:] for child in jsn['children']]\n\n        stat = ArtifactoryFileStat(\n            ctime=dateutil.parser.parse(jsn['created']),\n            mtime=dateutil.parser.parse(jsn['lastModified']),\n            created_by=jsn.get('createdBy', None),\n            modified_by=jsn.get('modifiedBy', None),\n            mime_type=jsn.get('mimeType', None),\n            size=int(jsn.get('size', '0')),\n            sha1=jsn.get('checksums', {'sha1': None})['sha1'],\n            md5=jsn.get('checksums', {'md5': None})['md5'],\n            is_dir=is_dir,\n            children=children)\n\n        return stat", "response": "Request remote file or directory status info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if given path is a directory.", "response": "def is_dir(self, pathobj):\n        \"\"\"\n        Returns True if given path is a directory\n        \"\"\"\n        try:\n            stat = self.stat(pathobj)\n\n            return stat.is_dir\n        except OSError as exc:\n            if exc.errno != errno.ENOENT:\n                raise\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef listdir(self, pathobj):\n        stat = self.stat(pathobj)\n\n        if not stat.is_dir:\n            raise OSError(20, \"Not a directory: %s\" % str(pathobj))\n\n        return stat.children", "response": "Returns a list of immediate sub - directories and files in pathobj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mkdir(self, pathobj, _):\n        if not pathobj.drive or not pathobj.root:\n            raise RuntimeError(\"Full path required: '%s'\" % str(pathobj))\n\n        if pathobj.exists():\n            raise OSError(17, \"File exists: '%s'\" % str(pathobj))\n\n        url = str(pathobj) + '/'\n        text, code = self.rest_put(url, session=pathobj.session, verify=pathobj.verify, cert=pathobj.cert)\n\n        if not code == 201:\n            raise RuntimeError(\"%s %d\" % (text, code))", "response": "Creates remote directory containing the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a directory from the cache", "response": "def rmdir(self, pathobj):\n        \"\"\"\n        Removes a directory\n        \"\"\"\n        stat = self.stat(pathobj)\n\n        if not stat.is_dir:\n            raise OSError(20, \"Not a directory: '%s'\" % str(pathobj))\n\n        url = str(pathobj) + '/'\n\n        text, code = self.rest_del(url, session=pathobj.session, verify=pathobj.verify, cert=pathobj.cert)\n\n        if code not in [200, 202, 204]:\n            raise RuntimeError(\"Failed to delete directory: '%s'\" % text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unlink(self, pathobj):\n\n        # TODO: Why do we forbid remove folder?\n        # if stat.is_dir:\n        #     raise OSError(1, \"Operation not permitted: '%s'\" % str(pathobj))\n\n        url = str(pathobj)\n        text, code = self.rest_del(url, session=pathobj.session, verify=pathobj.verify,\n                                   cert=pathobj.cert)\n\n        if code not in [200, 202, 204]:\n            raise RuntimeError(\"Failed to delete file: %d '%s'\" % (code, text))", "response": "Removes a file from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an empty file and return the ID of the new file.", "response": "def touch(self, pathobj):\n        \"\"\"\n        Create an empty file\n        \"\"\"\n        if not pathobj.drive or not pathobj.root:\n            raise RuntimeError('Full path required')\n\n        if pathobj.exists():\n            return\n\n        url = str(pathobj)\n        text, code = self.rest_put(url, session=pathobj.session, verify=pathobj.verify, cert=pathobj.cert)\n\n        if not code == 201:\n            raise RuntimeError(\"%s %d\" % (text, code))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef owner(self, pathobj):\n        stat = self.stat(pathobj)\n\n        if not stat.is_dir:\n            return stat.modified_by\n        else:\n            return 'nobody'", "response": "Returns file owner of pathobj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning file creator of a file", "response": "def creator(self, pathobj):\n        \"\"\"\n        Returns file creator\n        This makes little sense for Artifactory, but to be consistent\n        with pathlib, we return created_by instead, if available\n        \"\"\"\n        stat = self.stat(pathobj)\n\n        if not stat.is_dir:\n            return stat.created_by\n        else:\n            return 'nobody'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, src, dst, suppress_layouts=False):\n        url = '/'.join([src.drive,\n                        'api/copy',\n                        str(src.relative_to(src.drive)).rstrip('/')])\n\n        params = {'to': str(dst.relative_to(dst.drive)).rstrip('/'),\n                  'suppressLayouts': int(suppress_layouts)}\n\n        text, code = self.rest_post(url,\n                                    params=params,\n                                    session=src.session,\n                                    verify=src.verify,\n                                    cert=src.cert)\n\n        if code not in [200, 201]:\n            raise RuntimeError(\"%s\" % text)", "response": "Copy artifact from src to dst"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the parent of the path.", "response": "def parent(self):\n        \"\"\"\n        The logical parent of the path.\n        \"\"\"\n        obj = super(ArtifactoryPath, self).parent\n        obj.auth = self.auth\n        obj.verify = self.verify\n        obj.cert = self.cert\n        obj.session = self.session\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new ArtifactoryPath with the file name changed.", "response": "def with_name(self, name):\n        \"\"\"\n        Return a new path with the file name changed.\n        \"\"\"\n        obj = super(ArtifactoryPath, self).with_name(name)\n        obj.auth = self.auth\n        obj.verify = self.verify\n        obj.cert = self.cert\n        obj.session = self.session\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new ArtifactoryPath with the file suffix changed.", "response": "def with_suffix(self, suffix):\n        \"\"\"\n        Return a new path with the file suffix changed (or added, if none).\n        \"\"\"\n        obj = super(ArtifactoryPath, self).with_suffix(suffix)\n        obj.auth = self.auth\n        obj.verify = self.verify\n        obj.cert = self.cert\n        obj.session = self.session\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen the given Artifactory URI and return a file - like object - like object.", "response": "def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the given Artifactory URI and return a file-like object\n        HTTPResponse, as if it was a regular filesystem object.\n        The only difference is that this object doesn't support seek()\n        \"\"\"\n        if mode != 'r' or buffering != -1 or encoding or errors or newline:\n            raise NotImplementedError('Only the default open() ' +\n                                      'arguments are supported')\n\n        return self._accessor.open(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a file if it doesn t exist.", "response": "def touch(self, mode=0o666, exist_ok=True):\n        \"\"\"\n        Create a file if it doesn't exist.\n        Mode is ignored by Artifactory.\n        \"\"\"\n        if self.exists() and not exist_ok:\n            raise OSError(17, \"File exists\", str(self))\n\n        self._accessor.touch(self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading the given file object to this path.", "response": "def deploy(self, fobj, md5=None, sha1=None, parameters={}):\n        \"\"\"\n        Upload the given file object to this path\n        \"\"\"\n        return self._accessor.deploy(self, fobj, md5, sha1, parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupload the given file to this path", "response": "def deploy_file(self,\n                    file_name,\n                    calc_md5=True,\n                    calc_sha1=True,\n                    parameters={}):\n        \"\"\"\n        Upload the given file to this path\n        \"\"\"\n        if calc_md5:\n            md5 = md5sum(file_name)\n        if calc_sha1:\n            sha1 = sha1sum(file_name)\n\n        target = self\n\n        if self.is_dir():\n            target = self / pathlib.Path(file_name).name\n\n        with open(file_name, 'rb') as fobj:\n            target.deploy(fobj, md5, sha1, parameters)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy the artifact from this path to the destination path.", "response": "def copy(self, dst, suppress_layouts=False):\n        \"\"\"\n        Copy artifact from this path to destinaiton.\n        If files are on the same instance of artifactory, lightweight (local)\n        copying will be attempted.\n\n        The suppress_layouts parameter, when set to True, will allow artifacts\n        from one path to be copied directly into another path without enforcing\n        repository layouts. The default behaviour is to copy to the repository\n        root, but remap the [org], [module], [baseVer], etc. structure to the\n        target repository.\n\n        For example, if we have a builds repository using the default maven2\n        repository where we publish our builds. We also have a published\n        repository where a directory for production and a directory for\n        staging environments should hold the current promoted builds. How do\n        we copy the contents of a build over to the production folder?\n\n        >>> from artifactory import ArtifactoryPath\n        >>> source = ArtifactoryPath(\"http://example.com/artifactory/builds/product/product/1.0.0/\")\n        >>> dest = ArtifactoryPath(\"http://example.com/artifactory/published/production/\")\n\n        Using copy with the default, suppress_layouts=False, the artifacts inside\n        builds/product/product/1.0.0/ will not end up in the published/production\n        path as we intended, but rather the entire structure product/product/1.0.0\n        is placed in the destination repo.\n\n        >>> source.copy(dest)\n        >>> for p in dest: print p\n        http://example.com/artifactory/published/production/foo-0.0.1.gz\n        http://example.com/artifactory/published/production/foo-0.0.1.pom\n\n        >>> for p in ArtifactoryPath(\"http://example.com/artifactory/published/product/product/1.0.0.tar\"):\n        ...   print p\n        http://example.com/artifactory/published/product/product/1.0.0/product-1.0.0.tar.gz\n        http://example.com/artifactory/published/product/product/1.0.0/product-1.0.0.tar.pom\n\n        Using copy with suppress_layouts=True, the contents inside our source are copied\n        directly inside our dest as we intended.\n\n        >>> source.copy(dest, suppress_layouts=True)\n        >>> for p in dest: print p\n        http://example.com/artifactory/published/production/foo-0.0.1.gz\n        http://example.com/artifactory/published/production/foo-0.0.1.pom\n        http://example.com/artifactory/published/production/product-1.0.0.tar.gz\n        http://example.com/artifactory/published/production/product-1.0.0.tar.pom\n        \"\"\"\n        if self.drive == dst.drive:\n            self._accessor.copy(self, dst, suppress_layouts=suppress_layouts)\n        else:\n            with self.open() as fobj:\n                dst.deploy(fobj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves artifact from this path to destinaiton.", "response": "def move(self, dst):\n        \"\"\"\n        Move artifact from this path to destinaiton.\n        \"\"\"\n        if self.drive != dst.drive:\n            raise NotImplementedError(\n                \"Moving between instances is not implemented yet\")\n\n        self._accessor.move(self, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds new or modifies existing properties of the object.", "response": "def set_properties(self, properties, recursive=True):\n        \"\"\"\n        Adds new or modifies existing properties listed in properties\n\n        properties - is a dict which contains the property names and values to set.\n                     Property values can be a list or tuple to set multiple values\n                     for a key.\n        recursive  - on folders property attachment is recursive by default. It is\n                     possible to force recursive behavior.\n        \"\"\"\n        if not properties:\n            return\n\n        # If URL > 13KB, nginx default raise error '414 Request-URI Too Large'\n        MAX_SIZE = 50\n        if len(properties) > MAX_SIZE:\n            for chunk in chunks(properties, MAX_SIZE):\n                self._accessor.set_properties(self, chunk, recursive)\n        else:\n            self._accessor.set_properties(self, properties, recursive)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete properties listed in properties", "response": "def del_properties(self, properties, recursive=None):\n        \"\"\"\n        Delete properties listed in properties\n\n        properties - iterable contains the property names to delete. If it is an\n                     str it will be casted to tuple.\n        recursive  - on folders property attachment is recursive by default. It is\n                     possible to force recursive behavior.\n        \"\"\"\n        return self._accessor.del_properties(self, properties, recursive)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends AQL query to Artifactory", "response": "def aql(self, *args):\n        \"\"\"\n        Send AQL query to Artifactory\n        :param args:\n        :return:\n        \"\"\"\n        aql_query_url = '{}/api/search/aql'.format(self.drive)\n        aql_query_text = self.create_aql_text(*args)\n        r = self.session.post(aql_query_url, data=aql_query_text)\n        r.raise_for_status()\n        content = r.json()\n        return content['results']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate AQL querty from string or list or dict arguments", "response": "def create_aql_text(*args):\n        \"\"\"\n        Create AQL querty from string or list or dict arguments\n        \"\"\"\n        aql_query_text = \"\"\n        for arg in args:\n            if isinstance(arg, dict):\n                arg = \"({})\".format(json.dumps(arg))\n            elif isinstance(arg, list):\n                arg = \"({})\".format(json.dumps(arg)).replace(\"[\", \"\").replace(\"]\", \"\")\n            aql_query_text += arg\n        return aql_query_text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert raw AQL result to pathlib object", "response": "def from_aql(self, result):\n        \"\"\"\n        Convert raw AQL result to pathlib object\n        :param result: ONE raw result\n        :return:\n        \"\"\"\n        result_type = result.get('type')\n        if result_type not in ('file', 'folder'):\n            raise RuntimeError(\"Path object with type '{}' doesn't support. File or folder only\".format(result_type))\n\n        result_path = \"{}/{repo}/{path}/{name}\".format(self.drive, **result)\n        obj = ArtifactoryPath(result_path, auth=self.auth, verify=self.verify, cert=self.cert, session=self.session)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_watch(self, path, superficial=False):\n\n        wd = self.__watches.get(path)\n        if wd is None:\n            return\n\n        _LOGGER.debug(\"Removing watch for watch-handle (%d): [%s]\",\n                      wd, path)\n\n        del self.__watches[path]\n\n        self.remove_watch_with_id(wd)", "response": "Remove our tracking information and call inotify to stop watching\n        the given path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_inotify_event(self, wd):\n\n        b = os.read(wd, 1024)\n        if not b:\n            return\n\n        self.__buffer += b\n\n        while 1:\n            length = len(self.__buffer)\n\n            if length < _STRUCT_HEADER_LENGTH:\n                _LOGGER.debug(\"Not enough bytes for a header.\")\n                return\n\n            # We have, at least, a whole-header in the buffer.\n\n            peek_slice = self.__buffer[:_STRUCT_HEADER_LENGTH]\n\n            header_raw = struct.unpack(\n                            _HEADER_STRUCT_FORMAT,\n                            peek_slice)\n\n            header = _INOTIFY_EVENT(*header_raw)\n            type_names = self._get_event_names(header.mask)\n            _LOGGER.debug(\"Events received in stream: {}\".format(type_names))\n\n            event_length = (_STRUCT_HEADER_LENGTH + header.len)\n            if length < event_length:\n                return\n\n            filename = self.__buffer[_STRUCT_HEADER_LENGTH:event_length]\n\n            # Our filename is 16-byte aligned and right-padded with NULs.\n            filename_bytes = filename.rstrip(b'\\0')\n\n            self.__buffer = self.__buffer[event_length:]\n\n            path = self.__watches_r.get(header.wd)\n            if path is not None:\n                filename_unicode = filename_bytes.decode('utf8')\n                yield (header, type_names, path, filename_unicode)\n\n            buffer_length = len(self.__buffer)\n            if buffer_length < _STRUCT_HEADER_LENGTH:\n                break", "response": "Handle a series of events coming - in from inotify."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield one event after another.", "response": "def event_gen(\n            self, timeout_s=None, yield_nones=True, filter_predicate=None,\n            terminal_events=_DEFAULT_TERMINAL_EVENTS):\n        \"\"\"Yield one event after another. If `timeout_s` is provided, we'll\n        break when no event is received for that many seconds.\n        \"\"\"\n\n        # We will either return due to the optional filter or because of a\n        # timeout. The former will always set this. The latter will never set\n        # this.\n        self.__last_success_return = None\n\n        last_hit_s = time.time()\n        while True:\n            block_duration_s = self.__get_block_duration()\n\n            # Poll, but manage signal-related errors.\n\n            try:\n                events = self.__epoll.poll(block_duration_s)\n            except IOError as e:\n                if e.errno != EINTR:\n                    raise\n\n                if timeout_s is not None:\n                    time_since_event_s = time.time() - last_hit_s\n                    if time_since_event_s > timeout_s:\n                        break\n\n                continue\n\n            # Process events.\n\n            for fd, event_type in events:\n                # (fd) looks to always match the inotify FD.\n\n                names = self._get_event_names(event_type)\n                _LOGGER.debug(\"Events received from epoll: {}\".format(names))\n\n                for (header, type_names, path, filename) \\\n                        in self._handle_inotify_event(fd):\n                    last_hit_s = time.time()\n\n                    e = (header, type_names, path, filename)\n                    for type_name in type_names:\n                        if filter_predicate is not None and \\\n                           filter_predicate(type_name, e) is False:\n                             self.__last_success_return = (type_name, e)\n                             return\n                        elif type_name in terminal_events:\n                            raise TerminalEventException(type_name, e)\n\n                    yield e\n\n            if timeout_s is not None:\n                time_since_event_s = time.time() - last_hit_s\n                if time_since_event_s > timeout_s:\n                    break\n\n            if yield_nones is True:\n                yield None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef event_gen(self, ignore_missing_new_folders=False, **kwargs):\n\n        for event in self._i.event_gen(**kwargs):\n            if event is not None:\n                (header, type_names, path, filename) = event\n\n                if header.mask & inotify.constants.IN_ISDIR:\n                    full_path = os.path.join(path, filename)\n\n                    if (\n                        (header.mask & inotify.constants.IN_MOVED_TO) or\n                        (header.mask & inotify.constants.IN_CREATE)\n                       ) and \\\n                       (\n                        os.path.exists(full_path) is True or\n                        ignore_missing_new_folders is False\n                       ):\n                        _LOGGER.debug(\"A directory has been created. We're \"\n                                      \"adding a watch on it (because we're \"\n                                      \"being recursive): [%s]\", full_path)\n\n\n                        self._i.add_watch(full_path, self._mask)\n\n                    if header.mask & inotify.constants.IN_MOVED_FROM:\n                        _LOGGER.debug(\"A directory has been removed. We're \"\n                                      \"being recursive, but it would have \"\n                                      \"automatically been deregistered: [%s]\",\n                                      full_path)\n\n                        # The watch would've already been cleaned-up internally.\n                        self._i.remove_watch(full_path, superficial=True)\n                    elif header.mask & inotify.constants.IN_MOVED_FROM:\n                        _LOGGER.debug(\"A directory has been renamed. We're \"\n                                      \"being recursive, but it would have \"\n                                      \"automatically been deregistered: [%s]\",\n                                      full_path)\n\n                        self._i.remove_watch(full_path, superficial=True)\n                    elif header.mask & inotify.constants.IN_MOVED_TO:\n                        _LOGGER.debug(\"A directory has been renamed. We're \"\n                                      \"adding a watch on it (because we're \"\n                                      \"being recursive): [%s]\", full_path)\n\n                        self._i.add_watch(full_path, self._mask)\n\n            yield event", "response": "This is a secondary generator that wraps the principal one and adds watches as directories are added and removes watches as directories are removed and sets ignore_missing_new_folders to True."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, e, k, v, real_k=None, check_kw_name=False):\n        if self.escape:\n            v = v.strip().replace(\"\\\\\" + self.quote, self.quote)\n        return super(kv_transformer, self).set(e, k, v, real_k=real_k, check_kw_name=check_kw_name)", "response": "override base to handle escape case"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy_project(from_client, to_client, from_project, to_project, copy_machine_group=False):\n\n    # copy project\n    ret = from_client.get_project(from_project)\n    try:\n        ret = to_client.create_project(to_project, ret.get_description())\n    except LogException as ex:\n        if ex.get_error_code() == 'ProjectAlreadyExist':\n            # don't create the project as it already exists\n            pass\n        else:\n            raise\n\n    default_fetch_size = 100\n\n    # list logstore and copy them\n    offset, size = 0, default_fetch_size\n    while True:\n        ret = from_client.list_logstore(from_project, offset=offset, size=size)\n        count = ret.get_logstores_count()\n        total = ret.get_logstores_total()\n        for logstore_name in ret.get_logstores():\n            # copy logstore\n            ret = from_client.get_logstore(from_project, logstore_name)\n            res_shard = from_client.list_shards(from_project, logstore_name)\n            expected_rwshard_count = len([shard for shard in res_shard.shards if shard['status'].lower() == 'readwrite'])\n            ret = to_client.create_logstore(to_project, logstore_name, ret.get_ttl(),\n                                            min(expected_rwshard_count, MAX_INIT_SHARD_COUNT),\n                                            enable_tracking=ret.get_enable_tracking(),\n                                            append_meta=ret.append_meta,\n                                            auto_split=ret.auto_split,\n                                            max_split_shard=ret.max_split_shard,\n                                            preserve_storage=ret.preserve_storage\n                                            )\n\n            # copy index\n            try:\n                ret = from_client.get_index_config(from_project, logstore_name)\n                ret = to_client.create_index(to_project, logstore_name, ret.get_index_config())\n            except LogException as ex:\n                if ex.get_error_code() == 'IndexConfigNotExist':\n                    pass\n                else:\n                    raise\n\n        offset += count\n        if count < size or offset >= total:\n            break\n\n    # list logtail config and copy them\n    offset, size = 0, default_fetch_size\n    while True:\n        ret = from_client.list_logtail_config(from_project, offset=offset, size=size)\n        count = ret.get_configs_count()\n        total = ret.get_configs_total()\n\n        for config_name in ret.get_configs():\n            ret = from_client.get_logtail_config(from_project, config_name)\n            ret = to_client.create_logtail_config(to_project, ret.logtail_config)\n\n        offset += count\n        if count < size or offset >= total:\n            break\n\n    # list machine group and copy them\n    offset, size = 0, default_fetch_size\n    while copy_machine_group:\n        ret = from_client.list_machine_group(from_project, offset=offset, size=size)\n        count = ret.get_machine_group_count()\n        total = ret.get_machine_group_total()\n\n        for group_name in ret.get_machine_group():\n            ret = from_client.get_machine_group(from_project, group_name)\n            ret = to_client.create_machine_group(to_project, ret.get_machine_group())\n\n            # list all applied config and copy the relationship\n            ret = from_client.get_machine_group_applied_configs(from_project, group_name)\n            for config_name in ret.get_configs():\n                to_client.apply_config_to_machine_group(to_project, config_name, group_name)\n\n        offset += count\n        if count < size or offset >= total:\n            break", "response": "copy project logstore machine group and logtail config to target project"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_logstore(from_client, from_project, from_logstore, to_logstore, to_project=None, to_client=None):\n\n    # check client\n    if to_project is not None:\n        # copy to a different project in different client\n        to_client = to_client or from_client\n\n        # check if target project exists or not\n        ret = from_client.get_project(from_project)\n        try:\n            ret = to_client.create_project(to_project, ret.get_description())\n        except LogException as ex:\n            if ex.get_error_code() == 'ProjectAlreadyExist':\n                # don't create the project as it already exists\n                pass\n            else:\n                raise\n\n    to_project = to_project or from_project\n    to_client = to_client or from_client\n\n    # return if logstore are the same one\n    if from_client is to_client and from_project == to_project and from_logstore == to_logstore:\n        return\n\n    # copy logstore\n    ret = from_client.get_logstore(from_project, from_logstore)\n    res_shard = from_client.list_shards(from_project, from_logstore)\n    expected_rwshard_count = len([shard for shard in res_shard.shards if shard['status'].lower() == 'readwrite'])\n    try:\n        ret = to_client.create_logstore(to_project, to_logstore,\n                                        ttl=ret.get_ttl(),\n                                        shard_count=min(expected_rwshard_count, MAX_INIT_SHARD_COUNT),\n                                        enable_tracking=ret.get_enable_tracking(),\n                                        append_meta=ret.append_meta,\n                                        auto_split=ret.auto_split,\n                                        max_split_shard=ret.max_split_shard,\n                                        preserve_storage=ret.preserve_storage)\n    except LogException as ex:\n        if ex.get_error_code() == 'LogStoreAlreadyExist':\n            # update logstore's settings\n            ret = to_client.update_logstore(to_project, to_logstore,\n                                            ttl=ret.get_ttl(),\n                                            enable_tracking=ret.get_enable_tracking(),\n                                            append_meta=ret.append_meta,\n                                            auto_split=ret.auto_split,\n                                            max_split_shard=ret.max_split_shard,\n                                            preserve_storage=ret.preserve_storage\n                                            )\n\n            # arrange shard to expected count\n            res = arrange_shard(to_client, to_project, to_logstore, min(expected_rwshard_count, MAX_INIT_SHARD_COUNT))\n        else:\n            raise\n\n\n    # copy index\n    try:\n        ret = from_client.get_index_config(from_project, from_logstore)\n        ret = to_client.create_index(to_project, to_logstore, ret.get_index_config())\n    except LogException as ex:\n        if ex.get_error_code() == 'IndexConfigNotExist':\n            # source has no index\n            pass\n        elif ex.get_error_code() == 'IndexAlreadyExist':\n            # target already has index, overwrite it\n            ret = to_client.update_index(to_project, to_logstore, ret.get_index_config())\n            pass\n        else:\n            raise\n\n    # list logtail config linked to the logstore and copy them\n    default_fetch_size = 100\n    offset, size = 0, default_fetch_size\n    while True:\n        ret = from_client.list_logtail_config(from_project, offset=offset, size=size)\n        count = ret.get_configs_count()\n        total = ret.get_configs_total()\n\n        for config_name in ret.get_configs():\n            ret = from_client.get_logtail_config(from_project, config_name)\n            config = ret.logtail_config\n            if config.logstore_name != from_logstore:\n                continue\n\n            config.config_name = to_logstore + '_' + config_name\n            config.logstore_name = to_logstore\n            ret = to_client.create_logtail_config(to_project, config)\n\n        offset += count\n        if count < size or offset >= total:\n            break", "response": "copy logstore to target logstore"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all data using the fn", "response": "def list_more(fn, offset, size, batch_size, *args):\n    \"\"\"list all data using the fn\n    \"\"\"\n    if size < 0:\n        expected_total_size = six.MAXSIZE\n    else:\n        expected_total_size = size\n        batch_size = min(size, batch_size)\n\n    response = None\n    total_count_got = 0\n    while True:\n        ret = fn(*args, offset=offset, size=batch_size)\n        if response is None:\n            response = ret\n        else:\n            response.merge(ret)\n\n        count = ret.get_count()\n        total = ret.get_total()\n        offset += count\n        total_count_got += count\n        batch_size = min(batch_size, expected_total_size - total_count_got)\n\n        if count == 0 or offset >= total or total_count_got >= expected_total_size:\n            break\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_more(fn, offset, size, batch_size, *args):\n    if size < 0:\n        expected_total_size = six.MAXSIZE\n    else:\n        expected_total_size = size\n        batch_size = min(size, batch_size)\n\n    response = None\n    total_count_got = 0\n    complete = False\n    while True:\n        ret = fn(*args, offset=offset, size=batch_size)\n\n        if response is None:\n            response = ret\n        else:\n            response.merge(ret)\n\n        # if incompete, exit\n        if not ret.is_completed():\n            break\n\n        count = ret.get_count()\n        offset += count\n        total_count_got += count\n        batch_size = min(batch_size, expected_total_size - total_count_got)\n        if count == 0 or total_count_got >= expected_total_size:\n            break\n\n    return response", "response": "list all data using the fn\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse shard list :param shard_list: format like: 1,5-10,20 :param current_shard_list: current shard list :return:", "response": "def _parse_shard_list(shard_list, current_shard_list):\n    \"\"\"\n    parse shard list\n    :param shard_list: format like: 1,5-10,20\n    :param current_shard_list: current shard list\n    :return:\n    \"\"\"\n    if not shard_list:\n        return current_shard_list\n    target_shards = []\n    for n in shard_list.split(\",\"):\n        n = n.strip()\n        if n.isdigit() and n in current_shard_list:\n            target_shards.append(n)\n        elif n:\n            rng = n.split(\"-\")\n            if len(rng) == 2:\n                s = rng[0].strip()\n                e = rng[1].strip()\n                if s.isdigit() and e.isdigit():\n                    for x in range(int(s), int(e)+1):\n                        if str(x) in current_shard_list:\n                            target_shards.append(str(x))\n\n    logger.info(\"parse_shard, shard_list: '{0}' current shard '{1}' result: '{2}'\".format(shard_list,\n                                                                                          current_shard_list, target_shards))\n    if not target_shards:\n        raise LogException(\"InvalidParameter\", \"There's no available shard with settings {0}\".format(shard_list))\n\n    return target_shards"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_data(from_client, from_project, from_logstore, from_time, to_time=None,\n              to_client=None, to_project=None, to_logstore=None,\n              shard_list=None,\n              batch_size=None, compress=None, new_topic=None, new_source=None):\n    \"\"\"\n    copy data from one logstore to another one (could be the same or in different region), the time is log received time on server side.\n\n    \"\"\"\n    to_client = to_client or from_client\n    # increase the timeout to 2 min at least\n    from_client.timeout = max(from_client.timeout, 120)\n    to_client.timeout = max(to_client.timeout, 120)\n\n    to_project = to_project or from_project\n    to_logstore = to_logstore or from_logstore\n    to_time = to_time or \"end\"\n\n    cpu_count = multiprocessing.cpu_count() * 2\n    shards = from_client.list_shards(from_project, from_logstore).get_shards_info()\n    current_shards = [str(shard['shardID']) for shard in shards]\n    target_shards = _parse_shard_list(shard_list, current_shards)\n    worker_size = min(cpu_count, len(target_shards))\n\n    result = dict()\n    total_count = 0\n    with ProcessPoolExecutor(max_workers=worker_size) as pool:\n        futures = [pool.submit(copy_worker, from_client, from_project, from_logstore, shard,\n                               from_time, to_time,\n                               to_client, to_project, to_logstore,\n                               batch_size=batch_size, compress=compress,\n                               new_topic=new_topic, new_source=new_source)\n                   for shard in target_shards]\n\n        for future in as_completed(futures):\n            partition, count = future.result()\n            total_count += count\n            if count:\n                result[partition] = count\n\n    return LogResponse({}, {\"total_count\": total_count, \"shards\": result})", "response": "Copy data from one logstore to another."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_one_shard_to_multiple(client, project, logstore, shard_info, count, current_shard_count):\n    distance = shard_info['length'] // count\n    if distance <= 0 or count <= 1:\n        return [shard_info['info']], 0\n\n    rw_shards, increased_shard_count = {shard_info['id']: shard_info['info']}, 0\n    for x in range(1, count):\n        new_hash = shard_info['start'] + distance * x\n        new_hash = hex(new_hash)[2:].strip('lL')\n        new_hash = '0' * (TOTAL_HASH_LENGTH - len(new_hash)) + new_hash\n        try:\n            if x == 1:\n                res = client.split_shard(project, logstore, shard_info['id'], new_hash)\n            else:\n                res = client.split_shard(project, logstore, current_shard_count - 1, new_hash)\n\n            # new rw_shards\n            for shard in res.shards:\n                if shard['status'] == 'readonly':\n                    del rw_shards[shard['shardID']]\n                else:\n                    rw_shards[shard['shardID']] = shard\n\n            current_shard_count += res.count - 1\n            increased_shard_count += res.count - 1\n            logger.info(\"split shard: project={0}, logstore={1}, shard_info={2}, count={3}, current_shard_count={4}\".format(project, logstore, shard_info, count, current_shard_count))\n        except Exception as ex:\n            print(ex)\n            print(x, project, logstore, shard_info, count, current_shard_count)\n            raise\n\n    return rw_shards.values(), increased_shard_count", "response": "Split one shard to multiple shards."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform_data(from_client, from_project, from_logstore, from_time,\n                   to_time=None,\n                   to_client=None, to_project=None, to_logstore=None,\n                   shard_list=None,\n                   config=None,\n                   batch_size=None, compress=None,\n                   cg_name=None, c_name=None,\n                   cg_heartbeat_interval=None, cg_data_fetch_interval=None, cg_in_order=None,\n                   cg_worker_pool_size=None\n                   ):\n    \"\"\"\n    transform data from one logstore to another one (could be the same or in different region), the time is log received time on server side.\n\n    \"\"\"\n    if not config:\n        logger.info(\"transform_data: config is not configured, use copy data by default.\")\n        return copy_data(from_client, from_project, from_logstore, from_time, to_time=to_time,\n                         to_client=to_client, to_project=to_project, to_logstore=to_logstore,\n                         shard_list=shard_list,\n                         batch_size=batch_size, compress=compress)\n\n    to_client = to_client or from_client\n\n    # increase the timeout to 2 min at least\n    from_client.timeout = max(from_client.timeout, 120)\n    to_client.timeout = max(to_client.timeout, 120)\n    to_project = to_project or from_project\n    to_logstore = to_logstore or from_logstore\n\n    if not cg_name:\n        # batch mode\n        to_time = to_time or \"end\"\n        cpu_count = multiprocessing.cpu_count() * 2\n        shards = from_client.list_shards(from_project, from_logstore).get_shards_info()\n        current_shards = [str(shard['shardID']) for shard in shards]\n        target_shards = _parse_shard_list(shard_list, current_shards)\n\n        worker_size = min(cpu_count, len(target_shards))\n\n        result = dict()\n        total_count = 0\n        total_removed = 0\n        with ProcessPoolExecutor(max_workers=worker_size) as pool:\n            futures = [pool.submit(transform_worker, from_client, from_project, from_logstore, shard,\n                                   from_time, to_time, config,\n                                   to_client, to_project, to_logstore,\n                                   batch_size=batch_size, compress=compress)\n                       for shard in target_shards]\n\n            for future in as_completed(futures):\n                if future.exception():\n                    logger.error(\"get error when transforming data: {0}\".format(future.exception()))\n                else:\n                    partition, count, removed, processed, failed = future.result()\n                    total_count += count\n                    total_removed += removed\n                    if count:\n                        result[partition] = {\"total_count\": count, \"transformed\":\n                            processed, \"removed\": removed, \"failed\": failed}\n\n        return LogResponse({}, {\"total_count\": total_count, \"shards\": result})\n\n    else:\n        # consumer group mode\n        c_name = c_name or \"transform_data_{0}\".format(multiprocessing.current_process().pid)\n        cg_heartbeat_interval = cg_heartbeat_interval or 20\n        cg_data_fetch_interval = cg_data_fetch_interval or 2\n        cg_in_order = False if cg_in_order is None else cg_in_order\n        cg_worker_pool_size = cg_worker_pool_size or 3\n\n        option = LogHubConfig(from_client._endpoint, from_client._accessKeyId, from_client._accessKey,\n                              from_project, from_logstore, cg_name,\n                              c_name, cursor_position=CursorPosition.SPECIAL_TIMER_CURSOR,\n                              cursor_start_time=from_time,\n                              cursor_end_time=to_time,\n                              heartbeat_interval=cg_heartbeat_interval, data_fetch_interval=cg_data_fetch_interval,\n                              in_order=cg_in_order,\n                              worker_pool_size=cg_worker_pool_size)\n\n        TransformDataConsumer.set_transform_options(config, to_client, to_project, to_logstore)\n\n        result = {\"total_count\": 0, \"shards\": {}}\n        l = RLock()\n\n        def status_updator(shard_id, count=0, removed=0, processed=0, failed=0):\n            logger.info(\"status update is called, shard: {0}, count: {1}, removed: {2}, processed: {3}, failed: {4}\".format(shard_id, count, removed, processed, failed))\n\n            with l:\n                result[\"total_count\"] += count\n                if shard_id in result[\"shards\"]:\n                    data = result[\"shards\"][shard_id]\n                    result[\"shards\"][shard_id] = {\"total_count\": data[\"total_count\"] + count, \"transformed\": data[\"transformed\"] + processed, \"removed\": data[\"removed\"] + removed, \"failed\": data[\"failed\"] + failed}\n                else:\n                    result[\"shards\"][shard_id] = {\"total_count\": count, \"transformed\": processed, \"removed\": removed, \"failed\": failed}\n\n        worker = ConsumerWorker(TransformDataConsumer, consumer_option=option, args=(status_updator, ) )\n        worker.start()\n\n        try:\n            while worker.is_alive():\n                worker.join(timeout=60)\n            logger.info(\"transform_data: worker exit unexpected, try to shutdown it\")\n            worker.shutdown()\n        except KeyboardInterrupt:\n            logger.info(\"transform_data: *** try to exit **** \")\n            print(\"try to stop transforming data.\")\n            worker.shutdown()\n            worker.join(timeout=120)\n\n        return LogResponse({}, result)", "response": "Transform data from one logstore to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pluralize(singular):\n    if not singular:\n        return ''\n    plural = ABERRANT_PLURAL_MAP.get(singular)\n    if plural:\n        return plural\n    root = singular\n    try:\n        if singular[-1] == 'y' and singular[-2] not in VOWELS:\n            root = singular[:-1]\n            suffix = 'ies'\n        elif singular[-1] == 's':\n            if singular[-2] in VOWELS:\n                if singular[-3:] == 'ius':\n                    root = singular[:-2]\n                    suffix = 'i'\n                else:\n                    root = singular[:-1]\n                    suffix = 'ses'\n            else:\n                suffix = 'es'\n        elif singular[-2:] in ('ch', 'sh'):\n            suffix = 'es'\n        else:\n            suffix = 's'\n    except IndexError:\n        suffix = 's'\n    plural = root + suffix\n    return plural", "response": "Return plural form of given lowercase singular word."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_stats_query(query):\r\n    if not query:\r\n        return False\r\n\r\n    # remove all \" enclosed strings\r\n    nq = re.sub(r'\"[^\"]*\"', '', query)\r\n\r\n    # check if there's | .... select\r\n    if re.findall(r'\\|.*\\bselect\\b', nq, re.I|re.DOTALL):\r\n        return True\r\n\r\n    return False", "response": "check if the query is a normal search or select query\r\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_host_ip(logHost):\r\n        s = None\r\n        try:\r\n            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\r\n            s.connect((logHost, 80))\r\n            ip = s.getsockname()[0]\r\n            return ip\r\n        except Exception:\r\n            return '127.0.0.1'\r\n        finally:\r\n            if s:\r\n                s.close()", "response": "Returns the local ip of the log host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_request_authorization(method, resource, key, params, headers):\r\n        if not key:\r\n            return six.b('')\r\n        content = method + \"\\n\"\r\n        if 'Content-MD5' in headers:\r\n            content += headers['Content-MD5']\r\n        content += '\\n'\r\n        if 'Content-Type' in headers:\r\n            content += headers['Content-Type']\r\n        content += \"\\n\"\r\n        content += headers['Date'] + \"\\n\"\r\n        content += Util.canonicalized_log_headers(headers)\r\n        content += Util.canonicalized_resource(resource, params)\r\n        return Util.hmac_sha1(content, key)", "response": "Returns the authorization header for the given request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_unicode_to_str(data):\r\n        if six.PY2 and isinstance(data, six.text_type):\r\n            return data.encode('utf8')\r\n        elif six.PY3 and isinstance(data, six.binary_type):\r\n            return data.decode('utf8')\r\n        elif isinstance(data, collections.Mapping):\r\n            return dict((Util.convert_unicode_to_str(k), Util.convert_unicode_to_str(v))\r\n                        for k, v in six.iteritems(data))\r\n        elif isinstance(data, collections.Iterable) and not isinstance(data, (six.binary_type, six.string_types)):\r\n            return type(data)(map(Util.convert_unicode_to_str, data))\r\n\r\n        return data", "response": "Convert unicode to str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget header value with title try to get key from header and consider case sensitive", "response": "def h_v_t(header, key):\r\n        \"\"\"\r\n        get header value with title\r\n        try to get key from header and consider case sensitive\r\n        e.g. header['x-log-abc'] or header['X-Log-Abc']\r\n        :param header:\r\n        :param key:\r\n        :return:\r\n        \"\"\"\r\n        if key not in header:\r\n            key = key.title()\r\n\r\n            if key not in header:\r\n                raise ValueError(\"Unexpected header in response, missing: \" + key + \" headers:\\n\" + str(header))\r\n\r\n        return header[key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef h_v_td(header, key, default):\r\n        if key not in header:\r\n            key = key.title()\r\n\r\n        return header.get(key, default)", "response": "get header value with title with default value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate common logtail config from loaded json value", "response": "def generate_common_reg_log_config(json_value):\n        \"\"\"Generate common logtail config from loaded json value\n\n        :param json_value:\n        :return:\n        \"\"\"\n        input_detail = copy.deepcopy(json_value['inputDetail'])\n        output_detail = json_value['outputDetail']\n        logSample = json_value.get('logSample', '')\n        config_name = json_value['configName']\n        logstore_name = output_detail['logstoreName']\n        endpoint = output_detail.get('endpoint', '')\n\n        log_path = input_detail['logPath']\n        file_pattern = input_detail['filePattern']\n        time_format = input_detail['timeFormat']\n        log_begin_regex = input_detail.get('logBeginRegex', '')\n        log_parse_regex = input_detail.get('regex', '')\n        reg_keys = input_detail['key']\n        topic_format = input_detail['topicFormat']\n        filter_keys = input_detail['filterKey']\n        filter_keys_reg = input_detail['filterRegex']\n        log_type = input_detail.get('logType')\n\n        for item in ('logPath', 'filePattern', 'timeFormat', 'logBeginRegex', 'regex', 'key',\n                     'topicFormat', 'filterKey', 'filterRegex', 'logType'):\n            if item in input_detail:\n                del input_detail[item]\n\n        config = CommonRegLogConfigDetail(config_name, logstore_name, endpoint, log_path, file_pattern, time_format,\n                                          log_begin_regex, log_parse_regex, reg_keys,\n                                          topic_format, filter_keys, filter_keys_reg, logSample,\n                                          log_type, **input_detail)\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate apsara logtail config from loaded json value", "response": "def generate_apsara_log_config(json_value):\n        \"\"\"Generate apsara logtail config from loaded json value\n\n        :param json_value:\n        :return:\n        \"\"\"\n        input_detail = json_value['inputDetail']\n        output_detail = json_value['outputDetail']\n        config_name = json_value['configName']\n        logSample = json_value.get('logSample', '')\n\n        logstore_name = output_detail['logstoreName']\n        endpoint = output_detail.get('endpoint', '')\n        log_path = input_detail['logPath']\n        file_pattern = input_detail['filePattern']\n\n        log_begin_regex = input_detail.get('logBeginRegex', '')\n        topic_format = input_detail['topicFormat']\n        filter_keys = input_detail['filterKey']\n        filter_keys_reg = input_detail['filterRegex']\n\n        config = ApsaraLogConfigDetail(config_name, logstore_name, endpoint, log_path, file_pattern,\n                                       log_begin_regex, topic_format, filter_keys, filter_keys_reg, logSample)\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_logtail_config(json_value):\n        logger.warning(\"aliyun.log.LogtailConfigHelper is deprecated and will be removed in future version.\"\n                      \"Use LogtailConfigGenerator instead\")\n\n        if json_value['inputDetail']['logType'] == 'apsara_log':\n            return LogtailConfigHelper.generate_apsara_log_config(json_value)\n        return LogtailConfigHelper.generate_common_reg_log_config(json_value)", "response": "Generate logtail config from loaded json value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to get request as fast as possible, once empty and stop falg or time-out, just return Empty", "response": "def _get_batch_requests(self, timeout=None):\n        \"\"\"try to get request as fast as possible, once empty and stop falg or time-out, just return Empty\"\"\"\n        reqs = []\n        s = time()\n        while len(reqs) < self.batch_size and (time() - s) < timeout:\n            try:\n                req = self.queue.get(block=False)\n                self.queue.task_done()\n\n                reqs.append(req)\n            except Empty as ex:\n                if self.stop_flag:\n                    break\n                else:\n                    sleep(0.1)\n\n        if not reqs:\n            raise Empty\n        elif len(reqs) <= 1:\n            return reqs[0]\n        else:\n            logitems = []\n            req = reqs[0]\n            for req in reqs:\n                logitems.extend(req.get_log_items())\n\n            ret = PutLogsRequest(self.project, self.log_store, req.topic, logitems=logitems)\n            ret.__record__ = req.__record__\n\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the previous task is done and proceed to fire another task", "response": "def check_and_generate_next_task(self):\n        \"\"\"\n        check if the previous task is done and proceed to fire another task\n        :return:\n        \"\"\"\n        # if self.task_future is None:\n        #     # there's no any ongoing task\n        #     self._update_status(False)\n        #     self.generate_next_task()\n        #     return\n\n        if self.task_future is None or self.task_future.done():\n            task_success = False\n\n            task_result = self.get_task_result(self.task_future)\n            self.task_future = None\n\n            if task_result is not None and task_result.get_exception() is None:\n\n                task_success = True\n                if isinstance(task_result, InitTaskResult):\n                    # maintain check points\n                    assert self.consumer_status == ConsumerStatus.INITIALIZING, \\\n                        ClientWorkerException(\"get init task result, but status is: \" + str(self.consumer_status))\n\n                    init_result = task_result\n                    self.next_fetch_cursor = init_result.get_cursor()\n                    self.fetch_end_cursor = init_result.end_cursor\n                    self.checkpoint_tracker.set_memory_check_point(self.next_fetch_cursor)\n                    if init_result.is_cursor_persistent():\n                        self.checkpoint_tracker.set_persistent_check_point(self.next_fetch_cursor)\n\n                elif isinstance(task_result, ProcessTaskResult):\n                    # maintain check points\n                    process_task_result = task_result\n                    roll_back_checkpoint = process_task_result.get_rollback_check_point()\n                    if roll_back_checkpoint:\n                        self.last_fetch_log_group = None\n                        self.logger.info(\"user defined to roll-back check-point, cancel current fetching task\")\n                        self.cancel_current_fetch()\n                        self.next_fetch_cursor = roll_back_checkpoint\n\n            # log task status\n            self._sample_log_error(task_result)\n\n            # update status basing on task results\n            self._update_status(task_success)\n\n            #\n            self._generate_next_task()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates next task for this consumer", "response": "def _generate_next_task(self):\n        \"\"\"\n        submit consumer framework defined task\n        :return:\n        \"\"\"\n        if self.consumer_status == ConsumerStatus.INITIALIZING:\n            self.current_task_exist = True\n            self.task_future = self.executor.submit(consumer_initialize_task, self.processor, self.log_client,\n                                                    self.shard_id, self.cursor_position, self.cursor_start_time, self.cursor_end_time)\n\n        elif self.consumer_status == ConsumerStatus.PROCESSING:\n            if self.last_fetch_log_group is not None:\n                self.checkpoint_tracker.set_cursor(self.last_fetch_log_group.end_cursor)\n                self.current_task_exist = True\n\n                # must deep copy cause some revision will happen\n                last_fetch_log_group = copy.deepcopy(self.last_fetch_log_group)\n                self.last_fetch_log_group = None\n\n                if self.last_fetch_count > 0:\n                    self.task_future = self.executor.submit(consumer_process_task, self.processor,\n                                                            last_fetch_log_group.fetched_log_group_list,\n                                                            self.checkpoint_tracker)\n\n        elif self.consumer_status == ConsumerStatus.SHUTTING_DOWN:\n            self.current_task_exist = True\n            self.logger.info(\"start to cancel fetch job\")\n            self.cancel_current_fetch()\n            self.task_future = self.executor.submit(consumer_shutdown_task, self.processor, self.checkpoint_tracker)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if IPython has widgets installed and if so return True if so False if not", "response": "def can_use_widgets():\n    \"\"\" Expanded from from http://stackoverflow.com/a/34092072/1958900\n    \"\"\"\n    if 'IPython' not in sys.modules:\n        # IPython hasn't been imported, definitely not\n        return False\n    from IPython import get_ipython\n\n    # check for `kernel` attribute on the IPython instance\n    if getattr(get_ipython(), 'kernel', None) is None:\n        return False\n\n    try:\n        import ipywidgets as ipy\n        import traitlets\n    except ImportError:\n        return False\n\n    if int(ipy.__version__.split('.')[0]) < 6:\n        print('WARNING: widgets require ipywidgets 6.0 or later')\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nenable __call__ to accept event_list.", "response": "def support_event_list_simple(fn):\n    \"\"\"\n    enable __call__ to accept event_list.\n    :param fn:\n    :return:\n    \"\"\"\n    @wraps(fn)\n    def _wrapped(self, event, *args, **kwargs):\n        if _is_event_list(event):\n            result = []\n            for e in event:\n                ret = fn(self, e, *args, **kwargs)\n\n                if ret is None:\n                    continue\n\n                if isinstance(ret, (tuple, list)):\n                    result.extend(ret)\n                else:\n                    result.append(ret)\n\n            if result:\n                if len(result) == 1:\n                    return result[0]\n                return result\n            return None\n        else:\n            return fn(self, event, *args, **kwargs)\n\n    return _wrapped"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts string container or unicode to unicode", "response": "def u(d):\n    \"\"\"\n    convert string, string container or unicode\n    :param d:\n    :return:\n    \"\"\"\n    if six.PY2:\n        if isinstance(d, six.binary_type):\n            return d.decode(\"utf8\", \"ignore\")\n        elif isinstance(d, list):\n            return [u(x) for x in d]\n        elif isinstance(d, tuple):\n            return tuple(u(x) for x in d)\n        elif isinstance(d, dict):\n            return dict( (u(k), u(v)) for k, v in six.iteritems(d))\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_cn_keys_patch():\r\n    import sys\r\n    if sys.version_info[:2] == (3, 5):\r\n        import http.client as hc\r\n        old_parse = hc.parse_headers\r\n\r\n        def parse_header(*args, **kwargs):\r\n            fp = args[0]\r\n            old_readline = fp.readline\r\n\r\n            def new_readline(*args, **kwargs):\r\n                ret = old_readline(*args, **kwargs)\r\n                if ret.lower().startswith(b'x-log-query-info'):\r\n                    return b'x-log-query-info: \\r\\n'\r\n                return ret\r\n\r\n            fp.readline = new_readline\r\n\r\n            ret = old_parse(*args, **kwargs)\r\n            return ret\r\n\r\n        hc.parse_headers = parse_header", "response": "This patch applies the appropriate patch to the list of items in the\r\nGrownCalendar."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put_log_raw(self, project, logstore, log_group, compress=None):\r\n        body = log_group.SerializeToString()\r\n        raw_body_size = len(body)\r\n        headers = {'x-log-bodyrawsize': str(raw_body_size), 'Content-Type': 'application/x-protobuf'}\r\n\r\n        if compress is None or compress:\r\n            if lz4:\r\n                headers['x-log-compresstype'] = 'lz4'\r\n                body = lz_compresss(body)\r\n            else:\r\n                headers['x-log-compresstype'] = 'deflate'\r\n                body = zlib.compress(body)\r\n\r\n        params = {}\r\n        resource = '/logstores/' + logstore + \"/shards/lb\"\r\n\r\n        (resp, header) = self._send('POST', project, body, resource, params, headers)\r\n\r\n        return PutLogsResponse(header, resp)", "response": "Put logs to log service. using raw data in protobuf format"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nputs logs to log service. up to 512000 logs up to 10MB size Unsuccessful opertaion will cause an LogException. :type request: PutLogsRequest :param request: the PutLogs request parameters class :return: PutLogsResponse :raise: LogException", "response": "def put_logs(self, request):\r\n        \"\"\" Put logs to log service. up to 512000 logs up to 10MB size\r\n        Unsuccessful opertaion will cause an LogException.\r\n        \r\n        :type request: PutLogsRequest\r\n        :param request: the PutLogs request parameters class\r\n        \r\n        :return: PutLogsResponse\r\n\r\n        :raise: LogException\r\n        \"\"\"\r\n        if len(request.get_log_items()) > 512000:\r\n            raise LogException('InvalidLogSize',\r\n                               \"logItems' length exceeds maximum limitation: 512000 lines. now: {0}\".format(\r\n                                   len(request.get_log_items())))\r\n        logGroup = LogGroup()\r\n        logGroup.Topic = request.get_topic()\r\n        if request.get_source():\r\n            logGroup.Source = request.get_source()\r\n        else:\r\n            if self._source == '127.0.0.1':\r\n                self._source = Util.get_host_ip(request.get_project() + '.' + self._logHost)\r\n            logGroup.Source = self._source\r\n        for logItem in request.get_log_items():\r\n            log = logGroup.Logs.add()\r\n            log.Time = logItem.get_time()\r\n            contents = logItem.get_contents()\r\n            for key, value in contents:\r\n                content = log.Contents.add()\r\n                content.Key = self._get_unicode(key)\r\n                content.Value = self._get_binary(value)\r\n        if request.get_log_tags() is not None:\r\n            tags = request.get_log_tags()\r\n            for key, value in tags:\r\n                pb_tag = logGroup.LogTags.add()\r\n                pb_tag.Key = key\r\n                pb_tag.Value = value\r\n        body = logGroup.SerializeToString()\r\n\r\n        if len(body) > 10 * 1024 * 1024:  # 10 MB\r\n            raise LogException('InvalidLogSize',\r\n                               \"logItems' size exceeds maximum limitation: 10 MB. now: {0} MB.\".format(\r\n                                   len(body) / 1024.0 / 1024))\r\n\r\n        headers = {'x-log-bodyrawsize': str(len(body)), 'Content-Type': 'application/x-protobuf'}\r\n        is_compress = request.get_compress()\r\n\r\n        compress_data = None\r\n        if is_compress:\r\n            if lz4:\r\n                headers['x-log-compresstype'] = 'lz4'\r\n                compress_data = lz_compresss(body)\r\n            else:\r\n                headers['x-log-compresstype'] = 'deflate'\r\n                compress_data = zlib.compress(body)\r\n\r\n        params = {}\r\n        logstore = request.get_logstore()\r\n        project = request.get_project()\r\n        if request.get_hash_key() is not None:\r\n            resource = '/logstores/' + logstore + \"/shards/route\"\r\n            params[\"key\"] = request.get_hash_key()\r\n        else:\r\n            resource = '/logstores/' + logstore + \"/shards/lb\"\r\n\r\n        if is_compress:\r\n            (resp, header) = self._send('POST', project, compress_data, resource, params, headers)\r\n        else:\r\n            (resp, header) = self._send('POST', project, body, resource, params, headers)\r\n\r\n        return PutLogsResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all logstores in the specified project.", "response": "def list_logstores(self, request):\r\n        \"\"\" List all logstores of requested project.\r\n        Unsuccessful opertaion will cause an LogException.\r\n        \r\n        :type request: ListLogstoresRequest\r\n        :param request: the ListLogstores request parameters class.\r\n        \r\n        :return: ListLogStoresResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        resource = '/logstores'\r\n        project = request.get_project()\r\n        (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n        return ListLogstoresResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all topics in a logstore.", "response": "def list_topics(self, request):\r\n        \"\"\" List all topics in a logstore.\r\n        Unsuccessful opertaion will cause an LogException.\r\n        \r\n        :type request: ListTopicsRequest\r\n        :param request: the ListTopics request parameters class.\r\n        \r\n        :return: ListTopicsResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        if request.get_token() is not None:\r\n            params['token'] = request.get_token()\r\n        if request.get_line() is not None:\r\n            params['line'] = request.get_line()\r\n        params['type'] = 'topic'\r\n        logstore = request.get_logstore()\r\n        project = request.get_project()\r\n        resource = \"/logstores/\" + logstore\r\n        (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n\r\n        return ListTopicsResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_histograms(self, request):\r\n        headers = {}\r\n        params = {}\r\n        if request.get_topic() is not None:\r\n            params['topic'] = request.get_topic()\r\n        if request.get_from() is not None:\r\n            params['from'] = request.get_from()\r\n        if request.get_to() is not None:\r\n            params['to'] = request.get_to()\r\n        if request.get_query() is not None:\r\n            params['query'] = request.get_query()\r\n        params['type'] = 'histogram'\r\n        logstore = request.get_logstore()\r\n        project = request.get_project()\r\n        resource = \"/logstores/\" + logstore\r\n        (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n        return GetHistogramsResponse(resp, header)", "response": "Returns a list of histograms that match the specified request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of logs from a given topic and logstore.", "response": "def get_log(self, project, logstore, from_time, to_time, topic=None,\r\n                query=None, reverse=False, offset=0, size=100):\r\n        \"\"\" Get logs from log service. will retry when incomplete.\r\n        Unsuccessful opertaion will cause an LogException.\r\n        Note: for larger volume of data (e.g. > 1 million logs), use get_log_all\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type from_time: int/string\r\n        :param from_time: the begin timestamp or format of time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type to_time: int/string\r\n        :param to_time: the end timestamp or format of time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type topic: string\r\n        :param topic: topic name of logs, could be None\r\n\r\n        :type query: string\r\n        :param query: user defined query, could be None\r\n\r\n        :type reverse: bool\r\n        :param reverse: if reverse is set to true, the query will return the latest logs first, default is false\r\n\r\n        :type offset: int\r\n        :param offset: line offset of return logs\r\n\r\n        :type size: int\r\n        :param size: max line number of return logs, -1 means get all\r\n\r\n        :return: GetLogsResponse\r\n\r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        # need to use extended method to get more when: it's not select query, and size > default page size\r\n        if not is_stats_query(query) and (int(size) == -1 or int(size) > MAX_GET_LOG_PAGING_SIZE):\r\n            return query_more(self.get_log, int(offset), int(size), MAX_GET_LOG_PAGING_SIZE,\r\n                              project, logstore, from_time, to_time, topic,\r\n                              query, reverse)\r\n\r\n        ret = None\r\n        for _c in range(DEFAULT_QUERY_RETRY_COUNT):\r\n            headers = {}\r\n            params = {'from': parse_timestamp(from_time),\r\n                      'to': parse_timestamp(to_time),\r\n                      'type': 'log',\r\n                      'line': size,\r\n                      'offset': offset,\r\n                      'reverse': 'true' if reverse else 'false'}\r\n\r\n            if topic:\r\n                params['topic'] = topic\r\n            if query:\r\n                params['query'] = query\r\n\r\n            resource = \"/logstores/\" + logstore\r\n            (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n            ret = GetLogsResponse(resp, header)\r\n            if ret.is_completed():\r\n                break\r\n\r\n            time.sleep(DEFAULT_QUERY_RETRY_INTERVAL)\r\n\r\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_logs(self, request):\r\n        project = request.get_project()\r\n        logstore = request.get_logstore()\r\n        from_time = request.get_from()\r\n        to_time = request.get_to()\r\n        topic = request.get_topic()\r\n        query = request.get_query()\r\n        reverse = request.get_reverse()\r\n        offset = request.get_offset()\r\n        size = request.get_line()\r\n\r\n        return self.get_log(project, logstore, from_time, to_time, topic,\r\n                            query, reverse, offset, size)", "response": "Returns a list of logs from the specified log service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_log_all(self, project, logstore, from_time, to_time, topic=None,\r\n                    query=None, reverse=False, offset=0):\r\n        \"\"\" Get logs from log service. will retry when incomplete.\r\n        Unsuccessful opertaion will cause an LogException. different with `get_log` with size=-1,\r\n        It will try to iteratively fetch all data every 100 items and yield them, in CLI, it could apply jmes filter to\r\n        each batch and make it possible to fetch larger volume of data.\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type from_time: int/string\r\n        :param from_time: the begin timestamp or format of time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type to_time: int/string\r\n        :param to_time: the end timestamp or format of time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type topic: string\r\n        :param topic: topic name of logs, could be None\r\n\r\n        :type query: string\r\n        :param query: user defined query, could be None\r\n\r\n        :type reverse: bool\r\n        :param reverse: if reverse is set to true, the query will return the latest logs first, default is false\r\n\r\n        :type offset: int\r\n        :param offset: offset to start, by default is 0\r\n\r\n        :return: GetLogsResponse iterator\r\n\r\n        :raise: LogException\r\n        \"\"\"\r\n        while True:\r\n            response = self.get_log(project, logstore, from_time, to_time, topic=topic,\r\n                                    query=query, reverse=reverse, offset=offset, size=100)\r\n\r\n            yield response\r\n\r\n            count = response.get_count()\r\n            offset += count\r\n\r\n            if count == 0 or is_stats_query(query):\r\n                break", "response": "Gets all logs for a given topic and time range"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of logs from the specified project.", "response": "def get_project_logs(self, request):\r\n        \"\"\" Get logs from log service.\r\n        Unsuccessful opertaion will cause an LogException.\r\n        \r\n        :type request: GetProjectLogsRequest\r\n        :param request: the GetProjectLogs request parameters class.\r\n        \r\n        :return: GetLogsResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        if request.get_query() is not None:\r\n            params['query'] = request.get_query()\r\n        project = request.get_project()\r\n        resource = \"/logs\"\r\n        (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n        return GetLogsResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a cursor from the log service for batch pull logs", "response": "def get_cursor(self, project_name, logstore_name, shard_id, start_time):\r\n        \"\"\" Get cursor from log service for batch pull logs\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type shard_id: int\r\n        :param shard_id: the shard id\r\n\r\n        :type start_time: string/int\r\n        :param start_time: the start time of cursor, e.g 1441093445 or \"begin\"/\"end\", or readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :return: GetCursorResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        headers = {'Content-Type': 'application/json'}\r\n        params = {'type': 'cursor',\r\n                  'from': str(start_time) if start_time in (\"begin\", \"end\") else parse_timestamp(start_time)}\r\n\r\n        resource = \"/logstores/\" + logstore_name + \"/shards/\" + str(shard_id)\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetCursorResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the time that a cursor was started for a particular logstore and shard.", "response": "def get_cursor_time(self, project_name, logstore_name, shard_id, cursor):\r\n        \"\"\" Get cursor time from log service\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type shard_id: int\r\n        :param shard_id: the shard id\r\n\r\n        :type cursor: string\r\n        :param cursor: the cursor to get its service receive time\r\n\r\n        :return: GetCursorTimeResponse\r\n\r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        headers = {'Content-Type': 'application/json'}\r\n        params = {'type': 'cursor_time', 'cursor': cursor}\r\n        resource = \"/logstores/\" + logstore_name + \"/shards/\" + str(shard_id)\r\n\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetCursorTimeResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the previous cursor time from the log service.", "response": "def get_previous_cursor_time(self, project_name, logstore_name, shard_id, cursor, normalize=True):\r\n        \"\"\" Get previous cursor time from log service.\r\n        Note: normalize = true: if the cursor is out of range, it will be nornalized to nearest cursor\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name\r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type shard_id: int\r\n        :param shard_id: the shard id\r\n\r\n        :type cursor: string\r\n        :param cursor: the cursor to get its service receive time\r\n\r\n        :type normalize: bool\r\n        :param normalize: fix the cursor or not if it's out of scope\r\n\r\n        :return: GetCursorTimeResponse\r\n\r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        try:\r\n            pre_cursor_int = self._get_cursor_as_int(cursor) - 1\r\n            pre_cursor = e64(str(pre_cursor_int)).strip()\r\n        except Exception:\r\n            raise LogException(\"InvalidCursor\", \"Cursor {0} is invalid\".format(cursor))\r\n\r\n        try:\r\n            return self.get_cursor_time(project_name, logstore_name, shard_id, pre_cursor)\r\n        except LogException as ex:\r\n            if normalize and ex.get_error_code() == \"InvalidCursor\":\r\n                ret = self.get_begin_cursor(project_name, logstore_name, shard_id)\r\n                begin_cursor_int = self._get_cursor_as_int(ret.get_cursor())\r\n\r\n                if pre_cursor_int < begin_cursor_int:\r\n                    return self.get_cursor_time(project_name, logstore_name, shard_id, e64(str(begin_cursor_int)))\r\n\r\n                ret = self.get_end_cursor(project_name, logstore_name, shard_id)\r\n                end_cursor_int = self._get_cursor_as_int(ret.get_cursor())\r\n\r\n                if pre_cursor_int > end_cursor_int:\r\n                    return self.get_cursor_time(project_name, logstore_name, shard_id, e64(str(end_cursor_int)))\r\n\r\n            raise ex"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_begin_cursor(self, project_name, logstore_name, shard_id):\r\n        return self.get_cursor(project_name, logstore_name, shard_id, \"begin\")", "response": "Get the begin cursor from the log service for batch pull logs\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the end cursor from the log service for batch pull logs", "response": "def get_end_cursor(self, project_name, logstore_name, shard_id):\r\n        \"\"\" Get end cursor from log service for batch pull logs\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type shard_id: int\r\n        :param shard_id: the shard id\r\n\r\n        :return: GetLogsResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        return self.get_cursor(project_name, logstore_name, shard_id, \"end\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbatching pull log data from log service Unsuccessful opertaion will cause an LogException. :type project_name: string :param project_name: the Project name :type logstore_name: string :param logstore_name: the logstore name :type shard_id: int :param shard_id: the shard id :type cursor: string :param cursor: the start to cursor to get data :type count: int :param count: the required pull log package count, default 1000 packages :type end_cursor: string :param end_cursor: the end cursor position to get data :type compress: boolean :param compress: if use zip compress for transfer data, default is True :return: PullLogResponse :raise: LogException", "response": "def pull_logs(self, project_name, logstore_name, shard_id, cursor, count=None, end_cursor=None, compress=None):\r\n        \"\"\" batch pull log data from log service\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type shard_id: int\r\n        :param shard_id: the shard id\r\n\r\n        :type cursor: string\r\n        :param cursor: the start to cursor to get data\r\n\r\n        :type count: int\r\n        :param count: the required pull log package count, default 1000 packages\r\n\r\n        :type end_cursor: string\r\n        :param end_cursor: the end cursor position to get data\r\n\r\n        :type compress: boolean\r\n        :param compress: if use zip compress for transfer data, default is True\r\n\r\n        :return: PullLogResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        headers = {}\r\n        if compress is None or compress:\r\n            if lz4:\r\n                headers['Accept-Encoding'] = 'lz4'\r\n            else:\r\n                headers['Accept-Encoding'] = 'gzip'\r\n        else:\r\n            headers['Accept-Encoding'] = ''\r\n\r\n        headers['Accept'] = 'application/x-protobuf'\r\n\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name + \"/shards/\" + str(shard_id)\r\n        params['type'] = 'log'\r\n        params['cursor'] = cursor\r\n        count = count or 1000\r\n        params['count'] = str(count)\r\n        if end_cursor:\r\n            params['end_cursor'] = end_cursor\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers, \"binary\")\r\n\r\n        compress_type = Util.h_v_td(header, 'x-log-compresstype', '').lower()\r\n        if compress_type == 'lz4':\r\n            raw_size = int(Util.h_v_t(header, 'x-log-bodyrawsize'))\r\n            if lz4:\r\n                raw_data = lz_decompress(raw_size, resp)\r\n                return PullLogResponse(raw_data, header)\r\n            else:\r\n                raise LogException(\"ClientHasNoLz4\", \"There's no Lz4 lib available to decompress the response\", resp_header=header, resp_body=resp)\r\n        elif compress_type in ('gzip', 'deflate'):\r\n            raw_size = int(Util.h_v_t(header, 'x-log-bodyrawsize'))\r\n            raw_data = zlib.decompress(resp)\r\n            return PullLogResponse(raw_data, header)\r\n        else:\r\n            return PullLogResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pull_log(self, project_name, logstore_name, shard_id, from_time, to_time, batch_size=None, compress=None):\r\n        begin_cursor = self.get_cursor(project_name, logstore_name, shard_id, from_time).get_cursor()\r\n        end_cursor = self.get_cursor(project_name, logstore_name, shard_id, to_time).get_cursor()\r\n\r\n        while True:\r\n            res = self.pull_logs(project_name, logstore_name, shard_id, begin_cursor,\r\n                                 count=batch_size, end_cursor=end_cursor, compress=compress)\r\n\r\n            yield res\r\n            if res.get_log_count() <= 0:\r\n                break\r\n\r\n            begin_cursor = res.get_next_cursor()", "response": "This method is used to pull log data from a logstore shard using time - range\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndumping all logs seperatedly line into file_path", "response": "def pull_log_dump(self, project_name, logstore_name, from_time, to_time, file_path, batch_size=None,\r\n                      compress=None, encodings=None, shard_list=None, no_escape=None):\r\n        \"\"\" dump all logs seperatedly line into file_path, file_path, the time parameters are log received time on server side.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name\r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type from_time: string/int\r\n        :param from_time: curosr value, could be begin, timestamp or readable time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type to_time: string/int\r\n        :param to_time: curosr value, could be begin, timestamp or readable time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type file_path: string\r\n        :param file_path: file path with {} for shard id. e.g. \"/data/dump_{}.data\", {} will be replaced with each partition.\r\n\r\n        :type batch_size: int\r\n        :param batch_size: batch size to fetch the data in each iteration. by default it's 500\r\n\r\n        :type compress: bool\r\n        :param compress: if use compression, by default it's True\r\n\r\n        :type encodings: string list\r\n        :param encodings: encoding like [\"utf8\", \"latin1\"] etc to dumps the logs in json format to file. default is [\"utf8\",]\r\n\r\n        :type shard_list: string\r\n        :param shard_list: shard number list. could be comma seperated list or range: 1,20,31-40\r\n\r\n        :type no_escape: bool\r\n        :param no_escape: if not_escape the non-ANSI, default is to escape, set it to True if don't want it.\r\n\r\n        :return: LogResponse {\"total_count\": 30, \"files\": {'file_path_1': 10, \"file_path_2\": 20} })\r\n\r\n        :raise: LogException\r\n        \"\"\"\r\n        file_path = file_path.replace(\"{}\", \"{0}\")\r\n        if \"{0}\" not in file_path:\r\n            file_path += \"{0}\"\r\n\r\n        return pull_log_dump(self, project_name, logstore_name, from_time, to_time, file_path,\r\n                             batch_size=batch_size, compress=compress, encodings=encodings,\r\n                             shard_list=shard_list, no_escape=no_escape)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new logstore.", "response": "def create_logstore(self, project_name, logstore_name,\r\n                        ttl=30,\r\n                        shard_count=2,\r\n                        enable_tracking=False,\r\n                        append_meta=False,\r\n                        auto_split=True,\r\n                        max_split_shard=64,\r\n                        preserve_storage=False\r\n                        ):\r\n        \"\"\" create log store \r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type ttl: int\r\n        :param ttl: the life cycle of log in the logstore in days, default 30, up to 3650\r\n\r\n        :type shard_count: int\r\n        :param shard_count: the shard count of the logstore to create, default 2\r\n\r\n        :type enable_tracking: bool\r\n        :param enable_tracking: enable web tracking, default is False\r\n\r\n        :type append_meta: bool\r\n        :param append_meta: allow to append meta info (server received time and IP for external IP to each received log)\r\n\r\n        :type auto_split: bool\r\n        :param auto_split: auto split shard, max_split_shard will be 64 by default is True\r\n\r\n        :type max_split_shard: int\r\n        :param max_split_shard: max shard to split, up to 64\r\n\r\n        :type preserve_storage: bool\r\n        :param preserve_storage: if always persist data, TTL will be ignored.\r\n\r\n        :return: CreateLogStoreResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        if auto_split and (max_split_shard <= 0 or max_split_shard >= 64):\r\n            max_split_shard = 64\r\n        if preserve_storage:\r\n            ttl = 3650\r\n\r\n        params = {}\r\n        resource = \"/logstores\"\r\n        headers = {\"x-log-bodyrawsize\": '0', \"Content-Type\": \"application/json\"}\r\n        body = {\"logstoreName\": logstore_name, \"ttl\": int(ttl), \"shardCount\": int(shard_count),\r\n                \"enable_tracking\": enable_tracking,\r\n                \"autoSplit\": auto_split,\r\n                \"maxSplitShard\": max_split_shard,\r\n                \"appendMeta\": append_meta\r\n                }\r\n\r\n        body_str = six.b(json.dumps(body))\r\n\r\n        try:\r\n            (resp, header) = self._send(\"POST\", project_name, body_str, resource, params, headers)\r\n        except LogException as ex:\r\n            if ex.get_error_code() == \"LogStoreInfoInvalid\" and ex.get_error_message() == \"redundant key exist in json\":\r\n                logger.warning(\"LogStoreInfoInvalid, will retry with basic parameters. detail: {0}\".format(ex))\r\n                body = {\"logstoreName\": logstore_name, \"ttl\": int(ttl), \"shardCount\": int(shard_count),\r\n                        \"enable_tracking\": enable_tracking }\r\n\r\n                body_str = six.b(json.dumps(body))\r\n\r\n                (resp, header) = self._send(\"POST\", project_name, body_str, resource, params, headers)\r\n            else:\r\n                raise\r\n\r\n        return CreateLogStoreResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_logstore(self, project_name, logstore_name):\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name\r\n        (resp, header) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteLogStoreResponse(header, resp)", "response": "Delete the logstore with the given name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_logstore(self, project_name, logstore_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetLogStoreResponse(resp, header)", "response": "Get the logstore meta info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the logstore meta info", "response": "def update_logstore(self, project_name, logstore_name, ttl=None, enable_tracking=None, shard_count=None,\r\n                        append_meta=None,\r\n                        auto_split=None,\r\n                        max_split_shard=None,\r\n                        preserve_storage=None\r\n                        ):\r\n        \"\"\"\r\n        update the logstore meta info\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type ttl: int\r\n        :param ttl: the life cycle of log in the logstore in days\r\n\r\n        :type enable_tracking: bool\r\n        :param enable_tracking: enable web tracking\r\n\r\n        :type shard_count: int\r\n        :param shard_count: deprecated, the shard count could only be updated by split & merge\r\n\r\n        :type append_meta: bool\r\n        :param append_meta: allow to append meta info (server received time and IP for external IP to each received log)\r\n\r\n        :type auto_split: bool\r\n        :param auto_split: auto split shard, max_split_shard will be 64 by default is True\r\n\r\n        :type max_split_shard: int\r\n        :param max_split_shard: max shard to split, up to 64\r\n\r\n        :type preserve_storage: bool\r\n        :param preserve_storage: if always persist data, TTL will be ignored.\r\n\r\n        :return: UpdateLogStoreResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        res = self.get_logstore(project_name, logstore_name)\r\n        shard_count = res.get_shard_count()\r\n\r\n        if enable_tracking is None:\r\n            enable_tracking = res.get_enable_tracking()\r\n        if preserve_storage is None and ttl is None:\r\n            preserve_storage = res.preserve_storage\r\n        if ttl is None:\r\n            ttl = res.get_ttl()\r\n        if auto_split is None:\r\n            auto_split = res.auto_split\r\n        if append_meta is None:\r\n            append_meta = res.append_meta\r\n        if max_split_shard is None:\r\n            max_split_shard = res.max_split_shard\r\n\r\n        if auto_split and (max_split_shard <= 0 or max_split_shard >= 64):\r\n            max_split_shard = 64\r\n        if preserve_storage:\r\n            ttl = 3650\r\n\r\n        headers = {\"x-log-bodyrawsize\": '0', \"Content-Type\": \"application/json\"}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name\r\n        body = {\r\n            \"logstoreName\": logstore_name, \"ttl\": int(ttl), \"enable_tracking\": enable_tracking,\r\n            \"shardCount\": shard_count,\r\n            \"autoSplit\": auto_split,\r\n            \"maxSplitShard\": max_split_shard,\r\n            \"appendMeta\": append_meta\r\n        }\r\n        body_str = six.b(json.dumps(body))\r\n        try:\r\n            (resp, header) = self._send(\"PUT\", project_name, body_str, resource, params, headers)\r\n        except LogException as ex:\r\n            if ex.get_error_code() == \"LogStoreInfoInvalid\" and ex.get_error_message() == \"redundant key exist in json\":\r\n                logger.warning(\"LogStoreInfoInvalid, will retry with basic parameters. detail: {0}\".format(ex))\r\n                body = { \"logstoreName\": logstore_name, \"ttl\": int(ttl), \"enable_tracking\": enable_tracking,\r\n                         \"shardCount\": shard_count }\r\n\r\n                body_str = six.b(json.dumps(body))\r\n\r\n                (resp, header) = self._send(\"PUT\", project_name, body_str, resource, params, headers)\r\n            else:\r\n                raise\r\n\r\n        return UpdateLogStoreResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of the logstores in a projectListLogStoreResponse", "response": "def list_logstore(self, project_name, logstore_name_pattern=None, offset=0, size=100):\r\n        \"\"\" list the logstore in a projectListLogStoreResponse\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name\r\n\r\n        :type logstore_name_pattern: string\r\n        :param logstore_name_pattern: the sub name logstore, used for the server to return logstore names contain this sub name\r\n\r\n        :type offset: int\r\n        :param offset: the offset of all the matched names\r\n\r\n        :type size: int\r\n        :param size: the max return names count, -1 means all\r\n\r\n        :return: ListLogStoreResponse\r\n\r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        # need to use extended method to get more\r\n        if int(size) == -1 or int(size) > MAX_LIST_PAGING_SIZE:\r\n            return list_more(self.list_logstore, int(offset), int(size), MAX_LIST_PAGING_SIZE,\r\n                             project_name, logstore_name_pattern)\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores\"\r\n        if logstore_name_pattern is not None:\r\n            params['logstoreName'] = logstore_name_pattern\r\n        params['offset'] = str(offset)\r\n        params['size'] = str(size)\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return ListLogStoreResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an external store", "response": "def create_external_store(self, project_name, config):\r\n        \"\"\" create log store \r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type config : ExternalStoreConfig\r\n        :param config :external store config\r\n\r\n\r\n        :return: CreateExternalStoreResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        params = {}\r\n        resource = \"/externalstores\"\r\n        headers = {\"x-log-bodyrawsize\": '0', \"Content-Type\": \"application/json\"}\r\n\r\n        body_str = six.b(json.dumps(config.to_json()))\r\n\r\n        (resp, header) = self._send(\"POST\", project_name, body_str, resource, params, headers)\r\n        return CreateExternalStoreResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the specified log store", "response": "def delete_external_store(self, project_name, store_name):\r\n        \"\"\" delete log store\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type store_name: string\r\n        :param store_name: the external store name\r\n\r\n        :return: DeleteExternalStoreResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/externalstores/\" + store_name\r\n        (resp, header) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteExternalStoreResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the logstore meta info", "response": "def get_external_store(self, project_name, store_name):\r\n        \"\"\" get the logstore meta info\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type store_name: string\r\n        :param store_name: the logstore name\r\n\r\n        :return: GetLogStoreResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/externalstores/\" + store_name\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n\r\n        # add storeName if not existing\r\n        if 'externalStoreName' not in resp:\r\n            resp['externalStoreName'] = store_name\r\n\r\n        return GetExternalStoreResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_external_store(self, project_name, config):\r\n\r\n        headers = {\"x-log-bodyrawsize\": '0', \"Content-Type\": \"application/json\"}\r\n        params = {}\r\n        resource = \"/externalstores/\" + config.externalStoreName\r\n        body_str = six.b(json.dumps(config.to_json()))\r\n        (resp, header) = self._send(\"PUT\", project_name, body_str, resource, params, headers)\r\n        return UpdateExternalStoreResponse(header, resp)", "response": "Update the external store meta info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_external_store(self, project_name, external_store_name_pattern=None, offset=0, size=100):\r\n\r\n        # need to use extended method to get more\r\n        if int(size) == -1 or int(size) > MAX_LIST_PAGING_SIZE:\r\n            return list_more(self.list_external_store, int(offset), int(size), MAX_LIST_PAGING_SIZE,\r\n                             project_name, external_store_name_pattern)\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/externalstores\"\r\n        if external_store_name_pattern is not None:\r\n            params['externalStoreName'] = external_store_name_pattern\r\n        params['offset'] = str(offset)\r\n        params['size'] = str(size)\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return ListExternalStoreResponse(resp, header)", "response": "List the logstores in a project"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of the shards in a logstore", "response": "def list_shards(self, project_name, logstore_name):\r\n        \"\"\" list the shard meta of a logstore\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :return: ListShardResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name + \"/shards\"\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return ListShardResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_shard(self, project_name, logstore_name, shardId, split_hash):\r\n\r\n        headers = {}\r\n        params = {\"action\": \"split\", \"key\": split_hash}\r\n        resource = \"/logstores/\" + logstore_name + \"/shards/\" + str(shardId)\r\n        (resp, header) = self._send(\"POST\", project_name, None, resource, params, headers)\r\n        return ListShardResponse(resp, header)", "response": "split a readwrite shard into two shards"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_shard(self, project_name, logstore_name, shardId):\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name + \"/shards/\" + str(shardId)\r\n        (resp, header) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteShardResponse(header, resp)", "response": "Delete a readonly shard"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_index(self, project_name, logstore_name, index_detail):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name + \"/index\"\r\n        headers['Content-Type'] = 'application/json'\r\n        body = six.b(json.dumps(index_detail.to_json()))\r\n        headers['x-log-bodyrawsize'] = str(len(body))\r\n\r\n        (resp, header) = self._send(\"PUT\", project_name, body, resource, params, headers)\r\n        return UpdateIndexResponse(header, resp)", "response": "Update the index for a logstore"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_index(self, project_name, logstore_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name + \"/index\"\r\n        (resp, header) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteIndexResponse(header, resp)", "response": "Delete the index of a logstore"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_index_config(self, project_name, logstore_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore_name + \"/index\"\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetIndexResponse(resp, header)", "response": "Get the index config detail of a logstore"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_logtail_config(self, project_name, config_detail):\r\n\r\n        if config_detail.logstore_name:\r\n            # try to verify if the logstore exists or not.\r\n            self.get_logstore(project_name, config_detail.logstore_name)\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/configs\"\r\n        headers['Content-Type'] = 'application/json'\r\n        body = six.b(json.dumps(config_detail.to_json()))\r\n        headers['x-log-bodyrawsize'] = str(len(body))\r\n        (resp, headers) = self._send(\"POST\", project_name, body, resource, params, headers)\r\n        return CreateLogtailConfigResponse(headers, resp)", "response": "Create a logtail config in a project"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the logtail config in a project", "response": "def update_logtail_config(self, project_name, config_detail):\r\n        \"\"\" update logtail config in a project\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type config_detail: LogtailConfigGenerator or SeperatorFileConfigDetail or SimpleFileConfigDetail or FullRegFileConfigDetail or JsonFileConfigDetail or ApsaraFileConfigDetail or SyslogConfigDetail or CommonRegLogConfigDetail\r\n        :param config_detail: the logtail config detail info, use `LogtailConfigGenerator.from_json` to generate config: SeperatorFileConfigDetail or SimpleFileConfigDetail or FullRegFileConfigDetail or JsonFileConfigDetail or ApsaraFileConfigDetail or SyslogConfigDetail\r\n\r\n        :return: UpdateLogtailConfigResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/configs/\" + config_detail.config_name\r\n        headers['Content-Type'] = 'application/json'\r\n        body = six.b(json.dumps(config_detail.to_json()))\r\n        headers['x-log-bodyrawsize'] = str(len(body))\r\n        (resp, headers) = self._send(\"PUT\", project_name, body, resource, params, headers)\r\n        return UpdateLogtailConfigResponse(headers, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_logtail_config(self, project_name, config_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/configs/\" + config_name\r\n        (resp, headers) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteLogtailConfigResponse(headers, resp)", "response": "Delete a logtail config in a project"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the logtail config in a project", "response": "def get_logtail_config(self, project_name, config_name):\r\n        \"\"\" get logtail config in a project\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type config_name: string\r\n        :param config_name: the logtail config name\r\n\r\n        :return: GetLogtailConfigResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/configs/\" + config_name\r\n        (resp, headers) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetLogtailConfigResponse(resp, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_logtail_config(self, project_name, offset=0, size=100):\r\n        # need to use extended method to get more\r\n        if int(size) == -1 or int(size) > MAX_LIST_PAGING_SIZE:\r\n            return list_more(self.list_logtail_config, int(offset), int(size), MAX_LIST_PAGING_SIZE, project_name)\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/configs\"\r\n        params['offset'] = str(offset)\r\n        params['size'] = str(size)\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return ListLogtailConfigResponse(resp, header)", "response": "List logtail config names in a project"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new machine group in a project", "response": "def create_machine_group(self, project_name, group_detail):\r\n        \"\"\" create machine group in a project\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type group_detail: MachineGroupDetail\r\n        :param group_detail: the machine group detail config\r\n\r\n        :return: CreateMachineGroupResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups\"\r\n        headers['Content-Type'] = 'application/json'\r\n        body = six.b(json.dumps(group_detail.to_json()))\r\n        headers['x-log-bodyrawsize'] = str(len(body))\r\n        (resp, headers) = self._send(\"POST\", project_name, body, resource, params, headers)\r\n        return CreateMachineGroupResponse(headers, resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_machine_group(self, project_name, group_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_name\r\n        (resp, headers) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteMachineGroupResponse(headers, resp)", "response": "Delete a machine group in a project"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_machine_group(self, project_name, group_detail):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_detail.group_name\r\n        headers['Content-Type'] = 'application/json'\r\n        body = six.b(json.dumps(group_detail.to_json()))\r\n        headers['x-log-bodyrawsize'] = str(len(body))\r\n        (resp, headers) = self._send(\"PUT\", project_name, body, resource, params, headers)\r\n        return UpdateMachineGroupResponse(headers, resp)", "response": "Update a machine group in a project"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_machine_group(self, project_name, group_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_name\r\n        (resp, headers) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetMachineGroupResponse(resp, headers)", "response": "Get the latest set of attributes for a machine group"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_machine_group(self, project_name, offset=0, size=100):\r\n\r\n        # need to use extended method to get more\r\n        if int(size) == -1 or int(size) > MAX_LIST_PAGING_SIZE:\r\n            return list_more(self.list_machine_group, int(offset), int(size), MAX_LIST_PAGING_SIZE, project_name)\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups\"\r\n        params['offset'] = str(offset)\r\n        params['size'] = str(size)\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return ListMachineGroupResponse(resp, header)", "response": "List the names of all the machine groups in a project"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_machines(self, project_name, group_name, offset=0, size=100):\r\n\r\n        # need to use extended method to get more\r\n        if int(size) == -1 or int(size) > MAX_LIST_PAGING_SIZE:\r\n            return list_more(self.list_machines, int(offset), int(size), MAX_LIST_PAGING_SIZE, project_name, group_name)\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_name + \"/machines\"\r\n        params['offset'] = str(offset)\r\n        params['size'] = str(size)\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return ListMachinesResponse(resp, header)", "response": "List the machines in a machine group"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying a logtail config to a machine group", "response": "def apply_config_to_machine_group(self, project_name, config_name, group_name):\r\n        \"\"\" apply a logtail config to a machine group\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type config_name: string\r\n        :param config_name: the logtail config name to apply\r\n        \r\n        :type group_name: string\r\n        :param group_name: the machine group name \r\n\r\n        :return: ApplyConfigToMachineGroupResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_name + \"/configs/\" + config_name\r\n        (resp, header) = self._send(\"PUT\", project_name, None, resource, params, headers)\r\n        return ApplyConfigToMachineGroupResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a logtail config to a machine group", "response": "def remove_config_to_machine_group(self, project_name, config_name, group_name):\r\n        \"\"\" remove a logtail config to a machine group\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type config_name: string\r\n        :param config_name: the logtail config name to apply\r\n        \r\n        :type group_name: string\r\n        :param group_name: the machine group name \r\n\r\n        :return: RemoveConfigToMachineGroupResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_name + \"/configs/\" + config_name\r\n        (resp, header) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return RemoveConfigToMachineGroupResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_machine_group_applied_configs(self, project_name, group_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_name + \"/configs\"\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetMachineGroupAppliedConfigResponse(resp, header)", "response": "Get the logtail config names applied in a machine group"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_config_applied_machine_groups(self, project_name, config_name):\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/configs/\" + config_name + \"/machinegroups\"\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetConfigAppliedMachineGroupsResponse(resp, header)", "response": "Returns a list of machine group names where the logtail config applies to the given logtail config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all shipper tasks in a certain time range", "response": "def get_shipper_tasks(self, project_name, logstore_name, shipper_name, start_time, end_time, status_type='',\r\n                          offset=0, size=100):\r\n        \"\"\" get  odps/oss shipper tasks in a certain time range\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type shipper_name: string\r\n        :param shipper_name: the shipper name\r\n\r\n        :type start_time: int\r\n        :param start_time: the start timestamp\r\n\r\n        :type end_time: int\r\n        :param end_time: the end timestamp \r\n\r\n        :type status_type: string\r\n        :param status_type: support one of ['', 'fail', 'success', 'running'] , if the status_type = '' , return all kinds of status type\r\n\r\n        :type offset: int\r\n        :param offset: the begin task offset, -1 means all\r\n\r\n        :type size: int\r\n        :param size: the needed tasks count\r\n\r\n        :return: GetShipperTasksResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        # need to use extended method to get more\r\n        if int(size) == -1 or int(size) > MAX_LIST_PAGING_SIZE:\r\n            return list_more(self.get_shipper_tasks, int(offset), int(size), MAX_LIST_PAGING_SIZE,\r\n                             project_name, logstore_name, shipper_name, start_time, end_time, status_type)\r\n\r\n        headers = {}\r\n        params = {\"from\": str(int(start_time)),\r\n                  \"to\": str(int(end_time)),\r\n                  \"status\": status_type,\r\n                  \"offset\": str(int(offset)),\r\n                  \"size\": str(int(size))}\r\n\r\n        resource = \"/logstores/\" + logstore_name + \"/shipper/\" + shipper_name + \"/tasks\"\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetShipperTasksResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretries failed tasks , only the failed task can be retried Unsuccessful opertaion will cause an LogException. :type project_name: string :param project_name: the Project name :type logstore_name: string :param logstore_name: the logstore name :type shipper_name: string :param shipper_name: the shipper name :type task_list: string array :param task_list: the failed task_id list, e.g ['failed_task_id_1', 'failed_task_id_2',...], currently the max retry task count 10 every time :return: RetryShipperTasksResponse :raise: LogException", "response": "def retry_shipper_tasks(self, project_name, logstore_name, shipper_name, task_list):\r\n        \"\"\" retry failed tasks , only the failed task can be retried\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type logstore_name: string\r\n        :param logstore_name: the logstore name\r\n\r\n        :type shipper_name: string\r\n        :param shipper_name: the shipper name\r\n\r\n        :type task_list: string array\r\n        :param task_list: the failed task_id list, e.g ['failed_task_id_1', 'failed_task_id_2',...], currently the max retry task count 10 every time\r\n\r\n        :return: RetryShipperTasksResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\r\n        headers = {}\r\n        params = {}\r\n        body = six.b(json.dumps(task_list))\r\n        headers['Content-Type'] = 'application/json'\r\n        headers['x-log-bodyrawsize'] = str(len(body))\r\n        resource = \"/logstores/\" + logstore_name + \"/shipper/\" + shipper_name + \"/tasks\"\r\n\r\n        (resp, header) = self._send(\"PUT\", project_name, body, resource, params, headers)\r\n        return RetryShipperTasksResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_project(self, project_name, project_des):\r\n\r\n        params = {}\r\n        body = {\"projectName\": project_name, \"description\": project_des}\r\n\r\n        body = six.b(json.dumps(body))\r\n        headers = {'Content-Type': 'application/json', 'x-log-bodyrawsize': str(len(body))}\r\n        resource = \"/\"\r\n\r\n        (resp, header) = self._send(\"POST\", project_name, body, resource, params, headers)\r\n        return CreateProjectResponse(header, resp)", "response": "Create a new project"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_project(self, project_name):\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/\"\r\n\r\n        (resp, header) = self._send(\"GET\", project_name, None, resource, params, headers)\r\n        return GetProjectResponse(resp, header)", "response": "Get the project with the specified name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_project(self, project_name):\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/\"\r\n\r\n        (resp, header) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteProjectResponse(header, resp)", "response": "Delete the named project"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new consumer group", "response": "def create_consumer_group(self, project, logstore, consumer_group, timeout, in_order=False):\r\n        \"\"\" create consumer group\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type consumer_group: string\r\n        :param consumer_group: consumer group name\r\n\r\n        :type timeout: int\r\n        :param timeout: time-out\r\n\r\n        :type in_order: bool\r\n        :param in_order: if consume in order, default is False\r\n\r\n        :return: CreateConsumerGroupResponse\r\n        \"\"\"\r\n        request = CreateConsumerGroupRequest(project, logstore, ConsumerGroupEntity(consumer_group, timeout, in_order))\r\n        consumer_group = request.consumer_group\r\n        body_str = consumer_group.to_request_json()\r\n\r\n        headers = {\r\n            \"x-log-bodyrawsize\": '0',\r\n            \"Content-Type\": \"application/json\"\r\n        }\r\n        params = {}\r\n\r\n        project = request.get_project()\r\n        resource = \"/logstores/\" + request.get_logstore() + \"/consumergroups\"\r\n        (resp, header) = self._send(\"POST\", project, body_str, resource, params, headers)\r\n        return CreateConsumerGroupResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_consumer_group(self, project, logstore, consumer_group, timeout=None, in_order=None):\r\n        if in_order is None and timeout is None:\r\n            raise ValueError('in_order and timeout can\\'t all be None')\r\n        elif in_order is not None and timeout is not None:\r\n            body_dict = {\r\n                'order': in_order,\r\n                'timeout': timeout\r\n            }\r\n        elif in_order is not None:\r\n            body_dict = {\r\n                'order': in_order\r\n            }\r\n        else:\r\n            body_dict = {\r\n                'timeout': timeout\r\n            }\r\n        body_str = six.b(json.dumps(body_dict))\r\n\r\n        headers = {\r\n            \"x-log-bodyrawsize\": str(len(body_str)),\r\n            \"Content-Type\": \"application/json\"\r\n        }\r\n        params = {}\r\n        resource = \"/logstores/\" + logstore + \"/consumergroups/\" + consumer_group\r\n        (resp, header) = self._send(\"PUT\", project, body_str, resource, params, headers)\r\n        return UpdateConsumerGroupResponse(header, resp)", "response": "Update the fields of the specified consumer group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete the consumer group with the highest priority", "response": "def delete_consumer_group(self, project, logstore, consumer_group):\r\n        \"\"\" Delete consumer group\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type consumer_group: string\r\n        :param consumer_group: consumer group name\r\n\r\n        :return: None\r\n        \"\"\"\r\n\r\n        headers = {\"x-log-bodyrawsize\": '0'}\r\n        params = {}\r\n\r\n        resource = \"/logstores/\" + logstore + \"/consumergroups/\" + consumer_group\r\n        (resp, header) = self._send(\"DELETE\", project, None, resource, params, headers)\r\n        return DeleteConsumerGroupResponse(header, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_consumer_group(self, project, logstore):\r\n\r\n        resource = \"/logstores/\" + logstore + \"/consumergroups\"\r\n        params = {}\r\n        headers = {}\r\n\r\n        (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n        return ListConsumerGroupResponse(resp, header)", "response": "List consumer group\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :return: ListConsumerGroupResponse"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_check_point(self, project, logstore, consumer_group, shard, check_point,\r\n                           consumer='', force_success=True):\r\n        \"\"\" Update check point\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type consumer_group: string\r\n        :param consumer_group: consumer group name\r\n\r\n        :type shard: int\r\n        :param shard: shard id\r\n\r\n        :type check_point: string\r\n        :param check_point: checkpoint name\r\n\r\n        :type consumer: string\r\n        :param consumer: consumer name\r\n\r\n        :type force_success: bool\r\n        :param force_success: if force to succeed\r\n\r\n        :return: None\r\n        \"\"\"\r\n        request = ConsumerGroupUpdateCheckPointRequest(project, logstore, consumer_group,\r\n                                                       consumer, shard, check_point, force_success)\r\n        params = request.get_request_params()\r\n        body_str = request.get_request_body()\r\n        headers = {\"Content-Type\": \"application/json\"}\r\n        resource = \"/logstores/\" + logstore + \"/consumergroups/\" + consumer_group\r\n        (resp, header) = self._send(\"POST\", project, body_str, resource, params, headers)\r\n        return ConsumerGroupUpdateCheckPointResponse(header, resp)", "response": "Update the check point for a specific shard."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the latest check point for a given shard.", "response": "def get_check_point(self, project, logstore, consumer_group, shard=-1):\r\n        \"\"\" Get check point\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type consumer_group: string\r\n        :param consumer_group: consumer group name\r\n\r\n        :type shard: int\r\n        :param shard: shard id\r\n\r\n        :return: ConsumerGroupCheckPointResponse\r\n        \"\"\"\r\n        request = ConsumerGroupGetCheckPointRequest(project, logstore, consumer_group, shard)\r\n        params = request.get_params()\r\n        headers = {}\r\n        resource = \"/logstores/\" + logstore + \"/consumergroups/\" + consumer_group\r\n        (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n        return ConsumerGroupCheckPointResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a new check point for a particular shard.", "response": "def get_check_point_fixed(self, project, logstore, consumer_group, shard=-1):\r\n        \"\"\" Get check point\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type consumer_group: string\r\n        :param consumer_group: consumer group name\r\n\r\n        :type shard: int\r\n        :param shard: shard id\r\n\r\n        :return: ConsumerGroupCheckPointResponse\r\n        \"\"\"\r\n\r\n        res = self.get_check_point(project, logstore, consumer_group, shard)\r\n        res.check_checkpoint(self, project, logstore)\r\n\r\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a heartbeat request to the specified consumer group.", "response": "def heart_beat(self, project, logstore, consumer_group, consumer, shards=None):\r\n        \"\"\" Heatbeat consumer group\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type consumer_group: string\r\n        :param consumer_group: consumer group name\r\n\r\n        :type consumer: string\r\n        :param consumer: consumer name\r\n\r\n        :type shards: int list\r\n        :param shards: shard id list e.g. [0,1,2]\r\n\r\n        :return: None\r\n        \"\"\"\r\n        if shards is None:\r\n            shards = []\r\n        request = ConsumerGroupHeartBeatRequest(project, logstore, consumer_group, consumer, shards)\r\n        body_str = request.get_request_body()\r\n        params = request.get_params()\r\n        headers = {\"Content-Type\": \"application/json\"}\r\n        resource = \"/logstores/\" + logstore + \"/consumergroups/\" + consumer_group\r\n        (resp, header) = self._send('POST', project, body_str, resource, params, headers)\r\n        return ConsumerGroupHeartBeatResponse(resp, header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy logstore machine group and logtail config to target project", "response": "def copy_project(self, from_project, to_project, to_client=None, copy_machine_group=False):\r\n        \"\"\"\r\n        copy project, logstore, machine group and logtail config to target project,\r\n        expecting the target project doesn't contain same named logstores as source project\r\n\r\n        :type from_project: string\r\n        :param from_project: project name\r\n\r\n        :type to_project: string\r\n        :param to_project: project name\r\n\r\n        :type to_client: LogClient\r\n        :param to_client: logclient instance\r\n\r\n        :type copy_machine_group: bool\r\n        :param copy_machine_group: if copy machine group resources, False by default.\r\n\r\n        :return: None\r\n        \"\"\"\r\n        if to_client is None:\r\n            to_client = self\r\n        return copy_project(self, to_client, from_project, to_project, copy_machine_group)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy logstore entries from one logstore to another", "response": "def copy_logstore(self, from_project, from_logstore, to_logstore, to_project=None, to_client=None):\r\n        \"\"\"\r\n        copy logstore, index, logtail config to target logstore, machine group are not included yet.\r\n        the target logstore will be crated if not existing\r\n\r\n        :type from_project: string\r\n        :param from_project: project name\r\n\r\n        :type from_logstore: string\r\n        :param from_logstore: logstore name\r\n\r\n        :type to_logstore: string\r\n        :param to_logstore: target logstore name\r\n\r\n        :type to_project: string\r\n        :param to_project: target project name, copy to same project if not being specified, will try to create it if not being specified\r\n\r\n        :type to_client: LogClient\r\n        :param to_client: logclient instance, use it to operate on the \"to_project\" if being specified for cross region purpose\r\n\r\n        :return:\r\n        \"\"\"\r\n        return copy_logstore(self, from_project, from_logstore, to_logstore, to_project=to_project, to_client=to_client)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_project(self, offset=0, size=100):\r\n\r\n        # need to use extended method to get more\r\n        if int(size) == -1 or int(size) > MAX_LIST_PAGING_SIZE:\r\n            return list_more(self.list_project, int(offset), int(size), MAX_LIST_PAGING_SIZE)\r\n\r\n        headers = {}\r\n        params = {}\r\n        resource = \"/\"\r\n        params['offset'] = str(offset)\r\n        params['size'] = str(size)\r\n        (resp, header) = self._send(\"GET\", None, None, resource, params, headers)\r\n        return ListProjectResponse(resp, header)", "response": "List the project with the highest priority"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmigrates data from elasticsearch to aliyun log service :type hosts: string :param hosts: a comma-separated list of source ES nodes. e.g. \"localhost:9200,other_host:9200\" :type project_name: string :param project_name: specify the project_name of your log services. e.g. \"your_project\" :type indexes: string :param indexes: a comma-separated list of source index names. e.g. \"index1,index2\" :type query: string :param query: used to filter docs, so that you can specify the docs you want to migrate. e.g. '{\"query\": {\"match\": {\"title\": \"python\"}}}' :type scroll: string :param scroll: specify how long a consistent view of the index should be maintained for scrolled search. e.g. \"5m\" :type logstore_index_mappings: string :param logstore_index_mappings: specify the mappings of log service logstore and ES index. e.g. '{\"logstore1\": \"my_index*\", \"logstore2\": \"index1,index2\"}, \"logstore3\": \"index3\"}' :type pool_size: int :param pool_size: specify the size of process pool. e.g. 10 :type time_reference: string :param time_reference: specify what ES doc's field to use as log's time field. e.g. \"field1\" :type source: string :param source: specify the value of log's source field. e.g. \"your_source\" :type topic: string :param topic: specify the value of log's topic field. e.g. \"your_topic\" :type wait_time_in_secs: int :param wait_time_in_secs: specify the waiting time between initialize aliyun log and executing data migration task. e.g. 60 :type auto_creation: bool :param auto_creation: specify whether to let the tool create logstore and index automatically for you. e.g. True :return: MigrationResponse :raise: Exception", "response": "def es_migration(self, hosts,\r\n                     project_name,\r\n                     indexes=None,\r\n                     query=None,\r\n                     scroll=\"5m\",\r\n                     logstore_index_mappings=None,\r\n                     pool_size=10,\r\n                     time_reference=None,\r\n                     source=None,\r\n                     topic=None,\r\n                     wait_time_in_secs=60,\r\n                     auto_creation=True):\r\n        \"\"\"\r\n        migrate data from elasticsearch to aliyun log service\r\n\r\n        :type hosts: string\r\n        :param hosts: a comma-separated list of source ES nodes. e.g. \"localhost:9200,other_host:9200\"\r\n\r\n        :type project_name: string\r\n        :param project_name: specify the project_name of your log services. e.g. \"your_project\"\r\n\r\n        :type indexes: string\r\n        :param indexes: a comma-separated list of source index names. e.g. \"index1,index2\"\r\n\r\n        :type query: string\r\n        :param query: used to filter docs, so that you can specify the docs you want to migrate. e.g. '{\"query\": {\"match\": {\"title\": \"python\"}}}'\r\n\r\n        :type scroll: string\r\n        :param scroll: specify how long a consistent view of the index should be maintained for scrolled search. e.g. \"5m\"\r\n\r\n        :type logstore_index_mappings: string\r\n        :param logstore_index_mappings: specify the mappings of log service logstore and ES index. e.g. '{\"logstore1\": \"my_index*\", \"logstore2\": \"index1,index2\"}, \"logstore3\": \"index3\"}'\r\n\r\n        :type pool_size: int\r\n        :param pool_size: specify the size of process pool. e.g. 10\r\n\r\n        :type time_reference: string\r\n        :param time_reference: specify what ES doc's field to use as log's time field. e.g. \"field1\"\r\n\r\n        :type source: string\r\n        :param source: specify the value of log's source field. e.g. \"your_source\"\r\n\r\n        :type topic: string\r\n        :param topic: specify the value of log's topic field. e.g. \"your_topic\"\r\n\r\n        :type wait_time_in_secs: int\r\n        :param wait_time_in_secs: specify the waiting time between initialize aliyun log and executing data migration task. e.g. 60\r\n\r\n        :type auto_creation: bool\r\n        :param auto_creation: specify whether to let the tool create logstore and index automatically for you. e.g. True\r\n\r\n        :return: MigrationResponse\r\n\r\n        :raise: Exception\r\n        \"\"\"\r\n        from .es_migration import MigrationManager\r\n        from .es_migration import MigrationResponse\r\n\r\n        migration_manager = MigrationManager(hosts=hosts,\r\n                                             indexes=indexes,\r\n                                             query=query,\r\n                                             scroll=scroll,\r\n                                             endpoint=self._endpoint,\r\n                                             project_name=project_name,\r\n                                             access_key_id=self._accessKeyId,\r\n                                             access_key=self._accessKey,\r\n                                             logstore_index_mappings=logstore_index_mappings,\r\n                                             pool_size=pool_size,\r\n                                             time_reference=time_reference,\r\n                                             source=source,\r\n                                             topic=topic,\r\n                                             wait_time_in_secs=wait_time_in_secs,\r\n                                             auto_creation=auto_creation)\r\n        res = migration_manager.migrate()\r\n        resp = MigrationResponse()\r\n        resp.body = res\r\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies data from one logstore to another.", "response": "def copy_data(self, project, logstore, from_time, to_time=None,\r\n                  to_client=None, to_project=None, to_logstore=None,\r\n                  shard_list=None,\r\n                  batch_size=None, compress=None, new_topic=None, new_source=None):\r\n        \"\"\"\r\n        copy data from one logstore to another one (could be the same or in different region), the time is log received time on server side.\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type from_time: string/int\r\n        :param from_time: curosr value, could be begin, timestamp or readable time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type to_time: string/int\r\n        :param to_time: curosr value, default is \"end\", could be begin, timestamp or readable time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type to_client: LogClient\r\n        :param to_client: logclient instance, if empty will use source client\r\n\r\n        :type to_project: string\r\n        :param to_project: project name, if empty will use source project\r\n\r\n        :type to_logstore: string\r\n        :param to_logstore: logstore name, if empty will use source logstore\r\n\r\n        :type shard_list: string\r\n        :param shard_list: shard number list. could be comma seperated list or range: 1,20,31-40\r\n\r\n        :type batch_size: int\r\n        :param batch_size: batch size to fetch the data in each iteration. by default it's 500\r\n\r\n        :type compress: bool\r\n        :param compress: if use compression, by default it's True\r\n\r\n        :type new_topic: string\r\n        :param new_topic: overwrite the copied topic with the passed one\r\n\r\n        :type new_source: string\r\n        :param new_source: overwrite the copied source with the passed one\r\n\r\n        :return: LogResponse {\"total_count\": 30, \"shards\": {0: 10, 1: 20} })\r\n\r\n        \"\"\"\r\n        return copy_data(self, project, logstore, from_time, to_time=to_time,\r\n                         to_client=to_client, to_project=to_project, to_logstore=to_logstore,\r\n                         shard_list=shard_list,\r\n                         batch_size=batch_size, compress=compress, new_topic=new_topic, new_source=new_source)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transform_data(self, project, logstore, config, from_time, to_time=None,\r\n                       to_client=None, to_project=None, to_logstore=None,\r\n                       shard_list=None,\r\n                       batch_size=None, compress=None,\r\n                       cg_name=None, c_name=None,\r\n                       cg_heartbeat_interval=None, cg_data_fetch_interval=None, cg_in_order=None,\r\n                       cg_worker_pool_size=None\r\n                       ):\r\n        \"\"\"\r\n        transform data from one logstore to another one (could be the same or in different region), the time passed is log received time on server side. There're two mode, batch mode / consumer group mode. For Batch mode, just leave the cg_name and later options as None.\r\n\r\n        :type project: string\r\n        :param project: project name\r\n\r\n        :type logstore: string\r\n        :param logstore: logstore name\r\n\r\n        :type config: string\r\n        :param config: transform config imported or path of config (in python)\r\n\r\n        :type from_time: string/int\r\n        :param from_time: curosr value, could be begin, timestamp or readable time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type to_time: string/int\r\n        :param to_time: curosr value, could be begin, timestamp or readable time in readable time like \"%Y-%m-%d %H:%M:%S<time_zone>\" e.g. \"2018-01-02 12:12:10+8:00\", also support human readable string, e.g. \"1 hour ago\", \"now\", \"yesterday 0:0:0\", refer to https://aliyun-log-cli.readthedocs.io/en/latest/tutorials/tutorial_human_readable_datetime.html\r\n\r\n        :type to_client: LogClient\r\n        :param to_client: logclient instance, if empty will use source client\r\n\r\n        :type to_project: string\r\n        :param to_project: project name, if empty will use source project\r\n\r\n        :type to_logstore: string\r\n        :param to_logstore: logstore name, if empty will use source logstore\r\n\r\n        :type shard_list: string\r\n        :param shard_list: shard number list. could be comma seperated list or range: 1,20,31-40\r\n\r\n        :type batch_size: int\r\n        :param batch_size: batch size to fetch the data in each iteration. by default it's 500\r\n\r\n        :type compress: bool\r\n        :param compress: if use compression, by default it's True\r\n\r\n        :type cg_name: string\r\n        :param cg_name: consumer group name to enable scability and availability support.\r\n\r\n        :type c_name: string\r\n        :param c_name: consumer name for consumer group mode, must be different for each consuer in one group, normally leave it as default:  CLI-transform-data-${process_id}\r\n\r\n        :type cg_heartbeat_interval: int\r\n        :param cg_heartbeat_interval: cg_heartbeat_interval, default 20\r\n\r\n        :type cg_data_fetch_interval: int\r\n        :param cg_data_fetch_interval: cg_data_fetch_interval, default 2\r\n\r\n        :type cg_in_order: bool\r\n        :param cg_in_order: cg_in_order, default False\r\n\r\n        :type cg_worker_pool_size: int\r\n        :param cg_worker_pool_size: cg_worker_pool_size, default 2\r\n\r\n        :return: LogResponse {\"total_count\": 30, \"shards\": {0: {\"count\": 10, \"removed\": 1},  2: {\"count\": 20, \"removed\": 1}} })\r\n\r\n        \"\"\"\r\n        return transform_data(self, project, logstore, from_time, to_time=to_time,\r\n                              config=config,\r\n                              to_client=to_client, to_project=to_project, to_logstore=to_logstore,\r\n                              shard_list=shard_list,\r\n                              batch_size=batch_size, compress=compress,\r\n                              cg_name=cg_name, c_name=c_name,\r\n                              cg_heartbeat_interval=cg_heartbeat_interval,\r\n                              cg_data_fetch_interval=cg_data_fetch_interval,\r\n                              cg_in_order=cg_in_order,\r\n                              cg_worker_pool_size=cg_worker_pool_size\r\n                              )", "response": "Returns a new logstore with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef arrange_shard(self, project, logstore, count):\r\n        return arrange_shard(self, project, logstore, count)", "response": "arrange the logstore to the expected read - write count to a larger one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _n(v):\n        if v is None:\n            return \"\"\n\n        if isinstance(v, (dict, list)):\n            try:\n                v = json.dumps(v)\n            except Exception:\n                pass\n        elif six.PY2 and isinstance(v, six.text_type):\n            v = v.encode('utf8', \"ignore\")\n        elif six.PY3 and isinstance(v, six.binary_type):\n            v = v.decode('utf8', \"ignore\")\n\n        return str(v)", "response": "convert string to utf8 in Py2 or unicode in Py3\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if need to stop the current consumer", "response": "def _need_stop(self):\n        \"\"\"\n        check if need to stop:\n        1. end_cursor has been hit and there's no more shard assinged (wait for heatbeat_interval * 3)\n        :return:\n        \"\"\"\n        if not self.option.cursor_end_time:\n            return False\n\n        all_finish = True\n        for shard, consumer in self.shard_consumers.items():\n            if consumer.is_shutdown():\n                continue\n\n            # has not yet do any successful fetch yet or get some data\n            if consumer.last_success_fetch_time == 0 or consumer.last_fetch_count > 0:\n                return False\n\n        # init self.last_owned_consumer_finish_time if it's None\n        if all_finish and self.last_owned_consumer_finish_time == 0:\n            self.last_owned_consumer_finish_time = time.time()\n\n        if abs(time.time() - self.last_owned_consumer_finish_time) >= \\\n                self.option.consumer_group_time_out + self.option.heartbeat_interval:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(self, join=False):\n        Thread.start(self)\n\n        if join:\n            try:\n                while self.is_alive():\n                    self.join(timeout=60)\n                self.logger.info(\"worker {0} exit unexpected, try to shutdown it\".format(self.option.consumer_name))\n                self.shutdown()\n            except KeyboardInterrupt:\n                self.logger.info(\"*** try to exit **** \")\n                self.shutdown()", "response": "when calling with join=True, must call it in main thread, or else, the Keyboard Interrupt won't be caputured.\n        :param join: default False, if hold on until the worker is stopped by Ctrl+C or other reasons.\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consumer_process_task(processor, log_groups, check_point_tracker):\n    try:\n        check_point = processor.process(log_groups, check_point_tracker)\n        check_point_tracker.flush_check()\n    except Exception as e:\n        return TaskResult(e)\n    return ProcessTaskResult(check_point)", "response": "Process a single log group and return a TaskResult"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize a node in a shard.", "response": "def consumer_initialize_task(processor, consumer_client, shard_id, cursor_position, cursor_start_time, cursor_end_time=None):\n    \"\"\"\n    return TaskResult if failed, or else, return InitTaskResult\n    :param processor:\n    :param consumer_client:\n    :param shard_id:\n    :param cursor_position:\n    :param cursor_start_time:\n    :return:\n    \"\"\"\n    try:\n        processor.initialize(shard_id)\n        is_cursor_persistent = False\n        check_point = consumer_client.get_check_point(shard_id)\n        if check_point['checkpoint'] and len(check_point['checkpoint']) > 0:\n            is_cursor_persistent = True\n            cursor = check_point['checkpoint']\n        else:\n            if cursor_position == CursorPosition.BEGIN_CURSOR:\n                cursor = consumer_client.get_begin_cursor(shard_id)\n            elif cursor_position == CursorPosition.END_CURSOR:\n                cursor = consumer_client.get_end_cursor(shard_id)\n            else:\n                cursor = consumer_client.get_cursor(shard_id, cursor_start_time)\n\n        end_cursor = None\n        if cursor_end_time is not None:\n            end_cursor = consumer_client.get_cursor(shard_id, cursor_end_time)\n\n        return InitTaskResult(cursor, is_cursor_persistent, end_cursor)\n    except Exception as e:\n        return TaskResult(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef runExperiment(args):\n  numObjects = args.get(\"numObjects\", 10)\n  numLocations = args.get(\"numLocations\", 10)\n  numFeatures = args.get(\"numFeatures\", 10)\n  numColumns = args.get(\"numColumns\", 2)\n  profile = args.get(\"profile\", False)\n  noiseLevel = args.get(\"noiseLevel\", None)  # TODO: implement this?\n  numPoints = args.get(\"numPoints\", 10)\n  trialNum = args.get(\"trialNum\", 42)\n  l2Params = args.get(\"l2Params\", getL2Params())\n  l4Params = args.get(\"l4Params\", getL4Params())\n  objectSeed = args.get(\"objectSeed\", 41)\n\n  # Create the objects\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=1024,\n    externalInputSize=1024,\n    numCorticalColumns=numColumns,\n    seed=objectSeed,\n  )\n  objects.createRandomObjects(numObjects, numPoints=numPoints,\n                                    numLocations=numLocations,\n                                    numFeatures=numFeatures)\n\n  # print \"Objects are:\"\n  # for o in objects:\n  #   pairs = objects[o]\n  #   pairs.sort()\n  #   print str(o) + \": \" + str(pairs)\n\n  # Setup experiment and train the network\n  name = \"convergence_O%03d_L%03d_F%03d_C%03d_T%03d\" % (\n    numObjects, numLocations, numFeatures, numColumns, trialNum\n  )\n  exp = L4L2Experiment(\n    name,\n    L2Overrides=l2Params,\n    L4Overrides=l4Params,\n    numCorticalColumns=numColumns,\n    seed=trialNum\n  )\n\n  exp.learnObjects(objects.provideObjectsToLearn())\n  L2TimeLearn = 0\n  L2TimeInfer = 0\n\n  if profile:\n    # exp.printProfile(reset=True)\n    L2TimeLearn = getProfileInfo(exp)\n    args.update({\"L2TimeLearn\": L2TimeLearn})\n  exp.resetProfile()\n\n  # For inference, we will check and plot convergence for each object. For each\n  # object, we create a sequence of random sensations for each column.  We will\n  # present each sensation for 3 time steps to let it settle and ensure it\n  # converges.\n\n  for objectId in objects:\n    obj = objects[objectId]\n\n    # Create sequence of sensations for this object for all columns\n    objectSensations = {}\n    for c in range(numColumns):\n      objectCopy = [pair for pair in obj]\n      random.shuffle(objectCopy)\n      # stay multiple steps on each sensation\n      sensations = []\n      for pair in objectCopy:\n        for _ in xrange(2):\n          sensations.append(pair)\n      objectSensations[c] = sensations\n\n    inferConfig = {\n      \"object\": objectId,\n      \"numSteps\": len(objectSensations[0]),\n      \"pairs\": objectSensations\n    }\n\n    exp.infer(objects.provideObjectToInfer(inferConfig), objectName=objectId)\n    if profile:\n      L2TimeInfer += getProfileInfo(exp)\n      exp.resetProfile()\n      # exp.printProfile(reset=True)\n\n  if profile:\n    L2TimeInfer /= len(objects)\n    args.update({\"L2TimeInfer\": L2TimeInfer})\n\n  convergencePoint, _ = exp.averageConvergencePoint(\"L2 Representation\", 40, 40)\n  print \"objectSeed {} # distal syn {} # proximal syn {}, \" \\\n        \"# convergence point={:4.2f} train time {:4.3f} infer time {:4.3f}\".format(\n    objectSeed,\n    l2Params[\"sampleSizeDistal\"],\n    l2Params[\"sampleSizeProximal\"],\n    convergencePoint, L2TimeLearn, L2TimeInfer)\n\n  # Return our convergence point as well as all the parameters and objects\n  args.update({\"objects\": objects.getObjects()})\n  args.update({\"convergencePoint\": convergencePoint})\n\n  # prepare experiment results\n  numLateralConnections = []\n  numProximalConnections = []\n  for l2Columns in exp.L2Columns:\n    numLateralConnections.append(\n      l2Columns._pooler.numberOfDistalSynapses())\n    numProximalConnections.append(\n      np.sum(l2Columns._pooler.numberOfProximalSynapses()))\n\n  result = {\n    'trial': objectSeed,\n    'L2TimeLearn': args['L2TimeLearn'],\n    'L2TimeInfer': args['L2TimeInfer'],\n    'sampleSizeProximal': l2Params[\"sampleSizeProximal\"],\n    'sampleSizeDistal': l2Params[\"sampleSizeDistal\"],\n    'numLateralConnections': np.mean(np.array(numLateralConnections)),\n    'numProximalConnections': np.mean(np.array(numProximalConnections)),\n    'convergencePoint': args['convergencePoint']}\n  return result", "response": "Runs the cortical network and returns the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getProfileInfo(exp):\n\n  totalTime = 0.000001\n  for region in exp.network.regions.values():\n    timer = region.getComputeTimer()\n    totalTime += timer.getElapsed()\n\n  # Sort the region names\n  regionNames = list(exp.network.regions.keys())\n  regionNames.sort()\n\n  count = 1\n  profileInfo = []\n  L2Time = 0.0\n  L4Time = 0.0\n  for regionName in regionNames:\n    region = exp.network.regions[regionName]\n    timer = region.getComputeTimer()\n    count = max(timer.getStartCount(), count)\n    profileInfo.append([region.name,\n                        timer.getStartCount(),\n                        timer.getElapsed(),\n                        100.0 * timer.getElapsed() / totalTime,\n                        timer.getElapsed() / max(timer.getStartCount(), 1)])\n    if \"L2Column\" in regionName:\n      L2Time += timer.getElapsed()\n    elif \"L4Column\" in regionName:\n      L4Time += timer.getElapsed()\n  return L2Time", "response": "Prints the profiling information for the current set of regions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest multi - column convergence with varying amount of proximal and distal sampling.", "response": "def experimentVaryingSynapseSampling(expParams,\n                                     sampleSizeDistalList,\n                                     sampleSizeProximalList):\n  \"\"\"\n  Test multi-column convergence with varying amount of proximal/distal sampling\n\n  :return:\n  \"\"\"\n  numRpts = 20\n  df = None\n  args = []\n  for sampleSizeProximal in sampleSizeProximalList:\n    for sampleSizeDistal in sampleSizeDistalList:\n\n      for rpt in range(numRpts):\n        l4Params = getL4Params()\n        l2Params = getL2Params()\n        l2Params[\"sampleSizeProximal\"] = sampleSizeProximal\n        l2Params[\"minThresholdProximal\"] = sampleSizeProximal\n        l2Params[\"sampleSizeDistal\"] = sampleSizeDistal\n        l2Params[\"activationThresholdDistal\"] = sampleSizeDistal\n\n        args.append(\n          {\n            \"numObjects\": expParams['numObjects'],\n            \"numLocations\": expParams['numLocations'],\n            \"numFeatures\": expParams['numFeatures'],\n            \"numColumns\": expParams['numColumns'],\n            \"trialNum\": rpt,\n            \"l4Params\": l4Params,\n            \"l2Params\": l2Params,\n            \"profile\": True,\n            \"objectSeed\": rpt,\n          }\n        )\n\n  pool = Pool(processes=expParams['numWorkers'])\n  result = pool.map(runExperiment, args)\n\n  #\n  #       if df is None:\n  #         df = pd.DataFrame.from_dict(result, orient='index')\n  #       else:\n  #         df = pd.concat([df, pd.DataFrame.from_dict(result, orient='index')], axis=1)\n  #\n  # df = df.transpose()\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mmGetPlotUnionSDRActivity(self, title=\"Union SDR Activity Raster\",\n                                showReset=False, resetShading=0.25):\n    \"\"\" Returns plot of the activity of union SDR bits.\n    @param title an optional title for the figure\n    @param showReset if true, the first set of activities after a reset\n                        will have a gray background\n    @param resetShading If showReset is true, this float specifies the\n    intensity of the reset background with 0.0 being white and 1.0 being black\n    @return (Plot) plot\n    \"\"\"\n    unionSDRTrace = self.mmGetTraceUnionSDR().data\n    columnCount = self.getNumColumns()\n    activityType = \"Union SDR Activity\"\n    return self.mmGetCellTracePlot(unionSDRTrace, columnCount, activityType,\n                                   title=title, showReset=showReset,\n                                   resetShading=resetShading)", "response": "Returns the plot of the activity of union SDR bits."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mmGetPlotConnectionsPerColumn(self, title=\"Connections per Columns\"):\n    plot = Plot(self, title)\n    connectedCounts = numpy.ndarray(self.getNumColumns(), dtype=uintType)\n    self.getConnectedCounts(connectedCounts)\n\n    plot.addGraph(sorted(connectedCounts.tolist(), reverse=True),\n                  position=211,\n                  xlabel=\"column\", ylabel=\"# connections\")\n\n    plot.addHistogram(connectedCounts.tolist(),\n                      position=212,\n                      bins=len(connectedCounts) / 10,\n                      xlabel=\"# connections\", ylabel=\"# columns\")\n    return plot", "response": "Returns plot of the number of connections per column."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a pretty - printed string representation of overlap metric data.", "response": "def mmPrettyPrintDataOverlap(self):\n    \"\"\"\n    Returns pretty-printed string representation of overlap metric data.\n    (See `mmGetDataOverlap`.)\n\n    @return (string) Pretty-printed data\n    \"\"\"\n    matrix = self.mmGetDataOverlap()\n    resetsTrace = self.mmGetTraceResets()\n\n    text = \"\"\n\n    for i, row in enumerate(matrix):\n      if resetsTrace.data[i]:\n        text += \"\\n\"\n\n      for j, item in enumerate(row):\n        if resetsTrace.data[j]:\n          text += \"    \"\n\n        text += \"{:4}\".format(item)\n\n      text += \"\\n\"\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the metric that computes the stability confusion of the sequence of active cells for each iteration.", "response": "def mmGetMetricStabilityConfusion(self):\n    \"\"\"\n    For each iteration that doesn't follow a reset, looks at every other\n    iteration for the same world that doesn't follow a reset, and computes the\n    number of bits that show up in one or the other set of active cells for\n    that iteration, but not both. This metric returns the distribution of those\n    numbers.\n\n    @return (Metric) Stability confusion metric\n    \"\"\"\n    self._mmComputeSequenceRepresentationData()\n    numbers = self._mmData[\"stabilityConfusion\"]\n    return Metric(self, \"stability confusion\", numbers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a plot of the overlap metric between union SDRs within a sequence.", "response": "def mmGetPlotStability(self, title=\"Stability\", showReset=False,\n                         resetShading=0.25):\n    \"\"\"\n    Returns plot of the overlap metric between union SDRs within a sequence.\n    @param title an optional title for the figure\n    @return (Plot) plot\n    \"\"\"\n    plot = Plot(self, title)\n    self._mmComputeSequenceRepresentationData()\n    data = self._mmData[\"stabilityConfusion\"]\n    plot.addGraph(sorted(data, reverse=True),\n                  position=211,\n                  xlabel=\"Time steps\", ylabel=\"Overlap\")\n    plot.addHistogram(data,\n                      position=212,\n                      bins=100,\n                      xlabel=\"Overlap\", ylabel=\"# time steps\")\n    return plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mmGetMetricDistinctnessConfusion(self):\n    self._mmComputeSequenceRepresentationData()\n    numbers = self._mmData[\"distinctnessConfusion\"]\n    return Metric(self, \"distinctness confusion\", numbers)", "response": "Returns the Distinctness Confusion metric that is used to compute the number of bits that show up in both active cells and the number of bits that show up in both active cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mmUpdateDutyCycles(self):\n    period = self.getDutyCyclePeriod()\n\n    unionSDRArray = numpy.zeros(self.getNumColumns())\n    unionSDRArray[list(self._mmTraces[\"unionSDR\"].data[-1])] = 1\n\n    self._mmData[\"unionSDRDutyCycle\"] = \\\n      UnionTemporalPoolerMonitorMixin._mmUpdateDutyCyclesHelper(\n        self._mmData[\"unionSDRDutyCycle\"], unionSDRArray, period)\n\n    self._mmData[\"persistenceDutyCycle\"] = \\\n      UnionTemporalPoolerMonitorMixin._mmUpdateDutyCyclesHelper(\n        self._mmData[\"persistenceDutyCycle\"], self._poolingActivation, period)", "response": "Update the duty cycle variables internally tracked by the TM mixin."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the values for the overlap distance matrix stability and distinctness between sequences.", "response": "def _mmComputeSequenceRepresentationData(self):\n    \"\"\"\n    Calculates values for the overlap distance matrix, stability within a\n    sequence, and distinctness between sequences. These values are cached so\n    that they do need to be recomputed for calls to each of several accessor\n    methods that use these values.\n    \"\"\"\n    if not self._sequenceRepresentationDataStale:\n      return\n\n    unionSDRTrace = self.mmGetTraceUnionSDR()\n    sequenceLabelsTrace = self.mmGetTraceSequenceLabels()\n    resetsTrace = self.mmGetTraceResets()\n\n    n = len(unionSDRTrace.data)\n    overlapMatrix = numpy.empty((n, n), dtype=uintType)\n    stabilityConfusionUnionSDR = []\n    distinctnessConfusionUnionSDR = []\n\n    for i in xrange(n):\n      for j in xrange(i+1):\n        overlapUnionSDR = len(unionSDRTrace.data[i] & unionSDRTrace.data[j])\n\n        overlapMatrix[i][j] = overlapUnionSDR\n        overlapMatrix[j][i] = overlapUnionSDR\n\n        if (i != j and\n            sequenceLabelsTrace.data[i] is not None and\n            not resetsTrace.data[i] and\n            sequenceLabelsTrace.data[j] is not None and\n            not resetsTrace.data[j]):\n          if sequenceLabelsTrace.data[i] == sequenceLabelsTrace.data[j]:\n            stabilityConfusionUnionSDR.append(overlapUnionSDR)\n          else:\n            distinctnessConfusionUnionSDR.append(overlapUnionSDR)\n\n    self._mmData[\"overlap\"] = overlapMatrix\n    self._mmData[\"stabilityConfusion\"] = stabilityConfusionUnionSDR\n    self._mmData[\"distinctnessConfusion\"] = distinctnessConfusionUnionSDR\n\n    self._sequenceRepresentationDataStale = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef registerResearchRegion(regionTypeName, moduleName=None):\n  global _PY_REGIONS\n\n  if moduleName is None:\n    # the region is located in the regions/ directory\n    moduleName = \"htmresearch.regions.\" + regionTypeName\n  if regionTypeName not in _PY_REGIONS:\n    # Add new region class to the network.\n    module = __import__(moduleName, {}, {}, regionTypeName)\n    unregisteredClass = getattr(module, regionTypeName)\n    Network.registerRegion(unregisteredClass)\n    # Add region to list of registered PyRegions\n    _PY_REGIONS.append(regionTypeName)", "response": "Register this region so that NuPIC can later find it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loadMatlabImages(self, path, name):\n    try:\n      images = sc.loadmat(path)[name]\n    except KeyError:\n      raise KeyError('Wrong filename for provided images.')\n\n    self._initializeDimensions(images)\n\n    return images", "response": "Loads images from a. mat file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef loadNumpyImages(self, path, key=None):\n    data = np.load(path)\n\n    if isinstance(data, dict):\n      if key is None:\n        raise ValueError(\"Images are stored as a dict, a key must be provided!\")\n      try:\n        data = data[key]\n      except KeyError:\n        raise KeyError(\"Wrong key for provided data.\")\n\n    if not isinstance(data, np.ndarray):\n      raise TypeError(\"Data must be stored as a dict or numpy array.\")\n\n    self._initializeDimensions(data)\n\n    return data", "response": "Loads images from numpy file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _initializeDimensions(self, inputData):\n    if len(inputData.shape) == 2:\n      self.imageHeight, self.numImages = inputData.shape\n      self.imageWidth, self.numChannels = None, None\n\n    elif len(inputData.shape) == 3:\n      self.imageHeight, \\\n      self.imageWidth, \\\n      self.numImages = inputData.shape\n      self.numChannels = None\n\n    elif len(inputData.shape) == 4:\n      self.imageHeight, \\\n      self.imageWidth, \\\n      self.numChannels, \\\n      self.numImages = inputData.shape\n\n    else:\n      raise ValueError(\"The provided image set has more than 4 dimensions.\")", "response": "Initializes the dimensions of the training images."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an array of dimensions ( filterDim batchSize ) to be used as training batch.", "response": "def _getDataBatch(self, inputData):\n    \"\"\"\n    Returns an array of dimensions (filterDim, batchSize), to be used as\n    batch for training data.\n\n    This implementation uses random sub-patches as training batch.\n\n    Images are flattened to get a 2-dimensional batch.\n    \"\"\"\n    if not hasattr(self, 'numImages'):\n      self._initializeDimensions(inputData)\n\n    batch = np.zeros((self.filterDim, self.batchSize))\n\n    # choose correct patch size\n    if self.imageHeight is None:\n      patchSize = self.filterDim\n    elif self.numChannels is None or self.numChannels == 0:\n      patchSize = int(np.sqrt(self.filterDim))\n    else:\n      patchSize = int(np.sqrt(self.filterDim / self.numChannels))\n\n    minIndex = patchSize / 2\n\n    for i in xrange(self.batchSize):\n      # choose random image\n      imageIdx = np.random.choice(range(self.numImages))\n\n      # pick random starting row\n      rows = self.imageHeight - 2 * patchSize\n      rowNumber = minIndex + np.random.choice(range(rows))\n\n      if self.imageHeight is None:\n        patch = inputData[rowNumber : rowNumber + patchSize, imageIdx]\n      else:\n        # pick random starting column\n        cols = self.imageWidth - 2 * patchSize\n        colNumber = minIndex + np.random.choice(range(cols))\n\n        if self.numChannels is None:\n          patch = inputData[rowNumber : rowNumber + patchSize,\n                            colNumber : colNumber + patchSize,\n                            imageIdx]\n        else:\n          patch = inputData[rowNumber : rowNumber + patchSize,\n                            colNumber : colNumber + patchSize,\n                            :, imageIdx]\n      patch = np.reshape(patch, newshape=(self.filterDim, ))\n      batch[:, i] = patch\n\n    return batch"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns plot of the number of connections per column.", "response": "def mmGetPlotConnectionsPerColumn(self, title=None):\n    \"\"\"\n    Returns plot of # connections per column.\n\n    @return (Plot) plot\n    \"\"\"\n    plot = Plot(self, title)\n    plot.addGraph(sorted(self._connectedCounts.tolist(), reverse=True),\n                  position=211,\n                  xlabel=\"column\", ylabel=\"# connections\")\n    plot.addHistogram(self._connectedCounts.tolist(),\n                      position=212,\n                      bins=len(self._connectedCounts) / 10,\n                      xlabel=\"# connections\", ylabel=\"# columns\")\n    return plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the plot of the cell activity.", "response": "def mmGetCellActivityPlot(self, title=None, showReset=False,\n                            resetShading=0.25):\n    \"\"\" Returns plot of the cell activity.\n    @param title an optional title for the figure\n    @param showReset if true, the first set of cell activities after a reset\n                        will have a gray background\n    @param resetShading If showReset is true, this float specifies the\n    intensity of the reset background with 0.0 being white and 1.0 being black\n    @return (Plot) plot\n    \"\"\"\n    cellTrace = self._mmTraces[\"activeCells\"].data\n    cellCount = self.getNumColumns()\n    activityType = \"Cell Activity\"\n    return self.mmGetCellTracePlot(cellTrace, cellCount, activityType,\n                                   title=title, showReset=showReset,\n                                   resetShading=resetShading)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns plot of column permanences.", "response": "def mmGetPermanencesPlot(self, title=None):\n    \"\"\" Returns plot of column permanences.\n    @param title an optional title for the figure\n    @return (Plot) plot\n    \"\"\"\n    plot = Plot(self, title)\n\n    data = numpy.zeros((self.getNumColumns(), self.getNumInputs()))\n    for i in xrange(self.getNumColumns()):\n      self.getPermanence(i, data[i])\n\n    plot.add2DArray(data, xlabel=\"Permanences\", ylabel=\"Column\")\n    return plot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting the current state of the current object to the initial state.", "response": "def reset(self, params, repetition):\n    \"\"\"\n    Called once at the beginning of each experiment.\n    \"\"\"\n    self.startTime = time.time()\n    print(params)\n    seed = params[\"seed\"] + repetition\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # Get our directories correct\n    self.dataDir = os.path.join(params[\"datadir\"], \"speech_commands\")\n    self.resultsDir = os.path.join(params[\"path\"], params[\"name\"], \"plots\")\n\n    if not os.path.exists(self.resultsDir):\n      os.makedirs(self.resultsDir)\n\n    self.use_cuda = not params[\"no_cuda\"] and torch.cuda.is_available()\n    if self.use_cuda:\n      print(\"*********using cuda!\")\n    self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\n    self.use_preprocessed_dataset = False\n    self.loadDatasets(params)\n\n    # Parse 'n' and 'k' parameters\n    n = params[\"n\"]\n    k = params[\"k\"]\n    if isinstance(n, basestring):\n      n = map(int, n.split(\"_\"))\n    if isinstance(k, basestring):\n      k = map(int, k.split(\"_\"))\n\n    if params[\"model_type\"] == \"cnn\":\n      c1_out_channels = params[\"c1_out_channels\"]\n      c1_k = params[\"c1_k\"]\n      if isinstance(c1_out_channels, basestring):\n        c1_out_channels = map(int, c1_out_channels.split(\"_\"))\n      if isinstance(c1_k, basestring):\n        c1_k = map(int, c1_k.split(\"_\"))\n\n      # Parse 'c1_input_shape; parameter\n      if \"c1_input_shape\" in params:\n        c1_input_shape = map(int, params[\"c1_input_shape\"].split(\"_\"))\n      else:\n        c1_input_shape = (1, 32, 32)\n\n      sp_model = SparseNet(\n        inputSize=c1_input_shape,\n        outputSize=len(self.train_loader.dataset.classes),\n        outChannels=c1_out_channels,\n        c_k=c1_k,\n        kernelSize=5,\n        stride=1,\n        dropout=params[\"dropout\"],\n        n=n,\n        k=k,\n        boostStrength=params[\"boost_strength\"],\n        weightSparsity=params[\"weight_sparsity\"],\n        weightSparsityCNN=params[\"weight_sparsity_cnn\"],\n        boostStrengthFactor=params[\"boost_strength_factor\"],\n        kInferenceFactor=params[\"k_inference_factor\"],\n        useBatchNorm=params[\"use_batch_norm\"],\n        normalizeWeights=params.get(\"normalize_weights\", False)\n      )\n    elif params[\"model_type\"] == \"resnet9\":\n      sp_model = resnet9(num_classes=len(self.train_loader.dataset.classes),\n                         in_channels=1)\n    elif params[\"model_type\"] == \"linear\":\n      sp_model = SparseNet(\n        n=n,\n        k=k,\n        inputSize=32*32,\n        outputSize=len(self.train_loader.dataset.classes),\n        boostStrength=params[\"boost_strength\"],\n        weightSparsity=params[\"weight_sparsity\"],\n        boostStrengthFactor=params[\"boost_strength_factor\"],\n        kInferenceFactor=params[\"k_inference_factor\"],\n        dropout=params[\"dropout\"],\n        useBatchNorm=params[\"use_batch_norm\"],\n        normalizeWeights=params.get(\"normalize_weights\", False)\n      )\n    else:\n      raise RuntimeError(\"Unknown model type\")\n\n    if torch.cuda.device_count() > 1:\n      print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n      sp_model = torch.nn.DataParallel(sp_model)\n\n    self.model = sp_model.to(self.device)\n    self.learningRate = params[\"learning_rate\"]\n    self.optimizer = self.createOptimizer(params, self.model)\n    self.lr_scheduler = self.createLearningRateScheduler(params, self.optimizer)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling once for each training iteration (== epoch here).", "response": "def iterate(self, params, repetition, iteration):\n    \"\"\"\n    Called once for each training iteration (== epoch here).\n    \"\"\"\n    print(\"\\nStarting iteration\",iteration)\n    print(\"Learning rate:\", self.learningRate if self.lr_scheduler is None\n                                              else self.lr_scheduler.get_lr())\n    t1 = time.time()\n    ret = {}\n\n    # Update dataset epoch when using pre-processed speech dataset\n    if self.use_preprocessed_dataset:\n      t2 = time.time()\n      self.train_loader.dataset.next_epoch()\n      self.validation_loader.dataset.next_epoch()\n      self.test_loader.dataset.next_epoch()\n      self.bg_noise_loader.dataset.next_epoch()\n      print(\"Dataset Load time = {0:.3f} secs, \".format(time.time() - t2))\n\n    # Update learning rate using learning rate scheduler if configured\n    if self.lr_scheduler is not None:\n      # ReduceLROnPlateau lr_scheduler step should be called after validation,\n      # all other lr_schedulers should be called before training\n      if params[\"lr_scheduler\"] != \"ReduceLROnPlateau\":\n        self.lr_scheduler.step()\n\n    self.train(params, epoch=iteration, repetition=repetition)\n\n    # Run validation test\n    if self.validation_loader is not None:\n      validation = self.test(params, self.validation_loader)\n\n      # ReduceLROnPlateau step should be called after validation\n      if params[\"lr_scheduler\"] == \"ReduceLROnPlateau\":\n        self.lr_scheduler.step(validation[\"test_loss\"])\n\n      ret[\"validation\"] = validation\n      print(\"Validation: error=\", validation[\"testerror\"],\n            \"entropy=\", validation[\"entropy\"],\n            \"loss=\", validation[\"test_loss\"])\n      ret.update({\"validationerror\": validation[\"testerror\"]})\n\n    # Run test set\n    if self.test_loader is not None:\n      testResults = self.test(params, self.test_loader)\n      ret[\"testResults\"] = testResults\n      print(\"Test: error=\", testResults[\"testerror\"],\n            \"entropy=\", testResults[\"entropy\"],\n            \"loss=\", testResults[\"test_loss\"])\n      ret.update({\"testerror\": testResults[\"testerror\"]})\n\n    # Run bg noise set\n    if self.bg_noise_loader is not None:\n      bgResults = self.test(params, self.bg_noise_loader)\n      ret[\"bgResults\"] = bgResults\n      print(\"BG noise error=\", bgResults[\"testerror\"])\n      ret.update({\"bgerror\": bgResults[\"testerror\"]})\n\n    ret.update({\"elapsedTime\": time.time() - self.startTime})\n    ret.update({\"learningRate\": self.learningRate if self.lr_scheduler is None\n                                                  else self.lr_scheduler.get_lr()})\n\n    # Run noise set\n    if params.get(\"run_noise_tests\", False):\n      ret.update(self.runNoiseTests(params))\n      print(\"Noise test results: totalCorrect=\", ret[\"totalCorrect\"],\n            \"Test error=\", ret[\"testerror\"], \", entropy=\", ret[\"entropy\"])\n\n    ret.update({\"elapsedTime\": time.time() - self.startTime})\n    ret.update({\"learningRate\": self.learningRate if self.lr_scheduler is None\n                                                  else self.lr_scheduler.get_lr()})\n\n    print(\"Iteration time= {0:.3f} secs, \"\n          \"total elapsed time= {1:.3f} mins\".format(\n            time.time() - t1,ret[\"elapsedTime\"]/60.0))\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef finalize(self, params, rep):\n    if params.get(\"saveNet\", True):\n      saveDir = os.path.join(params[\"path\"], params[\"name\"],\n                             \"model_{}.pt\".format(rep))\n      torch.save(self.model, saveDir)", "response": "Save the model once we are done."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef createLearningRateScheduler(self, params, optimizer):\n    lr_scheduler = params.get(\"lr_scheduler\", None)\n    if lr_scheduler is None:\n      return None\n\n    if lr_scheduler == \"StepLR\":\n      lr_scheduler_params = \"{'step_size': 1, 'gamma':\" + str(params[\"learning_rate_factor\"]) + \"}\"\n\n    else:\n      lr_scheduler_params = params.get(\"lr_scheduler_params\", None)\n      if lr_scheduler_params is None:\n        raise ValueError(\"Missing 'lr_scheduler_params' for {}\".format(lr_scheduler))\n\n    # Get lr_scheduler class by name\n    clazz = eval(\"torch.optim.lr_scheduler.{}\".format(lr_scheduler))\n\n    # Parse scheduler parameters from config\n    lr_scheduler_params = eval(lr_scheduler_params)\n\n    return clazz(optimizer, **lr_scheduler_params)", "response": "Creates the learning rate scheduler and attachs the optimizer optimizer\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an optimizer instance based on the parameters.", "response": "def createOptimizer(self, params, model):\n    \"\"\"\n    Create a new instance of the optimizer\n    \"\"\"\n    lr = params[\"learning_rate\"]\n    print(\"Creating optimizer with learning rate=\", lr)\n    if params[\"optimizer\"] == \"SGD\":\n      optimizer = optim.SGD(model.parameters(), lr=lr,\n                            momentum=params[\"momentum\"],\n                            weight_decay=params[\"weight_decay\"],\n                            )\n    elif params[\"optimizer\"] == \"Adam\":\n      optimizer = optim.Adam(model.parameters(), lr=lr)\n    else:\n      raise LookupError(\"Incorrect optimizer value\")\n\n    return optimizer"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef train(self, params, epoch, repetition):\n    # Check for pre-trained model\n    modelCheckpoint = os.path.join(params[\"path\"], params[\"name\"],\n                                   \"model_{}_{}.pt\".format(repetition, epoch))\n    if os.path.exists(modelCheckpoint):\n      self.model = torch.load(modelCheckpoint, map_location=self.device)\n      return\n\n    self.model.train()\n    for batch_idx, (batch, target) in enumerate(self.train_loader):\n      data = batch[\"input\"]\n      if params[\"model_type\"] in [\"resnet9\", \"cnn\"]:\n        data = torch.unsqueeze(data, 1)\n      data, target = data.to(self.device), target.to(self.device)\n      self.optimizer.zero_grad()\n      output = self.model(data)\n      loss = F.nll_loss(output, target)\n      loss.backward()\n      self.optimizer.step()\n\n      # Log info every log_interval mini batches\n      if batch_idx % params[\"log_interval\"] == 0:\n        entropy = self.model.entropy()\n        print(\n          \"logging: \",self.model.getLearningIterations(),\n          \" learning iterations, elapsedTime\", time.time() - self.startTime,\n          \" entropy:\", float(entropy),\" / \", self.model.maxEntropy(),\n          \"loss:\", loss.item())\n        if params[\"create_plots\"]:\n          plotDutyCycles(self.model.dutyCycle,\n                         self.resultsDir + \"/figure_\"+str(epoch)+\"_\"+str(\n                           self.model.getLearningIterations()))\n\n      if batch_idx >= params[\"batches_in_epoch\"]:\n        break\n\n    self.model.postEpoch()\n\n    # Save model checkpoint on every epoch\n    if params.get(\"save_every_epoch\", False):\n      torch.save(self.model, modelCheckpoint)", "response": "Train one epoch of this model by iterating through mini batches and saving the model checkpoint if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the datasets for the Kaggle experiment.", "response": "def loadDatasets(self, params):\n    \"\"\"\n    The GSC dataset specifies specific files to be used as training, test,\n    and validation.  We assume the data has already been processed according\n    to those files into separate train, test, and valid directories.\n\n    For our experiment we use a subset of the data (10 categories out of 30),\n    just like the Kaggle competition.\n    \"\"\"\n    n_mels = 32\n\n    # Check if using pre-processed data or raw data\n    self.use_preprocessed_dataset = PreprocessedSpeechDataset.isValid(self.dataDir)\n    if self.use_preprocessed_dataset:\n      trainDataset = PreprocessedSpeechDataset(self.dataDir, subset=\"train\")\n      validationDataset = PreprocessedSpeechDataset(self.dataDir, subset=\"valid\",\n                                                    silence_percentage=0)\n      testDataset = PreprocessedSpeechDataset(self.dataDir, subset=\"test\",\n                                              silence_percentage=0)\n      bgNoiseDataset = PreprocessedSpeechDataset(self.dataDir, subset=\"noise\",\n                                                 silence_percentage=0)\n    else:\n      trainDataDir = os.path.join(self.dataDir, \"train\")\n      testDataDir = os.path.join(self.dataDir, \"test\")\n      validationDataDir = os.path.join(self.dataDir, \"valid\")\n      backgroundNoiseDir = os.path.join(self.dataDir, params[\"background_noise_dir\"])\n\n      dataAugmentationTransform = transforms.Compose([\n        ChangeAmplitude(),\n        ChangeSpeedAndPitchAudio(),\n        FixAudioLength(),\n        ToSTFT(),\n        StretchAudioOnSTFT(),\n        TimeshiftAudioOnSTFT(),\n        FixSTFTDimension(),\n      ])\n\n      featureTransform = transforms.Compose(\n        [\n          ToMelSpectrogramFromSTFT(n_mels=n_mels),\n          DeleteSTFT(),\n          ToTensor('mel_spectrogram', 'input')\n        ])\n\n      trainDataset = SpeechCommandsDataset(\n        trainDataDir,\n        transforms.Compose([\n          dataAugmentationTransform,\n          # add_bg_noise,               # Uncomment to allow adding BG noise\n                                        # during training\n          featureTransform\n        ]))\n\n      testFeatureTransform = transforms.Compose([\n        FixAudioLength(),\n        ToMelSpectrogram(n_mels=n_mels),\n        ToTensor('mel_spectrogram', 'input')\n      ])\n\n      validationDataset = SpeechCommandsDataset(\n        validationDataDir,\n        testFeatureTransform,\n        silence_percentage=0,\n      )\n\n      testDataset = SpeechCommandsDataset(\n        testDataDir,\n        testFeatureTransform,\n        silence_percentage=0,\n      )\n\n      bg_dataset = BackgroundNoiseDataset(\n        backgroundNoiseDir,\n        transforms.Compose([FixAudioLength(), ToSTFT()]),\n      )\n\n      bgNoiseTransform = transforms.Compose([\n        FixAudioLength(),\n        ToSTFT(),\n        AddBackgroundNoiseOnSTFT(bg_dataset),\n        ToMelSpectrogramFromSTFT(n_mels=n_mels),\n        DeleteSTFT(),\n        ToTensor('mel_spectrogram', 'input')\n      ])\n\n      bgNoiseDataset = SpeechCommandsDataset(\n        testDataDir,\n        bgNoiseTransform,\n        silence_percentage=0,\n      )\n\n    weights = trainDataset.make_weights_for_balanced_classes()\n    sampler = WeightedRandomSampler(weights, len(weights))\n\n    # print(\"Number of training samples=\",len(trainDataset))\n    # print(\"Number of validation samples=\",len(validationDataset))\n    # print(\"Number of test samples=\",len(testDataset))\n\n    self.train_loader = DataLoader(trainDataset,\n                                   batch_size=params[\"batch_size\"],\n                                   sampler=sampler\n                                   )\n\n    self.validation_loader = DataLoader(validationDataset,\n                                        batch_size=params[\"batch_size\"],\n                                        shuffle=False\n                                        )\n\n    self.test_loader = DataLoader(testDataset,\n                                  batch_size=params[\"batch_size\"],\n                                  sampler=None,\n                                  shuffle=False\n                                  )\n\n    self.bg_noise_loader = DataLoader(bgNoiseDataset,\n                                  batch_size=params[\"batch_size\"],\n                                  sampler=None,\n                                  shuffle=False\n                                  )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the feature index associated with the provided location.", "response": "def getFeatureID(self, location):\n    \"\"\"\n    Returns the feature index associated with the provided location.\n\n    In the case of a sphere, it is always the same if the location is valid.\n    \"\"\"\n    if not self.contains(location):\n      return self.EMPTY_FEATURE\n\n    return self.SPHERICAL_SURFACE"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef contains(self, location):\n    return self.almostEqual(\n      sum([coord ** 2 for coord in location]), self.radius ** 2\n    )", "response": "Checks that the provided point is on the sphere."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sampleLocationFromFeature(self, feature):\n    if feature == \"surface\":\n      coordinates = [random.gauss(0, 1.) for _ in xrange(self.dimension)]\n      norm = sqrt(sum([coord ** 2 for coord in coordinates]))\n      return [self.radius * coord / norm for coord in coordinates]\n    elif feature == \"random\":\n      return self.sampleLocation()\n    else:\n      raise NameError(\"No such feature in {}: {}\".format(self, feature))", "response": "Samples a location from the provided feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getFeatureID(self, location):\n    if not self.contains(location):\n      return self.EMPTY_FEATURE\n\n    if self.almostEqual(abs(location[2]), self.height / 2.):\n      if self.almostEqual(location[0] ** 2 + location[1] ** 2,\n                          self.radius ** 2):\n        return self.CYLINDER_EDGE\n      else:\n        return self.FLAT\n    else:\n      return self.CYLINDER_SURFACE", "response": "Returns the feature index associated with the provided location."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contains(self, location):\n    if self.almostEqual(location[0] ** 2 + location[1] ** 2, self.radius ** 2):\n      return abs(location[2]) < self.height / 2.\n    if self.almostEqual(location[2], self.height / 2.):\n      return location[0] ** 2 + location[1] ** 2 < self.radius ** 2\n    return False", "response": "Checks that the provided point is on the cylinder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsampling a location from the provided features.", "response": "def sampleLocationFromFeature(self, feature):\n    \"\"\"\n    Samples a location from the provided specific features.\n    \"\"\"\n    if feature == \"topDisc\":\n      return self._sampleLocationOnDisc(top=True)\n    elif feature == \"topEdge\":\n      return self._sampleLocationOnEdge(top=True)\n    elif feature == \"bottomDisc\":\n      return self._sampleLocationOnDisc(top=False)\n    elif feature == \"bottomEdge\":\n      return self._sampleLocationOnEdge(top=False)\n    elif feature == \"side\":\n      return self._sampleLocationOnSide()\n    elif feature == \"random\":\n      return self.sampleLocation()\n    else:\n      raise NameError(\"No such feature in {}: {}\".format(self, feature))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sampleLocationOnDisc(self, top=None):\n    if top is None:\n      z = random.choice([-1, 1]) * self.height / 2.\n    else:\n      z = self.height / 2. if top else - self.height / 2.\n    sampledAngle = 2 * random.random() * pi\n    sampledRadius = self.radius * sqrt(random.random())\n    x, y = sampledRadius * cos(sampledAngle), sampledRadius * sin(sampledAngle)\n    return [x, y, z]", "response": "Helper method to sample from top and bottom discs of a cylinder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sampleLocationOnEdge(self, top=None):\n    if top is None:\n      z = random.choice([-1, 1]) * self.height / 2.\n    else:\n      z = self.height / 2. if top else - self.height / 2.\n    sampledAngle = 2 * random.random() * pi\n    x, y = self.radius * cos(sampledAngle), self.radius * sin(sampledAngle)\n    return [x, y, z]", "response": "Helper method to sample from the top and bottom edges of a cylinder."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sampleLocationOnSide(self):\n    z = random.uniform(-1, 1) * self.height / 2.\n    sampledAngle = 2 * random.random() * pi\n    x, y = self.radius * cos(sampledAngle), self.radius * sin(sampledAngle)\n    return [x, y, z]", "response": "Helper method to sample from the lateral surface of a cylinder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot(self, numPoints=100):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # generate cylinder\n    x = np.linspace(- self.radius, self.radius, numPoints)\n    z = np.linspace(- self.height / 2., self.height / 2., numPoints)\n    Xc, Zc = np.meshgrid(x, z)\n    Yc = np.sqrt(self.radius ** 2 - Xc ** 2)\n\n    # plot\n    ax.plot_surface(Xc, Yc, Zc, alpha=0.2, rstride=20, cstride=10)\n    ax.plot_surface(Xc, -Yc, Zc, alpha=0.2, rstride=20, cstride=10)\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_zlabel(\"Z\")\n\n    plt.title(\"{}\".format(self))\n    return fig, ax", "response": "Specific plotting method for cylinders."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getFeatureID(self, location):\n    if not self.contains(location):\n      return self.EMPTY_FEATURE  # random feature\n\n    numFaces = sum(\n      [self.almostEqual(abs(coord), self.dimensions[i] / 2.) \\\n       for i, coord in enumerate(location)]\n    )\n\n    if numFaces == 1:\n      return self.FLAT\n    if numFaces == 2:\n      return self.EDGE\n    if numFaces == 3:\n      return self.POINTY", "response": "Returns the ID of the feature that is available for a given location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef contains(self, location):\n    for i, coord in enumerate(location):\n      if self.almostEqual(abs(coord), self.dimensions[i] / 2.):\n        return True\n    return False", "response": "A location is on the box if one of the dimension is satured."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sampleLocationFromFeature(self, feature):\n    if feature == \"face\":\n      return self._sampleFromFaces()\n    elif feature == \"edge\":\n      return self._sampleFromEdges()\n    elif feature == \"vertex\":\n      return self._sampleFromVertices()\n    elif feature == \"random\":\n      return self.sampleLocation()\n    else:\n      raise NameError(\"No such feature in {}: {}\".format(self, feature))", "response": "Samples a location from one specific feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sampleFromFaces(self):\n    coordinates = [random.uniform(-1, 1) * dim / 2. for dim in self.dimensions]\n    dim = random.choice(range(self.dimension))\n    coordinates[dim] = self.dimensions[dim] / 2. * random.choice([-1, 1])\n    return coordinates", "response": "We start by sampling a dimension to \"max out\", then sample the sign and\n    the other dimensions' values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sampleFromEdges(self):\n    dimensionsToChooseFrom = range(self.dimension)\n    random.shuffle(dimensionsToChooseFrom)\n    dim1, dim2 = dimensionsToChooseFrom[0], dimensionsToChooseFrom[1]\n    coordinates = [random.uniform(-1, 1) * dim / 2. for dim in self.dimensions]\n    coordinates[dim1] = self.dimensions[dim1] / 2. * random.choice([-1, 1])\n    coordinates[dim2] = self.dimensions[dim2] / 2. * random.choice([-1, 1])\n    return coordinates", "response": "We start by sampling dimensions to \"max out\", then sample the sign and\n    the other dimensions' values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot(self, numPoints=100):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # generate cylinder\n    x = np.linspace(- self.dimensions[0]/2., self.dimensions[0]/2., numPoints)\n    y = np.linspace(- self.dimensions[1]/2., self.dimensions[1]/2., numPoints)\n    z = np.linspace(- self.dimensions[2]/2., self.dimensions[2]/2., numPoints)\n\n    # plot\n    Xc, Yc = np.meshgrid(x, y)\n    ax.plot_surface(Xc, Yc, -self.dimensions[2]/2,\n                    alpha=0.2, rstride=20, cstride=10)\n    ax.plot_surface(Xc, Yc, self.dimensions[2]/2,\n                    alpha=0.2, rstride=20, cstride=10)\n    Yc, Zc = np.meshgrid(y, z)\n    ax.plot_surface(-self.dimensions[0]/2, Yc, Zc,\n                    alpha=0.2, rstride=20, cstride=10)\n    ax.plot_surface(self.dimensions[0]/2, Yc, Zc,\n                    alpha=0.2, rstride=20, cstride=10)\n    Xc, Zc = np.meshgrid(x, z)\n    ax.plot_surface(Xc, -self.dimensions[1]/2, Zc,\n                    alpha=0.2, rstride=20, cstride=10)\n    ax.plot_surface(Xc, self.dimensions[1]/2, Zc,\n                    alpha=0.2, rstride=20, cstride=10)\n\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_zlabel(\"Z\")\n\n    plt.title(\"{}\".format(self))\n    return fig, ax", "response": "Specific plotting method for boxes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getFeatureID(self, location):\n    truthyFeature = self.contains(location)\n    if not truthyFeature:\n      return self.EMPTY_FEATURE\n    elif truthyFeature=='face':\n      return self.FLAT\n    elif truthyFeature=='vertex':\n      return self.POINTY\n    elif truthyFeature==\"edge\":\n      return self.EDGE\n    elif truthyFeature==\"surface\":\n      return self.SURFACE\n    else:\n      return self.EMPTY_FEATURE", "response": "Returns the feature index associated with the provided location."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains(self, location):\n    for vertex in self.vertices:\n      V = np.array((vertex['x'], vertex['y'], vertex['z'])).T\n      if np.allclose(location, V, rtol=1.e-3):\n        return \"vertex\"\n    for face in self.faces:\n      edges = np.choose(np.array(list(combinations(range(3),2))), self.vertices[face])\n      for edge in edges:\n        if self._containsOnEdge(location, edge):\n          return \"edge\"\n      if self._containsOnFace(location,face):\n        return \"face\"\n    return False", "response": "Checks that the provided point is on the model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsample a location from the provided feature.", "response": "def sampleLocationFromFeature(self, feature):\n    \"\"\"\n    Samples a location from the provided specific feature.\n\n    TODO surface feature is not handled correctly.\n    forwarded to sample from face instead.\n    \"\"\"\n    if feature == \"surface\":\n      return self.sampleLocationFromFeature('face') # Temporary workaround for surfaces\n    elif feature==\"face\":\n      indx = self.rng.choice(range(self.faces.count))\n      rndFace = self.faces[indx]\n      return self._sampleLocationOnFace(rndFace)\n\n    elif feature == \"edge\":\n      indx = self.rng.choice(range(self.faces.count))\n      rndFace = self.faces[indx]\n      rndVertices = self.rng.sample(self.vertices[rndFace],2)\n      return self._sampleLocationOnEdge(rndVertices)\n\n    elif feature == \"vertex\":\n      rndVertexIndx = self.rng.choice(range(self.vertices.count))\n      return np.array(self.vertices[rndVertexIndx].tolist())\n\n    elif feature == \"surface\":\n      return self.sampleLocationFromFeature(\"face\")      # Temporary workaround for surfaces\n    elif feature == \"random\":\n      return self.sampleLocation()\n    else:\n      raise NameError(\"No such feature in {}: {}\".format(self, feature))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvisualizes a set of complex objects.", "response": "def visualize(self, numPoints=100):\n    \"\"\"\n    Visualization utility for models.\n    Helps to debug the math and logic. \n    Helps to monitor complex objects with difficult to define boundaries.\n\n    Only supports 3-dimensional objects.\n    TODO: center the objects using scale, rotate and translate operations on mesh objects.\n    \"\"\"\n    try:\n      import pyqtgraph as pg\n      import pyqtgraph.multiprocess as mp\n      import pyqtgraph.opengl as gl\n    except ImportError as e:\n      print(\"PyQtGraph needs to be installed.\")\n      return (None, None, None, None, None)\n\n    class PlyVisWindow:\n      \"\"\"\n      The pyqtgraph visualization utility window class\n\n      Creates a remote process with viewbox frame for visualizations\n      Provided access to mesh and scatter for realtime update to view. \n      \"\"\"\n      def __init__(self): \n        self.proc = mp.QtProcess()\n        self.rpg = self.proc._import('pyqtgraph')\n        self.rgl = self.proc._import('pyqtgraph.opengl')\n        self.rview = self.rgl.GLViewWidget()\n        self.rview.setBackgroundColor('k')\n        self.rview.setCameraPosition(distance=10)\n        self.grid = self.rgl.GLGridItem()\n        self.rview.addItem(self.grid)\n        self.rpg.setConfigOption('background', 'w')\n        self.rpg.setConfigOption('foreground', 'k')\n\n      def snapshot(self, name=\"\"):\n        \"\"\"\n        utility to grabframe of the visualization window.\n\n        @param name (string) helps to avoid overwriting grabbed images programmatically.\n        \"\"\"\n        self.rview.grabFrameBuffer().save(\"{}.png\".format(name))\n\n    \n    # We might need this for future purposes Dont Delete\n    # class MeshUpdate:\n    #     def __init__(self, proc):\n    #         self.data_x = proc.transfer([])\n    #         self.data_y = proc.transfer([])\n    #         self._t = None\n\n    #     @property\n    #     def t(self):\n    #         return self._t\n        \n    #     def update(self,x):\n    #         self.data_y.extend([x], _callSync='async')\n    #         self.data_x.extend([self.t], _callSync='async',)\n    #         self.curve.setData(y=self.data_y, _callSync='async')\n    \n    pg.mkQApp()\n    self.graphicsWindow = PlyVisWindow()\n    self.graphicsWindow.rview.setWindowTitle(self.file)\n    vertices = self.vertices.data\n    vertices = np.array(vertices.tolist())\n    faces = np.array([self.faces[i]['vertex_indices'] for i in  range(self.faces.count)])\n    self.mesh = self.graphicsWindow.rgl.GLMeshItem(vertexes=vertices, faces=faces, \n                    shader='normalColor', drawEdges=True, \n                    drawFaces=True, computeNormals=False, \n                    smooth=False)\n    self.graphicsWindow.rview.addItem(self.mesh)\n    self.graphicsWindow.rview.show()\n    pos = np.empty((numPoints,3))\n    size = np.ones((numPoints,))\n    color = np.ones((numPoints,4))\n    self.scatter = self.graphicsWindow.rgl.GLScatterPlotItem(pos=pos, size=size, color=color, pxMode=True)\n    self.graphicsWindow.rview.addItem(self.scatter)\n    return self.scatter, self.mesh, pos, size, color"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculateFirstDifference(filePath, outputFilePath):\n    predictedField = SWARM_CONFIG['inferenceArgs']['predictedField']\n\n    data = pd.read_csv(filePath, header=0, skiprows=[1,2])\n    predictedFieldVals = data[predictedField].astype('float')\n    firstDifference = predictedFieldVals.diff()\n    data[predictedField] = firstDifference\n\n    inputFile = open(filePath, \"r\")\n    outputFile = open(outputFilePath, \"w\")\n    csvReader = csv.reader(inputFile)\n    csvWriter = csv.writer(outputFile)\n    # write headlines\n    for _ in xrange(3):\n      readrow = csvReader.next()\n      csvWriter.writerow(readrow)\n    for i in xrange(len(data)):\n      csvWriter.writerow(list(data.iloc[i]))\n\n    inputFile.close()\n    outputFile.close()", "response": "Create an auxiliary data file that contains first difference of predicted field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addObject(self, featureIndices, name=None):\n    if name is None:\n      name = len(self.objects)\n\n    sequence = []\n    for f in featureIndices: sequence += [(0, f,)]\n\n    self.objects[name] = sequence", "response": "Adds a sequence of feature indices to the Machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a set of random sequences each with sequenceLength elements and adds them to the machine.", "response": "def createRandomSequences(self,\n                          numSequences,\n                          sequenceLength):\n    \"\"\"\n    Creates a set of random sequences, each with sequenceLength elements,\n    and adds them to the machine.\n    \"\"\"\n    for _ in xrange(numSequences):\n      self.addObject(\n        [numpy.random.randint(0, self.numFeatures)\n                                            for _ in xrange(sequenceLength)]\n      )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _getSDRPairs(self, pairs, noise=None, includeRandomLocation=False):\n    sensations = {}\n    for col in xrange(self.numColumns):\n      locationID, featureID = pairs[col]\n\n      # generate random location if requested\n      if includeRandomLocation:\n        locationID = random.randint(0, self.numLocations-1)\n        location = self.locations[col][locationID]\n\n      # generate union of locations if requested\n      elif isinstance(locationID, tuple):\n        location = set()\n        for idx in list(locationID):\n          location = location | self.locations[col][idx]\n      else:\n        location = self.locations[col][locationID]\n\n      # generate empty feature if requested\n      if featureID == -1:\n        feature = set()\n      # generate union of features if requested\n      elif isinstance(featureID, tuple):\n        feature = set()\n        for idx in list(featureID):\n          feature = feature | self.features[col][idx]\n      else:\n        feature = self.features[col][featureID]\n\n      if noise is not None:\n        location = self._addNoise(location, noise, self.externalInputSize)\n        feature = self._addNoise(feature, noise, self.sensorInputSize)\n\n      sensations[col] = (location, feature)\n\n    return sensations", "response": "This method generates a sensation dictionary for a list of location and feature index pairs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd noise to the given pattern and returns the new set.", "response": "def _addNoise(self, pattern, noiseLevel, inputSize):\n    \"\"\"\n    Adds noise to the given pattern and returns the new one.\n\n    A noiseLevel of 0.1 means that 10% of the ON bits will be replaced by\n    other randomly chosen ON bits.  The returned SDR will still contain the\n    same number of bits.\n\n    \"\"\"\n    if pattern is None:\n      return None\n\n    # Bits that could be noise. These can't be from the original set.\n    candidateBits = list(set(range(inputSize)) - set(pattern))\n    random.shuffle(candidateBits)\n\n    newBits = set()\n    for bit in pattern:\n      if random.random() < noiseLevel:\n        newBits.add(candidateBits.pop())\n      else:\n        newBits.add(bit)\n\n    return newBits"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a pool of features to be used for the experiments.", "response": "def _generateFeatures(self):\n    \"\"\"\n    Generates a pool of features to be used for the experiments.\n\n    For each index, numColumns SDR's are created, as locations for the same\n    feature should be different for each column.\n    \"\"\"\n    size = self.sensorInputSize\n    bits = self.numInputBits\n\n    self.features = []\n    for _ in xrange(self.numColumns):\n      self.features.append(\n        [self._generatePattern(bits, size) for _ in xrange(self.numFeatures)]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a simple experiment where three objects share a number of location and features.", "response": "def runSharedFeatures(noiseLevel=None, profile=False):\n  \"\"\"\n  Runs a simple experiment where three objects share a number of location,\n  feature pairs.\n\n  Parameters:\n  ----------------------------\n  @param    noiseLevel (float)\n            Noise level to add to the locations and features during inference\n\n  @param    profile (bool)\n            If True, the network will be profiled after learning and inference\n\n  \"\"\"\n  exp = L4L2Experiment(\n    \"shared_features\",\n    enableLateralSP=True,\n    enableFeedForwardSP=True\n  )\n\n  pairs = createThreeObjects()\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=1024,\n    externalInputSize=1024\n  )\n  for object in pairs:\n    objects.addObject(object)\n\n  exp.learnObjects(objects.provideObjectsToLearn())\n  if profile:\n    exp.printProfile()\n\n  inferConfig = {\n    \"numSteps\": 10,\n    \"noiseLevel\": noiseLevel,\n    \"pairs\": {\n      0: zip(range(10), range(10))\n    }\n  }\n\n  exp.infer(objects.provideObjectToInfer(inferConfig), objectName=0)\n  if profile:\n    exp.printProfile()\n\n  exp.plotInferenceStats(\n    fields=[\"L2 Representation\",\n            \"Overlap L2 with object\",\n            \"L4 Representation\"],\n  )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef runStretchExperiment(numObjects=25):\n  exp = L4L2Experiment(\n    \"profiling_experiment\",\n    enableLateralSP = True,\n    enableFeedForwardSP=True\n  )\n\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=1024,\n    externalInputSize=1024\n  )\n  objects.createRandomObjects(numObjects=numObjects, numPoints=10)\n  exp.learnObjects(objects.provideObjectsToLearn())\n  exp.printProfile()\n\n  inferConfig = {\n    \"numSteps\": len(objects[0]),\n    \"pairs\": {\n      0: objects[0]\n    }\n  }\n\n  exp.infer(objects.provideObjectToInfer(inferConfig), objectName=0)\n  exp.printProfile()\n\n  exp.plotInferenceStats(\n    fields=[\"L2 Representation\",\n            \"Overlap L2 with object\",\n            \"L4 Representation\"]\n  )", "response": "Runs a stretch experiment on the object machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses the boost strength to compute a boost factor for each unit represented in x. These factors are used to increase the impact of each unit to improve their chances of being chosen. This encourages participation of more columns in the learning process. The boosting function is a curve defined as: boostFactors = exp[ - boostStrength * (dutyCycle - targetDensity)] Intuitively this means that units that have been active (i.e. in the top-k) at the target activation level have a boost factor of 1, meaning their activity is not boosted. Columns whose duty cycle drops too much below that of their neighbors are boosted depending on how infrequently they have been active. Unit that has been active more than the target activation level have a boost factor below 1, meaning their activity is suppressed and they are less likely to be in the top-k. Note that we do not transmit the boosted values. We only use boosting to determine the winning units. The target activation density for each unit is k / number of units. The boostFactor depends on the dutyCycle via an exponential function: boostFactor ^ | |\\ | \\ 1 _ | \\ | _ | _ _ | _ _ _ _ +--------------------> dutyCycle | targetDensity :param ctx: Place where we can store information we will need to compute the gradients for the backward pass. :param x: Current activity of each unit. :param dutyCycles: The averaged duty cycle of each unit. :param k: The activity of the top k units will be allowed to remain, the rest are set to zero. :param boostStrength: A boost strength of 0.0 has no effect on x. :return: A tensor representing the activity of x after k-winner take all.", "response": "def forward(ctx, x, dutyCycles, k, boostStrength):\n    \"\"\"\n    Use the boost strength to compute a boost factor for each unit represented\n    in x. These factors are used to increase the impact of each unit to improve\n    their chances of being chosen. This encourages participation of more columns\n    in the learning process.\n\n    The boosting function is a curve defined as: boostFactors = exp[ -\n    boostStrength * (dutyCycle - targetDensity)] Intuitively this means that\n    units that have been active (i.e. in the top-k) at the target activation\n    level have a boost factor of 1, meaning their activity is not boosted.\n    Columns whose duty cycle drops too much below that of their neighbors are\n    boosted depending on how infrequently they have been active. Unit that has\n    been active more than the target activation level have a boost factor below\n    1, meaning their activity is suppressed and they are less likely to be in \n    the top-k.\n\n    Note that we do not transmit the boosted values. We only use boosting to\n    determine the winning units.\n\n    The target activation density for each unit is k / number of units. The\n    boostFactor depends on the dutyCycle via an exponential function:\n\n            boostFactor\n                ^\n                |\n                |\\\n                | \\\n          1  _  |  \\\n                |    _\n                |      _ _\n                |          _ _ _ _\n                +--------------------> dutyCycle\n                   |\n              targetDensity\n\n    :param ctx: \n      Place where we can store information we will need to compute the gradients\n      for the backward pass.\n\n    :param x: \n      Current activity of each unit.  \n\n    :param dutyCycles: \n      The averaged duty cycle of each unit.\n\n    :param k: \n      The activity of the top k units will be allowed to remain, the rest are\n      set to zero.\n                \n    :param boostStrength:     \n      A boost strength of 0.0 has no effect on x.\n\n    :return: \n      A tensor representing the activity of x after k-winner take all.\n    \"\"\"\n    if boostStrength > 0.0:\n      targetDensity = float(k) / x.size(1)\n      boostFactors = torch.exp((targetDensity - dutyCycles) * boostStrength)\n      boosted = x.detach() * boostFactors\n    else:\n      boosted = x.detach()\n\n    # Take the boosted version of the input x, find the top k winners.\n    # Compute an output that contains the values of x corresponding to the top k\n    # boosted values\n    res = torch.zeros_like(x)\n    topk, indices = boosted.topk(k, sorted=False)\n    for i in range(x.shape[0]):\n      res[i, indices[i]] = x[i, indices[i]]\n\n    ctx.save_for_backward(indices)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the gradients of the k - winners of the input x.", "response": "def forward(ctx, x, dutyCycles, k, boostStrength):\n    \"\"\"\n    Use the boost strength to compute a boost factor for each unit represented\n    in x. These factors are used to increase the impact of each unit to improve\n    their chances of being chosen. This encourages participation of more columns\n    in the learning process. See :meth:`k_winners.forward` for more details.\n\n    :param ctx:\n      Place where we can store information we will need to compute the gradients\n      for the backward pass.\n\n    :param x:\n      Current activity of each unit.\n\n    :param dutyCycles:\n      The averaged duty cycle of each unit.\n\n    :param k:\n      The activity of the top k units will be allowed to remain, the rest are\n      set to zero.\n\n    :param boostStrength:\n      A boost strength of 0.0 has no effect on x.\n\n    :return:\n      A tensor representing the activity of x after k-winner take all.\n    \"\"\"\n    batchSize = x.shape[0]\n    if boostStrength > 0.0:\n      targetDensity = float(k) / (x.shape[1] * x.shape[2] * x.shape[3])\n      boostFactors = torch.exp((targetDensity - dutyCycles) * boostStrength)\n      boosted = x.detach() * boostFactors\n    else:\n      boosted = x.detach()\n\n    # Take the boosted version of the input x, find the top k winners.\n    # Compute an output that only contains the values of x corresponding to the top k\n    # boosted values. The rest of the elements in the output should be 0.\n    boosted = boosted.reshape((batchSize, -1))\n    xr = x.reshape((batchSize, -1))\n    res = torch.zeros_like(boosted)\n    topk, indices = boosted.topk(k, dim=1, sorted=False)\n    res.scatter_(1, indices, xr.gather(1, indices))\n    res = res.reshape(x.shape)\n\n    ctx.save_for_backward(indices)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot(self, numPoints=100):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    for feature in self._FEATURES:\n\n      for _ in xrange(numPoints):\n        x, y, z = tuple(self.sampleLocationFromFeature(feature))\n        ax.scatter(x, y, z, marker=\".\")\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    plt.title(\"{}\".format(self))\n    return fig, ax", "response": "Plots the object in a 3D scatter plot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef printSequence(x, formatString=\"%d\"):\n  numElements = len(x)\n  s = \"\"\n  for j in range(numElements):\n    s += formatString % x[j]\n  print s", "response": "Print a list or numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint a bunch of sequences stored in a 2D numpy array.", "response": "def printSequences(x, formatString=\"%d\"):\n  \"\"\"\n  Print a bunch of sequences stored in a 2D numpy array.\n  \"\"\"\n  [seqLen, numElements] = x.shape\n  for i in range(seqLen):\n    s = \"\"\n    for j in range(numElements):\n      s += formatString % x[i][j]\n    print s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self, useRandomEncoder):\n    self.setRandomSeed(self.seed)\n\n    self.dim = numpy.shape(self.spatialConfig)[-1]\n\n    self.spatialMap = dict( zip( map(tuple, list(self.spatialConfig)),\n                              self.sensoryInputElements))\n\n    self.lengthMotorInput1D = (2*self.maxDisplacement + 1) * \\\n                                                    self.numActiveBitsMotorInput\n\n    uniqueSensoryElements = list(set(self.sensoryInputElementsPool))\n\n    if useRandomEncoder:\n      self.sensoryEncoder = SDRCategoryEncoder(n=1024,\n                                w=self.numActiveBitsSensoryInput,\n                                categoryList=uniqueSensoryElements,\n                                forced=True)\n      self.lengthSensoryInput = self.sensoryEncoder.getWidth()\n\n    else:\n      self.lengthSensoryInput = (len(self.sensoryInputElementsPool)+1) * \\\n                                          self.numActiveBitsSensoryInput\n\n      self.sensoryEncoder = CategoryEncoder(w=self.numActiveBitsSensoryInput,\n                            categoryList=uniqueSensoryElements, forced=True)\n\n    motorEncoder1D =  ScalarEncoder(n=self.lengthMotorInput1D,\n                                    w=self.numActiveBitsMotorInput,\n                                    minval=-self.maxDisplacement,\n                                    maxval=self.maxDisplacement,\n                                    clipInput=True,\n                                    forced=True)\n\n    self.motorEncoder = VectorEncoder(length=self.dim, encoder=motorEncoder1D)", "response": "Initialize the various data structures."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generateSensorimotorSequence(self, sequenceLength):\n    motorSequence = []\n    sensorySequence = []\n    sensorimotorSequence = []\n    currentEyeLoc = self.nupicRandomChoice(self.spatialConfig)\n\n    for i in xrange(sequenceLength):\n\n      currentSensoryInput = self.spatialMap[tuple(currentEyeLoc)]\n\n      nextEyeLoc, currentEyeV = self.getNextEyeLocation(currentEyeLoc)\n\n      if self.verbosity:\n          print \"sensory input = \", currentSensoryInput, \\\n            \"eye location = \", currentEyeLoc, \\\n            \" motor command = \", currentEyeV\n\n      sensoryInput = self.encodeSensoryInput(currentSensoryInput)\n      motorInput = self.encodeMotorInput(list(currentEyeV))\n      sensorimotorInput = numpy.concatenate((sensoryInput, motorInput))\n\n      sensorySequence.append(sensoryInput)\n      motorSequence.append(motorInput)\n      sensorimotorSequence.append(sensorimotorInput)\n\n      currentEyeLoc = nextEyeLoc\n\n    return (sensorySequence, motorSequence, sensorimotorSequence)", "response": "Generates sensorimotor sequences of length sequenceLength."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes sensorimotor input given the eye movements.", "response": "def encodeSensorimotorSequence(self, eyeLocs):\n    \"\"\"\n    Encode sensorimotor sequence given the eye movements. Sequence will have\n    length len(eyeLocs) - 1 because only the differences of eye locations can be\n    used to encoder motor commands.\n\n    @param eyeLocs  (list)\n        Numpy coordinates describing where the eye is looking.\n\n    @return (tuple) Contains:\n            sensorySequence       (list)\n                Encoded sensory input for whole sequence.\n\n            motorSequence         (list)\n                Encoded motor input for whole sequence.\n\n            sensorimotorSequence  (list)\n                Encoder sensorimotor input for whole sequence. This is useful\n                when you want to give external input to temporal memory.\n    \"\"\"\n    sequenceLength = len(eyeLocs) - 1\n\n    motorSequence = []\n    sensorySequence = []\n    sensorimotorSequence = []\n\n    for i in xrange(sequenceLength):\n      currentEyeLoc = eyeLocs[i]\n      nextEyeLoc = eyeLocs[i+1]\n\n      currentSensoryInput = self.spatialMap[currentEyeLoc]\n\n      currentEyeV = nextEyeLoc - currentEyeLoc\n\n      if self.verbosity:\n        print \"sensory input = \", currentSensoryInput, \\\n            \"eye location = \", currentEyeLoc, \\\n            \" motor command = \", currentEyeV\n\n      sensoryInput = self.encodeSensoryInput(currentSensoryInput)\n      motorInput = self.encodeMotorInput(list(currentEyeV))\n      sensorimotorInput = numpy.concatenate((sensoryInput, motorInput))\n\n      sensorySequence.append(sensoryInput)\n      motorSequence.append(motorInput)\n      sensorimotorSequence.append(sensorimotorInput)\n\n    return (sensorySequence, motorSequence, sensorimotorSequence)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getNextEyeLocation(self, currentEyeLoc):\n    possibleEyeLocs = []\n    for loc in self.spatialConfig:\n      shift = abs(max(loc - currentEyeLoc))\n      if self.minDisplacement <= shift <= self.maxDisplacement:\n        possibleEyeLocs.append(loc)\n\n    nextEyeLoc = self.nupicRandomChoice(possibleEyeLocs)\n\n    eyeDiff = nextEyeLoc  - currentEyeLoc\n\n    return nextEyeLoc, eyeDiff", "response": "Generate next eye location based on current eye location."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setRandomSeed(self, seed):\n    self.seed = seed\n    self._random = Random()\n    self._random.setSeed(seed)", "response": "Reset the nupic random generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes motor command to bit vector.", "response": "def encodeMotorInput(self, motorInput):\n    \"\"\"\n    Encode motor command to bit vector.\n\n    @param motorInput (1D numpy.array)\n        Motor command to be encoded.\n\n    @return           (1D numpy.array)\n        Encoded motor command.\n    \"\"\"\n    if not hasattr(motorInput, \"__iter__\"):\n      motorInput = list([motorInput])\n\n    return self.motorEncoder.encode(motorInput)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding motor command from bit vector.", "response": "def decodeMotorInput(self, motorInputPattern):\n    \"\"\"\n    Decode motor command from bit vector.\n\n    @param motorInputPattern (1D numpy.array)\n        Encoded motor command.\n\n    @return                  (1D numpy.array)\n        Decoded motor command.\n\n    \"\"\"\n    key = self.motorEncoder.decode(motorInputPattern)[0].keys()[0]\n    motorCommand = self.motorEncoder.decode(motorInputPattern)[0][key][1][0]\n    return motorCommand"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef printSensoryCodingScheme(self):\n    print \"\\nsensory coding scheme: \"\n    for loc in self.spatialConfig:\n      sensoryElement = self.spatialMap[tuple(loc)]\n      print sensoryElement, \"%s : \" % loc,\n      printSequence(self.encodeSensoryInput(sensoryElement))", "response": "Print the sensory inputs along with their encoded versions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getDefaultTMParams(self, inputSize, numInputBits):\n    sampleSize = int(1.5 * numInputBits)\n\n    if numInputBits == 20:\n      activationThreshold = 18\n      minThreshold = 18\n    elif numInputBits == 10:\n      activationThreshold = 8\n      minThreshold = 8\n    else:\n      activationThreshold = int(numInputBits * .6)\n      minThreshold = activationThreshold\n\n    return {\n      \"columnCount\": inputSize,\n      \"cellsPerColumn\": 16,\n      \"learn\": True,\n      \"learnOnOneCell\": False,\n      \"initialPermanence\": 0.41,\n      \"connectedPermanence\": 0.6,\n      \"permanenceIncrement\": 0.1,\n      \"permanenceDecrement\": 0.03,\n      \"minThreshold\": minThreshold,\n      \"basalPredictedSegmentDecrement\": 0.003,\n      \"apicalPredictedSegmentDecrement\": 0.0,\n      \"reducedBasalThreshold\": int(activationThreshold*0.6),\n      \"activationThreshold\": activationThreshold,\n      \"sampleSize\": sampleSize,\n      \"implementation\": \"ApicalTiebreak\",\n      \"seed\": self.seed\n    }", "response": "Returns a good default set of parameters to use in the TM region."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef averageSequenceAccuracy(self, minOverlap, maxOverlap,\n                              firstStat=0, lastStat=None):\n    \"\"\"\n    For each object, decide whether the TM uniquely classified it by checking\n    that the number of predictedActive cells are in an acceptable range.\n    \"\"\"\n    numCorrectSparsity = 0.0\n    numCorrectClassifications = 0.0\n    numStats = 0.0\n\n    # For each object or sequence we classify every point or element\n    #\n    # A sequence element is considered correctly classified only if the number\n    # of predictedActive cells is within a reasonable range and if the KNN\n    # Classifier correctly classifies the active cell representation as\n    # belonging to this sequence.\n    #\n    # A point on an object is considered correctly classified by the TM if the\n    # number of predictedActive cells is within range.\n    for stats in self.statistics[firstStat:lastStat]:\n\n      # Keep running total of how often the number of predictedActive cells are\n      # in the range.  We always skip the first (unpredictable) count.\n      predictedActiveStat = stats[\"TM PredictedActive C0\"][1:]\n      TMRepresentationStat = stats[\"TM Full Representation C0\"][1:]\n      # print \"\\n-----------\"\n      # print stats[\"object\"], predictedActiveStat\n      for numCells,sdr in zip(predictedActiveStat, TMRepresentationStat):\n        numStats += 1.0\n        # print \"numCells: \", numCells\n        if numCells in range(minOverlap, maxOverlap + 1):\n          numCorrectSparsity += 1.0\n\n          # Check KNN Classifier\n          sdr = list(sdr)\n          sdr.sort()\n          dense = numpy.zeros(self.numTMCells)\n          dense[sdr] = 1.0\n          (winner, inferenceResult, dist, categoryDist) = \\\n            self.classifier.infer(dense)\n          # print sdr, winner, stats['object'], winner == stats['object']\n          # print categoryDist\n          # print\n\n          if winner == stats['object']:\n            numCorrectClassifications += 1.0\n\n    if numStats==0:\n      return 0.0, 0.0\n\n    return ((numCorrectSparsity / numStats),\n            (numCorrectClassifications / numStats) )", "response": "This function calculates the average accuracy of a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _unsetLearningMode(self):\n    for region in self.TMRegions:\n      region.setParameter(\"learn\", False)\n    super(L4TMExperiment, self)._unsetLearningMode()", "response": "Unsets the learning mode to start inference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the learning mode of the L4TMExperiment.", "response": "def _setLearningMode(self):\n    \"\"\"\n    Sets the learning mode.\n    \"\"\"\n    for region in self.TMRegions:\n      region.setParameter(\"learn\", True)\n    super(L4TMExperiment, self)._setLearningMode()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _updateInferenceStats(self, statistics, objectName=None):\n    L4Representations = self.getL4Representations()\n    L4PredictedCells = self.getL4PredictedCells()\n    L4PredictedActiveCells = self.getL4PredictedActiveCells()\n    L2Representation = self.getL2Representations()\n    TMPredictedActive = self.getTMPredictedActiveCells()\n    TMNextPredicted = self.getTMNextPredictedCells()\n    TMRepresentation = self.getTMRepresentations()\n\n    for i in xrange(self.numColumns):\n      statistics[\"L4 Representation C\" + str(i)].append(\n        len(L4Representations[i])\n      )\n      statistics[\"L4 Predicted C\" + str(i)].append(\n        len(L4PredictedCells[i])\n      )\n      statistics[\"L4 PredictedActive C\" + str(i)].append(\n        len(L4PredictedActiveCells[i])\n      )\n      statistics[\"L2 Representation C\" + str(i)].append(\n        len(L2Representation[i])\n      )\n      statistics[\"L4 Apical Segments C\" + str(i)].append(\n        len(self.L4Columns[i]._tm.getActiveApicalSegments())\n      )\n      statistics[\"L4 Basal Segments C\" + str(i)].append(\n        len(self.L4Columns[i]._tm.getActiveBasalSegments())\n      )\n      statistics[\"TM Basal Segments C\" + str(i)].append(\n        len(self.TMColumns[i]._tm.getActiveBasalSegments())\n      )\n      statistics[\"TM PredictedActive C\" + str(i)].append(\n        len(TMPredictedActive[i])\n      )\n\n      # The number of cells that are in predictive state as a result of this\n      # input\n      statistics[\"TM NextPredicted C\" + str(i)].append(\n        len(TMNextPredicted[i])\n      )\n\n      # The indices of all active cells in the TM\n      statistics[\"TM Full Representation C\" + str(i)].append(\n        TMRepresentation[i]\n      )\n\n      # The indices of all active cells in the TM\n      statistics[\"L2 Full Representation C\" + str(i)].append(\n        L2Representation[i]\n      )\n\n      # Insert exact TM representation into the classifier if the number of\n      # predictive active cells is potentially unique (otherwise we say it\n      # failed to correctly predict this step).\n      if ( (len(TMPredictedActive[i]) < 1.5*self.numInputBits) and\n             (len(TMPredictedActive[i]) > 0.5*self.numInputBits) ):\n        sdr = list(TMPredictedActive[i])\n        sdr.sort()\n        self.classifier.learn(sdr, objectName, isSparse=self.numTMCells)\n\n      # add true overlap if objectName was provided\n      if objectName in self.objectL2Representations:\n        objectRepresentation = self.objectL2Representations[objectName]\n        statistics[\"Overlap L2 with object C\" + str(i)].append(\n          len(objectRepresentation[i] & L2Representation[i])\n        )", "response": "Updates the inference statistics for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef createL4L2TMColumn(network, networkConfig, suffix=\"\"):\n\n  externalInputName = \"externalInput\" + suffix\n  sensorInputName = \"sensorInput\" + suffix\n  L4ColumnName = \"L4Column\" + suffix\n  L2ColumnName = \"L2Column\" + suffix\n  TMColumnName = \"TMColumn\" + suffix\n\n  L4Params = copy.deepcopy(networkConfig[\"L4Params\"])\n  L4Params[\"basalInputWidth\"] = networkConfig[\"externalInputSize\"]\n  L4Params[\"apicalInputWidth\"] = networkConfig[\"L2Params\"][\"cellCount\"]\n\n  if networkConfig[\"externalInputSize\"] > 0:\n      network.addRegion(\n        externalInputName, \"py.RawSensor\",\n        json.dumps({\"outputWidth\": networkConfig[\"externalInputSize\"]}))\n  network.addRegion(\n    sensorInputName, \"py.RawSensor\",\n    json.dumps({\"outputWidth\": networkConfig[\"sensorInputSize\"]}))\n\n  network.addRegion(L4ColumnName, \"py.ApicalTMPairRegion\", json.dumps(L4Params))\n  network.addRegion(TMColumnName,\n                    \"py.ApicalTMSequenceRegion\",\n                    json.dumps(networkConfig[\"TMParams\"]))\n  network.addRegion(L2ColumnName,\n                    \"py.ColumnPoolerRegion\",\n                    json.dumps(networkConfig[\"L2Params\"]))\n\n  # Set phases appropriately so regions are executed in the proper sequence\n  # This is required for multiple columns - the order of execution is not the\n  # same as the order of region creation.\n  if networkConfig[\"externalInputSize\"] > 0:\n      network.setPhases(externalInputName,[0])\n  network.setPhases(sensorInputName,[0])\n\n  # L4 and L2 regions always have phases 2 and 3, respectively\n  # TM region has same phase as L4 since they do not interconnect\n  network.setPhases(L4ColumnName,[2])\n  network.setPhases(TMColumnName,[2])\n  network.setPhases(L2ColumnName,[3])\n\n  # Link sensors to L4\n  network.link(externalInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"basalInput\")\n  network.link(externalInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"basalGrowthCandidates\")\n  network.link(sensorInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"activeColumns\")\n\n  # Link main inputs to TM\n  network.link(sensorInputName, TMColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"activeColumns\")\n\n  # Link L4 to L2\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"feedforwardInput\")\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"predictedActiveCells\",\n               destInput=\"feedforwardGrowthCandidates\")\n\n  # Link L2 feedback to L4\n  if networkConfig[\"enableFeedback\"]:\n    network.link(L2ColumnName, L4ColumnName, \"UniformLink\", \"\",\n                 srcOutput=\"feedForwardOutput\", destInput=\"apicalInput\",\n                 propagationDelay=1)\n\n  # Link reset output to L2, TM, and L4\n  network.link(sensorInputName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n  network.link(sensorInputName, TMColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n  network.link(sensorInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n\n  enableProfiling(network)\n\n  return network", "response": "Create a single column containing one L4 L2 and one TM."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_movie(fig, update_figure, filename, title, fps=15, dpi=100):\n    FFMpegWriter = manimation.writers['ffmpeg']\n    metadata     = dict(title=title)\n    writer       = FFMpegWriter(fps=fps, metadata=metadata)\n\n    with writer.saving(fig, filename, dpi):\n        t = 0\n        while True:\n            if update_figure(t):\n                writer.grab_frame()\n                t += 1\n            else:\n                break", "response": "Helps us to create a movie."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the Spec for the given ApicalTMSequenceRegion.", "response": "def getSpec(cls):\n    \"\"\"\n    Return the Spec for ApicalTMSequenceRegion.\n    \"\"\"\n\n    spec = {\n      \"description\": ApicalTMSequenceRegion.__doc__,\n      \"singleNodeOnly\": True,\n      \"inputs\": {\n        \"activeColumns\": {\n          \"description\": (\"An array of 0's and 1's representing the active \"\n                          \"minicolumns, i.e. the input to the TemporalMemory\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": True,\n          \"regionLevel\": True,\n          \"isDefaultInput\": True,\n          \"requireSplitterMap\": False\n        },\n        \"resetIn\": {\n          \"description\": (\"A boolean flag that indicates whether\"\n                          \" or not the input vector received in this compute cycle\"\n                          \" represents the first presentation in a\"\n                          \" new temporal sequence.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 1,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False\n        },\n        \"apicalInput\": {\n          \"description\": \"An array of 0's and 1's representing top down input.\"\n          \" The input will be provided to apical dendrites.\",\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False\n        },\n        \"apicalGrowthCandidates\": {\n          \"description\": (\"An array of 0's and 1's representing apical input \"\n                          \"that can be learned on new synapses on apical \"\n                          \"segments. If this input is a length-0 array, the \"\n                          \"whole apicalInput is used.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False\n        },\n      },\n      \"outputs\": {\n        \"nextPredictedCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"cell that is predicted for the next timestep.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False\n        },\n\n        \"predictedActiveCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"cell that transitioned from predicted to active.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False\n        },\n\n        \"activeCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"cell that is currently active.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": True\n        },\n\n        \"winnerCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"'winner' cell in the TM.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False\n        },\n      },\n\n      \"parameters\": {\n        # Input sizes (the network API doesn't provide these during initialize)\n        \"columnCount\": {\n          \"description\": (\"The size of the 'activeColumns' input \" +\n                          \"(i.e. the number of columns)\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"apicalInputWidth\": {\n          \"description\": \"The size of the 'apicalInput' input\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n\n        \"learn\": {\n          \"description\": \"True if the TM should learn.\",\n          \"accessMode\": \"ReadWrite\",\n          \"dataType\": \"Bool\",\n          \"count\": 1,\n          \"defaultValue\": \"true\"\n        },\n        \"cellsPerColumn\": {\n          \"description\": \"Number of cells per column\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"activationThreshold\": {\n          \"description\": (\"If the number of active connected synapses on a \"\n                          \"segment is at least this threshold, the segment \"\n                          \"is said to be active.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"reducedBasalThreshold\": {\n          \"description\": (\"Activation threshold of basal segments for cells \"\n                          \"with active apical segments (with apicalTiebreak \"\n                          \"implementation). \"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"initialPermanence\": {\n          \"description\": \"Initial permanence of a new synapse.\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"connectedPermanence\": {\n          \"description\": (\"If the permanence value for a synapse is greater \"\n                          \"than this value, it is said to be connected.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"minThreshold\": {\n          \"description\": (\"If the number of synapses active on a segment is at \"\n                          \"least this threshold, it is selected as the best \"\n                          \"matching cell in a bursting column.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"sampleSize\": {\n          \"description\": (\"The desired number of active synapses for an \" +\n                          \"active cell\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1\n        },\n        \"learnOnOneCell\": {\n          \"description\": (\"If True, the winner cell for each column will be\"\n                          \" fixed between resets.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Bool\",\n          \"count\": 1,\n          \"defaultValue\": \"false\"\n        },\n        \"maxSynapsesPerSegment\": {\n          \"description\": \"The maximum number of synapses per segment\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1\n        },\n        \"maxSegmentsPerCell\": {\n          \"description\": \"The maximum number of segments per cell\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1\n        },\n        \"permanenceIncrement\": {\n          \"description\": (\"Amount by which permanences of synapses are \"\n                          \"incremented during learning.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"permanenceDecrement\": {\n          \"description\": (\"Amount by which permanences of synapses are \"\n                          \"decremented during learning.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"basalPredictedSegmentDecrement\": {\n          \"description\": (\"Amount by which active permanences of synapses of \"\n                          \"previously predicted but inactive segments are \"\n                          \"decremented.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"apicalPredictedSegmentDecrement\": {\n          \"description\": (\"Amount by which active permanences of synapses of \"\n                          \"previously predicted but inactive segments are \"\n                          \"decremented.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"seed\": {\n          \"description\": \"Seed for the random number generator.\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1\n        },\n        \"implementation\": {\n          \"description\": \"Apical implementation\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Byte\",\n          \"count\": 0,\n          \"constraints\": (\"enum: ApicalTiebreak, ApicalTiebreakCPP, ApicalDependent\"),\n          \"defaultValue\": \"ApicalTiebreakCPP\"\n        },\n      },\n    }\n\n    return spec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the next non - zero sequence of active and predicted cells.", "response": "def compute(self, inputs, outputs):\n    \"\"\"\n    Run one iteration of TM's compute.\n    \"\"\"\n\n    # Handle reset first (should be sent with an empty signal)\n    if \"resetIn\" in inputs:\n      assert len(inputs[\"resetIn\"]) == 1\n      if inputs[\"resetIn\"][0] != 0:\n        # send empty output\n        self._tm.reset()\n        outputs[\"activeCells\"][:] = 0\n        outputs[\"nextPredictedCells\"][:] = 0\n        outputs[\"predictedActiveCells\"][:] = 0\n        outputs[\"winnerCells\"][:] = 0\n        return\n\n    activeColumns = inputs[\"activeColumns\"].nonzero()[0]\n\n    if \"apicalInput\" in inputs:\n      apicalInput = inputs[\"apicalInput\"].nonzero()[0]\n    else:\n      apicalInput = np.empty(0, dtype=\"uint32\")\n\n    if \"apicalGrowthCandidates\" in inputs:\n      apicalGrowthCandidates = inputs[\"apicalGrowthCandidates\"].nonzero()[0]\n    else:\n      apicalGrowthCandidates = apicalInput\n\n    self._tm.compute(activeColumns, apicalInput, apicalGrowthCandidates,\n                     self.learn)\n\n    # Extract the active / predicted cells and put them into binary arrays.\n    outputs[\"activeCells\"][:] = 0\n    outputs[\"activeCells\"][self._tm.getActiveCells()] = 1\n    outputs[\"nextPredictedCells\"][:] = 0\n    outputs[\"nextPredictedCells\"][\n      self._tm.getNextPredictedCells()] = 1\n    outputs[\"predictedActiveCells\"][:] = 0\n    outputs[\"predictedActiveCells\"][\n      self._tm.getPredictedActiveCells()] = 1\n    outputs[\"winnerCells\"][:] = 0\n    outputs[\"winnerCells\"][self._tm.getWinnerCells()] = 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the value of a Spec parameter.", "response": "def setParameter(self, parameterName, index, parameterValue):\n    \"\"\"\n    Set the value of a Spec parameter. Most parameters are handled\n    automatically by PyRegion's parameter set mechanism. The ones that need\n    special treatment are explicitly handled here.\n    \"\"\"\n    if hasattr(self, parameterName):\n      setattr(self, parameterName, parameterValue)\n    else:\n      raise Exception(\"Unknown parameter: \" + parameterName)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef createExperimentArgs():\n  experimentArguments = []\n  # for n in [300, 500, 700, 900, 1100, 1300, 1500, 1700, 1900, 2100, 2300,\n  #           2500, 2700, 2900, 3100, 3300, 3500, 3700, 3900]:\n  for n in [1500, 1700, 1900, 2100]:\n    for a in [128]:\n      # Some parameter combinations are just not worth running!\n      if ( a==64 and n<=1500 ) or ( a==128 and n<= 1900 ) or ( a==256 ):\n        experimentArguments.append(\n          (\"./sdr_calculations2\", \"results_errorbars/temp_\"+str(n)+\"_\"+str(a)+\".csv\",\n           \"200000\", str(n), str(a), \"0\"),\n        )\n  return experimentArguments", "response": "Create the arguments for the experiment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createNoiseExperimentArgs():\n  experimentArguments = []\n  n = 6000\n  for a in [128]:\n    noisePct = 0.75\n    while noisePct <= 0.85:\n      noise = int(round(noisePct*a,0))\n      # Some parameter combinations are just not worth running!\n      experimentArguments.append(\n        (\"./sdr_calculations2\",\n         \"results_noise_10m/temp_\"+str(n)+\"_\"+str(a)+\"_\"+str(noise)+\"_30.csv\",\n         \"200000\", str(n), str(a), str(noise))\n      )\n      noisePct += 0.05\n  return experimentArguments", "response": "Create the arguments for the noise experiment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generateMinicolumnSDRs(n, w, threshold):\n  if not os.path.exists(\"sdrs\"):\n    os.makedirs(\"sdrs\")\n\n  filename = \"sdrs/{}_{}_{}.json\".format(n, w, threshold)\n\n  if len(glob.glob(filename)) > 0:\n    with open(filename, \"r\") as fIn:\n      sdrs = json.load(fIn)\n  else:\n    begin = time.time()\n    sdrs = enumerateDistantSDRsBruteForce(n, w, threshold)\n    end = time.time()\n    with open(filename, \"w\") as fOut:\n      json.dump([sdr.tolist() for sdr in sdrs], fOut)\n      print(\"Saved\", filename)\n    print(\"Elapsed time: {:.2f} seconds\".format(end - begin))\n\n  return sdrs", "response": "This function generates a list of SDRs for a given number of times."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef createEvenlySpreadSDRs(numSDRs, n, w):\n  assert w <= n\n\n  available = np.arange(n)\n  np.random.shuffle(available)\n\n  SDRs = []\n\n  for _ in xrange(numSDRs):\n    selected = available[:w]\n    available = available[w:]\n\n    if available.size == 0:\n      remainderSelected = np.random.choice(\n        np.setdiff1d(np.arange(n), selected),\n        size=(w - selected.size),\n        replace= False)\n      selected = np.append(selected, remainderSelected)\n\n      available = np.setdiff1d(np.arange(n), remainderSelected)\n      np.random.shuffle(available)\n\n    selected.sort()\n    SDRs.append(selected)\n\n  return SDRs", "response": "Create a set of ~random SDRs that use every available bit\n."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses a greedy algorithm to choose how each minicolumn should distribute contexts between its cells. @return (list of lists of lists of ints) iContext integers for each cell, grouped by minicolumn. For example, [[[1, 3], [2,4]], [[1, 2]]] would specify that cell 0 connects to location 1 and location 3, while cell 1 connects to locations 2 and 4, and cell 2 (in the second minicolumn) connects to locations 1 and 2.", "response": "def carefullyCollideContexts(numContexts, numCells, numMinicolumns):\n  \"\"\"\n  Use a greedy algorithm to choose how each minicolumn should distribute\n  contexts between its cells.\n\n  @return (list of lists of lists of ints)\n  iContext integers for each cell, grouped by minicolumn. For example,\n    [[[1, 3], [2,4]],\n     [[1, 2]]]\n  would specify that cell 0 connects to location 1 and location 3, while cell\n  1 connects to locations 2 and 4, and cell 2 (in the second minicolumn)\n  connects to locations 1 and 2.\n  \"\"\"\n  minicolumns = []\n\n  for _ in xrange(numMinicolumns):\n    contextsForCell = [set() for _ in xrange(numCells)]\n\n    contexts = range(numContexts)\n    random.shuffle(contexts)\n\n    while len(contexts) > 0:\n      eligibleCells = range(len(contextsForCell))\n      while len(contexts) > 0 and len(eligibleCells) > 0:\n        candidateAdditions = [(context, cell)\n                              for context in contexts\n                              for cell in eligibleCells]\n\n        # How many new duplicate collisions will come from this addition?\n        #\n        # For every other context in on this cell, check how many times this\n        # pair occurs elsewhere.\n        badness = [sum(sum(1 if (context in otherCellContexts and\n                                 otherContext in otherCellContexts) else 0\n                           for minicolumn in minicolumns\n                           for otherCellContexts in minicolumn)\n                       for otherContext in contextsForCell[cell])\n                   for context, cell in candidateAdditions]\n\n        selectedContext, selectedCell = candidateAdditions[\n          badness.index(min(badness))]\n\n        contextsForCell[selectedCell].add(selectedContext)\n        eligibleCells.remove(selectedCell)\n        contexts.remove(selectedContext)\n\n    minicolumns.append(contextsForCell)\n\n  return minicolumns"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef printSegmentForCell(tm, cell):\n  print \"Segments for cell\", cell, \":\"\n  for seg in tm.basalConnections._cells[cell]._segments:\n    print \"    \",\n    synapses = seg._synapses\n    for s in synapses:\n      print \"%d:%g\" %(s.presynapticCell,s.permanence),\n    print", "response": "Print segment information for this cell"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sequenceToWord(sequence):\n    reberString = ''\n    for i in xrange(len(sequence)):\n        index = np.where(sequence[i]==1.)[0][0]\n        reberString += chars[index]\n    return reberString", "response": "Converts a sequence of 1 - hot words into a reber string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef checkPrediction2(possibleOutcome, predictedOutcome):\n  missN = 0\n  for i in xrange(len(possibleOutcome)):\n    miss = 1\n    for j in xrange(len(predictedOutcome)):\n      if predictedOutcome[j] == possibleOutcome[i]:\n        miss = 0\n    missN += miss\n\n  fpN = 0\n  for i in xrange(len(predictedOutcome)):\n    fp = 1\n    for j in xrange(len(possibleOutcome)):\n      if predictedOutcome[i] == possibleOutcome[j]:\n        fp = 0\n    fpN += fp\n\n  return (missN, fpN)", "response": "checkPrediction2 - Check the prediction of a set of possible outcomes and predicted outcomes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plotConvergenceByColumn(results, columnRange, featureRange, numTrials):\n  ########################################################################\n  #\n  # Accumulate all the results per column in a convergence array.\n  #\n  # Convergence[f,c] = how long it took it to converge with f unique features\n  # and c columns.\n  convergence = numpy.zeros((max(featureRange), max(columnRange) + 1))\n  for r in results:\n    convergence[r[\"numFeatures\"] - 1,\n                r[\"numColumns\"]] += r[\"convergencePoint\"]\n  convergence /= numTrials\n  # For each column, print convergence as fct of number of unique features\n  for c in range(1, max(columnRange) + 1):\n    print c, convergence[:, c]\n  # Print everything anyway for debugging\n  print \"Average convergence array=\", convergence\n  ########################################################################\n  #\n  # Create the plot. x-axis=\n  plt.figure()\n  plotPath = os.path.join(\"plots\", \"convergence_by_column.pdf\")\n  # Plot each curve\n  legendList = []\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n  for i in range(len(featureRange)):\n    f = featureRange[i]\n    print columnRange\n    print convergence[f-1,columnRange]\n    legendList.append('Unique features={}'.format(f))\n    plt.plot(columnRange, convergence[f-1,columnRange],\n             color=colorList[i])\n  # format\n  plt.legend(legendList, loc=\"upper right\")\n  plt.xlabel(\"Number of columns\")\n  plt.xticks(columnRange)\n  plt.yticks(range(0,int(convergence.max())+1))\n  plt.ylabel(\"Average number of touches\")\n  plt.title(\"Number of touches to recognize one object (multiple columns)\")\n    # save\n  plt.savefig(plotPath)\n  plt.close()", "response": "Plots the convergence graph for each column."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plotConvergenceByObjectMultiColumn(results, objectRange, columnRange):\n  ########################################################################\n  #\n  # Accumulate all the results per column in a convergence array.\n  #\n  # Convergence[c,o] = how long it took it to converge with f unique features\n  # and c columns.\n\n  convergence = numpy.zeros((max(columnRange), max(objectRange) + 1))\n  for r in results:\n    if r[\"numColumns\"] in columnRange:\n      convergence[r[\"numColumns\"] - 1, r[\"numObjects\"]] += r[\"convergencePoint\"]\n\n  convergence /= numTrials\n\n  # print \"Average convergence array=\", convergence\n\n  ########################################################################\n  #\n  # Create the plot. x-axis=\n  plt.figure()\n  plotPath = os.path.join(\"plots\", \"convergence_by_object_multicolumn.jpg\")\n\n  # Plot each curve\n  legendList = []\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n\n  for i in range(len(columnRange)):\n    c = columnRange[i]\n    print \"columns={} objectRange={} convergence={}\".format(\n      c, objectRange, convergence[c-1,objectRange])\n    if c == 1:\n      legendList.append('1 column')\n    else:\n      legendList.append('{} columns'.format(c))\n    plt.plot(objectRange, convergence[c-1,objectRange],\n             color=colorList[i])\n\n  # format\n  plt.legend(legendList, loc=\"upper left\", prop={'size':10})\n  plt.xlabel(\"Number of objects in training set\")\n  plt.xticks(range(0,max(objectRange)+1,10))\n  plt.yticks(range(0,int(convergence.max())+2))\n  plt.ylabel(\"Average number of touches\")\n  plt.title(\"Object recognition with multiple columns (unique features = 5)\")\n\n    # save\n  plt.savefig(plotPath)\n  plt.close()", "response": "Plots the convergence graph for each object recognition with multiple columns."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute(self, inputs, outputs):\n    if len(self.queue) > 0:\n      # Take the top element of the data queue\n      data = self.queue.pop()\n    else:\n      raise Exception(\"RawValues: No data: queue is empty \")\n\n    # Copy data into output vectors\n    outputs[\"resetOut\"][0] = data[\"reset\"]\n    outputs[\"dataOut\"][:] = data[\"dataOut\"]", "response": "Compute the next record from the queue and outputs it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addDataToQueue(self, displacement, reset=False):\n    self.queue.appendleft({\n      \"dataOut\": list(displacement),\n      \"reset\": bool(reset)\n    })", "response": "Adds the given displacement to the internal queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun Union Temporal Pooler network on a list of input sequences.", "response": "def runNetworkOnSequences(self, inputSequences, inputCategories, tmLearn=True,\n                            upLearn=None, classifierLearn=False, verbosity=0,\n                            progressInterval=None):\n    \"\"\"\n    Runs Union Temporal Pooler network on specified sequence.\n\n    @param inputSequences           One or more sequences of input patterns.\n                                    Each should be terminated with None.\n\n    @param inputCategories          A sequence of category representations\n                                    for each element in inputSequences\n                                    Each should be terminated with None.\n\n    @param tmLearn:   (bool)        Temporal Memory learning mode\n    @param upLearn:   (None, bool)  Union Temporal Pooler learning mode. If None,\n                                    Union Temporal Pooler will not be run.\n    @param classifierLearn: (bool)  Classifier learning mode\n\n    @param progressInterval: (int)  Interval of console progress updates\n                                    in terms of timesteps.\n    \"\"\"\n\n    currentTime = time.time()\n    for i in xrange(len(inputSequences)):\n      sensorPattern = inputSequences[i]\n      inputCategory = inputCategories[i]\n\n      self.runNetworkOnPattern(sensorPattern,\n                               tmLearn=tmLearn,\n                               upLearn=upLearn,\n                               sequenceLabel=inputCategory)\n\n      if classifierLearn and sensorPattern is not None:\n        unionSDR = self.up.getUnionSDR()\n        upCellCount = self.up.getColumnDimensions()\n        self.classifier.learn(unionSDR, inputCategory, isSparse=upCellCount)\n        if verbosity > 1:\n          pprint.pprint(\"{0} is category {1}\".format(unionSDR, inputCategory))\n\n      if progressInterval is not None and i > 0 and i % progressInterval == 0:\n        elapsed = (time.time() - currentTime) / 60.0\n        print (\"Ran {0} / {1} elements of sequence in \"\n               \"{2:0.2f} minutes.\".format(i, len(inputSequences), elapsed))\n        currentTime = time.time()\n        print MonitorMixinBase.mmPrettyPrintMetrics(\n          self.tm.mmGetDefaultMetrics())\n\n    if verbosity >= 2:\n      traces = self.tm.mmGetDefaultTraces(verbosity=verbosity)\n      print MonitorMixinBase.mmPrettyPrintTraces(traces,\n                                                 breakOnResets=\n                                                 self.tm.mmGetTraceResets())\n\n      if upLearn is not None:\n        traces = self.up.mmGetDefaultTraces(verbosity=verbosity)\n        print MonitorMixinBase.mmPrettyPrintTraces(traces,\n                                                   breakOnResets=\n                                                   self.up.mmGetTraceResets())\n      print"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getUnionTemporalPoolerInput(self):\n    activeCells = numpy.zeros(self.tm.numberOfCells()).astype(realDType)\n    activeCells[list(self.tm.activeCellsIndices())] = 1\n\n    predictedActiveCells = numpy.zeros(self.tm.numberOfCells()).astype(\n      realDType)\n    predictedActiveCells[list(self.tm.predictedActiveCellsIndices())] = 1\n\n    burstingColumns = numpy.zeros(self.tm.numberOfColumns()).astype(realDType)\n    burstingColumns[list(self.tm.unpredictedActiveColumns)] = 1\n\n    return activeCells, predictedActiveCells, burstingColumns", "response": "Gets the Union Temporal Pooler input from the Temporal Memory\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getBurstingColumnsStats(self):\n    traceData = self.tm.mmGetTraceUnpredictedActiveColumns().data\n    resetData = self.tm.mmGetTraceResets().data\n    countTrace = []\n    for x in xrange(len(traceData)):\n      if not resetData[x]:\n        countTrace.append(len(traceData[x]))\n\n    mean = numpy.mean(countTrace)\n    stdDev = numpy.std(countTrace)\n    maximum = max(countTrace)\n    return mean, stdDev, maximum", "response": "Gets statistics on the Temporal Memory s bursting columns. Used as a metric\n    of Temporal Memory s learning performance. Used as a metric\n    of Temporal Memory s learning performance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_false_positive_experiment_correlation(seed,\n                                              num_neurons = 1,\n                                              a = 32,\n                                              dim = 4000,\n                                              num_samples = 20000,\n                                              num_dendrites = 500,\n                                              dendrite_length = 20,\n                                              num_trials = 1000,\n                                              nonlinearity = threshold_nonlinearity(10)):\n  \"\"\"\n  Run an experiment to test the false positive rate based on the correlation\n  between bits.  Correlation is measured as the average pairwise correlation\n  between bits for each pattern in the data (across all of the data).\n\n  To generate the results shown in the false positive vs. correlation figure,\n  we used the parameter settings:\n  1.  a = 32, dim = 2000\n  2.  a = 32, dim = 4000\n  3.  a = 64, dim = 4000\n\n  In each case, we ran approximately 4000 trials in parallel, and then binned\n  the results based on correlation.  The code for binning can be found in\n  plot_effect_of_correlation.py.  Note that your results may not match ours\n  exactly, as the number of different seeds used depends on how many processes\n  are created, but the general trend of the results should be very stable due\n  to the large number of data points.\n  \"\"\"\n  numpy.random.seed(seed)\n  possible_cluster_sizes = range(2, 10)\n\n\n  for trial in range(num_trials):\n    num_cluster_sizes = numpy.random.choice([1, 1, 2] + range(1, 8), 1)\n    cluster_sizes = numpy.random.choice(possible_cluster_sizes, num_cluster_sizes, replace = False)\n    num_cells_per_cluster_size = [numpy.random.randint(dim, 3*dim) for i in range(num_cluster_sizes)]\n    data = generate_correlated_data(dim = dim,\n                                    num_active = a,\n                                    num_samples = num_samples,\n                                    num_cells_per_cluster_size =\n                                        num_cells_per_cluster_size,\n                                    cluster_sizes = cluster_sizes)\n    correlation = get_pattern_correlations(data)\n    closest_correlations = get_biased_correlations(data, threshold = 10)\n    print \"Generated {} samples with total average pattern correlation {}, biased threshold-10 correlation {}, using cluster sizes {} with cells per cluster size of {}\".format(num_samples, correlation, closest_correlations, cluster_sizes, num_cells_per_cluster_size)\n\n\n    fps = []\n    fns = []\n    errors = []\n    for i in range((num_samples/2)/num_dendrites):\n      current_data = data.getSlice(i*(num_dendrites*2), (i+1)*(num_dendrites*2), 0, dim)\n      neuron = Neuron(size = dendrite_length*num_dendrites, num_dendrites = num_dendrites, dendrite_length = dendrite_length, dim = dim, nonlinearity = nonlinearity)\n      labels = numpy.asarray([1 for i in range(num_dendrites)] + [-1 for i in range(num_dendrites)])\n\n      neuron.HTM_style_initialize_on_data(current_data, labels)\n      error, fp, fn = get_error(current_data, labels, [neuron])\n      fps.append(fp)\n      fns.append(fn)\n      errors.append(error)\n\n    print \"Error at r = {} is {}, with {} false positives out of {} samples\".format(correlation, numpy.mean(errors), sum(fps), num_samples/2)\n    with open(\"correlation_results_a{}_n{}_s{}.txt\".format(a, dim, dendrite_length), \"a\") as f:\n      f.write(str(correlation) + \", \" + str(sum(fps)) + \", \" + str(num_samples/2) + \"\\n\")", "response": "Run an experiment to test the false positive rate between bits."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the visible - to - hidden connections.", "response": "def initialize_connections(self):\n        \"\"\"\n        Randomly initializes the visible-to-hidden connections.\n        \n        Each weight is independently sampled from a uniform distribution $U(0,1)$.\n        The weights are NOT normalized!\n        \"\"\"\n        n, m = self.connections.visible_to_hidden.shape\n        self.connections.visible_to_hidden = np.zeros((n, m))\n\n        for i in range(n):\n            random_vector = self._random.rand(m)\n            # norm          = np.sqrt( np.sum(random_vector * random_vector) )\n            self.connections.visible_to_hidden[i]  = random_vector"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an input array x it returns its associated encoding y that is the stable configuration of the hidden units while the visible units are clampled to x.", "response": "def encode(self, x):\n        \"\"\"\n        Given an input array `x` it returns its associated encoding `y(x)`, that is, \n        a stable configuration (local energy minimum) of the hidden units \n        while the visible units are clampled to `x`.\n        \n        Note that NO learning takes place.\n        \"\"\"\n        E      = self.energy\n        y_min  = self.find_energy_minimum(E, x)\n        return y_min"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencoding a whole batch of input arrays without learning.", "response": "def encode_batch(self, inputBatch):\n        \"\"\"Encodes a whole batch of input arrays, without learning.\"\"\"\n        X      = inputBatch\n        encode = self.encode\n        Y      = np.array([ encode(x) for x in X])\n        return Y"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef learn(self, x):\n        y = self.encode(x)\n        self.update_statistics([y]) \n        self.update_weights([x],[y])\n        return y", "response": "Encodes an input array and performs weight updates and updates to the activity \n        statistics according to the respective methods implemented below."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef learn_batch(self, inputBatch):\n        X = inputBatch\n        Y = self.encode_batch(X)\n        self.update_statistics(Y) \n        self.update_weights(X,Y)\n        return Y", "response": "Encodes a whole batch of input arrays and performs weight updates and updates to the activity \n        statistics according to the respective methods implemented below."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_statistics(self, activityVectors):\n        Y = activityVectors\n        n = self.output_size\n        A = np.zeros((n, n))\n        batchSize = len(Y)\n\n        for y in Y:\n            active_units = np.where( y == 1 )[0]\n            for i in active_units:\n                for j in active_units:\n                    A[i,j] += 1.\n\n        A = A/batchSize\n        self.average_activity = self.exponential_moving_average(self.average_activity, A, self.smoothing_period)", "response": "Updates the variable that maintains exponential moving averages of \n        individual and pairwise unit activiy"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_neighbours(y):\n        onBits     = np.where( y == 1. )[0]\n        offBits    = np.where( y == 0. )[0]\n        neighbours = [y.copy()]\n\n        for on in onBits:\n            z     = y.copy()\n            z[on] = 0.\n            neighbours.append(z)\n\n        for off in offBits:\n            z      = y.copy()\n            z[off] = 1.\n            neighbours.append(z)\n\n        # np.random.shuffle(neighbours)\n\n        return neighbours", "response": "r Returns a neigbourhood of elements that \n        share y with y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a randomly initialized weight matrix with the given sparsity and inputSize X outputSize.", "response": "def getSparseWeights(weightSparsity, inputSize, outputSize):\n  \"\"\"\n  Return a randomly initialized weight matrix\n  Size is outputSize X inputSize, with sparsity weightSparsity%\n  \"\"\"\n  # Initialize weights in the typical fashion.\n  w = torch.Tensor(outputSize, inputSize)\n  stdv = 1. / math.sqrt(w.size(1))\n  w.data.uniform_(-stdv, stdv)\n\n  numZeros = int(round((1.0 - weightSparsity) * inputSize))\n\n  outputIndices = np.arange(outputSize)\n  inputIndices = np.array([np.random.permutation(inputSize)[:numZeros]\n                           for _ in outputIndices], dtype=np.long)\n\n  # Create tensor indices for all non-zero weights\n  zeroIndices = np.empty((outputSize, numZeros, 2), dtype=np.long)\n  zeroIndices[:, :, 0] = outputIndices[:, None]\n  zeroIndices[:, :, 1] = inputIndices\n  zeroIndices = torch.LongTensor(zeroIndices.reshape(-1, 2))\n\n  zeroWts = (zeroIndices[:, 0], zeroIndices[:, 1])\n  w.data[zeroWts] = 0.0\n\n  return w"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plotOverlapHistogram(v, w, title, base=\"random\"):\n  overlaps = v.matmul(w.t())\n\n  # Plot histogram of overlaps\n  bins = np.linspace(float(overlaps.min()), float(overlaps.max()), 28)\n  plt.hist(overlaps, bins, alpha=0.5, label='All cols')\n  plt.legend(loc='upper right')\n  plt.xlabel(\"Overlap scores\")\n  plt.ylabel(\"Frequency\")\n  plt.title(title)\n  plt.savefig(base+\"_1\")\n  plt.close()\n\n  return overlaps", "response": "Plots the histogram of overlaps with the weight matrix w."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plotOverlaps(vList, w, base=\"random\", k=20):\n  for i,v in enumerate(vList):\n    if i==0:\n      col = \"m\"\n      label = \"Random vector\"\n    else:\n      col=\"c\"\n      label = \"\"\n      if i==1: label=\"Test images\"\n    # Get a sorted list of overlap values, in decreasing order\n    overlaps = v.matmul(w.t())\n    sortedOverlaps = overlaps.sort()[0].tolist()[0][::-1]\n    plt.plot(sortedOverlaps,col,label=label)\n\n  plt.axvspan(0, k, facecolor=\"g\", alpha=0.3, label=\"Active units\")\n  plt.legend(loc=\"upper right\")\n  plt.xlabel(\"Units\")\n  plt.ylabel(\"Overlap scores\")\n  plt.title(\"Sorted unit overlaps of a sparse net.\")\n  plt.savefig(base+\"_2\")\n  plt.close()", "response": "Plots the overlap curves for each vector v and the weight matrix w."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots histogram of non - zero weight values.", "response": "def analyzeWeightDistribution(weights,base):\n  \"\"\"Plot histogram of non-zero weight values.\"\"\"\n  weights = weights.numpy()\n  weights = weights.reshape(28*28*500)\n  bins = np.linspace(-0.1, 0.1, 50)\n  nz = weights.nonzero()[0]\n  nzw = weights[nz]\n  print(\"zero before and after:\", (weights==0).sum(), (nzw==0).sum())\n  print(\"Small weights:\", (abs(nzw)>0.0).sum(), (abs(nzw)<0.01).sum(), (abs(nzw)<0.02).sum(), (abs(nzw)<0.03).sum())\n  plt.hist(nzw, bins, alpha=0.5, label='All cols')\n  plt.legend(loc='upper right')\n  plt.xlabel(\"Weight values\")\n  plt.ylabel(\"Frequency\")\n  plt.title(\"Histogram of non-zero weight values\")\n  plt.savefig(base+\"_weights\")\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef analyzeModelWeightDistribution(modelName,base):\n  model = torch.load(modelName)\n  model.eval()\n  analyzeWeightDistribution(model.l1.weight.data, base)", "response": "Plot histogram of non - zero weight values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating activation level of the hidden layer.", "response": "def calculateHiddenLayerActivation(self, features):\n    \"\"\"\n    Calculate activation level of the hidden layer\n    :param features feature matrix with dimension (numSamples, numInputs)\n    :return: activation level (numSamples, numHiddenNeurons)\n    \"\"\"\n    if self.activationFunction is \"sig\":\n      H = sigmoidActFunc(features, self.inputWeights, self.bias)\n    else:\n      print \" Unknown activation function type\"\n      raise NotImplementedError\n    return H"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initializePhase(self, features, targets):\n    assert features.shape[0] == targets.shape[0]\n    assert features.shape[1] == self.inputs\n    assert targets.shape[1] == self.outputs\n\n    # randomly initialize the input->hidden connections\n    self.inputWeights = np.random.random((self.numHiddenNeurons, self.inputs))\n    self.inputWeights = self.inputWeights * 2 - 1\n\n    if self.activationFunction is \"sig\":\n      self.bias = np.random.random((1, self.numHiddenNeurons)) * 2 - 1\n    else:\n      print \" Unknown activation function type\"\n      raise NotImplementedError\n\n    H0 = self.calculateHiddenLayerActivation(features)\n    self.M = pinv(np.dot(np.transpose(H0), H0))\n    self.beta = np.dot(pinv(H0), targets)", "response": "Initialize the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstep 2 : Sequential learning phase", "response": "def train(self, features, targets):\n    \"\"\"\n    Step 2: Sequential learning phase\n    :param features feature matrix with dimension (numSamples, numInputs)\n    :param targets target matrix with dimension (numSamples, numOutputs)\n    \"\"\"\n    (numSamples, numOutputs) = targets.shape\n    assert features.shape[0] == targets.shape[0]\n\n    H = self.calculateHiddenLayerActivation(features)\n    Ht = np.transpose(H)\n    try:\n      self.M -= np.dot(self.M,\n                       np.dot(Ht, np.dot(\n          pinv(np.eye(numSamples) + np.dot(H, np.dot(self.M, Ht))),\n          np.dot(H, self.M))))\n\n      self.beta += np.dot(self.M, np.dot(Ht, targets - np.dot(H, self.beta)))\n    except np.linalg.linalg.LinAlgError:\n      print \"SVD not converge, ignore the current training cycle\""}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes prediction with feature matrix", "response": "def predict(self, features):\n    \"\"\"\n    Make prediction with feature matrix\n    :param features: feature matrix with dimension (numSamples, numInputs)\n    :return: predictions with dimension (numSamples, numOutputs)\n    \"\"\"\n    H = self.calculateHiddenLayerActivation(features)\n    prediction = np.dot(H, self.beta)\n    return prediction"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a combined column of the same size in a network.", "response": "def createCombinedSequenceColumn(network, networkConfig, suffix=\"\"):\n  \"\"\"\n  Create a a single column containing one L4, one L2, and one TM.\n\n  networkConfig is a dict that must contain the following keys (additional keys\n  ok):\n\n    {\n      \"externalInputSize\": 1024,\n      \"sensorInputSize\": 1024,\n      \"L2Params\": {\n        <constructor parameters for ColumnPoolerRegion>\n      },\n      \"TMParams\": {\n        <constructor parameters for ApicalTMPairRegion>\n      },\n    }\n\n  Region names are externalInput, sensorInput, L4Column, L2Column, and TMColumn.\n  Each name has an optional string suffix appended to it.\n  \"\"\"\n\n  externalInputName = \"externalInput\" + suffix\n  sensorInputName = \"sensorInput\" + suffix\n  L4ColumnName = \"L4Column\" + suffix\n  L2ColumnName = \"L2Column\" + suffix\n\n  if networkConfig[\"externalInputSize\"] > 0:\n      network.addRegion(\n        externalInputName, \"py.RawSensor\",\n        json.dumps({\"outputWidth\": networkConfig[\"externalInputSize\"]}))\n  network.addRegion(\n    sensorInputName, \"py.RawSensor\",\n    json.dumps({\"outputWidth\": networkConfig[\"sensorInputSize\"]}))\n\n  network.addRegion(L4ColumnName, \"py.ApicalTMPairRegion\",\n                    json.dumps(networkConfig[\"L4Params\"]))\n  network.addRegion(L2ColumnName,\n                    \"py.ColumnPoolerRegion\",\n                    json.dumps(networkConfig[\"L2Params\"]))\n\n  # Set phases appropriately so regions are executed in the proper sequence\n  # This is required for multiple columns - the order of execution is not the\n  # same as the order of region creation.\n  if networkConfig[\"externalInputSize\"] > 0:\n      network.setPhases(externalInputName,[0])\n  network.setPhases(sensorInputName,[0])\n\n  # L4 and L2 regions always have phases 2 and 3, respectively\n  network.setPhases(L4ColumnName,[2])\n  network.setPhases(L2ColumnName,[3])\n\n  # Link sensor to L4\n  network.link(sensorInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"activeColumns\")\n\n  # Create links for lateral connections including growth candidates\n  # Here we link both active cells from L4 and external input to the basal\n  # input to L4.\n  network.link(L4ColumnName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"basalInput\")\n  network.link(externalInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"basalInput\")\n\n  network.link(L4ColumnName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"winnerCells\", destInput=\"basalGrowthCandidates\")\n  network.link(externalInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"basalGrowthCandidates\")\n\n  # Link L4 to L2\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"feedforwardInput\")\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"predictedActiveCells\",\n               destInput=\"feedforwardGrowthCandidates\")\n\n  # Link L2 feedback to L4 if requested\n  if networkConfig[\"enableFeedback\"]:\n    network.link(L2ColumnName, L4ColumnName, \"UniformLink\", \"\",\n                 srcOutput=\"feedForwardOutput\", destInput=\"apicalInput\",\n                 propagationDelay=1)\n\n  # Link reset output to L2, TM, and L4\n  network.link(sensorInputName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n  network.link(sensorInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n\n  enableProfiling(network)\n\n  return network"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef runExperimentPool(numObjects,\n                      numLocations,\n                      numFeatures,\n                      numColumns,\n                      networkType=[\"MultipleL4L2Columns\"],\n                      longDistanceConnectionsRange = [0.0],\n                      numWorkers=7,\n                      nTrials=1,\n                      pointRange=1,\n                      numPoints=10,\n                      numInferenceRpts=1,\n                      l2Params=None,\n                      l4Params=None,\n                      resultsName=\"convergence_results.pkl\"):\n  \"\"\"\n  Allows you to run a number of experiments using multiple processes.\n  For each parameter except numWorkers, pass in a list containing valid values\n  for that parameter. The cross product of everything is run, and each\n  combination is run nTrials times.\n\n  Returns a list of dict containing detailed results from each experiment.\n  Also pickles and saves the results in resultsName for later analysis.\n\n  Example:\n    results = runExperimentPool(\n                          numObjects=[10],\n                          numLocations=[5],\n                          numFeatures=[5],\n                          numColumns=[2,3,4,5,6],\n                          numWorkers=8,\n                          nTrials=5)\n  \"\"\"\n  # Create function arguments for every possibility\n  args = []\n\n  for c in reversed(numColumns):\n    for o in reversed(numObjects):\n      for l in numLocations:\n        for f in numFeatures:\n          for n in networkType:\n            for p in longDistanceConnectionsRange:\n              for t in range(nTrials):\n                args.append(\n                  {\"numObjects\": o,\n                   \"numLocations\": l,\n                   \"numFeatures\": f,\n                   \"numColumns\": c,\n                   \"trialNum\": t,\n                   \"pointRange\": pointRange,\n                   \"numPoints\": numPoints,\n                   \"networkType\" : n,\n                   \"longDistanceConnections\" : p,\n                   \"plotInferenceStats\": False,\n                   \"settlingTime\": 3,\n                   \"numInferenceRpts\": numInferenceRpts,\n                   \"l2Params\": l2Params,\n                   \"l4Params\": l4Params\n                   }\n                )\n  print \"{} experiments to run, {} workers\".format(len(args), numWorkers)\n  # Run the pool\n  if numWorkers > 1:\n    pool = Pool(processes=numWorkers)\n    result = pool.map(runExperiment, args)\n  else:\n    result = []\n    for arg in args:\n      result.append(runExperiment(arg))\n\n  # print \"Full results:\"\n  # pprint.pprint(result, width=150)\n\n  # Pickle results for later use\n  with open(resultsName,\"wb\") as f:\n    cPickle.dump(result,f)\n\n  return result", "response": "This function runs the experiment pool."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plotConvergenceByColumnTopology(results, columnRange, featureRange, networkType, numTrials):\n  ########################################################################\n  #\n  # Accumulate all the results per column in a convergence array.\n  #\n  # Convergence[f, c, t] = how long it took it to  converge with f unique\n  # features, c columns and topology t.\n\n  convergence = numpy.zeros((max(featureRange), max(columnRange) + 1, len(networkType)))\n\n  networkTypeNames = {}\n  for i, topologyType in enumerate(networkType):\n    if \"Topology\" in topologyType:\n      networkTypeNames[i] = \"Normal\"\n    else:\n      networkTypeNames[i] = \"Dense\"\n\n  for r in results:\n    convergence[r[\"numFeatures\"] - 1, r[\"numColumns\"], networkType.index(r[\"networkType\"])] += r[\"convergencePoint\"]\n  convergence /= numTrials\n\n  # For each column, print convergence as fct of number of unique features\n  for c in range(1, max(columnRange) + 1):\n    for t in range(len(networkType)):\n      print c, convergence[:, c, t]\n\n  # Print everything anyway for debugging\n  print \"Average convergence array=\", convergence\n\n  ########################################################################\n  #\n  # Create the plot. x-axis=\n  plt.figure()\n  plotPath = os.path.join(\"plots\", \"convergence_by_column_topology.pdf\")\n\n  # Plot each curve\n  legendList = []\n  colormap = plt.get_cmap(\"jet\")\n  colorList = [colormap(x) for x in numpy.linspace(0., 1.,\n      len(featureRange)*len(networkType))]\n\n  for i in range(len(featureRange)):\n    for t in range(len(networkType)):\n      f = featureRange[i]\n      print columnRange\n      print convergence[f-1,columnRange, t]\n      legendList.append('Unique features={}, topology={}'.format(f, networkTypeNames[t]))\n      plt.plot(columnRange, convergence[f-1,columnRange, t],\n               color=colorList[i*len(networkType) + t])\n\n  # format\n  plt.legend(legendList, loc=\"upper right\")\n  plt.xlabel(\"Number of columns\")\n  plt.xticks(columnRange)\n  plt.yticks(range(0,int(convergence.max())+1))\n  plt.ylabel(\"Average number of touches\")\n  plt.title(\"Number of touches to recognize one object (multiple columns)\")\n\n    # save\n  plt.savefig(plotPath)\n  plt.close()", "response": "Plots the convergence graph for each column."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the convergence graph for each random feature and column.", "response": "def plotConvergenceByDistantConnectionChance(results, featureRange, columnRange, longDistanceConnectionsRange, numTrials):\n  \"\"\"\n  Plots the convergence graph: iterations vs number of columns.\n  Each curve shows the convergence for a given number of unique features.\n  \"\"\"\n  ########################################################################\n  #\n  # Accumulate all the results per column in a convergence array.\n  #\n  # Convergence[f, c, t] = how long it took it to  converge with f unique\n  # features, c columns and topology t.\n  convergence = numpy.zeros((len(featureRange), len(longDistanceConnectionsRange), len(columnRange)))\n\n  for r in results:\n      print longDistanceConnectionsRange.index(r[\"longDistanceConnections\"])\n      print columnRange.index(r[\"numColumns\"])\n      convergence[featureRange.index(r[\"numFeatures\"]),\n          longDistanceConnectionsRange.index(r[\"longDistanceConnections\"]),\n          columnRange.index(r[\"numColumns\"])] += r[\"convergencePoint\"]\n\n  convergence /= numTrials\n\n  # For each column, print convergence as fct of number of unique features\n  for i, c in enumerate(columnRange):\n    for j, r in enumerate(longDistanceConnectionsRange):\n      print c, r, convergence[:, j, i]\n\n  # Print everything anyway for debugging\n  print \"Average convergence array=\", convergence\n\n  ########################################################################\n  #\n  # Create the plot. x-axis=\n  plt.figure(figsize=(8, 6), dpi=80)\n  plotPath = os.path.join(\"plots\", \"convergence_by_random_connection_chance.pdf\")\n\n  # Plot each curve\n  legendList = []\n  colormap = plt.get_cmap(\"jet\")\n  colorList = [colormap(x) for x in numpy.linspace(0., 1.,\n      len(featureRange)*len(longDistanceConnectionsRange))]\n\n  for i, r in enumerate(longDistanceConnectionsRange):\n    for j, f in enumerate(featureRange):\n      currentColor = i*len(featureRange) + j\n      print columnRange\n      print convergence[j, i, :]\n      legendList.append('Connection_prob = {}, num features = {}'.format(r, f))\n      plt.plot(columnRange, convergence[j, i, :], color=colorList[currentColor])\n\n  # format\n  plt.legend(legendList, loc = \"lower left\")\n  plt.xlabel(\"Number of columns\")\n  plt.xticks(columnRange)\n  plt.yticks(range(0,int(convergence.max())+1))\n  plt.ylabel(\"Average number of touches\")\n  plt.title(\"Number of touches to recognize one object (multiple columns)\")\n\n    # save\n  plt.show()\n  plt.savefig(plotPath)\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the statistics of the output units Y and P.", "response": "def update_statistics(Y, P, beta=0.9):\n  \"\"\"\n  Args\n  ----\n  Y: \n      2d array whose columns encode the \n      activity of the output units\n  P:\n      2d array encoding the pairwise average \n      activity of the output units  \n\n  Returns\n  -------\n      The updated average activities\n  \"\"\"\n\n  (n, d) = Y.shape\n  \n\n  A = np.expand_dims(Y, axis=1) * np.expand_dims(Y, axis=0)\n  assert(A.shape == (n, n, d))\n  \n  Q = np.mean(A, axis=2)\n  Q[np.where(Q == 0.)] = 0.000001\n  assert(P.shape == Q.shape)\n\n  return beta*P + (1-beta)*Q"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a list of minibatches from inputs X and targets Y.", "response": "def random_mini_batches(X, Y, minibatch_size, seed=None):\n  \"\"\"\n  Compute a list of minibatches from inputs X and targets Y.\n  A datapoint is expected to be represented as a column in \n  the data matrices X and Y.\n  \"\"\"\n  d    = X.shape[1]\n  size = minibatch_size\n  minibatches = []\n\n  if Y is None:\n      Y = np.zeros((1, d))\n\n  np.random.seed(seed)\n  perm = np.random.permutation(d)\n\n  for t in range(0, d, size):\n      subset = perm[t: t+size]\n      minibatches.append((X[:, subset], Y[:, subset]))\n\n  return minibatches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlearning a set of objects. Then try to recognize each object. Output an interactive visualization. @param cellDimensions (pair) The cell dimensions of each module @param cellCoordinateOffsets (sequence) The \"cellCoordinateOffsets\" parameter for each module", "response": "def doPointsExperiment(cellDimensions, cellCoordinateOffsets):\n  \"\"\"\n  Learn a set of objects. Then try to recognize each object. Output an\n  interactive visualization.\n\n  @param cellDimensions (pair)\n  The cell dimensions of each module\n\n  @param cellCoordinateOffsets (sequence)\n  The \"cellCoordinateOffsets\" parameter for each module\n  \"\"\"\n  if not os.path.exists(\"traces\"):\n    os.makedirs(\"traces\")\n\n  locationConfigs = []\n  for i in xrange(5):\n    scale = 10.0 * (math.sqrt(2) ** i)\n\n    for _ in xrange(4):\n      orientation = np.radians(random.gauss(7.5, 7.5))\n      orientation = random.choice([orientation, -orientation])\n\n      locationConfigs.append({\n        \"cellsPerAxis\": cellDimensions[0],\n        \"scale\": scale,\n        \"orientation\": orientation,\n        \"activationThreshold\": 8,\n        \"initialPermanence\": 1.0,\n        \"connectedPermanence\": 0.5,\n        \"learningThreshold\": 8,\n        \"sampleSize\": 10,\n        \"permanenceIncrement\": 0.1,\n        \"permanenceDecrement\": 0.0,\n        \"cellCoordinateOffsets\": cellCoordinateOffsets,\n      })\n\n  L4Overrides = {\n    \"activationThreshold\": 15,\n    \"minThreshold\": 15,\n    \"initialPermanence\": 1.0,\n  }\n\n  column = PIUNCorticalColumn(locationConfigs, L4Overrides, bumpType=\"square\")\n  exp = PIUNExperiment(column, featureNames=(\"A\", \"B\"))\n\n  for objectDescription in OBJECTS:\n    exp.learnObject(objectDescription)\n\n  filename = \"traces/{}-points-{}-cells.html\".format(\n    len(cellCoordinateOffsets)**2, np.prod(cellDimensions))\n\n  with io.open(filename, \"w\", encoding=\"utf8\") as fileOut:\n    with trace(fileOut, exp, includeSynapses=True):\n      print \"Logging to\", filename\n      for objectDescription in OBJECTS:\n        succeeded = exp.inferObjectWithRandomMovements(objectDescription)\n        if not succeeded:\n          print 'Failed to infer object \"{}\"'.format(objectDescription[\"name\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns an experiment tracking the performance of the union sequence.", "response": "def run_tm_union_experiment(dim = 4000,\n                            cellsPerColumn=1,\n                            num_active = 40,\n                            activationThreshold=5,\n                            initialPermanence=0.8,\n                            connectedPermanence=0.50,\n                            minThreshold=5,\n                            maxNewSynapseCount=20,\n                            permanenceIncrement=0.05,\n                            permanenceDecrement=0.00,\n                            predictedSegmentDecrement=0.000,\n                            maxSegmentsPerCell=255,\n                            maxSynapsesPerSegment=255,\n                            seed=42,\n                            num_branches_range = range(50, 51, 1),\n                            onset_length = 5,\n                            training_iters = 10,\n                            num_trials = 10000,\n                            automatic_threshold = True,\n                            save_results = True):\n  \"\"\"\n  Run an experiment tracking the performance of the temporal memory given\n  different input dimensions.  The number of active cells is kept fixed, so we\n  are in effect varying the sparsity of the input.   We track performance by\n  comparing the cells predicted to be active with the cells actually active in\n  the sequence without noise at every timestep, and averaging across timesteps.\n  Three metrics are used, correlation (Pearson's r, by numpy.corrcoef),\n  set similarity (Jaccard index) and cosine similarity (using\n  scipy.spatial.distance.cosine).  The Jaccard set similarity is the\n  canonical metric used in the paper, but all three tend to produce very similar\n  results.\n\n  Output is written to tm_dim_{num_active}.txt, including sample size.\n\n  We tested two different dimension settings, 2000 and 4000.\n  \"\"\"\n  if automatic_threshold:\n    activationThreshold = min(num_active/2, maxNewSynapseCount/2)\n    minThreshold = min(num_active/2, maxNewSynapseCount/2)\n\n  for num_branches in num_branches_range:\n    overlaps = []\n    surprises = []\n    csims = []\n    for trial in range(num_trials):\n      if (trial + 1) % 100 == 0:\n        print trial + 1\n      tm = TM(columnDimensions=(dim,),\n              cellsPerColumn=cellsPerColumn,\n              activationThreshold=activationThreshold,\n              initialPermanence=initialPermanence,\n              connectedPermanence=connectedPermanence,\n              minThreshold=minThreshold,\n              maxNewSynapseCount=maxNewSynapseCount,\n              permanenceIncrement=permanenceIncrement,\n              permanenceDecrement=permanenceDecrement,\n              predictedSegmentDecrement=predictedSegmentDecrement,\n              maxSegmentsPerCell=maxSegmentsPerCell,\n              maxSynapsesPerSegment=maxSynapsesPerSegment,\n              seed=seed)\n\n      datapoints = []\n      canonical_active_cells = []\n      onset = generate_evenly_distributed_data_sparse(dim = dim, num_active = num_active, num_samples = onset_length)\n\n      for branch in range(num_branches):\n        datapoint = numpy.random.choice(dim, num_active, replace = False)\n        datapoints.append(datapoint)\n        for i in range(training_iters):\n          for j in range(onset.nRows()):\n            activeColumns = set(onset.rowNonZeros(j)[0])\n            tm.compute(activeColumns, learn = True)\n          tm.compute(datapoint, learn=True)\n          tm.reset()\n\n      for j in range(onset.nRows()):\n        activeColumns = set(onset.rowNonZeros(j)[0])\n        tm.compute(activeColumns, learn = False)\n      predicted_cells = tm.getPredictiveCells()\n\n      datapoint = numpy.random.choice(dim, num_active, replace = False)\n      overlap = (1. * len(set(predicted_cells) & set(datapoint)))/len(datapoint)\n      surprise = len(datapoint) - len(set(predicted_cells) & set(datapoint))\n      dense_predicted_cells = numpy.zeros((dim*cellsPerColumn,))\n      for cell in predicted_cells:\n        dense_predicted_cells[cell] = 1.\n      dense_active_cells = numpy.zeros((dim*cellsPerColumn,))\n      for cell in datapoint:\n        dense_active_cells[cell] = 1.\n      csim = 1 - cosine(dense_predicted_cells, dense_active_cells)\n      csim = numpy.nan_to_num(csim)\n      overlaps.append(overlap)\n      surprises.append(surprise)\n      csims.append(csim)\n\n    overlap = numpy.mean(overlaps)\n    surprise = numpy.mean(surprises)\n    csim = numpy.mean(csims)\n    print dim, overlap, surprise, csim\n    if save_results:\n      with open(\"tm_union_n{}_a{}_c{}.txt\".format(dim, num_active, cellsPerColumn), \"a\") as f:\n        f.write(str(num_branches)+\", \" + str(overlap) + \", \" + str(surprise) + \", \" + str(csim) + \", \" + str(num_trials) + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getMovingBar(startLocation,\n                 direction,\n                 imageSize=(20, 20),\n                 steps=5,\n                 barHalfLength=3,\n                 orientation='horizontal'):\n  \"\"\"\n  Generate a list of bars\n  :param startLocation:\n         (list) start location of the bar center, e.g. (10, 10)\n  :param direction:\n         direction of movement, e.g., (1, 0)\n  :param imageSize:\n         (list) number of pixels on horizontal and vertical dimension\n  :param steps:\n         (int) number of steps\n  :param barHalfLength:\n         (int) length of the bar\n  :param orientation:\n         (string) \"horizontal\" or \"vertical\"\n  :return:\n  \"\"\"\n  startLocation = np.array(startLocation)\n  direction = np.array(direction)\n  barMovie = []\n  for step in range(steps):\n    barCenter = startLocation + step * direction\n    barMovie.append(getBar(imageSize,\n                           barCenter,\n                           barHalfLength,\n                           orientation))\n\n  return barMovie", "response": "Generate a list of bars that are moved in the specified direction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getBar(imageSize, barCenter, barHalfLength, orientation='horizontal'):\n  (nX, nY) = imageSize\n  (xLoc, yLoc) = barCenter\n  bar = np.zeros((nX, nY), dtype=uintType)\n  if orientation == 'horizontal':\n    xmin = max(0, (xLoc - barHalfLength))\n    xmax = min(nX - 1, (xLoc + barHalfLength + 1))\n    bar[xmin:xmax, yLoc] = 1\n  elif orientation == 'vertical':\n    ymin = max(0, (yLoc - barHalfLength))\n    ymax = min(nY - 1, (yLoc + barHalfLength + 1))\n    bar[xLoc, ymin:ymax] = 1\n  else:\n    raise RuntimeError(\"orientation has to be horizontal or vertical\")\n  return bar", "response": "Generates a single horizontal or vertical bar from the image size and center of the bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generateRandomSDR(numSDR, numDims, numActiveInputBits, seed=42):\n  randomSDRs = np.zeros((numSDR, numDims), dtype=uintType)\n  indices = np.array(range(numDims))\n  np.random.seed(seed)\n  for i in range(numSDR):\n    randomIndices = np.random.permutation(indices)\n    activeBits = randomIndices[:numActiveInputBits]\n    randomSDRs[i, activeBits] = 1\n\n  return randomSDRs", "response": "Generates a set of random SDRs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the percentage of overlap between vectors x1 and x2.", "response": "def percentOverlap(x1, x2):\n  \"\"\"\n  Computes the percentage of overlap between vectors x1 and x2.\n\n  @param x1   (array) binary vector\n  @param x2   (array) binary vector\n  @param size (int)   length of binary vectors\n\n  @return percentOverlap (float) percentage overlap between x1 and x2\n  \"\"\"\n  nonZeroX1 = np.count_nonzero(x1)\n  nonZeroX2 = np.count_nonzero(x2)\n\n  percentOverlap = 0\n  minX1X2 = min(nonZeroX1, nonZeroX2)\n  if minX1X2 > 0:\n    overlap = float(np.dot(x1.T, x2))\n    percentOverlap = overlap / minX1X2\n\n  return percentOverlap"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addNoiseToVector(inputVector, noiseLevel, vectorType):\n  if vectorType == 'sparse':\n    corruptSparseVector(inputVector, noiseLevel)\n  elif vectorType == 'dense':\n    corruptDenseVector(inputVector, noiseLevel)\n  else:\n    raise ValueError(\"vectorType must be 'sparse' or 'dense' \")", "response": "Add noise to the SDRs\n"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef corruptDenseVector(vector, noiseLevel):\n  size = len(vector)\n  for i in range(size):\n    rnd = random.random()\n    if rnd < noiseLevel:\n      if vector[i] == 1:\n        vector[i] = 0\n      else:\n        vector[i] = 1", "response": "Corrupts a binary vector by inverting noiseLevel percent of its bits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncorrespond to a sparse vector.", "response": "def corruptSparseVector(sdr, noiseLevel):\n  \"\"\"\n  Add noise to sdr by turning off numNoiseBits active bits and turning on\n  numNoiseBits in active bits\n  @param sdr        (array) Numpy array of the  SDR\n  @param noiseLevel (float) amount of noise to be applied on the vector.\n  \"\"\"\n\n  numNoiseBits = int(noiseLevel * np.sum(sdr))\n  if numNoiseBits <= 0:\n    return sdr\n  activeBits = np.where(sdr > 0)[0]\n  inActiveBits = np.where(sdr == 0)[0]\n\n  turnOffBits = np.random.permutation(activeBits)\n  turnOnBits = np.random.permutation(inActiveBits)\n  turnOffBits = turnOffBits[:numNoiseBits]\n  turnOnBits = turnOnBits[:numNoiseBits]\n\n  sdr[turnOffBits] = 0\n  sdr[turnOnBits] = 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculateOverlapCurve(sp, inputVectors):\n  columnNumber = np.prod(sp.getColumnDimensions())\n  numInputVector, inputSize = inputVectors.shape\n\n  outputColumns = np.zeros((numInputVector, columnNumber), dtype=uintType)\n  outputColumnsCorrupted = np.zeros((numInputVector, columnNumber),\n                                    dtype=uintType)\n\n  noiseLevelList = np.linspace(0, 1.0, 21)\n  inputOverlapScore = np.zeros((numInputVector, len(noiseLevelList)))\n  outputOverlapScore = np.zeros((numInputVector, len(noiseLevelList)))\n  for i in range(numInputVector):\n    for j in range(len(noiseLevelList)):\n      inputVectorCorrupted = copy.deepcopy(inputVectors[i][:])\n      corruptSparseVector(inputVectorCorrupted, noiseLevelList[j])\n\n      sp.compute(inputVectors[i][:], False, outputColumns[i][:])\n      sp.compute(inputVectorCorrupted, False,\n                 outputColumnsCorrupted[i][:])\n\n      inputOverlapScore[i][j] = percentOverlap(inputVectors[i][:],\n                                               inputVectorCorrupted)\n      outputOverlapScore[i][j] = percentOverlap(outputColumns[i][:],\n                                                outputColumnsCorrupted[i][:])\n\n  return noiseLevelList, inputOverlapScore, outputOverlapScore", "response": "Evalulate noise robustness of SP for a given set of SDRs\n \u00b0C"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclassifies the SP output", "response": "def classifySPoutput(targetOutputColumns, outputColumns):\n  \"\"\"\n  Classify the SP output\n  @param targetOutputColumns (list) The target outputs, corresponding to\n                                    different classes\n  @param outputColumns (array) The current output\n  @return classLabel (int) classification outcome\n  \"\"\"\n  numTargets, numDims = targetOutputColumns.shape\n  overlap = np.zeros((numTargets,))\n  for i in range(numTargets):\n    overlap[i] = percentOverlap(outputColumns, targetOutputColumns[i, :])\n  classLabel = np.argmax(overlap)\n  return classLabel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef classificationAccuracyVsNoise(sp, inputVectors, noiseLevelList):\n  numInputVector, inputSize = inputVectors.shape\n\n  if sp is None:\n    targetOutputColumns = copy.deepcopy(inputVectors)\n  else:\n    columnNumber = np.prod(sp.getColumnDimensions())\n    # calculate target output given the uncorrupted input vectors\n    targetOutputColumns = np.zeros((numInputVector, columnNumber),\n                                   dtype=uintType)\n    for i in range(numInputVector):\n      sp.compute(inputVectors[i][:], False, targetOutputColumns[i][:])\n\n  outcomes = np.zeros((len(noiseLevelList), numInputVector))\n  for i in range(len(noiseLevelList)):\n    for j in range(numInputVector):\n      corruptedInputVector = copy.deepcopy(inputVectors[j][:])\n      corruptSparseVector(corruptedInputVector, noiseLevelList[i])\n\n      if sp is None:\n        outputColumns = copy.deepcopy(corruptedInputVector)\n      else:\n        outputColumns = np.zeros((columnNumber, ), dtype=uintType)\n        sp.compute(corruptedInputVector, False, outputColumns)\n\n      predictedClassLabel = classifySPoutput(targetOutputColumns, outputColumns)\n      outcomes[i][j] = predictedClassLabel == j\n\n  predictionAccuracy = np.mean(outcomes, 1)\n  return predictionAccuracy", "response": "Evaluate whether the SP output is classifiable with varying amount of noise\n \u00d7 the number of input vectors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plotExampleInputOutput(sp, inputVectors, saveFigPrefix=None):\n  numInputVector, inputSize = inputVectors.shape\n  numColumns = np.prod(sp.getColumnDimensions())\n\n  outputColumns = np.zeros((numInputVector, numColumns), dtype=uintType)\n  inputOverlap = np.zeros((numInputVector, numColumns), dtype=uintType)\n\n  connectedCounts = np.zeros((numColumns,), dtype=uintType)\n  sp.getConnectedCounts(connectedCounts)\n\n  winnerInputOverlap = np.zeros(numInputVector)\n  for i in range(numInputVector):\n    sp.compute(inputVectors[i][:], False, outputColumns[i][:])\n    inputOverlap[i][:] = sp.getOverlaps()\n    activeColumns = np.where(outputColumns[i][:] > 0)[0]\n    if len(activeColumns) > 0:\n      winnerInputOverlap[i] = np.mean(\n        inputOverlap[i][np.where(outputColumns[i][:] > 0)[0]])\n\n  fig, axs = plt.subplots(2, 1)\n  axs[0].imshow(inputVectors[:, :200], cmap='gray', interpolation=\"nearest\")\n  axs[0].set_ylabel('input #')\n  axs[0].set_title('input vectors')\n  axs[1].imshow(outputColumns[:, :200], cmap='gray', interpolation=\"nearest\")\n  axs[1].set_ylabel('input #')\n  axs[1].set_title('output vectors')\n  if saveFigPrefix is not None:\n    plt.savefig('figures/{}_example_input_output.pdf'.format(saveFigPrefix))\n\n  inputDensity = np.sum(inputVectors, 1) / float(inputSize)\n  outputDensity = np.sum(outputColumns, 1) / float(numColumns)\n  fig, axs = plt.subplots(2, 1)\n  axs[0].plot(inputDensity)\n  axs[0].set_xlabel('input #')\n  axs[0].set_ylim([0, 0.2])\n\n  axs[1].plot(outputDensity)\n  axs[1].set_xlabel('input #')\n  axs[1].set_ylim([0, 0.05])\n\n  if saveFigPrefix is not None:\n    plt.savefig('figures/{}_example_input_output_density.pdf'.format(saveFigPrefix))", "response": "Plots example input & output vectors."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inspectSpatialPoolerStats(sp, inputVectors, saveFigPrefix=None):\n  numInputVector, inputSize = inputVectors.shape\n  numColumns = np.prod(sp.getColumnDimensions())\n\n  outputColumns = np.zeros((numInputVector, numColumns), dtype=uintType)\n  inputOverlap = np.zeros((numInputVector, numColumns), dtype=uintType)\n\n  connectedCounts = np.zeros((numColumns, ), dtype=uintType)\n  sp.getConnectedCounts(connectedCounts)\n\n  winnerInputOverlap = np.zeros(numInputVector)\n  for i in range(numInputVector):\n    sp.compute(inputVectors[i][:], False, outputColumns[i][:])\n    inputOverlap[i][:] = sp.getOverlaps()\n    activeColumns = np.where(outputColumns[i][:] > 0)[0]\n    if len(activeColumns) > 0:\n      winnerInputOverlap[i] = np.mean(\n        inputOverlap[i][np.where(outputColumns[i][:] > 0)[0]])\n  avgInputOverlap = np.mean(inputOverlap, 0)\n\n  entropy = calculateEntropy(outputColumns)\n  activationProb = np.mean(outputColumns.astype(realDType), 0)\n\n  dutyCycleDist, binEdge = np.histogram(activationProb,\n                                        bins=10, range=[-0.005, 0.095])\n  dutyCycleDist = dutyCycleDist.astype('float32') / np.sum(dutyCycleDist)\n  binCenter = (binEdge[1:] + binEdge[:-1])/2\n\n  fig, axs = plt.subplots(2, 2)\n  axs[0, 0].hist(connectedCounts)\n  axs[0, 0].set_xlabel('# Connected Synapse')\n\n  axs[0, 1].hist(winnerInputOverlap)\n  axs[0, 1].set_xlabel('# winner input overlap')\n\n  axs[1, 0].bar(binEdge[:-1]+0.001, dutyCycleDist, width=.008)\n  axs[1, 0].set_xlim([-0.005, .1])\n  axs[1, 0].set_xlabel('Activation Frequency')\n  axs[1, 0].set_title('Entropy: {}'.format(entropy))\n\n  axs[1, 1].plot(connectedCounts, activationProb, '.')\n  axs[1, 1].set_xlabel('connection #')\n  axs[1, 1].set_ylabel('activation freq')\n  plt.tight_layout()\n\n  if saveFigPrefix is not None:\n    plt.savefig('figures/{}_network_stats.pdf'.format(saveFigPrefix))\n  return fig", "response": "Inspect the statistics of a spatial pooler given a set of input vectors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef binaryEntropyVectorized(x):\n  entropy = - x*np.log2(x) - (1-x)*np.log2(1-x)\n  entropy[x*(1 - x) == 0] = 0\n  return entropy", "response": "Calculate the entropy of a list of binary random variables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the mean entropy given activation history", "response": "def calculateEntropy(activeColumns, type='binary'):\n  \"\"\"\n  calculate the mean entropy given activation history\n  @param activeColumns (array) 2D numpy array of activation history\n  @return entropy (float) mean entropy\n  \"\"\"\n  activationProb   = np.mean(activeColumns, 0)\n  if type == 'binary':\n    totalEntropy     = np.sum(binaryEntropyVectorized(activationProb))\n  elif type == 'renyi':\n    totalEntropy = np.sum(renyiEntropyVectorized(activationProb))\n  else:\n    raise ValueError('unknown entropy type')\n\n  numberOfColumns  = activeColumns.shape[1]\n  # return mean entropy\n  return totalEntropy/numberOfColumns"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reconstructionError(sp, inputVectors, activeColumnVectors, threshold=0.):\n  batchSize        = inputVectors.shape[0]\n  connectionMatrix = getConnectedSyns(sp)\n\n  reconstructionVectors = np.dot(activeColumnVectors, connectionMatrix)\n  # numActiveColumns      = np.sum(activeColumnVectors, 1)[0]\n  numActiveColumns = int(sp._localAreaDensity * sp._numColumns) + 0.0\n  reconstructionVectors = reconstructionVectors/numActiveColumns\n\n  if threshold > 0.:\n    reconstructionVectors =np.where( \n                            reconstructionVectors > threshold, \n                            np.ones( reconstructionVectors.shape), \n                            np.zeros(reconstructionVectors.shape))\n\n  Err = np.sum(np.absolute(reconstructionVectors  - inputVectors))\n\n  return Err/batchSize", "response": "Computes the reconstruction error of a binary column vector."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the error of a reconstruction for a set of input vectors.", "response": "def witnessError(sp, inputVectors, activeColumnsCurrentEpoch):\n  \"\"\"\n  Computes a variation of a reconstruction error. It measures the average \n  hamming distance of an active column's connected synapses vector and its witnesses. \n  An input vector is called witness for a column, iff the column is among \n  the active columns for the input computed by the spatial pooler. \n  The error is given by\n  \\[     \n      Witness Error = (1/batchSize) * \\sum_{x \\in InputBatch} \n                          (1/numActiveColumns) * \\sum_{i active column of sdr(x)} \\| x -  syn(i) \\|_1.\n  \\]\n  Note: Turns out that in our setting (x and syn(i) binary vectors) we have \n  \\[\n      Witness Error = Reconstruction Error.\n  \\]\n  It can be shown that the error is optimized by the Hebbian-like update rule \n  of the spatial pooler. \n  \"\"\"\n  connectionMatrix = getConnectedSyns(sp)\n  batchSize        = inputVectors.shape[0]\n\n  # 1st sum... over each input in batch\n  Err = 0.\n  for i in range(batchSize):\n    activeColumns    = np.where(activeColumnsCurrentEpoch[i] > 0.)[0]\n    numActiveColumns = activeColumns.shape[0]\n    # 2nd sum... over each active colum\n    err = 0.\n    for j in activeColumns:\n      # Compute hamming distance and accumulate\n      err += np.sum(np.absolute(connectionMatrix[j] - inputVectors[i]))\n\n    Err += err/numActiveColumns\n\n  return Err/batchSize"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mutualInformation(sp, activeColumnsCurrentEpoch, column_1, column_2):\n  i, j        = column_1, column_2\n  batchSize   = activeColumnsCurrentEpoch.shape[0]\n\n  # Activity Counts\n  ci, cj, cij = 0., 0., dict([((0,0),0.), ((1,0),0.), ((0,1),0.), ((1,1),0.)])\n  for t in range(batchSize):\n    ai = activeColumnsCurrentEpoch[t, i]\n    aj = activeColumnsCurrentEpoch[t, j]\n    cij[(ai, aj)] += 1.\n    ci += ai\n    cj += aj\n\n  # Mutual information calculation \n  Iij = 0\n  for a,b in [(0,0), (1,0), (0,1), (1,1)]:\n    # Compute probabilities\n    pij = cij[(a,b)]/batchSize\n    pi  = ci/batchSize if a == 1 else 1. - ci/batchSize\n    pj  = cj/batchSize if b == 1 else 1. - cj/batchSize\n    # Add current term of mutual information \n    Iij += pij * np.log2(pij/(pi*pj)) if pij > 0 else 0\n\n  return Iij", "response": "Compute the mutual information of the binary variables that represent \n."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the mean of the mutual information for a given set of active columns.", "response": "def meanMutualInformation(sp, activeColumnsCurrentEpoch, columnsUnderInvestigation = []):\n  \"\"\"\n  Computes the mean of the mutual information \n  of pairs taken from a list of columns. \n  \"\"\"\n  if len(columnsUnderInvestigation) == 0:\n    columns = range(np.prod(sp.getColumnDimensions()))\n  else:\n    columns = columnsUnderInvestigation\n  numCols = len(columns)\n  sumMutualInfo = 0\n  normalizingConst = numCols*(numCols - 1)/2\n  for i in range(numCols):\n    for j in range(i+1, numCols):\n      sumMutualInfo += mutualInformation(sp, activeColumnsCurrentEpoch, columns[i], columns[j])\n\n  return sumMutualInfo/normalizingConst"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef learnL6Pattern(self, l6Pattern, cellsToLearnOn):\n    cellIndices = [self.trnCellIndex(x) for x in cellsToLearnOn]\n    newSegments = self.trnConnections.createSegments(cellIndices)\n    self.trnConnections.growSynapses(newSegments, l6Pattern, 1.0)", "response": "Learn the given l6Pattern on the given list of cells to learn on."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deInactivateCells(self, l6Input):\n\n    # Figure out which TRN cells recognize the L6 pattern.\n    self.trnOverlaps = self.trnConnections.computeActivity(l6Input, 0.5)\n    self.activeTRNSegments = np.flatnonzero(\n      self.trnOverlaps >= self.trnActivationThreshold)\n    self.activeTRNCellIndices = self.trnConnections.mapSegmentsToCells(\n      self.activeTRNSegments)\n\n    # for s, idx in zip(self.activeTRNSegments, self.activeTRNCellIndices):\n    #   print(self.trnOverlaps[s], idx, self.trnIndextoCoord(idx))\n\n    # Figure out which relay cells have dendrites in de-inactivated state\n    self.relayOverlaps = self.relayConnections.computeActivity(\n      self.activeTRNCellIndices, 0.5\n    )\n    self.activeRelaySegments = np.flatnonzero(\n      self.relayOverlaps >= self.relayThreshold)\n    self.burstReadyCellIndices = self.relayConnections.mapSegmentsToCells(\n      self.activeRelaySegments)\n\n    self.burstReadyCells.reshape(-1)[self.burstReadyCellIndices] = 1", "response": "De - inactivate cells according to the L6 input."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the feedForwardActivity of the trnCells according to the l6Input.", "response": "def computeFeedForwardActivity(self, feedForwardInput):\n    \"\"\"\n    Activate trnCells according to the l6Input. These in turn will impact\n    bursting mode in relay cells that are connected to these trnCells.\n    Given the feedForwardInput, compute which cells will be silent, tonic,\n    or bursting.\n\n    :param feedForwardInput:\n      a numpy matrix of shape relayCellShape containing 0's and 1's\n\n    :return:\n      feedForwardInput is modified to contain 0, 1, or 2. A \"2\" indicates\n      bursting cells.\n    \"\"\"\n    ff = feedForwardInput.copy()\n    # For each relay cell, see if any of its FF inputs are active.\n    for x in range(self.relayWidth):\n      for y in range(self.relayHeight):\n        inputCells = self._preSynapticFFCells(x, y)\n        for idx in inputCells:\n          if feedForwardInput[idx] != 0:\n            ff[x, y] = 1.0\n            continue\n\n    # If yes, and it is in burst mode, this cell bursts\n    # If yes, and it is not in burst mode, then we just get tonic input.\n\n    # ff += self.burstReadyCells * ff\n    ff2 = ff * 0.4 + self.burstReadyCells * ff\n    return ff2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset(self):\n    self.trnOverlaps = []\n    self.activeTRNSegments = []\n    self.activeTRNCellIndices = []\n    self.relayOverlaps = []\n    self.activeRelaySegments = []\n    self.burstReadyCellIndices = []\n    self.burstReadyCells = np.zeros((self.relayWidth, self.relayHeight))", "response": "Reset all the internal state of the internal state to zero."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trnIndextoCoord(self, i):\n    x = i % self.trnWidth\n    y = i / self.trnWidth\n    return x, y", "response": "Map a 1D cell index to a 2D coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmaps a 1D cell index to a 2D coordinate.", "response": "def relayIndextoCoord(self, i):\n    \"\"\"\n    Map 1D cell index to a 2D coordinate\n\n    :param i: integer 1D cell index\n\n    :return: (x, y), a 2D coordinate\n    \"\"\"\n    x = i % self.relayWidth\n    y = i / self.relayWidth\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initializeTRNToRelayCellConnections(self):\n    for x in range(self.relayWidth):\n      for y in range(self.relayHeight):\n\n        # Create one dendrite for each trn cell that projects to this relay cell\n        # This dendrite contains one synapse corresponding to this TRN->relay\n        # connection.\n        relayCellIndex = self.relayCellIndex((x,y))\n        trnCells = self._preSynapticTRNCells(x, y)\n        for trnCell in trnCells:\n          newSegment = self.relayConnections.createSegments([relayCellIndex])\n          self.relayConnections.growSynapses(newSegment,\n                                             [self.trnCellIndex(trnCell)], 1.0)", "response": "Initialize the TRN to relay cell connectivity."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of the x y coordinates of all TRN cells that project to the given relay cell.", "response": "def _preSynapticTRNCells(self, i, j):\n    \"\"\"\n    Given a relay cell at the given coordinate, return a list of the (x,y)\n    coordinates of all TRN cells that project to it. This assumes a 3X3 fan-in.\n\n    :param i, j: relay cell Coordinates\n\n    :return:\n    \"\"\"\n    xmin = max(i - 1, 0)\n    xmax = min(i + 2, self.trnWidth)\n    ymin = max(j - 1, 0)\n    ymax = min(j + 2, self.trnHeight)\n    trnCells = [\n      (x, y) for x in range(xmin, xmax) for y in range(ymin, ymax)\n    ]\n\n    return trnCells"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of the x y coordinates of all feed forward cells that project to it.", "response": "def _preSynapticFFCells(self, i, j):\n    \"\"\"\n    Given a relay cell at the given coordinate, return a list of the (x,y)\n    coordinates of all feed forward cells that project to it. This assumes a 3X3\n    fan-in.\n\n    :param i, j: relay cell Coordinates\n\n    :return:\n    \"\"\"\n    xmin = max(i - 1, 0)\n    xmax = min(i + 2, self.inputWidth)\n    ymin = max(j - 1, 0)\n    ymax = min(j + 2, self.inputHeight)\n    inputCells = [\n      (x, y) for x in range(xmin, xmax) for y in range(ymin, ymax)\n    ]\n\n    return inputCells"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlearning a set of objects. Then try to recognize each object. Output an interactive visualization. @param locationModuleWidth (int) The cell dimensions of each module @param cellCoordinateOffsets (sequence) The \"cellCoordinateOffsets\" parameter for each module", "response": "def doExperiment(locationModuleWidth,\n                 bumpType,\n                 cellCoordinateOffsets,\n                 numObjects,\n                 featuresPerObject,\n                 objectWidth,\n                 numFeatures,\n                 featureDistribution,\n                 useTrace,\n                 useRawTrace,\n                 logCellActivity,\n                 logNumFeatureOccurrences,\n                 noiseFactor,\n                 moduleNoiseFactor,\n                 numModules,\n                 numSensations,\n                 thresholds,\n                 seed1,\n                 seed2,\n                 anchoringMethod):\n  \"\"\"\n  Learn a set of objects. Then try to recognize each object. Output an\n  interactive visualization.\n\n  @param locationModuleWidth (int)\n  The cell dimensions of each module\n\n  @param cellCoordinateOffsets (sequence)\n  The \"cellCoordinateOffsets\" parameter for each module\n  \"\"\"\n  if not os.path.exists(\"traces\"):\n    os.makedirs(\"traces\")\n\n  if seed1 != -1:\n    np.random.seed(seed1)\n\n  if seed2 != -1:\n    random.seed(seed2)\n\n  features = [str(i) for i in xrange(numFeatures)]\n  objects = generateObjects(numObjects, featuresPerObject, objectWidth,\n                            numFeatures, featureDistribution)\n\n  if logNumFeatureOccurrences:\n    featureOccurrences = collections.Counter(feat[\"name\"]\n                                             for obj in objects\n                                             for feat in obj[\"features\"])\n    occurrencesConvergenceLog = []\n\n  locationConfigs = []\n  scale = 40.0\n\n  if thresholds == -1:\n    thresholds = int(math.ceil(numModules*0.8))\n  elif thresholds == 0:\n    thresholds = numModules\n  perModRange = float((90.0 if bumpType == \"square\" else 60.0) /\n                      float(numModules))\n  for i in xrange(numModules):\n    orientation = (float(i) * perModRange) + (perModRange / 2.0)\n\n    config = {\n      \"cellsPerAxis\": locationModuleWidth,\n      \"scale\": scale,\n      \"orientation\": np.radians(orientation),\n      \"activationThreshold\": 8,\n      \"initialPermanence\": 1.0,\n      \"connectedPermanence\": 0.5,\n      \"learningThreshold\": 8,\n      \"sampleSize\": 10,\n      \"permanenceIncrement\": 0.1,\n      \"permanenceDecrement\": 0.0,\n    }\n\n    if bumpType == \"square\":\n      config[\"cellCoordinateOffsets\"] = cellCoordinateOffsets\n      config[\"anchoringMethod\"] = anchoringMethod\n    elif bumpType == \"gaussian\":\n      config[\"bumpOverlapMethod\"] = \"probabilistic\"\n      config[\"baselineCellsPerAxis\"] = 6\n    else:\n      raise ValueError(\"Invalid bumpType\", bumpType)\n\n    locationConfigs.append(config)\n\n  l4Overrides = {\n    \"initialPermanence\": 1.0,\n    \"activationThreshold\": thresholds,\n    \"reducedBasalThreshold\": thresholds,\n    \"minThreshold\": numModules,\n    \"sampleSize\": numModules,\n    \"cellsPerColumn\": 16,\n  }\n\n  column = PIUNCorticalColumn(locationConfigs, L4Overrides=l4Overrides,\n                              bumpType=bumpType)\n  exp = PIUNExperiment(column, featureNames=features,\n                       numActiveMinicolumns=10,\n                       noiseFactor=noiseFactor,\n                       moduleNoiseFactor=moduleNoiseFactor)\n\n  for objectDescription in objects:\n    exp.learnObject(objectDescription)\n\n  convergence = collections.defaultdict(int)\n\n  try:\n    if useTrace:\n      filename = os.path.join(\n        SCRIPT_DIR,\n        \"traces/{}-points-{}-cells-{}-objects-{}-feats.html\".format(\n          len(cellCoordinateOffsets)**2, exp.column.L6aModules[0].numberOfCells(),\n          numObjects, numFeatures)\n      )\n      traceFileOut = io.open(filename, \"w\", encoding=\"utf8\")\n      traceHandle = trace(traceFileOut, exp, includeSynapses=True)\n      print \"Logging to\", filename\n\n    if useRawTrace:\n      rawFilename = os.path.join(\n        SCRIPT_DIR,\n        \"traces/{}-points-{}-cells-{}-objects-{}-feats.trace\".format(\n          len(cellCoordinateOffsets)**2, exp.column.L6aModules[0].numberOfCells(),\n          numObjects, numFeatures)\n      )\n      rawTraceFileOut = open(rawFilename, \"w\")\n      rawTraceHandle = rawTrace(rawTraceFileOut, exp, includeSynapses=False)\n      print \"Logging to\", rawFilename\n\n    if logCellActivity:\n      cellActivityTracer = PIUNCellActivityTracer(exp)\n      exp.addMonitor(cellActivityTracer)\n\n    for objectDescription in objects:\n\n      numSensationsToInference = exp.inferObjectWithRandomMovements(\n        objectDescription, numSensations)\n\n      if logNumFeatureOccurrences:\n        objectFeatureOccurrences = sorted(featureOccurrences[feat[\"name\"]]\n                                          for feat in objectDescription[\"features\"])\n        occurrencesConvergenceLog.append(\n          (objectFeatureOccurrences, numSensationsToInference))\n      convergence[numSensationsToInference] += 1\n      if numSensationsToInference is None:\n        print 'Failed to infer object \"{}\"'.format(objectDescription[\"name\"])\n  finally:\n    if useTrace:\n      traceHandle.__exit__()\n      traceFileOut.close()\n\n    if useRawTrace:\n      rawTraceHandle.__exit__()\n      rawTraceFileOut.close()\n\n  for step, num in sorted(convergence.iteritems()):\n    print \"{}: {}\".format(step, num)\n\n  result = {\n    \"convergence\": convergence,\n  }\n\n  if bumpType == \"gaussian\":\n    result[\"bumpSigma\"] = column.L6aModules[0].bumpSigma\n\n  if logCellActivity:\n    result[\"locationLayerTimelineByObject\"] = (\n      cellActivityTracer.locationLayerTimelineByObject)\n    result[\"inferredStepByObject\"] = cellActivityTracer.inferredStepByObject\n\n  if logNumFeatureOccurrences:\n    result[\"occurrencesConvergenceLog\"] = occurrencesConvergenceLog\n\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning multiple experiments in parallel.", "response": "def runMultiprocessNoiseExperiment(resultName, repeat, numWorkers,\n                                   appendResults, **kwargs):\n  \"\"\"\n  :param kwargs: Pass lists to distribute as lists, lists that should be passed intact as tuples.\n  :return: results, in the format [(arguments, results)].  Also saved to json at resultName, in the same format.\n  \"\"\"\n  experiments = [{}]\n  for key, values in kwargs.items():\n    if type(values) is list:\n      newExperiments = []\n      for experiment in experiments:\n        for val in values:\n          newExperiment = copy(experiment)\n          newExperiment[key] = val\n          newExperiments.append(newExperiment)\n      experiments = newExperiments\n    else:\n      [a.__setitem__(key, values) for a in experiments]\n\n  newExperiments = []\n  for experiment in experiments:\n    for _ in xrange(repeat):\n      newExperiments.append(copy(experiment))\n  experiments = newExperiments\n\n  return runExperiments(experiments, resultName, numWorkers, appendResults)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef countWordOverlapFrequencies(filename=\"goodOverlapPairs.pkl\"):\n  with open(filename,\"rb\") as f:\n    goodOverlapPairs = pickle.load(f)\n\n  with open(\"word_bitmaps_40_bits_minimum.pkl\",\"rb\") as f:\n    bitmaps = pickle.load(f)\n\n  # Count how often each word has a highly overlapping match with other words\n  wordFrequencies = {}\n  for w1, w2, overlap in goodOverlapPairs:\n    wordFrequencies[w1] = wordFrequencies.get(w1, 0) + 1\n\n\n  printTemplate = PrettyTable([\"Num High Overlaps\", \"Word\", \"On Bits\"],\n                              sortby=\"Num High Overlaps\", reversesort=True)\n\n  for word in wordFrequencies.iterkeys():\n    printTemplate.add_row([wordFrequencies[word], word, len(bitmaps[word])])\n\n  print printTemplate", "response": "Count how many high overlaps each word has and print it out\n "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset(self):\n    self.activeCells = np.empty(0, dtype=\"uint32\")\n    self.winnerCells = np.empty(0, dtype=\"uint32\")\n    self.predictedCells = np.empty(0, dtype=\"uint32\")\n    self.predictedActiveCells = np.empty(0, dtype=\"uint32\")\n    self.activeBasalSegments = np.empty(0, dtype=\"uint32\")\n    self.activeApicalSegments = np.empty(0, dtype=\"uint32\")\n    self.matchingBasalSegments = np.empty(0, dtype=\"uint32\")\n    self.matchingApicalSegments = np.empty(0, dtype=\"uint32\")\n    self.basalPotentialOverlaps = np.empty(0, dtype=\"int32\")\n    self.apicalPotentialOverlaps = np.empty(0, dtype=\"int32\")", "response": "Resets all cells and segments in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate predictions. @param basalInput (numpy array) List of active input bits for the basal dendrite segments @param apicalInput (numpy array) List of active input bits for the apical dendrite segments @param learn (bool) Whether learning is enabled. Some TM implementations may depolarize cells differently or do segment activity bookkeeping when learning is enabled.", "response": "def depolarizeCells(self, basalInput, apicalInput, learn):\n    \"\"\"\n    Calculate predictions.\n\n    @param basalInput (numpy array)\n    List of active input bits for the basal dendrite segments\n\n    @param apicalInput (numpy array)\n    List of active input bits for the apical dendrite segments\n\n    @param learn (bool)\n    Whether learning is enabled. Some TM implementations may depolarize cells\n    differently or do segment activity bookkeeping when learning is enabled.\n    \"\"\"\n    # Calculate predictions for this timestep\n    (activeApicalSegments,\n     matchingApicalSegments,\n     apicalPotentialOverlaps) = self._calculateSegmentActivity(\n       self.apicalConnections, apicalInput, self.connectedPermanence,\n       self.activationThreshold, self.minThreshold, self.reducedBasalThreshold)\n\n    apicallySupportedCells = self.apicalConnections.mapSegmentsToCells(\n      activeApicalSegments)\n    if not self.disableApicalDependence:\n      (activeBasalSegments,\n       matchingBasalSegments,\n       basalPotentialOverlaps) = self._calculateSegmentActivity(\n         self.basalConnections, basalInput,\n         self.connectedPermanence, self.activationThreshold,\n         self.minThreshold, self.reducedBasalThreshold,\n         reducedThresholdCells = apicallySupportedCells,)\n\n      predictedCells = np.intersect1d(\n        self.basalConnections.mapSegmentsToCells(activeBasalSegments),\n        apicallySupportedCells)\n    else:\n      (activeBasalSegments,\n      matchingBasalSegments,\n      basalPotentialOverlaps) = self._calculateSegmentActivity(\n        self.basalConnections, basalInput, self.connectedPermanence,\n        self.activationThreshold, self.minThreshold, self.reducedBasalThreshold)\n\n      predictedCells = self.basalConnections.mapSegmentsToCells(activeBasalSegments)\n\n    self.predictedCells = predictedCells\n    self.activeBasalSegments = activeBasalSegments\n    self.activeApicalSegments = activeApicalSegments\n    self.matchingBasalSegments = matchingBasalSegments\n    self.matchingApicalSegments = matchingApicalSegments\n    self.basalPotentialOverlaps = basalPotentialOverlaps\n    self.apicalPotentialOverlaps = apicalPotentialOverlaps"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nactivates cells in the specified columns using the result of previous previous depolarizeCells as predictions. Then learn.", "response": "def activateCells(self,\n                    activeColumns,\n                    basalReinforceCandidates,\n                    apicalReinforceCandidates,\n                    basalGrowthCandidates,\n                    apicalGrowthCandidates,\n                    learn=True):\n    \"\"\"\n    Activate cells in the specified columns, using the result of the previous\n    'depolarizeCells' as predictions. Then learn.\n\n    @param activeColumns (numpy array)\n    List of active columns\n\n    @param basalReinforceCandidates (numpy array)\n    List of bits that the active cells may reinforce basal synapses to.\n\n    @param apicalReinforceCandidates (numpy array)\n    List of bits that the active cells may reinforce apical synapses to.\n\n    @param basalGrowthCandidates (numpy array or None)\n    List of bits that the active cells may grow new basal synapses to.\n\n    @param apicalGrowthCandidates (numpy array or None)\n    List of bits that the active cells may grow new apical synapses to\n\n    @param learn (bool)\n    Whether to grow / reinforce / punish synapses\n    \"\"\"\n\n    # Calculate active cells\n    (correctPredictedCells,\n     burstingColumns) = np2.setCompare(self.predictedCells, activeColumns,\n                                       self.predictedCells / self.cellsPerColumn,\n                                       rightMinusLeft=True)\n\n    newActiveCells = np.concatenate((correctPredictedCells,\n                                     np2.getAllCellsInColumns(\n                                       burstingColumns, self.cellsPerColumn)))\n\n    # Calculate learning\n    (learningActiveBasalSegments,\n     learningActiveApicalSegments,\n     learningMatchingBasalSegments,\n     learningMatchingApicalSegments,\n     basalSegmentsToPunish,\n     apicalSegmentsToPunish,\n     newSegmentCells,\n     learningCells) = self._calculateLearning(activeColumns,\n                                              burstingColumns,\n                                              correctPredictedCells,\n                                              self.activeBasalSegments,\n                                              self.activeApicalSegments,\n                                              self.matchingBasalSegments,\n                                              self.matchingApicalSegments,\n                                              self.basalPotentialOverlaps,\n                                              self.apicalPotentialOverlaps)\n\n    if learn:\n      # Learn on existing segments\n      for learningSegments in (learningActiveBasalSegments,\n                               learningMatchingBasalSegments):\n        self._learn(self.basalConnections, self.rng, learningSegments,\n                    basalReinforceCandidates, basalGrowthCandidates,\n                    self.basalPotentialOverlaps,\n                    self.initialPermanence, self.sampleSize,\n                    self.permanenceIncrement, self.permanenceDecrement,\n                    self.maxSynapsesPerSegment)\n\n      for learningSegments in (learningActiveApicalSegments,\n                               learningMatchingApicalSegments):\n        self._learn(self.apicalConnections, self.rng, learningSegments,\n                    apicalReinforceCandidates, apicalGrowthCandidates,\n                    self.apicalPotentialOverlaps, self.initialPermanence,\n                    self.sampleSize, self.permanenceIncrement,\n                    self.permanenceDecrement, self.maxSynapsesPerSegment)\n\n      # Punish incorrect predictions\n      if self.basalPredictedSegmentDecrement != 0.0:\n        self.basalConnections.adjustActiveSynapses(\n          basalSegmentsToPunish, basalReinforceCandidates,\n          -self.basalPredictedSegmentDecrement)\n\n      if self.apicalPredictedSegmentDecrement != 0.0:\n        self.apicalConnections.adjustActiveSynapses(\n          apicalSegmentsToPunish, apicalReinforceCandidates,\n          -self.apicalPredictedSegmentDecrement)\n\n      # Only grow segments if there is basal *and* apical input.\n      if len(basalGrowthCandidates) > 0 and len(apicalGrowthCandidates) > 0:\n        self._learnOnNewSegments(self.basalConnections, self.rng,\n                                 newSegmentCells, basalGrowthCandidates,\n                                 self.initialPermanence, self.sampleSize,\n                                 self.maxSynapsesPerSegment)\n        self._learnOnNewSegments(self.apicalConnections, self.rng,\n                                 newSegmentCells, apicalGrowthCandidates,\n                                 self.initialPermanence, self.sampleSize,\n                                 self.maxSynapsesPerSegment)\n\n\n    # Save the results\n    newActiveCells.sort()\n    learningCells.sort()\n    self.activeCells = newActiveCells\n    self.winnerCells = learningCells\n    self.predictedActiveCells = correctPredictedCells"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculateLearning(self,\n                         activeColumns,\n                         burstingColumns,\n                         correctPredictedCells,\n                         activeBasalSegments,\n                         activeApicalSegments,\n                         matchingBasalSegments,\n                         matchingApicalSegments,\n                         basalPotentialOverlaps,\n                         apicalPotentialOverlaps):\n    \"\"\"\n    Learning occurs on pairs of segments. Correctly predicted cells always have\n    active basal and apical segments, and we learn on these segments. In\n    bursting columns, we either learn on an existing segment pair, or we grow a\n    new pair of segments.\n\n    @param activeColumns (numpy array)\n    @param burstingColumns (numpy array)\n    @param correctPredictedCells (numpy array)\n    @param activeBasalSegments (numpy array)\n    @param activeApicalSegments (numpy array)\n    @param matchingBasalSegments (numpy array)\n    @param matchingApicalSegments (numpy array)\n    @param basalPotentialOverlaps (numpy array)\n    @param apicalPotentialOverlaps (numpy array)\n\n    @return (tuple)\n    - learningActiveBasalSegments (numpy array)\n      Active basal segments on correct predicted cells\n\n    - learningActiveApicalSegments (numpy array)\n      Active apical segments on correct predicted cells\n\n    - learningMatchingBasalSegments (numpy array)\n      Matching basal segments selected for learning in bursting columns\n\n    - learningMatchingApicalSegments (numpy array)\n      Matching apical segments selected for learning in bursting columns\n\n    - basalSegmentsToPunish (numpy array)\n      Basal segments that should be punished for predicting an inactive column\n\n    - apicalSegmentsToPunish (numpy array)\n      Apical segments that should be punished for predicting an inactive column\n\n    - newSegmentCells (numpy array)\n      Cells in bursting columns that were selected to grow new segments\n\n    - learningCells (numpy array)\n      Every cell that has a learning segment or was selected to grow a segment\n    \"\"\"\n\n    # Correctly predicted columns\n    learningActiveBasalSegments = self.basalConnections.filterSegmentsByCell(\n      activeBasalSegments, correctPredictedCells)\n    learningActiveApicalSegments = self.apicalConnections.filterSegmentsByCell(\n      activeApicalSegments, correctPredictedCells)\n\n    # Bursting columns\n    cellsForMatchingBasal = self.basalConnections.mapSegmentsToCells(\n      matchingBasalSegments)\n    cellsForMatchingApical = self.apicalConnections.mapSegmentsToCells(\n      matchingApicalSegments)\n    matchingCells = np.intersect1d(\n      cellsForMatchingBasal, cellsForMatchingApical)\n\n    (matchingCellsInBurstingColumns,\n     burstingColumnsWithNoMatch) = np2.setCompare(\n       matchingCells, burstingColumns, matchingCells / self.cellsPerColumn,\n       rightMinusLeft=True)\n\n    (learningMatchingBasalSegments,\n     learningMatchingApicalSegments) = self._chooseBestSegmentPairPerColumn(\n       matchingCellsInBurstingColumns, matchingBasalSegments,\n       matchingApicalSegments, basalPotentialOverlaps, apicalPotentialOverlaps)\n    newSegmentCells = self._getCellsWithFewestSegments(\n      burstingColumnsWithNoMatch)\n\n    # Incorrectly predicted columns\n    if self.basalPredictedSegmentDecrement > 0.0:\n      correctMatchingBasalMask = np.in1d(\n        cellsForMatchingBasal / self.cellsPerColumn, activeColumns)\n      basalSegmentsToPunish = matchingBasalSegments[~correctMatchingBasalMask]\n    else:\n      basalSegmentsToPunish = ()\n\n    if self.apicalPredictedSegmentDecrement > 0.0:\n      correctMatchingApicalMask = np.in1d(\n        cellsForMatchingApical / self.cellsPerColumn, activeColumns)\n      apicalSegmentsToPunish = matchingApicalSegments[~correctMatchingApicalMask]\n    else:\n      apicalSegmentsToPunish = ()\n\n    # Make a list of every cell that is learning\n    learningCells =  np.concatenate(\n      (correctPredictedCells,\n       self.basalConnections.mapSegmentsToCells(learningMatchingBasalSegments),\n       newSegmentCells))\n\n    return (learningActiveBasalSegments,\n            learningActiveApicalSegments,\n            learningMatchingBasalSegments,\n            learningMatchingApicalSegments,\n            basalSegmentsToPunish,\n            apicalSegmentsToPunish,\n            newSegmentCells,\n            learningCells)", "response": "This function calculates the learning of segments on a single segment pair."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calculateSegmentActivity(connections, activeInput, connectedPermanence,\n                                activationThreshold, minThreshold,\n                                reducedThreshold,\n                                reducedThresholdCells = ()):\n    \"\"\"\n    Calculate the active and matching basal segments for this timestep.\n\n    @param connections (SparseMatrixConnections)\n    @param activeInput (numpy array)\n\n    @return (tuple)\n    - activeSegments (numpy array)\n      Dendrite segments with enough active connected synapses to cause a\n      dendritic spike\n\n    - matchingSegments (numpy array)\n      Dendrite segments with enough active potential synapses to be selected for\n      learning in a bursting column\n\n    - potentialOverlaps (numpy array)\n      The number of active potential synapses for each segment.\n      Includes counts for active, matching, and nonmatching segments.\n    \"\"\"\n    # Active apical segments lower the activation threshold for basal segments\n    overlaps = connections.computeActivity(activeInput, connectedPermanence)\n    outrightActiveSegments = np.flatnonzero(overlaps >= activationThreshold)\n    if (reducedThreshold != activationThreshold and\n            len(reducedThresholdCells) > 0):\n        potentiallyActiveSegments = np.flatnonzero(\n            (overlaps < activationThreshold) & (overlaps >= reducedThreshold))\n        cellsOfCASegments = connections.mapSegmentsToCells(\n            potentiallyActiveSegments)\n        # apically active segments are condit. active segments from apically\n        # active cells\n        conditionallyActiveSegments = potentiallyActiveSegments[\n            np.in1d(cellsOfCASegments, reducedThresholdCells)]\n        activeSegments = np.concatenate((outrightActiveSegments,\n                                         conditionallyActiveSegments))\n    else:\n        activeSegments = outrightActiveSegments\n\n\n\n    # Matching\n    potentialOverlaps = connections.computeActivity(activeInput)\n    matchingSegments = np.flatnonzero(potentialOverlaps >= minThreshold)\n\n    return (activeSegments,\n            matchingSegments,\n            potentialOverlaps)", "response": "Calculate the active and matching basal segments for this timestep."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlearn new segments and grow synapses on them.", "response": "def _learnOnNewSegments(connections, rng, newSegmentCells, growthCandidates,\n                          initialPermanence, sampleSize, maxSynapsesPerSegment):\n    \"\"\"\n    Create new segments, and grow synapses on them.\n\n    @param connections (SparseMatrixConnections)\n    @param rng (Random)\n    @param newSegmentCells (numpy array)\n    @param growthCandidates (numpy array)\n    \"\"\"\n\n    numNewSynapses = len(growthCandidates)\n\n    if sampleSize != -1:\n      numNewSynapses = min(numNewSynapses, sampleSize)\n\n    if maxSynapsesPerSegment != -1:\n      numNewSynapses = min(numNewSynapses, maxSynapsesPerSegment)\n\n    newSegments = connections.createSegments(newSegmentCells)\n    connections.growSynapsesToSample(newSegments, growthCandidates,\n                                     numNewSynapses, initialPermanence,\n                                     rng)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchoose the best pair of matching segments - one basal and one apical - for each column. Pairs are ranked by the sum of their potential overlaps. When there's a tie, the first pair wins. @param matchingCellsInBurstingColumns (numpy array) Cells in bursting columns that have at least one matching basal segment and at least one matching apical segment @param matchingBasalSegments (numpy array) @param matchingApicalSegments (numpy array) @param basalPotentialOverlaps (numpy array) @param apicalPotentialOverlaps (numpy array) @return (tuple) - learningBasalSegments (numpy array) The selected basal segments - learningApicalSegments (numpy array) The selected apical segments", "response": "def _chooseBestSegmentPairPerColumn(self,\n                                      matchingCellsInBurstingColumns,\n                                      matchingBasalSegments,\n                                      matchingApicalSegments,\n                                      basalPotentialOverlaps,\n                                      apicalPotentialOverlaps):\n    \"\"\"\n    Choose the best pair of matching segments - one basal and one apical - for\n    each column. Pairs are ranked by the sum of their potential overlaps.\n    When there's a tie, the first pair wins.\n\n    @param matchingCellsInBurstingColumns (numpy array)\n    Cells in bursting columns that have at least one matching basal segment and\n    at least one matching apical segment\n\n    @param matchingBasalSegments (numpy array)\n    @param matchingApicalSegments (numpy array)\n    @param basalPotentialOverlaps (numpy array)\n    @param apicalPotentialOverlaps (numpy array)\n\n    @return (tuple)\n    - learningBasalSegments (numpy array)\n      The selected basal segments\n\n    - learningApicalSegments (numpy array)\n      The selected apical segments\n    \"\"\"\n\n    basalCandidateSegments = self.basalConnections.filterSegmentsByCell(\n      matchingBasalSegments, matchingCellsInBurstingColumns)\n    apicalCandidateSegments = self.apicalConnections.filterSegmentsByCell(\n      matchingApicalSegments, matchingCellsInBurstingColumns)\n\n    # Sort everything once rather than inside of each call to argmaxMulti.\n    self.basalConnections.sortSegmentsByCell(basalCandidateSegments)\n    self.apicalConnections.sortSegmentsByCell(apicalCandidateSegments)\n\n    # Narrow it down to one pair per cell.\n    oneBasalPerCellFilter = np2.argmaxMulti(\n      basalPotentialOverlaps[basalCandidateSegments],\n      self.basalConnections.mapSegmentsToCells(basalCandidateSegments),\n      assumeSorted=True)\n    basalCandidateSegments = basalCandidateSegments[oneBasalPerCellFilter]\n    oneApicalPerCellFilter = np2.argmaxMulti(\n      apicalPotentialOverlaps[apicalCandidateSegments],\n      self.apicalConnections.mapSegmentsToCells(apicalCandidateSegments),\n      assumeSorted=True)\n    apicalCandidateSegments = apicalCandidateSegments[oneApicalPerCellFilter]\n\n    # Narrow it down to one pair per column.\n    cellScores = (basalPotentialOverlaps[basalCandidateSegments] +\n                  apicalPotentialOverlaps[apicalCandidateSegments])\n    columnsForCandidates = (\n      self.basalConnections.mapSegmentsToCells(basalCandidateSegments) /\n      self.cellsPerColumn)\n    onePerColumnFilter = np2.argmaxMulti(cellScores, columnsForCandidates,\n                                         assumeSorted=True)\n\n    learningBasalSegments = basalCandidateSegments[onePerColumnFilter]\n    learningApicalSegments = apicalCandidateSegments[onePerColumnFilter]\n\n    return (learningBasalSegments,\n            learningApicalSegments)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _getCellsWithFewestSegments(self, columns):\n    candidateCells = np2.getAllCellsInColumns(columns, self.cellsPerColumn)\n\n    # Arrange the segment counts into one row per minicolumn.\n    segmentCounts = np.reshape(\n      self.basalConnections.getSegmentCounts(candidateCells) +\n      self.apicalConnections.getSegmentCounts(candidateCells),\n      newshape=(len(columns),\n                self.cellsPerColumn))\n\n    # Filter to just the cells that are tied for fewest in their minicolumn.\n    minSegmentCounts = np.amin(segmentCounts, axis=1, keepdims=True)\n    candidateCells = candidateCells[np.flatnonzero(segmentCounts ==\n                                                   minSegmentCounts)]\n\n    # Filter to one cell per column, choosing randomly from the minimums.\n    # To do the random choice, add a random offset to each index in-place, using\n    # casting to floor the result.\n    (_,\n     onePerColumnFilter,\n     numCandidatesInColumns) = np.unique(candidateCells / self.cellsPerColumn,\n                                         return_index=True, return_counts=True)\n\n    offsetPercents = np.empty(len(columns), dtype=\"float32\")\n    self.rng.initializeReal32Array(offsetPercents)\n\n    np.add(onePerColumnFilter,\n           offsetPercents*numCandidatesInColumns,\n           out=onePerColumnFilter,\n           casting=\"unsafe\")\n\n    return candidateCells[onePerColumnFilter]", "response": "Get the cell that has the fewest total segments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform one timestep. Use the basal and apical input to form a set of basal and apical input bits then activate the specified columns then learn.", "response": "def compute(self,\n              activeColumns,\n              basalInput,\n              apicalInput=(),\n              basalGrowthCandidates=None,\n              apicalGrowthCandidates=None,\n              learn=True):\n    \"\"\"\n    Perform one timestep. Use the basal and apical input to form a set of\n    predictions, then activate the specified columns, then learn.\n\n    @param activeColumns (numpy array)\n    List of active columns\n\n    @param basalInput (numpy array)\n    List of active input bits for the basal dendrite segments\n\n    @param apicalInput (numpy array)\n    List of active input bits for the apical dendrite segments\n\n    @param basalGrowthCandidates (numpy array or None)\n    List of bits that the active cells may grow new basal synapses to.\n    If None, the basalInput is assumed to be growth candidates.\n\n    @param apicalGrowthCandidates (numpy array or None)\n    List of bits that the active cells may grow new apical synapses to\n    If None, the apicalInput is assumed to be growth candidates.\n\n    @param learn (bool)\n    Whether to grow / reinforce / punish synapses\n    \"\"\"\n    activeColumns = np.asarray(activeColumns)\n    basalInput = np.asarray(basalInput)\n    apicalInput = np.asarray(apicalInput)\n\n    if basalGrowthCandidates is None:\n      basalGrowthCandidates = basalInput\n    basalGrowthCandidates = np.asarray(basalGrowthCandidates)\n\n    if apicalGrowthCandidates is None:\n      apicalGrowthCandidates = apicalInput\n    apicalGrowthCandidates = np.asarray(apicalGrowthCandidates)\n\n    self.depolarizeCells(basalInput, apicalInput, learn)\n    self.activateCells(activeColumns, basalInput, apicalInput,\n                       basalGrowthCandidates, apicalGrowthCandidates, learn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute(self,\n              activeColumns,\n              apicalInput=(),\n              apicalGrowthCandidates=None,\n              learn=True):\n    \"\"\"\n    Perform one timestep. Activate the specified columns, using the predictions\n    from the previous timestep, then learn. Then form a new set of predictions\n    using the new active cells and the apicalInput.\n\n    @param activeColumns (numpy array)\n    List of active columns\n\n    @param apicalInput (numpy array)\n    List of active input bits for the apical dendrite segments\n\n    @param apicalGrowthCandidates (numpy array or None)\n    List of bits that the active cells may grow new apical synapses to\n    If None, the apicalInput is assumed to be growth candidates.\n\n    @param learn (bool)\n    Whether to grow / reinforce / punish synapses\n    \"\"\"\n    activeColumns = np.asarray(activeColumns)\n    apicalInput = np.asarray(apicalInput)\n\n    if apicalGrowthCandidates is None:\n      apicalGrowthCandidates = apicalInput\n    apicalGrowthCandidates = np.asarray(apicalGrowthCandidates)\n\n    self.prevPredictedCells = self.predictedCells\n\n    self.activateCells(activeColumns, self.activeCells, self.prevApicalInput,\n                       self.winnerCells, self.prevApicalGrowthCandidates, learn)\n    self.depolarizeCells(self.activeCells, apicalInput, learn)\n\n    self.prevApicalInput = apicalInput.copy()\n    self.prevApicalGrowthCandidates = apicalGrowthCandidates.copy()", "response": "Perform one timestep. Activate the specified columns then learn the specified cells then form a new set of predictions\n    using the new active cells and the apical input bits and the apicalGrowthCandidates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a coordinate encoder for encoding locations into sparse distributed representations.", "response": "def createLocationEncoder(t, w=15):\n  \"\"\"\n  A default coordinate encoder for encoding locations into sparse\n  distributed representations.\n  \"\"\"\n  encoder = CoordinateEncoder(name=\"positionEncoder\", n=t.l6CellCount, w=w)\n  return encoder"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a union of all locations that correspond to the union of all locations \u00b0x r.", "response": "def getUnionLocations(encoder, x, y, r, step=1):\n  \"\"\"\n  Return a union of location encodings that correspond to the union of all locations\n  within the specified circle.\n  \"\"\"\n  output = np.zeros(encoder.getWidth(), dtype=defaultDtype)\n  locations = set()\n  for dx in range(-r, r+1, step):\n    for dy in range(-r, r+1, step):\n      if dx*dx + dy*dy <= r*r:\n        e = encodeLocation(encoder, x+dx, y+dy, output)\n        locations = locations.union(set(e))\n\n  return locations"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef depolarizeCells(self, basalInput, apicalInput, learn):\n    (activeApicalSegments,\n     matchingApicalSegments,\n     apicalPotentialOverlaps) = self._calculateApicalSegmentActivity(\n       self.apicalConnections, apicalInput, self.connectedPermanence,\n       self.activationThreshold, self.minThreshold)\n\n    if learn or self.useApicalModulationBasalThreshold==False:\n      reducedBasalThresholdCells = ()\n    else:\n      reducedBasalThresholdCells = self.apicalConnections.mapSegmentsToCells(\n        activeApicalSegments)\n\n    (activeBasalSegments,\n     matchingBasalSegments,\n     basalPotentialOverlaps) = self._calculateBasalSegmentActivity(\n       self.basalConnections, basalInput, reducedBasalThresholdCells,\n       self.connectedPermanence,\n       self.activationThreshold, self.minThreshold, self.reducedBasalThreshold)\n\n    predictedCells = self._calculatePredictedCells(activeBasalSegments,\n                                                   activeApicalSegments)\n\n    self.predictedCells = predictedCells\n    self.activeBasalSegments = activeBasalSegments\n    self.activeApicalSegments = activeApicalSegments\n    self.matchingBasalSegments = matchingBasalSegments\n    self.matchingApicalSegments = matchingApicalSegments\n    self.basalPotentialOverlaps = basalPotentialOverlaps\n    self.apicalPotentialOverlaps = apicalPotentialOverlaps", "response": "This method depolarizes the cells in a single segment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calculateBasalLearning(self,\n                              activeColumns,\n                              burstingColumns,\n                              correctPredictedCells,\n                              activeBasalSegments,\n                              matchingBasalSegments,\n                              basalPotentialOverlaps):\n    \"\"\"\n    Basic Temporal Memory learning. Correctly predicted cells always have\n    active basal segments, and we learn on these segments. In bursting\n    columns, we either learn on an existing basal segment, or we grow a new one.\n\n    The only influence apical dendrites have on basal learning is: the apical\n    dendrites influence which cells are considered \"predicted\". So an active\n    apical dendrite can prevent some basal segments in active columns from\n    learning.\n\n    @param correctPredictedCells (numpy array)\n    @param burstingColumns (numpy array)\n    @param activeBasalSegments (numpy array)\n    @param matchingBasalSegments (numpy array)\n    @param basalPotentialOverlaps (numpy array)\n\n    @return (tuple)\n    - learningActiveBasalSegments (numpy array)\n      Active basal segments on correct predicted cells\n\n    - learningMatchingBasalSegments (numpy array)\n      Matching basal segments selected for learning in bursting columns\n\n    - basalSegmentsToPunish (numpy array)\n      Basal segments that should be punished for predicting an inactive column\n\n    - newBasalSegmentCells (numpy array)\n      Cells in bursting columns that were selected to grow new basal segments\n\n    - learningCells (numpy array)\n      Cells that have learning basal segments or are selected to grow a basal\n      segment\n    \"\"\"\n\n    # Correctly predicted columns\n    learningActiveBasalSegments = self.basalConnections.filterSegmentsByCell(\n      activeBasalSegments, correctPredictedCells)\n\n    cellsForMatchingBasal = self.basalConnections.mapSegmentsToCells(\n      matchingBasalSegments)\n    matchingCells = np.unique(cellsForMatchingBasal)\n\n    (matchingCellsInBurstingColumns,\n     burstingColumnsWithNoMatch) = np2.setCompare(\n       matchingCells, burstingColumns, matchingCells / self.cellsPerColumn,\n       rightMinusLeft=True)\n\n    learningMatchingBasalSegments = self._chooseBestSegmentPerColumn(\n      self.basalConnections, matchingCellsInBurstingColumns,\n      matchingBasalSegments, basalPotentialOverlaps, self.cellsPerColumn)\n    newBasalSegmentCells = self._getCellsWithFewestSegments(\n      self.basalConnections, self.rng, burstingColumnsWithNoMatch,\n      self.cellsPerColumn)\n\n    learningCells = np.concatenate(\n      (correctPredictedCells,\n       self.basalConnections.mapSegmentsToCells(learningMatchingBasalSegments),\n       newBasalSegmentCells))\n\n    # Incorrectly predicted columns\n    correctMatchingBasalMask = np.in1d(\n      cellsForMatchingBasal / self.cellsPerColumn, activeColumns)\n\n    basalSegmentsToPunish = matchingBasalSegments[~correctMatchingBasalMask]\n\n    return (learningActiveBasalSegments,\n            learningMatchingBasalSegments,\n            basalSegmentsToPunish,\n            newBasalSegmentCells,\n            learningCells)", "response": "Calculates basal learning for the given cells in the active columns and bursting columns."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating apical learning for each learning cell.", "response": "def _calculateApicalLearning(self,\n                               learningCells,\n                               activeColumns,\n                               activeApicalSegments,\n                               matchingApicalSegments,\n                               apicalPotentialOverlaps):\n    \"\"\"\n    Calculate apical learning for each learning cell.\n\n    The set of learning cells was determined completely from basal segments.\n    Do all apical learning on the same cells.\n\n    Learn on any active segments on learning cells. For cells without active\n    segments, learn on the best matching segment. For cells without a matching\n    segment, grow a new segment.\n\n    @param learningCells (numpy array)\n    @param correctPredictedCells (numpy array)\n    @param activeApicalSegments (numpy array)\n    @param matchingApicalSegments (numpy array)\n    @param apicalPotentialOverlaps (numpy array)\n\n    @return (tuple)\n    - learningActiveApicalSegments (numpy array)\n      Active apical segments on correct predicted cells\n\n    - learningMatchingApicalSegments (numpy array)\n      Matching apical segments selected for learning in bursting columns\n\n    - apicalSegmentsToPunish (numpy array)\n      Apical segments that should be punished for predicting an inactive column\n\n    - newApicalSegmentCells (numpy array)\n      Cells in bursting columns that were selected to grow new apical segments\n    \"\"\"\n\n    # Cells with active apical segments\n    learningActiveApicalSegments = self.apicalConnections.filterSegmentsByCell(\n      activeApicalSegments, learningCells)\n\n    # Cells with matching apical segments\n    learningCellsWithoutActiveApical = np.setdiff1d(\n      learningCells,\n      self.apicalConnections.mapSegmentsToCells(learningActiveApicalSegments))\n    cellsForMatchingApical = self.apicalConnections.mapSegmentsToCells(\n      matchingApicalSegments)\n    learningCellsWithMatchingApical = np.intersect1d(\n      learningCellsWithoutActiveApical, cellsForMatchingApical)\n    learningMatchingApicalSegments = self._chooseBestSegmentPerCell(\n      self.apicalConnections, learningCellsWithMatchingApical,\n      matchingApicalSegments, apicalPotentialOverlaps)\n\n    # Cells that need to grow an apical segment\n    newApicalSegmentCells = np.setdiff1d(learningCellsWithoutActiveApical,\n                                         learningCellsWithMatchingApical)\n\n    # Incorrectly predicted columns\n    correctMatchingApicalMask = np.in1d(\n      cellsForMatchingApical / self.cellsPerColumn, activeColumns)\n\n    apicalSegmentsToPunish = matchingApicalSegments[~correctMatchingApicalMask]\n\n    return (learningActiveApicalSegments,\n            learningMatchingApicalSegments,\n            apicalSegmentsToPunish,\n            newApicalSegmentCells)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calculateApicalSegmentActivity(connections, activeInput, connectedPermanence,\n                                activationThreshold, minThreshold):\n    \"\"\"\n    Calculate the active and matching apical segments for this timestep.\n\n    @param connections (SparseMatrixConnections)\n    @param activeInput (numpy array)\n\n    @return (tuple)\n    - activeSegments (numpy array)\n      Dendrite segments with enough active connected synapses to cause a\n      dendritic spike\n\n    - matchingSegments (numpy array)\n      Dendrite segments with enough active potential synapses to be selected for\n      learning in a bursting column\n\n    - potentialOverlaps (numpy array)\n      The number of active potential synapses for each segment.\n      Includes counts for active, matching, and nonmatching segments.\n    \"\"\"\n\n    # Active\n    overlaps = connections.computeActivity(activeInput, connectedPermanence)\n    activeSegments = np.flatnonzero(overlaps >= activationThreshold)\n\n    # Matching\n    potentialOverlaps = connections.computeActivity(activeInput)\n    matchingSegments = np.flatnonzero(potentialOverlaps >= minThreshold)\n\n    return (activeSegments,\n            matchingSegments,\n            potentialOverlaps)", "response": "Calculate the active and matching apical segments for this timestep."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the active and matching basal segments for this timestep.", "response": "def _calculateBasalSegmentActivity(connections, activeInput,\n                                reducedBasalThresholdCells, connectedPermanence,\n                                activationThreshold, minThreshold, reducedBasalThreshold):\n    \"\"\"\n    Calculate the active and matching basal segments for this timestep.\n\n    The difference with _calculateApicalSegmentActivity is that cells\n    with active apical segments (collected in reducedBasalThresholdCells) have\n    a lower activation threshold for their basal segments (set by\n    reducedBasalThreshold parameter).\n\n    @param connections (SparseMatrixConnections)\n    @param activeInput (numpy array)\n\n    @return (tuple)\n    - activeSegments (numpy array)\n      Dendrite segments with enough active connected synapses to cause a\n      dendritic spike\n\n    - matchingSegments (numpy array)\n      Dendrite segments with enough active potential synapses to be selected for\n      learning in a bursting column\n\n    - potentialOverlaps (numpy array)\n      The number of active potential synapses for each segment.\n      Includes counts for active, matching, and nonmatching segments.\n    \"\"\"\n    # Active apical segments lower the activation threshold for basal (lateral) segments\n    overlaps = connections.computeActivity(activeInput, connectedPermanence)\n    outrightActiveSegments = np.flatnonzero(overlaps >= activationThreshold)\n    if reducedBasalThreshold != activationThreshold and len(reducedBasalThresholdCells) > 0:\n        potentiallyActiveSegments = np.flatnonzero((overlaps < activationThreshold)\n                                        & (overlaps >= reducedBasalThreshold))\n        cellsOfCASegments = connections.mapSegmentsToCells(potentiallyActiveSegments)\n        # apically active segments are condit. active segments from apically active cells\n        conditionallyActiveSegments = potentiallyActiveSegments[np.in1d(cellsOfCASegments,\n                                                reducedBasalThresholdCells)]\n        activeSegments = np.concatenate((outrightActiveSegments, conditionallyActiveSegments))\n    else:\n        activeSegments = outrightActiveSegments\n\n\n\n    # Matching\n    potentialOverlaps = connections.computeActivity(activeInput)\n    matchingSegments = np.flatnonzero(potentialOverlaps >= minThreshold)\n\n    return (activeSegments,\n            matchingSegments,\n            potentialOverlaps)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the predicted cells given the set of active segments.", "response": "def _calculatePredictedCells(self, activeBasalSegments, activeApicalSegments):\n    \"\"\"\n    Calculate the predicted cells, given the set of active segments.\n\n    An active basal segment is enough to predict a cell.\n    An active apical segment is *not* enough to predict a cell.\n\n    When a cell has both types of segments active, other cells in its minicolumn\n    must also have both types of segments to be considered predictive.\n\n    @param activeBasalSegments (numpy array)\n    @param activeApicalSegments (numpy array)\n\n    @return (numpy array)\n    \"\"\"\n\n    cellsForBasalSegments = self.basalConnections.mapSegmentsToCells(\n      activeBasalSegments)\n    cellsForApicalSegments = self.apicalConnections.mapSegmentsToCells(\n      activeApicalSegments)\n\n    fullyDepolarizedCells = np.intersect1d(cellsForBasalSegments,\n                                           cellsForApicalSegments)\n    partlyDepolarizedCells = np.setdiff1d(cellsForBasalSegments,\n                                          fullyDepolarizedCells)\n\n    inhibitedMask = np.in1d(partlyDepolarizedCells / self.cellsPerColumn,\n                            fullyDepolarizedCells / self.cellsPerColumn)\n    predictedCells = np.append(fullyDepolarizedCells,\n                               partlyDepolarizedCells[~inhibitedMask])\n\n    if self.useApicalTiebreak == False:\n        predictedCells = cellsForBasalSegments\n\n    return predictedCells"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _chooseBestSegmentPerCell(cls,\n                                connections,\n                                cells,\n                                allMatchingSegments,\n                                potentialOverlaps):\n    \"\"\"\n    For each specified cell, choose its matching segment with largest number\n    of active potential synapses. When there's a tie, the first segment wins.\n\n    @param connections (SparseMatrixConnections)\n    @param cells (numpy array)\n    @param allMatchingSegments (numpy array)\n    @param potentialOverlaps (numpy array)\n\n    @return (numpy array)\n    One segment per cell\n    \"\"\"\n\n    candidateSegments = connections.filterSegmentsByCell(allMatchingSegments,\n                                                         cells)\n\n    # Narrow it down to one pair per cell.\n    onePerCellFilter = np2.argmaxMulti(potentialOverlaps[candidateSegments],\n                                       connections.mapSegmentsToCells(\n                                         candidateSegments))\n    learningSegments = candidateSegments[onePerCellFilter]\n\n    return learningSegments", "response": "This function chooses the best segment per cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the internal state of the internal state of the internal state of the internal state of the internal state.", "response": "def reset(self):\n    \"\"\"\n    Clear all cell and segment activity.\n    \"\"\"\n    super(ApicalTiebreakSequenceMemory, self).reset()\n\n    self.prevApicalInput = np.empty(0, dtype=\"uint32\")\n    self.prevApicalGrowthCandidates = np.empty(0, dtype=\"uint32\")\n    self.prevPredictedCells = np.empty(0, dtype=\"uint32\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlearn all provided objects in the canonical format and optionally resets the network.", "response": "def learnObjects(self, objects, reset=True):\n    \"\"\"\n    Learns all provided objects, and optionally resets the network.\n\n    The provided objects must have the canonical learning format, which is the\n    following.\n    objects should be a dict objectName: sensationList, where each\n    sensationList is a list of sensations, and each sensation is a mapping\n    from cortical column to a tuple of two SDR's respectively corresponding\n    to the location in object space and the feature.\n\n    For example, the input can look as follows, if we are learning a simple\n    object with two sensations (with very few active bits for simplicity):\n\n    objects = {\n      \"simple\": [\n        {\n          0: (set([1, 5, 10]), set([6, 12, 52]),  # location, feature for CC0\n          1: (set([6, 2, 15]), set([64, 1, 5]),  # location, feature for CC1\n        },\n        {\n          0: (set([5, 46, 50]), set([8, 10, 11]),  # location, feature for CC0\n          1: (set([1, 6, 45]), set([12, 17, 23]),  # location, feature for CC1\n        },\n      ]\n    }\n\n    In many uses cases, this object can be created by implementations of\n    ObjectMachines (cf htm.research.object_machine_factory), through their\n    method providedObjectsToLearn.\n\n    Parameters:\n    ----------------------------\n    @param   objects (dict)\n             Objects to learn, in the canonical format specified above\n\n    @param   reset (bool)\n             If set to True (which is the default value), the network will\n             be reset after learning.\n\n    \"\"\"\n    self._setLearningMode()\n\n    for objectName, sensationList in objects.iteritems():\n\n      # ignore empty sensation lists\n      if len(sensationList) == 0:\n        continue\n\n      # keep track of numbers of iterations to run\n      iterations = 0\n\n      for sensations in sensationList:\n        # learn each pattern multiple times\n        for _ in xrange(self.numLearningPoints):\n\n          for col in xrange(self.numColumns):\n            location, feature = sensations[col]\n            self.sensorInputs[col].addDataToQueue(list(feature), 0, 0)\n            self.externalInputs[col].addDataToQueue(list(location), 0, 0)\n          iterations += 1\n\n      # actually learn the objects\n      if iterations > 0:\n        self.network.run(iterations)\n\n      # update L2 representations\n      self._saveL2Representation(objectName)\n\n      if reset:\n        # send reset signal\n        self._sendReset()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer on a list of sensations and return a new object.", "response": "def infer(self, sensationList, reset=True, objectName=None):\n    \"\"\"\n    Infer on given sensations.\n\n    The provided sensationList is a list of sensations, and each sensation is\n    a mapping from cortical column to a tuple of two SDR's respectively\n    corresponding to the location in object space and the feature.\n\n    For example, the input can look as follows, if we are inferring a simple\n    object with two sensations (with very few active bits for simplicity):\n\n    sensationList = [\n      {\n        0: (set([1, 5, 10]), set([6, 12, 52]),  # location, feature for CC0\n        1: (set([6, 2, 15]), set([64, 1, 5]),  # location, feature for CC1\n      },\n\n      {\n        0: (set([5, 46, 50]), set([8, 10, 11]),  # location, feature for CC0\n        1: (set([1, 6, 45]), set([12, 17, 23]),  # location, feature for CC1\n      },\n    ]\n\n    In many uses cases, this object can be created by implementations of\n    ObjectMachines (cf htm.research.object_machine_factory), through their\n    method providedObjectsToInfer.\n\n    If the object is known by the caller, an object name can be specified\n    as an optional argument, and must match the objects given while learning.\n\n    Parameters:\n    ----------------------------\n    @param   sensationList (list)\n             List of sensations, in the canonical format specified above\n\n    @param   reset (bool)\n             If set to True (which is the default value), the network will\n             be reset after learning.\n\n    @param   objectName (str)\n             Name of the objects (must match the names given during learning).\n\n    \"\"\"\n    self._unsetLearningMode()\n    statistics = collections.defaultdict(list)\n\n    for sensations in sensationList:\n\n      # feed all columns with sensations\n      for col in xrange(self.numColumns):\n        location, feature = sensations[col]\n        self.sensorInputs[col].addDataToQueue(list(feature), 0, 0)\n        self.externalInputs[col].addDataToQueue(list(location), 0, 0)\n      self.network.run(1)\n      self._updateInferenceStats(statistics, objectName)\n\n    if reset:\n      # send reset signal\n      self._sendReset()\n\n    # save statistics\n    statistics[\"numSteps\"] = len(sensationList)\n    statistics[\"object\"] = objectName if objectName is not None else \"Unknown\"\n\n    self.statistics.append(statistics)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _saveL2Representation(self, objectName):\n    self.objectL2Representations[objectName] = self.getL2Representations()\n\n    try:\n      objectIndex = self.objectNameToIndex[objectName]\n    except KeyError:\n      # Grow the matrices as needed.\n      if self.objectNamesAreIndices:\n        objectIndex = objectName\n        if objectIndex >= self.objectL2RepresentationsMatrices[0].nRows():\n          for matrix in self.objectL2RepresentationsMatrices:\n            matrix.resize(objectIndex + 1, matrix.nCols())\n      else:\n        objectIndex = self.objectL2RepresentationsMatrices[0].nRows()\n        for matrix in self.objectL2RepresentationsMatrices:\n          matrix.resize(matrix.nRows() + 1, matrix.nCols())\n\n      self.objectNameToIndex[objectName] = objectIndex\n\n    for colIdx, matrix in enumerate(self.objectL2RepresentationsMatrices):\n      activeCells = self.L2Columns[colIdx]._pooler.getActiveCells()\n      matrix.setRowFromSparse(objectIndex, activeCells,\n                              np.ones(len(activeCells), dtype=\"float32\"))", "response": "Save the current active L2 cells as the representation for objectName."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sendReset(self, sequenceId=0):\n    for col in xrange(self.numColumns):\n      self.sensorInputs[col].addResetToQueue(sequenceId)\n      self.externalInputs[col].addResetToQueue(sequenceId)\n    self.network.run(1)", "response": "Send a reset signal to all the columns in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the cortical statistics for the specified fields.", "response": "def plotInferenceStats(self,\n                         fields,\n                         plotDir=\"plots\",\n                         experimentID=0,\n                         onePlot=True):\n    \"\"\"\n    Plots and saves the desired inference statistics.\n\n    Parameters:\n    ----------------------------\n    @param   fields (list(str))\n             List of fields to include in the plots\n\n    @param   experimentID (int)\n             ID of the experiment (usually 0 if only one was conducted)\n\n    @param   onePlot (bool)\n             If true, all cortical columns will be merged in one plot.\n\n    \"\"\"\n    if not os.path.exists(plotDir):\n      os.makedirs(plotDir)\n\n    plt.figure()\n    stats = self.statistics[experimentID]\n    objectName = stats[\"object\"]\n\n    for i in xrange(self.numColumns):\n      if not onePlot:\n        plt.figure()\n\n      # plot request stats\n      for field in fields:\n        fieldKey = field + \" C\" + str(i)\n        plt.plot(stats[fieldKey], marker='+', label=fieldKey)\n\n      # format\n      plt.legend(loc=\"upper right\")\n      plt.xlabel(\"Sensation #\")\n      plt.xticks(range(stats[\"numSteps\"]))\n      plt.ylabel(\"Number of active bits\")\n      plt.ylim(plt.ylim()[0] - 5, plt.ylim()[1] + 5)\n      plt.title(\"Object inference for object {}\".format(objectName))\n\n      # save\n      if not onePlot:\n        relPath = \"{}_exp_{}_C{}.png\".format(self.name, experimentID, i)\n        path = os.path.join(plotDir, relPath)\n        plt.savefig(path)\n        plt.close()\n\n    if onePlot:\n      relPath = \"{}_exp_{}.png\".format(self.name, experimentID)\n      path = os.path.join(plotDir, relPath)\n      plt.savefig(path)\n      plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef averageConvergencePoint(self, prefix, minOverlap, maxOverlap,\n                              settlingTime=1, firstStat=0, lastStat=None):\n\n    \"\"\"\n    For each object, compute the convergence time - the first point when all\n    L2 columns have converged.\n\n    Return the average convergence time and accuracy across all objects.\n\n    Using inference statistics for a bunch of runs, locate all traces with the\n    given prefix. For each trace locate the iteration where it finally settles\n    on targetValue. Return the average settling iteration and accuracy across\n    all runs.\n\n    :param prefix: Use this prefix to filter relevant stats.\n    :param minOverlap: Min target overlap\n    :param maxOverlap: Max target overlap\n    :param settlingTime: Setting time between iteration. Default 1\n    :return: Average settling iteration and accuracy across all runs\n    \"\"\"\n    convergenceSum = 0.0\n    numCorrect = 0.0\n    inferenceLength = 1000000\n\n    # For each object\n    for stats in self.statistics[firstStat:lastStat]:\n\n      # For each L2 column locate convergence time\n      convergencePoint = 0.0\n      for key in stats.iterkeys():\n        if prefix in key:\n          inferenceLength = len(stats[key])\n          columnConvergence = L4L2Experiment._locateConvergencePoint(\n            stats[key], minOverlap, maxOverlap)\n\n          convergencePoint = max(convergencePoint, columnConvergence)\n\n      convergenceSum += ceil(float(convergencePoint) / settlingTime)\n\n      if ceil(float(convergencePoint) / settlingTime) <= inferenceLength:\n        numCorrect += 1\n\n    if len(self.statistics[firstStat:lastStat]) == 0:\n      return 10000.0, 0.0\n\n    return (convergenceSum / len(self.statistics[firstStat:lastStat]),\n            numCorrect / len(self.statistics[firstStat:lastStat]) )", "response": "This function calculates the average convergence time and accuracy across all objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef printProfile(self, reset=False):\n    print \"Profiling information for {}\".format(type(self).__name__)\n    totalTime = 0.000001\n    for region in self.network.regions.values():\n      timer = region.getComputeTimer()\n      totalTime += timer.getElapsed()\n\n    # Sort the region names\n    regionNames = list(self.network.regions.keys())\n    regionNames.sort()\n\n    count = 1\n    profileInfo = []\n    L2Time = 0.0\n    L4Time = 0.0\n    for regionName in regionNames:\n      region = self.network.regions[regionName]\n      timer = region.getComputeTimer()\n      count = max(timer.getStartCount(), count)\n      profileInfo.append([region.name,\n                          timer.getStartCount(),\n                          timer.getElapsed(),\n                          100.0 * timer.getElapsed() / totalTime,\n                          timer.getElapsed() / max(timer.getStartCount(), 1)])\n      if \"L2Column\" in regionName:\n        L2Time += timer.getElapsed()\n      elif \"L4Column\" in regionName:\n        L4Time += timer.getElapsed()\n\n    profileInfo.append(\n      [\"Total time\", \"\", totalTime, \"100.0\", totalTime / count])\n    print tabulate(profileInfo, headers=[\"Region\", \"Count\",\n                                         \"Elapsed\", \"Pct of total\",\n                                         \"Secs/iteration\"],\n                   tablefmt=\"grid\", floatfmt=\"6.3f\")\n    print\n    print \"Total time in L2 =\", L2Time\n    print \"Total time in L4 =\", L4Time\n\n    if reset:\n      self.resetProfile()", "response": "Prints the profiling information for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an instance of the underlying algorithm. For example layer = L2 and column = 1 could return the actual instance of ColumnPooler that is responsible for column 1.", "response": "def getAlgorithmInstance(self, layer=\"L2\", column=0):\n    \"\"\"\n    Returns an instance of the underlying algorithm. For example,\n    layer=L2 and column=1 could return the actual instance of ColumnPooler\n    that is responsible for column 1.\n    \"\"\"\n    assert ( (column>=0) and (column<self.numColumns)), (\"Column number not \"\n                          \"in valid range\")\n\n    if layer == \"L2\":\n      return self.L2Columns[column].getAlgorithmInstance()\n    elif layer == \"L4\":\n      return self.L4Columns[column].getAlgorithmInstance()\n    else:\n      raise Exception(\"Invalid layer. Must be 'L4' or 'L2'\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget every L2 s current overlap with each object representation that has been learned.", "response": "def getCurrentObjectOverlaps(self):\n    \"\"\"\n    Get every L2's current overlap with each L2 object representation that has\n    been learned.\n\n    :return: 2D numpy array.\n    Each row represents a cortical column. Each column represents an object.\n    Each value represents the cortical column's current L2 overlap with the\n    specified object.\n    \"\"\"\n    overlaps = np.zeros((self.numColumns,\n                         len(self.objectL2Representations)),\n                        dtype=\"uint32\")\n\n    for i, representations in enumerate(self.objectL2RepresentationsMatrices):\n      activeCells = self.L2Columns[i]._pooler.getActiveCells()\n      overlaps[i, :] = representations.rightVecSumAtNZSparse(activeCells)\n\n    return overlaps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getCurrentClassification(self, minOverlap=None, includeZeros=True):\n    results = {}\n    l2sdr = self.getL2Representations()\n    sdrSize = self.config[\"L2Params\"][\"sdrSize\"]\n    if minOverlap is None:\n      minOverlap = sdrSize / 2\n\n    for objectName, objectSdr in self.objectL2Representations.iteritems():\n      count = 0\n      score = 0.0\n      for col in xrange(self.numColumns):\n        # Ignore inactive column\n        if len(l2sdr[col]) == 0:\n          continue\n\n        count += 1\n        overlap = len(l2sdr[col] & objectSdr[col])\n        if overlap >= minOverlap:\n          score += 1\n\n      if count == 0:\n        if includeZeros:\n          results[objectName] = 0\n      else:\n        if includeZeros or score>0.0:\n          results[objectName] = score / count\n\n    return results", "response": "Returns the current classification for every object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if objectName is currently unambiguously classified by every L2 object.", "response": "def isObjectClassified(self, objectName, minOverlap=None, maxL2Size=None):\n    \"\"\"\n    Return True if objectName is currently unambiguously classified by every L2\n    column. Classification is correct and unambiguous if the current L2 overlap\n    with the true object is greater than minOverlap and if the size of the L2\n    representation is no more than maxL2Size\n\n    :param minOverlap: min overlap to consider the object as recognized.\n                       Defaults to half of the SDR size\n\n    :param maxL2Size: max size for the L2 representation\n                       Defaults to 1.5 * SDR size\n\n    :return: True/False\n    \"\"\"\n    L2Representation = self.getL2Representations()\n    objectRepresentation = self.objectL2Representations[objectName]\n    sdrSize = self.config[\"L2Params\"][\"sdrSize\"]\n    if minOverlap is None:\n      minOverlap = sdrSize / 2\n    if maxL2Size is None:\n      maxL2Size = 1.5*sdrSize\n\n    numCorrectClassifications = 0\n    for col in xrange(self.numColumns):\n      overlapWithObject = len(objectRepresentation[col] & L2Representation[col])\n\n      if ( overlapWithObject >= minOverlap  and\n           len(L2Representation[col]) <= maxL2Size ):\n        numCorrectClassifications += 1\n\n    return numCorrectClassifications == self.numColumns"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getDefaultL4Params(self, inputSize, numInputBits):\n    sampleSize = int(1.5 * numInputBits)\n\n    if numInputBits == 20:\n      activationThreshold = 13\n      minThreshold = 13\n    elif numInputBits == 10:\n      activationThreshold = 8\n      minThreshold = 8\n    else:\n      activationThreshold = int(numInputBits * .6)\n      minThreshold = activationThreshold\n\n    return {\n      \"columnCount\": inputSize,\n      \"cellsPerColumn\": 16,\n      \"learn\": True,\n      \"initialPermanence\": 0.51,\n      \"connectedPermanence\": 0.6,\n      \"permanenceIncrement\": 0.1,\n      \"permanenceDecrement\": 0.02,\n      \"minThreshold\": minThreshold,\n      \"basalPredictedSegmentDecrement\": 0.0,\n      \"apicalPredictedSegmentDecrement\": 0.0,\n      \"activationThreshold\": activationThreshold,\n      \"reducedBasalThreshold\": int(activationThreshold*0.6),\n      \"sampleSize\": sampleSize,\n      \"implementation\": \"ApicalTiebreak\",\n      \"seed\": self.seed\n    }", "response": "Returns a good default set of parameters to use in the L4 region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getDefaultL2Params(self, inputSize, numInputBits):\n    if numInputBits == 20:\n      sampleSizeProximal = 10\n      minThresholdProximal = 5\n    elif numInputBits == 10:\n      sampleSizeProximal = 6\n      minThresholdProximal = 3\n    else:\n      sampleSizeProximal = int(numInputBits * .6)\n      minThresholdProximal = int(sampleSizeProximal * .6)\n\n    return {\n      \"inputWidth\": inputSize * 16,\n      \"cellCount\": 4096,\n      \"sdrSize\": 40,\n      \"synPermProximalInc\": 0.1,\n      \"synPermProximalDec\": 0.001,\n      \"initialProximalPermanence\": 0.6,\n      \"minThresholdProximal\": minThresholdProximal,\n      \"sampleSizeProximal\": sampleSizeProximal,\n      \"connectedPermanenceProximal\": 0.5,\n      \"synPermDistalInc\": 0.1,\n      \"synPermDistalDec\": 0.001,\n      \"initialDistalPermanence\": 0.41,\n      \"activationThresholdDistal\": 13,\n      \"sampleSizeDistal\": 20,\n      \"connectedPermanenceDistal\": 0.5,\n      \"seed\": self.seed,\n      \"learningMode\": True,\n    }", "response": "Returns a good default set of parameters to use in the L2 region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _locateConvergencePoint(stats, minOverlap, maxOverlap):\n    for i, v in enumerate(stats[::-1]):\n      if not (v >= minOverlap and v <= maxOverlap):\n        return len(stats) - i + 1\n\n    # Never differs - converged in one iteration\n    return 1", "response": "Find the convergence point in the stats array that diverges\n    from target values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the learning mode for the L4 and L2 regions.", "response": "def _setLearningMode(self):\n    \"\"\"\n    Sets the learning mode.\n    \"\"\"\n    for region in self.L4Regions:\n      region.setParameter(\"learn\", True)\n    for region in self.L2Regions:\n      region.setParameter(\"learningMode\", True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlearns a set of objects. Then try to recognize each object. Output an interactive visualization.", "response": "def doExperiment(numObjects,\n                 featuresPerObject,\n                 objectWidth,\n                 numFeatures,\n                 featureDistribution,\n                 useTrace,\n                 noiseFactor,\n                 moduleNoiseFactor,\n                 numModules,\n                 thresholds,\n                 inverseReadoutResolution,\n                 enlargeModuleFactor,\n                 bumpOverlapMethod,\n                 seed1,\n                 seed2):\n  \"\"\"\n  Learn a set of objects. Then try to recognize each object. Output an\n  interactive visualization.\n  \"\"\"\n  if not os.path.exists(\"traces\"):\n    os.makedirs(\"traces\")\n\n  if seed1 != -1:\n    np.random.seed(seed1)\n\n  if seed2 != -1:\n    random.seed(seed2)\n\n  features = [str(i) for i in xrange(numFeatures)]\n  objects = generateObjects(numObjects, featuresPerObject, objectWidth,\n                            numFeatures, featureDistribution)\n\n  locationConfigs = []\n  scale = 40.0\n\n  if thresholds == -1:\n    thresholds = int(math.ceil(numModules*0.8))\n  elif thresholds == 0:\n    thresholds = numModules\n  perModRange = float(60.0 / float(numModules))\n  for i in xrange(numModules):\n    orientation = (float(i) * perModRange) + (perModRange / 2.0)\n\n    locationConfigs.append({\n        \"scale\": scale,\n        \"orientation\": np.radians(orientation),\n        \"activationThreshold\": 8,\n        \"initialPermanence\": 1.0,\n        \"connectedPermanence\": 0.5,\n        \"learningThreshold\": 8,\n        \"sampleSize\": 10,\n        \"permanenceIncrement\": 0.1,\n        \"permanenceDecrement\": 0.0,\n        \"inverseReadoutResolution\": inverseReadoutResolution,\n        \"enlargeModuleFactor\": enlargeModuleFactor,\n        \"bumpOverlapMethod\": bumpOverlapMethod,\n    })\n\n  l4Overrides = {\n    \"initialPermanence\": 1.0,\n    \"activationThreshold\": thresholds,\n    \"reducedBasalThreshold\": thresholds,\n    \"minThreshold\": numModules,\n    \"sampleSize\": numModules,\n    \"cellsPerColumn\": 16,\n  }\n\n  column = PIUNCorticalColumn(locationConfigs, L4Overrides=l4Overrides,\n                              bumpType=\"gaussian2\")\n  exp = PIUNExperiment(column, featureNames=features,\n                       numActiveMinicolumns=10,\n                       noiseFactor=noiseFactor,\n                       moduleNoiseFactor=moduleNoiseFactor)\n\n  for objectDescription in objects:\n    exp.learnObject(objectDescription)\n\n  convergence = collections.defaultdict(int)\n  try:\n    if useTrace:\n      filename = os.path.join(\n        SCRIPT_DIR,\n        \"traces/{}-resolution-{}-modules-{}-objects-{}-feats.html\".format(\n          inverseReadoutResolution, numModules, numObjects, numFeatures)\n      )\n      traceFileOut = io.open(filename, \"w\", encoding=\"utf8\")\n      traceHandle = trace(traceFileOut, exp, includeSynapses=True)\n      print \"Logging to\", filename\n\n    for objectDescription in objects:\n      steps = exp.inferObjectWithRandomMovements(objectDescription)\n      convergence[steps] += 1\n      if steps is None:\n        print 'Failed to infer object \"{}\"'.format(objectDescription[\"name\"])\n  finally:\n    if useTrace:\n      traceHandle.__exit__()\n      traceFileOut.close()\n\n  for step, num in sorted(convergence.iteritems()):\n    print \"{}: {}\".format(step, num)\n\n  result = {\n    \"convergence\": convergence,\n  }\n\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns multiple experiments in parallel and save results to json at resultName.", "response": "def runMultiprocessNoiseExperiment(resultName, repeat, numWorkers,\n                                   appendResults, **kwargs):\n  \"\"\"\n  :param kwargs: Pass lists to distribute as lists, lists that should be passed intact as tuples.\n  :return: results, in the format [(arguments, results)].  Also saved to json at resultName, in the same format.\n  \"\"\"\n  experiments = [{}]\n  for key, values in kwargs.items():\n    if type(values) is list:\n      newExperiments = []\n      for experiment in experiments:\n        for val in values:\n          newExperiment = copy(experiment)\n          newExperiment[key] = val\n          newExperiments.append(newExperiment)\n      experiments = newExperiments\n    else:\n      [a.__setitem__(key, values) for a in experiments]\n\n  newExperiments = []\n  for experiment in experiments:\n    for _ in xrange(repeat):\n      newExperiments.append(copy(experiment))\n  experiments = newExperiments\n\n  if numWorkers > 1:\n    pool = Pool(processes=numWorkers)\n    rs = pool.map_async(experimentWrapper, experiments, chunksize=1)\n    while not rs.ready():\n      remaining = rs._number_left\n      pctDone = 100.0 - (100.0*remaining) / len(experiments)\n      print \"    =>\", remaining, \"experiments remaining, percent complete=\",pctDone\n      time.sleep(5)\n    pool.close()  # No more work\n    pool.join()\n    result = rs.get()\n  else:\n    result = []\n    for arg in experiments:\n      result.append(doExperiment(**arg))\n\n  # Save results for later use\n  results = [(arg,res) for arg, res in zip(experiments, result)]\n\n  if appendResults:\n    try:\n      with open(os.path.join(SCRIPT_DIR, resultName), \"r\") as f:\n        existingResults = json.load(f)\n        results = existingResults + results\n    except IOError:\n      pass\n\n  with open(os.path.join(SCRIPT_DIR, resultName),\"wb\") as f:\n    print \"Writing results to {}\".format(resultName)\n    json.dump(results,f)\n\n  return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a list of numFeatures features.", "response": "def generateFeatures(numFeatures):\n  \"\"\"Return string features.\n\n  If <=62 features are requested, output will be single character\n  alphanumeric strings. Otherwise, output will be [\"F1\", \"F2\", ...]\n  \"\"\"\n  # Capital letters, lowercase letters, numbers\n  candidates = ([chr(i+65) for i in xrange(26)] +\n                [chr(i+97) for i in xrange(26)] +\n                [chr(i+48) for i in xrange(10)])\n\n  if numFeatures > len(candidates):\n    candidates = [\"F{}\".format(i) for i in xrange(numFeatures)]\n    return candidates\n\n  return candidates[:numFeatures]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlearns a set of objects. Then try to recognize each object. Output an interactive visualization. @param cellDimensions (pair) The cell dimensions of each module @param cellCoordinateOffsets (sequence) The \"cellCoordinateOffsets\" parameter for each module", "response": "def doExperiment(cellDimensions,\n                 cellCoordinateOffsets,\n                 numObjects,\n                 featuresPerObject,\n                 objectWidth,\n                 numFeatures,\n                 useTrace,\n                 noiseFactor,\n                 moduleNoiseFactor,\n                 anchoringMethod=\"narrowing\",\n                 randomLocation=False,\n                 threshold=16):\n  \"\"\"\n  Learn a set of objects. Then try to recognize each object. Output an\n  interactive visualization.\n\n  @param cellDimensions (pair)\n  The cell dimensions of each module\n\n  @param cellCoordinateOffsets (sequence)\n  The \"cellCoordinateOffsets\" parameter for each module\n  \"\"\"\n  if not os.path.exists(\"traces\"):\n    os.makedirs(\"traces\")\n\n  features = generateFeatures(numFeatures)\n  objects = generateObjects(numObjects, featuresPerObject, objectWidth,\n                            features)\n\n  locationConfigs = []\n  scale = 5*cellDimensions[0] # One cell is about a quarter of a feature\n\n  numModules = 20\n  perModRange = float(90.0 / float(numModules))\n\n  if anchoringMethod == \"corners\":\n    cellCoordinateOffsets = (.0001, .5, .9999)\n\n  if anchoringMethod == \"discrete\":\n    cellCoordinateOffsets = (.5,)\n\n  for i in xrange(numModules):\n    orientation = float(i) * perModRange\n\n    locationConfigs.append({\n      \"cellDimensions\": cellDimensions,\n      \"moduleMapDimensions\": (scale, scale),\n      \"orientation\": np.radians(orientation),\n      \"cellCoordinateOffsets\": cellCoordinateOffsets,\n      \"activationThreshold\": 8,\n      \"initialPermanence\": 1.0,\n      \"connectedPermanence\": 0.5,\n      \"learningThreshold\": 8,\n      \"sampleSize\": 20,\n      \"permanenceIncrement\": 0.1,\n      \"permanenceDecrement\": 0.0,\n      \"anchoringMethod\": anchoringMethod,\n    })\n  l4Overrides = {\n    \"initialPermanence\": 1.0,\n    \"activationThreshold\": threshold,\n    \"reducedBasalThreshold\": threshold,\n    \"minThreshold\": threshold,\n    \"sampleSize\": numModules,\n    \"cellsPerColumn\": 16,\n  }\n\n  column = PIUNCorticalColumn(locationConfigs, L4Overrides=l4Overrides)\n  exp = PIUNExperiment(column, featureNames=features,\n                       numActiveMinicolumns=10,\n                       noiseFactor=noiseFactor,\n                       moduleNoiseFactor=moduleNoiseFactor)\n\n  for objectDescription in objects:\n    exp.learnObject(objectDescription, randomLocation=randomLocation, useNoise = False)\n    print 'Learned object {}'.format(objectDescription[\"name\"])\n\n  filename = \"traces/{}-points-{}-cells-{}-objects-{}-feats-{}-random.html\".format(\n    len(cellCoordinateOffsets)**2, np.prod(cellDimensions), numObjects, numFeatures, randomLocation)\n\n  convergence = collections.defaultdict(int)\n  if useTrace:\n    with io.open(filename, \"w\", encoding=\"utf8\") as fileOut:\n      with trace(fileOut, exp, includeSynapses=False):\n        print \"Logging to\", filename\n        for objectDescription in objects:\n          steps = exp.inferObjectWithRandomMovements(objectDescription, randomLocation=randomLocation)\n          convergence[steps] += 1\n          if steps is None:\n            print 'Failed to infer object \"{}\"'.format(objectDescription[\"name\"])\n          else:\n            print 'Inferred object {} after {} steps'.format(objectDescription[\"name\"], steps)\n  else:\n    for objectDescription in objects:\n      steps = exp.inferObjectWithRandomMovements(objectDescription, randomLocation=randomLocation)\n      convergence[steps] += 1\n      if steps is None:\n        print 'Failed to infer object \"{}\"'.format(objectDescription[\"name\"])\n      else:\n        print 'Inferred object {} after {} steps'.format(objectDescription[\"name\"], steps)\n\n  for step, num in sorted(convergence.iteritems()):\n    print \"{}: {}\".format(step, num)\n\n  return(convergence)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runMultiprocessNoiseExperiment(resultName=None, numWorkers = 0, **kwargs):\n\n  if resultName is None:\n    resultName = str(kwargs) + \".json\"\n\n  experiments = [{}]\n  for key, values in kwargs.items():\n    if type(values) is list:\n      newExperiments = []\n      for experiment in experiments:\n        for val in values:\n          newExperiment = copy(experiment)\n          newExperiment[key] = val\n          newExperiments.append(newExperiment)\n      experiments = newExperiments\n    else:\n      [a.__setitem__(key, values) for a in experiments]\n\n  if numWorkers == 0:\n    numWorkers = cpu_count()\n  if numWorkers > 1:\n    pool = Pool(processes=numWorkers)\n    rs = pool.map_async(experimentWrapper, experiments, chunksize=1)\n    while not rs.ready():\n      remaining = rs._number_left\n      pctDone = 100.0 - (100.0*remaining) / len(experiments)\n      print \"    => {} experiments remaining, percent complete={}\".format(\\\n        remaining, pctDone)\n      time.sleep(5)\n    pool.close()  # No more work\n    pool.join()\n    result = rs.get()\n  else:\n    result = []\n    for arg in experiments:\n      result.append(doExperiment(**arg))\n\n  # Save results for later use\n  results = [(arg,res) for arg, res in zip(experiments, result)]\n  with open(resultName,\"wb\") as f:\n    json.dump(results,f)\n\n  return results", "response": "Run multiple experiments in parallel and save results to json at resultName."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addMonitor(self, monitor):\n\n    token = self.nextMonitorToken\n    self.nextMonitorToken += 1\n\n    self.monitors[token] = monitor\n\n    return token", "response": "Subscribe to SingleLayer2DExperiment events.\n    Returns the next monitor token."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef learnObjects(self, objectPlacements):\n    for monitor in self.monitors.values():\n      monitor.afterPlaceObjects(objectPlacements)\n\n    for objectName, objectDict in self.objects.iteritems():\n      self.reset()\n\n      objectPlacement = objectPlacements[objectName]\n\n      for locationName, featureName in objectDict.iteritems():\n        egocentricLocation = (locationName[0] + objectPlacement[0],\n                              locationName[1] + objectPlacement[1])\n\n        locationSDR = self.locations[egocentricLocation]\n        featureSDR = self.features[featureName]\n        transitionSDR = np.empty(0)\n\n        self.locationLayer.reset()\n        self.inputLayer.reset()\n\n        for _ in xrange(10):\n          self.doTimestep(locationSDR, transitionSDR, featureSDR,\n                          egocentricLocation, learn=True)\n\n        self.inputRepresentations[(featureName, egocentricLocation)] = (\n          self.inputLayer.getActiveCells())\n\n      self.objectRepresentations[objectName] = self.objectLayer.getActiveCells()\n      self.learnedObjectPlacements[objectName] = objectPlacement", "response": "Learn each provided object in egocentric space."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _selectTransition(self, allocentricLocation, objectDict, visitCounts):\n\n    candidates = list(transition\n                      for transition in self.transitions.keys()\n                      if (allocentricLocation[0] + transition[0],\n                          allocentricLocation[1] + transition[1]) in objectDict)\n    random.shuffle(candidates)\n\n    selectedVisitCount = None\n    selectedTransition = None\n    selectedAllocentricLocation = None\n\n    for transition in candidates:\n      candidateLocation = (allocentricLocation[0] + transition[0],\n                           allocentricLocation[1] + transition[1])\n\n      if (selectedVisitCount is None or\n          visitCounts[candidateLocation] < selectedVisitCount):\n        selectedVisitCount = visitCounts[candidateLocation]\n        selectedTransition = transition\n        selectedAllocentricLocation = candidateLocation\n\n    return selectedAllocentricLocation, selectedTransition", "response": "Select the transition that lands us in the location we ve touched the least\n    often."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninfers each object with any location input.", "response": "def inferObjectsWithRandomMovements(self, objectPlacements, maxTouches=20,\n                                      settlingTime=2):\n    \"\"\"\n    Infer each object without any location input.\n    \"\"\"\n\n    for monitor in self.monitors.values():\n      monitor.afterPlaceObjects(objectPlacements)\n\n    for objectName, objectDict in self.objects.iteritems():\n      self.reset()\n\n      visitCounts = defaultdict(int)\n\n      learnedObjectPlacement = self.learnedObjectPlacements[objectName]\n\n      allocentricLocation = random.choice(objectDict.keys())\n      nextTransitionSDR = np.empty(0, dtype=\"uint32\")\n\n      # Traverse the object until it is inferred.\n      success = False\n\n      for _ in xrange(maxTouches):\n        featureName = objectDict[allocentricLocation]\n        egocentricLocation = (allocentricLocation[0] +\n                              objectPlacements[objectName][0],\n                              allocentricLocation[1] +\n                              objectPlacements[objectName][1])\n        featureSDR = self.features[featureName]\n\n        steps = ([nextTransitionSDR] +\n                 [np.empty(0)]*settlingTime)\n        for transitionSDR in steps:\n          self.doTimestep(np.empty(0), transitionSDR, featureSDR,\n                          egocentricLocation, learn=False)\n\n        visitCounts[allocentricLocation] += 1\n\n        # We should eventually infer the egocentric location where we originally\n        # learned this location on the object.\n        learnedEgocentricLocation = (\n          allocentricLocation[0] + learnedObjectPlacement[0],\n          allocentricLocation[1] + learnedObjectPlacement[1])\n\n        if (set(self.objectLayer.getActiveCells()) ==\n            set(self.objectRepresentations[objectName]) and\n\n            set(self.inputLayer.getActiveCells()) ==\n            set(self.inputRepresentations[(featureName,\n                                           learnedEgocentricLocation)]) and\n\n            set(self.locationLayer.getActiveCells()) ==\n            set(self.locations[learnedEgocentricLocation])):\n          success = True\n          break\n        else:\n          allocentricLocation, transitionName = self._selectTransition(\n            allocentricLocation, objectDict, visitCounts)\n          nextTransitionSDR = self.transitions[transitionName]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initializeConnections(self):\n\n    numColumns = self._numColumns\n    numInputs = self._numInputs\n\n    # The SP should be setup so that stimulusThreshold is reasonably high,\n    # similar to a TP's activation threshold. The pct of connected cells may\n    # need to be low to avoid false positives given the monosynaptic rule\n    initConnectedPct = self._initConnectedPct\n\n    # Store the set of all inputs that are within each column's potential pool.\n    # 'potentialPools' is a matrix, whose rows represent cortical columns, and\n    # whose columns represent the input bits. if potentialPools[i][j] == 1,\n    # then input bit 'j' is in column 'i's potential pool. A column can only be\n    # connected to inputs in its potential pool. The indices refer to a\n    # flattened version of both the inputs and columns. Namely, irrespective\n    # of the topology of the inputs and columns, they are treated as being a\n    # one dimensional array. Since a column is typically connected to only a\n    # subset of the inputs, many of the entries in the matrix are 0. Therefore\n    # the the potentialPool matrix is stored using the SparseBinaryMatrix\n    # class, to reduce memory footprint and compuation time of algorithms that\n    # require iterating over the data strcuture.\n    self._potentialPools = SparseBinaryMatrix(numInputs)\n    self._potentialPools.resize(numColumns, numInputs)\n\n    # Initialize the permanences for each column. Similar to the\n    # 'self._potentialPools', the permanences are stored in a matrix whose rows\n    # represent the cortical columns, and whose columns represent the input\n    # bits. if self._permanences[i][j] = 0.2, then the synapse connecting\n    # cortical column 'i' to input bit 'j'  has a permanence of 0.2. Here we\n    # also use the SparseMatrix class to reduce the memory footprint and\n    # computation time of algorithms that require iterating over the data\n    # structure. This permanence matrix is only allowed to have non-zero\n    # elements where the potential pool is non-zero.\n    self._permanences = SparseMatrix(numColumns, numInputs)\n\n    # 'self._connectedSynapses' is a similar matrix to 'self._permanences'\n    # (rows represent cortical columns, columns represent input bits) whose\n    # entries represent whether the cortial column is connected to the input\n    # bit, i.e. its permanence value is greater than 'synPermConnected'. While\n    # this information is readily available from the 'self._permanence' matrix,\n    # it is stored separately for efficiency purposes.\n    self._connectedSynapses = SparseBinaryMatrix(numInputs)\n    self._connectedSynapses.resize(numColumns, numInputs)\n\n    # Stores the number of connected synapses for each column. This is simply\n    # a sum of each row of 'self._connectedSynapses'. again, while this\n    # information is readily available from 'self._connectedSynapses', it is\n    # stored separately for efficiency purposes.\n    self._connectedCounts = numpy.zeros(numColumns, dtype=realDType)\n\n\n    # Initialize the set of permanence values for each column. Ensure that\n    # each column is connected to enough input bits to allow it to be\n    # activated.\n    for i in xrange(numColumns):\n      potential = self._mapPotential(i, wrapAround=self._wrapAround)\n      self._potentialPools.replaceSparseRow(i, potential.nonzero()[0])\n      perm = self._initPermanence(potential, initConnectedPct)\n      self._updatePermanencesForColumn(perm, i, raisePerm=True)", "response": "Initializes the connection matrix for each cell in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n    self._poolingActivation = numpy.zeros((self._numColumns), dtype=\"int32\")\n    self._poolingColumns = []\n    self._overlapDutyCycles = numpy.zeros(self._numColumns, dtype=realDType)\n    self._activeDutyCycles = numpy.zeros(self._numColumns, dtype=realDType)\n    self._minOverlapDutyCycles = numpy.zeros(self._numColumns,\n                                             dtype=realDType)\n    self._minActiveDutyCycles = numpy.zeros(self._numColumns,\n                                            dtype=realDType)\n    self._boostFactors = numpy.ones(self._numColumns, dtype=realDType)", "response": "Reset the state of the temporal pooler\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculatePoolingActivity(self, predictedActiveCells, learn):\n\n\n    overlaps = numpy.zeros(self._numColumns).astype(realDType)\n\n    # If no pooling columns or no predicted active inputs, return all zeros\n    if (sum(self._poolingActivation) == 0 or\n       len(predictedActiveCells.nonzero()[0]) == 0):\n      return overlaps\n\n    if learn:\n      # During learning, overlap is calculated based on potential synapses.\n      self._potentialPools.rightVecSumAtNZ_fast(predictedActiveCells, overlaps)\n    else:\n      # At inference stage, overlap is calculated based on connected synapses.\n      self._connectedSynapses.rightVecSumAtNZ_fast(predictedActiveCells, overlaps)\n\n    poolingColumns = self._poolingColumns\n\n    # Only consider columns that are in pooling state\n    mask = numpy.zeros(self._numColumns).astype(realDType)\n    mask[poolingColumns] = 1\n    overlaps = overlaps * mask\n    # Pooling TP cells that receive predicted input\n    # will have their overlap boosted by a large factor so that they are likely\n    # to win the inhibition competition\n    boostFactorPooling = self._boostStrength * self._numInputs\n    overlaps = boostFactorPooling * overlaps\n\n    if self._spVerbosity > 3:\n      print \"\\n============== In _calculatePoolingActivity ======\"\n      print \"Received predicted cell inputs from following indices:\"\n      print \"   \", predictedActiveCells.nonzero()[0]\n      print \"The following column indices are in pooling state:\"\n      print \"   \", poolingColumns\n      print \"Overlap score of pooling columns:\"\n      print \"   \", overlaps[poolingColumns]\n      print \"============== Leaving _calculatePoolingActivity  ======\\n\"\n\n    return overlaps", "response": "Calculates the number of active cells that are in pooling state and the number of predicted active cells that are connected to the predicted active cells."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _adaptSynapses(self, inputVector, activeColumns, predictedActiveCells):\n    inputIndices = numpy.where(inputVector > 0)[0]\n    predictedIndices = numpy.where(predictedActiveCells > 0)[0]\n    permChanges = numpy.zeros(self._numInputs)\n\n    # Decrement inactive TM cell -> active TP cell connections\n    permChanges.fill(-1 * self._synPermInactiveDec)\n\n    # Increment active TM cell -> active TP cell connections\n    permChanges[inputIndices] = self._synPermActiveInc\n\n    # Increment correctly predicted TM cell -> active TP cell connections\n    permChanges[predictedIndices] = self._synPredictedInc\n\n    if self._spVerbosity > 4:\n      print \"\\n============== _adaptSynapses ======\"\n      print \"Active input indices:\",inputIndices\n      print \"predicted input indices:\",predictedIndices\n      print \"\\n============== _adaptSynapses ======\\n\"\n\n    for i in activeColumns:\n      # Get the permanences of the synapses of TP cell i\n      perm = self._permanences.getRow(i)\n\n      # Only consider connections in column's potential pool (receptive field)\n      maskPotential = numpy.where(self._potentialPools.getRow(i) > 0)[0]\n      perm[maskPotential] += permChanges[maskPotential]\n      self._updatePermanencesForColumn(perm, i, raisePerm=False)", "response": "This method is used to adapt synapses for a single active cell."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef printParameters(self):\n    print \"------------PY  TemporalPooler Parameters ------------------\"\n    print \"numInputs                  = \", self.getNumInputs()\n    print \"numColumns                 = \", self.getNumColumns()\n    print \"columnDimensions           = \", self._columnDimensions\n    print \"numActiveColumnsPerInhArea = \", self.getNumActiveColumnsPerInhArea()\n    print \"potentialPct               = \", self.getPotentialPct()\n    print \"globalInhibition           = \", self.getGlobalInhibition()\n    print \"localAreaDensity           = \", self.getLocalAreaDensity()\n    print \"stimulusThreshold          = \", self.getStimulusThreshold()\n    print \"synPermActiveInc           = \", self.getSynPermActiveInc()\n    print \"synPermInactiveDec         = \", self.getSynPermInactiveDec()\n    print \"synPermConnected           = \", self.getSynPermConnected()\n    print \"minPctOverlapDutyCycle     = \", self.getMinPctOverlapDutyCycles()\n    print \"dutyCyclePeriod            = \", self.getDutyCyclePeriod()\n    print \"boostStrength              = \", self.getBoostStrength()\n    print \"spVerbosity                = \", self.getSpVerbosity()\n    print \"version                    = \", self._version", "response": "Prints the current parameters of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extractInputForTP(self, tm):\n\n    # bursting cells in layer 4\n    burstingColumns = tm.activeState[\"t\"].sum(axis=1)\n    burstingColumns[ burstingColumns < tm.cellsPerColumn ] = 0\n    burstingColumns[ burstingColumns == tm.cellsPerColumn ] = 1\n    # print \"Bursting column indices=\",burstingColumns.nonzero()[0]\n\n    # correctly predicted cells in layer 4\n    correctlyPredictedCells = numpy.zeros(self._inputDimensions).astype(realDType)\n    idx = (tm.predictedState[\"t-1\"] + tm.activeState[\"t\"]) == 2\n    idx = idx.reshape(self._inputDimensions)\n    correctlyPredictedCells[idx] = 1.0\n    # print \"Predicted->active cell indices=\",correctlyPredictedCells.nonzero()[0]\n\n    # all currently active cells in layer 4\n    spInputVector = tm.learnState[\"t\"].reshape(self._inputDimensions)\n    # spInputVector = tm.activeState[\"t\"].reshape(self._inputDimensions)\n\n    return (correctlyPredictedCells, spInputVector, burstingColumns)", "response": "Extract inputs for TP from the state of temporal memory tm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef train(self, inputData, numIterations, reset=False):\n    if not isinstance(inputData, np.ndarray):\n      inputData = np.array(inputData)\n\n    if reset:\n      self._reset()\n\n    for _ in xrange(numIterations):\n      self._iteration += 1\n\n      batch = self._getDataBatch(inputData)\n\n      # check input dimension, change if necessary\n      if batch.shape[0] != self.filterDim:\n        raise ValueError(\"Batches and filter dimesions don't match!\")\n\n      activations = self.encode(batch)\n      self._learn(batch, activations)\n\n      if self._iteration % self.decayCycle == 0:\n        self.learningRate *= self.learningRateDecay\n\n    if self.verbosity >= 1:\n      self.plotLoss()\n      self.plotBasis()", "response": "Train the SparseNet with the provided data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode(self, data, flatten=False):\n    if not isinstance(data, np.ndarray):\n      data = np.array(data)\n\n    # flatten if necessary\n    if flatten:\n      try:\n        data = np.reshape(data, (self.filterDim, data.shape[-1]))\n      except ValueError:\n        # only one data point\n        data = np.reshape(data, (self.filterDim, 1))\n\n    if data.shape[0] != self.filterDim:\n      raise ValueError(\"Data does not have the correct dimension!\")\n\n    # if single data point, convert to 2-dimensional array for consistency\n    if len(data.shape) == 1:\n      data = data[:, np.newaxis]\n\n\n    projection = self.basis.T.dot(data)\n    representation = self.basis.T.dot(self.basis) - np.eye(self.outputDim)\n    states = np.zeros((self.outputDim, data.shape[1]))\n\n    threshold = 0.5 * np.max(np.abs(projection), axis=0)\n    activations = self._thresholdNonLinearity(states, threshold)\n\n    for _ in xrange(self.numLcaIterations):\n      # update dynamic system\n      states *= (1 - self.lcaLearningRate)\n      states += self.lcaLearningRate * (projection - representation.dot(activations))\n      activations = self._thresholdNonLinearity(states, threshold)\n\n      # decay threshold\n      threshold *= self.thresholdDecay\n      threshold[threshold < self.minThreshold] = self.minThreshold\n\n    return activations", "response": "Encodes the input data into a sparse vector of activations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plotLoss(self, filename=None):\n    plt.figure()\n    plt.plot(self.losses.keys(), self.losses.values())\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Learning curve for {}\".format(self))\n\n    if filename is not None:\n      plt.savefig(filename)", "response": "Plots the loss history."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plotBasis(self, filename=None):\n    if np.floor(np.sqrt(self.filterDim)) ** 2 != self.filterDim:\n      print \"Basis visualization is not available if filterDim is not a square.\"\n      return\n\n    dim = int(np.sqrt(self.filterDim))\n\n    if np.floor(np.sqrt(self.outputDim)) ** 2 != self.outputDim:\n      outDimJ = np.sqrt(np.floor(self.outputDim / 2))\n      outDimI = np.floor(self.outputDim / outDimJ)\n      if outDimI > outDimJ:\n        outDimI, outDimJ = outDimJ, outDimI\n    else:\n      outDimI = np.floor(np.sqrt(self.outputDim))\n      outDimJ = outDimI\n\n    outDimI, outDimJ = int(outDimI), int(outDimJ)\n    basis = - np.ones((1 + outDimI * (dim + 1), 1 + outDimJ * (dim + 1)))\n\n    # populate array with basis values\n    k = 0\n    for i in xrange(outDimI):\n      for j in xrange(outDimJ):\n        colorLimit = np.max(np.abs(self.basis[:, k]))\n        mat = np.reshape(self.basis[:, k], (dim, dim)) / colorLimit\n        basis[1 + i * (dim + 1) : 1 + i * (dim + 1) + dim, \\\n              1 + j * (dim + 1) : 1 + j * (dim + 1) + dim] = mat\n        k += 1\n\n    plt.figure()\n    plt.subplot(aspect=\"equal\")\n    plt.pcolormesh(basis)\n\n    plt.axis([0, 1 + outDimJ * (dim + 1), 0, 1 + outDimI * (dim + 1)])\n    # remove ticks\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.title(\"Basis functions for {0}\".format(self))\n\n    if filename is not None:\n      plt.savefig(filename)", "response": "Plots the basis functions of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reset(self):\n    self.basis = np.random.randn(self.filterDim, self.outputDim)\n    self.basis /= np.sqrt(np.sum(self.basis ** 2, axis=0))\n    self._iteration = 0\n    self.losses = {}", "response": "Reinitializes basis functions iteration number and loss history."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _learn(self, batch, activations):\n    batchResiduals = batch - self.basis.dot(activations)\n    loss = np.mean(np.sqrt(np.sum(batchResiduals ** 2, axis=0)))\n    self.losses[self._iteration] = loss\n\n    if self.verbosity >= 2:\n      if self._iteration % self.showEvery == 0:\n        print \"At iteration {0}, loss is {1:.3f}\".format(self._iteration, loss)\n\n    # update basis\n    gradBasis = batchResiduals.dot(activations.T) / self.batchSize\n    self.basis += self.learningRate * gradBasis\n\n    # normalize basis\n    self.basis /= np.sqrt(np.sum(self.basis ** 2, axis=0))", "response": "Learns a single iteration on the provided batch and activations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _thresholdNonLinearity(self, input, threshold, thresholdType=None):\n    if thresholdType == None:\n      thresholdType = self.thresholdType\n\n    activation = np.copy(input)\n\n    if thresholdType == 'soft':\n      return np.maximum(np.abs(activation) - threshold, 0.) * np.sign(activation)\n\n    if thresholdType == 'absoluteHard':\n      activation[np.abs(activation) < threshold] = 0.\n      return activation\n\n    if thresholdType == 'relativeHard':\n      activation[activation < threshold] = 0.\n      return activation", "response": "Non linearity function to transform the activations during training and\n    encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(cls, proto):\n    sparsenet = object.__new__(cls)\n\n    sparsenet.filterDim = proto.filterDim\n    sparsenet.outputDim = proto.outputDim\n    sparsenet.batchSize = proto.batchSize\n\n    lossHistoryProto = proto.losses\n    sparsenet.losses = {}\n    for i in xrange(len(lossHistoryProto)):\n      sparsenet.losses[lossHistoryProto[i].iteration] = lossHistoryProto[i].loss\n    sparsenet._iteration = proto.iteration\n\n    sparsenet.basis = np.reshape(proto.basis, newshape=(sparsenet.filterDim,\n                                                        sparsenet.outputDim))\n\n    # training parameters\n    sparsenet.learningRate = proto.learningRate\n    sparsenet.decayCycle = proto.decayCycle\n    sparsenet.learningRateDecay = proto.learningRateDecay\n\n    # LCA parameters\n    sparsenet.numLcaIterations = proto.numLcaIterations\n    sparsenet.lcaLearningRate = proto.lcaLearningRate\n    sparsenet.thresholdDecay = proto.thresholdDecay\n    sparsenet.minThreshold = proto.minThreshold\n    sparsenet.thresholdType = proto.thresholdType\n\n    # debugging\n    sparsenet.verbosity = proto.verbosity\n    sparsenet.showEvery = proto.showEvery\n    sparsenet.seed = int(proto.seed)\n    if sparsenet.seed is not None:\n      np.random.seed(sparsenet.seed)\n      random.seed(sparsenet.seed)\n\n    return sparsenet", "response": "Reads deserialized data from proto object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, proto):\n    proto.filterDim = self.filterDim\n    proto.outputDim = self.outputDim\n    proto.batchSize = self.batchSize\n\n    lossHistoryProto = proto.init(\"losses\", len(self.losses))\n    i = 0\n    for iteration, loss in self.losses.iteritems():\n      iterationLossHistoryProto = lossHistoryProto[i]\n      iterationLossHistoryProto.iteration = iteration\n      iterationLossHistoryProto.loss = float(loss)\n      i += 1\n\n    proto.iteration = self._iteration\n\n    proto.basis = list(\n      self.basis.flatten().astype(type('float', (float,), {}))\n    )\n\n    # training parameters\n    proto.learningRate = self.learningRate\n    proto.decayCycle = self.decayCycle\n    proto.learningRateDecay = self.learningRateDecay\n\n    # LCA parameters\n    proto.numLcaIterations = self.numLcaIterations\n    proto.lcaLearningRate = self.lcaLearningRate\n    proto.thresholdDecay = self.thresholdDecay\n    proto.minThreshold = self.minThreshold\n    proto.thresholdType = self.thresholdType\n\n    # debugging\n    proto.verbosity = self.verbosity\n    proto.showEvery = self.showEvery\n    proto.seed = self.seed", "response": "Writes serialized data to proto object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset(self):\n\n    # Reset Union Temporal Pooler fields\n    self._poolingActivation = numpy.zeros(self.getNumColumns(), dtype=REAL_DTYPE)\n    self._unionSDR = numpy.array([], dtype=UINT_DTYPE)\n    self._poolingTimer = numpy.ones(self.getNumColumns(), dtype=REAL_DTYPE) * 1000\n    self._poolingActivationInitLevel = numpy.zeros(self.getNumColumns(), dtype=REAL_DTYPE)\n    self._preActiveInput = numpy.zeros(self.getNumInputs(), dtype=REAL_DTYPE)\n    self._prePredictedActiveInput = numpy.zeros((self.getNumInputs(), self._historyLength), dtype=REAL_DTYPE)\n\n    # Reset Spatial Pooler fields\n    self.setOverlapDutyCycles(numpy.zeros(self.getNumColumns(), dtype=REAL_DTYPE))\n    self.setActiveDutyCycles(numpy.zeros(self.getNumColumns(), dtype=REAL_DTYPE))\n    self.setMinOverlapDutyCycles(numpy.zeros(self.getNumColumns(), dtype=REAL_DTYPE))\n    self.setBoostFactors(numpy.ones(self.getNumColumns(), dtype=REAL_DTYPE))", "response": "Resets the state of the Union Temporal Pooler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing one cycle of the Union Temporal Pooler algorithm.", "response": "def compute(self, activeInput, predictedActiveInput, learn):\n    \"\"\"\n    Computes one cycle of the Union Temporal Pooler algorithm.\n    @param activeInput            (numpy array) A numpy array of 0's and 1's that comprises the input to the union pooler\n    @param predictedActiveInput   (numpy array) A numpy array of 0's and 1's that comprises the correctly predicted input to the union pooler\n    @param learn                  (boolen)      A boolen value indicating whether learning should be performed\n    \"\"\"\n    assert numpy.size(activeInput) == self.getNumInputs()\n    assert numpy.size(predictedActiveInput) == self.getNumInputs()\n    self._updateBookeepingVars(learn)\n\n    # Compute proximal dendrite overlaps with active and active-predicted inputs\n    overlapsActive = self._calculateOverlap(activeInput)\n    overlapsPredictedActive = self._calculateOverlap(predictedActiveInput)\n    totalOverlap = (overlapsActive * self._activeOverlapWeight +\n                    overlapsPredictedActive *\n                    self._predictedActiveOverlapWeight).astype(REAL_DTYPE)\n\n    if learn:\n      boostFactors = numpy.zeros(self.getNumColumns(), dtype=REAL_DTYPE)\n      self.getBoostFactors(boostFactors)\n      boostedOverlaps = boostFactors * totalOverlap\n    else:\n      boostedOverlaps = totalOverlap\n\n    activeCells = self._inhibitColumns(boostedOverlaps)\n    self._activeCells = activeCells\n\n    # Decrement pooling activation of all cells\n    self._decayPoolingActivation()\n\n    # Update the poolingActivation of current active Union Temporal Pooler cells\n    self._addToPoolingActivation(activeCells, overlapsPredictedActive)\n\n    # update union SDR\n    self._getMostActiveCells()\n\n    if learn:\n      # adapt permanence of connections from predicted active inputs to newly active cell\n      # This step is the spatial pooler learning rule, applied only to the predictedActiveInput\n      # Todo: should we also include unpredicted active input in this step?\n      self._adaptSynapses(predictedActiveInput, activeCells, self.getSynPermActiveInc(), self.getSynPermInactiveDec())\n\n      # Increase permanence of connections from predicted active inputs to cells in the union SDR\n      # This is Hebbian learning applied to the current time step\n      self._adaptSynapses(predictedActiveInput, self._unionSDR, self._synPermPredActiveInc, 0.0)\n\n      # adapt permenence of connections from previously predicted inputs to newly active cells\n      # This is a reinforcement learning rule that considers previous input to the current cell\n      for i in xrange(self._historyLength):\n        self._adaptSynapses(self._prePredictedActiveInput[:,i], activeCells, self._synPermPreviousPredActiveInc, 0.0)\n\n      # Homeostasis learning inherited from the spatial pooler\n      self._updateDutyCycles(totalOverlap.astype(UINT_DTYPE), activeCells)\n      self._bumpUpWeakColumns()\n      self._updateBoostFactors()\n      if self._isUpdateRound():\n        self._updateInhibitionRadius()\n        self._updateMinDutyCycles()\n\n    # save inputs from the previous time step\n    self._preActiveInput = copy.copy(activeInput)\n    self._prePredictedActiveInput = numpy.roll(self._prePredictedActiveInput,1,1)\n    if self._historyLength > 0:\n      self._prePredictedActiveInput[:, 0] = predictedActiveInput\n\n    return self._unionSDR"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _decayPoolingActivation(self):\n    if self._decayFunctionType == 'NoDecay':\n      self._poolingActivation = self._decayFunction.decay(self._poolingActivation)\n    elif self._decayFunctionType == 'Exponential':\n      self._poolingActivation = self._decayFunction.decay(\\\n                                self._poolingActivationInitLevel, self._poolingTimer)\n\n    return self._poolingActivation", "response": "Decays pooling activation of all cells in the alphabetical order"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _addToPoolingActivation(self, activeCells, overlaps):\n    self._poolingActivation[activeCells] = self._exciteFunction.excite(\n      self._poolingActivation[activeCells], overlaps[activeCells])\n\n    # increase pooling timers for all cells\n    self._poolingTimer[self._poolingTimer >= 0] += 1\n\n    # reset pooling timer for active cells\n    self._poolingTimer[activeCells] = 0\n    self._poolingActivationInitLevel[activeCells] = self._poolingActivation[activeCells]\n\n    return self._poolingActivation", "response": "Adds overlaps from specified active cells to cells pooling\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the most active cells in the Union SDR in sorted order.", "response": "def _getMostActiveCells(self):\n    \"\"\"\n    Gets the most active cells in the Union SDR having at least non-zero\n    activation in sorted order.\n    @return: a list of cell indices\n    \"\"\"\n    poolingActivation = self._poolingActivation\n    nonZeroCells = numpy.argwhere(poolingActivation > 0)[:,0]\n\n    # include a tie-breaker before sorting\n    poolingActivationSubset = poolingActivation[nonZeroCells] + \\\n                              self._poolingActivation_tieBreaker[nonZeroCells]\n    potentialUnionSDR = nonZeroCells[numpy.argsort(poolingActivationSubset)[::-1]]\n\n    topCells = potentialUnionSDR[0: self._maxUnionCells]\n\n    if max(self._poolingTimer) > self._minHistory:\n      self._unionSDR = numpy.sort(topCells).astype(UINT_DTYPE)\n    else:\n      self._unionSDR = []\n\n    return self._unionSDR"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and initialize the specified network instance.", "response": "def createNetwork(networkConfig):\n  \"\"\"\n  Create and initialize the specified network instance.\n\n  @param networkConfig: (dict) the configuration of this network.\n  @return network: (Network) The actual network\n  \"\"\"\n\n  registerAllResearchRegions()\n\n  network = Network()\n\n  if networkConfig[\"networkType\"] == \"L4L2Column\":\n    return createL4L2Column(network, networkConfig, \"_0\")\n  elif networkConfig[\"networkType\"] == \"MultipleL4L2Columns\":\n    return createMultipleL4L2Columns(network, networkConfig)\n  elif networkConfig[\"networkType\"] == \"MultipleL4L2ColumnsWithTopology\":\n    return createMultipleL4L2ColumnsWithTopology(network, networkConfig)\n  elif networkConfig[\"networkType\"] == \"L2456Columns\":\n    return createL2456Columns(network, networkConfig)\n  elif networkConfig[\"networkType\"] == \"L4L2TMColumn\":\n    return createL4L2TMColumn(network, networkConfig, \"_0\")\n  elif networkConfig[\"networkType\"] == \"CombinedSequenceColumn\":\n    return createCombinedSequenceColumn(network, networkConfig, \"_0\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef printNetwork(network):\n  print \"The network has\",len(network.regions.values()),\"regions\"\n  for p in range(network.getMaxPhase()):\n    print \"=== Phase\",p\n    for region in network.regions.values():\n      if network.getPhases(region.name)[0] == p:\n        print \"   \",region.name", "response": "Print out the network sorted by phase"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit skewed normal distributions to 1 - D capactity data and return the distribution parameters.", "response": "def fit_params_to_1d_data(logX):\n    \"\"\"\n    Fit skewed normal distributions to 1-D capactity data, \n    and return the distribution parameters.\n\n    Args\n    ----\n    logX:\n        Logarithm of one-dimensional capacity data, \n        indexed by module and phase resolution index\n\n    \"\"\"\n    m_max = logX.shape[0]\n    p_max = logX.shape[1]\n    params = np.zeros((m_max, p_max, 3)) \n    for m_ in range(m_max):\n        for p_ in range(p_max):\n            params[m_,p_] = skewnorm.fit(logX[m_,p_])\n    \n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the parameters describing a 1 - D capactity distribution for fractional number of modules.", "response": "def get_interpolated_params(m_frac, ph, params):    \n    \"\"\"\n    Get parameters describing a 1-D capactity distribution \n    for fractional number of modules.\n    \"\"\"\n    slope, offset = np.polyfit(np.arange(1,4), params[:3,ph,0], deg=1)\n    a   = slope*m_frac + offset\n    \n    slope, offset = np.polyfit(np.arange(1,4), params[:3,ph,1], deg=1)\n    loc   = slope*m_frac + offset\n    \n    slope, offset = np.polyfit(np.arange(1,4), params[:3,ph,2], deg=1)\n    scale   = slope*m_frac + offset\n\n    return (a, loc, scale)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a single column network containing L4 and L6a layers.", "response": "def createL4L6aLocationColumn(network, L4Params, L6aParams,\n                              inverseReadoutResolution=None,\n                              baselineCellsPerAxis=6, suffix=\"\"):\n  \"\"\"\n  Create a single column network containing L4 and L6a layers. L4 layer\n  processes sensor inputs while L6a processes motor commands using grid cell\n  modules. Sensory input is represented by the feature's active columns and\n  motor input is represented by the displacement vector [dx, dy].\n\n  The grid cell modules used by this network are based on\n  :class:`ThresholdedGaussian2DLocationModule` where the firing rate is computed\n  from on one or more Gaussian activity bumps. The cells are distributed\n  uniformly through the rhombus, packed in the optimal hexagonal arrangement.\n  ::\n\n    Phase\n    -----                    +-------+\n                 +---------->|       |<------------+\n     [2]         |     +---->|  L4   |--winner---+ |\n                 |     |     |       |           | |\n                 |     |     +-------+           | |\n                 |     |       |   ^             | |\n                 |     |       |   |             | |\n                 |     |       |   |             | |\n                 |     |       v   |             | |\n                 |     |     +-------+           | |\n                 |     |     |       |           | |\n     [1,3]       |     +---->|  L6a  |<----------+ |\n                 |     |     |       |--learnable--+\n                 |     |     +-------+\n                 |     |         ^\n            feature  reset       |\n                 |     |         |\n                 |     |         |\n     [0]      [sensorInput] [motorInput]\n\n\n  .. note::\n    Region names are \"motorInput\", \"sensorInput\", \"L4\", and \"L6a\".\n    Each name has an optional string suffix appended to it.\n\n  :param network: network to add the column\n  :type network: Network\n  :param L4Params:  constructor parameters for :class:`ApicalTMPairRegion`\n  :type L4Params: dict\n  :param L6aParams:  constructor parameters for :class:`GridCellLocationRegion`\n  :type L6aParams: dict\n  :param inverseReadoutResolution: Optional readout resolution.\n    The readout resolution specifies the diameter of the circle of phases in the\n    rhombus encoded by a bump. See `createRatModuleFromReadoutResolution.\n  :type inverseReadoutResolution: int\n  :param baselineCellsPerAxis: The baselineCellsPerAxis implies the readout\n    resolution of a grid cell module. If baselineCellsPerAxis=6, that implies\n    that the readout resolution is approximately 1/3. If baselineCellsPerAxis=8,\n    the readout resolution is approximately 1/4\n  :type baselineCellsPerAxis: int or float\n  :param suffix: optional string suffix appended to region name. Useful when\n                 creating multicolumn networks.\n  :type suffix: str\n\n  :return: Reference to the given network\n  :rtype: Network\n  \"\"\"\n  L6aParams = copy.deepcopy(L6aParams)\n  if inverseReadoutResolution is not None:\n    # Configure L6a based on 'resolution'\n    params = computeRatModuleParametersFromReadoutResolution(inverseReadoutResolution)\n    L6aParams.update(params)\n  else:\n    params = computeRatModuleParametersFromCellCount(L6aParams[\"cellsPerAxis\"],\n                                                     baselineCellsPerAxis)\n    L6aParams.update(params)\n\n  numOfcols = L4Params[\"columnCount\"]\n  cellsPerCol = L4Params[\"cellsPerColumn\"]\n  L6aParams[\"anchorInputSize\"] = numOfcols * cellsPerCol\n\n  # Configure L4 'basalInputSize' to be compatible L6a output\n  moduleCount = L6aParams[\"moduleCount\"]\n  cellsPerAxis = L6aParams[\"cellsPerAxis\"]\n\n  L4Params = copy.deepcopy(L4Params)\n  L4Params[\"basalInputWidth\"] = moduleCount * cellsPerAxis * cellsPerAxis\n\n  # Configure sensor output to be compatible with L4 params\n  columnCount = L4Params[\"columnCount\"]\n\n  # Add regions to network\n  motorInputName = \"motorInput\" + suffix\n  sensorInputName = \"sensorInput\" + suffix\n  L4Name = \"L4\" + suffix\n  L6aName = \"L6a\" + suffix\n  dimensions = L6aParams.get(\"dimensions\", 2)\n\n  network.addRegion(sensorInputName, \"py.RawSensor\",\n                    json.dumps({\"outputWidth\": columnCount}))\n  network.addRegion(motorInputName, \"py.RawValues\",\n                    json.dumps({\"outputWidth\": dimensions}))\n  network.addRegion(L4Name, \"py.ApicalTMPairRegion\", json.dumps(L4Params))\n  network.addRegion(L6aName, \"py.GridCellLocationRegion\",\n                    json.dumps(L6aParams))\n\n  # Link sensory input to L4\n  network.link(sensorInputName, L4Name, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"activeColumns\")\n\n  # Link motor input to L6a\n  network.link(motorInputName, L6aName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"displacement\")\n\n  # Link L6a to L4\n  network.link(L6aName, L4Name, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"basalInput\")\n  network.link(L6aName, L4Name, \"UniformLink\", \"\",\n               srcOutput=\"learnableCells\", destInput=\"basalGrowthCandidates\")\n\n  # Link L4 feedback to L6a\n  network.link(L4Name, L6aName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"anchorInput\")\n  network.link(L4Name, L6aName, \"UniformLink\", \"\",\n               srcOutput=\"winnerCells\", destInput=\"anchorGrowthCandidates\")\n\n  # Link reset signal to L4 and L6a\n  network.link(sensorInputName, L4Name, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n  network.link(sensorInputName, L6aName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n\n  # Set phases appropriately\n  network.setPhases(motorInputName, [0])\n  network.setPhases(sensorInputName, [0])\n  network.setPhases(L4Name, [2])\n  network.setPhases(L6aName, [1, 3])\n\n  return network"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef createL246aLocationColumn(network, L2Params, L4Params, L6aParams,\n                              baselineCellsPerAxis=6,\n                              inverseReadoutResolution=None, suffix=\"\"):\n  \"\"\"\n  Create a single column network composed of L2, L4 and L6a layers.\n  L2 layer computes the object representation using :class:`ColumnPoolerRegion`,\n  L4 layer processes sensors input while L6a processes motor commands using grid\n  cell modules. Sensory input is represented by the feature's active columns and\n  motor input is represented by the displacement vector [dx, dy].\n\n  The grid cell modules used by this network are based on\n  :class:`ThresholdedGaussian2DLocationModule` where the firing rate is computed\n  from on one or more Gaussian activity bumps. The cells are distributed\n  uniformly through the rhombus, packed in the optimal hexagonal arrangement.\n  ::\n\n   Phase                       +-------+\n   -----                reset  |       |\n                        +----->|  L2   |<------------------+\n   [3]                  |      |       |                   |\n                        |      +-------+                   |\n                        |        |   ^                     |\n                        |        |   |                     |\n                        |     +1 |   |                     |\n                        |        v   |                     |\n                        |      +-------+                   |\n                  +----------->|       |--winnerCells------+\n   [2]            |     |      |  L4   |<------------+\n                  |     +----->|       |--winner---+ |\n                  |     |      +-------+           | |\n                  |     |        |   ^             | |\n                  |     |        |   |             | |\n                  |     |        |   |             | |\n                  |     |        v   |             | |\n                  |     |      +-------+           | |\n                  |     |      |       |           | |\n    [1,3]         |     +----->|  L6a  |<----------+ |\n                  |     |      |       |--learnable--+\n                  |     |      +-------+\n             feature  reset        ^\n                  |     |          |\n                  |     |          |\n    [0]        [sensorInput]  [motorInput]\n\n\n  .. note::\n    Region names are \"motorInput\", \"sensorInput\". \"L2\", \"L4\", and \"L6a\".\n    Each name has an optional string suffix appended to it.\n\n  :param network: network to add the column\n  :type network: Network\n  :param L2Params:  constructor parameters for :class:`ColumnPoolerRegion`\n  :type L2Params: dict\n  :param L4Params:  constructor parameters for :class:`ApicalTMPairRegion`\n  :type L4Params: dict\n  :param L6aParams:  constructor parameters for :class:`GridCellLocationRegion`\n  :type L6aParams: dict\n  :param inverseReadoutResolution: Optional readout resolution.\n    The readout resolution specifies the diameter of the circle of phases in the\n    rhombus encoded by a bump. See `createRatModuleFromReadoutResolution.\n  :type inverseReadoutResolution: int\n  :param baselineCellsPerAxis: The baselineCellsPerAxis implies the readout\n    resolution of a grid cell module. If baselineCellsPerAxis=6, that implies\n    that the readout resolution is approximately 1/3. If baselineCellsPerAxis=8,\n    the readout resolution is approximately 1/4\n  :type baselineCellsPerAxis: int or float\n  :param suffix: optional string suffix appended to region name. Useful when\n                 creating multicolumn networks.\n  :type suffix: str\n  :return: Reference to the given network\n  :rtype: Network\n  \"\"\"\n\n  # Configure L2 'inputWidth' to be compatible with L4\n  numOfcols = L4Params[\"columnCount\"]\n  cellsPerCol = L4Params[\"cellsPerColumn\"]\n  L2Params = copy.deepcopy(L2Params)\n  L2Params[\"inputWidth\"] = numOfcols * cellsPerCol\n\n  # Configure L4 'apicalInputWidth' to be compatible L2 output\n  L4Params = copy.deepcopy(L4Params)\n  L4Params[\"apicalInputWidth\"] = L2Params[\"cellCount\"]\n\n  # Add L4 - L6a location layers\n  network = createL4L6aLocationColumn(network=network,\n                                      L4Params=L4Params,\n                                      L6aParams=L6aParams,\n                                      inverseReadoutResolution=inverseReadoutResolution,\n                                      baselineCellsPerAxis=baselineCellsPerAxis,\n                                      suffix=suffix)\n  L4Name = \"L4\" + suffix\n  sensorInputName = \"sensorInput\" + suffix\n\n  # Add L2 - L4 object layers\n  L2Name = \"L2\" + suffix\n  network.addRegion(L2Name, \"py.ColumnPoolerRegion\", json.dumps(L2Params))\n\n  # Link L4 to L2\n  network.link(L4Name, L2Name, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"feedforwardInput\")\n  network.link(L4Name, L2Name, \"UniformLink\", \"\",\n               srcOutput=\"winnerCells\",\n               destInput=\"feedforwardGrowthCandidates\")\n\n  # Link L2 feedback to L4\n  network.link(L2Name, L4Name, \"UniformLink\", \"\",\n               srcOutput=\"feedForwardOutput\", destInput=\"apicalInput\",\n               propagationDelay=1)\n\n  # Link reset output to L2\n  network.link(sensorInputName, L2Name, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n\n  # Set L2 phase to be after L4\n  network.setPhases(L2Name, [3])\n\n  return network", "response": "Creates a single column network with L2 L4 and L6a."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a network consisting of multiple columns.", "response": "def createMultipleL246aLocationColumn(network, numberOfColumns, L2Params,\n                                      L4Params, L6aParams,\n                                      inverseReadoutResolution=None,\n                                      baselineCellsPerAxis=6):\n  \"\"\"\n  Create a network consisting of multiple columns. Each column contains one L2,\n  one L4 and one L6a layers identical in structure to the network created by\n  :func:`createL246aLocationColumn`. In addition all the L2 columns are fully\n  connected to each other through their lateral inputs.\n  ::\n\n                            +----lateralInput--+\n                            | +--------------+ |\n                            | |       +1     | |\n Phase                      v |              v |\n -----                   +-------+         +-------+\n                  reset  |       |         |       | reset\n [3]              +----->|  L2   |         |  L2   |<----+\n                  |      |       |         |       |     |\n                  |      +-------+         +-------+     |\n                  |        |   ^             |   ^       |\n                  |     +1 |   |          +1 |   |       |\n                  |        |   |             |   |       |\n                  |        v   |             v   |       |\n                  |      +-------+         +-------+     |\n [2]        +----------->|       |         |       |<----------+\n            |     |      |  L4   |         |  L4   |     |     |\n            |     +----->|       |         |       |<----+     |\n            |     |      +-------+         +-------+     |     |\n            |     |        |   ^             |   ^       |     |\n            |     |        |   |             |   |       |     |\n            |     |        |   |             |   |       |     |\n            |     |        v   |             v   |       |     |\n            |     |      +-------+         +-------+     |     |\n            |     |      |       |         |       |     |     |\n [1,3]      |     +----->|  L6a  |         |  L6a  |<----+     |\n            |     |      |       |         |       |     |     |\n            |     |      +-------+         +-------+     |     |\n       feature  reset        ^                 ^      reset  feature\n            |     |          |                 |         |     |\n            |     |          |                 |         |     |\n [0]     [sensorInput]  [motorInput]      [motorInput] [sensorInput]\n\n  .. note::\n    Region names are \"motorInput\", \"sensorInput\". \"L2\", \"L4\", and \"L6a\".\n    Each name has column number appended to it.\n    For example: \"sensorInput_0\", \"L2_1\", \"L6a_0\" etc.\n\n  :param network: network to add the column\n  :type network: Network\n  :param numberOfColumns: Number of columns to create\n  :type numberOfColumns: int\n  :param L2Params:  constructor parameters for :class:`ColumnPoolerRegion`\n  :type L2Params: dict\n  :param L4Params:  constructor parameters for :class:`ApicalTMPairRegion`\n  :type L4Params: dict\n  :param L6aParams:  constructor parameters for :class:`GridCellLocationRegion`\n  :type L6aParams: dict\n  :param inverseReadoutResolution: Optional readout resolution.\n    The readout resolution specifies the diameter of the circle of phases in the\n    rhombus encoded by a bump. See `createRatModuleFromReadoutResolution.\n  :type inverseReadoutResolution: int\n  :param baselineCellsPerAxis: The baselineCellsPerAxis implies the readout\n    resolution of a grid cell module. If baselineCellsPerAxis=6, that implies\n    that the readout resolution is approximately 1/3. If baselineCellsPerAxis=8,\n    the readout resolution is approximately 1/4\n  :type baselineCellsPerAxis: int or float\n  :return: Reference to the given network\n  :rtype: Network\n  \"\"\"\n  L2Params = copy.deepcopy(L2Params)\n  L4Params = copy.deepcopy(L4Params)\n  L6aParams = copy.deepcopy(L6aParams)\n\n  # Update L2 numOtherCorticalColumns parameter\n  L2Params[\"numOtherCorticalColumns\"] = numberOfColumns - 1\n\n  for i in xrange(numberOfColumns):\n    # Make sure random seed is different for each column\n    L2Params[\"seed\"] = L2Params.get(\"seed\", 42) + i\n    L4Params[\"seed\"] = L4Params.get(\"seed\", 42) + i\n    L6aParams[\"seed\"] = L6aParams.get(\"seed\", 42) + i\n\n    # Create column\n    network = createL246aLocationColumn(network=network,\n                                        L2Params=L2Params,\n                                        L4Params=L4Params,\n                                        L6aParams=L6aParams,\n                                        inverseReadoutResolution=inverseReadoutResolution,\n                                        baselineCellsPerAxis=baselineCellsPerAxis,\n                                        suffix=\"_\" + str(i))\n\n  # Now connect the L2 columns laterally\n  if numberOfColumns > 1:\n    for i in xrange(numberOfColumns):\n      src = str(i)\n      for j in xrange(numberOfColumns):\n        if i != j:\n          dest = str(j)\n          network.link(\n            \"L2_\" + src, \"L2_\" + dest,\n            \"UniformLink\", \"\",\n            srcOutput=\"feedForwardOutput\", destInput=\"lateralInput\",\n            propagationDelay=1)\n\n  return network"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an experiment class according to the log file in", "response": "def rerunExperimentFromLogfile(logFilename):\n    \"\"\"\n    Create an experiment class according to the sequence of operations in\n    logFile and return resulting experiment instance. The log file is created\n    by setting the 'logCalls' constructor parameter to True\n    \"\"\"\n    callLog = LoggingDecorator.load(logFilename)\n\n    # Assume first one is call to constructor\n    exp = L246aNetwork(*callLog[0][1][\"args\"], **callLog[0][1][\"kwargs\"])\n\n    # Call subsequent methods, using stored parameters\n    for call in callLog[1:]:\n      method = getattr(exp, call[0])\n      method(*call[1][\"args\"], **call[1][\"kwargs\"])\n\n    return exp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlearn all provided objects in a single object.", "response": "def learn(self, objects):\n    \"\"\"\n    Learns all provided objects\n\n    :param objects: dict mapping object name to array of sensations, where each\n                    sensation is composed of location and feature SDR for each\n                    column. For example:\n                    {'obj1' : [[[1,1,1],[101,205,523, ..., 1021]],...], ...}\n                    Note: Each column must have the same number of sensations as\n                    the other columns.\n    :type objects: dict[str, array]\n    \"\"\"\n    self.setLearning(True)\n\n    for objectName, sensationList in objects.iteritems():\n      self.sendReset()\n      print \"Learning :\", objectName\n\n      prevLoc = [None] * self.numColumns\n      numFeatures = len(sensationList[0])\n      displacement = [0] * self.dimensions\n\n      for sensation in xrange(numFeatures):\n        for col in xrange(self.numColumns):\n          location = np.array(sensationList[col][sensation][0])\n          feature = sensationList[col][sensation][1]\n\n          # Compute displacement from previous location\n          if prevLoc[col] is not None:\n            displacement = location - prevLoc[col]\n          prevLoc[col] = location\n\n          # learn each pattern multiple times\n          for _ in xrange(self.repeat):\n            # Sense feature at location\n            self.motorInput[col].addDataToQueue(displacement)\n            self.sensorInput[col].addDataToQueue(feature, False, 0)\n            # Only move to the location on the first sensation.\n            displacement = [0] * self.dimensions\n\n      self.network.run(self.repeat * numFeatures)\n\n      # update L2 representations for the object\n      self.learnedObjects[objectName] = self.getL2Representations()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninfer the object given a list of sensations.", "response": "def infer(self, sensations, stats=None, objname=None):\n    \"\"\"\n    Attempt to recognize the object given a list of sensations.\n    You may use :meth:`getCurrentClassification` to extract the current object\n    classification from the network\n\n    :param sensations: Array of sensations, where each sensation is composed of\n                       displacement vector and feature SDR for each column.\n                       For example: [[[1,1,1],[101,205,523, ..., 1021]],...]\n                       Note: Each column must have the same number of sensations\n                       as the other columns.\n\n    :type sensations: list[tuple[list[int], list[int]]]\n    :param stats: Dictionary holding statistics information.\n                  See '_updateInferenceStats' for information on the statistics\n                  collected\n    :type stats: defaultdict[str, list]\n    :param objname: Name of the inferred object, if known\n    :type objname: str or None\n    \"\"\"\n    self.setLearning(False)\n\n    prevLoc = [None] * self.numColumns\n    numFeatures = len(sensations[0])\n    for sensation in xrange(numFeatures):\n      for col in xrange(self.numColumns):\n        assert numFeatures == len(sensations[col])\n\n        location, feature = sensations[col][sensation]\n        # Compute displacement from previous location\n        location = np.array(location)\n        displacement = [0] * self.dimensions\n        if prevLoc[col] is not None:\n          displacement = location - prevLoc[col]\n        prevLoc[col] = location\n\n        self.motorInput[col].addDataToQueue(displacement)\n        self.sensorInput[col].addDataToQueue(feature, False, 0)\n\n      self.network.run(1)\n      if stats is not None:\n        self._updateInferenceStats(stats=stats, objectName=objname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the active representation in L2.", "response": "def getL2Representations(self):\n    \"\"\"\n    Returns the active representation in L2.\n    \"\"\"\n    return [set(L2.getSelf()._pooler.getActiveCells()) for L2 in self.L2Regions]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decayWeights(self, decayConst=60):\n    self.weightsII -= self.weightsII*self.dt/decayConst\n    self.weightsELI -= self.weightsELI*self.dt/decayConst\n    self.weightsERI -= self.weightsERI*self.dt/decayConst\n    self.weightsIEL -= self.weightsIEL*self.dt/decayConst\n    self.weightsIER -= self.weightsIER*self.dt/decayConst", "response": "Decay the network s weights."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef learn(self,\n            runs,\n            dir=1,\n            periodic=False,\n            recurrent=True,\n            randomSpeed=False):\n    \"\"\"\n    Traverses a sinusoidal trajectory across the environment, learning during\n    the process.  A pair of runs across the environment (one in each direction)\n    takes 10 seconds if in a periodic larger environment, and 4 seconds in a \n    smaller nonperiodic environment.\n    :param runs: How many runs across the environment to do.  Each \"run\" is\n            defined as a full sweep across the environment in each direction.\n    :param dir: Which direction to move in first.  Valid values are 1 and -1.\n    :param periodic: Whether or not the learning environment should be\n            periodic (toroidal).\n    :param recurrent: Whether or not recurrent connections should be active\n            during learning.  Warning: True can lead to instability.\n    :param randomSpeed: Whether or not to use a random maximum speed for each\n            run, to better simulate real learning.  Can degrade performance.\n            Only supported in periodic environments.\n    \"\"\"\n    # Set up plotting\n    if self.plotting:\n      self.fig = plt.figure()\n      self.ax1 = self.fig.add_subplot(411)\n      self.ax2 = self.fig.add_subplot(412)\n      self.ax3 = self.fig.add_subplot(212)\n\n      plt.ion()\n      plt.tight_layout()\n      self.ax3.set_xlabel(\"Inhibitory-Inhibitory connections\")\n      self.fig.show()\n      self.fig.canvas.draw()\n\n    # Set up the trajectories and running times.\n    if not periodic:\n      time = 4.*runs\n      timings = [np.arange(0, time, self.dt)]\n      trajectories = [(np.sin(dir*(times*np.pi/2 - np.pi/2.))+1)/2]\n    else:\n      # Space the starting points of the runs out.  This tends to improve the\n      # translation-invariance of the weight profiles, and thus gives better\n      # overall path integration.\n      startingPoint = 0\n      trajectories = []\n      timings = []\n      time = 0\n      residTime = 0\n      for run in xrange(runs):\n        if randomSpeed:\n          speed = np.random.random() + 0.5\n        else:\n          speed = 1.\n        length = 10./speed\n\n        runTimes = np.arange(0, length, self.dt)\n        trajectory = (np.sin(dir*(runTimes*np.pi/(5/speed) - np.pi/2.)) + 1)*\\\n                      2.5 + startingPoint\n        trajectories.append(trajectory)\n        timings.append(runTimes + time)\n        time += length\n        startingPoint += 1./runs\n\n\n\n    for trajectory, timing in zip(trajectories, timings):\n      self.activationsI = np.zeros(self.activationsI.shape)\n      self.activationsER = np.zeros(self.activationsER.shape)\n      self.activationsEL = np.zeros(self.activationsEL.shape)\n      velocities = np.diff(trajectory)/self.dt\n      for i, t in enumerate(timing[:-1]):\n        x = trajectory[i] % 1\n        v = velocities[i]\n        feedforwardInputI = np.exp(-1.*(self.placeCodeI - x)**2 /\n                            (2*self.sigmaLoc**2))\n        feedforwardInputI *= self.placeGainI\n        feedforwardInputI += self.globalTonicMagnitude\n        feedforwardInputE = np.exp(-1.*(self.placeCodeE - x)**2 /\n                            (2*self.sigmaLoc**2))\n        feedforwardInputE *= self.placeGainE\n        feedforwardInputE += self.globalTonicMagnitude\n\n        self.update(feedforwardInputI,\n                    feedforwardInputE,\n                    v,\n                    recurrent=recurrent,\n                    envelope=(not periodic),\n                    iSpeedTuning=periodic,\n                    enforceDale=True,\n                    )\n        self.stdpUpdate(time=i)\n\n        if self.plotting:\n          residTime += self.dt\n          if residTime > PLOT_INTERVAL:\n            residTime -= PLOT_INTERVAL\n            self.ax3.matshow(self.weightsII, cmap=plt.cm.coolwarm)\n            self.plotActivation(position=x, time=t)\n\n      # Carry out any hanging STDP updates.\n      self.stdpUpdate(time=i, clearBuffer=True)\n\n    # Finally, enforce Dale's law.  Inhibitory neurons must be inhibitory,\n    # excitatory neurons must be excitatory.\n    # This could be handled through update, but it's faster to do it here.\n    np.minimum(self.weightsII, 0, self.weightsII)\n    np.minimum(self.weightsIER, 0, self.weightsIER)\n    np.minimum(self.weightsIEL, 0, self.weightsIEL)\n    np.maximum(self.weightsELI, 0, self.weightsELI)\n    np.maximum(self.weightsERI, 0, self.weightsERI)", "response": "Learn the environment for a set of runs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_weights(self, IIMax, IEMax, EIMax):\n    weights = [self.weightsII, self.weightsIEL, self.weightsIER,\n               self.weightsELI, self.weightsERI]\n    norms = [IIMax, IEMax, IEMax, EIMax, EIMax]\n    for w, n in zip(weights, norms):\n      maximum = np.amax(np.abs(w))\n      w /= maximum\n      w *= n", "response": "Rescale our weight matrices to have a certain maximum absolute value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef computeEnvelope(self, placeCode):\n    places = np.abs(placeCode - 0.5)\n    envelope = [1 if p < 1 - self.envelopeWidth else\n                      np.exp(-1.*self.envelopeFactor *\n                      ((p - 1 + self.envelopeWidth)/self.envelopeWidth)**2)\n                      for p in places]\n\n    return np.asarray(envelope)", "response": "Compute an envelope for use in suppressing border cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the current activations and the current time of the anonymization.", "response": "def stdpUpdate(self, time, clearBuffer=False):\n    \"\"\"\n    Adds the current activations to the tracking queue, and then performs an\n    STDP update if possible.\n    :param time: The current time.  Must be provided.\n    :param clearBuffer: Set as True to clear the activation buffer.\n            This should be done at the end of training.\n    \"\"\"\n    if clearBuffer:\n      while len(self.activationBuffer) > 1:\n        baseI, baseEL, baseER, t = self.activationBuffer.popleft()\n        for (I, EL, ER, i) in self.activationBuffer:\n          t = 1. * (i - t) * self.dt\n          self.weightsII += self.stdpKernel(self.learningRate *\\\n                                            self.learnFactorII *\\\n                                            self.dt *\\\n                                            baseI, I, t,\n                                            True, True)\n\n          self.weightsIEL += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorIE *\\\n                                             self.dt *\\\n                                             baseI, EL, t,\n                                             True, False)\n\n          self.weightsIER += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorIE *\\\n                                             self.dt *\\\n                                             baseI, ER, t,\n                                             True, False)\n\n          self.weightsELI += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorEI *\\\n                                             self.dt *\\\n                                             baseEL, I, t,\n                                             False, True)\n\n          self.weightsERI += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorEI *\\\n                                             self.dt *\\\n                                             baseER, I, t,\n                                             False, True)\n\n    else:\n      for I, EL, ER, i in reversed(self.activationBuffer):\n        t = (i - time) * self.dt\n        self.weightsII +=  self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorII *\\\n                                           self.dt *\\\n                                           self.instantaneousI, I, t,\n                                           True, True)\n\n        self.weightsIEL += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorIE *\\\n                                           self.dt *\\\n                                           self.instantaneousI, EL, t,\n                                           True, False)\n\n        self.weightsIER += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorIE *\\\n                                           self.dt *\\\n                                           self.instantaneousI, ER, t,\n                                           True, False)\n\n        self.weightsELI += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorEI *\\\n                                           self.dt *\\\n                                           self.instantaneousEL, I, t,\n                                           False, True)\n\n        self.weightsERI += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorEI *\\\n                                           self.dt *\\\n                                           self.instantaneousER, I, t,\n                                           False, True)\n\n      for I, EL, ER, i in self.activationBuffer:\n        t = (time - i) * self.dt\n        self.weightsII +=  self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorII *\\\n                                           self.dt *\\\n                                           I, self.instantaneousI, t,\n                                           True, True)\n\n        self.weightsIEL += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorIE *\\\n                                           self.dt *\\\n                                           I, self.instantaneousEL, t,\n                                           True, False)\n\n        self.weightsIER += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorIE *\\\n                                           self.dt *\\\n                                           I, self.instantaneousER, t,\n                                           True, False)\n\n        self.weightsELI += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorEI *\\\n                                           self.dt *\\\n                                           EL, self.instantaneousI, t,\n                                           False, True)\n\n        self.weightsERI += self.stdpKernel(self.learningRate *\\\n                                           self.learnFactorEI *\\\n                                           self.dt *\\\n                                           ER, self.instantaneousI, t,\n                                           False, True)\n\n      self.activationBuffer.append((np.copy(self.instantaneousI),\n                                    np.copy(self.instantaneousEL),\n                                    np.copy(self.instantaneousER), time))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nincreasing current activation by amount.", "response": "def excite(self, currentActivation, inputs):\n    \"\"\"\n    Increases current activation by amount.\n    @param currentActivation (numpy array) Current activation levels for each cell\n    @param inputs            (numpy array) inputs for each cell\n    \"\"\"\n\n    currentActivation += self._minValue + (self._maxValue - self._minValue) / (\n                         1 + numpy.exp(-self._steepness * (inputs - self._xMidpoint)))\n\n    return currentActivation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self):\n    plt.ion()\n    plt.show()\n    x = numpy.linspace(0, 15, 100)\n    y = numpy.zeros(x.shape)\n    y = self.excite(y, x)\n    plt.plot(x, y)\n    plt.xlabel('Input')\n    plt.ylabel('Persistence')\n    plt.title('Sigmoid Activation Function')", "response": "plot the activation function"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef computeAccuracyEnding(predictions, truths, iterations,\n                    resets=None, randoms=None, num=None,\n                    sequenceCounter=None):\n  \"\"\"\n  Compute accuracy on the sequence ending\n  \"\"\"\n  accuracy = []\n  numIteration = []\n  numSequences = []\n\n  for i in xrange(len(predictions) - 1):\n    if num is not None and i > num:\n      continue\n\n    if truths[i] is None:\n      continue\n\n    # identify the end of sequence\n    if resets is not None or randoms is not None:\n      if not (resets[i+1] or randoms[i+1]):\n        continue\n\n    correct = truths[i] is None or truths[i] in predictions[i]\n    accuracy.append(correct)\n    numSequences.append(sequenceCounter[i])\n    numIteration.append(iterations[i])\n\n  return (accuracy, numIteration, numSequences)", "response": "Compute the accuracy of the sequence ending in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createValidationDataSampler(dataset, ratio):\n  indices = np.random.permutation(len(dataset))\n  training_count = int(len(indices) * ratio)\n  train = torch.utils.data.SubsetRandomSampler(indices=indices[:training_count])\n  validate = torch.utils.data.SubsetRandomSampler(indices=indices[training_count:])\n  return (train, validate)", "response": "Create a sampler that can be used to split the dataset into training and validation subsets."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting the given dataset into multiple datasets grouped by groupby .", "response": "def splitDataset(dataset, groupby):\n  \"\"\"\n  Split the given dataset into multiple datasets grouped by the given groupby\n  function. For example::\n\n      # Split mnist dataset into 10 datasets, one dataset for each label\n      splitDataset(mnist, groupby=lambda x: x[1])\n\n      # Split mnist dataset into 5 datasets, one dataset for each label pair: [0,1], [2,3],...\n      splitDataset(mnist, groupby=lambda x: x[1] // 2)\n\n  :param dataset: Source dataset to split\n  :param groupby: Group by function. See :func:`itertools.groupby`\n  :return: List of datasets\n  \"\"\"\n\n  # Split dataset based on the group by function and keep track of indices\n  indicesByGroup = collections.defaultdict(list)\n  for k, g in itertools.groupby(enumerate(dataset), key=lambda x: groupby(x[1])):\n    indicesByGroup[k].extend([i[0] for i in g])\n\n  # Sort by group and create a Subset dataset for each of the group indices\n  _, indices = zip(*(sorted(indicesByGroup.items(), key=lambda x: x[0])))\n  return [Subset(dataset, indices=i) for i in indices]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_nonzero_counter(network, stats):\n  if hasattr(network, \"__counter_nonzero__\"):\n    raise ValueError(\"nonzero counter was already registered for this network\")\n\n  if not isinstance(stats, dict):\n    raise ValueError(\"stats must be a dictionary\")\n\n  network.__counter_nonzero__ = stats\n  handles = []\n  for name, module in network.named_modules():\n    handles.append(module.register_forward_hook(_nonzero_counter_hook))\n    if network != module:\n      if hasattr(module, \"__counter_nonzero__\"):\n        raise ValueError(\"nonzero counter was already registered for this module\")\n\n      child_data = dict()\n      network.__counter_nonzero__[name] = child_data\n      module.__counter_nonzero__ = child_data\n\n  network.__counter_nonzero_handles__ = handles", "response": "Registers forward hooks to count the number of nonzero floating points\n  values from all the tensors used by the given network during inference."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the TinyCIFARExperiment with the given parameters and default values from configuration file.", "response": "def initialize(self, params, repetition):\n    \"\"\"\n    Initialize experiment parameters and default values from configuration file.\n    Called at the beginning of each experiment and each repetition.\n    \"\"\"\n    super(TinyCIFARExperiment, self).initialize(params, repetition)\n    self.network_type = params.get(\"network_type\", \"sparse\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint out relevant information at each epoch", "response": "def logger(self, iteration, ret):\n    \"\"\"Print out relevant information at each epoch\"\"\"\n    print(\"Learning rate: {:f}\".format(self.lr_scheduler.get_lr()[0]))\n    entropies = getEntropies(self.model)\n    print(\"Entropy and max entropy: \", float(entropies[0]), entropies[1])\n    print(\"Training time for epoch=\", self.epoch_train_time)\n    for noise in self.noise_values:\n      print(\"Noise= {:3.2f}, loss = {:5.4f}, Accuracy = {:5.3f}%\".format(\n        noise, ret[noise][\"loss\"], 100.0*ret[noise][\"accuracy\"]))\n    print(\"Full epoch time =\", self.epoch_time)\n    if ret[0.0][\"accuracy\"] > 0.7:\n      self.best_noise_score = max(ret[0.1][\"accuracy\"], self.best_noise_score)\n      self.best_epoch = iteration"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot one inference run.", "response": "def plotOneInferenceRun(stats,\n                        fields,\n                        basename,\n                        itemType=\"\",\n                        plotDir=\"plots\",\n                        ymax=100,\n                        trialNumber=0):\n  \"\"\"\n  Plots individual inference runs.\n  \"\"\"\n  if not os.path.exists(plotDir):\n    os.makedirs(plotDir)\n\n  plt.figure()\n\n  # plot request stats\n  for field in fields:\n    fieldKey = field[0] + \" C0\"\n    plt.plot(stats[fieldKey], marker='+', label=field[1])\n\n  # format\n  plt.legend(loc=\"upper right\")\n  plt.xlabel(\"Input number\")\n  plt.xticks(range(stats[\"numSteps\"]))\n  plt.ylabel(\"Number of cells\")\n  plt.ylim(-5, ymax)\n  plt.title(\"Activity while inferring {}\".format(itemType))\n\n  # save\n  relPath = \"{}_exp_{}.pdf\".format(basename, trialNumber)\n  path = os.path.join(plotDir, relPath)\n  plt.savefig(path)\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plotMultipleInferenceRun(stats,\n                       fields,\n                       basename,\n                       plotDir=\"plots\"):\n  \"\"\"\n  Plots individual inference runs.\n  \"\"\"\n  if not os.path.exists(plotDir):\n    os.makedirs(plotDir)\n\n  plt.figure()\n\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n\n  # plot request stats\n  for i, field in enumerate(fields):\n    fieldKey = field[0] + \" C0\"\n    trace = []\n    for s in stats:\n      trace += s[fieldKey]\n    plt.plot(trace, label=field[1], color=colorList[i])\n\n  # format\n  plt.legend(loc=\"upper right\")\n  plt.xlabel(\"Input number\")\n  plt.xticks(range(0, len(stats)*stats[0][\"numSteps\"]+1,5))\n  plt.ylabel(\"Number of cells\")\n  plt.ylim(-5, 55)\n  plt.title(\"Inferring combined sensorimotor and temporal sequence stream\")\n\n  # save\n  relPath = \"{}_exp_combined.pdf\".format(basename)\n  path = os.path.join(plotDir, relPath)\n  plt.savefig(path)\n  plt.close()", "response": "Plots a single inference run for a single input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the accuracy vs number of features in a list of objects and their corresponding L2 terms.", "response": "def plotAccuracyDuringSensorimotorInference(resultsFig5B, title=\"\", yaxis=\"\"):\n  \"\"\"\n  Plot accuracy vs number of features\n  \"\"\"\n  # Read out results and get the ranges we want.\n  with open(resultsFig5B, \"rb\") as f:\n    results = cPickle.load(f)\n\n  objectRange = []\n  featureRange = []\n  for r in results:\n    if r[\"numObjects\"] not in objectRange: objectRange.append(r[\"numObjects\"])\n    if r[\"numFeatures\"] not in featureRange: featureRange.append(r[\"numFeatures\"])\n  objectRange.sort()\n  featureRange.sort()\n  print \"objectRange=\",objectRange\n  print \"featureRange=\",featureRange\n\n  ########################################################################\n  #\n  # Accumulate the TM accuracies for each condition in a list and compute mean\n  # and stdeviations\n  # For L2 we average across all feature ranges\n  accuracies = defaultdict(list)\n  l2Accuracies = defaultdict(list)\n  for r in results:\n    accuracies[(r[\"numObjects\"], r[\"numFeatures\"])].append(r[\"objectCorrectSparsityTM\"])\n    l2Accuracies[r[\"numObjects\"]].append(r[\"objectAccuracyL2\"])\n\n  # meanAccuracy[o,f] = accuracy of TM with o objects and f unique features.\n  meanAccuracy = numpy.zeros((max(objectRange)+1, max(featureRange) + 1))\n  stdev = numpy.zeros((max(objectRange)+1, max(featureRange) + 1))\n  meanL2Accuracy = numpy.zeros(max(objectRange)+1)\n  stdevL2 = numpy.zeros(max(objectRange)+1)\n  for o in objectRange:\n    for f in featureRange:\n      a = numpy.array(accuracies[(o, f)])\n      meanAccuracy[o, f] = 100.0*a.mean()\n      stdev[o, f] = 100.0*a.std()\n\n    # Accuracies for L2\n    a = numpy.array(l2Accuracies[o])\n    meanL2Accuracy[o] = 100.0*a.mean()\n    stdevL2[o] = 100.0*a.std()\n\n\n  # Create the plot.\n  plt.figure()\n  plotPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                          \"plots\", \"accuracy_during_sensorimotor_inference.pdf\")\n\n  # Plot each curve\n  legendList = []\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n\n  for i in range(len(featureRange)):\n    f = featureRange[i]\n    legendList.append('Sequence layer, feature pool size: {}'.format(f))\n    plt.errorbar(objectRange, meanAccuracy[objectRange, f],\n                 yerr=stdev[objectRange, f],\n                 color=colorList[i])\n\n  plt.errorbar(objectRange, meanL2Accuracy[objectRange],\n               yerr=stdevL2[objectRange],\n               color=colorList[len(featureRange)])\n  legendList.append('Sensorimotor layer')\n\n  # format\n  plt.legend(legendList, bbox_to_anchor=(0., 0.6, 1., .102), loc=\"right\", prop={'size':10})\n  plt.xlabel(\"Number of objects\")\n  plt.ylim(-10.0, 110.0)\n  plt.ylabel(yaxis)\n  plt.title(title)\n\n    # save\n  plt.savefig(plotPath)\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plotAccuracyAndMCsDuringDecrementChange(results, title=\"\", yaxis=\"\"):\n\n  decrementRange = []\n  mcRange = []\n  for r in results:\n    if r[\"basalPredictedSegmentDecrement\"] not in decrementRange:\n      decrementRange.append(r[\"basalPredictedSegmentDecrement\"])\n    if r[\"inputSize\"] not in mcRange:\n      mcRange.append(r[\"inputSize\"])\n  decrementRange.sort()\n  mcRange.sort()\n  print decrementRange\n  print mcRange\n\n  ########################################################################\n  #\n  # Accumulate all the results per column in a convergence array.\n  #\n  # accuracy[o,f] = accuracy with o objects in training\n  # and f unique features.\n  accuracy = numpy.zeros((len(mcRange), len(decrementRange)))\n  TMAccuracy = numpy.zeros((len(mcRange), len(decrementRange)))\n  totals = numpy.zeros((len(mcRange), len(decrementRange)))\n  for r in results:\n    dec = r[\"basalPredictedSegmentDecrement\"]\n    nf = r[\"inputSize\"]\n    accuracy[mcRange.index(nf), decrementRange.index(dec)] += r[\"objectAccuracyL2\"]\n    TMAccuracy[mcRange.index(nf), decrementRange.index(dec)] += r[\"sequenceCorrectClassificationsTM\"]\n    totals[mcRange.index(nf), decrementRange.index(dec)] += 1\n\n  for i,f in enumerate(mcRange):\n    print i, f, accuracy[i] / totals[i]\n    print i, f, TMAccuracy[i] / totals[i]\n    print i, f, totals[i]\n    print", "response": "Plots the accuracy vs decrement value of each object in the training\n."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plotAccuracyDuringSequenceInference(dirName, title=\"\", yaxis=\"\"):\n  # Read in results file\n  with open(os.path.join(dirName,\n            \"sequence_batch_high_dec_normal_features.pkl\"), \"rb\") as f:\n    results = cPickle.load(f)\n\n  locationRange = []\n  featureRange = []\n  for r in results:\n    if r[\"numLocations\"] not in locationRange: locationRange.append(r[\"numLocations\"])\n    if r[\"numFeatures\"] not in featureRange: featureRange.append(r[\"numFeatures\"])\n    locationRange.sort()\n  featureRange.sort()\n  if 10 in featureRange: featureRange.remove(10)\n  print \"locationRange=\",locationRange\n  print \"featureRange=\",featureRange\n\n  ########################################################################\n  #\n  # Accumulate the L2 accuracies for each condition in a list and compute mean\n  # and stdeviations\n  # For TM we average across all feature ranges\n  L2Accuracies = defaultdict(list)\n  TMAccuracies = defaultdict(list)\n  for r in results:\n    if r[\"numFeatures\"] in featureRange:\n      L2Accuracies[(r[\"numLocations\"], r[\"numFeatures\"])].append(r[\"sequenceAccuracyL2\"])\n      TMAccuracies[r[\"numLocations\"]].append(r[\"sequenceCorrectSparsityTM\"])\n\n  # meanAccuracy[o,f] = accuracy of TM with o objects and f unique features.\n  meanL2Accuracy = numpy.zeros((max(locationRange)+1, max(featureRange) + 1))\n  stdevL2 = numpy.zeros((max(locationRange)+1, max(featureRange) + 1))\n  meanTMAccuracy = numpy.zeros(max(locationRange)+1)\n  stdevTM = numpy.zeros(max(locationRange)+1)\n  for o in locationRange:\n    for f in featureRange:\n      a = numpy.array(L2Accuracies[(o, f)])\n      meanL2Accuracy[o, f] = 100.0*a.mean()\n      stdevL2[o, f] = 100.0*a.std()\n\n    # Accuracies for TM\n    a = numpy.array(TMAccuracies[o])\n    meanTMAccuracy[o] = 100.0*a.mean()\n    stdevTM[o] = 100.0*a.std()\n\n  ########################################################################\n  #\n  # Create the plot.\n  plt.figure()\n  plotPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                          \"plots\", \"accuracy_during_sequence_inference.pdf\")\n\n  # Plot each curve\n  legendList = []\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n\n  for i in range(len(featureRange)):\n    f = featureRange[i]\n    legendList.append('Sensorimotor layer, feature pool size: {}'.format(f))\n    plt.errorbar(locationRange, meanL2Accuracy[locationRange, f],\n             yerr=stdevL2[locationRange, f],\n             color=colorList[i])\n\n  plt.errorbar(locationRange, meanTMAccuracy[locationRange],\n               yerr=stdevTM[locationRange],\n               color=colorList[len(featureRange)])\n  legendList.append('Temporal sequence layer')\n\n  # format\n  plt.legend(legendList, bbox_to_anchor=(0., 0.65, 1., .102), loc=\"right\", prop={'size':10})\n  plt.xlabel(\"Size of location pool\")\n  # plt.xticks(range(0,max(locationRange)+1,10))\n  # plt.yticks(range(0,int(accuracy.max())+2,10))\n  plt.ylim(-10.0, 110.0)\n  plt.ylabel(yaxis)\n  plt.title(title)\n\n    # save\n  plt.savefig(plotPath)\n  plt.close()", "response": "Plots the accuracy vs number of locations and the number of stdeviations of each condition in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the images similar to those used in the section and the Fig 4A and 4B plots.", "response": "def gen4(dirName):\n  \"\"\"Plots 4A and 4B\"\"\"\n  # Generate images similar to those used in the first plot for the section\n  # \"Simulations with Pure Temporal Sequences\"\n  try:\n    resultsFig4A = os.path.join(dirName, \"pure_sequences_example.pkl\")\n    with open(resultsFig4A, \"rb\") as f:\n      results = cPickle.load(f)\n\n    for trialNum, stat in enumerate(results[\"statistics\"]):\n      plotOneInferenceRun(\n        stat,\n        itemType=\"a single sequence\",\n        fields=[\n          (\"L4 PredictedActive\", \"Predicted active cells in sensorimotor layer\"),\n          (\"TM NextPredicted\", \"Predicted cells in temporal sequence layer\"),\n          (\"TM PredictedActive\",\n           \"Predicted active cells in temporal sequence layer\"),\n        ],\n        basename=\"pure_sequences\",\n        trialNumber=trialNum,\n        plotDir=os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                             \"detailed_plots\")\n      )\n    print \"Plots for Fig 4A generated in 'detailed_plots'\"\n  except Exception, e:\n    print \"\\nCould not generate plots for Fig 4A: \"\n    traceback.print_exc()\n    print\n\n  # Generate the second plot for the section \"Simulations with Pure\n  # Temporal Sequences\"\n  try:\n    plotAccuracyDuringSequenceInference(\n      dirName,\n      title=\"Relative performance of layers while inferring temporal sequences\",\n      yaxis=\"Accuracy (%)\")\n\n    print \"Plots for Fig 4B generated in 'plots'\"\n  except Exception, e:\n    print \"\\nCould not generate plots for Fig 4B: \"\n    traceback.print_exc()\n    print\n\n  # Generate the accuracy vs number of sequences\n  try:\n    plotAccuracyVsSequencesDuringSequenceInference(\n      dirName,\n      title=\"Relative performance of layers while inferring temporal sequences\",\n      yaxis=\"Accuracy (%)\")\n\n    print \"Plots for Fig 4C generated in 'plots'\"\n  except Exception, e:\n    print \"\\nCould not generate plots for Fig 4C: \"\n    traceback.print_exc()\n    print"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexperiment 1 - Calculate error rate as a function of training sequence numbers \u00b0C", "response": "def runExperiment():\n  \"\"\"\n  Experiment 1: Calculate error rate as a function of training sequence numbers\n  :return:\n  \"\"\"\n  trainSeqN = [5, 10, 20, 50, 100, 200]\n  rptPerCondition = 5\n  correctRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  missRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  fpRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  for i in xrange(len(trainSeqN)):\n    for rpt in xrange(rptPerCondition):\n      train_seed = 1\n      numTrainSequence = trainSeqN[i]\n      net = initializeLSTMnet()\n      net = trainLSTMnet(net, numTrainSequence, seedSeq=train_seed)\n      (correctRate, missRate, fpRate) = testLSTMnet(net, numTestSequence, seedSeq=train_seed+rpt)\n      correctRateAll[i, rpt] = correctRate\n      missRateAll[i, rpt] = missRate\n      fpRateAll[i, rpt] = fpRate\n\n  np.savez('result/reberSequenceLSTM.npz',\n           correctRateAll=correctRateAll, missRateAll=missRateAll,\n           fpRateAll=fpRateAll, trainSeqN=trainSeqN)\n\n  plt.figure()\n  plt.subplot(2,2,1)\n  plt.semilogx(trainSeqN, 100*np.mean(correctRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' Hit Rate - Best Match (%)')\n  plt.subplot(2,2,2)\n  plt.semilogx(trainSeqN, 100*np.mean(missRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' Miss Rate (%)')\n  plt.subplot(2,2,3)\n  plt.semilogx(trainSeqN, 100*np.mean(fpRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' False Positive Rate (%)')\n  plt.savefig('result/ReberSequence_LSTMperformance.pdf')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef doExperiment(locationModuleWidth,\n                 bumpType,\n                 cellCoordinateOffsets,\n                 initialIncrement,\n                 minAccuracy,\n                 capacityResolution,\n                 capacityPercentageResolution,\n                 featuresPerObject,\n                 objectWidth,\n                 numFeatures,\n                 featureDistribution,\n                 useTrace,\n                 noiseFactor,\n                 moduleNoiseFactor,\n                 numModules,\n                 thresholds,\n                 seed1,\n                 seed2,\n                 anchoringMethod):\n  \"\"\"\n  Finds the capacity of the specified model and object configuration. The\n  algorithm has two stages. First it finds an upper bound for the capacity by\n  repeatedly incrementing the number of objects by initialIncrement. After it\n  finds a number of objects that is above capacity, it begins the second stage:\n  performing a binary search over the number of objects to find an exact\n  capacity.\n\n  @param initialIncrement (int)\n  For example, if this number is 128, this method will test 128 objects, then\n  256, and so on, until it finds an upper bound, then it will narrow in on the\n  breaking point. This number can't be incorrect, but the simulation will be\n  inefficient if it's too low or too high.\n\n  @param capacityResolution (int)\n  The resolution of the capacity. If capacityResolution=1, this method will find\n  the exact capacity. If the capacityResolution is higher, the method will\n  return a capacity that is potentially less than the actual capacity.\n\n  @param capacityPercentageResolution (float)\n  An alternate way of specifying the resolution. For example, if\n  capacityPercentageResolution=0.01, if the actual capacity is 3020 and this\n  method has reached 3000, it will stop searching, because the next increment\n  will be less than 1% of 3000.\n\n  @param minAccuracy (float)\n  The recognition success rate that the model must achieve.\n  \"\"\"\n  if not os.path.exists(\"traces\"):\n    os.makedirs(\"traces\")\n\n  if seed1 != -1:\n    np.random.seed(seed1)\n\n  if seed2 != -1:\n    random.seed(seed2)\n\n  features = [str(i) for i in xrange(numFeatures)]\n\n  locationConfigs = []\n  scale = 40.0\n\n  if thresholds == -1:\n    thresholds = int(math.ceil(numModules * 0.8))\n  elif thresholds == 0:\n    thresholds = numModules\n  perModRange = float((90.0 if bumpType == \"square\" else 60.0) /\n                      float(numModules))\n  for i in xrange(numModules):\n    orientation = (float(i) * perModRange) + (perModRange / 2.0)\n\n    config = {\n      \"cellsPerAxis\": locationModuleWidth,\n      \"scale\": scale,\n      \"orientation\": np.radians(orientation),\n      \"activationThreshold\": 8,\n      \"initialPermanence\": 1.0,\n      \"connectedPermanence\": 0.5,\n      \"learningThreshold\": 8,\n      \"sampleSize\": 10,\n      \"permanenceIncrement\": 0.1,\n      \"permanenceDecrement\": 0.0,\n    }\n\n    if bumpType == \"square\":\n      config[\"cellCoordinateOffsets\"] = cellCoordinateOffsets\n      config[\"anchoringMethod\"] = anchoringMethod\n    elif bumpType == \"gaussian\":\n      config[\"bumpOverlapMethod\"] = \"probabilistic\"\n      config[\"baselineCellsPerAxis\"] = 6\n    else:\n      raise ValueError(\"Invalid bumpType\", bumpType)\n\n    locationConfigs.append(config)\n\n  l4Overrides = {\n    \"initialPermanence\": 1.0,\n    \"activationThreshold\": thresholds,\n    \"reducedBasalThreshold\": thresholds,\n    \"minThreshold\": numModules,\n    \"sampleSize\": numModules,\n    \"cellsPerColumn\": 16,\n  }\n\n  numObjects = 0\n  accuracy = None\n  allLocationsAreUnique = None\n  occurrencesConvergenceLog = []\n\n  increment = initialIncrement\n  foundUpperBound = False\n\n  while True:\n    currentNumObjects = numObjects + increment\n    numFailuresAllowed = currentNumObjects * (1 - minAccuracy)\n    print \"Testing\", currentNumObjects\n\n    objects = generateObjects(currentNumObjects, featuresPerObject, objectWidth,\n                              numFeatures, featureDistribution)\n\n    column = PIUNCorticalColumn(locationConfigs, L4Overrides=l4Overrides,\n                                bumpType=bumpType)\n    exp = PIUNExperiment(column, featureNames=features,\n                         numActiveMinicolumns=10,\n                         noiseFactor=noiseFactor,\n                         moduleNoiseFactor=moduleNoiseFactor)\n\n    currentLocsUnique = True\n\n    for objectDescription in objects:\n      objLocsUnique = exp.learnObject(objectDescription)\n      currentLocsUnique = currentLocsUnique and objLocsUnique\n\n    numFailures = 0\n\n    try:\n      if useTrace:\n        filename = os.path.join(\n          SCRIPT_DIR,\n          \"traces/capacity-{}-points-{}-cells-{}-objects-{}-feats.html\".format(\n            len(cellCoordinateOffsets)**2, exp.column.L6aModules[0].numberOfCells(),\n            currentNumObjects, numFeatures)\n        )\n        traceFileOut = io.open(filename, \"w\", encoding=\"utf8\")\n        traceHandle = trace(traceFileOut, exp, includeSynapses=False)\n        print \"Logging to\", filename\n\n      for objectDescription in objects:\n        numSensationsToInference = exp.inferObjectWithRandomMovements(\n            objectDescription)\n        if numSensationsToInference is None:\n          numFailures += 1\n\n          if numFailures > numFailuresAllowed:\n            break\n    finally:\n      if useTrace:\n        traceHandle.__exit__()\n        traceFileOut.close()\n\n    if numFailures < numFailuresAllowed:\n      numObjects = currentNumObjects\n      accuracy = float(currentNumObjects - numFailures) / currentNumObjects\n      allLocationsAreUnique = currentLocsUnique\n    else:\n      foundUpperBound = True\n\n    if foundUpperBound:\n      increment /= 2\n\n      goalResolution = capacityResolution\n\n      if capacityPercentageResolution > 0:\n        goalResolution = min(goalResolution,\n                             capacityPercentageResolution*numObjects)\n\n      if increment < goalResolution:\n        break\n\n  result = {\n    \"numObjects\": numObjects,\n    \"accuracy\": accuracy,\n    \"allLocationsAreUnique\": allLocationsAreUnique,\n  }\n\n  print result\n  return result", "response": "This method is used to perform the actual experiment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the parameters of a TPRegion", "response": "def printTPRegionParams(tpregion):\n  \"\"\"\n  Note: assumes we are using TemporalMemory/TPShim in the TPRegion\n  \"\"\"\n  tm = tpregion.getSelf()._tfdr\n  print \"------------PY  TemporalMemory Parameters ------------------\"\n  print \"numberOfCols             =\", tm.getColumnDimensions()\n  print \"cellsPerColumn           =\", tm.getCellsPerColumn()\n  print \"minThreshold             =\", tm.getMinThreshold()\n  print \"activationThreshold      =\", tm.getActivationThreshold()\n  print \"newSynapseCount          =\", tm.getMaxNewSynapseCount()\n  print \"initialPerm              =\", tm.getInitialPermanence()\n  print \"connectedPerm            =\", tm.getConnectedPermanence()\n  print \"permanenceInc            =\", tm.getPermanenceIncrement()\n  print \"permanenceDec            =\", tm.getPermanenceDecrement()\n  print \"predictedSegmentDecrement=\", tm.getPredictedSegmentDecrement()\n  print"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns CLA model through data record 0 nTrain passes run CLA model through data record 0 nTrain nTrain", "response": "def runMultiplePass(df, model, nMultiplePass, nTrain):\n  \"\"\"\n  run CLA model through data record 0:nTrain nMultiplePass passes\n  \"\"\"\n  predictedField = model.getInferenceArgs()['predictedField']\n  print \"run TM through the train data multiple times\"\n  for nPass in xrange(nMultiplePass):\n    for j in xrange(nTrain):\n      inputRecord = getInputRecord(df, predictedField, j)\n      result = model.run(inputRecord)\n      if j % 100 == 0:\n        print \" pass %i, record %i\" % (nPass, j)\n    # reset temporal memory\n    model._getTPRegion().getSelf()._tfdr.reset()\n\n  return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns CLA model SP through data record 0 nTrain nMultiplePass passes", "response": "def runMultiplePassSPonly(df, model, nMultiplePass, nTrain):\n  \"\"\"\n  run CLA model SP through data record 0:nTrain nMultiplePass passes\n  \"\"\"\n\n  predictedField = model.getInferenceArgs()['predictedField']\n  print \"run TM through the train data multiple times\"\n  for nPass in xrange(nMultiplePass):\n    for j in xrange(nTrain):\n      inputRecord = getInputRecord(df, predictedField, j)\n      model._sensorCompute(inputRecord)\n      model._spCompute()\n      if j % 400 == 0:\n        print \" pass %i, record %i\" % (nPass, j)\n\n  return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getSpec(cls):\n    spec = {\n      \"description\":\"Sensor for sending sparse data to an HTM network.\",\n      \"singleNodeOnly\":True,\n      \"outputs\":{\n        \"dataOut\":{\n          \"description\":\"Encoded text\",\n          \"dataType\":\"Real32\",\n          \"count\":0,\n          \"regionLevel\":True,\n          \"isDefaultOutput\":True,\n          },\n        \"resetOut\":{\n          \"description\":\"Boolean reset output.\",\n          \"dataType\":\"Real32\",\n          \"count\":1,\n          \"regionLevel\":True,\n          \"isDefaultOutput\":False,\n          },\n        \"sequenceIdOut\":{\n          \"description\":\"Sequence ID\",\n          \"dataType\":'Real32',\n          \"count\":1,\n          \"regionLevel\":True,\n          \"isDefaultOutput\":False,\n        },\n      },\n      \"inputs\":{},\n      \"parameters\":{\n        \"verbosity\":{\n          \"description\":\"Verbosity level\",\n          \"dataType\":\"UInt32\",\n          \"accessMode\":\"ReadWrite\",\n          \"count\":1,\n          \"constraints\":\"\",\n        },\n        \"outputWidth\":{\n          \"description\":\"Size of output vector\",\n          \"dataType\":\"UInt32\",\n          \"accessMode\":\"ReadWrite\",\n          \"count\":1,\n          \"defaultValue\": 2048,\n          \"constraints\":\"\",\n        },\n      },\n      \"commands\":{\n        \"addDataToQueue\": {\n          \"description\": \"Add data\",\n        }\n      },\n    }\n\n    return spec", "response": "Return base class method for this region."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the next record from the queue and encode it.", "response": "def compute(self, inputs, outputs):\n    \"\"\"\n    Get the next record from the queue and encode it. The fields for inputs and\n    outputs are as defined in the spec above.\n    \"\"\"\n    if len(self.queue) > 0:\n      # Take the top element of the data queue\n      data = self.queue.pop()\n\n    else:\n      raise Exception(\"RawSensor: No data to encode: queue is empty \")\n\n    # Copy data into output vectors\n    outputs[\"resetOut\"][0] = data[\"reset\"]\n    outputs[\"sequenceIdOut\"][0] = data[\"sequenceId\"]\n    outputs[\"dataOut\"][:] = 0\n    outputs[\"dataOut\"][data[\"nonZeros\"]] = 1\n\n    if self.verbosity > 1:\n      print \"RawSensor outputs:\"\n      print \"sequenceIdOut: \", outputs[\"sequenceIdOut\"]\n      print \"resetOut: \", outputs[\"resetOut\"]\n      print \"dataOut: \", outputs[\"dataOut\"].nonzero()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addDataToQueue(self, nonZeros, reset, sequenceId):\n    if type(nonZeros) == type(\"\"):\n      nonZeroList = eval(nonZeros)\n    elif type(nonZeros) == type([]):\n      nonZeroList = nonZeros\n    else:\n      raise Exception(\"RawSensor.addDataToQueue: unknown type for nonZeros\")\n\n    self.queue.appendleft({\n      \"sequenceId\": int(sequenceId),\n      \"reset\": int(reset),\n      \"nonZeros\": nonZeroList,\n    })", "response": "Adds a data item to the sensor s internal queue."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convertSequenceMachineSequence(generatedSequences):\n  sequenceList = []\n  currentSequence = []\n  for s in generatedSequences:\n    if s is None:\n      sequenceList.append(currentSequence)\n      currentSequence = []\n    else:\n      currentSequence.append(s)\n\n  return sequenceList", "response": "Convert a sequence from the SequenceMachine into a list of sequences such\n athe that each sequence is a list of SDRs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating high order sequences using SequenceMachine", "response": "def generateSequences(n=2048, w=40, sequenceLength=5, sequenceCount=2,\n                      sharedRange=None, seed=42):\n  \"\"\"\n  Generate high order sequences using SequenceMachine\n  \"\"\"\n  # Lots of room for noise sdrs\n  patternAlphabetSize = 10*(sequenceLength * sequenceCount)\n  patternMachine = PatternMachine(n, w, patternAlphabetSize, seed)\n  sequenceMachine = SequenceMachine(patternMachine, seed)\n  numbers = sequenceMachine.generateNumbers(sequenceCount, sequenceLength,\n                                            sharedRange=sharedRange )\n  generatedSequences = sequenceMachine.generateFromNumbers(numbers)\n\n  return sequenceMachine, generatedSequences, numbers"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun inference on this set of sequences and compute error.", "response": "def runInference(exp, sequences, enableFeedback=True):\n  \"\"\"\n  Run inference on this set of sequences and compute error\n  \"\"\"\n  if enableFeedback:\n    print \"Feedback enabled: \"\n  else:\n    print \"Feedback disabled: \"\n\n  error = 0\n  activityTraces = []\n  responses = []\n  for i,sequence in enumerate(sequences):\n    (avgActiveCells, avgPredictedActiveCells, activityTrace, responsesThisSeq) = exp.infer(\n      sequence, sequenceNumber=i, enableFeedback=enableFeedback)\n    error += avgActiveCells\n    activityTraces.append(activityTrace)\n    responses.append(responsesThisSeq)\n    print \" \"\n  error /= len(sequences)\n  print \"Average error = \",error\n  return error, activityTraces, responses"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a simple L2 experiment where two objects share a location and feature pair.", "response": "def runLateralDisambiguation(noiseLevel=None, profile=False):\n  \"\"\"\n  Runs a simple experiment where two objects share a (location, feature) pair.\n  At inference, one column sees that ambiguous pair, and the other sees a\n  unique one. We should see the first column rapidly converge to a\n  unique representation.\n\n  Parameters:\n  ----------------------------\n  @param    noiseLevel (float)\n            Noise level to add to the locations and features during inference\n\n  @param    profile (bool)\n            If True, the network will be profiled after learning and inference\n\n  \"\"\"\n  exp = L4L2Experiment(\n    \"lateral_disambiguation\",\n    numCorticalColumns=2,\n  )\n\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=1024,\n    externalInputSize=1024,\n    numCorticalColumns=2,\n  )\n  objects.addObject([(1, 1), (2, 2)])\n  objects.addObject([(1, 1), (3, 2)])\n\n  exp.learnObjects(objects.provideObjectsToLearn())\n  if profile:\n    exp.printProfile()\n\n  inferConfig = {\n    \"noiseLevel\": noiseLevel,\n    \"numSteps\": 6,\n    \"pairs\": {\n      # this should activate 0 and 1\n      0: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)],\n      # this should activate 1\n      1: [(3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2)]\n    }\n  }\n\n  exp.infer(objects.provideObjectToInfer(inferConfig), objectName=1)\n  if profile:\n    exp.printProfile()\n\n  exp.plotInferenceStats(\n    fields=[\"L2 Representation\",\n            \"Overlap L2 with object\",\n            \"L4 Representation\"],\n    onePlot=False,\n  )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstretching test that learns a lot of objects. Parameters: ---------------------------- @param noiseLevel (float) Noise level to add to the locations and features during inference @param profile (bool) If True, the network will be profiled after learning and inference", "response": "def runStretch(noiseLevel=None, profile=False):\n  \"\"\"\n  Stretch test that learns a lot of objects.\n\n  Parameters:\n  ----------------------------\n  @param    noiseLevel (float)\n            Noise level to add to the locations and features during inference\n\n  @param    profile (bool)\n            If True, the network will be profiled after learning and inference\n\n  \"\"\"\n  exp = L4L2Experiment(\n    \"stretch_L10_F10_C2\",\n    numCorticalColumns=2,\n  )\n\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=1024,\n    externalInputSize=1024,\n    numCorticalColumns=2,\n  )\n  objects.createRandomObjects(10, 10, numLocations=10, numFeatures=10)\n  print \"Objects are:\"\n  for object, pairs in objects.objects.iteritems():\n    print str(object) + \": \" + str(pairs)\n\n  exp.learnObjects(objects.provideObjectsToLearn())\n  if profile:\n    exp.printProfile(reset=True)\n\n  # For inference, we will check and plot convergence for object 0. We create a\n  # sequence of random sensations for each column.  We will present each\n  # sensation for 4 time steps to let it settle and ensure it converges.\n  objectCopy1 = [pair for pair in objects[0]]\n  objectCopy2 = [pair for pair in objects[0]]\n  objectCopy3 = [pair for pair in objects[0]]\n  random.shuffle(objectCopy1)\n  random.shuffle(objectCopy2)\n  random.shuffle(objectCopy3)\n\n  # stay multiple steps on each sensation\n  objectSensations1 = []\n  for pair in objectCopy1:\n    for _ in xrange(4):\n      objectSensations1.append(pair)\n\n  # stay multiple steps on each sensation\n  objectSensations2 = []\n  for pair in objectCopy2:\n    for _ in xrange(4):\n      objectSensations2.append(pair)\n\n  # stay multiple steps on each sensation\n  objectSensations3 = []\n  for pair in objectCopy3:\n    for _ in xrange(4):\n      objectSensations3.append(pair)\n\n  inferConfig = {\n    \"numSteps\": len(objectSensations1),\n    \"noiseLevel\": noiseLevel,\n    \"pairs\": {\n      0: objectSensations1,\n      1: objectSensations2,\n      # 2: objectSensations3,  # Uncomment for 3 columns\n    }\n  }\n\n  exp.infer(objects.provideObjectToInfer(inferConfig), objectName=0)\n  if profile:\n    exp.printProfile()\n\n  exp.plotInferenceStats(\n    fields=[\"L2 Representation\",\n            \"Overlap L2 with object\",\n            \"L4 Representation\"],\n    onePlot=False,\n  )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trainModel(model, loader, optimizer, device, criterion=F.nll_loss,\n               batches_in_epoch=sys.maxsize, batch_callback=None,\n               progress_bar=None):\n  \"\"\"\n  Train the given model by iterating through mini batches. An epoch\n  ends after one pass through the training set, or if the number of mini\n  batches exceeds the parameter \"batches_in_epoch\".\n\n  :param model: pytorch model to be trained\n  :type model: torch.nn.Module\n  :param loader: train dataset loader\n  :type loader: :class:`torch.utils.data.DataLoader`\n  :param optimizer: Optimizer object used to train the model.\n         This function will train the model on every batch using this optimizer\n         and the :func:`torch.nn.functional.nll_loss` function\n  :param batches_in_epoch: Max number of mini batches to train.\n  :param device: device to use ('cpu' or 'cuda')\n  :type device: :class:`torch.device\n  :param criterion: loss function to use\n  :type criterion: function\n  :param batch_callback: Callback function to be called on every batch with the\n                         following parameters: model, batch_idx\n  :type batch_callback: function\n  :param progress_bar: Optional :class:`tqdm` progress bar args.\n                       None for no progress bar\n  :type progress_bar: dict or None\n  \"\"\"\n  model.train()\n  if progress_bar is not None:\n    loader = tqdm(loader, **progress_bar)\n    # update progress bar total based on batches_in_epoch\n    if batches_in_epoch < len(loader):\n      loader.total = batches_in_epoch\n\n  for batch_idx, (data, target) in enumerate(loader):\n    data, target = data.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n    if batch_callback is not None:\n      batch_callback(model=model, batch_idx=batch_idx)\n    if batch_idx >= batches_in_epoch:\n      break\n\n  if progress_bar is not None:\n    loader.n = loader.total\n    loader.close()", "response": "Train the given model on the given dataset loader."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating pre - trained model using given test dataset loader.", "response": "def evaluateModel(model, loader, device,\n                  batches_in_epoch=sys.maxsize,\n                  criterion=F.nll_loss, progress=None):\n  \"\"\"\n  Evaluate pre-trained model using given test dataset loader.\n\n  :param model: Pretrained pytorch model\n  :type model: torch.nn.Module\n  :param loader: test dataset loader\n  :type loader: :class:`torch.utils.data.DataLoader`\n  :param device: device to use ('cpu' or 'cuda')\n  :type device: :class:`torch.device\n  :param batches_in_epoch: Max number of mini batches to test on.\n  :param criterion: loss function to use\n  :type criterion: function\n  :param progress: Optional :class:`tqdm` progress bar args. None for no progress bar\n  :type progress: dict or None\n\n  :return: dictionary with computed \"accuracy\", \"loss\", \"total_correct\". The\n           loss value is computed using :func:`torch.nn.functional.nll_loss`\n  :rtype: dict\n  \"\"\"\n  model.eval()\n  loss = 0\n  correct = 0\n  dataset_len = len(loader.sampler)\n\n  if progress is not None:\n    loader = tqdm(loader, **progress)\n\n  with torch.no_grad():\n    for batch_idx, (data, target) in enumerate(loader):\n      data, target = data.to(device), target.to(device)\n      output = model(data)\n      loss += criterion(output, target, reduction='sum').item()\n      pred = output.max(1, keepdim=True)[1]\n      correct += pred.eq(target.view_as(pred)).sum().item()\n\n      if batch_idx >= batches_in_epoch:\n        break\n\n  if progress is not None:\n    loader.close()\n\n  loss /= dataset_len\n  accuracy = correct / dataset_len\n\n  return {\"total_correct\": correct,\n          \"loss\": loss,\n          \"accuracy\": accuracy}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun an experiment to test the false positive rate based on the number of synapses and synapses.", "response": "def run_false_positive_experiment_dim(\n                              numActive = 128,\n                              dim = 500,\n                              numSamples = 1000,\n                              numDendrites = 500,\n                              synapses = 24,\n                              numTrials = 10000,\n                              seed = 42,\n                              nonlinearity = sigmoid_nonlinearity(11.5, 5)):\n  \"\"\"\n  Run an experiment to test the false positive rate based on number of synapses\n  per dendrite, dimension and sparsity.  Uses two competing neurons, along the\n  P&M model.\n\n  Based on figure 5B in the original SDR paper.\n  \"\"\"\n  numpy.random.seed(seed)\n  fps = []\n  fns = []\n  totalUnclassified = 0\n\n  for trial in range(numTrials):\n\n    # data = generate_evenly_distributed_data_sparse(dim = dim,\n    #                                                num_active = numActive,\n    #                                                num_samples = numSamples)\n    # labels = numpy.asarray([1 for i in range(numSamples / 2)] +\n    #                        [-1 for i in range(numSamples / 2)])\n    # flipped_labels = labels * -1\n\n    negData = generate_evenly_distributed_data_sparse(dim = dim,\n                                                   num_active = numActive,\n                                                   num_samples = numSamples/2)\n    posData = generate_evenly_distributed_data_sparse(dim = dim,\n                                                   num_active = numActive,\n                                                   num_samples = numSamples/2)\n    halfLabels = numpy.asarray([1 for _ in range(numSamples / 2)])\n    flippedHalfLabels = halfLabels * -1\n\n    neuron = Neuron(size =synapses * numDendrites,\n                    num_dendrites = numDendrites,\n                    dendrite_length = synapses,\n                    dim = dim, nonlinearity = nonlinearity)\n    neg_neuron = Neuron(size =synapses * numDendrites,\n                        num_dendrites = numDendrites,\n                        dendrite_length = synapses,\n                        dim = dim, nonlinearity = nonlinearity)\n\n    neuron.HTM_style_initialize_on_positive_data(posData)\n    neg_neuron.HTM_style_initialize_on_positive_data(negData)\n\n    # Get error for positively labeled data\n    fp, fn, uc = get_error(posData, halfLabels, [neuron], [neg_neuron])\n    totalUnclassified += uc\n    fps.append(fp)\n    fns.append(fn)\n\n    # Get error for negatively labeled data\n    fp, fn, uc = get_error(negData, flippedHalfLabels, [neuron], [neg_neuron])\n    totalUnclassified += uc\n    fps.append(fp)\n    fns.append(fn)\n\n\n  print \"Error with n = {} : {} FP, {} FN, {} unclassified\".format(\n    dim, sum(fps), sum(fns), totalUnclassified)\n\n  result = {\n    \"dim\": dim,\n    \"totalFP\": sum(fps),\n    \"totalFN\": sum(fns),\n    \"total mistakes\": sum(fns + fps) + totalUnclassified,\n    \"error\": float(sum(fns + fps) + totalUnclassified) / (numTrials * numSamples),\n    \"totalSamples\": numTrials * numSamples,\n    \"a\": numActive,\n    \"num_dendrites\": numDendrites,\n    \"totalUnclassified\": totalUnclassified,\n    \"synapses\": 24,\n    \"seed\": seed,\n  }\n\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the objects in a canonical format to be sent to an experiment.", "response": "def provideObjectsToLearn(self, learningConfig, plot=False):\n    \"\"\"\n    Returns the objects in a canonical format to be sent to an experiment.\n\n    The input, learningConfig, should have the following format. It is a\n    mapping from object to a list of features to sample locations from, and\n    the number of points to sample from each feature. Note that these objects\n    should be first added with .addObjects().\n    These features can be either hard-coded with their key or accessed\n    with .getFeatures.\n\n    An other possibility is to directly specify locations. The machine will\n    use the object to find the corresponding feature (an empty feature will\n    be sent if the location is not on the object's surface).\n\n    learningConfig = {\n      # hard-coded keys and number of points\n      \"cube\": [(\"face\", 5), (\"edge\", 5), (\"vertex\", 3)],\n\n      # programmatically-accessed keys and number of points\n      \"cylinder\": [(feature, 5) for feature in cylinder.getFeatures()],\n\n      # specific locations\n      \"sphere\": [(10, 5, 3), (12, 45, 32), (12, 5, 46)],\n    }\n\n    The returned format is a a dictionary where the keys are object names, and\n    values are lists of sensations, each sensation being a mapping from\n    cortical column index to a pair of SDR's (one location and one feature).\n\n    Parameters:\n    ----------------------------\n    @param   learningConfig (dict)\n             Configuration for learning, as described above.\n\n    \"\"\"\n    objects = {}\n\n    for objectName, locationList in learningConfig.iteritems():\n\n      sensationList = []\n      physicalObject = self.objects[objectName]\n      if plot:\n        fig, ax = physicalObject.plot()\n\n      for element in locationList:\n\n        #  location name and number of points\n        if len(element) == 2:\n          featureName, numLocations = element\n          for _ in xrange(numLocations):\n            location = physicalObject.sampleLocationFromFeature(featureName)\n            sensationList.append(\n              self._getSDRPairs(\n                [(location,\n                  physicalObject.getFeatureID(location))] * self.numColumns\n              )\n            )\n            if plot:\n              x, y, z = tuple(location)\n              ax.scatter(x, y, z, marker=\"v\", s=100, c=\"r\")\n\n        # explicit location\n        elif len(element) == 3:\n          location = list(element)\n          sensationList.append(\n            self._getSDRPairs(\n              [(location,\n                physicalObject.getFeatureID(location))] * self.numColumns\n            )\n          )\n          if plot:\n            x, y, z = tuple(location)\n            ax.scatter(x, y, z, marker=\"v\", s=100, c=\"r\")\n\n        else:\n          raise ValueError(\"Unsupported type for location spec\")\n\n      objects[objectName] = sensationList\n      if plot:\n        plt.title(\"Learning points for object {}\".format(objectName))\n        plt.savefig(\"learn_{}.png\".format(objectName))\n        plt.close()\n\n    self._checkObjectsToLearn(objects)\n    return objects"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the sensations of the object in a canonical format to be sent to an experiment.", "response": "def provideObjectToInfer(self, inferenceConfig, plot=False):\n    \"\"\"\n    Returns the sensations in a canonical format to be sent to an experiment.\n\n    The input inferenceConfig should be a dict with the following form. The\n    \"pairs\" field provide a mapping from cortical column to a list of\n    sensations, each sensations being either:\n      - a feature key to sample a location from\n      - an explicit location\n\n    {\n      \"numSteps\": 2  # number of sensations\n      \"noiseLevel\": 0.05  # noise to add to sensations (optional)\n      \"objectName\": 0  # optional\n      \"pairs\": {\n        0: [\"random\", \"face\"]  # locations for cortical column 0\n        1: [(12, 32, 34), (23, 23, 32)]  # locations for cortical column 1\n      }\n    }\n\n    The returned format is a a lists of sensations, each sensation being a\n    mapping from cortical column index to a pair of SDR's (one location and\n    one feature).\n\n    Parameters:\n    ----------------------------\n    @param   inferenceConfig (dict)\n             Inference spec for experiment (cf above for format)\n\n    \"\"\"\n    if \"numSteps\" in inferenceConfig:\n      numSteps = inferenceConfig[\"numSteps\"]\n    else:\n      numSteps = len(inferenceConfig[\"pairs\"][0])\n\n    if \"noiseLevel\" in inferenceConfig:\n      noise = inferenceConfig[\"noiseLevel\"]\n    else:\n      noise = None\n\n    # some checks\n    if numSteps == 0:\n      raise ValueError(\"No inference steps were provided\")\n    for col in xrange(self.numColumns):\n      if len(inferenceConfig[\"pairs\"][col]) != numSteps:\n        raise ValueError(\"Incompatible numSteps and actual inference steps\")\n\n    if \"objectName\" in inferenceConfig:\n      physicalObject = self.objects[inferenceConfig[\"objectName\"]]\n    else:\n      physicalObject = None\n    if plot:\n      # don't use if object is not known\n      fig, ax = physicalObject.plot()\n      colors = plt.cm.rainbow(np.linspace(0, 1, numSteps))\n\n    sensationSteps = []\n    for step in xrange(numSteps):\n      pairs = [\n        inferenceConfig[\"pairs\"][col][step] for col in xrange(self.numColumns)\n      ]\n      for i in xrange(len(pairs)):\n        if isinstance(pairs[i], str):\n          location = physicalObject.sampleLocationFromFeature(pairs[i])\n          pairs[i] = (\n            location,\n            physicalObject.getFeatureID(location)\n          )\n        else:\n          location = pairs[i]\n          pairs[i] = (\n            location,\n            physicalObject.getFeatureID(location)\n          )\n        if plot:\n          x, y, z = tuple(location)\n          ax.scatter(x, y, z, marker=\"v\", s=100, c=colors[step])\n\n      sensationSteps.append(self._getSDRPairs(pairs, noise=noise))\n\n    if plot:\n      plt.title(\"Inference points for object {}\".format(\n        inferenceConfig[\"objectName\"])\n      )\n      plt.savefig(\"infer_{}.png\".format( inferenceConfig[\"objectName\"]))\n      plt.close()\n\n    self._checkObjectToInfer(sensationSteps)\n    return sensationSteps"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addObject(self, object, name=None):\n    if name is None:\n      name = len(self.objects)\n\n    self.objects[name] = object", "response": "Adds an object to the Machine."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getSDRPairs(self, pairs, noise=None):\n    sensations = {}\n    for col in xrange(self.numColumns):\n      location, featureID = pairs[col]\n      location = [int(coord) for coord in location]\n\n      location = self.locationEncoder.encode(\n        (np.array(location, dtype=\"int32\"), self._getRadius(location))\n      )\n      location = set(location.nonzero()[0])\n\n      # generate empty feature if requested\n      if featureID == -1:\n        feature = set()\n      # generate union of features if requested\n      elif isinstance(featureID, tuple):\n        feature = set()\n        for idx in list(featureID):\n          feature = feature | self.features[col][idx]\n      else:\n        feature = self.features[col][featureID]\n\n      if noise is not None:\n        location = self._addNoise(location, noise)\n        feature = self._addNoise(feature, noise)\n\n      sensations[col] = (location, feature)\n\n    return sensations", "response": "This method returns a list of sensations for each location and feature in each pair per\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getRadius(self, location):\n    # TODO: find better heuristic\n    return int(math.sqrt(sum([coord ** 2 for coord in location])))", "response": "Returns the radius associated with the given location."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _addNoise(self, pattern, noiseLevel):\n    if pattern is None:\n      return None\n\n    newBits = []\n    for bit in pattern:\n      if random.random() < noiseLevel:\n        newBits.append(random.randint(0, max(pattern)))\n      else:\n        newBits.append(bit)\n\n    return set(newBits)", "response": "Adds noise to the given list of patterns and returns a list of noisy copies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a random SDR with specified number of bits and total size.", "response": "def _generatePattern(self, numBits, totalSize):\n    \"\"\"\n    Generates a random SDR with specified number of bits and total size.\n    \"\"\"\n    cellsIndices = range(totalSize)\n    random.shuffle(cellsIndices)\n    return set(cellsIndices[:numBits])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning recent apically predicted cells for each tick of apical timer", "response": "def apicalCheck(self, apicalInput):\n    \"\"\"\n    Return 'recent' apically predicted cells for each tick  of apical timer\n    - finds active apical segments corresponding to predicted basal segment,\n\n    @param apicalInput (numpy array)\n    List of active input bits for the apical dendrite segments\n    \"\"\"\n    # Calculate predictions for this timestep\n    (activeApicalSegments, matchingApicalSegments,\n     apicalPotentialOverlaps) = self._calculateSegmentActivity(\n       self.apicalConnections, apicalInput, self.connectedPermanence,\n       self.activationThreshold, self.minThreshold, self.reducedBasalThreshold)\n\n    apicallySupportedCells = self.apicalConnections.mapSegmentsToCells(\n      activeApicalSegments)\n\n    predictedCells = np.intersect1d(\n      self.basalConnections.mapSegmentsToCells(self.activeBasalSegments),\n      apicallySupportedCells)\n\n    return predictedCells"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setupData(self, dataPath, numLabels=0, ordered=False, stripCats=False, seed=42, **kwargs):\n    self.split(dataPath, numLabels, **kwargs)\n\n    if not ordered:\n      self.randomizeData(seed)\n    \n    filename, ext = os.path.splitext(dataPath)\n    classificationFileName = \"{}_category.json\".format(filename)\n    dataFileName = \"{}_network{}\".format(filename, ext)\n    \n    if stripCats:\n      self.stripCategories()\n  \n    self.saveData(dataFileName, classificationFileName)\n    \n    return dataFileName", "response": "This method is used to setup a network data file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split(self, filePath=None, numLabels=3, textPreprocess=False, dataDict=None,\n            abbrCSV=\"\", contrCSV=\"\", ignoreCommon=100,\n            removeStrings=\"[identifier deleted]\", correctSpell=True):\n    \"\"\"\n    Split all the comments in a file into tokens, w/ or w/o preprocessing.\n    Specifying both filePath and dataDict will prefer filePath.\n\n    @param filePath        (str)    Path to csv file\n    @param dataDict        (dict)   Data as returned by readCSV()\n    @param numLabels       (int)    Number of columns of category labels.\n    @param textPreprocess  (bool)   True will preprocess text while tokenizing.\n    \n    @return dataDict       (dict)   Data as read in from filePath.\n\n    Please see TextPreprocess tokenize() for the other parameters; they're only\n    used when textPreprocess is True.\n    \"\"\"\n    if filePath:\n      dataDict = readCSV(filePath, numLabels=numLabels)\n\n    if dataDict is None:\n      raise Exception(\"No data given, or could not read CSV.\")\n\n    preprocessor = TextPreprocess(abbrCSV=abbrCSV, contrCSV=contrCSV)\n    expandAbbr = (abbrCSV != \"\")\n    expandContr = (contrCSV != \"\")\n\n    for recordNum, record in dataDict.iteritems():\n      comment, categories, uniqueID = record\n      \n      # Convert the categories to a string of their IDs\n      categories = string.join([str(self.categoryToId[c]) for c in categories])\n\n      if textPreprocess:\n        tokens, _ = preprocessor.tokenizeAndFilter(\n            comment, ignoreCommon, removeStrings, correctSpell, expandAbbr,\n            expandContr)\n      else:\n        tokens = preprocessor.tokenize(comment)\n\n      data = self._formatSequence(tokens, categories, recordNum, uniqueID)\n\n      self.records.append(data)\n      self.sequenceCount += 1\n    \n    return dataDict", "response": "Splits all the comments in a file into tokens and labels."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _formatSequence(tokens, categories, seqID, uniqueID):\n    record = {\"_category\":categories,\n              \"_sequenceId\":seqID}\n    data = []\n    reset = 1\n    for t in tokens:\n      tokenRecord = record.copy()\n      tokenRecord[\"_token\"] = t\n      tokenRecord[\"_reset\"] = reset\n      tokenRecord[\"ID\"] = uniqueID\n      reset = 0\n      data.append(tokenRecord)\n\n    return data", "response": "Write the sequence of data records for this sample."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef saveData(self, dataOutputFile, categoriesOutputFile):\n    if self.records is None:\n      return False\n\n    if not dataOutputFile.endswith(\"csv\"):\n      raise TypeError(\"data output file must be csv.\")\n    if not categoriesOutputFile.endswith(\"json\"):\n      raise TypeError(\"category output file must be json\")\n\n    # Ensure directory exists\n    dataOutputDirectory = os.path.dirname(dataOutputFile)\n    if not os.path.exists(dataOutputDirectory):\n      os.makedirs(dataOutputDirectory)\n\n    categoriesOutputDirectory = os.path.dirname(categoriesOutputFile)\n    if not os.path.exists(categoriesOutputDirectory):\n      os.makedirs(categoriesOutputDirectory)\n\n    with open(dataOutputFile, \"w\") as f:\n      # Header\n      writer = csv.DictWriter(f, fieldnames=self.fieldNames)\n      writer.writeheader()\n\n      # Types\n      writer.writerow(self.types)\n\n      # Special characters\n      writer.writerow(self.specials)\n\n      for data in self.records:\n        for record in data:\n          writer.writerow(record)\n\n    with open(categoriesOutputFile, \"w\") as f:\n      f.write(json.dumps(self.categoryToId,\n                         sort_keys=True,\n                         indent=4,\n                         separators=(\",\", \": \")))\n\n    return dataOutputFile", "response": "Save the processed data and the associated category mapping."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generateSequence(self, text, preprocess=False):\n    # TODO: enable text preprocessing; abstract out the logic in split() into a common method.\n    tokens = TextPreprocess().tokenize(text)\n    cat = [-1]\n    self.sequenceCount += 1\n    uniqueID = \"q\"\n    data = self._formatSequence(tokens, cat, self.sequenceCount-1, uniqueID)\n\n    return data", "response": "Generates a list of lists representing the text sequence in network data \n    format. Does not preprocess the text."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getSamples(netDataFile):\n    try:\n      with open(netDataFile) as f:\n        reader = csv.reader(f)\n        header = next(reader, None)\n        next(reader, None)\n        resetIdx = next(reader).index(\"R\")\n        tokenIdx = header.index(\"_token\")\n        catIdx = header.index(\"_category\")\n        idIdx = header.index(\"ID\")\n\n        currentSample = []\n        samples = OrderedDict()\n        for line in reader:\n          if int(line[resetIdx]) == 1:\n            if len(currentSample) != 0:\n              samples[line[idIdx]] = ([\" \".join(currentSample)],\n                                      [int(c) for c in line[catIdx].split(\" \")])\n            currentSample = [line[tokenIdx]]\n          else:\n            currentSample.append(line[tokenIdx])\n        samples[line[idIdx]] = ([\" \".join(currentSample)],\n                                [int(c) for c in line[catIdx].split(\" \")])\n\n        return samples\n\n    except IOError as e:\n      print \"Could not open the file {}.\".format(netDataFile)\n      raise e", "response": "Returns samples joined at reset points."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the classifications at the indices where the data sequences reset.", "response": "def getClassifications(networkDataFile):\n    \"\"\"\n    Returns the classifications at the indices where the data sequences\n    reset.\n    @param networkDataFile  (str)     Path to file in the FileRecordStream\n                                      format\n    @return                 (list)    list of string versions of the\n                                      classifications\n    Sample output: [\"0 1\", \"1\", \"1 2 3\"]\n    \"\"\"\n    try:\n      with open(networkDataFile) as f:\n        reader = csv.reader(f)\n        next(reader, None)\n        next(reader, None)\n        specials = next(reader)\n        resetIdx = specials.index(\"R\")\n        classIdx = specials.index(\"C\")\n\n        classifications = []\n        for line in reader:\n          if int(line[resetIdx]) == 1:\n            classifications.append(line[classIdx])\n        return classifications\n\n    except IOError as e:\n      print \"Could not open the file {}.\".format(networkDataFile)\n      raise e"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of tokens for each sequence WorkItem", "response": "def getNumberOfTokens(networkDataFile):\n    \"\"\"\n    Returns the number of tokens for each sequence\n    @param networkDataFile  (str)     Path to file in the FileRecordStream\n                                      format\n    @return                 (list)    list of number of tokens\n    \"\"\"\n    try:\n      with open(networkDataFile) as f:\n        reader = csv.reader(f)\n        next(reader, None)\n        next(reader, None)\n        resetIdx = next(reader).index(\"R\")\n\n        count = 0\n        numTokens = []\n        for line in reader:\n          if int(line[resetIdx]) == 1:\n            if count != 0:\n              numTokens.append(count)\n            count = 1\n          else:\n            count += 1\n        numTokens.append(count)\n        return numTokens\n\n    except IOError as e:\n      print \"Could not open the file {}.\".format(networkDataFile)\n      raise e"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getResetsIndices(networkDataFile):\n    try:\n      with open(networkDataFile) as f:\n        reader = csv.reader(f)\n        next(reader, None)\n        next(reader, None)\n        resetIdx = next(reader).index(\"R\")\n\n        resets = []\n        for i, line in enumerate(reader):\n          if int(line[resetIdx]) == 1:\n            resets.append(i)\n        return resets\n\n    except IOError as e:\n      print \"Could not open the file {}.\".format(networkDataFile)\n      raise e", "response": "Returns the indices at which the data sequences reset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lastNoiseCurve(expPath, suite, iteration=\"last\"):\n  noiseValues = [\"0.0\", \"0.05\", \"0.1\", \"0.15\", \"0.2\", \"0.25\", \"0.3\",\n                            \"0.35\", \"0.4\", \"0.45\", \"0.5\"]\n  print(\"\\nNOISE CURVE =====\",expPath,\"====== ITERATION:\",iteration,\"=========\")\n  try:\n    result = suite.get_value(expPath, 0, noiseValues, iteration)\n    info = []\n    for k in noiseValues:\n      info.append([k,result[k][\"testerror\"]])\n    print(tabulate(info, headers=[\"noise\",\"Test Error\"], tablefmt=\"grid\"))\n    print(\"totalCorrect:\", suite.get_value(expPath, 0, \"totalCorrect\", iteration))\n  except:\n    print(\"Couldn't load experiment\",expPath)", "response": "Print the noise errors from the last iteration of this experiment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef learningCurve(expPath, suite):\n  print(\"\\nLEARNING CURVE ================\",expPath,\"=====================\")\n  try:\n    headers=[\"testResults\",\"validation\",\"bgResults\",\"elapsedTime\"]\n    result = suite.get_value(expPath, 0, headers, \"all\")\n    info = []\n    maxValidationAccuracy = -1.0\n    maxTestAccuracy = -1.0\n    maxBGAccuracy = -1.0\n    maxIter = -1\n    for i,v in enumerate(zip(result[\"testResults\"],result[\"validation\"],\n                             result[\"bgResults\"], result[\"elapsedTime\"])):\n      info.append([i, v[0][\"testerror\"], v[1][\"testerror\"], v[2][\"testerror\"], int(v[3])])\n      if v[1][\"testerror\"] > maxValidationAccuracy:\n        maxValidationAccuracy = v[1][\"testerror\"]\n        maxTestAccuracy = v[0][\"testerror\"]\n        maxBGAccuracy = v[2][\"testerror\"]\n        maxIter = i\n    headers.insert(0,\"iteration\")\n    print(tabulate(info, headers=headers, tablefmt=\"grid\"))\n\n    print(\"Max validation score =\", maxValidationAccuracy, \" at iteration\", maxIter)\n    print(\"Test score at that iteration =\", maxTestAccuracy)\n    print(\"BG score at that iteration =\", maxBGAccuracy)\n  except:\n    print(\"Couldn't load experiment\",expPath)", "response": "Print the test validation and other scores from each iteration of this\n experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef findOptimalResults(expName, suite, outFile):\n  writer = csv.writer(outFile)\n  headers = [\"testAccuracy\", \"bgAccuracy\", \"maxTotalAccuracy\", \"experiment path\"]\n  writer.writerow(headers)\n  info = []\n  print(\"\\n================\",expName,\"=====================\")\n  try:\n    # Retrieve the last totalCorrect from each experiment\n    # Print them sorted from best to worst\n    values, params = suite.get_values_fix_params(\n      expName, 0, \"testerror\", \"last\")\n    for p in params:\n      expPath = p[\"name\"]\n      if not \"results\" in expPath:\n        expPath = os.path.join(\"results\", expPath)\n      maxTestAccuracy, maxValidationAccuracy, maxBGAccuracy, maxIter, maxTotalAccuracy = bestScore(expPath, suite)\n      row = [maxTestAccuracy, maxBGAccuracy, maxTotalAccuracy, expPath]\n      info.append(row)\n      writer.writerow(row)\n\n    print(tabulate(info, headers=headers, tablefmt=\"grid\"))\n  except:\n    print(\"Couldn't analyze experiment\",expName)", "response": "Find the optimal results for each experiment and print them to the file outFile"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getErrorBars(expPath, suite):\n  exps = suite.get_exps(expPath)\n  testScores = np.zeros(len(exps))\n  noiseScores = np.zeros(len(exps))\n  for i,e in enumerate(exps):\n    maxTestAccuracy, maxValidationAccuracy, maxBGAccuracy, maxIter, maxTotalAccuracy = bestScore(\n      e, suite)\n    testScores[i] = maxTestAccuracy\n    noiseScores[i] = maxTotalAccuracy\n    print(e, maxTestAccuracy, maxTotalAccuracy)\n\n  print(\"\")\n  print(\"Experiment:\", expPath, \"Number of sub-experiments\", len(exps))\n  print(\"test score mean and standard deviation:\", testScores.mean(), testScores.std())\n  print(\"noise score mean and standard deviation:\", noiseScores.mean(), noiseScores.std())", "response": "Print out the error bars for each experiment in the path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the intersection and differences between two arrays comparing Elements by their corresponding key.", "response": "def setCompare(a, b,\n               aKey=None, bKey=None,\n               leftMinusRight=False, rightMinusLeft=False):\n  \"\"\"\n  Compute the intersection and differences between two arrays, comparing\n  elements by their key.\n\n  @param a (numpy array)\n  The left set to compare.\n\n  @param b (numpy array)\n  The right set to compare.\n\n  @param aKey (numpy array or None)\n  If specified, elements in \"a\" are compared by their corresponding entry in\n  \"aKey\".\n\n  @param bKey (numpy array or None)\n  If specified, elements in \"b\" are compared by their corresponding entry in\n  \"bKey\".\n\n  @param leftMinusRight\n  If True, also calculate the set difference (a - b)\n\n  @param rightMinusLeft\n  If True, also calculate the set difference (b - a)\n\n  @return (numpy array or tuple)\n  Always returns the intersection of \"a\" and \"b\". The elements of this\n  intersection are values from \"a\" (which may be different from the values of\n  \"b\" or \"aKey\").\n\n  If leftMinusRight or rightMinusLeft are True, it returns a tuple:\n  - intersection (numpy array)\n    See above\n  - leftMinusRight (numpy array)\n    The elements in a that are not in b\n  - rightMinusLeft (numpy array)\n    The elements in b that are not in a\n  \"\"\"\n  aKey = aKey if aKey is not None else a\n  bKey = bKey if bKey is not None else b\n\n  aWithinBMask = np.in1d(aKey, bKey)\n\n  if rightMinusLeft:\n    bWithinAMask = np.in1d(bKey, aKey)\n\n    if leftMinusRight:\n      return (a[aWithinBMask],\n              a[~aWithinBMask],\n              b[bWithinAMask])\n    else:\n      return (a[aWithinBMask],\n              b[~bWithinAMask])\n  elif leftMinusRight:\n    return (a[aWithinBMask],\n            a[~aWithinBMask])\n  else:\n    return a[aWithinBMask]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all the cells in the specified columns.", "response": "def getAllCellsInColumns(columns, cellsPerColumn):\n  \"\"\"\n  Calculate all cell indices in the specified columns.\n\n  @param columns (numpy array)\n  @param cellsPerColumn (int)\n\n  @return (numpy array)\n  All cells within the specified columns. The cells are in the same order as the\n  provided columns, so they're sorted if the columns are sorted.\n  \"\"\"\n\n  # Add\n  #   [[beginningOfColumn0],\n  #    [beginningOfColumn1],\n  #     ...]\n  # to\n  #   [0, 1, 2, ..., cellsPerColumn - 1]\n  # to get\n  #   [beginningOfColumn0 + 0, beginningOfColumn0 + 1, ...\n  #    beginningOfColumn1 + 0, ...\n  #    ...]\n  # then flatten it.\n  return ((columns * cellsPerColumn).reshape((-1, 1)) +\n          np.arange(cellsPerColumn, dtype=\"uint32\")).flatten()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef letterSequence(letters, w=40):\n  sequence = []\n  for letter in letters:\n    i = ord(letter) - ord('A')\n    sequence.append(set(range(i*w,(i+1)*w)))\n  return sequence", "response": "Return a list of input vectors corresponding to sequence of letters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getHighOrderSequenceChunk(it, switchover=1000, w=40, n=2048):\n  if it%10==3:\n    s = numpy.random.randint(5)\n    if it <= switchover:\n      if s==0:\n        label=\"XABCDE\"\n      elif s==1:\n        label=\"YCBEAF\"\n      elif s==2:\n        label=\"GHIJKL\"\n      elif s==3:\n        label=\"WABCMN\"\n      else:\n        label=\"ZDBCAE\"\n    else:\n      if s==0:\n        label=\"XCBEAF\"\n      elif s==1:\n        label=\"YABCDE\"\n      elif s==2:\n        label=\"GABCMN\"\n      elif s==3:\n        label=\"WHIJKL\"\n      else:\n        label=\"ZDHICF\"\n\n    vecs = letterSequence(label)\n  else:\n    vecs= [getRandomVector(w, n)]\n    label=\".\"\n\n  return vecs,label", "response": "Given an iteration index returns a list of vectors to be appended to the internal sequence stream and a string label identifying the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addNoise(vecs, percent=0.1, n=2048):\n  noisyVecs = []\n  for vec in vecs:\n    nv = vec.copy()\n    for idx in vec:\n      if numpy.random.random() <= percent:\n        nv.discard(idx)\n        nv.add(numpy.random.randint(n))\n    noisyVecs.append(nv)\n\n  return noisyVecs", "response": "Add noise to the given sequence of vectors and return the modified sequence."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef computePredictionAccuracy(pac, pic):\n  pcols = float(pac + pic)\n  if pcols == 0:\n    return 0.0\n  else:\n    return (pac / pcols)", "response": "Given a temporal memory instance return the prediction accuracy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a CSV file with detailed trace of prediction missed predictions and accuracy and segment growth.", "response": "def createReport(tm, options, sequenceString, numSegments, numSynapses):\n  \"\"\"\n  Create CSV file with detailed trace of predictions, missed predictions,\n  accuracy, segment/synapse growth, etc.\n  \"\"\"\n  pac = tm.mmGetTracePredictedActiveColumns()\n  pic = tm.mmGetTracePredictedInactiveColumns()\n  upac = tm.mmGetTraceUnpredictedActiveColumns()\n\n  resultsFilename = os.path.join(\"results\", options.name+\"_\"+str(int(100*options.noise))+\".csv\")\n\n  with open(resultsFilename,\"wb\") as resultsFile:\n    csvWriter = csv.writer(resultsFile)\n\n    accuracies = numpy.zeros(len(pac.data))\n    smoothedAccuracies = []\n    am = 0\n    csvWriter.writerow([\"time\", \"element\", \"pac\", \"pic\", \"upac\", \"a\",\n                        \"am\", \"accuracy\", \"sum\", \"nSegs\", \"nSyns\"])\n    for i,j in enumerate(pac.data):\n      if i>0:\n        # Compute instantaneous and average accuracy.\n        a = computePredictionAccuracy(len(j), len(pic.data[i]))\n\n        #  Smooth the curve to get averaged results for the paper.\n        am = 0.99*am + 0.01*a\n        accuracies[i] = am\n        i0 = max(0, i-60+1)\n        accuracy = numpy.mean(accuracies[i0:i+1])\n        smoothedAccuracies.append(accuracy)\n\n        row=[i, sequenceString[i], len(j), len(pic.data[i]),\n                len(upac.data[i]), a, am,\n                accuracy,\n                numpy.sum(accuracies[i0:i+1]),\n                numSegments[i], numSynapses[i]]\n        csvWriter.writerow(row)\n\n  return smoothedAccuracies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkilling the cells in the order of the given order.", "response": "def killCells(i, options, tm):\n  \"\"\"\n  Kill cells as appropriate\n  \"\"\"\n\n  # Kill cells if called for\n  if options.simulation == \"killer\":\n\n    if i == options.switchover:\n      print \"i=\",i,\"Killing cells for the first time!\"\n      tm.killCells(percent = options.noise)\n\n    if i == options.secondKill:\n      print \"i=\",i,\"Killing cells again up to\",options.secondNoise\n      tm.killCells(percent = options.secondNoise)\n\n  elif options.simulation == \"killingMeSoftly\" and (i%100 == 0):\n    steps = (options.secondKill - options.switchover)/100\n    nsteps = (options.secondNoise - options.noise)/steps\n    noise = options.noise + nsteps*(i-options.switchover)/100\n    if i in xrange(options.switchover, options.secondKill+1):\n      print \"i=\",i,\"Killing cells!\"\n      tm.killCells(percent = noise)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef printTemporalMemory(tm, outFile):\n  table = PrettyTable([\"Parameter name\", \"Value\", ])\n\n  table.add_row([\"columnDimensions\", tm.getColumnDimensions()])\n  table.add_row([\"cellsPerColumn\", tm.getCellsPerColumn()])\n  table.add_row([\"activationThreshold\", tm.getActivationThreshold()])\n  table.add_row([\"minThreshold\", tm.getMinThreshold()])\n  table.add_row([\"maxNewSynapseCount\", tm.getMaxNewSynapseCount()])\n  table.add_row([\"permanenceIncrement\", tm.getPermanenceIncrement()])\n  table.add_row([\"permanenceDecrement\", tm.getPermanenceDecrement()])\n  table.add_row([\"initialPermanence\", tm.getInitialPermanence()])\n  table.add_row([\"connectedPermanence\", tm.getConnectedPermanence()])\n  table.add_row([\"predictedSegmentDecrement\", tm.getPredictedSegmentDecrement()])\n\n  print >>outFile, table.get_string().encode(\"utf-8\")", "response": "Prints out the parameters of a TemporalMemory instance to outFile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef printOptions(options, tm, outFile):\n  print >>outFile, \"TM parameters:\"\n  printTemporalMemory(tm, outFile)\n  print >>outFile, \"Experiment parameters:\"\n  for k,v in options.__dict__.iteritems():\n    print >>outFile, \"  %s : %s\" % (k,str(v))\n  outFile.flush()", "response": "Pretty print the set of options"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef runBasic(noiseLevel=None, profile=False):\n  exp = L4L2Experiment(\n    \"basic_continuous\",\n    numCorticalColumns=2\n  )\n\n  objects = createObjectMachine(\n    machineType=\"continuous\",\n    numInputBits=21,\n    sensorInputSize=1024,\n    externalInputSize=1024,\n    numCorticalColumns=2,\n  )\n\n  objects.addObject(Sphere(radius=20), name=\"sphere\")\n  objects.addObject(Cylinder(height=50, radius=20), name=\"cylinder\")\n  objects.addObject(Box(dimensions=[10, 20, 30,]), name=\"box\")\n  objects.addObject(Cube(width=20), name=\"cube\")\n\n  learnConfig = {\n    \"sphere\": [(\"surface\", 10)],\n    # the two learning config below will be exactly the same\n    \"box\": [(\"face\", 5), (\"edge\", 5), (\"vertex\", 5)],\n    \"cube\": [(feature, 5) for feature in objects[\"cube\"].getFeatures()],\n    \"cylinder\": [(feature, 5) for feature in objects[\"cylinder\"].getFeatures()]\n  }\n\n  exp.learnObjects(\n    objects.provideObjectsToLearn(learnConfig, plot=True),\n    reset=True\n  )\n  if profile:\n    exp.printProfile()\n\n  inferConfig = {\n    \"numSteps\": 4,\n    \"noiseLevel\": noiseLevel,\n    \"objectName\": \"cube\",\n    \"pairs\": {\n      0: [\"face\", \"face\", \"edge\", \"edge\"],\n      1: [\"edge\", \"face\", \"face\", \"edge\"]\n    }\n  }\n\n  exp.infer(\n    objects.provideObjectToInfer(inferConfig, plot=True),\n    objectName=\"cube\",\n    reset=True\n  )\n  if profile:\n    exp.printProfile()\n\n  exp.plotInferenceStats(\n    fields=[\"L2 Representation\",\n            \"Overlap L2 with object\",\n            \"L4 Representation\"],\n  )", "response": "Runs a basic experiment on continuous locations and inferring one of them."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot receptive fields for 16 randomly selected columns Plot 2D receptive fields for 16 randomly selected columns \u00b0C", "response": "def plotReceptiveFields(sp, nDim1=8, nDim2=8):\n  \"\"\"\n  Plot 2D receptive fields for 16 randomly selected columns\n  :param sp:\n  :return:\n  \"\"\"\n  columnNumber = np.product(sp.getColumnDimensions())\n  fig, ax = plt.subplots(nrows=4, ncols=4)\n  for rowI in range(4):\n    for colI in range(4):\n      col = np.random.randint(columnNumber)\n      connectedSynapses = np.zeros((nDim1*nDim2,), dtype=uintType)\n      sp.getConnectedSynapses(col, connectedSynapses)\n      receptiveField = connectedSynapses.reshape((nDim1, nDim2))\n      ax[rowI, colI].imshow(receptiveField, cmap='gray')\n      ax[rowI, colI].set_title(\"col: {}\".format(col))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot boostfactor for a selected column in a single column.", "response": "def plotBoostTrace(sp, inputVectors, columnIndex):\n  \"\"\"\n  Plot boostfactor for a selected column\n\n  Note that learning is ON for SP here\n\n  :param sp: sp instance\n  :param inputVectors: input data\n  :param columnIndex: index for the column of interest\n  \"\"\"\n  numInputVector, inputSize = inputVectors.shape\n  columnNumber = np.prod(sp.getColumnDimensions())\n  boostFactorsTrace = np.zeros((columnNumber, numInputVector))\n  activeDutyCycleTrace = np.zeros((columnNumber, numInputVector))\n  minActiveDutyCycleTrace = np.zeros((columnNumber, numInputVector))\n  for i in range(numInputVector):\n    outputColumns = np.zeros(sp.getColumnDimensions(), dtype=uintType)\n    inputVector = copy.deepcopy(inputVectors[i][:])\n    sp.compute(inputVector, True, outputColumns)\n\n    boostFactors = np.zeros((columnNumber, ), dtype=realDType)\n    sp.getBoostFactors(boostFactors)\n    boostFactorsTrace[:, i] = boostFactors\n\n    activeDutyCycle = np.zeros((columnNumber, ), dtype=realDType)\n    sp.getActiveDutyCycles(activeDutyCycle)\n    activeDutyCycleTrace[:, i] = activeDutyCycle\n\n    minActiveDutyCycle = np.zeros((columnNumber, ), dtype=realDType)\n    sp.getMinActiveDutyCycles(minActiveDutyCycle)\n    minActiveDutyCycleTrace[:, i] = minActiveDutyCycle\n\n  plt.figure()\n  plt.subplot(2, 1, 1)\n  plt.plot(boostFactorsTrace[columnIndex, :])\n  plt.ylabel('Boost Factor')\n  plt.subplot(2, 1, 2)\n  plt.plot(activeDutyCycleTrace[columnIndex, :])\n  plt.plot(minActiveDutyCycleTrace[columnIndex, :])\n  plt.xlabel(' Time ')\n  plt.ylabel('Active Duty Cycle')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering results containing the given condition", "response": "def filterResults(results, filter):\n  \"\"\"\n  Filter results containing the given condition\n  :param results: list of experiments returned by `suite.get_exps`\n  :param filter: list of conditions on the experiment parameters. For example:\n                 [\"dropout0.0\", \"dropout0.50\"]\n  :return: filtered results\n  \"\"\"\n  return [exp for exp in results if any(map(lambda v: v in exp, filter))]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the results anc configure the Linear Noise curve Plot", "response": "def configureNoisePlot(suite, experiments, labels, linestyles,\n                       marker, filter=None):\n  \"\"\"\n  Load experiment results anc configure the \"Noise curve\" Plot\n  :param suite: The configured experiment suite. Must call `parse_opt` and `\n                parse_cfg` before calling this functions\n  :param experiments: list containing the experiments to load\n  :param experiments: list containing the experiments to load\n  :param labels: list containing the plot labels for each experiment\n  :param linestyles: list containing the plot linestyle for each experiment\n  :param filter: list containing the specific parameters to filter\n  :return: tuple containing the experiment results and plot formats to be passed\n           to `plotNoiseCurve` function\n  \"\"\"\n  formats = dict()\n  results = []\n  for i in xrange(len(experiments)):\n    path = suite.get_exp(experiments[i])[0]\n    data = suite.get_exps(path=path)\n    if filter is not None:\n      data = filterResults(data, filter)\n\n    # Format Linear Noise curve\n    if len(data) > 1:\n      format = {exp: {\n        \"label\": \"{},{}\".format(labels[i], basename(exp)),\n        \"linestyle\": \"{}\".format(linestyles[i]),\n        \"marker\": \"{}\".format(marker[i])\n      } for exp in data}\n    else:\n      format = {data[0]: {\n        \"label\": \"{}\".format(labels[i]),\n        \"linestyle\": \"{}\".format(linestyles[i]),\n        \"marker\": \"{}\".format(marker[i])\n      }}\n\n    formats.update(format)\n    results.extend(data)\n\n  return (results, formats)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the experiment results anc configure the Dropout By Total Correct plot", "response": "def configureDropoutByTotalCorrectPlot(suite, experiments, labels, linestyles):\n  \"\"\"\n  Load experiment results anc configure the \"Dropout By Total Correct\" Plot\n  :param suite: The configured experiment suite. Must call `parse_opt` and `\n                parse_cfg` before calling this functions\n  :param experiments: list containing the experiments to load\n  :param labels: list containing the plot labels for each experiment\n  :param linestyles: list containing the plot linestyle for each experiment\n  :return: tuple containing the experiment results and plot formats to be passed\n           to `plotDropoutByTotalCorrect` function\n  \"\"\"\n  results = defaultdict(dict)\n  formats = dict()\n  for i in xrange(len(experiments)):\n    experiment = experiments[i]\n    formats[experiment] = {\"label\": labels[i], \"linestyle\": linestyles[i]}\n\n    path = suite.get_exp(experiment)[0]\n    data = suite.get_exps(path=path)\n    for exp in data:\n      dropout = suite.get_params(exp)[\"dropout\"]\n      totalCorrect = suite.get_value(exp, 0, \"totalCorrect\", \"last\")\n      results[experiment][dropout] = totalCorrect\n\n  return (results, formats)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot Sample MNIST images with noise.", "response": "def plotImagesWithNoise(suite, noise_values, plotPath):\n  \"\"\"\n  Plot Sample MNIST images with noise\n  :param suite: The configured experiment suite. Must call `parse_opt` and `\n                parse_cfg` before calling this functions\n  :param noise: list of noise values to plot\n  \"\"\"\n  datadir = suite.cfgparser.defaults()[\"datadir\"]\n\n  transform = transforms.Compose([transforms.ToTensor(),\n                                  transforms.Normalize((0.1307,), (0.3081,))])\n  dataset = datasets.MNIST(datadir, train=False, download=True,\n                           transform=transform)\n\n  num_noise = len(noise_values)\n  fig = plt.figure(figsize=(num_noise, 4))\n  for y in range(4):\n    for x in range(num_noise):\n      transform.transforms.append(\n        RandomNoise(noise_values[x], whiteValue=0.1307 + 2 * 0.3081))\n      img, _ = dataset[y]\n      transform.transforms.pop()\n\n      ax = fig.add_subplot(4, num_noise, y * num_noise + x + 1)\n      ax.set_axis_off()\n      ax.imshow(img.numpy().reshape((28, 28)), cmap='gray')\n      if y == 0:\n        ax.set_title(\"{0}%\".format(noise_values[x] * 100))\n\n  plt.tight_layout()\n  plt.savefig(plotPath)\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a set of random SDRs with varying sparsity.", "response": "def generateRandomSDRVaryingSparsity(numSDR, numDims, minSparsity, maxSparsity,\n                                     seed=42):\n  \"\"\"\n  Generate a set of random SDRs with varying sparsity\n  :param numSDR: number of SDRs\n  :param numDims: length of SDRs\n  :param minSparsity: minimum sparsity\n  :param maxSparsity: maximum sparsity\n  :param seed:\n  \"\"\"\n  randomSDRs = np.zeros((numSDR, numDims), dtype=uintType)\n  indices = np.array(range(numDims))\n  np.random.seed(seed)\n  for i in range(numSDR):\n    sparsity = np.random.random() * (maxSparsity - minSparsity) + minSparsity\n    numActiveInputBits = int(sparsity * numDims)\n    randomIndices = np.random.permutation(indices)\n    activeBits = randomIndices[:numActiveInputBits]\n    randomSDRs[i, activeBits] = 1\n  return randomSDRs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the plot of the cell activity.", "response": "def mmGetCellActivityPlot(self, title=\"\", showReset=False,\n                            resetShading=0.25, activityType=\"activeCells\"):\n    \"\"\"\n    Returns plot of the cell activity.\n\n    @param title        (string)  an optional title for the figure\n\n    @param showReset    (bool)    if true, the first set of cell activities\n                                  after a reset will have a gray background\n\n    @param resetShading (float)   if showReset is true, this float specifies the\n                                  intensity of the reset background with 0.0\n                                  being white and 1.0 being black\n\n    @param activityType (string)  The type of cell activity to display. Valid\n                                  types include \"activeCells\"\n\n    @return (Plot) plot\n    \"\"\"\n\n    cellTrace = copy.deepcopy(self._mmTraces[activityType].data)\n    for i in xrange(len(cellTrace)):\n      cellTrace[i] = self.getCellIndices(cellTrace[i])\n\n    return self.mmGetCellTracePlot(cellTrace, self.numberOfCells(),\n                                   activityType, title, showReset,\n                                   resetShading)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload next epoch from disk and add it to the data list", "response": "def next_epoch(self):\n    \"\"\"\n    Load next epoch from disk\n    \"\"\"\n    epoch = next(self._all_epochs)\n    folder = os.path.join(self._root, str(epoch), self._subset)\n    self.data = []\n    silence = None\n\n    gc.disable()\n\n    for filename in os.listdir(folder):\n      command = os.path.splitext(os.path.basename(filename))[0]\n      with open(os.path.join(folder, filename), \"r\") as pkl_file:\n        audio = pickle.load(pkl_file)\n\n      # Check for 'silence'\n      if command == \"silence\":\n        silence = audio\n      else:\n        target = self.classes.index(os.path.basename(command))\n        self.data.extend(itertools.product(audio, [target]))\n\n    gc.enable()\n\n    target = self.classes.index(\"silence\")\n    self.data += [(silence, target)] * int(len(self.data) * self._silence_percentage)\n    return epoch"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking weights for balanced classes", "response": "def make_weights_for_balanced_classes(self):\n    \"\"\"adopted from https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\"\"\"\n\n    nclasses = len(self.classes)\n    count = np.ones(nclasses)\n    for item in self.data:\n      count[item[1]] += 1\n\n    N = float(sum(count))\n    weight_per_class = N / count\n    weight = np.zeros(len(self.data))\n    for idx, item in enumerate(self.data):\n      weight[idx] = weight_per_class[item[1]]\n    return weight"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the given folder is a valid preprocessed dataset.", "response": "def isValid(folder, epoch=0):\n    \"\"\"\n    Check if the given folder is a valid preprocessed dataset\n    \"\"\"\n    # Validate by checking for the training 'silence.pkl' on the given epoch\n    # This file is unique to our pre-processed dataset generated by 'process_dataset.py'\n    return os.path.exists(os.path.join(folder, str(epoch), \"train\", \"silence.pkl\"))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nkill cells in the current language.", "response": "def killCells(self, percent = 0.05):\n    \"\"\"\n    Changes the percentage of cells that are now considered dead. The first\n    time you call this method a permutation list is set up. Calls change the\n    number of cells considered dead.\n    \"\"\"\n\n    if self.zombiePermutation is None:\n      self.zombiePermutation = numpy.random.permutation(self.numberOfCells())\n\n    self.numDead = int(round(percent * self.numberOfCells()))\n    if self.numDead > 0:\n      self.deadCells = set(self.zombiePermutation[0:self.numDead])\n    else:\n      self.deadCells = set()\n\n    print \"Total number of dead cells=\",len(self.deadCells)\n\n    numSegmentDeleted = 0\n\n    for cellIdx in self.deadCells:\n      # Destroy segments.  self.destroySegment() takes care of deleting synapses\n      for segment in self.connections.segmentsForCell(cellIdx):\n        self.connections.destroySegment(segment)\n        numSegmentDeleted += 1\n\n    print \"Total number of segments removed=\", numSegmentDeleted\n\n    # Strip out segments for dead cells\n    self.activeSegments = [s for s in self.activeSegments if\n                           s.cell not in self.deadCells]\n    self.matchingSegments = [s for s in self.matchingSegments if\n                             s.cell not in self.deadCells]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef burstColumn(self, column, columnMatchingSegments, prevActiveCells,\n                  prevWinnerCells, learn):\n    \"\"\"\n    Activates all of the cells in an unpredicted active column, chooses a winner\n    cell, and, if learning is turned on, learns on one segment, growing a new\n    segment if necessary.\n\n    @param column (int)\n    Index of bursting column.\n\n    @param columnMatchingSegments (iter)\n    Matching segments in this column, or None if there aren't any.\n\n    @param prevActiveCells (list)\n    Active cells in `t-1`.\n\n    @param prevWinnerCells (list)\n    Winner cells in `t-1`.\n\n    @param learn (bool)\n    Whether or not learning is enabled.\n\n    @return (tuple) Contains:\n                      `cells`         (iter),\n                      `winnerCell`    (int),\n    \"\"\"\n\n    start = self.cellsPerColumn * column\n\n    # Strip out destroyed cells before passing along to base _burstColumn()\n    cellsForColumn = [cellIdx\n                      for cellIdx\n                      in xrange(start, start + self.cellsPerColumn)\n                      if cellIdx not in self.deadCells]\n\n    return self._burstColumn(\n      self.connections, self._random, self.lastUsedIterationForSegment, column,\n      columnMatchingSegments, prevActiveCells, prevWinnerCells, cellsForColumn,\n      self.numActivePotentialSynapsesForSegment, self.iteration,\n      self.maxNewSynapseCount, self.initialPermanence, self.permanenceIncrement,\n      self.permanenceDecrement, self.maxSegmentsPerCell,\n      self.maxSynapsesPerSegment, learn)", "response": "Bursts a column of cells into a new segment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint statistics for the dead cells", "response": "def printDeadCells(self):\n    \"\"\"\n    Print statistics for the dead cells\n    \"\"\"\n    columnCasualties = numpy.zeros(self.numberOfColumns())\n    for cell in self.deadCells:\n      col = self.columnForCell(cell)\n      columnCasualties[col] += 1\n    for col in range(self.numberOfColumns()):\n      print col, columnCasualties[col]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset(self, params, repetition):\n    self.startTime = time.time()\n    print(params)\n    seed = params[\"seed\"] + repetition\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # Get our directories correct\n    self.dataDir = params[\"datadir\"]\n    if params.get(\"create_plots\", False):\n      self.resultsDir = os.path.join(params[\"path\"], params[\"name\"], \"plots\")\n\n      if not os.path.exists(self.resultsDir):\n        os.makedirs(self.resultsDir)\n\n    self.use_cuda = not params[\"no_cuda\"] and torch.cuda.is_available()\n    self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\n    transform = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Normalize((0.1307,), (0.3081,))])\n\n    train_dataset = datasets.MNIST(self.dataDir, train=True, download=True,\n                                   transform=transform)\n\n    # Create training and validation sampler from MNIST dataset by training on\n    # random X% of the training set and validating on the remaining (1-X)%,\n    # where X can be tuned via the \"validation\" parameter\n    validation = params.get(\"validation\", 50000.0 / 60000.0)\n    if validation < 1.0:\n      self.train_sampler, self.validation_sampler = createValidationDataSampler(\n        train_dataset, validation)\n\n      self.train_loader = torch.utils.data.DataLoader(train_dataset,\n                                                      batch_size=params[\"batch_size\"],\n                                                      sampler=self.train_sampler)\n\n      self.validation_loader = torch.utils.data.DataLoader(train_dataset,\n                                                           batch_size=params[\"batch_size\"],\n                                                           sampler=self.validation_sampler)\n    else:\n      # No validation. Normal training dataset\n      self.validation_loader = None\n      self.validation_sampler = None\n      self.train_sampler = None\n      self.train_loader = torch.utils.data.DataLoader(train_dataset,\n                                                      batch_size=params[\"batch_size\"],\n                                                      shuffle=True)\n\n\n    self.test_loader = torch.utils.data.DataLoader(\n      datasets.MNIST(self.dataDir, train=False, transform=transform),\n      batch_size=params[\"test_batch_size\"], shuffle=True)\n\n    # Parse 'n' and 'k' parameters\n    n = params[\"n\"]\n    k = params[\"k\"]\n    if isinstance(n, basestring):\n      n = map(int, n.split(\"_\"))\n    if isinstance(k, basestring):\n      k = map(int, k.split(\"_\"))\n\n    if params[\"use_cnn\"]:\n      c1_out_channels = params[\"c1_out_channels\"]\n      c1_k = params[\"c1_k\"]\n      if isinstance(c1_out_channels, basestring):\n        c1_out_channels = map(int, c1_out_channels.split(\"_\"))\n      if isinstance(c1_k, basestring):\n        c1_k = map(int, c1_k.split(\"_\"))\n\n      # Parse 'c1_input_shape; parameter\n      if \"c1_input_shape\" in params:\n        c1_input_shape = map(int, params[\"c1_input_shape\"].split(\"_\"))\n      else:\n        c1_input_shape = (1, 28, 28)\n\n      sp_model = SparseNet(\n        inputSize=c1_input_shape,\n        outChannels=c1_out_channels,\n        c_k=c1_k,\n        kernelSize=5,\n        stride=1,\n        dropout=params[\"dropout\"],\n        n=n,\n        k=k,\n        outputSize=10,\n        boostStrength=params[\"boost_strength\"],\n        weightSparsity=params[\"weight_sparsity\"],\n        weightSparsityCNN=params[\"weight_sparsity_cnn\"],\n        boostStrengthFactor=params[\"boost_strength_factor\"],\n        kInferenceFactor=params[\"k_inference_factor\"],\n        useBatchNorm=params[\"use_batch_norm\"],\n        normalizeWeights=params.get(\"normalize_weights\", False)\n      )\n    else:\n      sp_model = SparseNet(\n        n=n,\n        k=k,\n        outputSize=10,\n        boostStrength=params[\"boost_strength\"],\n        weightSparsity=params[\"weight_sparsity\"],\n        weightSparsityCNN=params[\"weight_sparsity_cnn\"],\n        boostStrengthFactor=params[\"boost_strength_factor\"],\n        kInferenceFactor=params[\"k_inference_factor\"],\n        dropout=params[\"dropout\"],\n        useBatchNorm=params[\"use_batch_norm\"],\n        normalizeWeights=params.get(\"normalize_weights\", False)\n      )\n    if torch.cuda.device_count() > 1:\n      print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n      sp_model = torch.nn.DataParallel(sp_model)\n\n    self.model = sp_model.to(self.device)\n    self.learningRate = params[\"learning_rate\"]\n    self.optimizer = self.createOptimizer(params, self.model)\n    self.lr_scheduler = self.createLearningRateScheduler(params, self.optimizer)", "response": "Reset the object to the initial state of the experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iterate(self, params, repetition, iteration):\n    try:\n      print(\"\\nStarting iteration\",iteration)\n      t1 = time.time()\n      ret = {}\n\n      # Update learning rate using learning rate scheduler if configured\n      if self.lr_scheduler is not None:\n        # ReduceLROnPlateau lr_scheduler step should be called after validation,\n        # all other lr_schedulers should be called before training\n        if params[\"lr_scheduler\"] != \"ReduceLROnPlateau\":\n          self.lr_scheduler.step()\n\n      self.train(params, epoch=iteration)\n\n      # Run validation test\n      if self.validation_loader is not None:\n        validation = self.test(params, self.validation_loader)\n\n        # ReduceLROnPlateau step should be called after validation\n        if params[\"lr_scheduler\"] == \"ReduceLROnPlateau\":\n          self.lr_scheduler.step(validation[\"test_loss\"])\n\n        ret[\"validation\"] = validation\n        print(\"Validation: Test error=\", validation[\"testerror\"],\n              \"entropy=\", validation[\"entropy\"])\n\n      # Run noise test\n      if (params[\"test_noise_every_epoch\"] or\n          iteration == params[\"iterations\"] - 1):\n        ret.update(self.runNoiseTests(params))\n        print(\"Noise test results: totalCorrect=\", ret[\"totalCorrect\"],\n              \"Test error=\", ret[\"testerror\"], \", entropy=\", ret[\"entropy\"])\n        if ret[\"totalCorrect\"] > 100000 and ret[\"testerror\"] > 98.3:\n          print(\"*******\")\n          print(params)\n\n      ret.update({\"elapsedTime\": time.time() - self.startTime})\n      ret.update({\"learningRate\": self.learningRate if self.lr_scheduler is None\n                                                    else self.lr_scheduler.get_lr()})\n\n      print(\"Iteration time= {0:.3f} secs, \"\n            \"total elapsed time= {1:.3f} mins\".format(\n              time.time() - t1,ret[\"elapsedTime\"]/60.0))\n\n    except Exception as e:\n      # Tracebacks are not printed if using multiprocessing so we do it here\n      tb = sys.exc_info()[2]\n      traceback.print_tb(tb)\n      raise RuntimeError(\"Something went wrong in iterate\", e)\n\n    return ret", "response": "This function is called every time a training iteration is performed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntraining one epoch of this model by iterating through mini batches and applying the model s duty cycle to each mini - batch.", "response": "def train(self, params, epoch):\n    \"\"\"\n    Train one epoch of this model by iterating through mini batches. An epoch\n    ends after one pass through the training set, or if the number of mini\n    batches exceeds the parameter \"batches_in_epoch\".\n    \"\"\"\n\n    # Callback used to log information on every batch\n    def log(model, batch_idx):\n      if batch_idx % params[\"log_interval\"] == 0:\n        entropy = model.entropy()\n        print(\"logging: {} learning iterations, entropy: {} / {}\".format(\n          model.getLearningIterations(), float(entropy), model.maxEntropy()))\n\n        if params[\"create_plots\"]:\n          plotDutyCycles(model.dutyCycle,\n                         self.resultsDir + \"/figure_\" + str(epoch) + \"_\" +\n                         str(model.getLearningIterations()))\n\n\n    # Adjust first epoch batch size to stabilize the dutycycles at the\n    # beginning of the training\n    loader = self.train_loader\n    batches_in_epoch = params[\"batches_in_epoch\"]\n    if \"first_epoch_batch_size\" in params:\n      if epoch == 0:\n        batches_in_epoch = params.get(\"batches_in_first_epoch\", batches_in_epoch)\n        loader = torch.utils.data.DataLoader(self.train_loader.dataset,\n                                             batch_size=params[\"first_epoch_batch_size\"],\n                                             sampler=self.train_loader.sampler)\n\n    trainModel(model=self.model, loader=loader,\n               optimizer=self.optimizer, device=self.device,\n               batches_in_epoch=batches_in_epoch,\n               batch_callback=log)\n\n    self.model.postEpoch()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting Union Pooler and clear active cell history", "response": "def reset(self):\n    \"\"\"\n    Reset Union Pooler, clear active cell history\n    \"\"\"\n    self._unionSDR = numpy.zeros(shape=(self._numInputs,))\n    self._activeCellsHistory = []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef updateHistory(self, activeCells, forceOutput=False):\n    self._activeCellsHistory.append(activeCells)\n    if len(self._activeCellsHistory) > self._historyLength:\n      self._activeCellsHistory.pop(0)\n\n    self._unionSDR = numpy.zeros(shape=(self._numInputs,))\n    if (len(self._activeCellsHistory) >= self._minHistory) or forceOutput:\n      for i in self._activeCellsHistory:\n        self._unionSDR[i] = 1\n\n    return self._unionSDR", "response": "Update the union pooler algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unionIntoArray(self, inputVector, outputVector, forceOutput=False):\n    if isinstance(inputVector, numpy.ndarray):\n      if inputVector.size == self._numInputs:\n        activeBits = numpy.where(inputVector)[0]\n      else:\n        raise ValueError(\n          \"Input vector dimensions don't match. Expecting %s but got %s\" % (\n            self._numInputs, inputVector.size))\n    elif isinstance(inputVector, list):\n      if len(inputVector) > 0:\n        if max(inputVector) >= self._numInputs:\n          raise ValueError(\n            \"Non-zero entry indices exceed input dimension of union pooler. \"\n            \"Expecting %s but got %s\" % (self._numInputs, max(inputVector)))\n      activeBits = inputVector\n    else:\n      raise TypeError(\"Unsuported input types\")\n\n    if len(outputVector) != self._numInputs:\n      raise ValueError(\n        \"Output vector dimension does match dimension of union pooler \"\n        \"Expecting %s but got %s\" % (self._numInputs, len(outputVector)))\n\n    unionSDR = self.updateHistory(activeBits, forceOutput)\n\n    numpy.copyto(outputVector, unionSDR, casting=\"unsafe\")", "response": "Create a union of the inputVector and copy the result into the outputVector."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the sparsity of the current union SDR", "response": "def getSparsity(self):\n    \"\"\"\n    Return the sparsity of the current union SDR\n    \"\"\"\n    sparsity = numpy.sum(self._unionSDR) / self._numInputs\n    return sparsity"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots a Panda dataframe.", "response": "def plotDataframe(table, title, plotPath):\n  \"\"\"\n  Plot Panda dataframe.\n\n  :param table: Panda dataframe returned by :func:`analyzeWeightPruning`\n  :type table: :class:`pandas.DataFrame`\n  :param title: Plot title\n  :type title: str\n  :param plotPath: Plot full path\n  :type plotPath: str\n\n  \"\"\"\n  plt.figure()\n  axes = table.T.plot(subplots=True, sharex=True, grid=True, legend=True,\n                      title=title, figsize=(8, 11))\n\n  # Use fixed scale for \"accuracy\"\n  accuracy = next(ax for ax in axes if ax.lines[0].get_label() == 'accuracy')\n  accuracy.set_ylim(0.0, 1.0)\n\n  plt.savefig(plotPath)\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun analyzeWeightPruning in parallel and saves the results as a panda dataframe.", "response": "def run(pool, expName, name, args):\n  \"\"\"\n  Runs :func:`analyzeWeightPruning` in parallel and save the results\n\n  :param pool: multiprocessing pool\n  :param expName: Experiment name\n  :param name: File/Plot name (i.e. 'weight_prunning')\n  :param args: Argument list to be passed to :func:`analyzeWeightPruning`\n  :return: panda dataframe with all the results\n\n  \"\"\"\n  tables = pool.map(analyzeWeightPruning, args)\n  merged = pd.concat(tables, axis=1).sort_index(axis=1)\n  filename = \"{}_{}\".format(name, expName)\n  plotDataframe(merged, filename, \"{}.pdf\".format(filename))\n  print()\n  print(filename)\n  print(tabulate(merged, headers='keys', tablefmt='fancy_grid',\n                 numalign=\"right\"))\n  merged.to_csv(\"{}.csv\".format(filename))\n  return merged"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving all directories that are not in categories list.", "response": "def remove_unused_directories(src_folder, categories):\n  \"\"\"\n  list all directories.\n  Remove all that are not in categories list (excluding background noise)\n  :param src_folder:\n  :param categories:\n  :return:\n  \"\"\"\n  files = os.listdir(src_folder)\n  os.mkdir(\"unused\")\n  for name in files:\n    full_path = os.path.join(src_folder, name)\n    if os.path.isdir(full_path):\n      if not( name in categories or name == \"_background_noise_\"):\n        newPath = os.path.join(\"unused\", name)\n        print(\"moving: \", full_path, \"-->\", newPath)\n        shutil.move(full_path, newPath)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalizeSparseWeights(m):\n  if isinstance(m, SparseWeightsBase):\n    _, inputSize = m.module.weight.shape\n    fan = int(inputSize * m.weightSparsity)\n    gain = nn.init.calculate_gain('leaky_relu', math.sqrt(5))\n    std = gain / np.math.sqrt(fan)\n    bound = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n    nn.init.uniform_(m.module.weight, -bound, bound)\n    if m.module.bias is not None:\n      bound = 1 / math.sqrt(fan)\n      nn.init.uniform_(m.module.bias, -bound, bound)", "response": "Normalizes the weights of a SparseWeightsBase object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting datetime as x - axis", "response": "def getDatetimeAxis():\n  \"\"\"\n  use datetime as x-axis\n  \"\"\"\n  dataSet = 'nyc_taxi'\n  filePath = './data/' + dataSet + '.csv'\n  data = pd.read_csv(filePath, header=0, skiprows=[1, 2],\n                     names=['datetime', 'value', 'timeofday', 'dayofweek'])\n\n  xaxisDate = pd.to_datetime(data['datetime'])\n  return xaxisDate"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute(self, xt1, yt1, xt, yt, theta1t1, theta2t1, theta1, theta2, learn):\n    dx = xt - xt1\n    dy = yt - yt1\n\n    self.minDx = min(self.minDx, dx)\n    self.maxDx = max(self.maxDx, dx)\n\n    print >>sys.stderr, \"Learn: \", learn\n    print >>sys.stderr, \"Training iterations: \", self.trainingIterations\n    print >>sys.stderr, \"Test iterations: \", self.testIterations\n    print >>sys.stderr, \"Xt's: \", xt1, yt1, xt, yt, \"Delta's: \", dx, dy\n    print >>sys.stderr, \"Theta t-1: \", theta1t1, theta2t1, \"t:\",theta1, theta2\n\n    bottomUpSDR = self.encodeThetas(theta1, theta2)\n    self.decodeThetas(bottomUpSDR)\n\n    # Encode the inputs appropriately and train the HTM\n    externalSDR = self.encodeDeltas(dx,dy)\n\n    if learn:\n      # During learning we provide the current pose angle as bottom up input\n      bottomUpSDR = self.encodeThetas(theta1, theta2)\n      self.trainTM(bottomUpSDR, externalSDR)\n      self.trainingIterations += 1\n    else:\n      # During inference we provide the previous pose angle as bottom up input\n      # If we don't get a prediction, we keep trying random shifts until we get\n      # something.\n      predictedCells = []\n      newt1 = theta1t1\n      newt2 = theta2t1\n      newdx = dx\n      newdy = dy\n      angleRange = 10\n      numAttempts = 1\n      while len(predictedCells) == 0 and numAttempts < 3:\n        print >> sys.stderr, \"Attempt:\", numAttempts,\n        print >> sys.stderr, \"Trying to predict using thetas:\", newt1, newt2,\n        print >> sys.stderr, \"and deltas:\", newdx, newdy\n\n        externalSDR = self.encodeDeltas(newdx, newdy)\n        bottomUpSDR = self.encodeThetas(newt1, newt2)\n        predictedCells = self.inferTM(bottomUpSDR, externalSDR)\n        predictedValues = self.decodeThetas(predictedCells)\n\n        print >> sys.stderr, \"Predicted values\",predictedValues\n\n        newt1 = theta1t1 + random.randrange(-angleRange,angleRange)\n        newt2 = theta2t1 + random.randrange(-angleRange,angleRange)\n        newdx = dx + (random.random()/2.0 - 0.25)\n        newdy = dy + (random.random()/2.0 - 0.25)\n\n        # Ensure we are in bounds otherwise we get an exception\n        newt1 = min(self.maxTheta1, max(self.minTheta1, newt1))\n        newt2 = min(self.maxTheta2, max(self.minTheta2, newt2))\n        newdx = min(2.0, max(-2.0, newdx))\n        newdy = min(2.0, max(-2.0, newdy))\n\n        numAttempts += 1\n        if numAttempts % 10 == 0: angleRange += 2\n\n      print predictedValues\n      # Accumulate errors for our metrics\n      if len(predictedCells) == 0:\n        self.numMissedPredictions += 1\n      self.testIterations += 1\n      error = abs(predictedValues[0] - theta1) + abs(predictedValues[1] - theta2)\n      self.totalPredictionError += error\n      if self.maxPredictionError < error:\n        self.maxPredictionError = error\n        print >> sys.stderr, \"Error: \", error\n\n\n    print >> sys.stderr", "response": "Compute the next set of cells from the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encodeDeltas(self, dx,dy):\n    dxe = self.dxEncoder.encode(dx)\n    dye = self.dyEncoder.encode(dy)\n    ex = numpy.outer(dxe,dye)\n    return ex.flatten().nonzero()[0]", "response": "Return the SDR for dx and dy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encodeThetas(self, theta1, theta2):\n    # print >> sys.stderr, \"encoded theta1 value = \", theta1\n    # print >> sys.stderr, \"encoded theta2 value = \", theta2\n    t1e = self.theta1Encoder.encode(theta1)\n    t2e = self.theta2Encoder.encode(theta2)\n    # print >> sys.stderr, \"encoded theta1 = \", t1e.nonzero()[0]\n    # print >> sys.stderr, \"encoded theta2 = \", t2e.nonzero()[0]\n    ex = numpy.outer(t2e,t1e)\n    return ex.flatten().nonzero()[0]", "response": "Return the SDR for theta1 and theta2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving the set of predicted cells return the predicted theta1 and theta2", "response": "def decodeThetas(self, predictedCells):\n    \"\"\"\n    Given the set of predicted cells, return the predicted theta1 and theta2\n    \"\"\"\n    a = numpy.zeros(self.bottomUpInputSize)\n    a[predictedCells] = 1\n    a = a.reshape((self.theta1Encoder.getWidth(), self.theta1Encoder.getWidth()))\n    theta1PredictedBits = a.mean(axis=0).nonzero()[0]\n    theta2PredictedBits = a.mean(axis=1).nonzero()[0]\n\n    # To decode it we need to create a flattened array again and pass it\n    # to encoder.\n    # TODO: We use encoder's topDownCompute method - not sure if that is best.\n    t1 = numpy.zeros(self.theta1Encoder.getWidth())\n    t1[theta1PredictedBits] = 1\n    t1Prediction = self.theta1Encoder.topDownCompute(t1)[0].value\n\n    t2 = numpy.zeros(self.theta2Encoder.getWidth())\n    t2[theta2PredictedBits] = 1\n    t2Prediction = self.theta2Encoder.topDownCompute(t2)[0].value\n\n    # print >> sys.stderr, \"predicted cells = \", predictedCells\n    # print >> sys.stderr, \"decoded theta1 bits = \", theta1PredictedBits\n    # print >> sys.stderr, \"decoded theta2 bits = \", theta2PredictedBits\n    # print >> sys.stderr, \"decoded theta1 value = \", t1Prediction\n    # print >> sys.stderr, \"decoded theta2 value = \", t2Prediction\n\n    return t1Prediction, t2Prediction"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns inference and return the set of predicted cells", "response": "def inferTM(self, bottomUp, externalInput):\n    \"\"\"\n    Run inference and return the set of predicted cells\n    \"\"\"\n    self.reset()\n    # print >> sys.stderr, \"Bottom up: \", bottomUp\n    # print >> sys.stderr, \"ExternalInput: \",externalInput\n    self.tm.compute(bottomUp,\n                    basalInput=externalInput,\n                    learn=False)\n    # print >> sys.stderr, (\"new active cells \" + str(self.tm.getActiveCells()))\n    # print >> sys.stderr, (\"new predictive cells \" + str(self.tm.getPredictiveCells()))\n    return self.tm.getPredictiveCells()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, filename=\"temp.pkl\"):\n    output = open(filename, 'wb')\n    cPickle.dump(self.tm, output, protocol=cPickle.HIGHEST_PROTOCOL)", "response": "Save the current TM to a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the TM from a file.", "response": "def load(self, filename=\"temp.pkl\"):\n    \"\"\"\n    Save TM in the filename specified above\n    \"\"\"\n    inputFile = open(filename, 'rb')\n    self.tm = cPickle.load(inputFile)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef classify(self, encoding, num=1):\n    probDist = numpy.exp(encoding) / numpy.sum(numpy.exp(encoding))\n    sortIdx = numpy.argsort(probDist)\n    return sortIdx[-num:].tolist()", "response": "Classify with basic one - hot local incoding\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrains LSTM network on buffered dataset history", "response": "def train(self, params):\n    \"\"\"\n    Train LSTM network on buffered dataset history\n    After training, run LSTM on history[:-1] to get the state correct\n    :param params:\n    :return:\n    \"\"\"\n    if params['reset_every_training']:\n      n = params['encoding_num']\n      self.net = buildNetwork(n, params['num_cells'], n,\n                               hiddenclass=LSTMLayer,\n                               bias=True,\n                               outputbias=params['output_bias'],\n                               recurrent=True)\n      self.net.reset()\n\n    # prepare training dataset\n    ds = SequentialDataSet(params['encoding_num'], params['encoding_num'])\n    history = self.window(self.history, params)\n    resets = self.window(self.resets, params)\n\n    for i in xrange(1, len(history)):\n      if not resets[i - 1]:\n        ds.addSample(self.encoder.encode(history[i - 1]),\n                     self.encoder.encode(history[i]))\n      if resets[i]:\n        ds.newSequence()\n\n    print \"Train LSTM network on buffered dataset of length \", len(history)\n    if params['num_epochs'] > 1:\n      trainer = RPropMinusTrainer(self.net,\n                                  dataset=ds,\n                                  verbose=params['verbosity'] > 0)\n\n      if len(history) > 1:\n        trainer.trainEpochs(params['num_epochs'])\n\n      # run network on buffered dataset after training to get the state right\n      self.net.reset()\n      for i in xrange(len(history) - 1):\n        symbol = history[i]\n        output = self.net.activate(self.encoder.encode(symbol))\n        self.encoder.classify(output, num=params['num_predictions'])\n\n        if resets[i]:\n          self.net.reset()\n    else:\n      self.trainer.setData(ds)\n      self.trainer.train()\n\n      # run network on buffered dataset after training to get the state right\n      self.net.reset()\n      for i in xrange(len(history) - 1):\n        symbol = history[i]\n        output = self.net.activate(self.encoder.encode(symbol))\n        self.encoder.classify(output, num=params['num_predictions'])\n\n        if resets[i]:\n          self.net.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nkills cells from the network", "response": "def killCells(self, killCellPercent):\n    \"\"\"\n    kill a fraction of LSTM cells from the network\n    :param killCellPercent:\n    :return:\n    \"\"\"\n    if killCellPercent <= 0:\n      return\n\n    inputLayer = self.net['in']\n    lstmLayer = self.net['hidden0']\n\n    numLSTMCell = lstmLayer.outdim\n    numDead = round(killCellPercent * numLSTMCell)\n    zombiePermutation = numpy.random.permutation(numLSTMCell)\n    deadCells = zombiePermutation[0:numDead]\n\n    # remove connections from input layer to dead LSTM cells\n    connectionInputToHidden = self.net.connections[inputLayer][0]\n    weightInputToHidden = reshape(connectionInputToHidden.params,\n                                  (connectionInputToHidden.outdim,\n                                   connectionInputToHidden.indim))\n\n    for cell in deadCells:\n      for dim in range(4):\n        weightInputToHidden[dim * numLSTMCell + cell, :] *= 0\n\n    newParams = reshape(weightInputToHidden,\n                        (connectionInputToHidden.paramdim,))\n    self.net.connections[inputLayer][0]._setParameters(\n      newParams, connectionInputToHidden.owner)\n\n    # remove dead connections within LSTM layer\n    connectionHiddenToHidden = self.net.recurrentConns[0]\n    weightHiddenToHidden = reshape(connectionHiddenToHidden.params,\n                                   (connectionHiddenToHidden.outdim,\n                                    connectionHiddenToHidden.indim))\n\n    for cell in deadCells:\n      weightHiddenToHidden[:, cell] *= 0\n\n    newParams = reshape(weightHiddenToHidden,\n                        (connectionHiddenToHidden.paramdim,))\n    self.net.recurrentConns[0]._setParameters(\n      newParams, connectionHiddenToHidden.owner)\n\n    # remove connections from dead LSTM cell to output layer\n    connectionHiddenToOutput = self.net.connections[lstmLayer][0]\n    weightHiddenToOutput = reshape(connectionHiddenToOutput.params,\n                                   (connectionHiddenToOutput.outdim,\n                                    connectionHiddenToOutput.indim))\n    for cell in deadCells:\n      weightHiddenToOutput[:, cell] *= 0\n\n    newParams = reshape(weightHiddenToOutput,\n                        (connectionHiddenToOutput.paramdim,))\n    self.net.connections[lstmLayer][0]._setParameters(\n      newParams, connectionHiddenToOutput.owner)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _seed(self, seed=-1):\n        if seed != -1:\n          self._random = np.random.RandomState(seed)\n        else:\n          self._random = np.random.RandomState()", "response": "Initialize the random seed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_weights(self):\n        n = self._outputSize\n        m = self._inputSize\n        self._Q = self._random.sample((n,m))\n\n        # Normalize the weights of each units\n        for i in range(n):\n            self._Q[i] /=  np.sqrt( np.dot(self._Q[i], self._Q[i]) )", "response": "Randomly initializes the visible - to - hidden connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode(self, x):\n        n = self._outputSize\n        y = np.zeros(n)\n        Q = self._Q\n        W = self._W\n        t = self._t\n        lam = self._lambda\n\n\n        try:\n            \n            y_star = np.random.sample(n)\n            y_star = fixed_point(lambda p: expit(lam * ( np.dot(Q,x) + np.dot(W,p) - t)), \n                                 y_star, maxiter=2000, method='del2')\n\n        except RuntimeError:\n            pass\n\n\n        winner = np.where(y_star > 0.5)[0]\n        y[ winner ] = 1.\n\n        return y", "response": "Given an input array x it returns its associated encoding y."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the weight of the user - specified data.", "response": "def update_weights(self, x, y):\n        \"\"\"Weight updates as described in the F\u00f6ldi\u00e1k's paper.\"\"\"\n        m     = self._inputSize\n        n     = self._outputSize\n        W     = self._W\n        Q     = self._Q\n        t     = self._t\n        alpha = self._alpha\n        beta  = self._beta\n        gamma = self._gamma\n        p     = self._p\n\n        for i in range(n):\n            for j in range(n):\n                Delta_W  = - alpha * ( y[i] * y[j] - p*p )\n                W[i,j]  += Delta_W\n\n                if i==j or W[i,j] > 0:\n                    W[i,j] = 0.\n\n\n        for i in range(n):\n            for j in range(m):\n                Delta_Q  = beta * y[i] * ( x[j] - Q[i,j] )\n                Q[i,j]  += Delta_Q\n\n\n\n        for i in range(n):\n            Delta_t  = gamma * ( y[i] - p )\n            t[i]    += Delta_t"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms an experimentatl local inhibition.", "response": "def _inhibitColumnsWithLateral(self, overlaps, lateralConnections):\n    \"\"\"\n    Performs an experimentatl local inhibition. Local inhibition is \n    iteratively performed on a column by column basis.\n    \"\"\"\n    n,m = self.shape\n    y   = np.zeros(n)\n    s   = self.sparsity\n    L   = lateralConnections\n    desiredWeight = self.codeWeight\n    inhSignal     = np.zeros(n)\n    sortedIndices = np.argsort(overlaps, kind='mergesort')[::-1]\n\n    currentWeight = 0\n    for i in sortedIndices:\n\n      if overlaps[i] < self._stimulusThreshold:\n        break\n\n      inhTooStrong = ( inhSignal[i] >= s )\n\n      if not inhTooStrong:\n        y[i]              = 1.\n        currentWeight    += 1\n        inhSignal[:]     += L[i,:]\n\n      if self.enforceDesiredWeight and currentWeight == desiredWeight:\n        break\n\n    activeColumns = np.where(y==1.0)[0]\n\n    return activeColumns"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef feedforward(self):\n    m = self._numInputs\n    n = self._numColumns\n    W = np.zeros((n, m))\n    for i in range(self._numColumns):\n        self.getPermanence(i, W[i, :])\n\n    return W", "response": "Returns a numpy array where each row is a row of the column with the permanence of each column."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadapts synapses based on the input vector and the active columns.", "response": "def _adaptSynapses(self, inputVector, activeColumns, overlaps):\n    \"\"\"\n    The primary method in charge of learning. Adapts the permanence values of\n    the synapses based on the input vector, and the chosen columns after\n    inhibition round. Permanence values are increased for synapses connected to\n    input bits that are turned on, and decreased for synapses connected to\n    inputs bits that are turned off.\n\n    Parameters:\n    ----------------------------\n    :param inputVector:\n                    A numpy array of 0's and 1's that comprises the input to\n                    the spatial pooler. There exists an entry in the array\n                    for every input bit.\n    :param activeColumns:\n                    An array containing the indices of the columns that\n                    survived inhibition.\n    \"\"\"\n    inputIndices = np.where(inputVector > 0)[0]\n\n    permChanges = np.zeros(self._numInputs, dtype=realDType)\n    permChanges.fill(-1 * self._synPermInactiveDec)\n    permChanges[inputIndices] = self._synPermActiveInc\n    for columnIndex in activeColumns:\n      perm = self._permanences[columnIndex]\n      maskPotential = np.where(self._potentialPools[columnIndex] > 0)[0]\n      perm[maskPotential] += permChanges[maskPotential]\n      self._updatePermanencesForColumn(perm, columnIndex, raisePerm=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning an experiment tracking the performance of the temporal memory given noise. The number of active cells and the dimensions of the TM are fixed. We track performance by comparing the cells predicted to be active with the cells actually active in the sequence without noise at every timestep, and averaging across timesteps. Three metrics are used, correlation (Pearson's r, by numpy.corrcoef), set similarity (Jaccard index) and cosine similarity (using scipy.spatial.distance.cosine). The Jaccard set similarity is the canonical metric used in the paper, but all three metrics tend to produce very similar results. Typically, this experiment is run to test the influence of activation threshold on noise tolerance, with multiple different thresholds tested. However, this experiment could also be used to examine the influence of factors such as sparsity and sequence length. Output is written to tm_noise_{threshold}}.txt, including sample size. We used three different activation threshold settings, 8, 12 and 16, mirroring the parameters used in the Poirazi neuron model experiment.", "response": "def run_tm_noise_experiment(dim = 2048,\n                            cellsPerColumn=1,\n                            num_active = 40,\n                            activationThreshold=16,\n                            initialPermanence=0.8,\n                            connectedPermanence=0.50,\n                            minThreshold=16,\n                            maxNewSynapseCount=20,\n                            permanenceIncrement=0.05,\n                            permanenceDecrement=0.00,\n                            predictedSegmentDecrement=0.000,\n                            maxSegmentsPerCell=255,\n                            maxSynapsesPerSegment=255,\n                            seed=42,\n                            num_samples = 1,\n                            num_trials = 1000,\n                            sequence_length = 20,\n                            training_iters = 1,\n                            automatic_threshold = False,\n                            noise_range = range(0, 100, 5)):\n\n  \"\"\"\n  Run an experiment tracking the performance of the temporal memory given\n  noise.  The number of active cells and the dimensions of the TM are\n  fixed. We track performance by comparing the cells predicted to be\n  active with the cells actually active in the sequence without noise at\n  every timestep, and averaging across timesteps. Three metrics are used,\n  correlation (Pearson's r, by numpy.corrcoef), set similarity (Jaccard\n  index) and cosine similarity (using scipy.spatial.distance.cosine). The\n  Jaccard set similarity is the canonical metric used in the paper, but\n  all three metrics tend to produce very similar results.\n\n  Typically, this experiment is run to test the influence of activation\n  threshold on noise tolerance, with multiple different thresholds tested.\n  However, this experiment could also be used to examine the influence of\n  factors such as sparsity and sequence length.\n\n  Output is written to tm_noise_{threshold}}.txt, including sample size.\n\n  We used three different activation threshold settings, 8, 12 and 16, mirroring\n  the parameters used in the Poirazi neuron model experiment.\n  \"\"\"\n  if automatic_threshold:\n    activationThreshold = min(num_active/2, maxNewSynapseCount/2)\n    minThreshold = min(num_active/2, maxNewSynapseCount/2)\n\n  for noise in noise_range:\n    print noise\n    for trial in range(num_trials):\n      tm = TM(columnDimensions=(dim,),\n          cellsPerColumn=cellsPerColumn,\n          activationThreshold=activationThreshold,\n          initialPermanence=initialPermanence,\n          connectedPermanence=connectedPermanence,\n          minThreshold=minThreshold,\n          maxNewSynapseCount=maxNewSynapseCount,\n          permanenceIncrement=permanenceIncrement,\n          permanenceDecrement=permanenceDecrement,\n          predictedSegmentDecrement=predictedSegmentDecrement,\n          maxSegmentsPerCell=maxSegmentsPerCell,\n          maxSynapsesPerSegment=maxSynapsesPerSegment,\n          )#seed=seed)\n\n      datapoints = []\n      canonical_active_cells = []\n\n      for sample in range(num_samples):\n        data = generate_evenly_distributed_data_sparse(dim = dim, num_active = num_active, num_samples = sequence_length)\n        datapoints.append(data)\n        for i in range(training_iters):\n          for j in range(data.nRows()):\n            activeColumns = set(data.rowNonZeros(j)[0])\n            tm.compute(activeColumns, learn = True)\n          tm.reset()\n\n        current_active_cells = []\n        for j in range(data.nRows()):\n          activeColumns = set(data.rowNonZeros(j)[0])\n          tm.compute(activeColumns, learn = True)\n          current_active_cells.append(tm.getActiveCells())\n        canonical_active_cells.append(current_active_cells)\n        tm.reset()\n\n      # Now that the TM has been trained, check its performance on each sequence with noise added.\n      correlations = []\n      similarities = []\n      csims = []\n      for datapoint, active_cells in zip(datapoints, canonical_active_cells):\n        data = copy.deepcopy(datapoint)\n        apply_noise(data, noise)\n\n        predicted_cells = []\n\n        for j in range(data.nRows()):\n          activeColumns = set(data.rowNonZeros(j)[0])\n          tm.compute(activeColumns, learn = False)\n          predicted_cells.append(tm.getPredictiveCells())\n\n        similarity = [(0.+len(set(predicted) & set(active)))/len((set(predicted) | set(active))) for predicted, active in zip (predicted_cells[:-1], active_cells[1:])]\n        dense_predicted_cells = convert_cell_lists_to_dense(2048*32, predicted_cells[:-1])\n        dense_active_cells = convert_cell_lists_to_dense(2048*32, active_cells[1:])\n        correlation = [numpy.corrcoef(numpy.asarray([predicted, active]))[0, 1] for predicted, active in zip(dense_predicted_cells, dense_active_cells)]\n        csim = [1 - cosine(predicted, active) for predicted, active in zip(dense_predicted_cells, dense_active_cells)]\n\n        correlation = numpy.nan_to_num(correlation)\n        csim = numpy.nan_to_num(csim)\n        correlations.append(numpy.mean(correlation))\n        similarities.append(numpy.mean(similarity))\n        csims.append(numpy.mean(csim))\n\n    correlation = numpy.mean(correlations)\n    similarity = numpy.mean(similarities)\n    csim = numpy.mean(csims)\n    with open(\"tm_noise_{}.txt\".format(activationThreshold), \"a\") as f:\n      f.write(str(noise)+\", \" + str(correlation) + \", \" + str(similarity) + \", \" + str(csim) + \", \" + str(num_trials) + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plotSensationByColumn(suite, name):\n  path = suite.cfgparser.get(name, \"path\")\n  path = os.path.join(path, name)\n\n  touches = {}\n  for exp in suite.get_exps(path=path):\n    params = suite.get_params(exp)\n    cols = params[\"num_cortical_columns\"]\n    features = params[\"num_features\"]\n    if not features in touches:\n      touches[features] = {}\n\n    touches[features][cols] = np.mean(\n      suite.get_histories_over_repetitions(exp, \"touches\", np.mean))\n\n  ax = plt.figure(tight_layout={\"pad\": 0}).gca()\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n  for i, features in enumerate(sorted(touches)):\n    cols = touches[features]\n    plt.plot(cols.keys(), cols.values(), \"-\",\n             label=\"Unique features={}\".format(features),\n             color=colorList[i])\n\n  # format\n  plt.xlabel(\"Number of columns\")\n  plt.ylabel(\"Average number of touches\")\n  plt.title(\"Number of touches to recognize one object (multiple columns)\")\n  ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n  plt.legend(framealpha=1.0)\n\n  # save\n  path = suite.cfgparser.get(name, \"path\")\n  plotPath = os.path.join(path, \"{}.pdf\".format(name))\n  plt.savefig(plotPath)\n  plt.close()", "response": "Plots the convergence graph for a single column."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset(self, params, repetition):\n    print params[\"name\"], \":\", repetition\n\n    self.debug = params.get(\"debug\", False)\n\n    L2Params = json.loads('{' + params[\"l2_params\"] + '}')\n    L4Params = json.loads('{' + params[\"l4_params\"] + '}')\n    L6aParams = json.loads('{' + params[\"l6a_params\"] + '}')\n\n    # Make sure random seed is different for each repetition\n    seed = params.get(\"seed\", 42)\n    np.random.seed(seed + repetition)\n    random.seed(seed + repetition)\n    L2Params[\"seed\"] = seed + repetition\n    L4Params[\"seed\"] = seed + repetition\n    L6aParams[\"seed\"] = seed + repetition\n\n    # Configure L6a params\n    numModules = params[\"num_modules\"]\n    L6aParams[\"scale\"] = [params[\"scale\"]] * numModules\n    angle = params[\"angle\"] / numModules\n    orientation = range(angle / 2, angle * numModules, angle)\n    L6aParams[\"orientation\"] = np.radians(orientation).tolist()\n\n    # Create multi-column L2-L4-L6a network\n    self.numColumns = params[\"num_cortical_columns\"]\n    network = Network()\n    network = createMultipleL246aLocationColumn(network=network,\n                                                numberOfColumns=self.numColumns,\n                                                L2Params=L2Params,\n                                                L4Params=L4Params,\n                                                L6aParams=L6aParams)\n    network.initialize()\n\n    self.network = network\n    self.sensorInput = []\n    self.motorInput = []\n    self.L2Regions = []\n    self.L4Regions = []\n    self.L6aRegions = []\n    for i in xrange(self.numColumns):\n      col = str(i)\n      self.sensorInput.append(network.regions[\"sensorInput_\" + col].getSelf())\n      self.motorInput.append(network.regions[\"motorInput_\" + col].getSelf())\n      self.L2Regions.append(network.regions[\"L2_\" + col])\n      self.L4Regions.append(network.regions[\"L4_\" + col])\n      self.L6aRegions.append(network.regions[\"L6a_\" + col])\n\n    # Use the number of iterations as the number of objects. This will allow us\n    # to execute one iteration per object and use the \"iteration\" parameter as\n    # the object index\n    numObjects = params[\"iterations\"]\n\n    # Generate feature SDRs\n    numFeatures = params[\"num_features\"]\n    numOfMinicolumns = L4Params[\"columnCount\"]\n    numOfActiveMinicolumns = params[\"num_active_minicolumns\"]\n    self.featureSDR = [{\n      str(f): sorted(np.random.choice(numOfMinicolumns, numOfActiveMinicolumns))\n      for f in xrange(numFeatures)\n    } for _ in xrange(self.numColumns)]\n\n    # Generate objects used in the experiment\n    self.objects = generateObjects(numObjects=numObjects,\n                                   featuresPerObject=params[\"features_per_object\"],\n                                   objectWidth=params[\"object_width\"],\n                                   numFeatures=numFeatures,\n                                   distribution=params[\"feature_distribution\"])\n\n    # Make sure the objects are unique\n    uniqueObjs = np.unique([{\"features\": obj[\"features\"]}\n                            for obj in self.objects])\n    assert len(uniqueObjs) == len(self.objects)\n\n    self.sdrSize = L2Params[\"sdrSize\"]\n\n    # Learn objects\n    self.numLearningPoints = params[\"num_learning_points\"]\n    self.numOfSensations = params[\"num_sensations\"]\n    self.learnedObjects = {}\n    self.learn()", "response": "Reset the object to the initial state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlearn all objects on every column and store the L2 representation of each object in the learnedObjects dictionary.", "response": "def learn(self):\n    \"\"\"\n    Learn all objects on every column. Each column will learn all the features\n    of every object and store the the object's L2 representation to be later\n    used in the inference stage\n    \"\"\"\n    self.setLearning(True)\n\n    for obj in self.objects:\n      self.sendReset()\n\n      previousLocation = [None] * self.numColumns\n      displacement = [0., 0.]\n      features = obj[\"features\"]\n      numOfFeatures = len(features)\n\n      # Randomize touch sequences\n      touchSequence = np.random.permutation(numOfFeatures)\n\n      for sensation in xrange(numOfFeatures):\n        for col in xrange(self.numColumns):\n          # Shift the touch sequence for each column\n          colSequence = np.roll(touchSequence, col)\n          feature = features[colSequence[sensation]]\n          # Move the sensor to the center of the object\n          locationOnObject = np.array([feature[\"top\"] + feature[\"height\"] / 2.,\n                                       feature[\"left\"] + feature[\"width\"] / 2.])\n          # Calculate displacement from previous location\n          if previousLocation[col] is not None:\n            displacement = locationOnObject - previousLocation[col]\n          previousLocation[col] = locationOnObject\n\n          # learn each pattern multiple times\n          activeColumns = self.featureSDR[col][feature[\"name\"]]\n          for _ in xrange(self.numLearningPoints):\n            # Sense feature at location\n            self.motorInput[col].addDataToQueue(displacement)\n            self.sensorInput[col].addDataToQueue(activeColumns, False, 0)\n            # Only move to the location on the first sensation.\n            displacement = [0, 0]\n\n      self.network.run(numOfFeatures * self.numLearningPoints)\n\n      # update L2 representations for the object\n      self.learnedObjects[obj[\"name\"]] = self.getL2Representations()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninfer the object with the network.", "response": "def infer(self, objectToInfer, stats=None):\n    \"\"\"\n    Attempt to recognize the specified object with the network. Randomly move\n    the sensor over the object until the object is recognized.\n    \"\"\"\n    self.setLearning(False)\n    self.sendReset()\n\n    touches = None\n    previousLocation = [None] * self.numColumns\n    displacement = [0., 0.]\n    features = objectToInfer[\"features\"]\n    objName = objectToInfer[\"name\"]\n    numOfFeatures = len(features)\n\n    # Randomize touch sequences\n    touchSequence = np.random.permutation(numOfFeatures)\n\n    for sensation in xrange(self.numOfSensations):\n      # Add sensation for all columns at once\n      for col in xrange(self.numColumns):\n        # Shift the touch sequence for each column\n        colSequence = np.roll(touchSequence, col)\n        feature = features[colSequence[sensation]]\n        # Move the sensor to the center of the object\n        locationOnObject = np.array([feature[\"top\"] + feature[\"height\"] / 2.,\n                                     feature[\"left\"] + feature[\"width\"] / 2.])\n        # Calculate displacement from previous location\n        if previousLocation[col] is not None:\n          displacement = locationOnObject - previousLocation[col]\n        previousLocation[col] = locationOnObject\n\n        # Sense feature at location\n        self.motorInput[col].addDataToQueue(displacement)\n        self.sensorInput[col].addDataToQueue(self.featureSDR[col][feature[\"name\"]],\n                                             False, 0)\n      self.network.run(1)\n      if self.debug:\n        self._updateInferenceStats(statistics=stats, objectName=objName)\n\n      if touches is None and self.isObjectClassified(objName, minOverlap=30):\n        touches = sensation + 1\n        if not self.debug:\n          return touches\n\n    return self.numOfSensations if touches is None else touches"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _updateInferenceStats(self, statistics, objectName=None):\n    L6aLearnableCells = self.getL6aLearnableCells()\n    L6aSensoryAssociatedCells = self.getL6aSensoryAssociatedCells()\n    L6aRepresentations = self.getL6aRepresentations()\n    L4Representations = self.getL4Representations()\n    L4PredictedCells = self.getL4PredictedCells()\n    L2Representation = self.getL2Representations()\n\n    for i in xrange(self.numColumns):\n      statistics[\"L6a SensoryAssociatedCells C\" + str(i)].append(\n        len(L6aSensoryAssociatedCells[i]))\n      statistics[\"L6a LearnableCells C\" + str(i)].append(\n        len(L6aLearnableCells[i]))\n      statistics[\"L6a Representation C\" + str(i)].append(\n        len(L6aRepresentations[i]))\n      statistics[\"L4 Representation C\" + str(i)].append(\n        len(L4Representations[i]))\n      statistics[\"L4 Predicted C\" + str(i)].append(len(L4PredictedCells[i]))\n      statistics[\"L2 Representation C\" + str(i)].append(\n        len(L2Representation[i]))\n      statistics[\"Full L2 SDR C\" + str(i)].append(sorted(\n        [int(c) for c in L2Representation[i]]))\n      statistics[\"L4 Apical Segments C\" + str(i)].append(len(\n        self.L4Regions[i].getSelf()._tm.getActiveApicalSegments()))\n\n      # add true overlap and classification result if objectName was learned\n      if objectName in self.learnedObjects:\n        objectRepresentation = self.learnedObjects[objectName]\n        statistics[\"Overlap L2 with object C\" + str(i)].append(\n          len(objectRepresentation[i] & L2Representation[i]))\n\n    if objectName in self.learnedObjects:\n      if self.isObjectClassified(objectName, minOverlap=30):\n        statistics[\"Correct classification\"].append(1.0)\n      else:\n        statistics[\"Correct classification\"].append(0.0)", "response": "Updates the inference statistics for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing NRMSE in a sliding window.", "response": "def NRMSE_sliding(data, pred, windowSize):\n  \"\"\"\n  Computing NRMSE in a sliding window\n  :param data:\n  :param pred:\n  :param windowSize:\n  :return: (window_center, NRMSE)\n  \"\"\"\n\n  halfWindowSize = int(round(float(windowSize)/2))\n  window_center = range(halfWindowSize, len(data)-halfWindowSize, int(round(float(halfWindowSize)/5.0)))\n  nrmse = []\n  for wc in window_center:\n    nrmse.append(NRMSE(data[wc-halfWindowSize:wc+halfWindowSize],\n                       pred[wc-halfWindowSize:wc+halfWindowSize]))\n\n  return (window_center, nrmse)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a single L2456 column.", "response": "def _createL2456Column(network, networkConfig, suffix=\"\"):\n  \"\"\"\n  Create a single L2456 column with appropriate suffix on the name.\n  \"\"\"\n\n  locationInputName = \"locationInput\" + suffix\n  sensorInputName = \"sensorInput\" + suffix\n  coarseSensorInputName = \"coarseSensorInput\" + suffix\n  L2ColumnName = \"L2Column\" + suffix\n  L4ColumnName = \"L4Column\" + suffix\n  L5ColumnName = \"L5Column\" + suffix\n  L6ColumnName = \"L6Column\" + suffix\n\n  # TODO: Convert locationInput to a coordinate sensor region once its ready\n  # Add the three sensors to network.\n  network.addRegion(\n    locationInputName, \"py.CoordinateSensorRegion\",\n    json.dumps(networkConfig[\"locationParams\"]))\n  network.addRegion(\n    coarseSensorInputName, \"py.RawSensor\",\n    json.dumps(networkConfig[\"coarseSensorParams\"]))\n  network.addRegion(\n    sensorInputName, \"py.RawSensor\",\n    json.dumps(networkConfig[\"sensorParams\"]))\n\n  # Add L2/L5 column pooler regions\n  network.addRegion(\n    L2ColumnName, \"py.ColumnPoolerRegion\",\n    json.dumps(networkConfig[\"L2Params\"]))\n  network.addRegion(\n    L5ColumnName, \"py.ColumnPoolerRegion\",\n    json.dumps(networkConfig[\"L5Params\"]))\n\n  # Add L4/L6 extended temporal memory regions\n  L6Params = copy.deepcopy(networkConfig[\"L6Params\"])\n  L6Params[\"basalInputWidth\"] = networkConfig[\"locationParams\"][\"outputWidth\"]\n  L6Params[\"apicalInputWidth\"] = networkConfig[\"L5Params\"][\"cellCount\"]\n  network.addRegion(\n    L6ColumnName, \"py.ApicalTMPairRegion\",\n    json.dumps(L6Params))\n\n  L4Params = copy.deepcopy(networkConfig[\"L4Params\"])\n  L4Params[\"basalInputWidth\"] = (\n    L6Params[\"columnCount\"] * L6Params[\"cellsPerColumn\"] )\n  L4Params[\"apicalInputWidth\"] = networkConfig[\"L2Params\"][\"cellCount\"]\n  network.addRegion(\n    L4ColumnName, \"py.ApicalTMPairRegion\",\n    json.dumps(L4Params))\n\n  # Once regions are created, ensure inputs match column counts\n  assert(network.regions[L6ColumnName].getParameter(\"columnCount\") ==\n         network.regions[coarseSensorInputName].getParameter(\"outputWidth\")), \\\n         \"L6 column count must equal coarse sensor width\"\n\n  assert(network.regions[L4ColumnName].getParameter(\"columnCount\") ==\n         network.regions[sensorInputName].getParameter(\"outputWidth\")), \\\n         \"L4 column count must equal sensor width\"\n\n  # Link up the sensors\n  network.link(locationInputName, L6ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"basalInput\")\n  network.link(coarseSensorInputName, L6ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"activeColumns\")\n  network.link(sensorInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"dataOut\", destInput=\"activeColumns\")\n\n  # Link L6 to L4\n  network.link(L6ColumnName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"basalInput\")\n\n  # Link L4 to L2, L6 to L5\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"feedforwardInput\")\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"predictedActiveCells\",\n               destInput=\"feedforwardGrowthCandidates\")\n  network.link(L6ColumnName, L5ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"feedforwardInput\")\n  network.link(L6ColumnName, L5ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"predictedActiveCells\",\n               destInput=\"feedforwardGrowthCandidates\")\n\n  # Link L2 feedback to L4, L5 to L6\n  network.link(L2ColumnName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"feedForwardOutput\", destInput=\"apicalInput\",\n               propagationDelay=1)\n  network.link(L5ColumnName, L6ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"feedForwardOutput\", destInput=\"apicalInput\",\n               propagationDelay=1)\n\n  # Link reset outputs to L5 and L2. For L6 and L4, an empty input is sufficient\n  # for a reset.\n  network.link(sensorInputName, L5ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n  network.link(sensorInputName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n\n\n  # Set phases appropriately so regions are executed in the proper sequence.\n  # This is required particularly when we create multiple columns - the order of\n  # execution is not the same as the order of region creation.\n  # All sensors have phase 0\n  # All L6's have phase 2\n  # All L5's have phase 3\n  # All L4's have phase 4\n  # All L2's have phase 5\n  # Note: we skip phase 1 in case we add spatial poolers on top of the sensors.\n\n  network.setPhases(locationInputName,[0])\n  network.setPhases(sensorInputName,[0])\n  network.setPhases(coarseSensorInputName,[0])\n  network.setPhases(L6ColumnName,[2])\n  network.setPhases(L5ColumnName,[3])\n  network.setPhases(L4ColumnName, [4])\n  network.setPhases(L2ColumnName, [5])\n\n  return network"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createL2456Columns(network, networkConfig):\n\n  # Create each column\n  numCorticalColumns = networkConfig[\"numCorticalColumns\"]\n  for i in xrange(numCorticalColumns):\n    networkConfigCopy = copy.deepcopy(networkConfig)\n    randomSeedBase = networkConfigCopy[\"randomSeedBase\"]\n    networkConfigCopy[\"L2Params\"][\"seed\"] = randomSeedBase + i\n    networkConfigCopy[\"L4Params\"][\"seed\"] = randomSeedBase + i\n    networkConfigCopy[\"L5Params\"][\"seed\"] = randomSeedBase + i\n    networkConfigCopy[\"L6Params\"][\"seed\"] = randomSeedBase + i\n    networkConfigCopy[\"L2Params\"][\n      \"numOtherCorticalColumns\"] = numCorticalColumns - 1\n    networkConfigCopy[\"L5Params\"][\n      \"numOtherCorticalColumns\"] = numCorticalColumns - 1\n\n    suffix = \"_\" + str(i)\n    network = _createL2456Column(network, networkConfigCopy, suffix)\n\n  # Now connect the L2 columns laterally to every other L2 column, and\n  # the same for L5 columns.\n  for i in range(networkConfig[\"numCorticalColumns\"]):\n    suffixSrc = \"_\" + str(i)\n    for j in range(networkConfig[\"numCorticalColumns\"]):\n      if i != j:\n        suffixDest = \"_\" + str(j)\n        network.link(\n            \"L2Column\" + suffixSrc, \"L2Column\" + suffixDest,\n            \"UniformLink\", \"\",\n            srcOutput=\"feedForwardOutput\", destInput=\"lateralInput\")\n        network.link(\n            \"L5Column\" + suffixSrc, \"L5Column\" + suffixDest,\n            \"UniformLink\", \"\",\n            srcOutput=\"feedForwardOutput\", destInput=\"lateralInput\")\n\n  enableProfiling(network)\n\n  return network", "response": "Create a network consisting of multiple L2456 columns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint the parameters of a TPRegion object.", "response": "def printTPRegionParams(tpregion):\n  \"\"\"\n  Note: assumes we are using TemporalMemory/TPShim in the TPRegion\n  \"\"\"\n  tm = tpregion.getSelf()._tfdr\n  print \"------------PY  TemporalMemory Parameters ------------------\"\n  print \"numberOfCols             =\", tm.columnDimensions\n  print \"cellsPerColumn           =\", tm.cellsPerColumn\n  print \"minThreshold             =\", tm.minThreshold\n  print \"activationThreshold      =\", tm.activationThreshold\n  print \"newSynapseCount          =\", tm.maxNewSynapseCount\n  print \"initialPerm              =\", tm.initialPermanence\n  print \"connectedPerm            =\", tm.connectedPermanence\n  print \"permanenceInc            =\", tm.permanenceIncrement\n  print \"permanenceDec            =\", tm.permanenceDecrement\n  print \"predictedSegmentDecrement=\", tm.predictedSegmentDecrement\n  print"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build(self, input_shape, dtype=tf.float32):\n    with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n      nonZeros = int(round(input_shape[-1].value * self.sparsity))\n\n      # Create random mask with k elements set to 1, all other elements set to 0\n      values = tf.random_uniform(input_shape)\n      topk, _ = tf.math.top_k(input=values, k=nonZeros, sorted=False)\n      kth = tf.reduce_min(topk, axis=1, keepdims=True)\n      mask = tf.cast(tf.greater_equal(values, kth), dtype=dtype)\n      self.mask = tf.get_variable(self.name, initializer=mask, trainable=False,\n                                  synchronization=tf.VariableSynchronization.NONE)\n      keras.backend.track_variable(self.mask)\n      self._built = True", "response": "Builds the keras. Variable object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the transition traces for the current language.", "response": "def _mmComputeTransitionTraces(self):\n    \"\"\"\n    Computes the transition traces, if necessary.\n\n    Transition traces are the following:\n\n        predicted => active cells\n        predicted => inactive cells\n        predicted => active columns\n        predicted => inactive columns\n        unpredicted => active columns\n    \"\"\"\n    if not self._mmTransitionTracesStale:\n      return\n\n    self._mmData[\"predictedActiveCellsForSequence\"] = defaultdict(set)\n\n    self._mmTraces[\"predictedActiveCells\"] = IndicesTrace(self,\n      \"predicted => active cells (correct)\")\n    self._mmTraces[\"predictedInactiveCells\"] = IndicesTrace(self,\n      \"predicted => inactive cells (extra)\")\n    self._mmTraces[\"predictedActiveColumns\"] = IndicesTrace(self,\n      \"predicted => active columns (correct)\")\n    self._mmTraces[\"predictedInactiveColumns\"] = IndicesTrace(self,\n      \"predicted => inactive columns (extra)\")\n    self._mmTraces[\"unpredictedActiveColumns\"] = IndicesTrace(self,\n      \"unpredicted => active columns (bursting)\")\n\n    predictedCellsTrace = self._mmTraces[\"predictedCells\"]\n\n    for i, activeColumns in enumerate(self.mmGetTraceActiveColumns().data):\n      predictedActiveCells = set()\n      predictedInactiveCells = set()\n      predictedActiveColumns = set()\n      predictedInactiveColumns = set()\n\n      for predictedCell in predictedCellsTrace.data[i]:\n        predictedColumn = self.columnForCell(predictedCell)\n\n        if predictedColumn  in activeColumns:\n          predictedActiveCells.add(predictedCell)\n          predictedActiveColumns.add(predictedColumn)\n\n          sequenceLabel = self.mmGetTraceSequenceLabels().data[i]\n          if sequenceLabel is not None:\n            self._mmData[\"predictedActiveCellsForSequence\"][sequenceLabel].add(\n              predictedCell)\n        else:\n          predictedInactiveCells.add(predictedCell)\n          predictedInactiveColumns.add(predictedColumn)\n\n      unpredictedActiveColumns = set(activeColumns) - set(predictedActiveColumns)\n\n      self._mmTraces[\"predictedActiveCells\"].data.append(predictedActiveCells)\n      self._mmTraces[\"predictedInactiveCells\"].data.append(predictedInactiveCells)\n      self._mmTraces[\"predictedActiveColumns\"].data.append(predictedActiveColumns)\n      self._mmTraces[\"predictedInactiveColumns\"].data.append(\n        predictedInactiveColumns)\n      self._mmTraces[\"unpredictedActiveColumns\"].data.append(\n        unpredictedActiveColumns)\n\n    self._mmTransitionTracesStale = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the biased correlations for each bit across all the neurons in the data.", "response": "def get_biased_correlations(data, threshold= 10):\n  \"\"\"\n  Gets the highest few correlations for each bit, across the entirety of the\n  data.  Meant to provide a comparison point for the pairwise correlations\n  reported in the literature, which are typically between neighboring neurons\n  tuned to the same inputs.  We would expect these neurons to be among the most\n  correlated in any region, so pairwise correlations between most likely do not\n  provide an unbiased estimator of correlations between arbitrary neurons.\n  \"\"\"\n  data = data.toDense()\n  correlations = numpy.corrcoef(data, rowvar = False)\n  highest_correlations = []\n  for row in correlations:\n    highest_correlations += sorted(row, reverse = True)[1:threshold+1]\n  return numpy.mean(highest_correlations)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the average pairwise correlation between all patterns across the entire dataset.", "response": "def get_pattern_correlations(data):\n  \"\"\"\n  Gets the average correlation between all bits in patterns, across the entire\n  dataset.  Assumes input is a sparse matrix.  Weighted by pattern rather than\n  by bit; this is the average pairwise correlation for every pattern in the\n  data, and is not the average pairwise correlation for all bits that ever\n  cooccur.  This is a subtle but important difference.\n  \"\"\"\n\n  patterns = [data.rowNonZeros(i)[0] for i in range(data.nRows())]\n  dense_data = data.toDense()\n  correlations = numpy.corrcoef(dense_data, rowvar = False)\n  correlations = numpy.nan_to_num(correlations)\n  pattern_correlations = []\n  for pattern in patterns:\n    pattern_correlations.append([correlations[i, j]\n        for i in pattern for j in pattern if i != j])\n  return numpy.mean(pattern_correlations)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_correlated_data(dim = 2000,\n                             num_active = 40,\n                             num_samples = 1000,\n                             num_cells_per_cluster_size = [2000]*8,\n                             cluster_sizes = range(2, 10)):\n  \"\"\"\n  Generates a set of data drawn from a uniform distribution, but with bits\n  clustered to force correlation between neurons.  Clusters are randomly chosen\n  to form an activation pattern, in such a way as to maintain sparsity.\n\n  The number of clusters of each size is defined by num_cells_per_cluster_size,\n  as cluster_size*num_clusters = num_cells. This parameter can be thought of as\n  indicating how thoroughly the clusters \"cover\" the space. Typical values are\n  1-3 times the total dimension.  These are specificied independently for each\n  cluster size, to allow for more variation.\n  \"\"\"\n\n  # Chooses clusters to\n  clusters = []\n  cells = set(range(dim))\n\n  # Associate cluster sizes with how many cells they should have, then generate\n  # clusters.\n  for size, num_cells in zip(cluster_sizes, num_cells_per_cluster_size):\n\n    # Must have (num_cells/size) clusters in order to have num_cells cells\n    # across all clusters with this size.\n    for i in range(int(1.*num_cells/size)):\n      cluster = tuple(numpy.random.choice(dim, size, replace = False))\n      clusters.append(cluster)\n\n  # Now generate a list of num_samples SDRs\n  datapoints = []\n  for sample in range(num_samples):\n\n    # This conditional is necessary in the case that very, very few clusters are\n    # created, as otherwise numpy.random.choice can attempt to choose more\n    # clusters than is possible.  Typically this case will not occur, but it is\n    # possible if we are attempting to generate extremely correlated data.\n    if len(clusters) > num_active/2:\n      chosen_clusters = numpy.random.choice(len(clusters),\n          num_active/2, replace = False)\n      current_clusters = [clusters[i] for i in chosen_clusters]\n    else:\n      current_clusters = clusters\n\n    # Pick clusters until doing so would exceed our target num_active.\n    current_cells = set()\n    for cluster in current_clusters:\n      if len(current_cells) + len(cluster) < num_active:\n        current_cells |= set(cluster)\n      else:\n        break\n\n    # Add random cells to the SDR if we still don't have enough active.\n    if len(current_cells) < num_active:\n      possible_cells = cells - current_cells\n      new_cells = numpy.random.choice(tuple(possible_cells),\n          num_active - len(current_cells), replace = False)\n      current_cells |= set(new_cells)\n    datapoints.append(list(current_cells))\n\n  data = SM32()\n  data.reshape(num_samples, dim)\n  for sample, datapoint in enumerate(datapoints):\n    for i in datapoint:\n      data[sample, i] = 1.\n  return data", "response": "Generates a set of data drawn from a uniform distribution and with a uniform distribution of correlated neurons."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply noise to a sparse matrix.", "response": "def apply_noise(data, noise):\n  \"\"\"\n  Applies noise to a sparse matrix.  Noise can be an integer between 0 and\n  100, indicating the percentage of ones in the original input to move, or\n  a float in [0, 1), indicating the same thing.\n  The input matrix is modified in-place, and nothing is returned.\n  This operation does not affect the sparsity of the matrix, or of any\n  individual datapoint.\n  \"\"\"\n  if noise >= 1:\n    noise = noise/100.\n\n  for i in range(data.nRows()):\n    ones = data.rowNonZeros(i)[0]\n    replace_indices = numpy.random.choice(ones,\n        size = int(len(ones)*noise), replace = False)\n    for index in replace_indices:\n      data[i, index] = 0\n\n    new_indices = numpy.random.choice(data.nCols(),\n        size = int(len(ones)*noise), replace = False)\n\n    for index in new_indices:\n      while data[i, index] == 1:\n        index = numpy.random.randint(0, data.nCols())\n      data[i, index] = 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shuffle_sparse_matrix_and_labels(matrix, labels):\n  print \"Shuffling data\"\n  new_matrix = matrix.toDense()\n  rng_state = numpy.random.get_state()\n  numpy.random.shuffle(new_matrix)\n  numpy.random.set_state(rng_state)\n  numpy.random.shuffle(labels)\n\n  print \"Data shuffled\"\n  return SM32(new_matrix), numpy.asarray(labels)", "response": "Shuffles a sparse matrix and sets of labels together."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting a sparse matrix into num_categories non - empty ones.", "response": "def split_sparse_matrix(matrix, num_categories):\n  \"\"\"\n  An analog of numpy.split for our sparse matrix.  If the number of\n  categories does not divide the number of rows in the matrix, all\n  overflow is placed in the final bin.\n\n  In the event that there are more categories than rows, all later\n  categories are considered to be an empty sparse matrix.\n  \"\"\"\n  if matrix.nRows() < num_categories:\n    return [matrix.getSlice(i, i+1, 0, matrix.nCols())\n        for i in range(matrix.nRows())] + [SM32()\n        for i in range(num_categories - matrix.nRows())]\n  else:\n    inc = matrix.nRows()/num_categories\n    divisions = [matrix.getSlice(i*inc, (i+1)*inc, 0, matrix.nCols())\n        for i in range(num_categories - 1)]\n\n    # Handle the last bin separately.  All overflow goes into it.\n    divisions.append(matrix.getSlice((num_categories - 1)*inc, matrix.nRows(),\n        0, matrix.nCols()))\n\n    return divisions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_evenly_distributed_data_sparse(dim = 2000,\n                                            num_active = 40,\n                                            num_samples = 1000):\n  \"\"\"\n  Generates a set of data drawn from a uniform distribution.  The binning\n  structure from Poirazi & Mel is ignored, and all (dim choose num_active)\n  arrangements are possible.\n  \"\"\"\n  indices = [numpy.random.choice(dim, size = num_active, replace = False)\n      for _ in range(num_samples)]\n  data = SM32()\n\n  data.reshape(0, dim)\n  for row in indices:\n    data.addRowNZ(row, [1]*num_active)\n\n  # data.reshape(num_samples, dim)\n  # for sample, datapoint in enumerate(indices):\n  #   for index in datapoint:\n  #     data[sample, index] = 1.\n\n  return data", "response": "Generates a set of data drawn from a uniform distribution of Poirazi & Mel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_evenly_distributed_data(dim = 2000,\n                                     num_active = 40,\n                                     num_samples = 1000,\n                                     num_negatives = 500):\n  \"\"\"\n  Generates a set of data drawn from a uniform distribution.  The binning\n  structure from Poirazi & Mel is ignored, and all (dim choose num_active)\n  arrangements are possible. num_negatives samples are put into a separate\n  negatives category for output compatibility with generate_data, but are\n  otherwise identical.\n  \"\"\"\n\n  sparse_data = [numpy.random.choice(dim, size = num_active, replace = False)\n      for i in range(num_samples)]\n  data = [[0 for i in range(dim)] for i in range(num_samples)]\n  for datapoint, sparse_datapoint in zip(data, sparse_data):\n    for i in sparse_datapoint:\n      datapoint[i] = 1\n\n  negatives = data[:num_negatives]\n  positives = data[num_negatives:]\n  return positives, negatives", "response": "Generates a set of data drawn from a uniform distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates data from the appendix V of 2001.", "response": "def generate_data(dim = 40, num_samples = 30000, num_bins = 10, sparse = False):\n  \"\"\"\n  Generates data following appendix V of (Poirazi & Mel, 2001).\n  Positive and negative examples are drawn from the same distribution, but\n  are multiplied by different square matrices, one of them uniform on [-1, 1\n  and one the sum of a uniform matrix and a normal one.  It is assumed that half\n  the samples are negative in this case.\n\n  Initially samples of dimension dim are produced, with values in each\n  dimension being floats, but they are binned into discrete categories, with\n  num_bins bins per dimension.  This binning produces an SDR.\n  \"\"\"\n\n  positives, negatives = [], []\n  positive, negative = generate_matrices(dim)\n\n  for i in range(num_samples):\n    phase_1 = generate_phase_1(dim)\n    phase_2 = generate_phase_2(phase_1, dim)\n\n\n    if i < num_samples/2:\n      positives.append(numpy.dot(positive, phase_2))\n    else:\n      negatives.append(numpy.dot(negative, phase_2))\n\n  binned_data = bin_data(positives + negatives, dim, num_bins)\n  positives = binned_data[:len(binned_data)/2]\n  negatives = binned_data[len(binned_data)/2:]\n\n  if sparse:\n    positives = SM32(positives)\n    negatives = SM32(negatives)\n\n  return positives, negatives"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_phase_1(dim = 40):\n  phase_1 = numpy.random.normal(0, 1, dim)\n  for i in range(dim - 4, dim):\n    phase_1[i] = 1.0\n  return phase_1", "response": "Generate a phase vector for the Poirazi & Mel model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_matrices(dim = 40):\n  positive = numpy.random.uniform(-1, 1, (dim, dim))\n  negative = positive + numpy.random.normal(0, 1, (dim, dim))\n  return positive, negative", "response": "Generates the matrix for positive and negative examples\n"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate bins for the encoder.", "response": "def generate_RF_bins(data, dim = 40, num_bins = 10):\n  \"\"\"\n  Generates bins for the encoder.  Bins are designed to have equal frequency,\n  per Poirazi & Mel (2001), which requires reading the data once.\n  Bins are represented as the intervals dividing them.\n  \"\"\"\n  intervals = []\n  for i in range(dim):\n    current_dim_data = [data[x][i] for x in range(len(data))]\n    current_dim_data = numpy.sort(current_dim_data)\n    intervals.append([current_dim_data[int(len(current_dim_data)*x/num_bins)]\n        for x in range(1, num_bins)])\n  return intervals"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a datapoint and intervals representing bins returns the number of bins that are contained in the datapoint.", "response": "def bin_number(datapoint, intervals):\n  \"\"\"\n  Given a datapoint and intervals representing bins, returns the number\n  represented in binned form, where the bin including the value is\n  set to 1 and all others are 0.\n\n  \"\"\"\n  index = numpy.searchsorted(intervals, datapoint)\n  return [0 if index != i else 1 for i in range(len(intervals) + 1)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bin_data(data, dim = 40, num_bins = 10):\n  intervals = generate_RF_bins(data, dim, num_bins)\n  binned_data = [numpy.concatenate([bin_number(data[x][i], intervals[i])\n      for i in range(len(data[x]))]) for x in range(len(data))]\n  return binned_data", "response": "Bin the data using generate_RF_bins and bin_number."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef killCells(self, killCellPercent):\n    if killCellPercent <= 0:\n      return\n\n    numHiddenNeurons = self.net.numHiddenNeurons\n\n    numDead = round(killCellPercent * numHiddenNeurons)\n    zombiePermutation = numpy.random.permutation(numHiddenNeurons)\n    deadCells = zombiePermutation[0:numDead]\n    liveCells = zombiePermutation[numDead:]\n\n    self.net.inputWeights = self.net.inputWeights[liveCells, :]\n    self.net.bias = self.net.bias[:, liveCells]\n    self.net.beta = self.net.beta[liveCells, :]\n    self.net.M = self.net.M[liveCells, liveCells]\n    self.net.numHiddenNeurons = numHiddenNeurons - numDead", "response": "kill cells from the network"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef numenta(self, X, Y):\n    batchSize    = len(X)\n    W            = self.connections.visible_to_hidden\n    n, m         = W.shape\n    bias         = self.connections.hidden_bias\n    incr         = self.weight_incr\n    decr         = self.weight_decr\n    boost_bias   = self.boost_strength_bias\n    alpha        = self.average_activity\n\n    #---------------------------\n    # visible-to-hidden updates\n    #---------------------------\n    for i in range(batchSize):\n        y     = Y[i]\n        x     = X[i]\n        x_bar = np.ones(m) - x   \n\n        # Hebbian-like update\n        W[ np.where(y == 1)[0] ] += incr*x - decr*x_bar \n\n\n    # Clip the visible-to-hidden connections \n    # to be between $0$ and $1$ \n    tooSmall = np.where(W < 0.)\n    tooBig   = np.where(W > 1.)\n    W[ tooSmall ] = 0.\n    W[ tooBig   ] = 1.\n\n\n    #---------------\n    # (Hidden) Bias\n    #---------------\n    for i in range(n):\n        bias[i] = boost_bias * alpha[i,i]", "response": "r Method that updates the network s connections. \n    Numenta s classic update."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef numenta_new_local_inhibition(self, X, Y):\n    batchSize    = len(X)\n    W            = self.connections.visible_to_hidden\n    H            = self.connections.hidden_to_hidden\n    n, m         = W.shape\n    bias         = self.connections.hidden_bias\n    incr         = self.weight_incr\n    decr         = self.weight_decr\n    boost_bias   = self.boost_strength_bias\n    boost_hidden = self.boost_strength_hidden\n    alpha        = self.average_activity\n\n    #---------------------------\n    # visible-to-hidden updates\n    #---------------------------\n    for i in range(batchSize):\n        y     = Y[i]\n        x     = X[i]\n        x_bar = np.ones(m) - x   \n\n        # Hebbian-like update\n        W[ np.where(y == 1)[0] ] += incr*x - decr*x_bar \n\n\n    # Clip the visible-to-hidden connections \n    # to be between $0$ and $1$ \n    tooSmall = np.where(W < 0.)\n    tooBig   = np.where(W > 1.)\n    W[ tooSmall ] = 0.\n    W[ tooBig   ] = 1.\n\n\n    #---------------\n    # (Hidden) Bias\n    #---------------\n    for i in range(n):\n        bias[i] = boost_bias * alpha[i,i]\n\n    #--------------------------\n    # Hidden-to-hidden updates\n    #--------------------------\n    for i in range(n):\n        for j in range(n):\n            H[i,j] =  alpha[i,j] /( np.sum( alpha[i,:] ) - alpha[i,i] )  \n\n    for i in range(n):\n        H[i,i] = 0.", "response": "r This function creates a new local inhibition procedure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the dataset with the given name and returns a DataFrame with the data in the dataset.", "response": "def readDataSet(dataSet, noise=0):\n  \"\"\"\n  :param dataSet: dataset name\n  :param noise: amount of noise added to the dataset\n  :return:\n  \"\"\"\n  filePath = 'data/'+dataSet+'.csv'\n\n  if dataSet == 'nyc_taxi' or dataSet == 'nyc_taxi_perturb' or dataSet == 'nyc_taxi_perturb_baseline':\n    seq = pd.read_csv(filePath, header=0, skiprows=[1, 2], names=['time', 'data', 'timeofday', 'dayofweek'])\n    seq['time'] = pd.to_datetime(seq['time'])\n\n    if noise > 0:\n      for i in xrange(len(seq)):\n        value = seq['data'][i]\n        noiseValue = np.random.normal(scale=(value * noise))\n        value += noiseValue\n        value = max(0, value)\n        value = min(40000, value)\n        seq['data'][i] = value\n  # elif dataSet == 'sine':\n  #   df = pd.read_csv(filePath, header=0, skiprows=[1, 2], names=['time', 'data'])\n  #   sequence = df['data']\n  #   seq = pd.DataFrame(np.array(sequence), columns=['data'])\n  else:\n    raise(' unrecognized dataset type ')\n\n  return seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loadThingObjects(numCorticalColumns=1, objDataPath='./data/'):\n  # create empty simple object machine\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=1024,\n    externalInputSize=1024,\n    numCorticalColumns=numCorticalColumns,\n    numFeatures=0,\n    numLocations=0,\n  )\n\n  for _ in range(numCorticalColumns):\n    objects.locations.append([])\n    objects.features.append([])\n\n  objFiles = []\n  for f in os.listdir(objDataPath):\n    if os.path.isfile(os.path.join(objDataPath, f)):\n      if '.log' in f:\n        objFiles.append(f)\n\n  idx = 0\n  OnBitsList = []\n  for f in objFiles:\n    objName = f.split('.')[0]\n    objName = objName[4:]\n    objFile = open('{}/{}'.format(objDataPath, f))\n\n    sensationList = []\n    for line in objFile.readlines():\n      # parse thing data file and extract feature/location vectors\n      sense = line.split('=>')[1].strip(' ').strip('\\n')\n      OnBitsList.append(float(line.split('] =>')[0].split('/')[1]))\n      location = sense.split('],[')[0].strip('[')\n      feature = sense.split('],[')[1].strip(']')\n      location = np.fromstring(location, sep=',', dtype=np.uint8)\n      feature = np.fromstring(feature, sep=',', dtype=np.uint8)\n\n      # add the current sensation to object Machine\n      sensationList.append((idx, idx))\n      for c in range(numCorticalColumns):\n        objects.locations[c].append(set(location.tolist()))\n        objects.features[c].append(set(feature.tolist()))\n      idx += 1\n    objects.addObject(sensationList, objName)\n    print \"load object file: {} object name: {} sensation # {}\".format(\n      f, objName, len(sensationList))\n  OnBitsList\n  OnBitsList = np.array(OnBitsList)\n\n  plt.figure()\n  plt.hist(OnBitsList)\n  return objects, OnBitsList", "response": "Load simulated sensation data on a number of different objects on a number of different objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef printFrequencyStatistics(counts, frequencies, numWords, size):\n  avgBits = float(counts.sum())/numWords\n  print \"Retina width=128, height=128\"\n  print \"Total number of words processed=\",numWords\n  print \"Average number of bits per word=\",avgBits,\n  print \"avg sparsity=\",avgBits/size\n  print \"counts matrix sum=\",counts.sum(),\n  print \"max=\",counts.max(), \"min=\",counts.min(),\n  print \"mean=\",counts.sum()/float(size)\n  print \"frequency matrix sum=\",frequencies.sum(),\n  print \"max=\",frequencies.max(), \"min=\",frequencies.min(),\n  print \"mean=\",frequencies.sum()/float(size)\n  print \"Number of bits with zero entries\",frequencies.nZeroCols()", "response": "Print interesting statistics regarding the counts and frequency matrices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a uniformly random counts matrix through sampling.", "response": "def countRandomBitFrequencies(numTerms = 100000, percentSparsity = 0.01):\n  \"\"\"Create a uniformly random counts matrix through sampling.\"\"\"\n  # Accumulate counts by inplace-adding sparse matrices\n  counts = SparseMatrix()\n  size = 128*128\n  counts.resize(1, size)\n\n  # Pre-allocate buffer sparse matrix\n  sparseBitmap = SparseMatrix()\n  sparseBitmap.resize(1, size)\n\n  random.seed(42)\n\n  # Accumulate counts for each bit for each word\n  numWords=0\n  for term in xrange(numTerms):\n    bitmap = random.sample(xrange(size), int(size*percentSparsity))\n    bitmap.sort()\n\n    sparseBitmap.setRowFromSparse(0, bitmap, [1]*len(bitmap))\n    counts += sparseBitmap\n    numWords += 1\n\n  # Compute normalized version of counts as a separate matrix\n  frequencies = SparseMatrix()\n  frequencies.resize(1, size)\n  frequencies.copy(counts)\n  frequencies.divide(float(numWords))\n\n  # Wrap up by printing some statistics and then saving the normalized version\n  printFrequencyStatistics(counts, frequencies, numWords, size)\n\n  frequencyFilename = \"bit_frequencies_random.pkl\"\n  print \"Saving frequency matrix in\",frequencyFilename\n  with open(frequencyFilename, \"wb\") as frequencyPickleFile:\n    pickle.dump(frequencies, frequencyPickleFile)\n\n  return counts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the frequency histogram of the active bit.", "response": "def plotlyFrequencyHistogram(counts):\n  \"\"\"\n  x-axis is a count of how many times a bit was active\n  y-axis is number of bits that have that frequency\n  \"\"\"\n  data = [\n    go.Histogram(\n      x=tuple(count for _, _, count in counts.getNonZerosSorted())\n    )\n  ]\n  py.plot(data, filename=os.environ.get(\"HEATMAP_NAME\",\n                                        str(datetime.datetime.now())))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a random tensor that is initialized like a weight matrix Size is outputSize X inputSize where weightSparsity% of each row is non - zero.", "response": "def getSparseTensor(numNonzeros, inputSize, outputSize,\n                    onlyPositive=False,\n                    fixedRange=1.0/24):\n  \"\"\"\n  Return a random tensor that is initialized like a weight matrix\n  Size is outputSize X inputSize, where weightSparsity% of each row is non-zero\n  \"\"\"\n  # Initialize weights in the typical fashion.\n  w = torch.Tensor(outputSize, inputSize, )\n\n  if onlyPositive:\n    w.data.uniform_(0, fixedRange)\n  else:\n    w.data.uniform_(-fixedRange, fixedRange)\n\n  # Zero out weights for sparse weight matrices\n  if numNonzeros < inputSize:\n    numZeros = inputSize - numNonzeros\n\n    outputIndices = np.arange(outputSize)\n    inputIndices = np.array([np.random.permutation(inputSize)[:numZeros]\n                             for _ in outputIndices], dtype=np.long)\n\n    # Create tensor indices for all non-zero weights\n    zeroIndices = np.empty((outputSize, numZeros, 2), dtype=np.long)\n    zeroIndices[:, :, 0] = outputIndices[:, None]\n    zeroIndices[:, :, 1] = inputIndices\n    zeroIndices = torch.LongTensor(zeroIndices.reshape(-1, 2))\n\n    zeroWts = (zeroIndices[:, 0], zeroIndices[:, 1])\n    w.data[zeroWts] = 0.0\n\n  return w"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getPermutedTensors(W, kw, n, m2, noisePct):\n  W2 = W.repeat(m2, 1)\n  nz = W[0].nonzero()\n  numberToZero = int(round(noisePct * kw))\n  for i in range(m2):\n    indices = np.random.permutation(kw)[0:numberToZero]\n    for j in indices:\n      W2[i,nz[j]] = 0\n  return W2", "response": "Generates m2 noisy versions of W with random noise Pct."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nestimates a reasonable value of theta for this k.", "response": "def getTheta(k, nTrials=100000):\n  \"\"\"\n  Estimate a reasonable value of theta for this k.\n  \"\"\"\n  theDots = np.zeros(nTrials)\n  w1 = getSparseTensor(k, k, nTrials, fixedRange=1.0/k)\n  for i in range(nTrials):\n    theDots[i] = w1[i].dot(w1[i])\n\n  dotMean = theDots.mean()\n  print(\"k=\", k, \"min/mean/max diag of w dot products\",\n        theDots.min(), dotMean, theDots.max())\n\n  theta = dotMean / 2.0\n  print(\"Using theta as mean / 2.0 = \", theta)\n\n  return theta, theDots"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef returnMatches(kw, kv, n, theta, inputScaling=1.0):\n  # How many weight vectors and input vectors to generate at a time\n  m1 = 4\n  m2 = 1000\n\n  weights = getSparseTensor(kw, n, m1, fixedRange=1.0 / kw)\n\n  # Initialize random input vectors using given scaling and see how many match\n  inputVectors = getSparseTensor(kv, n, m2,\n                                 onlyPositive=True,\n                                 fixedRange= 2*inputScaling / kw,\n                                 )\n  dot = inputVectors.matmul(weights.t())\n  numMatches = ((dot >= theta).sum()).item()\n  pctMatches = numMatches / float(m1*m2)\n\n  return pctMatches, numMatches, m1*m2", "response": "Returns the percentage that matched number that matched number that matched total match comparisons\n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef returnFalseNegatives(kw, noisePct, n, theta):\n\n  W = getSparseTensor(kw, n, 1, fixedRange=1.0 / kw)\n\n  # Get permuted versions of W and see how many match\n  m2 = 10\n  inputVectors = getPermutedTensors(W, kw, n, m2, noisePct)\n  dot = inputVectors.matmul(W.t())\n\n  numMatches = ((dot >= theta).sum()).item()\n  pctMatches = numMatches / float(m2)\n\n  return pctMatches, numMatches, m2", "response": "Generate a weight vector W with kw non - zero components and return the match statistics."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a number of trials of returnMatches and returns an overall probability of matches given the parameters.", "response": "def computeMatchProbability(args):\n  \"\"\"\n  Runs a number of trials of returnMatches() and returns an overall probability\n  of matches given the parameters.\n\n  :param args is a dictionary containing the following keys:\n\n  kw: k for the weight vectors\n\n  kv: k for the input vectors. If -1, kv is set to n/2\n\n  n:  dimensionality of input vector\n\n  theta: threshold for matching after dot product\n\n  nTrials: number of trials to run\n\n  inputScaling: scale factor for the input vectors. 1.0 means the scaling\n    is the same as the stored weight vectors.\n\n  :return: args updated with the percent that matched\n  \"\"\"\n  kv = args[\"k\"]\n  n = args[\"n\"]\n  kw = args[\"kw\"]\n  theta = args[\"theta\"]\n\n  if kv == -1:\n    kv = int(round(n/2.0))\n\n  numMatches = 0\n  totalComparisons = 0\n  for t in range(args[\"nTrials\"]):\n    pct, num, total = returnMatches(kw, kv, n, theta, args[\"inputScaling\"])\n    numMatches += num\n    totalComparisons += total\n\n  pctMatches = float(numMatches) / totalComparisons\n  print(\"kw, kv, n, s:\", kw, kv, n, args[\"inputScaling\"],\n        \", matches:\", numMatches,\n        \", comparisons:\", totalComparisons,\n        \", pct matches:\", pctMatches)\n\n  args.update({\"pctMatches\": pctMatches})\n\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the impact of S on match probabilities for a fixed value of n.", "response": "def computeScaledProbabilities(\n        listOfScales=[1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0],\n        listofkValues=[64, 128, 256],\n        kw=32,\n        n=1000,\n        numWorkers=10,\n        nTrials=1000,\n  ):\n  \"\"\"\n  Compute the impact of S on match probabilities for a fixed value of n.\n  \"\"\"\n  # Create arguments for the possibilities we want to test\n  args = []\n  theta, _ = getTheta(kw)\n  for ki, k in enumerate(listofkValues):\n    for si, s in enumerate(listOfScales):\n      args.append({\n          \"k\": k, \"kw\": kw, \"n\": n, \"theta\": theta,\n          \"nTrials\": nTrials, \"inputScaling\": s,\n          \"errorIndex\": [ki, si],\n          })\n\n  result = computeMatchProbabilityParallel(args, numWorkers)\n\n  errors = np.zeros((len(listofkValues), len(listOfScales)))\n  for r in result:\n    errors[r[\"errorIndex\"][0], r[\"errorIndex\"][1]] = r[\"pctMatches\"]\n\n  print(\"Errors using scaled inputs, for kw=\", kw)\n  print(repr(errors))\n  plotScaledMatches(listofkValues, listOfScales, errors,\n              \"images/scalar_effect_of_scale_kw\" + str(kw) + \".pdf\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef computeMatchProbabilityOmega(k, bMax, theta, nTrials=100):\n  omegaProb = np.zeros(bMax+1)\n\n  for b in range(1, bMax+1):\n    xwb = getSparseTensor(b, b, nTrials, fixedRange=1.0/k)\n    xib = getSparseTensor(b, b, nTrials, onlyPositive=True, fixedRange=2.0/k)\n    r = xwb.matmul(xib.t())\n    numMatches = ((r >= theta).sum()).item()\n    omegaProb[b] = numMatches / float(nTrials * nTrials)\n\n  print(omegaProb)\n\n  return omegaProb", "response": "This function computes the Omega match probability of matching when a given set of values is found in two vectors with exactly b components in common."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plotMatches2(listofNValues, errors,\n                 listOfScales, scaleErrors,\n                 fileName = \"images/scalar_matches.pdf\"):\n  \"\"\"\n  Plot two figures side by side in an aspect ratio appropriate for the paper.\n  \"\"\"\n  w, h = figaspect(0.4)\n  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(w,h))\n\n  plotMatches(listofNValues, errors, fileName=None, fig=fig, ax=ax1)\n  plotScaledMatches(listOfScales, scaleErrors, fileName=None, fig=fig, ax=ax2)\n\n  plt.savefig(fileName)\n  plt.close()", "response": "Plots two figures side by side in an aspect ratio appropriate for the paper."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createPregeneratedGraphs():\n  # Graph for computeMatchProbabilities(kw=32, nTrials=3000)\n  listofNValues = [250, 500, 1000, 1500, 2000, 2500]\n  kw = 32\n  errors = np.array([\n    [3.65083333e-03, 3.06166667e-04, 1.89166667e-05,\n     4.16666667e-06, 1.50000000e-06, 9.16666667e-07],\n    [2.44633333e-02, 3.64491667e-03, 3.16083333e-04,\n     6.93333333e-05, 2.16666667e-05, 8.66666667e-06],\n    [7.61641667e-02, 2.42496667e-02, 3.75608333e-03,\n     9.78333333e-04, 3.33250000e-04, 1.42250000e-04],\n    [2.31302500e-02, 2.38609167e-02, 2.28072500e-02,\n     2.33225000e-02, 2.30650000e-02, 2.33988333e-02]\n  ])\n\n\n  # Graph for computeScaledProbabilities(nTrials=3000)\n  listOfScales = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n  scaleErrors = np.array([\n    [1.94166667e-05, 1.14900000e-03, 7.20725000e-03, 1.92405833e-02,\n     3.60794167e-02, 5.70276667e-02, 7.88510833e-02],\n    [3.12500000e-04, 7.07616667e-03, 2.71600000e-02, 5.72415833e-02,\n     8.95497500e-02, 1.21294333e-01, 1.50582500e-01],\n    [3.97708333e-03, 3.31468333e-02, 8.04755833e-02, 1.28687750e-01,\n     1.71220000e-01, 2.07019250e-01, 2.34703167e-01]\n  ])\n\n  plotMatches2(listofNValues, errors,\n               listOfScales, scaleErrors,\n               \"images/scalar_matches_kw\" + str(kw) + \".pdf\")", "response": "Creates graphs based on previous runs of the scripts."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears the active cells and learn cells.", "response": "def reset(self):\n    \"\"\"\n    Clear the active cells.\n    \"\"\"\n    self.bumpPhases = np.empty((2,0), dtype=\"float\")\n    self.phaseDisplacement = np.empty((0,2), dtype=\"float\")\n    self.cellsForActivePhases = np.empty(0, dtype=\"int\")\n    self.activeCells = np.empty(0, dtype=\"int\")\n    self.learningCells = np.empty(0, dtype=\"int\")\n    self.sensoryAssociatedCells = np.empty(0, dtype=\"int\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef activateRandomLocation(self):\n    self.bumpPhases = np.array([np.random.random(2)]).T\n    self._computeActiveCells()", "response": "Activates a random location in the sequence of cells."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef movementCompute(self, displacement, noiseFactor = 0):\n\n    if noiseFactor != 0:\n      displacement = copy.deepcopy(displacement)\n      xnoise = np.random.normal(0, noiseFactor)\n      ynoise = np.random.normal(0, noiseFactor)\n      displacement[0] += xnoise\n      displacement[1] += ynoise\n\n    # Calculate delta in the module's coordinates.\n    phaseDisplacement = np.matmul(self.A, displacement)\n\n    # Shift the active coordinates.\n    np.add(self.bumpPhases, phaseDisplacement[:,np.newaxis], out=self.bumpPhases)\n\n    # In Python, (x % 1.0) can return 1.0 because of floating point goofiness.\n    # Generally this doesn't cause problems, it's just confusing when you're\n    # debugging.\n    np.round(self.bumpPhases, decimals=9, out=self.bumpPhases)\n    np.mod(self.bumpPhases, 1.0, out=self.bumpPhases)\n\n    self._computeActiveCells()\n    self.phaseDisplacement = phaseDisplacement", "response": "Compute the current active cells by a vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the location from a sensory input.", "response": "def _sensoryComputeInferenceMode(self, anchorInput):\n    \"\"\"\n    Infer the location from sensory input. Activate any cells with enough active\n    synapses to this sensory input. Deactivate all other cells.\n\n    @param anchorInput (numpy array)\n    A sensory input. This will often come from a feature-location pair layer.\n    \"\"\"\n    if len(anchorInput) == 0:\n      return\n\n    overlaps = self.connections.computeActivity(anchorInput,\n                                                self.connectedPermanence)\n    activeSegments = np.where(overlaps >= self.activationThreshold)[0]\n\n    sensorySupportedCells = np.unique(\n      self.connections.mapSegmentsToCells(activeSegments))\n\n    self.bumpPhases = self.cellPhases[:,sensorySupportedCells]\n    self._computeActiveCells()\n    self.activeSegments = activeSegments\n    self.sensoryAssociatedCells = sensorySupportedCells"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlearning on new segments and grow new synapses.", "response": "def _learn(connections, rng, learningSegments, activeInput,\n             potentialOverlaps, initialPermanence, sampleSize,\n             permanenceIncrement, permanenceDecrement, maxSynapsesPerSegment):\n    \"\"\"\n    Adjust synapse permanences, grow new synapses, and grow new segments.\n\n    @param learningActiveSegments (numpy array)\n    @param learningMatchingSegments (numpy array)\n    @param segmentsToPunish (numpy array)\n    @param activeInput (numpy array)\n    @param potentialOverlaps (numpy array)\n    \"\"\"\n    # Learn on existing segments\n    connections.adjustSynapses(learningSegments, activeInput,\n                               permanenceIncrement, -permanenceDecrement)\n\n    # Grow new synapses. Calculate \"maxNew\", the maximum number of synapses to\n    # grow per segment. \"maxNew\" might be a number or it might be a list of\n    # numbers.\n    if sampleSize == -1:\n      maxNew = len(activeInput)\n    else:\n      maxNew = sampleSize - potentialOverlaps[learningSegments]\n\n    if maxSynapsesPerSegment != -1:\n      synapseCounts = connections.mapSegmentsToSynapseCounts(\n        learningSegments)\n      numSynapsesToReachMax = maxSynapsesPerSegment - synapseCounts\n      maxNew = np.where(maxNew <= numSynapsesToReachMax,\n                        maxNew, numSynapsesToReachMax)\n\n    connections.growSynapsesToSample(learningSegments, activeInput,\n                                     maxNew, initialPermanence, rng)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n    self.activePhases = np.empty((0,2), dtype=\"float\")\n    self.phaseDisplacement = np.empty((0,2), dtype=\"float\")\n    self.cellsForActivePhases = np.empty(0, dtype=\"int\")\n    self.activeCells = np.empty(0, dtype=\"int\")\n    self.sensoryAssociatedCells = np.empty(0, dtype=\"int\")", "response": "Clear the active cells and phase displacement."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef activateRandomLocation(self):\n    self.activePhases = np.array([np.random.random(2)])\n    if self.anchoringMethod == \"discrete\":\n      # Need to place the phase in the middle of a cell\n      self.activePhases = np.floor(\n        self.activePhases * self.cellDimensions)/self.cellDimensions\n    self._computeActiveCells()", "response": "Activates a random location in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the current active cells by a vector.", "response": "def movementCompute(self, displacement, noiseFactor = 0):\n    \"\"\"\n    Shift the current active cells by a vector.\n\n    @param displacement (pair of floats)\n    A translation vector [di, dj].\n    \"\"\"\n\n    if noiseFactor != 0:\n      displacement = copy.deepcopy(displacement)\n      xnoise = np.random.normal(0, noiseFactor)\n      ynoise = np.random.normal(0, noiseFactor)\n      displacement[0] += xnoise\n      displacement[1] += ynoise\n\n\n    # Calculate delta in the module's coordinates.\n    phaseDisplacement = (np.matmul(self.rotationMatrix, displacement) *\n                         self.phasesPerUnitDistance)\n\n    # Shift the active coordinates.\n    np.add(self.activePhases, phaseDisplacement, out=self.activePhases)\n\n    # In Python, (x % 1.0) can return 1.0 because of floating point goofiness.\n    # Generally this doesn't cause problems, it's just confusing when you're\n    # debugging.\n    np.round(self.activePhases, decimals=9, out=self.activePhases)\n    np.mod(self.activePhases, 1.0, out=self.activePhases)\n\n    self._computeActiveCells()\n    self.phaseDisplacement = phaseDisplacement"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the location of a sensory input.", "response": "def _sensoryComputeInferenceMode(self, anchorInput):\n    \"\"\"\n    Infer the location from sensory input. Activate any cells with enough active\n    synapses to this sensory input. Deactivate all other cells.\n\n    @param anchorInput (numpy array)\n    A sensory input. This will often come from a feature-location pair layer.\n    \"\"\"\n    if len(anchorInput) == 0:\n      return\n\n    overlaps = self.connections.computeActivity(anchorInput,\n                                                self.connectedPermanence)\n    activeSegments = np.where(overlaps >= self.activationThreshold)[0]\n\n    sensorySupportedCells = np.unique(\n      self.connections.mapSegmentsToCells(activeSegments))\n\n    inactivated = np.setdiff1d(self.activeCells, sensorySupportedCells)\n    inactivatedIndices = np.in1d(self.cellsForActivePhases,\n                                 inactivated).nonzero()[0]\n    if inactivatedIndices.size > 0:\n      self.activePhases = np.delete(self.activePhases, inactivatedIndices,\n                                    axis=0)\n\n    activated = np.setdiff1d(sensorySupportedCells, self.activeCells)\n\n    # Find centers of point clouds\n    if \"corners\" in self.anchoringMethod:\n      activatedCoordsBase = np.transpose(\n        np.unravel_index(sensorySupportedCells,\n                         self.cellDimensions)).astype('float')\n    else:\n      activatedCoordsBase = np.transpose(\n        np.unravel_index(activated, self.cellDimensions)).astype('float')\n\n    # Generate points to add\n    activatedCoords = np.concatenate(\n      [activatedCoordsBase + [iOffset, jOffset]\n       for iOffset in self.cellCoordinateOffsets\n       for jOffset in self.cellCoordinateOffsets]\n    )\n    if \"corners\" in self.anchoringMethod:\n      self.activePhases = activatedCoords / self.cellDimensions\n\n    else:\n      if activatedCoords.size > 0:\n        self.activePhases = np.append(self.activePhases,\n                                      activatedCoords / self.cellDimensions,\n                                      axis=0)\n\n    self._computeActiveCells()\n    self.activeSegments = activeSegments\n    self.sensoryAssociatedCells = sensorySupportedCells"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassociating this location with a sensory input. Subsequently, anchorInput will activate the current location during anchor(). @param anchorInput (numpy array) A sensory input. This will often come from a feature-location pair layer.", "response": "def _sensoryComputeLearningMode(self, anchorInput):\n    \"\"\"\n    Associate this location with a sensory input. Subsequently, anchorInput will\n    activate the current location during anchor().\n\n    @param anchorInput (numpy array)\n    A sensory input. This will often come from a feature-location pair layer.\n    \"\"\"\n    overlaps = self.connections.computeActivity(anchorInput,\n                                                self.connectedPermanence)\n    activeSegments = np.where(overlaps >= self.activationThreshold)[0]\n\n    potentialOverlaps = self.connections.computeActivity(anchorInput)\n    matchingSegments = np.where(potentialOverlaps >=\n                                self.learningThreshold)[0]\n\n    # Cells with a active segment: reinforce the segment\n    cellsForActiveSegments = self.connections.mapSegmentsToCells(\n      activeSegments)\n    learningActiveSegments = activeSegments[\n      np.in1d(cellsForActiveSegments, self.activeCells)]\n    remainingCells = np.setdiff1d(self.activeCells, cellsForActiveSegments)\n\n    # Remaining cells with a matching segment: reinforce the best\n    # matching segment.\n    candidateSegments = self.connections.filterSegmentsByCell(\n      matchingSegments, remainingCells)\n    cellsForCandidateSegments = (\n      self.connections.mapSegmentsToCells(candidateSegments))\n    candidateSegments = candidateSegments[\n      np.in1d(cellsForCandidateSegments, remainingCells)]\n    onePerCellFilter = np2.argmaxMulti(potentialOverlaps[candidateSegments],\n                                       cellsForCandidateSegments)\n    learningMatchingSegments = candidateSegments[onePerCellFilter]\n\n    newSegmentCells = np.setdiff1d(remainingCells, cellsForCandidateSegments)\n\n    for learningSegments in (learningActiveSegments,\n                             learningMatchingSegments):\n      self._learn(self.connections, self.rng, learningSegments,\n                  anchorInput, potentialOverlaps,\n                  self.initialPermanence, self.sampleSize,\n                  self.permanenceIncrement, self.permanenceDecrement,\n                  self.maxSynapsesPerSegment)\n\n    # Remaining cells without a matching segment: grow one.\n    numNewSynapses = len(anchorInput)\n\n    if self.sampleSize != -1:\n      numNewSynapses = min(numNewSynapses, self.sampleSize)\n\n    if self.maxSynapsesPerSegment != -1:\n      numNewSynapses = min(numNewSynapses, self.maxSynapsesPerSegment)\n\n    newSegments = self.connections.createSegments(newSegmentCells)\n\n    self.connections.growSynapsesToSample(\n      newSegments, anchorInput, numNewSynapses,\n      self.initialPermanence, self.rng)\n    self.activeSegments = activeSegments\n    self.sensoryAssociatedCells = self.activeCells"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the sensor s location relative to a specific object and body s location relative to a specific object and body s location relative to a specific object.", "response": "def metricCompute(self, sensorToBody, bodyToSpecificObject):\n    \"\"\"\n    Compute the\n      \"sensor's location relative to a specific object\"\n    from the\n      \"body's location relative to a specific object\"\n    and the\n      \"sensor's location relative to body\"\n\n    @param sensorToBody (numpy array)\n    Active cells of a single module that represents the sensor's location\n    relative to the body\n\n    @param bodyToSpecificObject (numpy array)\n    Active cells of a single module that represents the body's location relative\n    to a specific object\n    \"\"\"\n    overlaps = self.metricConnections.computeActivity({\n      \"bodyToSpecificObject\": bodyToSpecificObject,\n      \"sensorToBody\": sensorToBody,\n    })\n\n    self.activeMetricSegments = np.where(overlaps >= 2)[0]\n    self.activeCells = np.unique(\n      self.metricConnections.mapSegmentsToCells(\n        self.activeMetricSegments))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef anchorCompute(self, anchorInput, learn):\n    if learn:\n      self._anchorComputeLearningMode(anchorInput)\n    else:\n      overlaps = self.anchorConnections.computeActivity(\n        anchorInput, self.connectedPermanence)\n\n      self.activeSegments = np.where(overlaps >= self.activationThreshold)[0]\n      self.activeCells = np.unique(\n        self.anchorConnections.mapSegmentsToCells(self.activeSegments))", "response": "Compute the the\n    sensor s location relative to a specific object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute(self, egocentricLocation):\n    offsetInCellFields = (np.matmul(self.rotationMatrix, egocentricLocation) *\n                          self.cellFieldsPerUnitDistance)\n\n    np.mod(offsetInCellFields, self.cellDimensions, out=offsetInCellFields)\n    self.activeCells = np.unique(\n      np.ravel_multi_index(np.floor(offsetInCellFields).T.astype('int'),\n                           self.cellDimensions))", "response": "Compute the new active cells from the given sensor location relative to\n    body vector."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking an attempt to determine if a pre - release version of htmresearch - core is installed already.", "response": "def htmresearchCorePrereleaseInstalled():\n  \"\"\"\n  Make an attempt to determine if a pre-release version of htmresearch-core is\n  installed already.\n\n  @return: boolean\n  \"\"\"\n  try:\n    coreDistribution = pkg_resources.get_distribution(\"htmresearch-core\")\n    if pkg_resources.parse_version(coreDistribution.version).is_prerelease:\n      # A pre-release dev version of htmresearch-core is installed.\n      return True\n  except pkg_resources.DistributionNotFound:\n    pass  # Silently ignore.  The absence of htmresearch-core will be handled by\n          # setuptools by default\n\n  return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the requirements. txt file and parse into requirements for setup s install_requirements option.", "response": "def findRequirements():\n  \"\"\"\n  Read the requirements.txt file and parse into requirements for setup's\n  install_requirements option.\n  \"\"\"\n  requirementsPath = os.path.join(REPO_DIR, \"requirements.txt\")\n  requirements = parse_file(requirementsPath)\n\n  # User has a pre-release version of numenta packages installed, which is only\n  # possible if the user installed and built the packages from source and\n  # it is up to the user to decide when to update these packages.  We'll\n  # quietly remove the entries in requirements.txt so as to not conflate the\n  # two.\n  if nupicPrereleaseInstalled():\n    requirements = [req for req in requirements if \"nupic\" not in req]\n\n  if htmresearchCorePrereleaseInstalled():\n    requirements = [req for req in requirements if \"htmresearch-core\" not in req]\n\n  return requirements"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef learnObjects(self, objects, reset=True):\n    self._setLearningMode()\n\n    for objectName, sensationList in objects.iteritems():\n\n      # ignore empty sensation lists\n      if len(sensationList) == 0:\n        continue\n\n      # keep track of numbers of iterations to run\n      iterations = 0\n\n      for sensations in sensationList:\n        # learn each pattern multiple times\n        for _ in xrange(self.numLearningPoints):\n\n          for col in xrange(self.numColumns):\n            location, coarseFeature, fineFeature = sensations[col]\n            self.locationInputs[col].addDataToQueue(list(location), 0, 0)\n            self.coarseSensors[col].addDataToQueue(list(coarseFeature), 0, 0)\n            self.sensors[col].addDataToQueue(list(fineFeature), 0, 0)\n          iterations += 1\n\n      # actually learn the objects\n      if iterations > 0:\n        self.network.run(iterations)\n\n      # update L2 and L5 representations for this object\n      self.objectRepresentationsL2[objectName] = self.getL2Representations()\n      self.objectRepresentationsL5[objectName] = self.getL5Representations()\n\n      if reset:\n        # send reset signal\n        self._sendReset()", "response": "Learns all provided objects and resets the network."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninfer on a given set of sensations for a single object.", "response": "def infer(self, sensationList, reset=True, objectName=None):\n    \"\"\"\n    Infer on a given set of sensations for a single object.\n\n    The provided sensationList is a list of sensations, and each sensation is a\n    mapping from cortical column to a tuple of three SDR's respectively\n    corresponding to the locationInput, the coarseSensorInput, and the\n    sensorInput.\n\n    For example, the input can look as follows, if we are inferring a simple\n    object with two sensations (with very few active bits for simplicity):\n\n    sensationList = [\n        {\n          # location, coarse feature, fine feature for CC0, sensation 1\n          0: ( [1, 5, 10], [9, 32, 75], [6, 12, 52] ),\n          # location, coarse feature, fine feature for CC1, sensation 1\n          1: ( [6, 2, 15], [11, 42, 92], [7, 11, 50] ),\n        },\n        {\n          # location, coarse feature, fine feature for CC0, sensation 2\n          0: ( [2, 9, 10], [10, 35, 78], [6, 12, 52] ),\n          # location, coarse feature, fine feature for CC1, sensation 2\n          1: ( [1, 4, 12], [10, 32, 52], [6, 10, 52] ),\n        },\n    ]\n\n    If the object is known by the caller, an object name can be specified\n    as an optional argument, and must match the objects given while learning.\n    This is used later when evaluating inference statistics.\n\n    Parameters:\n    ----------------------------\n    @param   objects (dict)\n             Objects to learn, in the canonical format specified above\n\n    @param   reset (bool)\n             If set to True (which is the default value), the network will\n             be reset after learning.\n\n    @param   objectName (str)\n             Name of the objects (must match the names given during learning).\n\n    \"\"\"\n    self._unsetLearningMode()\n    statistics = collections.defaultdict(list)\n\n    if objectName is not None:\n      if objectName not in self.objectRepresentationsL2:\n        raise ValueError(\"The provided objectName was not given during\"\n                         \" learning\")\n\n    for sensations in sensationList:\n\n      # feed all columns with sensations\n      for col in xrange(self.numColumns):\n        location, coarseFeature, fineFeature = sensations[col]\n        self.locationInputs[col].addDataToQueue(list(location), 0, 0)\n        self.coarseSensors[col].addDataToQueue(list(coarseFeature), 0, 0)\n        self.sensors[col].addDataToQueue(list(fineFeature), 0, 0)\n      self.network.run(1)\n      self._updateInferenceStats(statistics, objectName)\n\n    if reset:\n      # send reset signal\n      self._sendReset()\n\n    # save statistics\n    statistics[\"numSteps\"] = len(sensationList)\n    statistics[\"object\"] = objectName if objectName is not None else \"Unknown\"\n    self.statistics.append(statistics)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a reset signal to the network.", "response": "def _sendReset(self, sequenceId=0):\n    \"\"\"\n    Sends a reset signal to the network.\n    \"\"\"\n    # Handle logging - this has to be done first\n    if self.logCalls:\n      frame = inspect.currentframe()\n      args, _, _, values = inspect.getargvalues(frame)\n      values.pop('frame')\n      values.pop('self')\n      (_, filename,\n       _, _, _, _) = inspect.getouterframes(inspect.currentframe())[1]\n      if os.path.splitext(os.path.basename(__file__))[0] != \\\n         os.path.splitext(os.path.basename(filename))[0]:\n        self.callLog.append([inspect.getframeinfo(frame)[2], values])\n\n    for col in xrange(self.numColumns):\n      self.locationInputs[col].addResetToQueue(sequenceId)\n      self.coarseSensors[col].addResetToQueue(sequenceId)\n      self.sensors[col].addResetToQueue(sequenceId)\n    self.network.run(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a good default set of parameters to use in L2456 regions", "response": "def getDefaultParams(self):\n    \"\"\"\n    Returns a good default set of parameters to use in L2456 regions\n    \"\"\"\n    return   {\n      \"sensorParams\": {\n        \"outputWidth\": self.sensorInputSize,\n      },\n\n      \"coarseSensorParams\": {\n        \"outputWidth\": self.sensorInputSize,\n      },\n\n      \"locationParams\": {\n        \"activeBits\": 41,\n        \"outputWidth\": self.sensorInputSize,\n        \"radius\": 2,\n        \"verbosity\": 0,\n      },\n\n      \"L4Params\": {\n        \"columnCount\": self.sensorInputSize,\n        \"cellsPerColumn\": 8,\n        \"learn\": True,\n        \"learnOnOneCell\": False,\n        \"initialPermanence\": 0.51,\n        \"connectedPermanence\": 0.6,\n        \"permanenceIncrement\": 0.1,\n        \"permanenceDecrement\": 0.02,\n        \"minThreshold\": 10,\n        \"basalPredictedSegmentDecrement\": 0.002,\n        \"activationThreshold\": 13,\n        \"sampleSize\": 20,\n        \"implementation\": \"ApicalTiebreakCPP\",\n      },\n\n      \"L2Params\": {\n        \"inputWidth\": self.sensorInputSize * 8,\n        \"cellCount\": 4096,\n        \"sdrSize\": 40,\n        \"synPermProximalInc\": 0.1,\n        \"synPermProximalDec\": 0.001,\n        \"initialProximalPermanence\": 0.6,\n        \"minThresholdProximal\": 10,\n        \"sampleSizeProximal\": 20,\n        \"connectedPermanenceProximal\": 0.5,\n        \"synPermDistalInc\": 0.1,\n        \"synPermDistalDec\": 0.001,\n        \"initialDistalPermanence\": 0.41,\n        \"activationThresholdDistal\": 13,\n        \"sampleSizeDistal\": 20,\n        \"connectedPermanenceDistal\": 0.5,\n        \"learningMode\": True,\n      },\n\n      \"L6Params\": {\n        \"columnCount\": self.sensorInputSize,\n        \"cellsPerColumn\": 8,\n        \"learn\": True,\n        \"learnOnOneCell\": False,\n        \"initialPermanence\": 0.51,\n        \"connectedPermanence\": 0.6,\n        \"permanenceIncrement\": 0.1,\n        \"permanenceDecrement\": 0.02,\n        \"minThreshold\": 10,\n        \"basalPredictedSegmentDecrement\": 0.004,\n        \"activationThreshold\": 13,\n        \"sampleSize\": 20,\n      },\n\n      \"L5Params\": {\n        \"inputWidth\": self.sensorInputSize * 8,\n        \"cellCount\": 4096,\n        \"sdrSize\": 40,\n        \"synPermProximalInc\": 0.1,\n        \"synPermProximalDec\": 0.001,\n        \"initialProximalPermanence\": 0.6,\n        \"minThresholdProximal\": 10,\n        \"sampleSizeProximal\": 20,\n        \"connectedPermanenceProximal\": 0.5,\n        \"synPermDistalInc\": 0.1,\n        \"synPermDistalDec\": 0.001,\n        \"initialDistalPermanence\": 0.41,\n        \"activationThresholdDistal\": 13,\n        \"sampleSizeDistal\": 20,\n        \"connectedPermanenceDistal\": 0.5,\n        \"learningMode\": True,\n      },\n\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve and store Python region instances for each column in the archive.", "response": "def _retrieveRegions(self):\n    \"\"\"\n    Retrieve and store Python region instances for each column\n    \"\"\"\n    self.sensors = []\n    self.coarseSensors = []\n    self.locationInputs = []\n    self.L4Columns = []\n    self.L2Columns = []\n    self.L5Columns = []\n    self.L6Columns = []\n\n    for i in xrange(self.numColumns):\n      self.sensors.append(\n        self.network.regions[\"sensorInput_\" + str(i)].getSelf()\n      )\n      self.coarseSensors.append(\n        self.network.regions[\"coarseSensorInput_\" + str(i)].getSelf()\n      )\n      self.locationInputs.append(\n        self.network.regions[\"locationInput_\" + str(i)].getSelf()\n      )\n      self.L4Columns.append(\n        self.network.regions[\"L4Column_\" + str(i)].getSelf()\n      )\n      self.L2Columns.append(\n        self.network.regions[\"L2Column_\" + str(i)].getSelf()\n      )\n      self.L5Columns.append(\n        self.network.regions[\"L5Column_\" + str(i)].getSelf()\n      )\n      self.L6Columns.append(\n        self.network.regions[\"L6Column_\" + str(i)].getSelf()\n      )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unsetLearningMode(self):\n\n    for column in self.L4Columns:\n      column.setParameter(\"learn\", 0, False)\n    for column in self.L6Columns:\n      column.setParameter(\"learn\", 0, False)\n\n    for column in self.L2Columns:\n      column.setParameter(\"learningMode\", 0, False)\n    for column in self.L5Columns:\n      column.setParameter(\"learningMode\", 0, False)", "response": "Unsets the learning mode to start inference."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the learning mode.", "response": "def _setLearningMode(self):\n    \"\"\"\n    Sets the learning mode.\n    \"\"\"\n    for column in self.L4Columns:\n      column.setParameter(\"learn\", 0, True)\n    for column in self.L6Columns:\n      column.setParameter(\"learn\", 0, True)\n\n    for column in self.L2Columns:\n      column.setParameter(\"learningMode\", 0, True)\n    for column in self.L5Columns:\n      column.setParameter(\"learningMode\", 0, True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _updateInferenceStats(self, statistics, objectName=None):\n    L4Representations = self.getL4Representations()\n    L4PredictedCells = self.getL4PredictedCells()\n    L2Representations = self.getL2Representations()\n    L5Representations = self.getL5Representations()\n    L6Representations = self.getL6Representations()\n    L6PredictedCells = self.getL6PredictedCells()\n\n    for i in xrange(self.numColumns):\n      statistics[\"L4 Representation C\" + str(i)].append(\n        len(L4Representations[i])\n      )\n      statistics[\"L4 Predicted C\" + str(i)].append(\n        len(L4PredictedCells[i])\n      )\n      statistics[\"L2 Representation C\" + str(i)].append(\n        len(L2Representations[i])\n      )\n\n      statistics[\"L6 Representation C\" + str(i)].append(\n        len(L6Representations[i])\n      )\n      statistics[\"L6 Predicted C\" + str(i)].append(\n        len(L6PredictedCells[i])\n      )\n      statistics[\"L5 Representation C\" + str(i)].append(\n        len(L5Representations[i])\n      )\n\n      # add true overlap if objectName was provided\n      if objectName is not None:\n        objectRepresentationL2 = self.objectRepresentationsL2[objectName]\n        statistics[\"Overlap L2 with object C\" + str(i)].append(\n          len(objectRepresentationL2[i] & L2Representations[i])\n        )\n\n        objectRepresentationL5 = self.objectRepresentationsL5[objectName]\n        statistics[\"Overlap L5 with object C\" + str(i)].append(\n          len(objectRepresentationL5[i] & L5Representations[i])\n        )", "response": "Updates the inference statistics for the current language."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef n_to_one(num_inputs, num_streams, bits_per_axis, weight_per_axis):\n    num_bits = num_streams*bits_per_axis\n    # Setting up the scalar encoder\n    encoder_params = {\n        \"dimensions\"      : num_streams, \n        \"max_values\"      : [[0.,1.]]*num_streams,\n        \"bits_per_axis\"   : [bits_per_axis]*num_streams, \n        \"weight_per_axis\" : [weight_per_axis]*num_streams,\n        \"wrap_around\"     : False\n    }\n    scalar_encode = ScalarEncoder(**encoder_params)\n\n    xs = np.random.sample(num_inputs)\n    ys = np.random.sample(num_inputs)\n    input_vectors = np.zeros((num_inputs, num_bits))\n\n    for i in range(num_inputs):\n        input_vectors[i] = scalar_encode([xs[i]] + [ys[i]]*(num_streams - 1) )\n    \n    return input_vectors", "response": "Creates a number of inputs for a toy version of a resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a dictionary of random object descriptions.", "response": "def createRandomObjectDescriptions(numObjects,\n                                   numLocationsPerObject,\n                                   featurePool=(\"A\", \"B\", \"C\")):\n  \"\"\"\n  Returns {\"Object 1\": [(0, \"C\"), (1, \"B\"), (2, \"C\"), ...],\n           \"Object 2\": [(0, \"C\"), (1, \"A\"), (2, \"B\"), ...]}\n  \"\"\"\n  return dict((\"Object %d\" % i,\n               zip(xrange(numLocationsPerObject),\n                   [random.choice(featurePool)\n                    for _ in xrange(numLocationsPerObject)]))\n              for i in xrange(1, numObjects + 1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef doExperiment(numColumns, l2Overrides, objectDescriptions, noiseMu,\n                 noiseSigma, numInitialTraversals, noiseEverywhere):\n  \"\"\"\n  Touch every point on an object 'numInitialTraversals' times, then evaluate\n  whether it has inferred the object by touching every point once more and\n  checking the number of correctly active and incorrectly active cells.\n\n  @param numColumns (int)\n  The number of sensors to use\n\n  @param l2Overrides (dict)\n  Parameters for the ColumnPooler\n\n  @param objectDescriptions (dict)\n  A mapping of object names to their feature-locations.\n  See 'createRandomObjectDescriptions'.\n\n  @param noiseMu (float)\n  The average amount of noise in a feedforward input. The noise level for each\n  column's input is determined once per touch. It is a gaussian distribution\n  with mean 'noiseMu' and sigma 'noiseSigma'.\n\n  @param noiseSigma (float)\n  The sigma for the gaussian distribution of noise levels. If the noiseSigma is\n  0, then the noise level will always be 'noiseMu'.\n\n  @param numInitialTraversals (int)\n  The number of times to traverse the object before testing whether the object\n  has been inferred.\n\n  @param noiseEverywhere (bool)\n  If true, add noise to every column's input, and record accuracy of every\n  column. If false, add noise to one column's input, and only record accuracy\n  of that column.\n  \"\"\"\n\n  # For each column, keep a mapping from feature-location names to their SDRs\n  layer4sdr = lambda : np.array(sorted(random.sample(xrange(L4_CELL_COUNT),\n                                                     40)), dtype=\"uint32\")\n  featureLocationSDRs = [defaultdict(layer4sdr) for _ in xrange(numColumns)]\n\n  params = {\"inputWidth\": L4_CELL_COUNT,\n            \"lateralInputWidths\": [4096]*(numColumns-1),\n            \"seed\": random.randint(0, 1024)}\n  params.update(l2Overrides)\n\n  l2Columns = [ColumnPooler(**params)\n               for _ in xrange(numColumns)]\n\n  # Learn the objects\n  objectL2Representations = {}\n  for objectName, featureLocations in  objectDescriptions.iteritems():\n    for featureLocationName in featureLocations:\n      # Touch it enough times for the distal synapses to reach the\n      # connected permanence, and then once more.\n      for _ in xrange(4):\n        allLateralInputs = [l2.getActiveCells() for l2 in l2Columns]\n        for columnNumber, l2 in enumerate(l2Columns):\n          feedforwardInput = featureLocationSDRs[columnNumber][featureLocationName]\n          lateralInputs = [lateralInput\n                           for i, lateralInput in enumerate(allLateralInputs)\n                           if i != columnNumber]\n          l2.compute(feedforwardInput, lateralInputs, learn=True)\n    objectL2Representations[objectName] = [set(l2.getActiveCells())\n                                           for l2 in l2Columns]\n    for l2 in l2Columns:\n      l2.reset()\n\n  results = []\n\n  # Try to infer the objects\n  for objectName, featureLocations in objectDescriptions.iteritems():\n    for l2 in l2Columns:\n      l2.reset()\n\n    sensorPositionsIterator = greedySensorPositions(numColumns, len(featureLocations))\n\n    # Touch each location at least numInitialTouches times, and then touch it\n    # once more, testing it. For each traversal, touch each point on the object\n    # ~once. Not once per sensor -- just once. So we translate the \"number of\n    # traversals\" into a \"number of touches\" according to the number of sensors.\n    numTouchesPerTraversal = len(featureLocations) / float(numColumns)\n    numInitialTouches = int(math.ceil(numInitialTraversals * numTouchesPerTraversal))\n\n    if noiseEverywhere:\n      numTestTouches = int(math.ceil(1 * numTouchesPerTraversal))\n    else:\n      numTestTouches = len(featureLocations)\n\n    for touch in xrange(numInitialTouches + numTestTouches):\n      sensorPositions = next(sensorPositionsIterator)\n\n      # Give the system a few timesteps to settle, allowing lateral connections\n      # to cause cells to be inhibited.\n      for _ in xrange(3):\n        allLateralInputs = [l2.getActiveCells() for l2 in l2Columns]\n        for columnNumber, l2 in enumerate(l2Columns):\n          position = sensorPositions[columnNumber]\n          featureLocationName = featureLocations[position]\n          feedforwardInput = featureLocationSDRs[columnNumber][featureLocationName]\n\n          if noiseEverywhere or columnNumber == 0:\n            noiseLevel = random.gauss(noiseMu, noiseSigma)\n            noiseLevel = max(0.0, min(1.0, noiseLevel))\n            feedforwardInput = noisy(feedforwardInput, noiseLevel, L4_CELL_COUNT)\n\n          lateralInputs = [lateralInput\n                           for i, lateralInput in enumerate(allLateralInputs)\n                           if i != columnNumber]\n\n          l2.compute(feedforwardInput, lateralInputs, learn=False)\n\n      if touch >= numInitialTouches:\n        if noiseEverywhere:\n          for columnNumber, l2 in enumerate(l2Columns):\n            activeCells = set(l2.getActiveCells())\n            correctCells = objectL2Representations[objectName][columnNumber]\n            results.append((len(activeCells & correctCells),\n                            len(activeCells - correctCells)))\n        else:\n          activeCells = set(l2Columns[0].getActiveCells())\n          correctCells = objectL2Representations[objectName][0]\n          results.append((len(activeCells & correctCells),\n                          len(activeCells - correctCells)))\n\n  return results", "response": "This function is used to perform the experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns and plot the experiment and varying the number of cortical columns.", "response": "def plotSuccessRate_varyNumColumns(noiseSigma, noiseEverywhere):\n  \"\"\"\n  Run and plot the experiment, varying the number of cortical columns.\n  \"\"\"\n\n  #\n  # Run the experiment\n  #\n  noiseLevels = [x * 0.01 for x in xrange(0, 101, 5)]\n  l2Overrides = {\"sampleSizeDistal\": 20}\n  columnCounts = [1, 2, 3, 4]\n\n  results = defaultdict(list)\n\n  for trial in xrange(1):\n    print \"trial\", trial\n    objectDescriptions = createRandomObjectDescriptions(10, 10)\n\n    for numColumns in columnCounts:\n      print \"numColumns\", numColumns\n      for noiseLevel in noiseLevels:\n        r = doExperiment(numColumns, l2Overrides, objectDescriptions,\n                         noiseLevel, noiseSigma, numInitialTraversals=6,\n                         noiseEverywhere=noiseEverywhere)\n        results[(numColumns, noiseLevel)].extend(r)\n\n  #\n  # Plot it\n  #\n  numCorrectActiveThreshold = 30\n  numIncorrectActiveThreshold = 10\n\n  plt.figure()\n  colors = dict(zip(columnCounts,\n                    ('r', 'k', 'g', 'b')))\n  markers = dict(zip(columnCounts,\n                     ('o', '*', 'D', 'x')))\n\n  for numColumns in columnCounts:\n    y = []\n    for noiseLevel in noiseLevels:\n      trials = results[(numColumns, noiseLevel)]\n      numPassed = len([True for numCorrect, numIncorrect in trials\n                       if numCorrect >= numCorrectActiveThreshold\n                       and numIncorrect <= numIncorrectActiveThreshold])\n      y.append(numPassed / float(len(trials)))\n\n    plt.plot(noiseLevels, y,\n             color=colors[numColumns],\n             marker=markers[numColumns])\n\n  lgnd = plt.legend([\"%d columns\" % numColumns\n                     for numColumns in columnCounts],\n                    bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n  plt.xlabel(\"Mean feedforward noise level\")\n  plt.xticks([0.01 * n for n in xrange(0, 101, 10)])\n  plt.ylabel(\"Success rate\")\n  plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n  plt.title(\"Inference with normally distributed noise (stdev=%.2f)\" % noiseSigma)\n\n  plotPath = os.path.join(\"plots\",\n                          \"successRate_varyColumnCount_sigma%.2f_%s.pdf\"\n                          % (noiseSigma, time.strftime(\"%Y%m%d-%H%M%S\")))\n  plt.savefig(plotPath, bbox_extra_artists=(lgnd,), bbox_inches=\"tight\")\n  print \"Saved file %s\" % plotPath"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logCellActivity_varyNumColumns(noiseSigma, noiseEverywhere):\n  noiseLevels = [0.30, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]\n  l2Overrides = {\"sampleSizeDistal\": 20}\n  columnCounts = [1, 2, 3, 4, 5]\n\n  results = defaultdict(list)\n\n  for trial in xrange(1):\n    print \"trial\", trial\n    objectDescriptions = createRandomObjectDescriptions(10, 10)\n\n    for numColumns in columnCounts:\n      print \"numColumns\", numColumns\n      for noiseLevel in noiseLevels:\n        r = doExperiment(numColumns, l2Overrides, objectDescriptions,\n                         noiseLevel, noiseSigma, numInitialTraversals=6,\n                         noiseEverywhere=noiseEverywhere)\n        results[(numColumns, noiseLevel)].extend(r)\n\n  d = []\n  for (numColumns, noiseLevel), cellCounts in results.iteritems():\n    d.append({\"numColumns\": numColumns,\n              \"noiseLevel\": noiseLevel,\n              \"results\": cellCounts})\n\n  filename = os.path.join(\"plots\",\n                          \"varyColumns_sigma%.2f_%s.json\"\n                          % (noiseSigma, time.strftime(\"%Y%m%d-%H%M%S\")))\n  with open(filename, \"w\") as fout:\n    json.dump(d, fout)\n\n  print \"Wrote to\", filename\n  print \"Visualize this file at: http://numenta.github.io/htmresearch/visualizations/grid-of-scatterplots/L2-columns-with-noise.html\"", "response": "Runs the experiment and saves the number of columns that are not yet active and the number of cells that are not yet active."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a list of sensations return the SDRs that would be obtained by numTraversals random traversals of that set of sensations.", "response": "def randomTraversal(sensations, numTraversals):\n    \"\"\"\n    Given a list of sensations, return the SDRs that would be obtained by\n    numTraversals random traversals of that set of sensations.\n\n    Each sensation is a dict mapping cortical column index to a pair of SDR's\n    (one location and one feature).\n    \"\"\"\n    newSensations = []\n    for _ in range(numTraversals):\n      s = copy.deepcopy(sensations)\n      random.shuffle(s)\n      newSensations += s\n    return newSensations"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the overlap between each pair of objects.", "response": "def objectConfusion(self):\n    \"\"\"\n    Compute overlap between each pair of objects.  Computes the average number\n    of feature/location pairs that are identical, as well as the average number\n    of shared locations and features.\n\n    This function will raise an exception if two objects are identical.\n\n    Returns the tuple:\n      (avg common pairs, avg common locations, avg common features)\n    \"\"\"\n    objects = self.getObjects()\n\n    if len(objects) == 0:\n      return 0.0, 0.0, 0.0\n\n    sumCommonLocations = 0\n    sumCommonFeatures = 0\n    sumCommonPairs = 0\n    numObjects = 0\n    commonPairHistogram = numpy.zeros(len(objects[0]), dtype=numpy.int32)\n    for o1, s1 in objects.iteritems():\n      for o2, s2 in objects.iteritems():\n        if o1 != o2:\n          # Count number of common locations id's and common feature id's\n          commonLocations = 0\n          commonFeatures = 0\n          for pair1 in s1:\n            for pair2 in s2:\n              if pair1[0] == pair2[0]: commonLocations += 1\n              if pair1[1] == pair2[1]: commonFeatures += 1\n\n          # print \"Confusion\",o1,o2,\", common pairs=\",len(set(s1)&set(s2)),\n          # print \", common locations=\",commonLocations,\"common features=\",commonFeatures\n\n          if len(set(s1) & set(s2)) == len(s1):\n            raise RuntimeError(\"Two objects are identical!\")\n\n          sumCommonPairs += len(set(s1) & set(s2))\n          sumCommonLocations += commonLocations\n          sumCommonFeatures += commonFeatures\n          commonPairHistogram[len(set(s1) & set(s2))] += 1\n          numObjects += 1\n\n    # print \"Common pair histogram=\", commonPairHistogram\n\n    return (sumCommonPairs / float(numObjects),\n            sumCommonLocations / float(numObjects),\n            sumCommonFeatures / float(numObjects)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _checkObjectsToLearn(self, objects):\n    for objectName, sensationList in objects.iteritems():\n      if objectName not in self.objects:\n        raise ValueError(\n          \"Invalid object name \\\"{}\\\" sent to experiment\".format(objectName)\n        )\n\n      for sensations in sensationList:\n        if set(sensations.keys()) != set(range(self.numColumns)):\n          raise ValueError(\n            \"Invalid number of cortical column sensations sent to experiment\"\n          )\n        for pair in sensations.values():\n          if not isinstance(pair, tuple) or len(pair) != 2 or \\\n                  not isinstance(pair[0], set) or \\\n                  not isinstance(pair[1], set):\n            raise ValueError(\"Invalid SDR's sent to experiment\")", "response": "Checks that the objects sent to the Learn experiment are in the correct format before being sent to the Learn experiment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the objects sent to the specified experiment are in the correct format before being sent to the specified experiment.", "response": "def _checkObjectToInfer(self, sensationList):\n    \"\"\"\n    Checks that objects have the correct format before being sent to the\n    experiment.\n    \"\"\"\n    for sensations in sensationList:\n      if set(sensations.keys()) != set(range(self.numColumns)):\n        raise ValueError(\n          \"Invalid number of cortical column sensations sent to experiment\"\n        )\n      for pair in sensations.values():\n        if not isinstance(pair, tuple) or len(pair) != 2 or \\\n                not isinstance(pair[0], set) or \\\n                not isinstance(pair[1], set):\n          raise ValueError(\"Invalid SDR's sent to experiment\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a random SDR with specified number of bits and total size.", "response": "def _generatePattern(numBits, totalSize):\n    \"\"\"\n    Generates a random SDR with specified number of bits and total size.\n    \"\"\"\n    indices = random.sample(xrange(totalSize), numBits)\n    return set(indices)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the two vectors of the two given points.", "response": "def compute(self, xt1, yt1, xt, yt, theta1t1, theta2t1, theta1, theta2):\n    \"\"\"\n    Accumulate the various inputs.\n    \"\"\"\n    dx = xt - xt1\n    dy = yt - yt1\n\n    if self.numPoints < self.maxPoints:\n      self.dxValues[self.numPoints,0] = dx\n      self.dxValues[self.numPoints,1] = dy\n\n      self.thetaValues[self.numPoints,0] = theta1\n      self.thetaValues[self.numPoints,1] = theta2\n\n      self.numPoints += 1\n\n      # print >>sys.stderr, \"Xt's: \", xt1, yt1, xt, yt, \"Delta's: \", dx, dy\n      # print >>sys.stderr, \"Theta t-1: \", theta1t1, theta2t1, \"t:\",theta1, theta2\n\n    elif self.numPoints == self.maxPoints:\n      print >> sys.stderr,\"Max points exceeded, analyzing \",self.maxPoints,\"points only\"\n      self.numPoints += 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(cell1, cell2, moduleDimensions):\n  cell1Coords = np.unravel_index(cell1, moduleDimensions)\n  cell2Coords = np.unravel_index(cell2, moduleDimensions)\n  transformCoords = [(c2 - c1) % m\n                     for c1, c2, m in itertools.izip(cell1Coords, cell2Coords,\n                                                     moduleDimensions)]\n  return np.ravel_multi_index(transformCoords, moduleDimensions)", "response": "Return the transform index for given cells."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the index corresponding to the other half of the transform.", "response": "def unbind(cell1, transform, moduleDimensions):\n  \"\"\"Return the cell index corresponding to the other half of the transform.\n\n  Assumes that `transform = bind(cell1, cell2)` and, given `cell1` and\n  `transform`, returns `cell2`.\n  \"\"\"\n  cell1Coords = np.unravel_index(cell1, moduleDimensions)\n  transformCoords = np.unravel_index(transform, moduleDimensions)\n  cell2Coords = [(t + c1) % m\n                 for c1, t, m in itertools.izip(cell1Coords, transformCoords,\n                                                moduleDimensions)]\n  return np.ravel_multi_index(cell2Coords, moduleDimensions)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns one iteration of the online sensorimotor algorithm. This function has three stages: - The FEEDFORWARD pass drives Prerequisites: `trainFeatures` must have been run already :param ff: feedforward sensory input :param motor: the motor command for next move, in the form of delta coordinates :param objClass: the object class to train the classifier, or None if not learning", "response": "def compute(self, ff, motor, objClass, outputFile):\n    \"\"\"Run one iteration of the online sensorimotor algorithm.\n\n    This function has three stages:\n\n    - The FEEDFORWARD pass drives\n\n    Prerequisites: `trainFeatures` must have been run already\n\n    :param ff: feedforward sensory input\n    :param motor: the motor command for next move, in the form of delta\n        coordinates\n    :param objClass: the object class to train the classifier, or None\n        if not learning\n    \"\"\"\n    delta = motor\n\n    # FEEDFORWARD\n\n    # Determine active feature representation in l4, using lateral input\n    # from l6 previous step feedback\n    l4BasalInput = getGlobalIndices(self.predictedL6Cells, self._cellsPerModule)\n    self.l4TM.compute(activeColumns=ff, basalInput=l4BasalInput,\n                      learn=False)\n    predictedL4Cells = self.l4TM.getPredictedCells()\n    activeL4Cells = self.l4TM.getActiveCells()\n\n    # Drive L6 activation from l4\n    for m, connections in enumerate(self.l6Connections):\n      newCells = []\n      activeConnectedPerSegment = connections.computeActivity(activeL4Cells, 0.5)[0]\n      for flatIdx, activeConnected in enumerate(activeConnectedPerSegment):\n        if activeConnected >= self.l6ActivationThreshold:\n          cellIdx = connections.segmentForFlatIdx(flatIdx).cell\n          newCells.append(cellIdx)\n\n      #for cell in newCells:\n      #  print connections.segmentsForCell(cell)\n      #print newCells\n      #assert len(newCells) <= 1\n\n      self.activeL6Cells[m].insert(0, newCells)\n      # TODO: This is the number of steps, not necessarily the number of cells\n      lenBefore = len(self.activeL6Cells[m])\n      del self.activeL6Cells[m][self.maxActivePerModule:]\n      lenAfter = len(self.activeL6Cells[m])\n      #assert lenBefore == lenAfter, \"Debug assert to check that we aren't hitting limit on L6 activity. Can remove when we set max active low enough relative to object size (times number of train/test iterations)\"\n\n    self.activeL6BeforeMotor = [list(itertools.chain(*l6Module))\n                                for l6Module in self.activeL6Cells]\n\n    # Replace l5 activity with new transforms\n    self.activeL5Cells = []\n    for activeL6Module in self.activeL6Cells:\n      transforms = set()\n      for newCell in activeL6Module[0]:\n        for prevCell in itertools.chain(*activeL6Module[1:]):\n          if newCell == prevCell:\n            continue\n          # Transform from prev to new\n          t1 = bind(prevCell, newCell, self.moduleDimensions)\n          transforms.add(t1)\n          # Transform from new to prev\n          t2 = bind(newCell, prevCell, self.moduleDimensions)\n          transforms.add(t2)\n      self.activeL5Cells.append(list(transforms))\n\n\n    # Pool into object representation\n    globalL5ActiveCells = getGlobalIndices(self.activeL5Cells, self._cellsPerModule)\n    denseL5 = np.zeros(self._cellsPerModule * self.numModules, dtype=\"bool\")\n    denseL5[globalL5ActiveCells] = 1\n    self.prediction = self.classifier.infer(denseL5)\n    if objClass is not None:\n      self.classifier.learn(denseL5, objClass)\n\n    #print globalL5ActiveCells\n\n    # MOTOR\n\n    # Update L6 based on motor command\n    numActivePerModuleBefore = [sum([len(cells) for cells in active]) for active in self.activeL6Cells]\n\n    self.activeL6Cells = [\n        [[pathIntegrate(c, self.moduleDimensions, delta)\n          for c in steps]\n         for steps in prevActiveCells]\n        for prevActiveCells in self.activeL6Cells]\n\n    numActivePerModuleAfter = [sum([len(cells) for cells in active]) for active in self.activeL6Cells]\n    assert numActivePerModuleAfter == numActivePerModuleBefore\n\n    # FEEDBACK\n\n    # Get all transforms associated with object\n    # TODO: Get transforms from object in addition to current activity\n    predictiveTransforms = [l5Active for l5Active in self.activeL5Cells]\n\n    # Get set of predicted l6 representations (including already active)\n    # and store them for next step l4 compute\n    self.predictedL6Cells = []\n    for l6, l5 in itertools.izip(self.activeL6Cells, predictiveTransforms):\n      predictedCells = []\n      for activeL6Cell in set(itertools.chain(*l6)):\n        for activeL5Cell in l5:\n          predictedCell = unbind(activeL6Cell, activeL5Cell, self.moduleDimensions)\n          predictedCells.append(predictedCell)\n      self.predictedL6Cells.append(set(\n          list(itertools.chain(*l6)) + predictedCells))\n\n    # Log this step\n    if outputFile:\n      log = RelationalMemoryLog.new_message()\n      log.ts = time.time()\n      sensationProto = log.init(\"sensation\", len(ff))\n      for i in xrange(len(ff)):\n        sensationProto[i] = int(ff[i])\n      predictedL4Proto = log.init(\"predictedL4\", len(predictedL4Cells))\n      for i in xrange(len(predictedL4Cells)):\n        predictedL4Proto[i] = int(predictedL4Cells[i])\n      activeL4Proto = log.init(\"activeL4\", len(activeL4Cells))\n      for i in xrange(len(activeL4Cells)):\n        activeL4Proto[i] = int(activeL4Cells[i])\n      activeL6HistoryProto = log.init(\"activeL6History\", len(self.activeL6Cells))\n      for i in xrange(len(self.activeL6Cells)):\n        activeL6ModuleProto = activeL6HistoryProto.init(i, len(self.activeL6Cells[i]))\n        for j in xrange(len(self.activeL6Cells[i])):\n          activeL6ModuleStepProto = activeL6ModuleProto.init(j, len(self.activeL6Cells[i][j]))\n          for k in xrange(len(self.activeL6Cells[i][j])):\n            activeL6ModuleStepProto[k] = int(self.activeL6Cells[i][j][k])\n      activeL5Proto = log.init(\"activeL5\", len(self.activeL5Cells))\n      for i in xrange(len(self.activeL5Cells)):\n        activeL5ModuleProto = activeL5Proto.init(i, len(self.activeL5Cells[i]))\n        for j in xrange(len(self.activeL5Cells[i])):\n          activeL5ModuleProto[j] = int(self.activeL5Cells[i][j])\n\n      classifierResults = [(i, distance)\n                           for i, distance in enumerate(self.prediction[2])\n                           if distance is not None]\n      classifierResultsProto = log.init(\"classifierResults\",\n                                        len(classifierResults))\n      for i in xrange(len(classifierResults)):\n        classifierResultProto = classifierResultsProto[i]\n        classifierResultProto.label = classifierResults[i][0]\n        classifierResultProto.distance = float(classifierResults[i][1])\n\n      motorDeltaProto = log.init(\"motorDelta\", len(delta))\n      for i in xrange(len(delta)):\n        motorDeltaProto[i] = int(delta[i])\n      predictedL6Proto = log.init(\"predictedL6\", len(self.predictedL6Cells))\n      for i in xrange(len(self.predictedL6Cells)):\n        predictedL6ModuleProto = predictedL6Proto.init(i, len(self.predictedL6Cells[i]))\n        for j, c in enumerate(self.predictedL6Cells[i]):\n          predictedL6ModuleProto[j] = int(c)\n\n      json.dump(log.to_dict(), outputFile)\n      outputFile.write(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading trials from a folder and returns an array indexed by m k t.", "response": "def gather_data(mypath, key=\"width\"):\n    \"\"\"\n        Loads trials from a folder and \n        returns an array indexed by: m, k, t.\n    \"\"\"\n    filenames = [join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))]\n    filenames = [f for f in filenames if f.endswith(\".p\")] \n    W = []\n    for filename in filenames:\n        with open(filename, 'r') as f:\n            data = pickle.load(f)\n            W.append(np.expand_dims(data[key], axis=-1) )\n\n    W = np.concatenate(W, axis=-1)\n    return W"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef defaultSTDPKernel(preSynActivation,\n                      postSynActivation,\n                      dt,\n                      inhibitoryPresyn=False,\n                      inhibitoryPostsyn=False):\n  \"\"\"\n  This function implements a modified version of the STDP kernel from\n  Widloski & Fiete, 2014.\n  :param preSynActivation: Vector of pre-synaptic activations\n  :param postSynActivation: Vector of post-synaptic activations\n  :param dt: the difference in time between the two (in seconds), positive if\n          after and negative if before\n  :return: A matrix of synapse weight changes.\n  \"\"\"\n\n  stdpScaler = 1\n  stdpTimeScaler = 1.\n\n  # Set up STDP directions\n  if inhibitoryPresyn and not inhibitoryPostsyn:\n    #I-E, anti-Hebbian (weakening inhibitory connections)\n    stdpScaler *= 1\n  elif not inhibitoryPresyn and inhibitoryPostsyn:\n    # E-I, Hebbian\n    stdpScaler *= 1\n  elif inhibitoryPresyn and inhibitoryPostsyn:\n    # I-I, Hebbian (strengthening inhibitory connections)\n    stdpScaler *= -1\n\n  # Set up parameters\n  if dt < 0 and not inhibitoryPresyn:\n    # Anti-causal\n    stdpScaler *= 1\n    stdpTimeScaler *= 3\n  elif dt > 0 and not inhibitoryPresyn:\n    stdpScaler *= 1.2\n    stdpTimeScaler *= 4\n  elif dt > 0 and inhibitoryPresyn:\n    stdpScaler *= .5\n    stdpTimeScaler *= 4\n  elif dt < 0 and inhibitoryPresyn:\n    stdpScaler *= 1\n    stdpTimeScaler *= 2\n\n  timeFactor = np.exp(-1*np.abs(dt)/(STDP_TIME_CONSTANT*stdpTimeScaler))\n\n  updates = np.outer(preSynActivation*timeFactor*np.sign(dt)*stdpScaler,\n                     postSynActivation)\n\n  return updates", "response": "This function is a modified version of the STDP kernel from Widloski & Fiete 2014."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the error of our path integration relative to an ideal module.", "response": "def calculatePathIntegrationError(self, time, dt=None, trajectory=None,\n                                    envelope=False, inputNoise=None):\n    \"\"\"\n    Calculate the error of our path integration, relative to an ideal module.\n    To do this, we track the movement of an individual bump\n\n    Note that the network must be trained before this is done.\n    :param time: How long to simulate for in seconds.  We recommend using a\n            small value, e.g. ~10s.\n    :param trajectory: An optional trajectory that specifies how the network moves.\n    :param inputNoise: Whether or not to apply noise, and how much.\n    :return: A tuple of the true trajectory and the inferred trajectory.\n    \"\"\"\n    # Set up plotting\n    if self.plotting:\n      self.fig = plt.figure()\n      self.ax1 = self.fig.add_subplot(411)\n      self.ax2 = self.fig.add_subplot(412)\n      self.ax3 = self.fig.add_subplot(413)\n      self.ax4 = self.fig.add_subplot(414)\n      plt.tight_layout()\n      plt.ion()\n      self.fig.show()\n      self.fig.canvas.draw()\n      mouse = plt.imread(os.path.dirname(os.path.realpath(__file__))\n                         + \"/mouse_graphic.png\")\n\n      self.ax1.set_xlabel(\"Excitatory population activity\")\n      self.ax2.set_xlabel(\"Inhibitory population activity\")\n      self.ax3.set_xlabel(\"Movement in cells\")\n      self.ax3.set_ylabel(\"Cost\")\n      self.ax4.set_xlabel(\"Location\")\n\n      plt.tight_layout()\n\n\n    if dt is None:\n      oldDt = self.dt\n    else:\n      oldDt = self.dt\n      self.dt = dt\n\n    # Simulate for a second to get nice starting activation bumps.\n    # Turn plotting off so as not to confuse the viewer\n    oldPlotting = self.plotting\n    self.plotting = False\n    self.simulate(1, 1, 1, 0, envelope=envelope, inputNoise=None)\n    self.plotting = oldPlotting\n\n\n    estimatedVelocities = []\n    trueVelocities = []\n\n    times = np.arange(0, time, self.dt)\n    if trajectory is None:\n      # Sum together two different sinusoidals for a more interesting path.\n      trajectory = (np.sin((-times*np.pi/10 - np.pi/2.))+1)*2.5\n      trajectory += (np.cos((-times*np.pi/3 - np.pi/2.))+1)*.75\n      velocities = np.diff(trajectory)/self.dt\n\n    oldActivations = copy.copy(self.activationsI)\n    oldX = trajectory[0]\n    for i, t in enumerate(times[:-1]):\n      v = velocities[i]\n      x = trajectory[i]\n\n      feedforwardInputI = np.ones(self.activationsI.shape)\n      feedforwardInputE = np.ones(self.activationsEL.shape)\n\n      if inputNoise is not None:\n        noisesI = np.random.random_sample(feedforwardInputI.shape)*inputNoise\n        noisesE = np.random.random_sample(feedforwardInputE.shape)*inputNoise\n      else:\n        noisesE = 1.\n        noisesI = 1.\n\n      self.update(feedforwardInputI*noisesI, feedforwardInputE*noisesE,\n                  v, True, envelope=envelope)\n\n      estimationTime = np.abs(np.mod(t, ESTIMATION_INTERVAL))\n      if estimationTime <= 0.00001 or \\\n          np.abs(estimationTime - ESTIMATION_INTERVAL) <= 0.00001:\n\n          rotations = [np.sum(np.abs(np.roll(oldActivations, i) -\n                                     self.activationsI))\n                       for i in range(-10, 11, 1)]\n\n          shift = np.argmin(rotations) - 10\n\n          trueVelocities.append(x - oldX)\n          oldX = x\n          oldActivations = copy.copy(self.activationsI)\n          estimatedVelocities.append(shift)\n\n      if self.plotting:\n        plotTime = np.abs(np.mod(t, PLOT_INTERVAL))\n        if plotTime <= 0.00001 or np.abs(plotTime - PLOT_INTERVAL) <= 0.00001:\n          self.ax3.clear()\n          self.ax3.plot(np.arange(-len(rotations)/2 + 1, len(rotations)/2 + 1, 1),\n                                  rotations,\n                                  color=\"g\",\n                                  label=\"Shift\")\n          self.ax3.legend(loc=\"best\")\n          self.ax3.set_xlabel(\"Movement in cells\")\n          self.ax3.set_ylabel(\"Cost\")\n          self.ax3.axvline(x=shift)\n\n          self.ax4.clear()\n          self.ax4.set_xlim(np.amin(trajectory), np.amax(trajectory))\n          self.ax4.set_ylim(0, 1)\n          mouse_bound = (x - 0.25*np.sign(v), x + 0.25*np.sign(v), .05, .55)\n          self.ax4.imshow(mouse,\n                          aspect='auto',\n                          extent=mouse_bound,\n                          zorder=-1)\n          self.ax4.set_xlabel(\"Location\")\n          self.ax4.axes.get_yaxis().set_visible(False)\n\n          self.fig.canvas.draw()\n\n          self.plotActivation(time=t, velocity=v, boosting=False)\n\n    self.dt = oldDt\n    return(np.asarray(trueVelocities), np.asarray(estimatedVelocities))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef updatePlaceWeights(self):\n    self.weightsPI += np.outer(self.activationsI - self.boostTarget,\n                               self.activationsP)*self.dt*\\\n                               self.learnFactorP*self.learningRate\n    self.weightsPEL += np.outer(self.activationsEL - self.boostTarget,\n                               self.activationsP)*self.dt*\\\n                               self.learnFactorP*self.learningRate\n    self.weightsPER += np.outer(self.activationsER - self.boostTarget,\n                               self.activationsP)*self.dt*\\\n                               self.learnFactorP*self.learningRate\n\n    np.minimum(self.weightsPI, 1, self.weightsPI)\n    np.minimum(self.weightsPEL, 1, self.weightsPEL)\n    np.minimum(self.weightsPER, 1, self.weightsPER)", "response": "Updates the internal weights of the currently - active places."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, feedforwardInputI, feedforwardInputE, v, recurrent=True,\n             envelope=False, iSpeedTuning=False, enforceDale=True):\n    \"\"\"\n    Do one update of the CAN network, of length self.dt.\n    :param feedforwardInputI: The feedforward input to inhibitory cells.\n    :param feedforwardInputR: The feedforward input to excitatory cells.\n    :param placeActivity: Activity of the place code.\n    :param v: The current velocity.\n    :param recurrent: Whether or not recurrent connections should be used.\n    :param envelope: Whether or not an envelope should be applied.\n    :param iSpeedTuning: Whether or not inhibitory cells should also have their\n             activations partially depend on current movement speed.  This is\n             necessary for periodic training, serving a role similar to that of\n             the envelope.\n    :param Whether or not Dale's law should be enforced locally.  Helps with\n             training with recurrent weights active, but can slow down training.\n    \"\"\"\n\n    np.matmul(self.activationsP * self.placeGainI, self.weightsPI,\n              self.instantaneousI)\n    np.matmul(self.activationsP* self.placeGainE, self.weightsPEL,\n              self.instantaneousEL)\n    np.matmul(self.activationsP * self.placeGainE, self.weightsPER,\n              self.instantaneousER)\n\n    self.instantaneousI += self.boostEffectI*\\\n                           self.activationHistoryI +\\\n                           feedforwardInputI\n    self.instantaneousEL += self.boostEffectE*\\\n                            self.activationHistoryEL +\\\n                            feedforwardInputE\n    self.instantaneousER += self.boostEffectE*\\\n                            self.activationHistoryER +\\\n                            feedforwardInputE\n\n    if enforceDale:\n      weightsII =  np.minimum(self.weightsII, 0)\n      weightsIER = np.minimum(self.weightsIER, 0)\n      weightsIEL = np.minimum(self.weightsIEL, 0)\n      weightsELI = np.maximum(self.weightsELI, 0)\n      weightsERI = np.maximum(self.weightsERI, 0)\n    else:\n      weightsII =  self.weightsII\n      weightsIER = self.weightsIER\n      weightsIEL = self.weightsIEL\n      weightsELI = self.weightsELI\n      weightsERI = self.weightsERI\n\n    if recurrent:\n      self.instantaneousI += (np.matmul(self.activationsEL, weightsELI) +\\\n                np.matmul(self.activationsER, weightsERI) +\\\n                np.matmul(self.activationsI, weightsII))\n\n      self.instantaneousEL += np.matmul(self.activationsI, weightsIEL)\n      self.instantaneousER += np.matmul(self.activationsI, weightsIER)\n\n    self.instantaneousI += self.tonicMagnitude\n    self.instantaneousEL += self.tonicMagnitude\n    self.instantaneousER += self.tonicMagnitude\n\n    self.instantaneousEL *= max((1 - self.velocityGain*v), 0)\n    self.instantaneousER *= max((1 + self.velocityGain*v), 0)\n    if iSpeedTuning:\n      self.instantaneousI *= min(self.velocityGain*np.abs(v), 1)\n\n\n    if envelope:\n      self.instantaneousI *= self.envelopeI\n      self.instantaneousER *= self.envelopeE\n      self.instantaneousEL *= self.envelopeE\n\n    # Input must be positive.\n    np.maximum(self.instantaneousI, 0., self.instantaneousI)\n    np.maximum(self.instantaneousEL, 0., self.instantaneousEL)\n    np.maximum(self.instantaneousER, 0., self.instantaneousER)\n\n    # Activity decay and timestep adjustment\n    self.activationsI += (self.instantaneousI - self.activationsI/self.decayConstant)*self.dt\n    self.activationsEL += (self.instantaneousEL - self.activationsEL/self.decayConstant)*self.dt\n    self.activationsER += (self.instantaneousER - self.activationsER/self.decayConstant)*self.dt\n\n    # Finally, clip activations for stability\n    np.minimum(self.activationsI, self.clip, self.activationsI)\n    np.minimum(self.activationsEL, self.clip, self.activationsEL)\n    np.minimum(self.activationsER, self.clip, self.activationsER)\n\n    self.activationHistoryI += (-self.activationsI + np.sum(self.activationsI)/np.sum(self.envelopeI))*self.dt\n    self.activationHistoryEL += (-self.activationsEL + np.sum(self.activationsEL)/np.sum(self.envelopeE))*self.dt\n    self.activationHistoryER += (-self.activationsER + np.sum(self.activationsER)/np.sum(self.envelopeE))*self.dt\n\n    self.activationHistoryI -= self.dt*self.activationHistoryI/self.alpha\n    self.activationHistoryEL -= self.dt*self.activationHistoryEL/self.alpha\n    self.activationHistoryER -= self.dt*self.activationHistoryER/self.alpha", "response": "Update the internal state of the CAN network."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef learn(self,\n             runs,\n             dir=1,\n             periodic=False,\n             recurrent=True,\n             randomSpeed=False,\n             learnRecurrent=False,\n             envelope=True,):\n    \"\"\"\n    Traverses a sinusoidal trajectory across the environment, learning during\n    the process.  A pair of runs across the environment (one in each direction)\n    takes 10 seconds if in a periodic larger environment, and 4 seconds in a\n    smaller nonperiodic environment.\n    :param runs: How many runs across the environment to do.  Each \"run\" is\n            defined as a full sweep across the environment in each direction.\n    :param dir: Which direction to move in first.  Valid values are 1 and -1.\n    :param periodic: Whether or not the learning environment should be\n            periodic (toroidal).\n    :param recurrent: Whether or not recurrent connections should be active\n            during learning.  Warning: True can lead to instability.\n    :param randomSpeed: Whether or not to use a random maximum speed for each\n            run, to better simulate real learning.  Can degrade performance.\n            Only supported in periodic environments.\n    :param learnRecurrent: Whether or not to learn recurrent connections.\n    :param envelope: Whether or not the envelope should be active in learning.\n    \"\"\"\n\n    # Simulate for a second to get nice starting activation bumps.\n    # Turn plotting off so as not to confuse the viewer\n    oldPlotting = self.plotting\n    self.plotting = False\n    self.simulate(10, 1, 1, 0, envelope=False, inputNoise=None, save=False)\n    self.plotting = oldPlotting\n\n    # Set up plotting\n    if self.plotting:\n      self.fig = plt.figure()\n      self.ax1 = self.fig.add_subplot(611)\n      self.ax2 = self.fig.add_subplot(612)\n      self.ax3 = self.fig.add_subplot(613)\n      self.ax4 = self.fig.add_subplot(212)\n\n      self.ax3.set_xlabel(\"Inhibitory-Inhibitory connections\")\n      plt.ion()\n      self.fig.show()\n      self.fig.canvas.draw()\n\n\n    # Set up the trajectories and running times.\n    if not periodic:\n      time = 4.*runs\n      timings = [np.arange(0, time, self.dt)]\n      trajectories = [(np.sin(dir*(times*np.pi/2 - np.pi/2.))+1)/2]\n    else:\n      # Space the starting points of the runs out.  This tends to improve the\n      # translation-invariance of the weight profiles, and thus gives better\n      # overall path integration.\n      startingPoint = 0\n      trajectories = []\n      timings = []\n      time = 0\n      residTime = 0\n      for run in xrange(runs):\n        if randomSpeed:\n          speed = np.random.random() + 0.5\n        else:\n          speed = 1.\n        length = 10. / speed\n\n        runTimes = np.arange(0, length, self.dt)\n        trajectory = (np.sin(dir * (runTimes * np.pi / (5 / speed) - np.pi / 2.)) + 1) * \\\n                     2.5 + startingPoint\n        trajectories.append(trajectory)\n        timings.append(runTimes + time)\n        time += length\n        startingPoint += 1. / runs\n\n    for trajectory, timing in zip(trajectories, timings):\n      self.activationsI = np.random.random_sample(self.activationsI.shape)\n      self.activationsER = np.random.random_sample(self.activationsER.shape)\n      self.activationsEL = np.random.random_sample(self.activationsEL.shape)\n      self.activationHistoryI = np.zeros(self.activationsI.shape)\n      self.activationHistoryEL = np.zeros(self.activationsEL.shape)\n      self.activationHistoryER = np.zeros(self.activationsER.shape)\n      velocities = np.diff(trajectory)/self.dt\n      for i, t in enumerate(timing[:-1]):\n        x = trajectory[i] % self.roomSize\n        v = velocities[i]\n        self.activationsP = np.exp(-1.*(self.placeCode - x)**2 /\n                              (2*self.sigmaLoc**2))\n\n        self.update(0, 0, v,\n                    recurrent=recurrent,\n                    envelope=envelope,\n                    iSpeedTuning=periodic,\n                    enforceDale=True,\n                    )\n\n        self.stdpUpdate(t, onlyPlace=not learnRecurrent)\n\n        # Enforce Dale's law for place cells.  Place cells must be excitatory.\n        np.maximum(self.weightsPI, 0, self.weightsPI)\n        np.maximum(self.weightsPEL, 0, self.weightsPEL)\n        np.maximum(self.weightsPER, 0, self.weightsPER)\n\n        # Also keep the place weights from being too large.\n        np.minimum(self.weightsPI, 1., self.weightsPI)\n        np.minimum(self.weightsPEL, 3., self.weightsPEL)\n        np.minimum(self.weightsPER, 3., self.weightsPER)\n\n        if self.plotting:\n          residTime += self.dt\n          if residTime > PLOT_INTERVAL:\n            residTime -= PLOT_INTERVAL\n            self.ax4.matshow(self.weightsPI, cmap=plt.cm.coolwarm)\n            self.plotActivation(position=x, time=t)\n      self.stdpUpdate(t, onlyPlace=not learnRecurrent, clearBuffer=True)\n\n    # Finally, enforce Dale's law for recurrent connections.\n    # Inhibitory neurons must be inhibitory,\n    # excitatory neurons must be excitatory.\n    # This could be handled through update, but it's faster to do it here.\n    np.minimum(self.weightsII, 0, self.weightsII)\n    np.minimum(self.weightsIER, 0, self.weightsIER)\n    np.minimum(self.weightsIEL, 0, self.weightsIEL)\n    np.maximum(self.weightsELI, 0, self.weightsELI)\n    np.maximum(self.weightsERI, 0, self.weightsERI)", "response": "Traverses a sinusoidal trajectory across the environment, learning during\n    the process.  A pair of runs across the environment (one in each direction)\n    takes 10 seconds if in a periodic larger environment, and 4 seconds in a\n    smaller nonperiodic environment.\n    :param runs: How many runs across the environment to do.  Each \"run\" is\n            defined as a full sweep across the environment in each direction.\n    :param dir: Which direction to move in first.  Valid values are 1 and -1.\n    :param periodic: Whether or not the learning environment should be\n            periodic (toroidal).\n    :param recurrent: Whether or not recurrent connections should be active\n            during learning.  Warning: True can lead to instability.\n    :param randomSpeed: Whether or not to use a random maximum speed for each\n            run, to better simulate real learning.  Can degrade performance.\n            Only supported in periodic environments.\n    :param learnRecurrent: Whether or not to learn recurrent connections.\n    :param envelope: Whether or not the envelope should be active in learning."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the activation of the current cell populations.", "response": "def plotActivation(self, position=None, time=None, velocity=None, boosting=True):\n    \"\"\"\n    Plot the activation of the current cell populations.  Assumes that\n    two axes have already been created, ax1 and ax2.  If done in a Jupyter\n    notebook, this plotting will overwrite the old plot.\n    :param position: The current location of the animal\n    :param time: The current time in the simulation\n    :param velocity: The current velocity of the animal\n    \"\"\"\n    self.ax1.clear()\n    x = np.arange(0, len(self.activationsER), 1)\n    self.ax1.plot(x, self.activationsEL, color = \"b\", label = \"EL Activation\")\n    self.ax1.plot(x, self.activationsER, color = \"r\", label = \"ER Activation\")\n    self.ax1.legend(loc = \"best\")\n\n    self.ax2.clear()\n    x = np.arange(0, len(self.activationsI), 1)\n    self.ax2.plot(x, self.activationsI, color = \"k\", label = \"I Activation\")\n    self.ax2.legend(loc = \"best\")\n\n    if boosting:\n      self.ax3.clear()\n      x = np.arange(0, len(self.activationHistoryI), 1)\n      self.ax3.plot(x, self.activationHistoryI*self.boostEffectI, color = \"k\", label = \"Boost history\")\n      if position is not None:\n        self.ax3.axvline(x=position*len(self.activationsI))\n      self.ax3.legend(loc = \"best\")\n      self.ax3.set_xlabel(\"Boosting\")\n\n    titleString = \"\"\n    if time is not None:\n      titleString += \"Time = {}\".format(str(time))\n    if velocity is not None:\n      titleString += \"  Velocity = {}\".format(str(velocity)[:4])\n    if position is not None:\n      titleString += \"  Position = {}\".format(str(position)[:4])\n    self.ax1.set_title(titleString)\n\n    self.ax1.set_xlabel(\"Excitatory activity\")\n    self.ax2.set_xlabel(\"Inhibitory activity\")\n\n    self.fig.canvas.draw()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stdpUpdate(self, time, clearBuffer=False, onlyPlace=False):\n    if clearBuffer:\n      while len(self.activationBuffer) > 1:\n        baseI, baseEL, baseER, baseP, t = self.activationBuffer.popleft()\n        for (I, EL, ER, P, i) in self.activationBuffer:\n          t = (i - t) * self.dt\n          self.weightsPI += placeSTDPKernel(self.learningRate *\\\n                                            self.learnFactorP *\\\n                                            self.dt *\\\n                                            baseP, I, t,\n                                            )\n\n          self.weightsPEL += placeSTDPKernel(self.learningRate *\\\n                                             self.learnFactorP *\\\n                                             self.dt *\\\n                                             baseP, EL, t,\n                                             )\n\n          self.weightsPER += placeSTDPKernel(self.learningRate *\\\n                                             self.learnFactorP *\\\n                                             self.dt *\\\n                                             baseP, ER, t,\n                                             )\n\n          if not onlyPlace:\n            self.weightsELI += self.stdpKernel(self.learningRate *\\\n                                               self.learnFactorEI *\\\n                                               self.dt *\\\n                                               baseEL, I, t,\n                                               False, True)\n\n            self.weightsERI += self.stdpKernel(self.learningRate *\\\n                                               self.learnFactorEI *\\\n                                               self.dt *\\\n                                               baseER, I, t,\n                                               False, True)\n\n    else:\n      for I, EL, ER, P, i in reversed(self.activationBuffer):\n        t = (i - time) * self.dt\n        self.weightsPI +=  placeSTDPKernel(self.learningRate *\\\n                                           self.learnFactorP *\\\n                                           self.dt *\\\n                                           self.activationsP, I, t,\n                                           )\n\n        self.weightsPEL += placeSTDPKernel(self.learningRate *\\\n                                           self.learnFactorP *\\\n                                           self.dt *\\\n                                           self.activationsP, EL, t,\n                                           )\n\n        self.weightsPER += placeSTDPKernel(self.learningRate *\\\n                                           self.learnFactorP *\\\n                                           self.dt *\\\n                                           self.activationsP, ER, t,\n                                           )\n\n        if not onlyPlace:\n          self.weightsELI += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorEI *\\\n                                             self.dt *\\\n                                             self.instantaneousEL, I, t,\n                                             False, True)\n\n          self.weightsERI += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorEI *\\\n                                             self.dt *\\\n                                             self.instantaneousER, I, t,\n                                             False, True)\n\n      for I, EL, ER, P, i in self.activationBuffer:\n        t = (time - i) * self.dt\n        self.weightsPI +=  placeSTDPKernel(self.learningRate *\\\n                                           self.learnFactorP *\\\n                                           self.dt *\\\n                                           P, self.instantaneousI, t,\n                                           )\n\n        self.weightsPEL += placeSTDPKernel(self.learningRate *\\\n                                           self.learnFactorP *\\\n                                           self.dt *\\\n                                           P, self.instantaneousEL, t,\n                                           )\n\n        self.weightsPER += placeSTDPKernel(self.learningRate *\\\n                                           self.learnFactorP *\\\n                                           self.dt *\\\n                                           P, self.instantaneousER, t,\n                                           )\n\n        if not onlyPlace:\n          self.weightsELI += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorEI *\\\n                                             self.dt *\\\n                                             EL, self.instantaneousI, t,\n                                             False, True)\n\n          self.weightsERI += self.stdpKernel(self.learningRate *\\\n                                             self.learnFactorEI *\\\n                                             self.dt *\\\n                                             ER, self.instantaneousI, t,\n                                             False, True)\n\n      self.activationBuffer.append((np.copy(self.instantaneousI),\n                                    np.copy(self.instantaneousEL),\n                                    np.copy(self.instantaneousER),\n                                    np.copy(self.activationsP),\n                                    time))", "response": "Updates the current activations of the current time and performs STDP update if possible."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the internal state of the CAN network.", "response": "def update(self, feedforwardInputI, feedforwardInputE, v, recurrent=True,\n             envelope=False, iSpeedTuning=False):\n    \"\"\"\n    Do one update of the CAN network, of length self.dt.\n    :param feedforwardInputI: The feedforward input to inhibitory cells.\n    :param feedforwardInputR: The feedforward input to excitatory cells.\n    :param placeActivity: Activity of the place code.\n    :param v: The current velocity.\n    :param recurrent: Whether or not recurrent connections should be used.\n    :param envelope: Whether or not an envelope should be applied.\n    :param iSpeedTuning: Whether or not inhibitory cells should also have their\n             activations partially depend on current movement speed.  This is\n             necessary for periodic training, serving a role similar to that of\n             the envelope.\n    \"\"\"\n    np.matmul(self.weightsPI, self.activationsP*self.placeGainI,\n              self.instantaneousI)\n    if recurrent:\n      self.instantaneousI += np.matmul(np.minimum(self.weightsII, 0),\n                                       self.activationsI,\n                                       )\n    self.instantaneousI += self.boostEffect*self.activationHistoryI +\\\n                           feedforwardInputI + self.tonicMagnitude\n\n    if envelope:\n      self.instantaneousI *= self.envelope\n\n    if iSpeedTuning:\n      self.instantaneousI *= min(self.velocityGain * np.abs(v), 1)\n\n    np.maximum(self.instantaneousI, 0., self.instantaneousI)\n\n    for k, a in self.instantaneous.items():\n      #np.matmul(self.weightsPE[k], self.activationsP * self.placeGainE,  a)\n      #a += feedforwardInputE + self.tonicMagnitude +\\\n      #     self.boostEffect*self.histories[k]\n      #if recurrent:\n      #  a += np.matmul(np.minimum(self.weightsIE[k], 0), self.activationsI)\n      #  self.instantaneousI += np.matmul(np.maximum(self.weightsEI[k], 0),\n      #                                   self.activations[k]\n       #                                  )\n\n      a *= max((1 + np.dot(v, self.directions[k]), 0))\n\n      # Input must be positive\n      np.maximum(a, 0, a)\n\n      if envelope:\n        a *= self.envelope\n\n      # Activity decay and timestep adjustment\n      self.activations[k] += (a - self.activations[k]/self.decayConstant)*self.dt\n\n      # Finally, clip activations for stability\n      np.minimum(self.activations[k], self.clip, self.activations[k])\n\n      self.histories[k] -= (self.activations[k]\n                                    - np.mean(self.activations[k]))*self.dt\n\n    # Activity decay and timestep adjustment\n    self.activationsI += (self.instantaneousI -\n                          self.activationsI/self.decayConstant)*self.dt\n\n    # Finally, clip activations for stability\n    np.minimum(self.activationsI, self.clip, self.activationsI)\n\n    self.activationHistoryI -= (self.activationsI -\n                                np.mean(self.activationsI))*self.dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef computeEnvelope(self):\n    envelopeX = [1 if np.abs(p) < 1 - self.envelopeWidth else\n                      np.exp(-1.*self.envelopeFactor *\n                      ((np.abs(p) - 1 + self.envelopeWidth)/self.envelopeWidth)**2)\n                      for p in np.linspace(-1, 1, self.dimensions[0])]\n\n    envelopeY = [1 if np.abs(p) < 1 - self.envelopeWidth else\n                      np.exp(-1.*self.envelopeFactor *\n                      ((np.abs(p) - 1 + self.envelopeWidth)/self.envelopeWidth)**2)\n                      for p in np.linspace(-1, 1, self.dimensions[1])]\n\n    return np.asarray(np.outer(envelopeX, envelopeY).flatten())", "response": "Compute an envelope for use in suppressing border cells."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecay the network s weights.", "response": "def decayWeights(self, decayConst=60):\n    \"\"\"\n    Decay the network's weights.\n\n    :param decayConst: The time constant (in seconds) to use for decay.\n            Note: If applied, decay must be used extremely carefully, as\n            it has a tendency to cause asymmetries in the network weights.\n    \"\"\"\n    self.weightsII -= self.weightsII*self.dt/decayConst\n    self.weightsPI -= self.weightsPI*self.dt/decayConst\n    for k, w in self.weightsIE.items():\n      w -= w*self.dt/decayConst\n    for k, w in self.weightsEI.items():\n      w -= w*self.dt/decayConst\n    for k, w in self.weightsPE.items():\n      w -= w*self.dt/decayConst"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef learnPlaceCode(self,\n                     runs,\n                     dir=1,\n                     periodic=False,\n                     recurrent=True,\n                     randomSpeed=False,\n                     learnRecurrent=False):\n    \"\"\"\n    Traverses a sinusoidal trajectory across the environment, learning during\n    the process.  A pair of runs across the environment (one in each direction)\n    takes 10 seconds if in a periodic larger environment, and 4 seconds in a\n    smaller nonperiodic environment.\n    :param runs: How many runs across the environment to do.  Each \"run\" is\n            defined as a full sweep across the environment in each direction.\n    :param dir: Which direction to move in first.  Valid values are 1 and -1.\n    :param periodic: Whether or not the learning environment should be\n            periodic (toroidal).\n    :param recurrent: Whether or not recurrent connections should be active\n            during learning.  Warning: True can lead to instability.\n    :param randomSpeed: Whether or not to use a random maximum speed for each\n            run, to better simulate real learning.  Can degrade performance.\n            Only supported in periodic environments.\n    :param learnRecurrent: Whether or not to learn recurrent connections.\n    \"\"\"\n\n    # Simulate for a second to get nice starting activation bumps.\n    # Turn plotting off so as not to confuse the viewer\n    self.plotting = False\n    self.simulate(10, 1, 1, 0, envelope=False, inputNoise=None)\n    self.plotting = True\n\n    # Set up plotting\n    if self.plotting:\n      self.fig = plt.figure()\n      self.ax1 = self.fig.add_subplot(411)\n      self.ax2 = self.fig.add_subplot(412)\n      self.ax3 = self.fig.add_subplot(413)\n      self.ax4 = self.fig.add_subplot(414)\n\n      plt.ion()\n      plt.tight_layout()\n      self.ax3.set_xlabel(\"Inhibitory-Inhibitory connections\")\n      self.fig.show()\n      self.fig.canvas.draw()\n\n    if self.movie:\n      history = []\n\n\n    # Set up the trajectories and running times.\n    if not periodic:\n      time = 4.*runs\n      timings = [np.arange(0, time, self.dt)]\n      trajectories = [(np.sin(dir*(times*np.pi/2 - np.pi/2.))+1)/2]\n\n    else:\n      # Space the starting points of the runs out.  This tends to improve the\n      # translation-invariance of the weight profiles, and thus gives better\n      # overall path integration.\n      startingPoint = 0\n      trajectories = []\n      timings = []\n      time = 0\n      residTime = 0\n      for run in xrange(runs):\n        if randomSpeed:\n          speed = np.random.random() + 0.5\n        else:\n          speed = 1.\n        length = 10. / speed\n\n        runTimes = np.arange(0, length, self.dt)\n        trajectory[:, 0] = (np.sin(dir * (runTimes * np.pi / (5 / speed) - np.pi / 2.)) + 1) * \\\n                           2.5 + startingPoint\n        trajectory[:, 1] = (np.sin(dir * (runTimes * np.pi / (5 / speed) - np.pi / 2.)) + 1) * \\\n                           2.5\n        trajectories.append(trajectory)\n        timings.append(runTimes + time)\n        time += length\n        startingPoint += 1. / runs\n\n    for trajectory, timing in zip(trajectories, timings):\n      self.activationsI = np.zeros(self.activationsI.shape)\n      self.activationsER = np.zeros(self.activationsER.shape)\n      self.activationsEL = np.zeros(self.activationsEL.shape)\n      velocities = np.diff(trajectory)/self.dt\n\n      for i, t in enumerate(timing[:-1]):\n        x = trajectory[i] % 1\n        v = velocities[i]\n        self.activationsP = np.exp(-1.*(self.placeCode - x)**2 /\n                              (2*self.sigmaLoc**2))\n\n        self.update(0, 0, v,\n                    recurrent=recurrent,\n                    envelope=(not periodic),\n                    iSpeedTuning=periodic,\n                    enforceDale=True,\n                    )\n\n        self.stdpUpdate(t, onlyPlace=not learnRecurrent)\n\n        # Finally, enforce Dale's law.  Place neurons must be excitatory.\n        # Also keep the place weights from being too large.\n        np.maximum(self.weightsPI, 0, self.weightsPI)\n        np.minimum(self.weightsPI, 3., self.weightsPI)\n        for k, w in self.weightsPE.items():\n          np.maximum(w, 0, w)\n          np.minimum(w, 3., w)\n\n        residTime += self.dt\n        if residTime > PLOT_INTERVAL:\n          residTime -= PLOT_INTERVAL\n          if self.plotting:\n              self.ax4.matshow(self.weightsPI, cmap=plt.cm.coolwarm)\n              self.plotActivation(position=x, time=t)\n          if self.movie:\n            history.append(np.copy(self.weightsPI))\n\n\n      if self.movie:\n        self.createMovie(np.stack(history, -1), \"PIWeightEvolution\",\n                         self.numInhibitory, self.numPlaces)\n\n\n      self.stdpUpdate(t, onlyPlace=not learnRecurrent, clearBuffer=True)\n\n      # Enforce Dale's law\n      np.minimum(self.weightsII, 0, self.weightsII)\n      np.maximum(self.weightsPI, 0, self.weightsPI)\n      for k, w in self.weightsIE.items():\n        np.minimum(w, 0, w)\n      for k, w in self.weightsEI.items():\n        np.maximum(w, 0, w)\n      for k, w in self.weightsPE.items():\n        np.maximum(w, 0, w)", "response": "Learn the place code of each ison - environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the activation of the current cell populations.", "response": "def plotActivation(self, position=None, time=None, velocity=None):\n    \"\"\"\n    Plot the activation of the current cell populations.  Assumes that\n    two axes have already been created, ax1 and ax2.  If done in a Jupyter\n    notebook, this plotting will overwrite the old plot.\n    :param position: The current location of the animal\n    :param time: The current time in the simulation\n    :param velocity: The current velocity of the animal\n    \"\"\"\n    self.ax1.clear()\n    y = self.activations[\"n\"] + self.activations[\"s\"] + self.activations[\"e\"] + \\\n        self.activations[\"w\"]\n    self.ax1.matshow(y.reshape(self.dimensions))\n\n    self.ax2.clear()\n    self.ax2.matshow(self.activationsI.reshape(self.dimensions))\n\n    self.ax3.clear()\n    self.ax3.matshow(self.activationHistoryI.reshape(self.dimensions))\n\n    titleString = \"\"\n    if time is not None:\n      titleString += \"Time = {}\".format(str(time))\n    if velocity is not None:\n      titleString += \"  Velocity = {}\".format(str(velocity)[:4])\n    if position is not None:\n      titleString += \"  Position = {}\".format(str(position)[:4])\n    plt.suptitle(titleString)\n\n    self.ax1.set_xlabel(\"Excitatory activity\")\n    self.ax2.set_xlabel(\"Inhibitory activity\")\n    self.ax3.set_xlabel(\"Boosting activity\")\n\n    plt.tight_layout()\n\n    self.fig.canvas.draw()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the current activations and instantaneous activations for the current time.", "response": "def stdpUpdate(self, time, clearBuffer=False, onlyPlace=False):\n    \"\"\"\n    Adds the current activations to the tracking queue, and then performs an\n    STDP update if possible.\n    :param time: The current time.  Must be provided.\n    :param clearBuffer: Set as True to clear the activation buffer.\n            This should be done at the end of training.\n    :param onlyPlace: Only learn place connections.\n    \"\"\"\n    if clearBuffer:\n      while len(self.activationBuffer) > 1:\n        baseI, baseE, baseP, t = self.activationBuffer.popleft()\n        for (I, E, P, i) in self.activationBuffer:\n          t = (i - t) * self.dt\n          self.sdtpKernels(t, I, E, baseI, baseE, baseP, onlyPlace=onlyPlace)\n\n    else:\n      for I, E, P, i in reversed(self.activationBuffer):\n        t = (i - time) * self.dt\n        self.sdtpKernels(t,\n                         I,\n                         E,\n                         self.instantaneousI,\n                         self.instantaneous,\n                         self.activationsP,\n                         onlyPlace=onlyPlace)\n\n      for I, E, P, i in self.activationBuffer:\n        t = (time - i) * self.dt\n        self.sdtpKernels(t,\n                         self.instantaneousI,\n                         self.instantaneous,\n                         I,\n                         E,\n                         P,\n                         onlyPlace=onlyPlace)\n\n      self.activationBuffer.append((np.copy(self.instantaneousI),\n                                    {k: np.copy(self.instantaneous[k])\n                                    for k in self.instantaneous},\n                                    np.copy(self.activationsP),\n                                    time))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the exact match metric for the cluster number of samples that have all the labels classified correctly.", "response": "def exact_match(pred, target):\n  \"\"\"\n  Compute \"Exact match\" metric, also called \"Subset accuracy\" indicating the\n  number of samples that have all their labels classified correctly.\n  See https://en.wikipedia.org/wiki/Multi-label_classification\n\n  :param pred: Predicted labels\n  :param target: Correct labels\n  :return: containing a 1 at the prediction where all labels in the prediction\n           match all labels in the target\n  \"\"\"\n  res = torch.eq(target.sort(dim=1)[0], pred.sort(dim=1)[0])\n  return res.prod(dim=1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a noisy copy of a set of active bits.", "response": "def noisy(pattern, noiseLevel, totalNumCells):\n  \"\"\"\n  Generate a noisy copy of a pattern.\n\n  Given number of active bits w = len(pattern),\n  deactivate noiseLevel*w cells, and activate noiseLevel*w other cells.\n\n  @param pattern (set)\n  A set of active indices\n\n  @param noiseLevel (float)\n  The percentage of the bits to shuffle\n\n  @param totalNumCells (int)\n  The number of cells in the SDR, active and inactive\n\n  @return (set)\n  A noisy set of active indices\n  \"\"\"\n  n = int(noiseLevel * len(pattern))\n\n  noised = set(pattern)\n\n  noised.difference_update(random.sample(noised, n))\n\n  for _ in xrange(n):\n    while True:\n      v = random.randint(0, totalNumCells - 1)\n      if v not in pattern and v not in noised:\n        noised.add(v)\n        break\n\n  return noised"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate random objects. @param numObjects (int) The number of objects to generate @param locationsPerObject (int) The number of points on each object @param featurePoolSize (int) The number of possible features @return For example, { 0: [0, 1, 2], 1: [0, 2, 1], 2: [2, 0, 1], } is 3 objects. The first object has Feature 0 and Location 0, Feature 1 at Location 1, Feature 2 at location 2, etc.", "response": "def createRandomObjects(numObjects, locationsPerObject, featurePoolSize):\n  \"\"\"\n  Generate random objects.\n\n  @param numObjects (int)\n  The number of objects to generate\n\n  @param locationsPerObject (int)\n  The number of points on each object\n\n  @param featurePoolSize (int)\n  The number of possible features\n\n  @return\n  For example, { 0: [0, 1, 2],\n                 1: [0, 2, 1],\n                 2: [2, 0, 1], }\n  is 3 objects. The first object has Feature 0 and Location 0, Feature 1 at\n  Location 1, Feature 2 at location 2, etc.\n  \"\"\"\n\n  allFeatures = range(featurePoolSize)\n  allLocations = range(locationsPerObject)\n  objects = dict((name,\n                     [random.choice(allFeatures) for _ in xrange(locationsPerObject)])\n                    for name in xrange(numObjects))\n\n  return objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntouches every point on an object 'numInitialTraversals' times, then evaluate whether it has inferred the object by touching every point once more and checking the number of correctly active and incorrectly active cells. @param numColumns (int) The number of sensors to use @param l2Overrides (dict) Parameters for the ColumnPooler @param objects (dict) A mapping of object names to their features. See 'createRandomObjects'. @param noiseLevels (list of floats) The noise levels to experiment with. The experiment is run once per noise level. Noise is applied at a constant rate to exactly one cortical column. It's applied to the same cortical column every time, and this is the cortical column that is measured. @param noisyFeature (bool) Whether to use a noisy feature @param noisyLocation (bool) Whether to use a noisy location", "response": "def doExperiment(numColumns, objects, l2Overrides, noiseLevels, numInitialTraversals,\n                 noisyFeature, noisyLocation):\n  \"\"\"\n  Touch every point on an object 'numInitialTraversals' times, then evaluate\n  whether it has inferred the object by touching every point once more and\n  checking the number of correctly active and incorrectly active cells.\n\n  @param numColumns (int)\n  The number of sensors to use\n\n  @param l2Overrides (dict)\n  Parameters for the ColumnPooler\n\n  @param objects (dict)\n  A mapping of object names to their features.\n  See 'createRandomObjects'.\n\n  @param noiseLevels (list of floats)\n  The noise levels to experiment with. The experiment is run once per noise\n  level. Noise is applied at a constant rate to exactly one cortical column.\n  It's applied to the same cortical column every time, and this is the cortical\n  column that is measured.\n\n  @param noisyFeature (bool)\n  Whether to use a noisy feature\n\n  @param noisyLocation (bool)\n  Whether to use a noisy location\n  \"\"\"\n\n  featureSDR = lambda : set(random.sample(xrange(NUM_L4_COLUMNS), 40))\n  locationSDR = lambda : set(random.sample(xrange(1024), 40))\n\n  featureSDRsByColumn = [defaultdict(featureSDR) for _ in xrange(numColumns)]\n  locationSDRsByColumn = [defaultdict(locationSDR) for _ in xrange(numColumns)]\n\n  exp = L4L2Experiment(\n    \"Experiment\",\n    numCorticalColumns=numColumns,\n    inputSize=NUM_L4_COLUMNS,\n    externalInputSize=1024,\n    seed=random.randint(2048, 4096)\n  )\n\n  exp.learnObjects(\n    dict((objectName,\n          [dict((column,\n                 (locationSDRsByColumn[column][location],\n                  featureSDRsByColumn[column][features[location]]))\n                for column in xrange(numColumns))\n           for location in xrange(len(features))])\n         for objectName, features in objects.iteritems()))\n\n  results = defaultdict(list)\n\n  for noiseLevel in noiseLevels:\n    # Try to infer the objects\n    for objectName, features in objects.iteritems():\n      exp.sendReset()\n\n      inferredL2 = exp.objectL2Representations[objectName]\n\n      sensorPositionsIterator = greedySensorPositions(numColumns, len(features))\n\n      # Touch each location at least numInitialTouches times, and then touch it\n      # once more, testing it. For each traversal, touch each point on the object\n      # ~once. Not once per sensor -- just once. So we translate the \"number of\n      # traversals\" into a \"number of touches\" according to the number of sensors.\n      numTouchesPerTraversal = len(features) / float(numColumns)\n      numInitialTouches = int(math.ceil(numInitialTraversals * numTouchesPerTraversal))\n      numTestTouches = len(features)\n\n      for touch in xrange(numInitialTouches + numTestTouches):\n        sensorPositions = next(sensorPositionsIterator)\n\n        sensation = dict(\n          (column,\n           (locationSDRsByColumn[column][sensorPositions[column]],\n            featureSDRsByColumn[column][features[sensorPositions[column]]]))\n          for column in xrange(1, numColumns))\n\n        # Add noise to the first column.\n        featureSDR = featureSDRsByColumn[0][features[sensorPositions[0]]]\n        if noisyFeature:\n          featureSDR = noisy(featureSDR, noiseLevel, NUM_L4_COLUMNS)\n\n        locationSDR = locationSDRsByColumn[0][sensorPositions[0]]\n        if noisyLocation:\n          locationSDR = noisy(locationSDR, noiseLevel, 1024)\n\n        sensation[0] = (locationSDR, featureSDR)\n\n        exp.infer([sensation]*TIMESTEPS_PER_SENSATION, reset=False,\n                  objectName=objectName)\n\n        if touch >= numInitialTouches:\n          activeCells = exp.getL2Representations()[0]\n          correctCells = inferredL2[0]\n          results[noiseLevel].append((len(activeCells & correctCells),\n                                      len(activeCells - correctCells)))\n\n  return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logCellActivity_noisyFeature_varyNumColumns(name=\"cellActivity\"):\n  noiseLevels = [0.0, 0.30, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75,\n                 0.80, 0.85, 0.90]\n  l2Overrides = {\"sampleSizeDistal\": 20}\n  columnCounts = [1, 2, 3, 4]\n\n  results = defaultdict(list)\n\n  for trial in xrange(1):\n    print \"trial\", trial\n\n    objects = createRandomObjects(10, 10, 10)\n\n    for numColumns in columnCounts:\n      print \"numColumns\", numColumns\n      r = doExperiment(numColumns, objects, l2Overrides, noiseLevels,\n                       numInitialTraversals=6, noisyFeature=True,\n                       noisyLocation=False)\n\n      for noiseLevel, counts in r.iteritems():\n        results[(numColumns, noiseLevel)].extend(counts)\n\n  d = []\n  for (numColumns, noiseLevel), cellCounts in results.iteritems():\n    d.append({\"numColumns\": numColumns,\n              \"noiseLevel\": noiseLevel,\n              \"results\": cellCounts})\n\n  filename = os.path.join(\"plots\",\n                          \"%s_%s.json\"\n                          % (name, time.strftime(\"%Y%m%d-%H%M%S\")))\n  with open(filename, \"w\") as fout:\n    json.dump(d, fout)\n\n  print \"Wrote to\", filename\n  print \"Visualize this file at: http://numenta.github.io/htmresearch/visualizations/grid-of-scatterplots/L2-columns-with-noise.html\"", "response": "Run the experiment and save the number of columns that are not yet active."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexperimenting 1 - Calculate error rate as a function of training sequence numbers \u00b0C", "response": "def runExperiment():\n  \"\"\"\n  Experiment 1: Calculate error rate as a function of training sequence numbers\n  :return:\n  \"\"\"\n  trainSeqN = [5, 10, 20, 50, 100, 200]\n  rptPerCondition = 5\n  correctRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  missRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  fpRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  for i in xrange(len(trainSeqN)):\n    for rpt in xrange(rptPerCondition):\n      tm = initializeTM()\n      train_seed = 1\n      numTrainSequence = trainSeqN[i]\n      trainTMonReberSeq(tm, numTrainSequence, seedSeq=train_seed)\n      (correctRate, missRate, fpRate) = testTMOnReberSeq(tm, numTestSequence, seedSeq=train_seed+rpt)\n      correctRateAll[i, rpt] = correctRate\n      missRateAll[i, rpt] = missRate\n      fpRateAll[i, rpt] = fpRate\n\n  np.savez('result/reberSequenceTM.npz',\n           correctRateAll=correctRateAll, missRateAll=missRateAll,\n           fpRateAll=fpRateAll, trainSeqN=trainSeqN)\n\n  plt.figure()\n  plt.subplot(2,2,1)\n  plt.semilogx(trainSeqN, 100*np.mean(correctRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' Hit Rate - Best Match (%)')\n  plt.subplot(2,2,2)\n  plt.semilogx(trainSeqN, 100*np.mean(missRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' Miss Rate (%)')\n  plt.subplot(2,2,3)\n  plt.semilogx(trainSeqN, 100*np.mean(fpRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' False Positive Rate (%)')\n  plt.savefig('result/ReberSequence_TMperformance.pdf')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createL4L2Column(network, networkConfig, suffix=\"\"):\n\n  externalInputName = \"externalInput\" + suffix\n  sensorInputName = \"sensorInput\" + suffix\n  L4ColumnName = \"L4Column\" + suffix\n  L2ColumnName = \"L2Column\" + suffix\n\n  L4Params = copy.deepcopy(networkConfig[\"L4Params\"])\n  L4Params[\"basalInputWidth\"] = networkConfig[\"externalInputSize\"]\n  L4Params[\"apicalInputWidth\"] = networkConfig[\"L2Params\"][\"cellCount\"]\n\n  if networkConfig[\"externalInputSize\"] > 0:\n    network.addRegion(\n      externalInputName, \"py.RawSensor\",\n      json.dumps({\"outputWidth\": networkConfig[\"externalInputSize\"]}))\n  network.addRegion(\n    sensorInputName, \"py.RawSensor\",\n    json.dumps({\"outputWidth\": networkConfig[\"sensorInputSize\"]}))\n\n  # Fixup network to include SP, if defined in networkConfig\n  if networkConfig[\"externalInputSize\"] > 0:\n    _addLateralSPRegion(network, networkConfig, suffix)\n  _addFeedForwardSPRegion(network, networkConfig, suffix)\n\n  network.addRegion(\n    L4ColumnName, networkConfig[\"L4RegionType\"],\n    json.dumps(L4Params))\n  network.addRegion(\n    L2ColumnName, \"py.ColumnPoolerRegion\",\n    json.dumps(networkConfig[\"L2Params\"]))\n\n  # Set phases appropriately so regions are executed in the proper sequence\n  # This is required when we create multiple columns - the order of execution\n  # is not the same as the order of region creation.\n  if networkConfig[\"externalInputSize\"] > 0:\n    network.setPhases(externalInputName,[0])\n  network.setPhases(sensorInputName,[0])\n\n  _setLateralSPPhases(network, networkConfig)\n  _setFeedForwardSPPhases(network, networkConfig)\n\n  # L4 and L2 regions always have phases 2 and 3, respectively\n  network.setPhases(L4ColumnName,[2])\n  network.setPhases(L2ColumnName,[3])\n\n  # Link SP region(s), if applicable\n  if networkConfig[\"externalInputSize\"] > 0:\n    _linkLateralSPRegion(network, networkConfig, externalInputName, L4ColumnName)\n  _linkFeedForwardSPRegion(network, networkConfig, sensorInputName, L4ColumnName)\n\n  # Link L4 to L2\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"activeCells\", destInput=\"feedforwardInput\")\n  network.link(L4ColumnName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"predictedActiveCells\",\n               destInput=\"feedforwardGrowthCandidates\")\n\n  # Link L2 feedback to L4\n  if networkConfig.get(\"enableFeedback\", True):\n    network.link(L2ColumnName, L4ColumnName, \"UniformLink\", \"\",\n                 srcOutput=\"feedForwardOutput\", destInput=\"apicalInput\",\n                 propagationDelay=1)\n\n  # Link reset output to L2 and L4\n  network.link(sensorInputName, L2ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n  network.link(sensorInputName, L4ColumnName, \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n\n  enableProfiling(network)\n\n  return network", "response": "Create a single column containing one L4 and one L2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a network consisting of multiple columns.", "response": "def createMultipleL4L2Columns(network, networkConfig):\n  \"\"\"\n  Create a network consisting of multiple columns.  Each column contains one L4\n  and one L2, is identical in structure to the network created by\n  createL4L2Column. In addition all the L2 columns are fully connected to each\n  other through their lateral inputs.\n\n  Region names have a column number appended as in externalInput_0,\n  externalInput_1, etc.\n\n  networkConfig must be of the following format (see createL4L2Column for\n  further documentation):\n\n    {\n      \"networkType\": \"MultipleL4L2Columns\",\n      \"numCorticalColumns\": 3,\n      \"externalInputSize\": 1024,\n      \"sensorInputSize\": 1024,\n      \"L4Params\": {\n        <constructor parameters for ApicalTMPairRegion\n      },\n      \"L2Params\": {\n        <constructor parameters for ColumnPoolerRegion>\n      },\n      \"lateralSPParams\": {\n        <constructor parameters for optional SPRegion>\n      },\n      \"feedForwardSPParams\": {\n        <constructor parameters for optional SPRegion>\n      }\n    }\n  \"\"\"\n\n  # Create each column\n  numCorticalColumns = networkConfig[\"numCorticalColumns\"]\n  for i in xrange(numCorticalColumns):\n    networkConfigCopy = copy.deepcopy(networkConfig)\n    layerConfig = networkConfigCopy[\"L2Params\"]\n    layerConfig[\"seed\"] = layerConfig.get(\"seed\", 42) + i\n\n    layerConfig[\"numOtherCorticalColumns\"] = numCorticalColumns - 1\n\n    suffix = \"_\" + str(i)\n    network = createL4L2Column(network, networkConfigCopy, suffix)\n\n  # Now connect the L2 columns laterally\n  for i in range(networkConfig[\"numCorticalColumns\"]):\n    suffixSrc = \"_\" + str(i)\n    for j in range(networkConfig[\"numCorticalColumns\"]):\n      if i != j:\n        suffixDest = \"_\" + str(j)\n        network.link(\n          \"L2Column\" + suffixSrc, \"L2Column\" + suffixDest,\n          \"UniformLink\", \"\",\n          srcOutput=\"feedForwardOutput\", destInput=\"lateralInput\",\n          propagationDelay=1)\n\n  enableProfiling(network)\n\n  return network"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef createMultipleL4L2ColumnsWithTopology(network, networkConfig):\n  numCorticalColumns = networkConfig[\"numCorticalColumns\"]\n  output_lateral_connections = [[] for i in\n      xrange(numCorticalColumns)]\n  input_lateral_connections = [[] for i in\n      xrange(numCorticalColumns)]\n\n\n  # If no column positions are provided, create a grid by default.\n  # This is typically what the user wants, so it makes sense to have it as\n  # a default.\n  columnPositions = networkConfig.get(\"columnPositions\", None)\n  if columnPositions is None:\n    columnPositions = []\n    side_length = int(numpy.ceil(numpy.sqrt(numCorticalColumns)))\n    for i in range(side_length):\n      for j in range(side_length):\n        columnPositions.append((i, j))\n    columnPositions = columnPositions[:numCorticalColumns]\n\n  # Determine which columns will be mutually connected.\n  # This has to be done before the actual creation of the network, as each\n  # individual column need to know how many columns it is laterally connected\n  # to.  These results are then used to actually connect the columns, once\n  # the network is created. It's awkward, but unavoidable.\n  longDistanceConnections = networkConfig.get(\"longDistanceConnections\", 0.)\n  for i, src_pos in enumerate(columnPositions):\n    for j, dest_pos in enumerate(columnPositions):\n      if i != j:\n        if (numpy.linalg.norm(numpy.asarray(src_pos) -\n            numpy.asarray(dest_pos)) <=\n            networkConfig[\"maxConnectionDistance\"] or\n            numpy.random.rand() < longDistanceConnections):\n          output_lateral_connections[i].append(j)\n          input_lateral_connections[j].append(i)\n\n  # Create each column\n  for i in xrange(numCorticalColumns):\n    networkConfigCopy = copy.deepcopy(networkConfig)\n    layerConfig = networkConfigCopy[\"L2Params\"]\n    layerConfig[\"seed\"] = layerConfig.get(\"seed\", 42) + i\n\n    layerConfig[\"numOtherCorticalColumns\"] = len(input_lateral_connections[i])\n\n    suffix = \"_\" + str(i)\n    network = createL4L2Column(network, networkConfigCopy, suffix)\n\n  # Now connect the L2 columns laterally\n  for i, connections in enumerate(output_lateral_connections):\n    suffixSrc = \"_\" + str(i)\n    for j in connections:\n      suffixDest = \"_\" + str(j)\n      network.link(\"L2Column\" + suffixSrc, \"L2Column\" + suffixDest,\n        \"UniformLink\", \"\", srcOutput=\"feedForwardOutput\",\n        destInput=\"lateralInput\", propagationDelay=1)\n\n  enableProfiling(network)\n  return network", "response": "Creates a network consisting of multiple columns with a topology of the L4 and one L2."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loadExperimentData(folder, area):\n  filename = os.path.join(folder, \"Combo3_{}.mat\".format(area))\n  contents = sio.loadmat(filename, variable_names=['data'],\n                         struct_as_record=False, squeeze_me=True)\n  return contents['data']", "response": "Loads the experiment s data from a MATLAB file into a python friendly structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets list of folders containing the data of each experiment.", "response": "def getExperimentList():\n  \"\"\"\n  Get list of folders containing the data of each experiment\n  :return: list of folders\n  \"\"\"\n  contents = sio.loadmat(\"./data/DataFolderList.mat\",\n                         variable_names=['DataFolderList'],\n                         struct_as_record=False,\n                         squeeze_me=True)\n\n  return contents['DataFolderList']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef classifierPredict(testVector, storedVectors):\n  numClasses = storedVectors.shape[0]\n  output = np.zeros((numClasses,))\n\n  for i in range(numClasses):\n    output[i] = np.sum(np.minimum(testVector, storedVectors[i, :]))\n  return output", "response": "Predict the overlap of the testVector with the stored representations for each object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef computeUniquePointsSensed(nCols, nPoints, s):\n  if nCols == 1:\n    return min(s, nPoints)\n  elif nCols < nPoints:\n    q = float(nCols) / nPoints\n    unique = min(int(round(( 1.0 - math.pow(1.0 - q, s)) * nPoints)),\n                 nPoints)\n    return unique\n  else:\n    return nPoints", "response": "Compute the number of unique points in a network with nCols columns senses an object s times."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the convergence point that diverges from target overlap values.", "response": "def locateConvergencePoint(stats):\n  \"\"\"\n  Walk backwards through stats until you locate the first point that diverges\n  from target overlap values.  We need this to handle cases where it might get\n  to target values, diverge, and then get back again.  We want the last\n  convergence point.\n  \"\"\"\n  n = len(stats)\n  for i in range(n):\n    if stats[n-i-1] < 1:\n      return n-i\n\n  # Never differs - converged in one iteration\n  return 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_ideal_classifier(args={}):\n  numObjects = args.get(\"numObjects\", 10)\n  numLocations = args.get(\"numLocations\", 10)\n  numFeatures = args.get(\"numFeatures\", 10)\n  numPoints = args.get(\"numPoints\", 10)\n  trialNum = args.get(\"trialNum\", 42)\n  useLocation = args.get(\"useLocation\", 1)\n  numColumns = args.get(\"numColumns\", 1)\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=150,\n    externalInputSize=2400,\n    numCorticalColumns=numColumns,\n    numFeatures=numFeatures,\n    numLocations=numLocations,\n    seed=trialNum\n  )\n  random.seed(trialNum)\n\n  objects.createRandomObjects(numObjects, numPoints=numPoints,\n                              numLocations=numLocations,\n                              numFeatures=numFeatures)\n\n  objectSDRs = objects.provideObjectsToLearn()\n  objectNames = objectSDRs.keys()\n  featureWidth = objects.sensorInputSize\n  locationWidth = objects.externalInputSize\n\n  # compute the number of sensations across all objects\n  numInputVectors = numPoints * numObjects\n\n  if useLocation:\n    inputWidth = featureWidth + locationWidth\n  else:\n    inputWidth = featureWidth\n\n  # create \"training\" dataset\n  data = np.zeros((numInputVectors, inputWidth))\n  label = np.zeros((numInputVectors, numObjects))\n  k = 0\n  for i in range(numObjects):\n    numSensations = len(objectSDRs[objectNames[i]])\n    for j in range(numSensations):\n      activeBitsFeature = np.array(list(objectSDRs[objectNames[i]][j][0][1]))\n      data[k, activeBitsFeature] = 1\n      if useLocation:\n        activeBitsLocation = np.array(list(objectSDRs[objectNames[i]][j][0][0]))\n        data[k, featureWidth + activeBitsLocation] = 1\n      label[k, i] = 1\n      k += 1\n\n  # enumerate number of distinct \"words\".\n  # Note: this could be done much more easily if we simply use the\n  # location/feature pairs that are stored in the object machine.\n  wordList = np.zeros((0, inputWidth), dtype='int32')\n  featureList = np.zeros((numInputVectors,))\n  for i in range(numInputVectors):\n    index = findWordInVocabulary(data[i, :], wordList)\n    if index is not None:\n      featureList[i] = index\n    else:\n      newWord = np.zeros((1, inputWidth), dtype='int32')\n      newWord[0, :] = data[i, :]\n      wordList = np.concatenate((wordList, newWord))\n      featureList[i] = wordList.shape[0] - 1\n\n  numWords = wordList.shape[0]\n\n  # convert objects to vectorized word representations\n  storedObjectRepresentations = np.zeros((numObjects, numWords), dtype=np.int32)\n  k = 0\n  for i in range(numObjects):\n    numSensations = len(objectSDRs[objectNames[i]])\n    for j in range(numSensations):\n      index = findWordInVocabulary(data[k, :], wordList)\n      storedObjectRepresentations[i, index] += 1\n      k += 1\n\n  # Cool plot of feature vectors\n  # plt.figure()\n  # plt.imshow(np.transpose(storedObjectRepresentations))\n  # plt.xlabel('Object #')\n  # plt.ylabel('Word #')\n  # plt.title(\"Object representations\")\n\n  # Create random order of sensations for each object\n  objectSensations = []\n  for i in range(numObjects):\n    senseList = []\n    wordIndices = np.where(storedObjectRepresentations[i, :])[0]\n    # An object can have multiple instances of a word, in which case we\n    # add all of them\n    for w in wordIndices:\n      senseList.extend(storedObjectRepresentations[i, w]*[w])\n    random.shuffle(senseList)\n    objectSensations.append(senseList)\n\n  # plot accuracy as a function of number of sensations\n  accuracyList = []\n  classificationOutcome = np.zeros((numObjects, numPoints+1))\n  for sensationNumber in range(1, numPoints+1):\n    bowVectorsTest = np.zeros((numObjects, numWords), dtype=np.int32)\n    for objectId in range(numObjects):\n      # No. sensations for object objectId\n      sensations = objectSensations[objectId]\n      numPointsToInclude = computeUniquePointsSensed(numColumns,\n                                             len(sensations), sensationNumber)\n      for j in range(numPointsToInclude):\n        index = sensations[j]\n        bowVectorsTest[objectId, index] += 1\n\n    # Count the number of correct classifications.\n    # A correct classification is where object i is unambiguously recognized.\n    numCorrect = 0\n    for i in range(numObjects):\n      overlaps = classifierPredict(bowVectorsTest[i, :], storedObjectRepresentations)\n      bestOverlap = max(overlaps)\n      outcome = ( (overlaps[i] == bestOverlap) and\n                  len(np.where(overlaps==bestOverlap)[0]) == 1)\n      numCorrect += outcome\n      classificationOutcome[i, sensationNumber] = outcome\n    accuracy = float(numCorrect) / numObjects\n    accuracyList.append(accuracy)\n\n  convergencePoint = np.zeros((numObjects, ))\n  for i in range(numObjects):\n    if np.max(classificationOutcome[i, :])>0:\n      convergencePoint[i] = locateConvergencePoint(classificationOutcome[i, :])\n    else:\n      convergencePoint[i] = 11\n\n  args.update({\"accuracy\": accuracyList})\n  args.update({\"numTouches\": range(1, 11)})\n  args.update({\"convergencePoint\": np.mean(convergencePoint)})\n  args.update({\"classificationOutcome\": classificationOutcome})\n  print \"objects={}, features={}, locations={}, distinct words={}, numColumns={}\".format(\n    numObjects, numFeatures, numLocations, numWords, numColumns),\n  print \"==> convergencePoint:\", args[\"convergencePoint\"]\n\n  return args", "response": "Create and train a classifier using given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plotConvergenceByObject(results, objectRange, featureRange, numTrials,\n                            linestyle='-'):\n  \"\"\"\n  Plots the convergence graph: iterations vs number of objects.\n  Each curve shows the convergence for a given number of unique features.\n  \"\"\"\n  ########################################################################\n  #\n  # Accumulate all the results per column in a convergence array.\n  #\n  # Convergence[f,o] = how long it took it to converge with f unique features\n  # and o objects.\n\n  convergence = numpy.zeros((max(featureRange), max(objectRange) + 1))\n  for r in results:\n    if r[\"numFeatures\"] in featureRange:\n      convergence[r[\"numFeatures\"] - 1, r[\"numObjects\"]] += r[\"convergencePoint\"]\n\n  convergence /= numTrials\n\n  ########################################################################\n  #\n  # Create the plot. x-axis=\n\n  # Plot each curve\n  legendList = []\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n\n  for i in range(len(featureRange)):\n    f = featureRange[i]\n    print \"features={} objectRange={} convergence={}\".format(\n      f,objectRange, convergence[f-1,objectRange])\n    legendList.append('Unique features={}'.format(f))\n    plt.plot(objectRange, convergence[f-1, objectRange],\n             color=colorList[i], linestyle=linestyle)\n\n  # format\n  plt.legend(legendList, loc=\"lower right\", prop={'size':10})\n  plt.xlabel(\"Number of objects in training set\")\n  plt.xticks(range(0,max(objectRange)+1,10))\n  plt.yticks(range(0,int(convergence.max())+2))\n  plt.ylabel(\"Average number of touches\")\n  plt.title(\"Number of touches to recognize one object (single column)\")", "response": "Plots the convergence graph for each object in the training set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the convergence graph for each column.", "response": "def plotConvergenceByColumn(results, columnRange, featureRange, numTrials, lineStype='-'):\n  \"\"\"\n  Plots the convergence graph: iterations vs number of columns.\n  Each curve shows the convergence for a given number of unique features.\n  \"\"\"\n  ########################################################################\n  #\n  # Accumulate all the results per column in a convergence array.\n  #\n  # Convergence[f,c] = how long it took it to  converge with f unique features\n  # and c columns.\n  convergence = numpy.zeros((max(featureRange), max(columnRange) + 1))\n  for r in results:\n    if r[\"numColumns\"] > max(columnRange):\n      continue\n    convergence[r[\"numFeatures\"] - 1,\n                r[\"numColumns\"]] += r[\"convergencePoint\"]\n  convergence /= numTrials\n  # For each column, print convergence as fct of number of unique features\n  # for c in range(1, max(columnRange) + 1):\n  #   print c, convergence[:, c]\n  # Print everything anyway for debugging\n  # print \"Average convergence array=\", convergence\n  ########################################################################\n  #\n  # Create the plot. x-axis=\n  # Plot each curve\n  legendList = []\n  colorList = ['r', 'b', 'g', 'm', 'c', 'k', 'y']\n  for i in range(len(featureRange)):\n    f = featureRange[i]\n    print f, columnRange\n    print convergence[f-1,columnRange]\n    legendList.append('Unique features={}'.format(f))\n    plt.plot(columnRange, convergence[f-1,columnRange],\n             color=colorList[i], linestyle=lineStype)\n  print\n  # format\n  plt.legend(legendList, loc=\"upper right\")\n  plt.xlabel(\"Number of columns\")\n  plt.xticks(columnRange)\n  plt.ylim([0, 4])\n  plt.yticks(range(0,int(convergence.max())+1))\n  plt.ylabel(\"Average number of sensations\")\n  plt.title(\"Number of sensations to recognize one object (multiple columns)\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_multiple_column_experiment():\n  # Create the objects\n  featureRange = [5, 10, 20, 30]\n  pointRange = 1\n  objectRange = [100]\n  numLocations = [10]\n  numPoints = 10\n  numTrials = 10\n  columnRange = [1, 2, 3, 4, 5, 6, 7, 8]\n  useLocation = 1\n\n  resultsDir = os.path.dirname(os.path.realpath(__file__))\n\n  args = []\n  for c in reversed(columnRange):\n    for o in reversed(objectRange):\n      for l in numLocations:\n        for f in featureRange:\n          for t in range(numTrials):\n            args.append(\n              {\"numObjects\": o,\n               \"numLocations\": l,\n               \"numFeatures\": f,\n               \"numColumns\": c,\n               \"trialNum\": t,\n               \"pointRange\": pointRange,\n               \"numPoints\": numPoints,\n               \"useLocation\": useLocation\n               }\n            )\n\n  print \"Number of experiments:\",len(args)\n  idealResultsFile = os.path.join(resultsDir,\n           \"ideal_multi_column_useLocation_{}.pkl\".format(useLocation))\n  pool = Pool(processes=cpu_count())\n  result = pool.map(run_ideal_classifier, args)\n\n  # Pickle results for later use\n  with open(idealResultsFile, \"wb\") as f:\n    cPickle.dump(result, f)\n\n  htmResultsFile = os.path.join(resultsDir, \"column_convergence_results.pkl\")\n  runExperimentPool(\n    numObjects=objectRange,\n    numLocations=[10],\n    numFeatures=featureRange,\n    numColumns=columnRange,\n    numPoints=10,\n    nTrials=numTrials,\n    numWorkers=cpu_count(),\n    resultsName=htmResultsFile)\n\n  with open(htmResultsFile, \"rb\") as f:\n    results = cPickle.load(f)\n\n  with open(idealResultsFile, \"rb\") as f:\n    resultsIdeal = cPickle.load(f)\n\n  plt.figure()\n  plotConvergenceByColumn(results, columnRange, featureRange, numTrials)\n  plotConvergenceByColumn(resultsIdeal, columnRange, featureRange, numTrials,\n                          \"--\")\n  plt.savefig('plots/ideal_observer_multiple_column.pdf')", "response": "Runs the ideal observer against a multi - column sensorimotor network."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef single_column_accuracy_comparison():\n  pointRange = 1\n  numTrials = 10\n  args = []\n\n  resultsDir = os.path.dirname(os.path.realpath(__file__))\n\n  for t in range(numTrials):\n    for useLocation in [0, 1]:\n      args.append(\n        {\"numObjects\": 100,\n         \"numLocations\": 10,\n         \"numFeatures\": 10,\n         \"numColumns\": 1,\n         \"trialNum\": t,\n         \"pointRange\": pointRange,\n         \"numPoints\": 10,\n         \"useLocation\": useLocation\n         }\n      )\n\n  print \"{} experiments to run, {} workers\".format(len(args), cpu_count())\n\n  idealResultsFile = os.path.join(resultsDir, \"ideal_model_result.pkl\")\n\n  # Run all experiments and pickle results for later use\n  pool = Pool(processes=cpu_count())\n  resultsIdeal = pool.map(run_ideal_classifier, args)\n\n  with open(idealResultsFile, \"wb\") as f:\n    cPickle.dump(resultsIdeal, f)\n\n  # run HTM network\n  columnRange = [1]\n  objectRange = [100]\n  numAmbiguousLocationsRange = [0]\n  htmResultsFile = os.path.join(resultsDir, \"single_column_convergence_results.pkl\")\n  runExperimentPool(\n    numObjects=objectRange,\n    numLocations=[10],\n    numFeatures=[10],\n    numColumns=columnRange,\n    numPoints=10,\n    settlingTime=1,\n    nTrials=numTrials,\n    numWorkers=cpu_count(),\n    ambiguousLocationsRange=numAmbiguousLocationsRange,\n    resultsName=htmResultsFile)\n\n  # Read results from pickle files\n  with open(idealResultsFile, \"rb\") as f:\n    resultsIdeal = cPickle.load(f)\n  with open(htmResultsFile, \"rb\") as f:\n    resultsModel = cPickle.load(f)\n\n  # plot accuracy across sensations\n  accuracyIdeal = 0\n  accuracyBOF = 0\n  for r in resultsIdeal:\n    if r[\"useLocation\"]:\n      accuracyIdeal += np.array(r['accuracy'])\n    else:\n      accuracyBOF += np.array(r['accuracy'])\n\n  accuracyIdeal /= len(resultsIdeal) / 2\n  accuracyBOF /= len(resultsIdeal) / 2\n\n  numTouches = len(accuracyIdeal)\n  accuracyModel = 0\n  for r in resultsModel:\n    accuracyModel += np.array(r['classificationPerSensation'])\n  accuracyModel /= len(resultsModel)\n\n  plt.figure()\n  plt.plot(np.arange(numTouches)+1, accuracyIdeal, '-o', label='Ideal observer (with location')\n  plt.plot(np.arange(numTouches) + 1, accuracyBOF, '-s', label='Ideal observer (no location)')\n  plt.plot(np.arange(numTouches)+1, accuracyModel, '-^', label='Sensorimotor network')\n  plt.xlabel(\"Number of sensations\")\n  plt.ylabel(\"Accuracy\")\n  plt.legend()\n  plt.savefig('plots/ideal_observer_comparison_single_column.pdf')", "response": "Plots accuracy of the ideal observer with and without locations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the call log into a pickled version of the call logs.", "response": "def save(callLog, logFilename):\n    \"\"\"\n    Save the call log history into this file.\n\n    @param  logFilename (path)\n            Filename in which to save a pickled version of the call logs.\n\n    \"\"\"\n    with open(logFilename, \"wb\") as outp:\n      cPickle.dump(callLog, outp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprunes all the weights whose absolute magnitude is less than minWeight.", "response": "def pruneWeights(self, minWeight):\n    \"\"\"\n    Prune all the weights whose absolute magnitude is less than minWeight\n    :param minWeight: min weight to prune. If zero then no pruning\n    :type minWeight: float\n    \"\"\"\n    if minWeight == 0.0:\n      return\n\n    # Collect all weights\n    weights = [v for k, v in self.named_parameters() if 'weight' in k]\n    for w in weights:\n      # Filter weights above threshold\n      mask = torch.ge(torch.abs(w.data), minWeight)\n      # Zero other weights\n      w.data.mul_(mask.type(torch.float32))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pruneDutycycles(self, threshold=0.0):\n    if threshold < 0.0:\n      return\n\n    # Collect all layers with 'dutyCycle'\n    for m in self.modules():\n      if m == self:\n        continue\n      if hasattr(m, 'pruneDutycycles'):\n        m.pruneDutycycles(threshold)", "response": "Prune all the units with dutycycles whose absolute magnitude is less than threshold."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _getDefaultCombinedL4Params(self, numInputBits, inputSize,\n                                  numExternalInputBits, externalInputSize,\n                                  L2CellCount):\n    \"\"\"\n    Returns a good default set of parameters to use in a combined L4 region.\n    \"\"\"\n    sampleSize = numExternalInputBits + numInputBits\n    activationThreshold = int(max(numExternalInputBits, numInputBits) * .6)\n    minThreshold = activationThreshold\n\n    return {\n      \"columnCount\": inputSize,\n      \"cellsPerColumn\": 16,\n      \"learn\": True,\n      \"learnOnOneCell\": False,\n      \"initialPermanence\": 0.41,\n      \"connectedPermanence\": 0.6,\n      \"permanenceIncrement\": 0.1,\n      \"permanenceDecrement\": 0.02,\n      \"minThreshold\": minThreshold,\n      \"basalPredictedSegmentDecrement\": 0.001,\n      \"apicalPredictedSegmentDecrement\": 0.0,\n      \"reducedBasalThreshold\": int(activationThreshold*0.6),\n      \"activationThreshold\": activationThreshold,\n      \"sampleSize\": sampleSize,\n      \"implementation\": \"ApicalTiebreak\",\n      \"seed\": self.seed,\n      \"basalInputWidth\": inputSize*16 + externalInputSize,\n      \"apicalInputWidth\": L2CellCount,\n    }", "response": "Returns a good default set of parameters to use in a combined L4 region."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef averageSequenceAccuracy(self, minOverlap, maxOverlap):\n    numCorrect = 0.0\n    numStats = 0.0\n    prefix = \"L4 PredictedActive\"\n\n    # For each object\n    for stats in self.statistics:\n\n      # Keep running total of how often the number of predictedActive cells are\n      # in the range.\n      for key in stats.iterkeys():\n        if prefix in key:\n          for numCells in stats[key]:\n            numStats += 1.0\n            if numCells in range(minOverlap, maxOverlap + 1):\n              numCorrect += 1.0\n\n    return numCorrect / numStats", "response": "Calculates the average probability of the sequence that is uniquely classified by checking whether the TM uniquely classified it by checking whether the number of predictedActive cells are in an acceptable range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the inference statistics for the current object.", "response": "def _updateInferenceStats(self, statistics, objectName=None):\n    \"\"\"\n    Updates the inference statistics.\n\n    Parameters:\n    ----------------------------\n    @param  statistics (dict)\n            Dictionary in which to write the statistics\n\n    @param  objectName (str)\n            Name of the inferred object, if known. Otherwise, set to None.\n\n    \"\"\"\n    L4Representations = self.getL4Representations()\n    L4PredictedCells = self.getL4PredictedCells()\n    L4PredictedActiveCells = self.getL4PredictedActiveCells()\n    L2Representation = self.getL2Representations()\n\n    for i in xrange(self.numColumns):\n      statistics[\"L4 Representation C\" + str(i)].append(\n        len(L4Representations[i])\n      )\n      statistics[\"L4 Predicted C\" + str(i)].append(\n        len(L4PredictedCells[i])\n      )\n      statistics[\"L4 PredictedActive C\" + str(i)].append(\n        len(L4PredictedActiveCells[i])\n      )\n      statistics[\"L2 Representation C\" + str(i)].append(\n        len(L2Representation[i])\n      )\n      statistics[\"L4 Apical Segments C\" + str(i)].append(\n        len(self.L4Columns[i]._tm.getActiveApicalSegments())\n      )\n      statistics[\"L4 Basal Segments C\" + str(i)].append(\n        len(self.L4Columns[i]._tm.getActiveBasalSegments())\n      )\n\n      # add true overlap if objectName was provided\n      if objectName in self.objectL2Representations:\n        objectRepresentation = self.objectL2Representations[objectName]\n        statistics[\"Overlap L2 with object C\" + str(i)].append(\n          len(objectRepresentation[i] & L2Representation[i])\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing n choose k.", "response": "def choose(n, k):\n  \"\"\"\n  Computes \"n choose k\". This can handle higher values than\n  scipy.special.binom().\n  \"\"\"\n  if isinstance(k, numbers.Number):\n    return _choose(n, k)\n  else:\n    return np.array([_choose(n, k2) for k2 in k])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getExpectedValue(distribution):\n  k = np.array(distribution.possibleValues)\n  return np.sum(k * distribution.pmf(k))", "response": "Calculates the expected value of a given distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generateExpectedList(numUniqueFeatures, numLocationsPerObject, maxNumObjects):\n  # We're choosing a location, checking its feature, and checking how many\n  # *other* occurrences there are of this feature. So we check n - 1 locations.\n  maxNumOtherLocations = maxNumObjects*10 - 1\n\n  results = zip(itertools.count(1),\n                findBinomialNsWithExpectedSampleMinimum(\n                  itertools.count(1), 1./numUniqueFeatures, numLocationsPerObject,\n                  maxNumOtherLocations))\n\n  finalResults = [(numOtherLocations, interpolatedN / numLocationsPerObject)\n                  for numOtherLocations, (interpolatedN, _, _) in results]\n\n  return [(0, 0.)] + finalResults", "response": "Generates a list of expected number of occurrences of an object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a list of lower bound values for the most unique feature.", "response": "def generateLowerBoundList(confidence, numUniqueFeatures, numLocationsPerObject,\n                           maxNumObjects):\n  \"\"\"\n  Metric: How unique is each object's most unique feature? Calculate the\n  probabilistic lower bound for the number of occurrences of an object's most\n  unique feature. For example, if confidence is 0.8, the tick \"3\" will be placed\n  at the point where 80% of objects are completely composed of features with 3\n  or more total occurrences, and 20% of objects have at least one feature that\n  has 2 or fewer total occurrences.\n  \"\"\"\n  # We're choosing a location, checking its feature, and checking how many\n  # *other* occurrences there are of this feature. So we check n - 1 locations.\n  maxNumOtherLocations = maxNumObjects*10 - 1\n\n  results = zip(itertools.count(1),\n                findBinomialNsWithLowerBoundSampleMinimum(\n                  confidence,\n                  itertools.count(1), 1./numUniqueFeatures, numLocationsPerObject,\n                  maxNumOtherLocations))\n\n  finalResults = [(numOtherLocations, interpolatedN / numLocationsPerObject)\n                  for numOtherLocations, (interpolatedN, _, _) in results]\n\n  return finalResults"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract a set of tick locations and labels. The input ticks are assumed to mean \"How many *other* occurrences are there of the sensed feature?\" but we want to show how many *total* occurrences there are. So we add 1. We label tick 2, and then 5, 10, 15, 20, ... @param ticks A list of ticks, typically calculated by one of the above generate*List functions.", "response": "def getTotalExpectedOccurrencesTicks_2_5(ticks):\n  \"\"\"\n  Extract a set of tick locations and labels. The input ticks are assumed to\n  mean \"How many *other* occurrences are there of the sensed feature?\" but we\n  want to show how many *total* occurrences there are. So we add 1.\n\n  We label tick 2, and then 5, 10, 15, 20, ...\n\n  @param ticks\n  A list of ticks, typically calculated by one of the above generate*List functions.\n  \"\"\"\n  locs = [loc\n          for label, loc in ticks]\n  labels = [(str(label + 1) if\n             (label + 1 == 2\n             or (label+1) % 5 == 0)\n             else \"\")\n            for label, loc in ticks]\n  return locs, labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tempoAdjust1(self, tempoFactor):\n\n    if self.apicalIntersect.any():\n      tempoFactor = tempoFactor * 0.5\n\n    else:\n      tempoFactor = tempoFactor * 2\n\n    return tempoFactor", "response": "Adjust tempo based on recent active apical input only\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tempoAdjust2(self, tempoFactor):\n\n    late_votes = (len(self.adtm.getNextBasalPredictedCells()) - len(self.apicalIntersect)) * -1\n    early_votes = len(self.apicalIntersect)\n    votes = late_votes + early_votes\n    print('vote tally', votes)\n\n    if votes > 0:\n      tempoFactor = tempoFactor * 0.5\n      print 'speed up'\n\n    elif votes < 0:\n      tempoFactor = tempoFactor * 2\n      print 'slow down'\n\n    elif votes == 0:\n      print 'pick randomly'\n      if random.random() > 0.5:\n        tempoFactor = tempoFactor * 0.5\n        print 'random pick: speed up'\n      else:\n        tempoFactor = tempoFactor * 2\n        print 'random pick: slow down'\n\n    return tempoFactor", "response": "Adjusts the tempo by aggregating active basal cells for pre vs. post and the MC clock from last sequence item."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tempoAdjust3(self, tempoFactor):\n\n    late_votes = (len(self.adtm.getNextBasalPredictedCells()) - len(self.apicalIntersect)) * -1\n    early_votes = len(self.apicalIntersect)\n    votes = late_votes + early_votes\n    print('vote tally', votes)\n\n    if votes > 0:\n      tempoFactor = tempoFactor * 0.5\n      self.prevVote = 0.5\n      print 'speed up'\n\n    elif votes < 0:\n      tempoFactor = tempoFactor * 2\n      self.prevVote = 2\n      print 'slow down'\n\n    elif votes == 0:\n\n      if self.prevVote == 0:\n        print 'pick randomly'\n        if random.random() > 0.5:\n          tempoFactor = tempoFactor * 0.5\n          self.prevVote = 0.5\n          print 'random pick: speed up'\n        else:\n          tempoFactor = tempoFactor * 2\n          self.prevVote = 2\n          print 'random pick: slow down'\n\n      else:\n        print 'tied: re-choose last choice'\n        tempoFactor = tempoFactor * self.prevVote\n\n    return tempoFactor", "response": "Adjusts the tempo by aggregating active basal cells for pre vs. post and adjusts the tempo by re - choosing the last one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadjust the tempo of the last sequence item in a new order.", "response": "def tempoAdjust4(self, tempoFactor):\n    \"\"\"\n    Adjust tempo by aggregating active basal cell votes for pre vs. post\n    -> if vote total = 0 (tied), use result of last vote\n    -> if two wrong scale decisions in a row, use max/min tempo in opposite direction\n\n    :param tempoFactor: scaling signal to MC clock from last sequence item\n    :return: adjusted scaling signal\n    \"\"\"\n\n    late_votes = (len(self.adtm.getNextBasalPredictedCells()) - len(self.apicalIntersect)) * -1\n    early_votes = len(self.apicalIntersect)\n    votes = late_votes + early_votes\n    print('vote tally', votes)\n\n    if votes > 0:\n      newScale = 0.5\n      print 'speed up'\n\n    elif votes < 0:\n      newScale = 2\n      print 'slow down'\n\n    elif votes == 0:\n\n      if not self.scaleAdjustmentHistory:\n        print 'pick randomly'\n        if random.random() > 0.5:\n          newScale = 0.5\n          print 'random pick: speed up'\n        else:\n          newScale = 2\n          print 'random pick: slow down'\n\n      else:\n        print 'tied: re-choose last choice'\n        newScale = self.scaleAdjustmentHistory[-1]\n\n\n    if self.disableVoting == 1:\n\n      newScale = self.scaleAdjustmentHistory[-1]\n\n    if len(self.scaleAdjustmentHistory) >1:\n\n      if self.scaleAdjustmentHistory[-2] == self.scaleAdjustmentHistory[-1]:\n\n        prevDirection = self.scaleAdjustmentHistory[-1]\n        newScale = (prevDirection * prevDirection)**-2\n        print('2 errors in a row; new scale is', tempoFactor * newScale)\n\n        self.scaleAdjustmentHistory = []\n        self.scaleAdjustmentHistory.append(prevDirection)\n        tempoFactor = tempoFactor * newScale\n        return tempoFactor\n\n    self.scaleAdjustmentHistory.append(newScale)\n    tempoFactor = tempoFactor * newScale\n    print tempoFactor\n    return tempoFactor"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sampleRange(rng, start, end, step, k):\n  array = numpy.empty(k, dtype=\"uint32\")\n  rng.sample(numpy.arange(start, end, step, dtype=\"uint32\"), array)\n  return array", "response": "Sample from a random number generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsamples k random integers from arr.", "response": "def _sample(rng, arr, k):\n  \"\"\"\n  Equivalent to:\n\n  random.sample(arr, k)\n\n  except it uses our random number generator.\n  \"\"\"\n  selected = numpy.empty(k, dtype=\"uint32\")\n  rng.sample(numpy.asarray(arr, dtype=\"uint32\"),\n             selected)\n  return selected"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _countWhereGreaterEqualInRows(sparseMatrix, rows, threshold):\n  return sum(sparseMatrix.countWhereGreaterOrEqual(row, row+1,\n                                                   0, sparseMatrix.nCols(),\n                                                   threshold)\n             for row in rows)", "response": "Like countWhereGreaterOrEqual but for an arbitrary selection of rows and\n \u2013 threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute(self, feedforwardInput=(), lateralInputs=(),\n              feedforwardGrowthCandidates=None, learn=True,\n              predictedInput = None,):\n    \"\"\"\n    Runs one time step of the column pooler algorithm.\n\n    @param  feedforwardInput (sequence)\n            Sorted indices of active feedforward input bits\n\n    @param  lateralInputs (list of sequences)\n            For each lateral layer, a list of sorted indices of active lateral\n            input bits\n\n    @param  feedforwardGrowthCandidates (sequence or None)\n            Sorted indices of feedforward input bits that active cells may grow\n            new synapses to. If None, the entire feedforwardInput is used.\n\n    @param  learn (bool)\n            If True, we are learning a new object\n\n    @param predictedInput (sequence)\n           Sorted indices of predicted cells in the TM layer.\n    \"\"\"\n\n    if feedforwardGrowthCandidates is None:\n      feedforwardGrowthCandidates = feedforwardInput\n\n    # inference step\n    if not learn:\n      self._computeInferenceMode(feedforwardInput, lateralInputs)\n\n    # learning step\n    elif not self.onlineLearning:\n      self._computeLearningMode(feedforwardInput, lateralInputs,\n                                feedforwardGrowthCandidates)\n    # online learning step\n    else:\n      if (predictedInput is not None and\n          len(predictedInput) > self.predictedInhibitionThreshold):\n        predictedActiveInput = numpy.intersect1d(feedforwardInput,\n                                                 predictedInput)\n        predictedGrowthCandidates = numpy.intersect1d(\n            feedforwardGrowthCandidates, predictedInput)\n        self._computeInferenceMode(predictedActiveInput, lateralInputs)\n        self._computeLearningMode(predictedActiveInput, lateralInputs,\n                                  feedforwardGrowthCandidates)\n      elif not self.minSdrSize <= len(self.activeCells) <= self.maxSdrSize:\n        # If the pooler doesn't have a single representation, try to infer one,\n        # before actually attempting to learn.\n        self._computeInferenceMode(feedforwardInput, lateralInputs)\n        self._computeLearningMode(feedforwardInput, lateralInputs,\n                                  feedforwardGrowthCandidates)\n      else:\n        # If there isn't predicted input and we have a single SDR,\n        # we are extending that representation and should just learn.\n        self._computeLearningMode(feedforwardInput, lateralInputs,\n                                  feedforwardGrowthCandidates)", "response": "This method runs one time step of the column pooler algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _computeLearningMode(self, feedforwardInput, lateralInputs,\n                                 feedforwardGrowthCandidates):\n    \"\"\"\n    Learning mode: we are learning a new object in an online fashion. If there\n    is no prior activity, we randomly activate 'sdrSize' cells and create\n    connections to incoming input. If there was prior activity, we maintain it.\n    If we have a union, we simply do not learn at all.\n\n    These cells will represent the object and learn distal connections to each\n    other and to lateral cortical columns.\n\n    Parameters:\n    ----------------------------\n    @param  feedforwardInput (sequence)\n            Sorted indices of active feedforward input bits\n\n    @param  lateralInputs (list of sequences)\n            For each lateral layer, a list of sorted indices of active lateral\n            input bits\n\n    @param  feedforwardGrowthCandidates (sequence or None)\n            Sorted indices of feedforward input bits that the active cells may\n            grow new synapses to.  This is assumed to be the predicted active\n            cells of the input layer.\n    \"\"\"\n    prevActiveCells = self.activeCells\n\n    # If there are not enough previously active cells, then we are no longer on\n    # a familiar object.  Either our representation decayed due to the passage\n    # of time (i.e. we moved somewhere else) or we were mistaken.  Either way,\n    # create a new SDR and learn on it.\n    # This case is the only way different object representations are created.\n    # enforce the active cells in the output layer\n    if len(self.activeCells) < self.minSdrSize:\n      self.activeCells = _sampleRange(self._random,\n                                      0, self.numberOfCells(),\n                                      step=1, k=self.sdrSize)\n      self.activeCells.sort()\n\n    # If we have a union of cells active, don't learn.  This primarily affects\n    # online learning.\n    if len(self.activeCells) > self.maxSdrSize:\n      return\n\n    # Finally, now that we have decided which cells we should be learning on, do\n    # the actual learning.\n    if len(feedforwardInput) > 0:\n      self._learn(self.proximalPermanences, self._random,\n                  self.activeCells, feedforwardInput,\n                  feedforwardGrowthCandidates, self.sampleSizeProximal,\n                  self.initialProximalPermanence, self.synPermProximalInc,\n                  self.synPermProximalDec, self.connectedPermanenceProximal)\n\n      # External distal learning\n      for i, lateralInput in enumerate(lateralInputs):\n        self._learn(self.distalPermanences[i], self._random,\n                    self.activeCells, lateralInput, lateralInput,\n                    self.sampleSizeDistal, self.initialDistalPermanence,\n                    self.synPermDistalInc, self.synPermDistalDec,\n                    self.connectedPermanenceDistal)\n\n      # Internal distal learning\n      self._learn(self.internalDistalPermanences, self._random,\n                  self.activeCells, prevActiveCells, prevActiveCells,\n                  self.sampleSizeDistal, self.initialDistalPermanence,\n                  self.synPermDistalInc, self.synPermDistalDec,\n                  self.connectedPermanenceDistal)", "response": "Compute the learning mode of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the inference mode of the feedforward and lateral objects.", "response": "def _computeInferenceMode(self, feedforwardInput, lateralInputs):\n    \"\"\"\n    Inference mode: if there is some feedforward activity, perform\n    spatial pooling on it to recognize previously known objects, then use\n    lateral activity to activate a subset of the cells with feedforward\n    support. If there is no feedforward activity, use lateral activity to\n    activate a subset of the previous active cells.\n\n    Parameters:\n    ----------------------------\n    @param  feedforwardInput (sequence)\n            Sorted indices of active feedforward input bits\n\n    @param  lateralInputs (list of sequences)\n            For each lateral layer, a list of sorted indices of active lateral\n            input bits\n    \"\"\"\n\n    prevActiveCells = self.activeCells\n\n    # Calculate the feedforward supported cells\n    overlaps = self.proximalPermanences.rightVecSumAtNZGteThresholdSparse(\n      feedforwardInput, self.connectedPermanenceProximal)\n    feedforwardSupportedCells = numpy.where(\n      overlaps >= self.minThresholdProximal)[0]\n\n    # Calculate the number of active segments on each cell\n    numActiveSegmentsByCell = numpy.zeros(self.cellCount, dtype=\"int\")\n    overlaps = self.internalDistalPermanences.rightVecSumAtNZGteThresholdSparse(\n      prevActiveCells, self.connectedPermanenceDistal)\n    numActiveSegmentsByCell[overlaps >= self.activationThresholdDistal] += 1\n    for i, lateralInput in enumerate(lateralInputs):\n      overlaps = self.distalPermanences[i].rightVecSumAtNZGteThresholdSparse(\n        lateralInput, self.connectedPermanenceDistal)\n      numActiveSegmentsByCell[overlaps >= self.activationThresholdDistal] += 1\n\n    chosenCells = []\n\n    # First, activate the FF-supported cells that have the highest number of\n    # lateral active segments (as long as it's not 0)\n    if len(feedforwardSupportedCells) == 0:\n      pass\n    else:\n      numActiveSegsForFFSuppCells = numActiveSegmentsByCell[\n        feedforwardSupportedCells]\n\n      # This loop will select the FF-supported AND laterally-active cells, in\n      # order of descending lateral activation, until we exceed the sdrSize\n      # quorum - but will exclude cells with 0 lateral active segments.\n      ttop = numpy.max(numActiveSegsForFFSuppCells)\n      while ttop > 0 and len(chosenCells) < self.sdrSize:\n        chosenCells = numpy.union1d(chosenCells,\n                    feedforwardSupportedCells[numActiveSegsForFFSuppCells >= ttop])\n        ttop -= 1\n\n    # If we haven't filled the sdrSize quorum, add in inertial cells.\n    if len(chosenCells) < self.sdrSize:\n      if self.useInertia:\n        prevCells = numpy.setdiff1d(prevActiveCells, chosenCells)\n        inertialCap = int(len(prevCells) * self.inertiaFactor)\n        if inertialCap > 0:\n          numActiveSegsForPrevCells = numActiveSegmentsByCell[prevCells]\n          # We sort the previously-active cells by number of active lateral\n          # segments (this really helps).  We then activate them in order of\n          # descending lateral activation.\n          sortIndices = numpy.argsort(numActiveSegsForPrevCells)[::-1]\n          prevCells = prevCells[sortIndices]\n          numActiveSegsForPrevCells = numActiveSegsForPrevCells[sortIndices]\n\n          # We use inertiaFactor to limit the number of previously-active cells\n          # which can become active, forcing decay even if we are below quota.\n          prevCells = prevCells[:inertialCap]\n          numActiveSegsForPrevCells = numActiveSegsForPrevCells[:inertialCap]\n\n          # Activate groups of previously active cells by order of their lateral\n          # support until we either meet quota or run out of cells.\n          ttop = numpy.max(numActiveSegsForPrevCells)\n          while ttop >= 0 and len(chosenCells) < self.sdrSize:\n            chosenCells = numpy.union1d(chosenCells,\n                        prevCells[numActiveSegsForPrevCells >= ttop])\n            ttop -= 1\n\n    # If we haven't filled the sdrSize quorum, add cells that have feedforward\n    # support and no lateral support.\n    discrepancy = self.sdrSize - len(chosenCells)\n    if discrepancy > 0:\n      remFFcells = numpy.setdiff1d(feedforwardSupportedCells, chosenCells)\n\n      # Inhibit cells proportionally to the number of cells that have already\n      # been chosen. If ~0 have been chosen activate ~all of the feedforward\n      # supported cells. If ~sdrSize have been chosen, activate very few of\n      # the feedforward supported cells.\n\n      # Use the discrepancy:sdrSize ratio to determine the number of cells to\n      # activate.\n      n = (len(remFFcells) * discrepancy) // self.sdrSize\n      # Activate at least 'discrepancy' cells.\n      n = max(n, discrepancy)\n      # If there aren't 'n' available, activate all of the available cells.\n      n = min(n, len(remFFcells))\n\n      if len(remFFcells) > n:\n        selected = _sample(self._random, remFFcells, n)\n        chosenCells = numpy.append(chosenCells, selected)\n      else:\n        chosenCells = numpy.append(chosenCells, remFFcells)\n\n    chosenCells.sort()\n    self.activeCells = numpy.asarray(chosenCells, dtype=\"uint32\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef numberOfConnectedProximalSynapses(self, cells=None):\n    if cells is None:\n      cells = xrange(self.numberOfCells())\n\n    return _countWhereGreaterEqualInRows(self.proximalPermanences, cells,\n                                         self.connectedPermanenceProximal)", "response": "Returns the number of proximal connected synapses on these cells."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef numberOfProximalSynapses(self, cells=None):\n    if cells is None:\n      cells = xrange(self.numberOfCells())\n\n    n = 0\n    for cell in cells:\n      n += self.proximalPermanences.nNonZerosOnRow(cell)\n    return n", "response": "Returns the number of proximal synapses with permanence > 0 on these cells."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef numberOfDistalSegments(self, cells=None):\n    if cells is None:\n      cells = xrange(self.numberOfCells())\n\n    n = 0\n\n    for cell in cells:\n      if self.internalDistalPermanences.nNonZerosOnRow(cell) > 0:\n        n += 1\n\n      for permanences in self.distalPermanences:\n        if permanences.nNonZerosOnRow(cell) > 0:\n          n += 1\n\n    return n", "response": "Returns the total number of distal segments for the given cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of connected distal synapses on these cells.", "response": "def numberOfConnectedDistalSynapses(self, cells=None):\n    \"\"\"\n    Returns the number of connected distal synapses on these cells.\n\n    Parameters:\n    ----------------------------\n    @param  cells (iterable)\n            Indices of the cells. If None return count for all cells.\n    \"\"\"\n    if cells is None:\n      cells = xrange(self.numberOfCells())\n\n    n = _countWhereGreaterEqualInRows(self.internalDistalPermanences, cells,\n                                      self.connectedPermanenceDistal)\n\n    for permanences in self.distalPermanences:\n      n += _countWhereGreaterEqualInRows(permanences, cells,\n                                         self.connectedPermanenceDistal)\n\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _learn(# mutated args\n             permanences, rng,\n\n             # activity\n             activeCells, activeInput, growthCandidateInput,\n\n             # configuration\n             sampleSize, initialPermanence, permanenceIncrement,\n             permanenceDecrement, connectedPermanence):\n    \"\"\"\n    For each active cell, reinforce active synapses, punish inactive synapses,\n    and grow new synapses to a subset of the active input bits that the cell\n    isn't already connected to.\n\n    Parameters:\n    ----------------------------\n    @param  permanences (SparseMatrix)\n            Matrix of permanences, with cells as rows and inputs as columns\n\n    @param  rng (Random)\n            Random number generator\n\n    @param  activeCells (sorted sequence)\n            Sorted list of the cells that are learning\n\n    @param  activeInput (sorted sequence)\n            Sorted list of active bits in the input\n\n    @param  growthCandidateInput (sorted sequence)\n            Sorted list of active bits in the input that the activeCells may\n            grow new synapses to\n\n    For remaining parameters, see the __init__ docstring.\n    \"\"\"\n\n    permanences.incrementNonZerosOnOuter(\n      activeCells, activeInput, permanenceIncrement)\n    permanences.incrementNonZerosOnRowsExcludingCols(\n      activeCells, activeInput, -permanenceDecrement)\n    permanences.clipRowsBelowAndAbove(\n      activeCells, 0.0, 1.0)\n    if sampleSize == -1:\n      permanences.setZerosOnOuter(\n        activeCells, activeInput, initialPermanence)\n    else:\n      existingSynapseCounts = permanences.nNonZerosPerRowOnCols(\n        activeCells, activeInput)\n\n      maxNewByCell = numpy.empty(len(activeCells), dtype=\"int32\")\n      numpy.subtract(sampleSize, existingSynapseCounts, out=maxNewByCell)\n\n      permanences.setRandomZerosOnOuter(\n        activeCells, growthCandidateInput, maxNewByCell, initialPermanence, rng)", "response": "This function is called by _getActiveBitSequence and _getActiveBitSequence_internal."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a PoolOfPairsLocation1DExperiment for n active minicolumns and cellsPerColumn and writes the results to a file.", "response": "def runExperiment(n, w, threshold, cellsPerColumn, folder, numTrials=5,\n                  cleverTMSDRs=False):\n  \"\"\"\n  Run a PoolOfPairsLocation1DExperiment various union sizes.\n  \"\"\"\n  if not os.path.exists(folder):\n    try:\n      os.makedirs(folder)\n    except OSError:\n      # Multiple parallel tasks might create the folder. That's fine.\n      pass\n\n  filename = \"{}/n_{}_w_{}_threshold_{}_cellsPerColumn_{}.json\".format(\n    folder, n, w, threshold, cellsPerColumn)\n  if len(glob.glob(filename)) == 0:\n    print(\"Starting: {}/n_{}_w_{}_threshold_{}_cellsPerColumn_{}\".format(\n      folder, n, w, threshold, cellsPerColumn))\n\n    result = defaultdict(list)\n    for _ in xrange(numTrials):\n\n      exp = PoolOfPairsLocation1DExperiment(**{\n        \"numMinicolumns\": n,\n        \"numActiveMinicolumns\": w,\n        \"poolingThreshold\": threshold,\n        \"cellsPerColumn\": cellsPerColumn,\n        \"minicolumnSDRs\": generateMinicolumnSDRs(n=n, w=w, threshold=threshold),\n      })\n\n      if cleverTMSDRs:\n        exp.trainWithSpecificPairSDRs(carefullyCollideContexts(\n          numContexts=25, numCells=cellsPerColumn, numMinicolumns = n))\n      else:\n        exp.train()\n\n      for unionSize in [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25]:\n        additionalSDRCounts = exp.testInferenceOnUnions(unionSize)\n\n        result[unionSize] += additionalSDRCounts\n\n    with open(filename, \"w\") as fOut:\n      json.dump(sorted(result.items(), key=lambda x: x[0]),\n                fOut)\n      print(\"Wrote:\", filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrain the pair layer and pooling layer.", "response": "def train(self):\n    \"\"\"\n    Train the pair layer and pooling layer.\n    \"\"\"\n    for iDriving, cDriving in enumerate(self.drivingOperandSDRs):\n      minicolumnSDR = self.minicolumnSDRs[iDriving]\n      self.pairLayerProximalConnections.associate(minicolumnSDR, cDriving)\n      for iContext, cContext in enumerate(self.contextOperandSDRs):\n        iResult = (iContext + iDriving) % self.numLocations\n        cResult = self.resultSDRs[iResult]\n        self.pairLayer.compute(minicolumnSDR, basalInput=cContext)\n        cPair = self.pairLayer.getWinnerCells()\n        self.poolingLayer.associate(cResult, cPair)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrains the pair layer and pooling layer with a set of specific pair SDRs.", "response": "def trainWithSpecificPairSDRs(self, pairLayerContexts):\n    \"\"\"\n    Train the pair layer and pooling layer, manually choosing which contexts\n    each cell will encode (i.e. the pair layer's distal connections).\n\n    @param pairLayerContexts (list of lists of lists of ints)\n    iContext integers for each cell, grouped by minicolumn. For example,\n      [[[1, 3], [2,4]],\n       [[1, 2]]]\n    would specify that cell 0 connects to location 1 and location 3, while cell\n    1 connects to locations 2 and 4, and cell 2 (in the second minicolumn)\n    connects to locations 1 and 2.\n    \"\"\"\n    # Grow basal segments in the pair layer.\n    for iMinicolumn, contextsByCell in enumerate(pairLayerContexts):\n      for iCell, cellContexts in enumerate(contextsByCell):\n        iCellAbsolute = iMinicolumn*self.pairLayer.getCellsPerColumn() + iCell\n        for context in cellContexts:\n          segments = self.pairLayer.basalConnections.createSegments(\n            [iCellAbsolute])\n          self.pairLayer.basalConnections.growSynapses(\n            segments, self.contextOperandSDRs[context], 1.0)\n\n    # Associate the pair layer's minicolumn SDRs with offset cell SDRs,\n    # and associate the pooling layer's location SDRs with a pool of pair SDRs.\n    for iDriving, cDriving in enumerate(self.drivingOperandSDRs):\n      minicolumnSDR = self.minicolumnSDRs[iDriving]\n      self.pairLayerProximalConnections.associate(minicolumnSDR, cDriving)\n\n      for iContext, cContext in enumerate(self.contextOperandSDRs):\n        iResult = (iContext + iDriving) % self.numLocations\n        cResult = self.resultSDRs[iResult]\n        cPair = [\n          iMinicolumn*self.pairLayer.getCellsPerColumn() + iCell\n          for iMinicolumn in minicolumnSDR\n          for iCell, cellContexts in enumerate(pairLayerContexts[iMinicolumn])\n          if iContext in cellContexts]\n        assert len(cPair) == len(minicolumnSDR)\n\n        self.poolingLayer.associate(cResult, cPair)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning an experiment to test the false positive rate based on number of synapses per dendrite and dimension and sparsity.", "response": "def run_HTM_false_positive_experiment_synapses(num_neurons = 1,\n                         a = 512,\n                         dim = 16000,\n                         num_samples = 1000,\n                         num_dendrites = 500,\n                         test_dendrite_lengths = range(2, 32, 2),\n                         num_trials = 1000):\n  \"\"\"\n  Run an experiment to test the false positive rate based on number of\n  synapses per dendrite, dimension and sparsity.  Uses a single neuron,\n  with a threshold nonlinearity of theta = s/2.\n\n  Based on figure 5B in the original SDR paper.\n\n  The parameters used in generating the figure for this experiment are:\n  1.  a = 512, dim = 16000\n  2.  a = 4000, dim = 16000\n  3.  a = 32, dim = 2000\n  4.  a = 32, dim = 4000\n  In each case, we used 1000 samples per trial, 1000 trials, 500 dendrite\n  segments, and tested dendrite lengths in (2, 4, ..., 30), with the threshold\n  for each segment being half its length.\n  \"\"\"\n  for dendrite_length in test_dendrite_lengths:\n    nonlinearity = threshold_nonlinearity(dendrite_length / 2)\n\n    fps = []\n    fns = []\n\n    for trial in range(num_trials):\n\n      neuron = Neuron(size = dendrite_length*num_dendrites, num_dendrites = num_dendrites, dendrite_length = dendrite_length, dim = dim, nonlinearity = nonlinearity)\n      pos, neg = generate_evenly_distributed_data_sparse(dim = dim, num_active = a, num_samples = num_samples/2), generate_evenly_distributed_data_sparse(dim = dim, num_active = a, num_samples = num_samples/2)\n      #labels = numpy.asarray([1 for i in range(num_samples/2)] + [-1 for i in range(num_samples/2)])\n\n      neuron.HTM_style_initialize_on_data(pos, numpy.asarray([1 for i in range(num_samples/2)]))\n\n      error, fp, fn = get_error(neg, [-1 for i in range(num_samples/2)], [neuron])\n\n      fps.append(fp)\n      fns.append(fn)\n      print \"Error at {} synapses per dendrite is {}, with {} false positives and {} false negatives\".format(dendrite_length, fp/(num_samples/2.), fp, fn)\n\n    with open(\"num_dendrites_FP_{}_{}.txt\".format(a, dim), \"a\") as f:\n      f.write(str(dendrite_length) + \", \" + str(sum(fps)) + \", \" + str(num_trials*num_samples/2.) + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_noise_experiment(num_neurons = 1,\n             a = 128,\n             dim = 6000,\n             test_noise_levels = range(15, 100, 5),\n             num_samples = 500,\n             num_dendrites = 500,\n             dendrite_length = 30,\n             theta = 8,\n             num_trials = 100):\n  \"\"\"\n  Tests the impact of noise on a neuron, using an HTM approach to a P&M\n  model of a neuron.  Nonlinearity is a simple threshold at theta, as in the\n  original version of this experiment, and each dendrite is bound by the\n  initialization to a single pattern.  Only one neuron is used, unlike in the\n  P&M classification experiment, and a successful identification is simply\n  defined as at least one dendrite having theta active synapses.\n\n  Training is done via HTM-style initialization.  In the event that the init\n  fails to produce an error rate of 0 without noise (which anecdotally never\n  occurs), we simple reinitialize.\n\n  Results are saved to the file noise_FN_{theta}.txt.\n\n  This corresponds to the false negative vs. noise level figure in the paper.\n  To generate the results shown, we used theta = 8, theta = 12 and theta = 16,\n  with noise levels in range(15, 85, 5), 500 dendrites and 30 synapses per\n  dendrite.  We generated 500 sample SDRs, one per dendrite, and ran 100 trials\n  at each noise level.  Each SDR had a = 128, dim = 6000.\n  \"\"\"\n\n  nonlinearity = threshold_nonlinearity(theta)\n  for noise in test_noise_levels:\n\n    fps = []\n    fns = []\n\n    for trial in range(num_trials):\n\n      successful_initialization = False\n      while not successful_initialization:\n        neuron = Neuron(size = dendrite_length*num_dendrites, num_dendrites = num_dendrites, dendrite_length = dendrite_length, dim = dim, nonlinearity = nonlinearity)\n        data = generate_evenly_distributed_data_sparse(dim = dim, num_active = a, num_samples = num_samples)\n        labels = [1 for i in range(num_samples)]\n\n        neuron.HTM_style_initialize_on_data(data, labels)\n\n        error, fp, fn = get_error(data, labels, [neuron])\n        print \"Initialization error is {}, with {} false positives and {} false negatives\".format(error, fp, fn)\n        if error == 0:\n          successful_initialization = True\n        else:\n          print \"Repeating to get a successful initialization\"\n\n      apply_noise(data, noise)\n      error, fp, fn = get_error(data, labels, [neuron])\n      fps.append(fp)\n      fns.append(fn)\n      print \"Error at noise {} is {}, with {} false positives and {} false negatives\".format(noise, error, fp, fn)\n\n    with open(\"noise_FN_{}.txt\".format(theta), \"a\") as f:\n      f.write(str(noise) + \", \" + str(numpy.sum(fns)) + \", \" + str(num_trials*num_samples) + \"\\n\")", "response": "Runs the noise experiment on a single neuron."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the object with the given experiment parameters and default values from the given configuration file.", "response": "def initialize(self, params, repetition):\n    \"\"\"\n    Initialize experiment parameters and default values from configuration file\n    \"\"\"\n    self.name = params[\"name\"]\n    self.dataDir = params.get(\"datadir\", \"data\")\n    self.seed = params.get(\"seed\", 42) + repetition\n\n    torch.manual_seed(self.seed)\n    np.random.seed(self.seed)\n    random.seed(self.seed)\n\n    # Training\n    self.epochs = params.get(\"epochs\", 1)\n    self.batch_size = params.get(\"batch_size\", 16)\n    self.batches_in_epoch = params.get(\"batches_in_epoch\", 60000)\n    self.first_epoch_batch_size = params.get(\"first_epoch_batch_size\", self.batch_size)\n    self.batches_in_first_epoch = params.get(\"batches_in_first_epoch\", self.batches_in_epoch)\n\n    # Testing\n    self.test_batch_size = params.get(\"test_batch_size\", 1000)\n\n    # Optimizer\n    self.optimizer_class = eval(params.get(\"optimizer\", \"torch.optim.SGD\"))\n    self.optimizer_params = eval(params.get(\"optimizer_params\", \"{}\"))\n    self.lr_scheduler_class = eval(params.get(\"lr_scheduler\", None))\n    self.lr_scheduler_params = eval(params.get(\"lr_scheduler_params\", \"{}\"))\n    self.loss_function = eval(params.get(\"loss_function\", \"torch.nn.functional.nll_loss\"))\n\n    # CNN parameters\n    c, h, w = map(int, params.get(\"input_shape\", \"1_28_28\").split(\"_\"))\n    self.in_channels = c\n    self.out_channels = map(int, params.get(\"out_channels\", \"30_30\").split(\"_\"))\n    self.kernel_size = map(int, params.get(\"kernel_size\", \"5_5\").split(\"_\"))\n    self.stride = map(int, params.get(\"stride\", \"1_1\").split(\"_\"))\n    self.padding = map(int, params.get(\"padding\", \"0_0\").split(\"_\"))\n\n    # Compute Flatten CNN output len\n    self.maxpool = []\n    self.maxpool.append(\n      ((w + 2 * self.padding[0] - self.kernel_size[0]) // self.stride[0] + 1) // 2)\n    self.maxpool.append(\n      ((self.maxpool[0] + 2 * self.padding[1] - self.kernel_size[1]) // self.stride[1] + 1) // 2)\n\n    self.cnn_output_len = [self.maxpool[i] * self.maxpool[i] * self.out_channels[i]\n                           for i in range(len(self.maxpool))]\n\n    # Linear parameteers\n    self.n = params.get(\"n\", 1000)\n    self.output_size = params.get(\"output_size\", 10)\n\n    # Sparse parameters\n    if \"c1_k\" in params:\n      self.cnn_k = map(int, params[\"c1_k\"].split(\"_\"))\n    else:\n      self.cnn_k = self.cnn_output_len\n\n    self.k = params.get(\"k\", self.n)\n    self.k_inference_factor = params.get(\"k_inference_factor\", 1.0)\n    self.boost_strength = params.get(\"boost_strength\", 1.0)\n    self.boost_strength_factor = params.get(\"boost_strength_factor\", 1.0)\n    self.weight_sparsity = params.get(\"weight_sparsity\", 1.0)\n    self.weight_sparsity_cnn = params.get(\"weight_sparsity_cnn\", 1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createDenseCNNModel(self):\n\n    # Create denseCNN2 model\n    model = nn.Sequential(\n      nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels[0],\n                kernel_size=self.kernel_size[0], stride=self.stride[0],\n                padding=self.padding[0]),\n      nn.MaxPool2d(kernel_size=2),\n      nn.ReLU(),\n\n      nn.Conv2d(in_channels=self.out_channels[0], out_channels=self.out_channels[1],\n                kernel_size=self.kernel_size[1], stride=self.stride[1],\n                padding=self.padding[1]),\n      nn.MaxPool2d(kernel_size=2),\n      nn.ReLU(),\n\n      Flatten(),\n\n      nn.Linear(self.cnn_output_len[1], self.n),\n      nn.ReLU(),\n\n      nn.Linear(self.n, self.output_size),\n      nn.LogSoftmax(dim=1)\n    )\n    model.to(self.device)\n    if torch.cuda.device_count() > 1:\n      model = torch.nn.DataParallel(model)\n\n    return model", "response": "Create a dense CNN model for the current CNN."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createSparseCNNModel(self):\n    # Create sparseCNN2 model\n    model = nn.Sequential(\n      nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels[0],\n                kernel_size=self.kernel_size[0], stride=self.stride[0],\n                padding=self.padding[0]),\n      nn.MaxPool2d(kernel_size=2),\n      KWinners2d(n=self.cnn_output_len[0], k=self.cnn_k[0],\n                 channels=self.out_channels[0],\n                 kInferenceFactor=self.k_inference_factor,\n                 boostStrength=self.boost_strength,\n                 boostStrengthFactor=self.boost_strength_factor),\n\n      nn.Conv2d(in_channels=self.out_channels[0], out_channels=self.out_channels[1],\n                kernel_size=self.kernel_size[1], stride=self.stride[1],\n                padding=self.padding[1]),\n      nn.MaxPool2d(kernel_size=2),\n      KWinners2d(n=self.cnn_output_len[1], k=self.cnn_k[1],\n                 channels=self.out_channels[1],\n                 kInferenceFactor=self.k_inference_factor,\n                 boostStrength=self.boost_strength,\n                 boostStrengthFactor=self.boost_strength_factor),\n\n      Flatten(),\n\n      SparseWeights(\n        nn.Linear(self.cnn_output_len[1], self.n), self.weight_sparsity),\n      KWinners(n=self.n, k=self.k, kInferenceFactor=self.k_inference_factor,\n               boostStrength=self.boost_strength,\n               boostStrengthFactor=self.boost_strength_factor),\n\n      nn.Linear(self.n, self.output_size),\n      nn.LogSoftmax(dim=1)\n    )\n\n    model.to(self.device)\n    if torch.cuda.device_count() > 1:\n      model = torch.nn.DataParallel(model)\n\n    return model", "response": "Create a sparse CNN model for the current CNN."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the learning rate scheduler and attach it to the optimizer", "response": "def createLearningRateScheduler(self, optimizer):\n    \"\"\"\n    Creates the learning rate scheduler and attach the optimizer\n    \"\"\"\n    if self.lr_scheduler_class is None:\n      return None\n\n    return self.lr_scheduler_class(optimizer, **self.lr_scheduler_params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getPyBrainDataSetScalarEncoder(sequence, nTrain, encoderInput, encoderOutput,\n                                   predictionStep=1, useTimeOfDay=True, useDayOfWeek=True):\n  \"\"\"\n  Use scalar encoder for the data\n  :param sequence:\n  :param nTrain:\n  :param predictionStep:\n  :param useTimeOfDay:\n  :param useDayOfWeek:\n  :return:\n  \"\"\"\n  print \"generate a pybrain dataset of sequences\"\n  print \"the training data contains \", str(nTrain-predictionStep), \"records\"\n\n  if encoderInput is None:\n    inDim = 1 + int(useTimeOfDay) + int(useDayOfWeek)\n  else:\n    inDim = encoderInput.n + int(useTimeOfDay) + int(useDayOfWeek)\n\n  if encoderOutput is None:\n    outDim = 1\n  else:\n    outDim = encoderOutput.n\n\n  ds = SequentialDataSet(inDim, outDim)\n  if useTimeOfDay:\n    print \"include time of day as input field\"\n  if useDayOfWeek:\n    print \"include day of week as input field\"\n\n  for i in xrange(nTrain-predictionStep):\n\n    sample = getSingleSample(i, sequence, useTimeOfDay, useDayOfWeek)\n\n    if encoderOutput is None:\n      dataSDROutput = [sequence['normdata'][i+predictionStep]]\n    else:\n      dataSDROutput = encoderOutput.encode(sequence['data'][i+predictionStep])\n\n\n    ds.addSample(sample, dataSDROutput)\n\n  return ds", "response": "Generate a pybrain dataset of nTrain - predictionStep data from encoderInput and encoderOutput."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset(self, params, repetition):\n\n    pprint.pprint(params)\n\n    self.initialize(params, repetition)\n\n    # Load CIFAR dataset\n    dataDir = params.get('dataDir', 'data')\n    self.transform_train = transforms.Compose([\n      transforms.RandomCrop(32, padding=4),\n      transforms.RandomHorizontalFlip(),\n      transforms.ToTensor(),\n      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    self.trainset = datasets.CIFAR10(root=dataDir, train=True, download=True,\n                                transform=self.transform_train)\n\n    self.createModel(params, repetition)\n\n    print(\"Torch reports\", torch.cuda.device_count(), \"GPUs available\")\n    if torch.cuda.device_count() > 1:\n      self.model = torch.nn.DataParallel(self.model)\n\n    self.model.to(self.device)\n\n    self.optimizer = self.createOptimizer(self.model)\n    self.lr_scheduler = self.createLearningRateScheduler(self.optimizer)\n    self.test_loaders = self.createTestLoaders(self.noise_values)", "response": "Called at the beginning of each experiment and each repetition"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the object with the given parameters and default values from the configuration file.", "response": "def initialize(self, params, repetition):\n    \"\"\"\n    Initialize experiment parameters and default values from configuration file.\n    Called by reset() at the beginning of each experiment and each repetition.\n    \"\"\"\n    self.name = params[\"name\"]\n    self.dataDir = params.get(\"datadir\", \"data\")\n    self.seed = params.get(\"seed\", 42) + repetition\n\n    torch.manual_seed(self.seed)\n    np.random.seed(self.seed)\n    random.seed(self.seed)\n\n    # Training\n    self.epochs = params.get(\"epochs\", 1)\n    self.batch_size = params.get(\"batch_size\", 128)\n    self.batches_in_epoch = params.get(\"batches_in_epoch\", 100000)\n    self.first_epoch_batch_size = params.get(\"first_epoch_batch_size\",\n                                             self.batch_size)\n    self.batches_in_first_epoch = params.get(\"batches_in_first_epoch\",\n                                             self.batches_in_epoch)\n\n    # Testing\n    self.test_batch_size = params.get(\"test_batch_size\", 1000)\n    self.test_batches_in_epoch = params.get(\"test_batches_in_epoch\", 100000)\n    self.noise_values = map(float,\n                            params.get(\"noise_values\", \"0.0, 0.1\").split(\", \"))\n    self.best_noise_score = 0.0\n    self.best_epoch = -1\n\n    # Optimizer\n    self.optimizer_class = eval(params.get(\"optimizer\", \"torch.optim.SGD\"))\n    self.lr = params.get(\"learning_rate\", 0.05)\n    self.momentum = params.get(\"momentum\", 0.9)\n    self.weight_decay = params.get(\"weight_decay\", 0.0)\n    self.optimizer_params = eval(params.get(\"optimizer_params\", \"{}\"))\n    self.lr_scheduler_gamma = params.get(\"lr_scheduler_gamma\", 0.9)\n    self.loss_function = eval(params.get(\"loss_function\",\n                                         \"torch.nn.functional.cross_entropy\"))\n\n    # Network parameters\n    self.conv1_sparsity = params.get(\"conv1_sparsity\", 1.0)\n    self.network_type = params.get(\"network_type\", \"sparse\")\n    self.growth_rate = params.get(\"growth_rate\", 12)\n    self.nblocks = map(int,\n                       params.get(\"nblocks\", \"6, 12, 24, 16\").split(\", \"))\n    self.k_inference_factor = params.get(\"k_inference_factor\", 1.5)\n\n    self.dense_sparsities = map(float,\n                                params.get(\"dense_sparsities\",\n                                           \"1.0, 1.0, 1.0, 1.0\").split(\", \"))\n    self.transition_sparsities = map(float,\n                                     params.get(\"transition_sparsities\",\n                                                \"0.1, 0.1, 0.2\").split(\", \"))\n    self.linear_sparsity = params.get(\"linear_sparsity\", 0.0)\n    self.linear_weight_sparsity = params.get(\"linear_weight_sparsity\", 0.3)\n    self.linear_n = params.get(\"linear_n\", 500)\n    self.avg_pool_size = params.get(\"avg_pool_size\", 2)\n    self.dense_c1_out_planes = params.get(\"dense_c1_out_planes\", 4*self.growth_rate)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef createLearningRateScheduler(self, optimizer):\n    return torch.optim.lr_scheduler.StepLR(optimizer,\n                                           step_size=1,\n                                           gamma=self.lr_scheduler_gamma)", "response": "Creates the learning rate scheduler and attach it to the optimizer\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an optimizer for the given model", "response": "def createOptimizer(self, model):\n    \"\"\"\n    Create a new instance of the optimizer\n    \"\"\"\n    return torch.optim.SGD(model.parameters(),\n                           lr=self.lr,\n                           momentum=self.momentum,\n                           weight_decay=self.weight_decay)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlearning each provided object.", "response": "def learnObjects(self):\n    \"\"\"\n    Learn each provided object.\n\n    This method simultaneously learns 4 sets of synapses:\n    - location -> input\n    - input -> location\n    - input -> object\n    - object -> input\n    \"\"\"\n    for objectName, objectFeatures in self.objects.iteritems():\n      self.reset()\n\n      for module in self.locationModules:\n        module.activateRandomLocation()\n\n      for feature in objectFeatures:\n        locationOnObject = (feature[\"top\"] + feature[\"height\"]/2,\n                            feature[\"left\"] + feature[\"width\"]/2)\n        self.move(objectName, locationOnObject)\n\n        featureName = feature[\"name\"]\n        featureSDR = self.features[featureName]\n        for _ in xrange(10):\n          self.sense(featureSDR, learn=True)\n\n        self.locationRepresentations[(objectName, locationOnObject)] = (\n          self.getActiveLocationCells())\n        self.inputRepresentations[(objectName, locationOnObject, featureName)] = (\n          self.inputLayer.getActiveCells())\n\n      self.objectRepresentations[objectName] = self.objectLayer.getActiveCells()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer each object without any location input.", "response": "def inferObjectsWithRandomMovements(self):\n    \"\"\"\n    Infer each object without any location input.\n    \"\"\"\n    for objectName, objectFeatures in self.objects.iteritems():\n      self.reset()\n\n      inferred = False\n      prevTouchSequence = None\n\n      for _ in xrange(4):\n\n        while True:\n          touchSequence = list(objectFeatures)\n          random.shuffle(touchSequence)\n\n          if prevTouchSequence is not None:\n            if touchSequence[0] == prevTouchSequence[-1]:\n              continue\n\n          break\n\n        for i, feature in enumerate(touchSequence):\n          locationOnObject = (feature[\"top\"] + feature[\"height\"]/2,\n                              feature[\"left\"] + feature[\"width\"]/2)\n          self.move(objectName, locationOnObject)\n\n          featureName = feature[\"name\"]\n          featureSDR = self.features[featureName]\n          self.sense(featureSDR, learn=False)\n\n          inferred = (\n            set(self.objectLayer.getActiveCells()) ==\n            set(self.objectRepresentations[objectName]) and\n\n            set(self.inputLayer.getActiveCells()) ==\n            set(self.inputRepresentations[(objectName,\n                                           locationOnObject,\n                                           featureName)]) and\n\n            set(self.getActiveLocationCells()) ==\n            set(self.locationRepresentations[(objectName, locationOnObject)]))\n\n          if inferred:\n            break\n\n        prevTouchSequence = touchSequence\n\n        if inferred:\n          break"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming all the files in the source dataset for the given command and save athe", "response": "def transform_folder(args):\n  \"\"\"\n  Transform all the files in the source dataset for the given command and save\n  the results as a single pickle file in the destination dataset\n  :param args: tuple with the following arguments:\n               - the command name: 'zero', 'one', 'two', ...\n               - transforms to apply to wav file\n               - full path of the source dataset\n               - full path of the destination dataset\n  \"\"\"\n  command, (transform, src, dest) = args\n  try:\n    print(progress.value, \"remaining\")\n\n    # Apply transformations to all files\n    data = []\n    data_dir = os.path.join(src, command)\n    for filename in os.listdir(data_dir):\n      path = os.path.join(data_dir, filename)\n      data.append(transform({'path': path}))\n\n    # Save results\n    pickleFile = os.path.join(dest, \"{}.pkl\".format(command))\n    gc.disable()\n    with open(pickleFile, \"wb\") as f:\n      pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n    gc.enable()\n\n    # Update progress\n    with progress.get_lock():\n      progress.value -= 1\n\n  except Exception as e:\n    print(command, e, file=sys.stderr)\n    traceback.print_exc()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the percentage of cells that are now considered dead. The first time you call this method a permutation list is set up. Calls change the number of cells considered dead.", "response": "def killCells(self, percent=0.05):\n    \"\"\"\n    Changes the percentage of cells that are now considered dead. The first\n    time you call this method a permutation list is set up. Calls change the\n    number of cells considered dead.\n    \"\"\"\n    numColumns = numpy.prod(self.getColumnDimensions())\n\n    if self.zombiePermutation is None:\n      self.zombiePermutation = numpy.random.permutation(numColumns)\n\n    self.numDead = int(round(percent * numColumns))\n\n    if self.numDead > 0:\n      self.deadCols = self.zombiePermutation[0:self.numDead]\n    else:\n      self.deadCols = numpy.array([])\n\n    self.deadColumnInputSpan = self.getConnectedSpan(self.deadCols)\n    self.removeDeadColumns()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef killCellRegion(self, centerColumn, radius):\n    self.deadCols = topology.wrappingNeighborhood(centerColumn,\n                                          radius,\n                                          self._columnDimensions)\n    self.deadColumnInputSpan = self.getConnectedSpan(self.deadCols)\n    self.removeDeadColumns()", "response": "Kill cells around a centerColumn within radius"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute(self, inputVector, learn, activeArray):\n    if not isinstance(inputVector, numpy.ndarray):\n      raise TypeError(\"Input vector must be a numpy array, not %s\" %\n                      str(type(inputVector)))\n\n    if inputVector.size != self._numInputs:\n      raise ValueError(\n          \"Input vector dimensions don't match. Expecting %s but got %s\" % (\n              inputVector.size, self._numInputs))\n\n    self._updateBookeepingVars(learn)\n    inputVector = numpy.array(inputVector, dtype=realDType)\n    inputVector.reshape(-1)\n    self._overlaps = self._calculateOverlap(inputVector)\n    # self._overlaps[self.deadCols] = 0\n\n    # Apply boosting when learning is on\n    if learn:\n      self._boostedOverlaps = self._boostFactors * self._overlaps\n    else:\n      self._boostedOverlaps = self._overlaps\n\n    # Apply inhibition to determine the winning columns\n    activeColumns = self._inhibitColumns(self._boostedOverlaps)\n\n    if learn:\n      self._adaptSynapses(inputVector, activeColumns)\n      self._updateDutyCycles(self._overlaps, activeColumns)\n      self._bumpUpWeakColumns()\n      self._updateTargetActivityDensity()\n      self._updateBoostFactors()\n      if self._isUpdateRound():\n        self._updateInhibitionRadius()\n        self._updateMinDutyCycles()\n\n      # self.growRandomSynapses()\n\n    activeArray.fill(0)\n    activeArray[activeColumns] = 1", "response": "This method computes the current state of the base class and returns the indices of the active columns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing the sequence by subtracting the mean and", "response": "def normalizeSequence(sequence):\n  \"\"\"\n  normalize sequence by subtracting the mean and\n  :param sequence: a list of data samples\n  :param considerDimensions: a list of dimensions to consider\n  :return: normalized sequence\n  \"\"\"\n  seq = np.array(sequence).astype('float64')\n\n  meanSeq = np.mean(seq)\n  stdSeq = np.std(seq)\n  seq = (seq - np.mean(seq)) / np.std(seq)\n\n  sequence = seq.tolist()\n  return sequence, meanSeq, stdSeq"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns constructor arguments associated with ColumnPooler.", "response": "def getConstructorArguments():\n  \"\"\"\n  Return constructor argument associated with ColumnPooler.\n  @return defaults (list)   a list of args and default values for each argument\n  \"\"\"\n  argspec = inspect.getargspec(ColumnPooler.__init__)\n  return argspec.args[1:], argspec.defaults"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getSpec(cls):\n    spec = dict(\n      description=ColumnPoolerRegion.__doc__,\n      singleNodeOnly=True,\n      inputs=dict(\n        feedforwardInput=dict(\n          description=\"The primary feed-forward input to the layer, this is a\"\n                      \" binary array containing 0's and 1's\",\n          dataType=\"Real32\",\n          count=0,\n          required=True,\n          regionLevel=True,\n          isDefaultInput=True,\n          requireSplitterMap=False),\n\n        feedforwardGrowthCandidates=dict(\n          description=(\"An array of 0's and 1's representing feedforward input \" +\n                       \"that can be learned on new proximal synapses. If this \" +\n                       \"input isn't provided, the whole feedforwardInput is \"\n                       \"used.\"),\n          dataType=\"Real32\",\n          count=0,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        predictedInput=dict(\n          description=(\"An array of 0s and 1s representing input cells that \" +\n                       \"are predicted to become active in the next time step. \" +\n                       \"If this input is not provided, some features related \" +\n                       \"to online learning may not function properly.\"),\n          dataType=\"Real32\",\n          count=0,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        lateralInput=dict(\n          description=\"Lateral binary input into this column, presumably from\"\n                      \" other neighboring columns.\",\n          dataType=\"Real32\",\n          count=0,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        resetIn=dict(\n          description=\"A boolean flag that indicates whether\"\n                      \" or not the input vector received in this compute cycle\"\n                      \" represents the first presentation in a\"\n                      \" new temporal sequence.\",\n          dataType='Real32',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n      ),\n      outputs=dict(\n        feedForwardOutput=dict(\n          description=\"The default output of ColumnPoolerRegion. By default this\"\n                      \" outputs the active cells. You can change this \"\n                      \" dynamically using the defaultOutputType parameter.\",\n          dataType=\"Real32\",\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=True),\n\n        activeCells=dict(\n          description=\"A binary output containing a 1 for every\"\n                      \" cell that is currently active.\",\n          dataType=\"Real32\",\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n      ),\n      parameters=dict(\n        learningMode=dict(\n          description=\"Whether the node is learning (default True).\",\n          accessMode=\"ReadWrite\",\n          dataType=\"Bool\",\n          count=1,\n          defaultValue=\"true\"),\n        onlineLearning=dict(\n          description=\"Whether to use onlineLearning or not (default False).\",\n          accessMode=\"ReadWrite\",\n          dataType=\"Bool\",\n          count=1,\n          defaultValue=\"false\"),\n        learningTolerance=dict(\n          description=\"How much variation in SDR size to accept when learning. \"\n                      \"Only has an effect if online learning is enabled. \"\n                      \"Should be at most 1 - inertiaFactor.\",\n          accessMode=\"ReadWrite\",\n          dataType=\"Real32\",\n          count=1,\n          defaultValue=\"false\"),\n        cellCount=dict(\n          description=\"Number of cells in this layer\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\"),\n        inputWidth=dict(\n          description='Number of inputs to the layer.',\n          accessMode='Read',\n          dataType='UInt32',\n          count=1,\n          constraints=''),\n        numOtherCorticalColumns=dict(\n          description=\"The number of lateral inputs that this L2 will receive. \"\n                      \"This region assumes that every lateral input is of size \"\n                      \"'cellCount'.\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\"),\n        sdrSize=dict(\n          description=\"The number of active cells invoked per object\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\"),\n        maxSdrSize=dict(\n          description=\"The largest number of active cells in an SDR tolerated \"\n                      \"during learning. Stops learning when unions are active.\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\"),\n        minSdrSize=dict(\n          description=\"The smallest number of active cells in an SDR tolerated \"\n                      \"during learning.  Stops learning when possibly on a \"\n                      \"different object or sequence\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\"),\n\n        #\n        # Proximal\n        #\n        synPermProximalInc=dict(\n          description=\"Amount by which permanences of proximal synapses are \"\n                      \"incremented during learning.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1),\n        synPermProximalDec=dict(\n          description=\"Amount by which permanences of proximal synapses are \"\n                      \"decremented during learning.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1),\n        initialProximalPermanence=dict(\n          description=\"Initial permanence of a new proximal synapse.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\"),\n        sampleSizeProximal=dict(\n          description=\"The desired number of active synapses for an active cell\",\n          accessMode=\"Read\",\n          dataType=\"Int32\",\n          count=1),\n        minThresholdProximal=dict(\n          description=\"If the number of synapses active on a proximal segment \"\n                      \"is at least this threshold, it is considered as a \"\n                      \"candidate active cell\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\"),\n        connectedPermanenceProximal=dict(\n          description=\"If the permanence value for a synapse is greater \"\n                      \"than this value, it is said to be connected.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\"),\n        predictedInhibitionThreshold=dict(\n          description=\"How many predicted cells are required to cause \"\n                      \"inhibition in the pooler.  Only has an effect if online \"\n                      \"learning is enabled.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\"),\n\n        #\n        # Distal\n        #\n        synPermDistalInc=dict(\n          description=\"Amount by which permanences of synapses are \"\n                      \"incremented during learning.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1),\n        synPermDistalDec=dict(\n          description=\"Amount by which permanences of synapses are \"\n                      \"decremented during learning.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1),\n        initialDistalPermanence=dict(\n          description=\"Initial permanence of a new synapse.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\"),\n        sampleSizeDistal=dict(\n          description=\"The desired number of active synapses for an active \"\n                      \"segment.\",\n          accessMode=\"Read\",\n          dataType=\"Int32\",\n          count=1),\n        activationThresholdDistal=dict(\n          description=\"If the number of synapses active on a distal segment is \"\n                      \"at least this threshold, the segment is considered \"\n                      \"active\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\"),\n        connectedPermanenceDistal=dict(\n          description=\"If the permanence value for a synapse is greater \"\n                      \"than this value, it is said to be connected.\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\"),\n        inertiaFactor=dict(\n          description=\"Controls the proportion of previously active cells that \"\n                      \"remain active through inertia in the next timestep (in  \"\n                      \"the absence of inhibition).\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\"),\n\n\n\n        seed=dict(\n          description=\"Seed for the random number generator.\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1),\n        defaultOutputType=dict(\n          description=\"Controls what type of cell output is placed into\"\n                      \" the default output 'feedForwardOutput'\",\n          accessMode=\"ReadWrite\",\n          dataType=\"Byte\",\n          count=0,\n          constraints=\"enum: active,predicted,predictedActiveCells\",\n          defaultValue=\"active\"),\n      ),\n      commands=dict(\n        reset=dict(description=\"Explicitly reset TM states now.\"),\n      )\n    )\n\n    return spec", "response": "Returns the Spec for the ColumnPoolerRegion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the internal objects.", "response": "def initialize(self):\n    \"\"\"\n    Initialize the internal objects.\n    \"\"\"\n    if self._pooler is None:\n      params = {\n        \"inputWidth\": self.inputWidth,\n        \"lateralInputWidths\": [self.cellCount] * self.numOtherCorticalColumns,\n        \"cellCount\": self.cellCount,\n        \"sdrSize\": self.sdrSize,\n        \"onlineLearning\": self.onlineLearning,\n        \"maxSdrSize\": self.maxSdrSize,\n        \"minSdrSize\": self.minSdrSize,\n        \"synPermProximalInc\": self.synPermProximalInc,\n        \"synPermProximalDec\": self.synPermProximalDec,\n        \"initialProximalPermanence\": self.initialProximalPermanence,\n        \"minThresholdProximal\": self.minThresholdProximal,\n        \"sampleSizeProximal\": self.sampleSizeProximal,\n        \"connectedPermanenceProximal\": self.connectedPermanenceProximal,\n        \"predictedInhibitionThreshold\": self.predictedInhibitionThreshold,\n        \"synPermDistalInc\": self.synPermDistalInc,\n        \"synPermDistalDec\": self.synPermDistalDec,\n        \"initialDistalPermanence\": self.initialDistalPermanence,\n        \"activationThresholdDistal\": self.activationThresholdDistal,\n        \"sampleSizeDistal\": self.sampleSizeDistal,\n        \"connectedPermanenceDistal\": self.connectedPermanenceDistal,\n        \"inertiaFactor\": self.inertiaFactor,\n        \"seed\": self.seed,\n      }\n      self._pooler = ColumnPooler(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute(self, inputs, outputs):\n    # Handle reset first (should be sent with an empty signal)\n    if \"resetIn\" in inputs:\n      assert len(inputs[\"resetIn\"]) == 1\n      if inputs[\"resetIn\"][0] != 0:\n        # send empty output\n        self.reset()\n        outputs[\"feedForwardOutput\"][:] = 0\n        outputs[\"activeCells\"][:] = 0\n        return\n\n    feedforwardInput = numpy.asarray(inputs[\"feedforwardInput\"].nonzero()[0],\n                                     dtype=\"uint32\")\n\n    if \"feedforwardGrowthCandidates\" in inputs:\n      feedforwardGrowthCandidates = numpy.asarray(\n        inputs[\"feedforwardGrowthCandidates\"].nonzero()[0], dtype=\"uint32\")\n    else:\n      feedforwardGrowthCandidates = feedforwardInput\n\n    if \"lateralInput\" in inputs:\n      lateralInputs = tuple(numpy.asarray(singleInput.nonzero()[0],\n                                          dtype=\"uint32\")\n                            for singleInput\n                            in numpy.split(inputs[\"lateralInput\"],\n                                           self.numOtherCorticalColumns))\n    else:\n      lateralInputs = ()\n\n    if \"predictedInput\" in inputs:\n      predictedInput = numpy.asarray(\n        inputs[\"predictedInput\"].nonzero()[0], dtype=\"uint32\")\n    else:\n      predictedInput = None\n\n    # Send the inputs into the Column Pooler.\n    self._pooler.compute(feedforwardInput, lateralInputs,\n                         feedforwardGrowthCandidates, learn=self.learningMode,\n                         predictedInput = predictedInput)\n\n    # Extract the active / predicted cells and put them into binary arrays.\n    outputs[\"activeCells\"][:] = 0\n    outputs[\"activeCells\"][self._pooler.getActiveCells()] = 1\n\n    # Send appropriate output to feedForwardOutput.\n    if self.defaultOutputType == \"active\":\n      outputs[\"feedForwardOutput\"][:] = outputs[\"activeCells\"]\n    else:\n      raise Exception(\"Unknown outputType: \" + self.defaultOutputType)", "response": "Compute the next iteration of the compute."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_param_to_dirname(param):\n    if type(param) == types.StringType:\n        return param\n    else:\n        return re.sub(\"0+$\", '0', '%f'%param)", "response": "Helper function to convert a parameter value to a valid directory name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_opt(self):\n        optparser = optparse.OptionParser()\n        optparser.add_option('-c', '--config',\n            action='store', dest='config', type='string', default='experiments.cfg', \n            help=\"your experiments config file\")\n        optparser.add_option('-n', '--numcores',\n            action='store', dest='ncores', type='int', default=cpu_count(), \n            help=\"number of processes you want to use, default is %i\"%cpu_count())  \n        optparser.add_option('-d', '--del',\n            action='store_true', dest='delete', default=False, \n            help=\"delete experiment folder if it exists\")\n        optparser.add_option('-e', '--experiment',\n            action='append', dest='experiments', type='string',\n            help=\"run only selected experiments, by default run all experiments in config file.\")\n        optparser.add_option('-b', '--browse',\n            action='store_true', dest='browse', default=False, \n            help=\"browse existing experiments.\")      \n        optparser.add_option('-B', '--Browse',\n            action='store_true', dest='browse_big', default=False, \n            help=\"browse existing experiments, more verbose than -b\")      \n        optparser.add_option('-p', '--progress',\n            action='store_true', dest='progress', default=False, \n            help=\"like browse, but only shows name and progress bar\")\n\n        options, args = optparser.parse_args()\n        self.options = options\n        return options, args", "response": "Parses the command line options for different settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_cfg(self):\n        self.cfgparser = ConfigParser()\n        if not self.cfgparser.read(self.options.config):\n            raise SystemExit('config file %s not found.'%self.options.config) \n\n        # Change the current working directory to be relative to 'experiments.cfg'\n        projectDir = os.path.dirname(self.options.config)\n        projectDir = os.path.abspath(projectDir)\n        os.chdir(projectDir)", "response": "Parses the given config file for experiments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a directory if it does not exist.", "response": "def mkdir(self, path):\n        \"\"\" create a directory if it does not exist. \"\"\"\n        if not os.path.exists(path):\n            os.makedirs(path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_exps(self, path='.'):\n        exps = []\n        for dp, dn, fn in os.walk(path):\n            if 'experiment.cfg' in fn:\n                subdirs = [os.path.join(dp, d) for d in os.listdir(dp) if os.path.isdir(os.path.join(dp, d))]\n                if all(map(lambda s: self.get_exps(s) == [], subdirs)):       \n                    exps.append(dp)\n        return exps", "response": "get all experiments in the given path and return the experiment IDs that are not already in the experiment cache"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef items_to_params(self, items):\n        params = {}\n        for t,v in items:       \n            try:\n                # try to evaluate parameter (float, int, list)\n                if v in ['grid', 'list']:\n                    params[t] = v\n                else:\n                    params[t] = eval(v)\n                if isinstance(params[t], ndarray):\n                    params[t] = params[t].tolist()\n            except (NameError, SyntaxError):\n                # otherwise assume string\n                params[t] = v\n        return params", "response": "evaluate the found items to become floats ints or lists. \n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the parameters of the experiment given. cfgname = path given.", "response": "def get_params(self, exp, cfgname='experiment.cfg'):\n        \"\"\" reads the parameters of the experiment (= path) given.\n        \"\"\"\n        cfgp = ConfigParser()\n        cfgp.read(os.path.join(exp, cfgname))\n        section = cfgp.sections()[0]\n        params = self.items_to_params(cfgp.items(section))\n        params['name'] = section\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_exp(self, name, path='.'):\n        exps = []\n        for dp, dn, df in os.walk(path):\n            if 'experiment.cfg' in df:\n                cfgp = ConfigParser()\n                cfgp.read(os.path.join(dp, 'experiment.cfg'))\n                if name in cfgp.sections():\n                    exps.append(dp)\n        return exps", "response": "function\n            returns the correct path of the experiment. cfg file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_config_file(self, params, path):\n        cfgp = ConfigParser()\n        cfgp.add_section(params['name'])\n        for p in params:\n            if p == 'name':\n                continue\n            cfgp.set(params['name'], p, params[p])\n        f = open(os.path.join(path, 'experiment.cfg'), 'w')\n        cfgp.write(f)\n        f.close()", "response": "write a config file for this single exp in the folder path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the whole history for one experiment and one repetition. tags can be a string or a list of strings or a list of strings or a list of lists of strings.", "response": "def get_history(self, exp, rep, tags):\n        \"\"\" returns the whole history for one experiment and one repetition.\n            tags can be a string or a list of strings. if tags is a string,\n            the history is returned as list of values, if tags is a list of \n            strings or 'all', history is returned as a dictionary of lists\n            of values.\n        \"\"\"\n        params = self.get_params(exp)\n           \n        if params == None:\n            raise SystemExit('experiment %s not found.'%exp)         \n        \n        # make list of tags, even if it is only one\n        if tags != 'all' and not hasattr(tags, '__iter__'):\n            tags = [tags] \n        \n        results = {}\n        logfile = os.path.join(exp, '%i.log'%rep)\n        try:\n            f = open(logfile)\n        except IOError:\n            if len(tags) == 1:\n                return []\n            else:\n                return {}\n\n        for line in f:\n            dic = json.loads(line)\n            for tag in tags:\n              if not tag in results:\n                results[tag] = []\n\n              if tag in dic:\n                results[tag].append(dic[tag])\n              else:\n                results[tag].append(None)\n                            \n        f.close()\n        if len(results) == 0:\n            if len(tags) == 1:\n                return []\n            else:\n                return {}\n            # raise ValueError('tag(s) not found: %s'%str(tags))\n        if len(tags) == 1:\n            return results[results.keys()[0]]\n        else:\n            return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_history_tags(self, exp, rep=0):\n        history = self.get_history(exp, rep, 'all')\n        return history.keys()", "response": "Returns a list of all available tags for the given experiment and repetition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_value(self, exp, rep, tags, which='last'):\n        history = self.get_history(exp, rep, tags)\n        \n        # empty histories always return None\n        if len(history) == 0:\n            return None\n            \n        # distinguish dictionary (several tags) from list\n        if type(history) == dict:\n            for h in history:\n                if which == 'last':\n                    history[h] = history[h][-1]\n                if which == 'min':\n                    history[h] = min(history[h])\n                if which == 'max':\n                    history[h] = max(history[h])\n                if type(which) == int:\n                    history[h] = history[h][which]\n            return history\n            \n        else:\n            if which == 'last':\n                return history[-1]\n            if which == 'min':\n                return min(history)\n            if which == 'max':\n                return max(history)\n            if type(which) == int:\n                return history[which]\n            else: \n                return None", "response": "Like get_history but returns only one value rather\n            than the whole list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_values_fix_params(self, exp, rep, tag, which='last', **kwargs):\n        subexps = self.get_exps(exp)\n        tagvalues = ['%s%s'%(k, convert_param_to_dirname(kwargs[k])) for k in kwargs]\n        \n        values = [self.get_value(se, rep, tag, which) for se in subexps if all(map(lambda tv: tv in se, tagvalues))]\n        params = [self.get_params(se) for se in subexps if all(map(lambda tv: tv in se, tagvalues))]\n        \n        return values, params", "response": "this function uses get_value but returns all values where the\n            subperiments match the kwargs arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbrowsing all the experiments and return information about the most important ones.", "response": "def browse(self):\n        \"\"\" go through all subfolders (starting at '.') and return information\n            about the existing experiments. if the -B option is given, all \n            parameters are shown, -b only displays the most important ones.\n            this function does *not* execute any experiments.\n        \"\"\"\n        for d in self.get_exps('.'):\n            params = self.get_params(d)\n            name = params['name']\n            basename = name.split('/')[0]\n            # if -e option is used, only show requested experiments\n            if self.options.experiments and basename not in self.options.experiments:\n                continue\n                \n            fullpath = os.path.join(params['path'], name)\n            \n            # calculate progress\n            prog = 0\n            for i in range(params['repetitions']):\n                prog += progress(params, i)\n            prog /= params['repetitions']\n            \n            # if progress flag is set, only show the progress bars\n            if self.options.progress:\n                bar = \"[\"\n                bar += \"=\"*int(prog/4)\n                bar += \" \"*int(25-prog/4)\n                bar += \"]\"\n                print '%3i%% %27s %s'%(prog,bar,d)\n                continue\n            \n            print '%16s %s'%('experiment', d)\n                           \n            try:\n                minfile = min(\n                    (os.path.join(dirname, filename)\n                    for dirname, dirnames, filenames in os.walk(fullpath)\n                    for filename in filenames\n                    if filename.endswith(('.log', '.cfg'))),\n                    key=lambda fn: os.stat(fn).st_mtime)\n            \n                maxfile = max(\n                    (os.path.join(dirname, filename)\n                    for dirname, dirnames, filenames in os.walk(fullpath)\n                    for filename in filenames\n                    if filename.endswith(('.log', '.cfg'))),\n                    key=lambda fn: os.stat(fn).st_mtime)\n            except ValueError:\n                print '         started %s'%'not yet'\n                \n            else:      \n                print '         started %s'%time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.stat(minfile).st_mtime))\n                print '           ended %s'%time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.stat(maxfile).st_mtime))\n            \n            for k in ['repetitions', 'iterations']:\n                print '%16s %s'%(k, params[k])   \n            \n            print '%16s %i%%'%('progress', prog)\n            \n            if self.options.browse_big:\n                # more verbose output\n                for p in [p for p in params if p not in ('repetitions', 'iterations', 'path', 'name')]:\n                    print '%16s %s'%(p, params[p])\n                    \n            print"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expand_param_list(self, paramlist):\n        # for one single experiment, still wrap it in list\n        if type(paramlist) == types.DictType:\n            paramlist = [paramlist]\n        \n        # get all options that are iteratable and build all combinations (grid) or tuples (list)\n        iparamlist = []\n        for params in paramlist:\n            if ('experiment' in params and params['experiment'] == 'single'):\n                iparamlist.append(params)\n            else:\n                iterparams = [p for p in params if hasattr(params[p], '__iter__')]\n                if len(iterparams) > 0:\n                    # write intermediate config file\n                    self.mkdir(os.path.join(params['path'], params['name']))\n                    self.write_config_file(params, os.path.join(params['path'], params['name']))\n\n                    # create sub experiments (check if grid or list is requested)\n                    if 'experiment' in params and params['experiment'] == 'list':\n                        iterfunc = itertools.izip\n                    elif ('experiment' not in params) or ('experiment' in params and params['experiment'] == 'grid'):\n                        iterfunc = itertools.product\n                    else:\n                        raise SystemExit(\"unexpected value '%s' for parameter 'experiment'. Use 'grid', 'list' or 'single'.\"%params['experiment'])\n\n                    for il in iterfunc(*[params[p] for p in iterparams]):\n                        par = params.copy()\n                        converted = str(zip(iterparams, map(convert_param_to_dirname, il)))\n                        par['name'] = par['name'] + '/' + re.sub(\"[' \\[\\],()]\", '', converted)\n                        for i, ip in enumerate(iterparams):\n                            par[ip] = il[i]\n                        iparamlist.append(par)\n                else:\n                    iparamlist.append(params)\n\n        return iparamlist", "response": "expands the parameters list according to one of these schemes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a subdirectory of the current experiment and writes the current config file in the folder.", "response": "def create_dir(self, params, delete=False):\n        \"\"\" creates a subdirectory for the experiment, and deletes existing\n            files, if the delete flag is true. then writes the current\n            experiment.cfg file in the folder.\n        \"\"\"\n        # create experiment path and subdir\n        fullpath = os.path.join(params['path'], params['name'])\n        self.mkdir(fullpath)\n\n        # delete old histories if --del flag is active\n        if delete and os.path.exists(fullpath):\n            os.system('rm %s/*' % fullpath)\n     \n        # write a config file for this single exp. in the folder\n        self.write_config_file(params, fullpath)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the experiments as given in the config file.", "response": "def start(self):\n        \"\"\" starts the experiments as given in the config file. \"\"\"     \n        self.parse_opt()\n        self.parse_cfg()\n\n        # if -b, -B or -p option is set, only show information, don't\n        # start the experiments\n        if self.options.browse or self.options.browse_big or self.options.progress:\n            self.browse()\n            raise SystemExit\n        \n        # read main configuration file\n        paramlist = []\n        for exp in self.cfgparser.sections():\n            if not self.options.experiments or exp in self.options.experiments:\n                params = self.items_to_params(self.cfgparser.items(exp))\n                params['name'] = exp\n                paramlist.append(params)\n                \n        self.do_experiment(paramlist)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning one experiment programatically and returns.", "response": "def do_experiment(self, params):\n        \"\"\" runs one experiment programatically and returns.\n            params: either parameter dictionary (for one single experiment) or a list of parameter\n            dictionaries (for several experiments).\n        \"\"\"\n        paramlist = self.expand_param_list(params)\n        \n        # create directories, write config files\n        for pl in paramlist:\n            # check for required param keys\n            if ('name' in pl) and ('iterations' in pl) and ('repetitions' in pl) and ('path' in pl):\n               self.create_dir(pl, self.options.delete)\n            else:\n                print 'Error: parameter set does not contain all required keys: name, iterations, repetitions, path'\n                return False\n            \n        # create experiment list \n        explist = []\n            \n        # expand paramlist for all repetitions and add self and rep number\n        for p in paramlist:\n            explist.extend(zip( [self]*p['repetitions'], [p]*p['repetitions'], xrange(p['repetitions']) ))\n                \n        # if only 1 process is required call each experiment seperately (no worker pool)\n        if self.options.ncores == 1:\n            for e in explist:\n                mp_runrep(e)\n        else:\n            # create worker processes    \n            pool = Pool(processes=self.options.ncores)\n            pool.map(mp_runrep, explist)\n        \n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_rep(self, params, rep):\n        try:\n            name = params['name']\n            fullpath = os.path.join(params['path'], params['name'])\n            logname = os.path.join(fullpath, '%i.log'%rep)\n            # check if repetition exists and has been completed\n            restore = 0\n            if os.path.exists(logname):\n                logfile = open(logname, 'r')\n                lines = logfile.readlines()\n                logfile.close()\n\n                # if completed, continue loop\n                if 'iterations' in params and len(lines) == params['iterations']:\n                    return False\n                # if not completed, check if restore_state is supported\n                if not self.restore_supported:\n                    # not supported, delete repetition and start over\n                    # print 'restore not supported, deleting %s' % logname\n                    os.remove(logname)\n                    restore = 0\n                else:\n                    restore = len(lines)\n\n            self.reset(params, rep)\n\n            if restore:\n                logfile = open(logname, 'a')\n                self.restore_state(params, rep, restore)\n            else:\n                logfile = open(logname, 'w')\n\n            # loop through iterations and call iterate\n            for it in xrange(restore, params['iterations']):\n                dic = self.iterate(params, rep, it) or {}\n                dic['iteration'] = it\n\n                if self.restore_supported:\n                    self.save_state(params, rep, it)\n\n                if dic is not None:\n                  json.dump(dic, logfile)\n                  logfile.write('\\n')\n                  logfile.flush()\n\n            logfile.close()\n\n            self.finalize(params, rep)\n\n        except:\n            import traceback\n            traceback.print_exc()\n            raise", "response": "run a single repetition including directory creation log files etc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cluster(self, linkageMethod=\"single\"):\n    if self._overlaps is None:\n      self._populateOverlaps()\n    overlaps = self._overlaps\n\n    linkage = scipy.cluster.hierarchy.linkage(-overlaps, method=linkageMethod)\n    self._linkage = linkage", "response": "Perform hierarchical clustering on training vectors using specified linkage\n    method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getClusterPrototypes(self, numClusters, numPrototypes=1):\n    linkage = self.getLinkageMatrix()\n    linkage[:, 2] -= linkage[:, 2].min()\n\n    clusters = scipy.cluster.hierarchy.fcluster(\n      linkage, numClusters, criterion=\"maxclust\")\n    prototypes = []\n    clusterSizes = []\n\n    for cluster_id in numpy.unique(clusters):\n      ids = numpy.arange(len(clusters))[clusters == cluster_id]\n      clusterSizes.append(len(ids))\n      if len(ids) > numPrototypes:\n        cluster_prototypes = HierarchicalClustering._getPrototypes(\n          ids, self._overlaps, numPrototypes)\n      else:\n        cluster_prototypes = numpy.ones(numPrototypes) * -1\n        cluster_prototypes[:len(ids)] = ids\n      prototypes.append(cluster_prototypes)\n\n    return numpy.vstack(prototypes).astype(int), numpy.array(clusterSizes)", "response": "Create numClusters flat clusters and find approximately numPrototypes\n    prototypes per flat cluster. Returns an array with each row containing the indices of the prototypes for each flat cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a compressed overlap array and a set of indices specifying a subset of those in that array return the set of topNumber indices of vectors that have maximum average overlap with other vectors in that array.", "response": "def _getPrototypes(indices, overlaps, topNumber=1):\n    \"\"\"\n    Given a compressed overlap array and a set of indices specifying a subset\n    of those in that array, return the set of topNumber indices of vectors that\n    have maximum average overlap with other vectors in `indices`.\n\n    @param indices (arraylike) Array of indices for which to get prototypes.\n\n    @param overlaps (numpy.ndarray) Condensed array of overlaps of the form\n        returned by _computeOverlaps().\n\n    @param topNumber (int) The number of prototypes to return. Optional,\n        defaults to 1.\n\n    @returns (numpy.ndarray) Array of indices of prototypes\n    \"\"\"\n    # find the number of data points based on the length of the overlap array\n    # solves for n: len(overlaps) = n(n-1)/2\n    n = numpy.roots([1, -1, -2 * len(overlaps)]).max()\n    k = len(indices)\n\n    indices = numpy.array(indices, dtype=int)\n    rowIdxs = numpy.ndarray((k, k-1), dtype=int)\n    colIdxs = numpy.ndarray((k, k-1), dtype=int)\n\n    for i in xrange(k):\n      rowIdxs[i, :] = indices[i]\n      colIdxs[i, :i] = indices[:i]\n      colIdxs[i, i:] = indices[i+1:]\n\n    idx = HierarchicalClustering._condensedIndex(rowIdxs, colIdxs, n)\n    subsampledOverlaps = overlaps[idx]\n\n    meanSubsampledOverlaps = subsampledOverlaps.mean(1)\n    biggestOverlapSubsetIdxs = numpy.argsort(\n      -meanSubsampledOverlaps)[:topNumber]\n\n    return indices[biggestOverlapSubsetIdxs]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a set of n points for which pairwise overlaps are stored in a flat array X in the format returned by _computeOverlaps (upper triangular of the overlap matrix in row-major order), this function returns the indices in X that correspond to the overlaps for the pairs of points specified. Example ------- Consider the case with n = 5 data points for which pairwise overlaps are stored in array X, which has length 10 = n(n-1)/2. To obtain the overlap of points 2 and 3 and the overlap of points 4 and 1, call idx = _condensedIndex([2, 4], [3, 1], 5) # idx == [6, 1] Note: Since X does not contain the diagonal (self-comparisons), it is invalid to pass arrays such that indicesA[i] == indicesB[i] for any i. @param indicesA (arraylike) First dimension of pairs of datapoint indices @param indicesB (arraylike) Second dimension of pairs of datapoint indices @param n (int) Number of datapoints @returns (numpy.ndarray) Indices in condensed overlap matrix containing specified overlaps. Dimension will be same as indicesA and indicesB.", "response": "def _condensedIndex(indicesA, indicesB, n):\n    \"\"\"\n    Given a set of n points for which pairwise overlaps are stored in a flat\n    array X in the format returned by _computeOverlaps (upper triangular of the\n    overlap matrix in row-major order), this function returns the indices in X\n    that correspond to the overlaps for the pairs of points specified.\n\n    Example\n    -------\n    Consider the case with n = 5 data points for which pairwise overlaps are\n    stored in array X, which has length 10 = n(n-1)/2. To obtain the overlap\n    of points 2 and 3 and the overlap of points 4 and 1, call \n\n      idx = _condensedIndex([2, 4], [3, 1], 5) # idx == [6, 1]\n\n    Note: Since X does not contain the diagonal (self-comparisons), it is\n    invalid to pass arrays such that indicesA[i] == indicesB[i] for any i.\n\n    @param indicesA (arraylike) First dimension of pairs of datapoint indices\n\n    @param indicesB (arraylike) Second dimension of pairs of datapoint indices\n\n    @param n (int) Number of datapoints\n\n    @returns (numpy.ndarray) Indices in condensed overlap matrix containing\n        specified overlaps. Dimension will be same as indicesA and indicesB.\n    \"\"\"\n    indicesA = numpy.array(indicesA, dtype=int)\n    indicesB = numpy.array(indicesB, dtype=int)\n    n = int(n)\n\n    # Ensure that there are no self-comparisons\n    assert (indicesA != indicesB).all()\n\n    # re-arrange indices to ensure that rowIxs[i] < colIxs[i] for all i\n    rowIxs = numpy.where(indicesA < indicesB, indicesA, indicesB)\n    colIxs = numpy.where(indicesA < indicesB, indicesB, indicesA)\n\n    flatIxs = rowIxs * (n - 1) - (rowIxs + 1) * rowIxs / 2 + colIxs - 1\n\n    return flatIxs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _computeOverlaps(data, selfOverlaps=False, dtype=\"int16\"):\n    nVectors = data.shape[0]\n    nDims = data.shape[1]\n    nPairs = (nVectors+1)*nVectors/2 if selfOverlaps else (\n      nVectors*(nVectors-1)/2)\n    overlaps = numpy.ndarray(nPairs, dtype=dtype)\n    pos = 0\n\n    for i in xrange(nVectors):\n      start = i if selfOverlaps else i+1\n      a = data[i]\n      b = data[start:]\n      newOverlaps = a.multiply(b).getnnz(1)\n      run = newOverlaps.shape[0]\n      overlaps[pos:pos+run] = newOverlaps\n      pos += run\n    return overlaps", "response": "Compute all pairwise overlaps between the rows of the input."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nanalyzes the impact of each list parameter in this experiment", "response": "def analyzeParameters(expName, suite):\n  \"\"\"\n  Analyze the impact of each list parameter in this experiment\n  \"\"\"\n  print(\"\\n================\",expName,\"=====================\")\n  try:\n    expParams = suite.get_params(expName)\n    pprint.pprint(expParams)\n\n    for p in [\"boost_strength\", \"k\", \"learning_rate\", \"weight_sparsity\",\n              \"k_inference_factor\", \"boost_strength_factor\",\n              \"c1_out_channels\", \"c1_k\", \"learning_rate_factor\",\n              \"batches_in_epoch\",\n              ]:\n      if p in expParams and type(expParams[p]) == list:\n        print(\"\\n\",p)\n        for v1 in expParams[p]:\n          # Retrieve the last totalCorrect from each experiment\n          # Print them sorted from best to worst\n          values, params = suite.get_values_fix_params(\n            expName, 0, \"testerror\", \"last\", **{p:v1})\n          v = np.array(values)\n          try:\n            print(\"Average/min/max for\", p, v1, \"=\", v.mean(), v.min(), v.max())\n            # sortedIndices = v.argsort()\n            # for i in sortedIndices[::-1]:\n            #   print(v[i],params[i][\"name\"])\n          except:\n            print(\"Can't compute stats for\",p)\n\n  except:\n    print(\"Couldn't load experiment\",expName)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsummarize the totalCorrect value from the last iteration for each experiment and print them sorted from best to worst", "response": "def summarizeResults(expName, suite):\n  \"\"\"\n  Summarize the totalCorrect value from the last iteration for each experiment\n  in the directory tree.\n  \"\"\"\n  print(\"\\n================\",expName,\"=====================\")\n\n  try:\n    # Retrieve the last totalCorrect from each experiment\n    # Print them sorted from best to worst\n    values, params = suite.get_values_fix_params(\n      expName, 0, \"totalCorrect\", \"last\")\n    v = np.array(values)\n    sortedIndices = v.argsort()\n    for i in sortedIndices[::-1]:\n      print(v[i], params[i][\"name\"])\n\n    print()\n  except:\n    print(\"Couldn't analyze experiment\",expName)\n\n  try:\n    # Retrieve the last totalCorrect from each experiment\n    # Print them sorted from best to worst\n    values, params = suite.get_values_fix_params(\n      expName, 0, \"testerror\", \"last\")\n    v = np.array(values)\n    sortedIndices = v.argsort()\n    for i in sortedIndices[::-1]:\n      print(v[i], params[i][\"name\"])\n\n    print()\n  except:\n    print(\"Couldn't analyze experiment\",expName)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the test and overall noise errors from each iteration of this experiment.", "response": "def learningCurve(expPath, suite):\n  \"\"\"\n  Print the test and overall noise errors from each iteration of this experiment\n  \"\"\"\n  print(\"\\nLEARNING CURVE ================\",expPath,\"=====================\")\n  try:\n    headers=[\"testerror\",\"totalCorrect\",\"elapsedTime\",\"entropy\"]\n    result = suite.get_value(expPath, 0, headers, \"all\")\n    info = []\n    for i,v in enumerate(zip(result[\"testerror\"],result[\"totalCorrect\"],\n                             result[\"elapsedTime\"],result[\"entropy\"])):\n      info.append([i, v[0], v[1], int(v[2]), v[3]])\n    headers.insert(0,\"iteration\")\n    print(tabulate(info, headers=headers, tablefmt=\"grid\"))\n  except:\n    print(\"Couldn't load experiment\",expPath)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn base class method for this region.", "response": "def getSpec(cls):\n    \"\"\"Return base spec for this region. See base class method for more info.\"\"\"\n    spec = {\n      \"description\": CoordinateSensorRegion.__doc__,\n      \"singleNodeOnly\": True,\n      \"inputs\": {},  # input data is added to queue via \"addDataToQueue\" command\n      \"outputs\": {\n        \"dataOut\": {\n          \"description\": \"Encoded coordinate SDR.\",\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": True,\n        },\n        \"resetOut\": {\n          \"description\": \"0/1 reset flag output.\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False,\n        },\n        \"sequenceIdOut\": {\n          \"description\": \"Sequence ID\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False,\n        },\n      },\n      \"parameters\": {\n        \"activeBits\": {\n          \"description\": \"The number of bits that are set to encode a single \"\n                         \"coordinate value\",\n          \"dataType\": \"uint\",\n          \"accessMode\": \"ReadWrite\",\n          \"count\": 1,\n          \"defaultValue\": 21\n        },\n        \"outputWidth\": {\n          \"description\": \"Size of output vector\",\n          \"dataType\": \"UInt32\",\n          \"accessMode\": \"ReadWrite\",\n          \"count\": 1,\n          \"defaultValue\": 1000\n        },\n        \"radius\": {\n          \"description\": \"Radius around 'coordinate'\",\n          \"dataType\": \"UInt32\",\n          \"accessMode\": \"ReadWrite\",\n          \"count\": 1,\n          \"defaultValue\": 2\n        },\n        \"verbosity\": {\n          \"description\": \"Verbosity level\",\n          \"dataType\": \"UInt32\",\n          \"accessMode\": \"ReadWrite\",\n          \"count\": 1\n        },\n      },\n      \"commands\": {\n        \"addDataToQueue\": {\n          \"description\": CoordinateSensorRegion.addDataToQueue.__doc__,\n        },\n        \"addResetToQueue\": {\n          \"description\": CoordinateSensorRegion.addResetToQueue.__doc__,\n        }\n      },\n    }\n\n    return spec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the next record from the queue and encode it.", "response": "def compute(self, inputs, outputs):\n    \"\"\"\n    Get the next record from the queue and encode it.\n    @param inputs This parameter is ignored. The data comes from the queue\n    @param outputs See definition in the spec above.\n    \"\"\"\n    if len(self.queue) > 0:\n      data = self.queue.pop()\n\n    else:\n      raise Exception(\"CoordinateSensor: No data to encode: queue is empty\")\n\n    outputs[\"resetOut\"][0] = data[\"reset\"]\n    outputs[\"sequenceIdOut\"][0] = data[\"sequenceId\"]\n    sdr = self.encoder.encode((numpy.array(data[\"coordinate\"]), self.radius))\n    outputs[\"dataOut\"][:] = sdr\n\n    if self.verbosity > 1:\n      print \"CoordinateSensor outputs:\"\n      print \"Coordinate = \", data[\"coordinate\"]\n      print \"sequenceIdOut: \", outputs[\"sequenceIdOut\"]\n      print \"resetOut: \", outputs[\"resetOut\"]\n      print \"dataOut: \", outputs[\"dataOut\"].nonzero()[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef addDataToQueue(self, coordinate, reset, sequenceId):\n    if type(coordinate) == type(\"\"):\n      coordinateList = eval(coordinate)\n    elif type(coordinate) == type([]):\n      coordinateList = coordinate\n    else:\n      raise Exception(\"CoordinateSensor.addDataToQueue: unknown type for \"\n                      \"coordinate\")\n\n    self.queue.appendleft({\n      \"sequenceId\": int(sequenceId),\n      \"reset\": int(reset),\n      \"coordinate\": coordinateList,\n    })", "response": "Adds a data item to the sensor s internal queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef printDiagnostics(exp, sequences, objects, args, verbosity=0):\n  print \"Experiment start time:\", time.ctime()\n  print \"\\nExperiment arguments:\"\n  pprint.pprint(args)\n\n  r = sequences.objectConfusion()\n  print \"Average common pairs in sequences=\", r[0],\n  print \", features=\",r[2]\n\n  r = objects.objectConfusion()\n  print \"Average common pairs in objects=\", r[0],\n  print \", locations=\",r[1],\n  print \", features=\",r[2]\n\n  # For detailed debugging\n  if verbosity > 0:\n    print \"\\nObjects are:\"\n    for o in objects:\n      pairs = objects[o]\n      pairs.sort()\n      print str(o) + \": \" + str(pairs)\n    print \"\\nSequences:\"\n    for i in sequences:\n      print i,sequences[i]\n\n  print \"\\nNetwork parameters:\"\n  pprint.pprint(exp.config)", "response": "Useful diagnostics for debugging."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates all possible tuple arguments for the kwargs.", "response": "def createArgs(**kwargs):\n  \"\"\"\n  Each kwarg is a list. Return a list of dicts representing all possible\n  combinations of the kwargs.\n  \"\"\"\n  if len(kwargs) == 0: return [{}]\n  kargs = deepcopy(kwargs)\n  k1 = kargs.keys()[0]\n  values = kargs.pop(k1)\n  args = []\n\n  # Get all other combinations\n  otherArgs = createArgs(**kargs)\n\n  # Create combinations for values associated with k1\n  for v in values:\n    newArgs = deepcopy(otherArgs)\n    arg = {k1: v}\n    for newArg in newArgs:\n      newArg.update(arg)\n      args.append(newArg)\n\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef randomizeSequence(sequence, symbolsPerSequence, numColumns, sparsity, p = 0.25):\n  randomizedSequence = []\n  sparseCols = int(numColumns * sparsity)\n  numSymbolsToChange = int(symbolsPerSequence * p)\n  symIndices = np.random.permutation(np.arange(symbolsPerSequence))  \n  for symbol in range(symbolsPerSequence):\n    randomizedSequence.append(sequence[symbol])\n  i = 0\n  while numSymbolsToChange > 0:\n    randomizedSequence[symIndices[i]] = generateRandomSymbol(numColumns, sparseCols)\n    i += 1\n    numSymbolsToChange -= 1\n  return randomizedSequence", "response": "Takes a sequence as input and randomizes a percentage p of it by choosing the next SDRs at random."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a high - order sequence by taking an initial sequence and the changing", "response": "def generateHOSequence(sequence, symbolsPerSequence, numColumns, sparsity):\n  \"\"\"\n  Generates a high-order sequence by taking an initial sequence and the changing\n  its first and last SDRs by random SDRs\n  \n  @param sequence (array) sequence to be randomized\n  @param symbolsPerSequence (int) number of symbols per sequence\n  @param numColumns (int) number of columns in the TM\n  @param sparsity (float) percentage of sparsity  \n  @return randomizedSequence (array) sequence that contains p percentage of new SDRs\n  \"\"\"\n  sequenceHO = []\n  sparseCols = int(numColumns * sparsity)\n  for symbol in range(symbolsPerSequence):\n    if symbol == 0 or symbol == (symbolsPerSequence - 1):\n      sequenceHO.append(generateRandomSymbol(numColumns, sparseCols))\n    else:\n      sequenceHO.append(sequence[symbol])\n  return sequenceHO"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the percentage of overlap between two SDRs @param x1 (array) SDR @param x2 (array) SDR @return percentageOverlap (float) percentage overlap between x1 and x2", "response": "def percentOverlap(x1, x2, numColumns):\n  \"\"\"\n  Calculates the percentage of overlap between two SDRs\n  \n  @param x1 (array) SDR\n  @param x2 (array) SDR  \n  @return percentageOverlap (float) percentage overlap between x1 and x2\n  \"\"\"  \n  nonZeroX1 = np.count_nonzero(x1)\n  nonZeroX2 = np.count_nonzero(x2)\n  sparseCols = min(nonZeroX1, nonZeroX2)  \n  # transform input vector specifying columns into binary vector\n  binX1 = np.zeros(numColumns, dtype=\"uint32\")\n  binX2 = np.zeros(numColumns, dtype=\"uint32\")\n  for i in range(sparseCols):\n    binX1[x1[i]] = 1\n    binX2[x2[i]] = 1\n  return float(np.dot(binX1, binX2))/float(sparseCols)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generateRandomSymbol(numColumns, sparseCols):\n  symbol = list()\n  remainingCols = sparseCols\n  while remainingCols > 0:\n    col = random.randrange(numColumns)\n    if col not in symbol:\n      symbol.append(col)\n      remainingCols -= 1\n  return symbol", "response": "Generates a random SDR with the specified number of active columns and the specified number of sparse columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generateRandomSequence(numSymbols, numColumns, sparsity):\n  sequence = []\n  sparseCols = int(numColumns * sparsity)\n  for _ in range(numSymbols):\n    sequence.append(generateRandomSymbol(numColumns, sparseCols))\n  return sequence", "response": "Generates a random sequence comprising numSymbols SDRs\n  \n"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing pairwise correlations from spike trains.", "response": "def computePWCorrelations(spikeTrains, removeAutoCorr):\n  \"\"\"\n  Computes pairwise correlations from spikeTrains\n  \n  @param spikeTrains (array) spike trains obtained from the activation of cells in the TM\n         the array dimensions are: numCells x timeSteps\n  @param removeAutoCorr (boolean) if true, auto-correlations are removed by substracting\n         the diagonal of the correlation matrix         \n  @return corrMatrix (array) numCells x numCells matrix containing the Pearson correlation\n          coefficient of spike trains of cell i and cell j\n  @return numNegPCC (int) number of negative pairwise correlations (PCC(i,j) < 0)\n  \"\"\"\n  numCells = np.shape(spikeTrains)[0]\n  corrMatrix = np.zeros((numCells, numCells))\n  numNegPCC = 0\n  for i in range(numCells):\n    for j in range(numCells):\n      if i == j and removeAutoCorr == True:\n        continue\n      if not all(spikeTrains[i,:] == 0) and not all(spikeTrains[j,:] == 0):\n        corrMatrix[i,j] = np.corrcoef(spikeTrains[i,:], spikeTrains[j,:])[0,1]    \n        if corrMatrix[i,j] < 0:\n          numNegPCC += 1\n  return (corrMatrix, numNegPCC)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accuracy(current, predicted):\n  acc = 0\n  if np.count_nonzero(predicted) > 0:\n    acc = float(np.dot(current, predicted))/float(np.count_nonzero(predicted))\n  return acc", "response": "Computes the accuracy of the TM at time - step t based on the current active columns at time - step t - 1 and the predicted active columns at time - step t - 1."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate indices of cell pairs each pair of cells are from the same column", "response": "def sampleCellsWithinColumns(numCellPairs, cellsPerColumn, numColumns, seed=42):\n  \"\"\"\n  Generate indices of cell pairs, each pair of cells are from the same column\n  @return cellPairs (list) list of cell pairs\n  \"\"\"\n  np.random.seed(seed)\n  cellPairs = []\n  for i in range(numCellPairs):\n    randCol = np.random.randint(numColumns)\n    randCells = np.random.choice(np.arange(cellsPerColumn), (2, ), replace=False)\n\n    cellsPair = randCol * cellsPerColumn + randCells\n    cellPairs.append(cellsPair)\n  return cellPairs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate indices of cell pairs each pair of cells are from different column", "response": "def sampleCellsAcrossColumns(numCellPairs, cellsPerColumn, numColumns, seed=42):\n  \"\"\"\n  Generate indices of cell pairs, each pair of cells are from different column\n  @return cellPairs (list) list of cell pairs\n  \"\"\"\n  np.random.seed(seed)\n  cellPairs = []\n  for i in range(numCellPairs):\n    randCols = np.random.choice(np.arange(numColumns), (2, ), replace=False)\n    randCells = np.random.choice(np.arange(cellsPerColumn), (2, ), replace=False)\n\n    cellsPair = np.zeros((2, ))\n    for j in range(2):\n      cellsPair[j] = randCols[j] * cellsPerColumn + randCells[j]\n    cellPairs.append(cellsPair.astype('int32'))\n  return cellPairs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nobtain a random sample of cells from the whole spike train matrix consisting of numCells cells from the start of simulation time up to currentTS @param spikeTrains (array) array containing the spike trains of cells in the TM @param numCells (int) number of cells to be sampled from the matrix of spike trains @param totalCells (int) total number of cells in the TM @param currentTS (int) time-step upper bound of sample (sample will go from time-step 0 up to currentTS) @param timeWindow (int) number of time-steps to sample from the spike trains @return subSpikeTrains (array) spike train matrix sampled from the total spike train matrix", "response": "def subSample(spikeTrains, numCells, totalCells, currentTS, timeWindow):\n  \"\"\"\n  Obtains a random sample of cells from the whole spike train matrix consisting of numCells cells\n  from the start of simulation time up to currentTS\n  \n  @param spikeTrains (array) array containing the spike trains of cells in the TM\n  @param numCells (int) number of cells to be sampled from the matrix of spike trains\n  @param totalCells (int) total number of cells in the TM\n  @param currentTS (int) time-step upper bound of sample (sample will go from time-step 0 up to currentTS)\n  @param timeWindow (int) number of time-steps to sample from the spike trains\n  @return subSpikeTrains (array) spike train matrix sampled from the total spike train matrix\n  \"\"\"\n  indices = np.random.permutation(np.arange(totalCells))\n  if currentTS > 0 and currentTS < timeWindow:\n    subSpikeTrains = np.zeros((numCells, currentTS), dtype = \"uint32\")\n    for i in range(numCells):\n      subSpikeTrains[i,:] = spikeTrains[indices[i],:]\n  elif currentTS > 0 and currentTS >= timeWindow:\n    subSpikeTrains = np.zeros((numCells, timeWindow), dtype = \"uint32\")    \n    for i in range(numCells):\n      subSpikeTrains[i,:] = spikeTrains[indices[i],(currentTS-timeWindow):currentTS]\n  elif currentTS == 0:\n    # This option takes the whole spike train history\n    totalTS = np.shape(spikeTrains)[1]\n    subSpikeTrains = np.zeros((numCells, totalTS), dtype = \"uint32\")\n    for i in range(numCells):\n      subSpikeTrains[i,:] = spikeTrains[indices[i],:]\n  elif currentTS < 0:\n    # This option takes a timestep at random and a time window \n    # specified by the user after the chosen time step\n    totalTS = np.shape(spikeTrains)[1]\n    subSpikeTrains = np.zeros((numCells, timeWindow), dtype = \"uint32\")\n    rnd = random.randrange(totalTS - timeWindow)\n    print \"Starting from timestep: \" + str(rnd)\n    for i in range(numCells):\n      subSpikeTrains[i,:] = spikeTrains[indices[i],rnd:(rnd+timeWindow)]\n    \n  return subSpikeTrains"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subSampleWholeColumn(spikeTrains, colIndices, cellsPerColumn, currentTS, timeWindow):\n  numColumns = np.shape(colIndices)[0]\n  numCells = numColumns * cellsPerColumn\n\n  if currentTS > 0 and currentTS < timeWindow:\n    subSpikeTrains = np.zeros((numCells, currentTS), dtype = \"uint32\")\n    for i in range(numColumns):\n      currentCol = colIndices[i]\n      initialCell = cellsPerColumn * currentCol\n      for j in range(cellsPerColumn):\n        subSpikeTrains[(cellsPerColumn*i) + j,:] = spikeTrains[initialCell + j,:]\n  elif currentTS > 0 and currentTS >= timeWindow:\n    subSpikeTrains = np.zeros((numCells, timeWindow), dtype = \"uint32\")\n    for i in range(numColumns):\n      currentCol = colIndices[i]\n      initialCell = cellsPerColumn * currentCol\n      for j in range(cellsPerColumn):\n        subSpikeTrains[(cellsPerColumn*i) + j,:] = spikeTrains[initialCell + j,(currentTS-timeWindow):currentTS]\n  elif currentTS == 0:\n    # This option takes the whole spike train history\n    totalTS = np.shape(spikeTrains)[1]\n    subSpikeTrains = np.zeros((numCells, totalTS), dtype = \"uint32\")\n    for i in range(numColumns):\n      currentCol = colIndices[i]\n      initialCell = cellsPerColumn * currentCol\n      for j in range(cellsPerColumn):\n        subSpikeTrains[(cellsPerColumn*i) + j,:] = spikeTrains[initialCell + j,:]\n  elif currentTS < 0:\n    totalTS = np.shape(spikeTrains)[1]\n    subSpikeTrains = np.zeros((numCells, timeWindow), dtype = \"uint32\")\n    rnd = random.randrange(totalTS - timeWindow)\n    print \"Starting from timestep: \" + str(rnd)\n    for i in range(numColumns):\n      currentCol = colIndices[i]\n      initialCell = cellsPerColumn * currentCol\n      for j in range(cellsPerColumn):\n        subSpikeTrains[(cellsPerColumn*i) + j,:] = spikeTrains[initialCell + j,rnd:(rnd+timeWindow)]\n    \n  return subSpikeTrains", "response": "This function returns a matrix of spike trains sampled from the total spike train matrix of the whole column."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nestimate entropy in spike trains.", "response": "def computeEntropy(spikeTrains):\n  \"\"\"\n  Estimates entropy in spike trains.\n  \n  @param spikeTrains (array) matrix of spike trains\n  @return entropy (float) entropy\n  \"\"\"\n  MIN_ACTIVATION_PROB = 0.000001\n  activationProb = np.mean(spikeTrains, 1)\n  activationProb[activationProb < MIN_ACTIVATION_PROB] = MIN_ACTIVATION_PROB\n  activationProb = activationProb / np.sum(activationProb)\n  entropy = -np.dot(activationProb, np.log2(activationProb))\n  return entropy"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the inter - spike interval from a spike train matrix.", "response": "def computeISI(spikeTrains):\n  \"\"\"\n  Estimates the inter-spike interval from a spike train matrix.\n  \n  @param spikeTrains (array) matrix of spike trains\n  @return isi (array) matrix with the inter-spike interval obtained from the spike train.\n          Each entry in this matrix represents the number of time-steps in-between 2 spikes\n          as the algorithm scans the spike train matrix.\n  \"\"\"\n  zeroCount = 0\n  isi = []\n  cells = 0\n  for i in range(np.shape(spikeTrains)[0]):\n    if cells > 0 and cells % 250 == 0:\n      print str(cells) + \" cells processed\"\n    for j in range(np.shape(spikeTrains)[1]):\n      if spikeTrains[i][j] == 0:\n        zeroCount += 1\n      elif zeroCount > 0:\n        isi.append(zeroCount)\n        zeroCount = 0\n    zeroCount = 0\n    cells += 1\n  print \"**All cells processed**\"\n  return isi"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef poissonSpikeGenerator(firingRate, nBins, nTrials):\n  dt = 0.001 # we are simulating a ms as a single bin in a vector, ie 1sec = 1000bins\n  poissonSpikeTrain = np.zeros((nTrials, nBins), dtype = \"uint32\")\n  for i in range(nTrials):\n    for j in range(int(nBins)):\n      if random.random() < firingRate*dt:\n        poissonSpikeTrain[i,j] = 1\n  return poissonSpikeTrain", "response": "Generates a Poisson spike train."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a raster from spike trains. nchanelseq.", "response": "def raster(event_times_list, color='k'):\n  \"\"\"\n  Creates a raster from spike trains.\n  \n  @param event_times_list (array) matrix containing times in which a cell fired\n  @param color (string) color of spike in raster\n  \n  @return ax (int) position of plot axes\n  \"\"\"\n  ax = plt.gca()\n  for ith, trial in enumerate(event_times_list):\n    plt.vlines(trial, ith + .5, ith + 1.5, color=color)\n  plt.ylim(.5, len(event_times_list) + .5)\n  return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rasterPlot(spikeTrain, model):\n  nTrials = np.shape(spikeTrain)[0]\n  spikes = []\n  for i in range(nTrials):\n    spikes.append(spikeTrain[i].nonzero()[0].tolist())\n  plt.figure()\n  ax = raster(spikes)\n  plt.xlabel('Time')\n  plt.ylabel('Neuron')\n  # plt.show()\n  plt.savefig(\"raster\" + str(model))\n  plt.close()", "response": "Plots raster and saves figure in working directory\n  \n"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncount the number of times in which a binary pattern occurs in a spike train.", "response": "def countInSample(binaryWord, spikeTrain):\n  \"\"\"\n  Counts the times in which a binary pattern (e.g. [1 0 0 1 1 0 1]) occurs in a spike train\n  \n  @param binaryWord (array) binary vector to be search within the spike train matrix.\n  @param spikeTrain (array) matrix of spike trains. Dimensions of binary word are 1xN, whereas\n         dimensions of spikeTrain must be NxtimeSteps\n  @return count (int) number of occurrences of binaryWord in spikeTrains\n  \"\"\"\n  count = 0\n  timeSteps = np.shape(spikeTrain)[1]\n  for i in range(timeSteps):\n    if np.dot(binaryWord, spikeTrain[:,i]) == np.count_nonzero(binaryWord):\n      count += 1\n      #print i\n  return count"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef saveTM(tm):\n  # Save the TM to a file for future use\n  proto1 = TemporalMemoryProto_capnp.TemporalMemoryProto.new_message()\n  tm.write(proto1)\n  # Write the proto to a file and read it back into a new proto\n  with open('tm.nta', 'wb') as f:\n    proto1.write(f)", "response": "Saves the temporal memory and the sequences generated for its training."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inputAnalysis(allSequences, model, numColumns):\n  records = np.shape(allSequences)[0]\n  symbols = np.shape(allSequences)[1]\n  totalItems = records * symbols\n  \n  overlapMatrix = np.zeros((totalItems, totalItems))\n  \n  for i in range(totalItems):\n    if i % 500 == 0:\n      print str(i) + \" rows processed\"\n    for j in range(totalItems):\n      if model == \"random\":\n        overlapMatrix[i, j] = percentOverlap(allSequences[int(i/symbols)][i%symbols], \n                                             allSequences[int(j/symbols)][j%symbols], \n                                             numColumns)\n      elif model == \"periodic\":\n        overlapMatrix[i, j] = percentOverlap(allSequences[i][0], allSequences[j][0], \n                                             numColumns)\n  print \"***All rows processed!***\"    \n  # substract diagonal from correlation matrix\n  overlapMatrix = np.subtract(overlapMatrix, np.identity(totalItems))\n  \n  return overlapMatrix", "response": "This function calculates the overlap score of each SDR used as input to the temporal memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing pairwise correlations between spike trains within a column.", "response": "def computePWCorrelationsWithinCol(spikeTrains, removeAutoCorr, cellsPerColumn):\n  \"\"\"\n  Computes pairwise correlations from spikeTrains\n  \n  @param spikeTrains (array) spike trains obtained from the activation of cells in the TM\n     the array dimensions are: numCells x timeSteps\n  @param removeAutoCorr (boolean) if true, auto-correlations are removed by substracting\n     the diagonal of the correlation matrix\n  @param cellsPerColumn (int) number of cells per column in thr TM\n  @return corrMatrix (array) numCells x numCells matrix containing the Pearson correlation\n      coefficient of spike trains of cell i and cell j\n  @return numNegPCC (int) number of negative pairwise correlations (PCC(i,j) < 0)\n  \"\"\"\n  numCells = np.shape(spikeTrains)[0]\n  numCols = numCells / cellsPerColumn\n  corrMatrix = np.zeros((numCells, numCells))\n  numNegPCC = 0\n  indices = np.zeros(cellsPerColumn, dtype=\"uint32\")\n  \n  for col in range(numCols):\n    for k in range(cellsPerColumn):\n      indices[k] = (cellsPerColumn * col) + k\n    for i in indices:\n      for j in indices:\n        if i == j and removeAutoCorr == True:\n          continue\n        if not all(spikeTrains[i,:] == 0) and not all(spikeTrains[j,:] == 0):          \n          corrMatrix[i,j] = np.corrcoef(spikeTrains[i,:], spikeTrains[j,:])[0,1]  \n          if corrMatrix[i,j] < 0:\n            numNegPCC += 1\n          \n  return (corrMatrix, numNegPCC)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef readCSV(csvFile, numLabels=0):\n  try:\n    with open(csvFile, \"rU\") as f:\n      reader = csv.reader(f)\n      headers = next(reader, None)\n      try:\n        sampleIdx = headers.index(\"Sample\")\n        idIdx = headers.index(\"ID\")\n      except ValueError as e:\n        print (\"Could not find 'ID' and/or 'Sample' columns, so assuming \"\n               \"they are 0 and 2, respectively.\")\n        sampleIdx = 2\n        idIdx = 0\n      \n      dataDict = {}\n\n      if numLabels > 0:\n        labelIdx = range(sampleIdx + 1, sampleIdx + 1 + numLabels)\n        for lineNumber, line in enumerate(reader):\n          dataDict[lineNumber] = (line[sampleIdx],\n                                  [line[i] for i in labelIdx if line[i]],\n                                  line[idIdx])\n      else:\n        for lineNumber, line in enumerate(reader):\n          dataDict[lineNumber] = (line[sampleIdx], [], line[idIdx])\n\n      return dataDict\n\n  except IOError as e:\n    print e", "response": "Read in a CSV file with the following formatting."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mapLabelRefs(dataDict):\n  labelRefs = [label for label in set(\n    itertools.chain.from_iterable([x[1] for x in dataDict.values()]))]\n\n  for recordNumber, data in dataDict.iteritems():\n    dataDict[recordNumber] = (data[0], numpy.array(\n      [labelRefs.index(label) for label in data[1]]), data[2])\n\n  return labelRefs, dataDict", "response": "Map the label strings in dataDict with corresponding ints."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the individual buckets in csvFile to their own CSV files.", "response": "def bucketCSVs(csvFile, bucketIdx=2):\n  \"\"\"Write the individual buckets in csvFile to their own CSV files.\"\"\"\n  try:\n    with open(csvFile, \"rU\") as f:\n      reader = csv.reader(f)\n      headers = next(reader, None)\n      dataDict = OrderedDict()\n      for lineNumber, line in enumerate(reader):\n        if line[bucketIdx] in dataDict:\n          dataDict[line[bucketIdx]].append(line)\n        else:\n          # new bucket\n          dataDict[line[bucketIdx]] = [line]\n  except IOError as e:\n    print e\n\n  filePaths = []\n  for i, (_, lines) in enumerate(dataDict.iteritems()):\n    bucketFile = csvFile.replace(\".\", \"_\"+str(i)+\".\")\n    writeCSV(lines, headers, bucketFile)\n    filePaths.append(bucketFile)\n\n  return filePaths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef readDir(dirPath, numLabels, modify=False):\n  samplesDict = defaultdict(list)\n  for _, _, files in os.walk(dirPath):\n    for f in files:\n      basename, extension = os.path.splitext(os.path.basename(f))\n      if \".\" in basename and extension == \".csv\":\n        category = basename.split(\".\")[-1]\n        if modify:\n          category = category.replace(\"0\", \"/\")\n          category = category.replace(\"_\", \" \")\n        samplesDict[category] = readCSV(\n          os.path.join(dirPath, f), numLabels=numLabels)\n\n  return samplesDict", "response": "Reads in data from a directory of CSV files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite data with column headers to a CSV file.", "response": "def writeCSV(data, headers, csvFile):\n  \"\"\"Write data with column headers to a CSV.\"\"\"\n  with open(csvFile, \"wb\") as f:\n    writer = csv.writer(f, delimiter=\",\")\n    writer.writerow(headers)\n    writer.writerows(data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a dictionary to a CSV file.", "response": "def writeFromDict(dataDict, headers, csvFile):\n  \"\"\"\n  Write dictionary to a CSV, where keys are row numbers and values are a list.\n  \"\"\"\n  with open(csvFile, \"wb\") as f:\n    writer = csv.writer(f, delimiter=\",\")\n    writer.writerow(headers)\n    for row in sorted(dataDict.keys()):\n      writer.writerow(dataDict[row])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads data file specified in args and reshuffle the categories in the original data file.", "response": "def readDataAndReshuffle(args, categoriesInOrderOfInterest=None):\n  \"\"\"\n  Read data file specified in args, optionally reshuffle categories, print out\n  some statistics, and return various data structures. This routine is pretty\n  specific and only used in some simple test scripts.\n\n  categoriesInOrderOfInterest (list) Optional list of integers representing\n                                     the priority order of various categories.\n                                     The categories in the original data file\n                                     will be reshuffled to the order in this\n                                     array, up to args.numLabels, if specified.\n\n  Returns the tuple:\n    (dataset, labelRefs, documentCategoryMap, documentTextMap)\n\n  Return format:\n      dataset = [\n        [\"fox eats carrots\", [0], docId],\n        [\"fox eats peppers\", [0], docId],\n        [\"carrots are healthy\", [1], docId],\n        [\"peppers is healthy\", [1], docId],\n      ]\n\n      labelRefs = [Category0Name, Category1Name, ...]\n\n      documentCategoryMap = {\n        docId: [categoryIndex0, categoryIndex1, ...],\n        docId: [categoryIndex0, categoryIndex1, ...],\n                :\n      }\n\n      documentTextMap = {\n        docId: documentText,\n        docId: documentText,\n                :\n      }\n\n  \"\"\"\n  # Read data\n  dataDict = readCSV(args.dataPath, 1)\n  labelRefs, dataDict = mapLabelRefs(dataDict)\n\n  if \"numLabels\" in args:\n    numLabels = args.numLabels\n  else:\n    numLabels = len(labelRefs)\n\n  if categoriesInOrderOfInterest is None:\n      categoriesInOrderOfInterest = range(0,numLabels)\n  else:\n    categoriesInOrderOfInterest=categoriesInOrderOfInterest[0:numLabels]\n\n  # Select data based on categories of interest. Shift category indices down\n  # so we go from 0 to numLabels-1\n  dataSet = []\n  documentTextMap = {}\n  counts = numpy.zeros(len(labelRefs))\n  for document in dataDict.itervalues():\n    try:\n      docId = int(document[2])\n    except:\n      raise RuntimeError(\"docId \"+str(docId)+\" is not an integer\")\n    oldCategoryIndex = document[1][0]\n    documentTextMap[docId] = document[0]\n    if oldCategoryIndex in categoriesInOrderOfInterest:\n      newIndex = categoriesInOrderOfInterest.index(oldCategoryIndex)\n      dataSet.append([document[0], [newIndex], docId])\n      counts[newIndex] += 1\n\n  # For each document, figure out which categories it belongs to\n  # Include the shifted category index\n  documentCategoryMap = {}\n  for doc in dataDict.iteritems():\n    docId = int(doc[1][2])\n    oldCategoryIndex = doc[1][1][0]\n    if oldCategoryIndex in categoriesInOrderOfInterest:\n      newIndex = categoriesInOrderOfInterest.index(oldCategoryIndex)\n      v = documentCategoryMap.get(docId, [])\n      v.append(newIndex)\n      documentCategoryMap[docId] = v\n\n  labelRefs = [labelRefs[i] for i in categoriesInOrderOfInterest]\n  print \"Total number of unique documents\",len(documentCategoryMap)\n  print \"Category counts: \",counts\n  print \"Categories in training/test data:\", labelRefs\n\n  return dataSet, labelRefs, documentCategoryMap, documentTextMap"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef createModel(modelName, **kwargs):\n\n  if modelName not in TemporalMemoryTypes.getTypes():\n    raise RuntimeError(\"Unknown model type: \" + modelName)\n\n  return getattr(TemporalMemoryTypes, modelName)(**kwargs)", "response": "Returns a ClassficationModel of the appropriate type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getConstructorArguments(modelName):\n\n  if modelName not in TemporalMemoryTypes.getTypes():\n    raise RuntimeError(\"Unknown model type: \" + modelName)\n\n  argspec = inspect.getargspec(\n                            getattr(TemporalMemoryTypes, modelName).__init__)\n  return (argspec.args[1:], argspec.defaults)", "response": "Returns the constructor arguments and associated default values for the given model type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getTypes(cls):\n\n    for attrName in dir(cls):\n      attrValue = getattr(cls, attrName)\n      if (isinstance(attrValue, type)):\n        yield attrName", "response": "Get the acceptable model types for the given class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing all the dendrites of the neuron to a set of random connections", "response": "def initialize_dendrites(self):\n    \"\"\"\n    Initialize all the dendrites of the neuron to a set of random connections\n    \"\"\"\n    # Wipe any preexisting connections by creating a new connection matrix\n    self.dendrites = SM32()\n    self.dendrites.reshape(self.dim, self.num_dendrites)\n\n    for row in range(self.num_dendrites):\n      synapses = numpy.random.choice(self.dim, self.dendrite_length, replace = False)\n      for synapse in synapses:\n        self.dendrites[synapse, row] = 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the activation of a datapoint", "response": "def calculate_activation(self, datapoint):\n    \"\"\"\n    Only for a single datapoint\n    \"\"\"\n\n    activations = datapoint * self.dendrites\n    activations = self.nonlinearity(activations)\n    return activations.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses a style of initialization inspired by the temporal memory. When a new positive example is found, a dendrite is chosen and a number of synapses are created to the example. This works intelligently with an amount of data larger than the number of available dendrites. In this case, data is clustered, and then similar datapoints are allotted to shared dendrites, with as many overlapping bits as possible chosen. In practice, it is still better to simply allocate enough dendrites to have one per datapoint, but this method at least allows initialization to work on larger amounts of data.", "response": "def HTM_style_initialize_on_data(self, data, labels):\n    \"\"\"\n    Uses a style of initialization inspired by the temporal memory.  When a new\n    positive example is found, a dendrite is chosen and a number of synapses are\n    created to the example.\n\n    This works intelligently with an amount of data larger than the number of\n    available dendrites. In this case, data is clustered, and then similar\n    datapoints are allotted to shared dendrites, with as many overlapping bits\n    as possible chosen.  In practice, it is still better to simply allocate\n    enough dendrites to have one per datapoint, but this method at least allows\n    initialization to work on larger amounts of data.\n    \"\"\"\n    current_dendrite = 0\n    self.dendrites = SM32()\n    self.dendrites.reshape(self.dim, self.num_dendrites)\n\n    # We want to avoid training on any negative examples\n    data = copy.deepcopy(data)\n    data.deleteRows([i for i, v in enumerate(labels) if v != 1])\n\n    if data.nRows() > self.num_dendrites:\n      print \"Neuron using clustering to initialize dendrites\"\n      data = (data.toDense())\n      model = KMeans(n_clusters = self.num_dendrites, n_jobs=1)\n      clusters = model.fit_predict(data)\n      multisets = [[Counter(), []] for i in range(self.num_dendrites)]\n      sparse_data = [[i for i, d in enumerate(datapoint) if d == 1] for datapoint in data]\n\n      for datapoint, cluster in zip(sparse_data, clusters):\n        multisets[cluster][0] = multisets[cluster][0] + Counter(datapoint)\n        multisets[cluster][1].append(set(datapoint))\n\n      for i, multiset in enumerate(multisets):\n        shared_elements = set(map(lambda x: x[0], filter(lambda x: x[1] > 1, multiset[0].most_common(self.dendrite_length))))\n        dendrite_connections = shared_elements\n        while len(shared_elements) < self.dendrite_length:\n          most_distant_point = multiset[1][numpy.argmin([len(dendrite_connections.intersection(point)) for point in multiset[1]])]\n          new_connection = random.sample(most_distant_point - dendrite_connections, 1)[0]\n          dendrite_connections.add(new_connection)\n\n        for synapse in dendrite_connections:\n          self.dendrites[synapse, current_dendrite] = 1.\n        current_dendrite += 1\n\n    else:\n      for i in range(data.nRows()):\n        ones = data.rowNonZeros(i)[0]\n        dendrite_connections = numpy.random.choice(ones, size = self.dendrite_length, replace = False)\n        for synapse in dendrite_connections:\n          self.dendrites[synapse, current_dendrite] = 1.\n\n        current_dendrite += 1\n\n    self.initialize_permanences()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a version of permanence - based training on a datapoint.", "response": "def HTM_style_train_on_datapoint(self, datapoint, label):\n    \"\"\"\n    Run a version of permanence-based training on a datapoint.  Due to the fixed dendrite count and dendrite length,\n    we are forced to more efficiently use each synapse, deleting synapses and resetting them if they are not found useful.\n    \"\"\"\n    activations = datapoint * self.dendrites\n    self.nonlinearity(activations)\n\n    #activations will quite likely still be sparse if using a threshold nonlinearity, so want to keep it sparse\n    activation = numpy.sign(activations.sum())\n\n\n    if label >= 1 and activation >= 0.5:\n      strongest_branch = activations.rowMax(0)[0]\n      datapoint.transpose()\n      inc_vector = self.dendrites.getSlice(0, self.dim, strongest_branch, strongest_branch + 1) * self.permanence_increment\n      inc_vector.elementNZMultiply(datapoint)\n      dec_vector = self.dendrites.getSlice(0, self.dim, strongest_branch, strongest_branch + 1) * self.permanence_decrement\n      dec_vector.elementNZMultiply(1 - datapoint)\n\n\n\n      self.permanences.setSlice(0, strongest_branch, self.permanences.getSlice(0, self.dim, strongest_branch, strongest_branch + 1) + inc_vector - dec_vector)\n\n      positions, scores = self.permanences.colNonZeros(strongest_branch)[0], self.permanences.colNonZeros(strongest_branch)[1]\n\n      for position, score in zip(positions, scores):\n        if score < self.permanence_threshold:\n          self.dendrites[position, strongest_branch] = 0\n          self.permanences[position, strongest_branch] = 0\n          new_connection = random.sample(set(datapoint.colNonZeros(0)[0]) - set(self.dendrites.colNonZeros(strongest_branch)[0]), 1)[0]\n          self.dendrites[new_connection, strongest_branch] = 1.\n          self.permanences[new_connection, strongest_branch] = self.initial_permanence\n\n\n    elif label < 1 and activation >= 0.5:\n      # Need to weaken some connections\n      strongest_branch = activations.rowMax(0)[0]\n\n      dec_vector = self.dendrites.getSlice(0, self.dim, strongest_branch, strongest_branch + 1) * self.permanence_decrement\n      datapoint.transpose()\n      dec_vector.elementNZMultiply(datapoint)\n      self.permanences.setSlice(0, strongest_branch, self.permanences.getSlice(0, self.dim, strongest_branch, strongest_branch + 1) - dec_vector)\n\n\n    elif label >= 1 and activation < 0.5:\n      # Need to create some new connections\n      weakest_branch = numpy.argmin(self.permanences.colSums())\n      if numpy.median(self.permanences.getCol(weakest_branch)) < self.permanence_threshold:\n        self.permanences.setColToZero(weakest_branch)\n        self.dendrites.setColToZero(weakest_branch)\n\n        ones = datapoint.rowNonZeros(0)[0]\n        dendrite_connections = numpy.random.choice(ones, size = self.dendrite_length, replace = False)\n        for synapse in dendrite_connections:\n          self.dendrites[synapse, weakest_branch] = 1.\n          self.permanences[synapse, weakest_branch] = self.initial_permanence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the spatial pooler class in the parent classes", "response": "def _getParentSpatialPoolerClass(name):\n    \"\"\"\n    Find the spatial pooler in the parent classes\n    :param name: pooler class name as in _getPoolerClass\n    :return: the parent spatial pooler class\n    \"\"\"\n    baseClassList = list(_getPoolerClass(name).__bases__)\n    spatialPoolerParent = None\n    while len(baseClassList) > 0 and spatialPoolerParent is None:\n      v = baseClassList.pop()\n      if v.__name__ is \"SpatialPooler\":\n        spatialPoolerParent = v\n      if v.__name__ is not 'object':\n        baseClassList += list(v.__bases__)\n\n    if spatialPoolerParent is None:\n      raise RuntimeError(\"Union pooler class does not inherit from \"\n                         \"spatial pooler class\")\n    return spatialPoolerParent"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the additional specs in three groups based on the selected pooler class and type.", "response": "def _getAdditionalSpecs(poolerClass=_getDefaultPoolerClass(), poolerType=\"union\"):\n  \"\"\"Build the additional specs in three groups (for the inspector)\n\n  Use the type of the default argument to set the Spec type, defaulting\n  to \"Byte\" for None and complex types\n\n  Determines the pooler parameters based on the selected implementation.\n  \"\"\"\n  typeNames = {int: \"UInt32\", float: \"Real32\", str: \"Byte\", bool: \"bool\",\n               tuple: \"tuple\"}\n\n  def getArgType(arg):\n    t = typeNames.get(type(arg), \"Byte\")\n    count = 0 if t == \"Byte\" else 1\n    if t == \"tuple\":\n      t = typeNames.get(type(arg[0]), \"Byte\")\n      count = len(arg)\n    if t == \"bool\":\n      t = \"UInt32\"\n    return (t, count)\n\n  def getConstraints(arg):\n    t = typeNames.get(type(arg), \"Byte\")\n    if t == \"Byte\":\n      return \"multiple\"\n    elif t == \"bool\":\n      return \"bool\"\n    else:\n      return \"\"\n\n  # Get arguments from pooler constructors, figure out types of\n  # variables and populate poolerSpec.\n  pArgTuples = _buildArgs(poolerClass)\n  poolerSpec = {}\n  for argTuple in pArgTuples:\n    d = dict(\n      description=argTuple[1],\n      accessMode=\"ReadWrite\",\n      dataType=getArgType(argTuple[2])[0],\n      count=getArgType(argTuple[2])[1],\n      constraints=getConstraints(argTuple[2]))\n    poolerSpec[argTuple[0]] = d\n\n  # Get arguments from spatial pooler constructors, figure out types of\n  # variables and populate poolerSpec.\n  # This allows setting SP parameters\n  pArgTuples = _buildArgs(_getParentSpatialPoolerClass(poolerType))\n  for argTuple in pArgTuples:\n    d = dict(\n      description=argTuple[1],\n      accessMode=\"ReadWrite\",\n      dataType=getArgType(argTuple[2])[0],\n      count=getArgType(argTuple[2])[1],\n      constraints=getConstraints(argTuple[2]))\n    poolerSpec[argTuple[0]] = d\n\n  # Add special parameters that weren't handled automatically\n  # Pooler parameters only\n  poolerSpec.update(dict(\n    columnCount=dict(\n      description=\"Total number of columns (coincidences).\",\n      accessMode=\"ReadWrite\",\n      dataType=\"UInt32\",\n      count=1,\n      constraints=\"\"),\n\n    inputWidth=dict(\n      description=\"Size of inputs to the UP.\",\n      accessMode=\"ReadWrite\",\n      dataType=\"UInt32\",\n      count=1,\n      constraints=\"\"),\n\n    historyLength=dict(\n      description=\"The union window length\",\n      accessMode=\"ReadWrite\",\n      dataType=\"UInt32\",\n      count=1,\n      constraints=\"\"),\n\n    minHistory=dict(\n      description=\"don't perform union (output all zeros) until buffer\"\n                  \" length >= minHistory\",\n      accessMode=\"ReadWrite\",\n      dataType=\"UInt32\",\n      count=1,\n      constraints=\"\"),\n\n     poolerType=dict(\n      description=\"Type of pooler to use: union\",\n      accessMode=\"ReadWrite\",\n      dataType=\"Byte\",\n      count=0,\n      constraints=\"enum: union\"),\n     ))\n\n\n  # The last group is for parameters that aren't specific to pooler\n  otherSpec = dict(\n    learningMode=dict(\n      description=\"1 if the node is learning (default 1).\",\n      accessMode=\"ReadWrite\",\n      dataType=\"UInt32\",\n      count=1,\n      constraints=\"bool\"),\n\n    inferenceMode=dict(\n      description=\"1 if the node outputs current inference (default 1).\",\n      accessMode=\"ReadWrite\",\n      dataType=\"UInt32\",\n      count=1,\n      constraints=\"bool\"),\n\n\n\n  )\n\n  return poolerSpec, otherSpec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the pooler object", "response": "def initialize(self):\n    \"\"\"\n    Initialize the self._poolerClass\n    \"\"\"\n\n    # Retrieve the necessary extra arguments that were handled automatically\n    autoArgs = {name: getattr(self, name) for name in self._poolerArgNames}\n    autoArgs[\"inputDimensions\"] = [self._inputWidth]\n    autoArgs[\"columnDimensions\"] = [self._columnCount]\n    autoArgs[\"potentialRadius\"] = self._inputWidth\n    autoArgs[\"historyLength\"] = self._historyLength\n    autoArgs[\"minHistory\"] = self._minHistory\n\n    # Allocate the pooler\n    self._pooler = self._poolerClass(**autoArgs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute(self, inputs, outputs):\n\n    resetSignal = False\n    if 'resetIn' in inputs:\n      if len(inputs['resetIn']) != 1:\n        raise Exception(\"resetIn has invalid length\")\n\n      if inputs['resetIn'][0] != 0:\n        resetSignal = True\n\n    outputs[\"mostActiveCells\"][:] = numpy.zeros(\n                                      self._columnCount, dtype=GetNTAReal())\n\n    if self._poolerType == \"simpleUnion\":\n      self._pooler.unionIntoArray(inputs[\"activeCells\"],\n                                  outputs[\"mostActiveCells\"],\n                                  forceOutput = resetSignal)\n    else:\n      predictedActiveCells = inputs[\"predictedActiveCells\"] if (\n        \"predictedActiveCells\" in inputs) else numpy.zeros(self._inputWidth,\n                                                           dtype=uintDType)\n\n      mostActiveCellsIndices = self._pooler.compute(inputs[\"activeCells\"],\n                                                    predictedActiveCells,\n                                                    self.learningMode)\n\n      outputs[\"mostActiveCells\"][mostActiveCellsIndices] = 1\n\n    if resetSignal:\n        self.reset()", "response": "Run one iteration of TemporalPoolerRegion s compute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the base Spec for the TemporalPoolerRegion.", "response": "def getBaseSpec(cls):\n    \"\"\"Return the base Spec for TemporalPoolerRegion.\n\n    Doesn't include the pooler parameters\n    \"\"\"\n    spec = dict(\n      description=TemporalPoolerRegion.__doc__,\n      singleNodeOnly=True,\n      inputs=dict(\n        activeCells=dict(\n          description=\"Active cells\",\n          dataType=\"Real32\",\n          count=0,\n          required=True,\n          regionLevel=False,\n          isDefaultInput=True,\n          requireSplitterMap=False),\n\n        predictedActiveCells=dict(\n          description=\"Predicted Active Cells\",\n          dataType=\"Real32\",\n          count=0,\n          required=True,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        resetIn=dict(\n          description=\"\"\"A boolean flag that indicates whether\n                         or not the input vector received in this compute cycle\n                         represents the start of a new temporal sequence.\"\"\",\n          dataType='Real32',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        sequenceIdIn=dict(\n          description=\"Sequence ID\",\n          dataType='UInt64',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n      ),\n      outputs=dict(\n        mostActiveCells=dict(\n          description=\"Most active cells in the pooler SDR having non-zero activation\",\n          dataType=\"Real32\",\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=True),\n      ),\n\n      parameters=dict(),\n    )\n\n    return spec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the Spec for TemporalPoolerRegion.", "response": "def getSpec(cls):\n    \"\"\"\n    Return the Spec for TemporalPoolerRegion.\n\n    The parameters collection is constructed based on the parameters specified\n    by the various components (poolerSpec and otherSpec)\n    \"\"\"\n    spec = cls.getBaseSpec()\n    p, o = _getAdditionalSpecs()\n    spec[\"parameters\"].update(p)\n    spec[\"parameters\"].update(o)\n\n    return spec"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value of a Spec parameter.", "response": "def setParameter(self, parameterName, index, parameterValue):\n    \"\"\"\n    Set the value of a Spec parameter. Most parameters are handled\n    automatically by PyRegion's parameter set mechanism. The ones that need\n    special treatment are explicitly handled here.\n    \"\"\"\n    if parameterName in self._poolerArgNames:\n      setattr(self._pooler, parameterName, parameterValue)\n    elif hasattr(self, parameterName):\n      setattr(self, parameterName, parameterValue)\n    else:\n      raise Exception(\"Unknown parameter: \" + parameterName)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexperimenting 1 - Calculate error rate as a function of training sequence numbers \u00b0C", "response": "def runExperiment():\n  \"\"\"\n  Experiment 1: Calculate error rate as a function of training sequence numbers\n  :return:\n  \"\"\"\n  trainSeqN = [5, 10, 20, 50, 100, 200]\n  rptPerCondition = 20\n  correctRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  missRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  fpRateAll = np.zeros((len(trainSeqN), rptPerCondition))\n  for i in xrange(len(trainSeqN)):\n    for rpt in xrange(rptPerCondition):\n      numTrainSequence = trainSeqN[i]\n      correctRate, missRate, fpRate = runSingleExperiment(numTrainSequence=numTrainSequence)\n      correctRateAll[i, rpt] = correctRate\n      missRateAll[i, rpt] = missRate\n      fpRateAll[i, rpt] = fpRate\n\n  plt.figure()\n  plt.subplot(2,2,1)\n  plt.semilogx(trainSeqN, 100*np.mean(correctRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' Hit Rate - Best Match (%)')\n  plt.subplot(2,2,2)\n  plt.semilogx(trainSeqN, 100*np.mean(missRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' Miss Rate (%)')\n  plt.subplot(2,2,3)\n  plt.semilogx(trainSeqN, 100*np.mean(fpRateAll,1),'-*')\n  plt.xlabel(' Training Sequence Number')\n  plt.ylabel(' False Positive Rate (%)')\n  plt.savefig('result/ReberSequence_HMMperformance.pdf')\n  plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef computeCapacity(results, threshold):\n  closestBelow = None\n  closestAbove = None\n  for numObjects, accuracy in sorted(results):\n    if accuracy >= threshold:\n      if closestAbove is None or closestAbove[0] < numObjects:\n        closestAbove = (numObjects, accuracy)\n        closestBelow = None\n    else:\n      if closestBelow is None:\n        closestBelow = (numObjects, accuracy)\n  if closestBelow is None or closestAbove is None:\n    print closestBelow, threshold, closestAbove\n    raise ValueError(\n        \"Results must include a value above and below threshold of {}\".format(threshold))\n\n  print \"  Capacity threshold is between {} and {}\".format(closestAbove[0], closestBelow[0])\n\n  return closestAbove[0]", "response": "Computes the capacity of the objects in the results list that are above threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndebugs a thing experiment given a logFile", "response": "def debugExperiment(logFile):\n  \"\"\"\n  Debug a thing experiment given a logFile\n  \"\"\"\n\n  exp = rerunExperimentFromLogfile(logFile)\n  exp.logCalls = False\n\n  L2Representations = exp.objectL2Representations\n  print \"Learned object representations:\"\n  pprint.pprint(L2Representations, width=400)\n  print \"==========================\"\n\n  print \"\\nRun inference with a point on the capsule\"\n  sensationList = [\n    {0: getObjectPair(\"Capsule\", 0)},\n  ]\n  exp.infer(sensationList, reset= False)\n  print \"Output for capsule:\", exp.getL2Representations()\n  print \"Intersection with sphere:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Sphere\"][0])\n  print \"Intersection with capsule:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Capsule\"][0])\n  print \"Intersection with cube:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Cube\"][0])\n  exp.sendReset()\n\n  print \"\\nRun inference with a point on the sphere\"\n  sensationList = [\n    {0: getObjectPair(\"Sphere\", 0)},\n  ]\n  exp.infer(sensationList, reset= False)\n  print \"Output for sphere:\", exp.getL2Representations()\n  print \"Intersection with sphere:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Sphere\"][0])\n  print \"Intersection with Capsule:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Capsule\"][0])\n  print \"Intersection with cube:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Cube\"][0])\n  exp.sendReset()\n\n  print \"\\nRun inference with two points on the sphere\"\n  sensationList = [\n    {0: getObjectPair(\"Sphere\", 0)},\n    {0: getObjectPair(\"Sphere\", 2)},\n  ]\n  exp.infer(sensationList, reset= False)\n  print \"Output for sphere:\", exp.getL2Representations()\n  print \"Intersection with sphere:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Sphere\"][0])\n  print \"Intersection with Capsule:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Capsule\"][0])\n  print \"Intersection with cube:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Cube\"][0])\n  exp.sendReset()\n\n\n  print \"\\nRun inference with a point on the cube\"\n  sensationList = [\n    {0: getObjectPair(\"Cube\", 2)},\n  ]\n  exp.infer(sensationList, reset= False)\n  print \"Output for cube:\", exp.getL2Representations()\n  print \"Intersection with sphere:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Sphere\"][0])\n  print \"Intersection with Capsule:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Capsule\"][0])\n  print \"Intersection with cube:\", len(\n    exp.getL2Representations()[0] & L2Representations[\"Cube\"][0])\n  exp.sendReset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset(self):\n\n    self.activeCells = np.empty(0, dtype=\"uint32\")\n    self.activeDeltaSegments = np.empty(0, dtype=\"uint32\")\n    self.activeFeatureLocationSegments = np.empty(0, dtype=\"uint32\")", "response": "Reset all cells and delta segments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute(self, deltaLocation=(), newLocation=(),\n              featureLocationInput=(), featureLocationGrowthCandidates=(),\n              learn=True):\n    \"\"\"\n    Run one time step of the Location Memory algorithm.\n\n    @param deltaLocation (sorted numpy array)\n    @param newLocation (sorted numpy array)\n    @param featureLocationInput (sorted numpy array)\n    @param featureLocationGrowthCandidates (sorted numpy array)\n    \"\"\"\n    prevActiveCells = self.activeCells\n\n    self.activeDeltaSegments = np.where(\n      (self.internalConnections.computeActivity(\n        prevActiveCells, self.connectedPermanence\n      ) >= self.activationThreshold)\n      &\n      (self.deltaConnections.computeActivity(\n        deltaLocation, self.connectedPermanence\n      ) >= self.activationThreshold))[0]\n\n    # When we're moving, the feature-location input has no effect.\n    if len(deltaLocation) == 0:\n      self.activeFeatureLocationSegments = np.where(\n        self.featureLocationConnections.computeActivity(\n          featureLocationInput, self.connectedPermanence\n        ) >= self.activationThreshold)[0]\n    else:\n      self.activeFeatureLocationSegments = np.empty(0, dtype=\"uint32\")\n\n\n    if len(newLocation) > 0:\n      # Drive activations by relaying this location SDR.\n      self.activeCells = newLocation\n\n      if learn:\n        # Learn the delta.\n        self._learnTransition(prevActiveCells, deltaLocation, newLocation)\n\n        # Learn the featureLocationInput.\n        self._learnFeatureLocationPair(newLocation, featureLocationInput,\n                                       featureLocationGrowthCandidates)\n\n\n    elif len(prevActiveCells) > 0:\n      if len(deltaLocation) > 0:\n        # Drive activations by applying the deltaLocation to the current location.\n        # Completely ignore the featureLocationInput. It's outdated, associated\n        # with the previous location.\n\n        cellsForDeltaSegments = self.internalConnections.mapSegmentsToCells(\n          self.activeDeltaSegments)\n\n        self.activeCells = np.unique(cellsForDeltaSegments)\n      else:\n        # Keep previous active cells active.\n        # Modulate with the featureLocationInput.\n\n        if len(self.activeFeatureLocationSegments) > 0:\n\n          cellsForFeatureLocationSegments = (\n            self.featureLocationConnections.mapSegmentsToCells(\n              self.activeFeatureLocationSegments))\n          self.activeCells = np.intersect1d(prevActiveCells,\n                                            cellsForFeatureLocationSegments)\n        else:\n          self.activeCells = prevActiveCells\n\n    elif len(featureLocationInput) > 0:\n      # Drive activations with the featureLocationInput.\n\n      cellsForFeatureLocationSegments = (\n        self.featureLocationConnections.mapSegmentsToCells(\n          self.activeFeatureLocationSegments))\n\n      self.activeCells = np.unique(cellsForFeatureLocationSegments)", "response": "Compute the Location Memory algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _learnTransition(self, prevActiveCells, deltaLocation, newLocation):\n\n    prevLocationPotentialOverlaps = self.internalConnections.computeActivity(\n      prevActiveCells)\n    deltaPotentialOverlaps = self.deltaConnections.computeActivity(\n      deltaLocation)\n\n    matchingDeltaSegments = np.where(\n      (prevLocationPotentialOverlaps >= self.learningThreshold) &\n      (deltaPotentialOverlaps >= self.learningThreshold))[0]\n\n    # Cells with a active segment pair: reinforce the segment\n    cellsForActiveSegments = self.internalConnections.mapSegmentsToCells(\n      self.activeDeltaSegments)\n    learningActiveDeltaSegments = self.activeDeltaSegments[\n      np.in1d(cellsForActiveSegments, newLocation)]\n    remainingCells = np.setdiff1d(newLocation, cellsForActiveSegments)\n\n    # Remaining cells with a matching segment pair: reinforce the best matching\n    # segment pair.\n    candidateSegments = self.internalConnections.filterSegmentsByCell(\n      matchingDeltaSegments, remainingCells)\n    cellsForCandidateSegments = self.internalConnections.mapSegmentsToCells(\n      candidateSegments)\n    candidateSegments = matchingDeltaSegments[\n      np.in1d(cellsForCandidateSegments, remainingCells)]\n    onePerCellFilter = np2.argmaxMulti(\n      prevLocationPotentialOverlaps[candidateSegments] +\n      deltaPotentialOverlaps[candidateSegments],\n      cellsForCandidateSegments)\n    learningMatchingDeltaSegments = candidateSegments[onePerCellFilter]\n\n    newDeltaSegmentCells = np.setdiff1d(remainingCells, cellsForCandidateSegments)\n\n    for learningSegments in (learningActiveDeltaSegments,\n                             learningMatchingDeltaSegments):\n      self._learn(self.internalConnections, self.rng, learningSegments,\n                  prevActiveCells, prevActiveCells,\n                  prevLocationPotentialOverlaps,\n                  self.initialPermanence, self.sampleSize,\n                  self.permanenceIncrement, self.permanenceDecrement,\n                  self.maxSynapsesPerSegment)\n      self._learn(self.deltaConnections, self.rng, learningSegments,\n                  deltaLocation, deltaLocation, deltaPotentialOverlaps,\n                  self.initialPermanence, self.sampleSize,\n                  self.permanenceIncrement, self.permanenceDecrement,\n                  self.maxSynapsesPerSegment)\n\n    numNewLocationSynapses = len(prevActiveCells)\n    numNewDeltaSynapses = len(deltaLocation)\n\n    if self.sampleSize != -1:\n      numNewLocationSynapses = min(numNewLocationSynapses, self.sampleSize)\n      numNewDeltaSynapses = min(numNewDeltaSynapses, self.sampleSize)\n\n    if self.maxSynapsesPerSegment != -1:\n      numNewLocationSynapses = min(numNewLocationSynapses,\n                                   self.maxSynapsesPerSegment)\n      numNewDeltaSynapses = min(numNewLocationSynapses,\n                                self.maxSynapsesPerSegment)\n\n    newPrevLocationSegments = self.internalConnections.createSegments(\n      newDeltaSegmentCells)\n    newDeltaSegments = self.deltaConnections.createSegments(\n      newDeltaSegmentCells)\n\n    assert np.array_equal(newPrevLocationSegments, newDeltaSegments)\n\n    self.internalConnections.growSynapsesToSample(\n      newPrevLocationSegments, prevActiveCells, numNewLocationSynapses,\n      self.initialPermanence, self.rng)\n    self.deltaConnections.growSynapsesToSample(\n      newDeltaSegments, deltaLocation, numNewDeltaSynapses,\n      self.initialPermanence, self.rng)", "response": "Learn the transition of the newLocation SDR."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlearn a new location for a given feature location.", "response": "def _learnFeatureLocationPair(self, newLocation, featureLocationInput,\n                                featureLocationGrowthCandidates):\n    \"\"\"\n    Grow / reinforce synapses between the location layer's dendrites and the\n    input layer's active cells.\n    \"\"\"\n\n    potentialOverlaps = self.featureLocationConnections.computeActivity(\n      featureLocationInput)\n    matchingSegments = np.where(potentialOverlaps > self.learningThreshold)[0]\n\n    # Cells with a active segment pair: reinforce the segment\n    cellsForActiveSegments = self.featureLocationConnections.mapSegmentsToCells(\n      self.activeFeatureLocationSegments)\n    learningActiveSegments = self.activeFeatureLocationSegments[\n      np.in1d(cellsForActiveSegments, newLocation)]\n    remainingCells = np.setdiff1d(newLocation, cellsForActiveSegments)\n\n    # Remaining cells with a matching segment pair: reinforce the best matching\n    # segment pair.\n    candidateSegments = self.featureLocationConnections.filterSegmentsByCell(\n      matchingSegments, remainingCells)\n    cellsForCandidateSegments = (\n      self.featureLocationConnections.mapSegmentsToCells(\n        candidateSegments))\n    candidateSegments = candidateSegments[\n      np.in1d(cellsForCandidateSegments, remainingCells)]\n    onePerCellFilter = np2.argmaxMulti(potentialOverlaps[candidateSegments],\n                                       cellsForCandidateSegments)\n    learningMatchingSegments = candidateSegments[onePerCellFilter]\n\n    newSegmentCells = np.setdiff1d(remainingCells, cellsForCandidateSegments)\n\n    for learningSegments in (learningActiveSegments,\n                             learningMatchingSegments):\n      self._learn(self.featureLocationConnections, self.rng, learningSegments,\n                  featureLocationInput, featureLocationGrowthCandidates,\n                  potentialOverlaps,\n                  self.initialPermanence, self.sampleSize,\n                  self.permanenceIncrement, self.permanenceDecrement,\n                  self.maxSynapsesPerSegment)\n\n    numNewSynapses = len(featureLocationInput)\n\n    if self.sampleSize != -1:\n      numNewSynapses = min(numNewSynapses, self.sampleSize)\n\n    if self.maxSynapsesPerSegment != -1:\n      numNewSynapses = min(numNewSynapses, self.maxSynapsesPerSegment)\n\n    newSegments = self.featureLocationConnections.createSegments(\n      newSegmentCells)\n\n    self.featureLocationConnections.growSynapsesToSample(\n      newSegments, featureLocationGrowthCandidates, numNewSynapses,\n      self.initialPermanence, self.rng)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef train(self, params):\n    # self.net = buildNetwork(params['encoding_num'] * params['num_lags'],\n    #                         params['num_cells'],\n    #                         params['encoding_num'],\n    #                         bias=True,\n    #                         outputbias=True)\n\n    ds = SupervisedDataSet(params['encoding_num'] * params['num_lags'],\n                           params['encoding_num'])\n    history = self.window(self.history, params['learning_window'])\n\n    n = params['encoding_num']\n    for i in xrange(params['num_lags'], len(history)):\n      targets = numpy.zeros((1, n))\n      targets[0, :] = self.encoder.encode(history[i])\n\n      features = numpy.zeros((1, n * params['num_lags']))\n      for lags in xrange(params['num_lags']):\n        features[0, lags * n:(lags + 1) * n] = self.encoder.encode(\n          history[i - (lags + 1)])\n      ds.addSample(features, targets)\n\n    trainer = BackpropTrainer(self.net,\n                              dataset=ds,\n                              verbose=params['verbosity'] > 0)\n\n    if len(history) > 1:\n      trainer.trainEpochs(params['num_epochs'])", "response": "Train TDNN network on buffered dataset history\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getSpec(cls):\n    spec = dict(\n      description=GridCellLocationRegion.__doc__,\n      singleNodeOnly=True,\n      inputs=dict(\n        anchorInput=dict(\n          description=\"An array of 0's and 1's representing the sensory input \"\n                      \"during inference. This will often come from a \"\n                      \"feature-location pair layer (L4 active cells).\",\n          dataType=\"Real32\",\n          count=0,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False,\n        ),\n        anchorGrowthCandidates=dict(\n          description=\"An array of 0's and 1's representing the sensory input \"\n                      \"during learning. This will often come from a \"\n                      \"feature-location pair layer (L4 winner cells).\",\n          dataType=\"Real32\",\n          count=0,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False,\n        ),\n        displacement=dict(\n          description=\"An array of floats representing the displacement as a \"\n                      \"multi dimensional translation vector [d1, d2, ..., dk].\",\n          dataType=\"Real32\",\n          count=0,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False,\n        ),\n        resetIn=dict(\n          description=\"Clear all cell activity.\",\n          dataType=\"Real32\",\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False,\n        )\n      ),\n      outputs=dict(\n        activeCells=dict(\n          description=\"A binary output containing a 1 for every\"\n                      \" cell that is currently active.\",\n          dataType=\"Real32\",\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False\n        ),\n        learnableCells=dict(\n          description=\"A binary output containing a 1 for every\"\n                      \" cell that is currently learnable\",\n          dataType=\"Real32\",\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False\n        ),\n        sensoryAssociatedCells=dict(\n          description=\"A binary output containing a 1 for every\"\n                      \" cell that is currently associated with a sensory input\",\n          dataType=\"Real32\",\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False\n        )\n      ),\n      parameters=dict(\n        moduleCount=dict(\n          description=\"Number of grid cell modules\",\n          dataType=\"UInt32\",\n          accessMode=\"Read\",\n          count=1\n        ),\n        cellsPerAxis=dict(\n          description=\"Determines the number of cells. Determines how space is \"\n                      \"divided between the cells\",\n          dataType=\"UInt32\",\n          accessMode=\"Read\",\n          count=1\n        ),\n        scale=dict(\n          description=\"Determines the amount of world space covered by all of \"\n                      \"the cells combined. In grid cell terminology, this is \"\n                      \"equivalent to the 'scale' of a module. One scale value \"\n                      \"for each grid cell module. Array size must match \"\n                      \"'moduleCount' parameter\",\n          dataType=\"Real32\",\n          accessMode=\"Read\",\n          count=0,\n        ),\n        orientation=dict(\n          description=\"The rotation of this map, measured in radians. One \"\n                      \"orientation value for each grid cell module. Array size \"\n                      \"must match 'moduleCount' parameter\",\n          dataType=\"Real32\",\n          accessMode=\"Read\",\n          count=0,\n        ),\n        anchorInputSize=dict(\n          description=\"The number of input bits in the anchor input\",\n          dataType=\"UInt32\",\n          accessMode=\"Read\",\n          count=1,\n        ),\n        activeFiringRate=dict(\n          description=\"Between 0.0 and 1.0. A cell is considered active if its \"\n                      \"firing rate is at least this value\",\n          dataType=\"Real32\",\n          accessMode=\"Read\",\n          count=1,\n        ),\n        bumpSigma=dict(\n          description=\"Specifies the diameter of a gaussian bump, in units of \"\n                      \"'rhombus edges'. A single edge of the rhombus has length \"\n                      \"1, and this bumpSigma would typically be less than 1. We \"\n                      \"often use 0.18172 as an estimate for the sigma of a rat \"\n                      \"entorhinal bump\",\n          dataType=\"Real32\",\n          accessMode=\"Read\",\n          count=1,\n          defaultValue=0.18172,\n        ),\n        activationThreshold=dict(\n          description=\"If the number of active connected synapses on a \"\n                      \"segment is at least this threshold, the segment \"\n                      \"is said to be active\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          constraints=\"\",\n          defaultValue=10\n        ),\n        initialPermanence=dict(\n          description=\"Initial permanence of a new synapse\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\",\n          defaultValue=0.21\n        ),\n        connectedPermanence=dict(\n          description=\"If the permanence value for a synapse is greater \"\n                      \"than this value, it is said to be connected\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          constraints=\"\",\n          defaultValue=0.50\n        ),\n        learningThreshold=dict(\n          description=\"Minimum overlap required for a segment to learned\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          defaultValue=10\n        ),\n        sampleSize=dict(\n          description=\"The desired number of active synapses for an \"\n                      \"active cell\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          defaultValue=20\n        ),\n        permanenceIncrement=dict(\n          description=\"Amount by which permanences of synapses are \"\n                      \"incremented during learning\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          defaultValue=0.1\n        ),\n        permanenceDecrement=dict(\n          description=\"Amount by which permanences of synapses are \"\n                      \"decremented during learning\",\n          accessMode=\"Read\",\n          dataType=\"Real32\",\n          count=1,\n          defaultValue=0.0\n        ),\n        maxSynapsesPerSegment=dict(\n          description=\"The maximum number of synapses per segment\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          defaultValue=-1\n        ),\n        bumpOverlapMethod=dict(\n          description=\"Specifies the firing rate of a cell when it's part of \"\n                      \"two bumps. ('probabilistic' or 'sum')\",\n          dataType=\"Byte\",\n          accessMode=\"Read\",\n          constraints=(\"enum: probabilistic, sum\"),\n          defaultValue=\"probabilistic\",\n          count=0,\n        ),\n        learningMode=dict(\n          description=\"A boolean flag that indicates whether or not we should \"\n                      \"learn by associating the location with the sensory \"\n                      \"input\",\n          dataType=\"Bool\",\n          accessMode=\"ReadWrite\",\n          count=1,\n          defaultValue=False\n        ),\n        dualPhase=dict(\n          description=\"A boolean flag that indicates whether or not we should \"\n                      \"process movement and sensation using two phases of the \"\n                      \"same network. When this flag is enabled, the compute \"\n                      \"method will alternate between movement and sensation on \"\n                      \"each phase. When this flag is disabled both movement and \"\n                      \"sensations will be processed on a single phase.\",\n          dataType=\"Bool\",\n          accessMode=\"ReadWrite\",\n          count=1,\n          defaultValue=True\n        ),\n        dimensions=dict(\n          description=\"The number of dimensions represented in the displacement\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          defaultValue=2\n        ),\n        seed=dict(\n          description=\"Seed for the random number generator\",\n          accessMode=\"Read\",\n          dataType=\"UInt32\",\n          count=1,\n          defaultValue=42\n        )\n      ),\n      commands=dict(\n        reset=dict(description=\"Clear all cell activity\"),\n        activateRandomLocation=dict(description=\"Set the location to a random \"\n                                                \"point\"),\n      )\n    )\n    return spec", "response": "Returns the spec for the grid cell location region."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self):\n\n    if self._modules is None:\n      self._modules = []\n      for i in xrange(self.moduleCount):\n        self._modules.append(ThresholdedGaussian2DLocationModule(\n          cellsPerAxis=self.cellsPerAxis,\n          scale=self.scale[i],\n          orientation=self.orientation[i],\n          anchorInputSize=self.anchorInputSize,\n          activeFiringRate=self.activeFiringRate,\n          bumpSigma=self.bumpSigma,\n          activationThreshold=self.activationThreshold,\n          initialPermanence=self.initialPermanence,\n          connectedPermanence=self.connectedPermanence,\n          learningThreshold=self.learningThreshold,\n          sampleSize=self.sampleSize,\n          permanenceIncrement=self.permanenceIncrement,\n          permanenceDecrement=self.permanenceDecrement,\n          maxSynapsesPerSegment=self.maxSynapsesPerSegment,\n          bumpOverlapMethod=self.bumpOverlapMethod,\n          seed=self.seed))\n\n      # Create a projection matrix for each module used to convert higher\n      # dimension displacements to 2D\n      if self.dimensions > 2:\n        self._projection = [\n          self.createProjectionMatrix(dimensions=self.dimensions)\n            for _ in xrange(self.moduleCount)]", "response": "Initialize the grid cell modules."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute(self, inputs, outputs):\n\n    if inputs.get(\"resetIn\", False):\n      self.reset()\n      if self.learningMode:\n        # Initialize to random location after reset when learning\n        self.activateRandomLocation()\n\n      # send empty output\n      outputs[\"activeCells\"][:] = 0\n      outputs[\"learnableCells\"][:] = 0\n      outputs[\"sensoryAssociatedCells\"][:] = 0\n      return\n\n    displacement = inputs.get(\"displacement\", np.array([]))\n    anchorInput = inputs.get(\"anchorInput\", np.array([])).nonzero()[0]\n    anchorGrowthCandidates = inputs.get(\"anchorGrowthCandidates\", np.array([])).nonzero()[0]\n\n    # Concatenate the output of all modules\n    activeCells = np.array([], dtype=np.uint32)\n    learnableCells = np.array([], dtype=np.uint32)\n    sensoryAssociatedCells = np.array([], dtype=np.uint32)\n\n    # Only process input when data is available\n    shouldMove = displacement.any()\n    shouldSense = anchorInput.any() or anchorGrowthCandidates.any()\n\n    if shouldMove and len(displacement) != self.dimensions:\n      raise TypeError(\"displacement must have {} dimensions\".format(self.dimensions))\n\n    # Handles dual phase movement/sensation processing\n    if self.dualPhase:\n      if self._sensing:\n        shouldMove = False\n      else:\n        shouldSense = False\n\n      # Toggle between movement and sensation\n      self._sensing = not self._sensing\n\n    for i in xrange(self.moduleCount):\n      module = self._modules[i]\n\n      # Compute movement\n      if shouldMove:\n        movement = displacement\n        if self.dimensions > 2:\n          # Project n-dimension displacements to 2D\n          movement = np.matmul(self._projection[i], movement)\n\n        module.movementCompute(movement)\n\n      # Compute sensation\n      if shouldSense:\n        module.sensoryCompute(anchorInput, anchorGrowthCandidates,\n                              self.learningMode)\n\n      # Concatenate outputs\n      start = i * self.cellCount\n      activeCells = np.append(activeCells,\n                              module.getActiveCells() + start)\n      learnableCells = np.append(learnableCells,\n                                 module.getLearnableCells() + start)\n      sensoryAssociatedCells = np.append(sensoryAssociatedCells,\n                                         module.getSensoryAssociatedCells() + start)\n\n    outputs[\"activeCells\"][:] = 0\n    outputs[\"activeCells\"][activeCells] = 1\n    outputs[\"learnableCells\"][:] = 0\n    outputs[\"learnableCells\"][learnableCells] = 1\n    outputs[\"sensoryAssociatedCells\"][:] = 0\n    outputs[\"sensoryAssociatedCells\"][sensoryAssociatedCells] = 1", "response": "Compute the location based on the displacement and anchorInput."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setParameter(self, parameterName, index, parameterValue):\n    spec = self.getSpec()\n    if parameterName not in spec['parameters']:\n      raise Exception(\"Unknown parameter: \" + parameterName)\n\n    setattr(self, parameterName, parameterValue)", "response": "Set the value of a Spec parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the size of the output array", "response": "def getOutputElementCount(self, name):\n    \"\"\"\n    Returns the size of the output array\n    \"\"\"\n    if name in [\"activeCells\", \"learnableCells\", \"sensoryAssociatedCells\"]:\n      return self.cellCount * self.moduleCount\n    else:\n      raise Exception(\"Invalid output name specified: \" + name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a 2D projection matrix for the n - dimensional displacement of a location module.", "response": "def createProjectionMatrix(self, dimensions=3):\n    \"\"\"\n    Compute  projection matrix used to convert n-dimensional displacement into\n    2D displacement compatible with :class:`ThresholdedGaussian2DLocationModule`\n    algorithm. To compute the 2D displacement each module must multiply the\n    n-dimensional displacement with the projection matrix for that module.\n\n    :param dimensions: Number of dimensions. Must be greater than 2. Default 3.\n    :type int dimensions:\n\n    :return: The projection matrix\n    :rtype: array(2, n)\n    \"\"\"\n    if dimensions < 3:\n      raise ValueError(\"dimensions value must be 3 or higher\")\n\n    b1 = np.random.multivariate_normal(mean=np.zeros(dimensions),\n                                       cov=np.eye(dimensions))\n    b1 /= np.linalg.norm(b1)\n\n    # Choose a random vector orthogonal to b1\n    while True:\n      randomVector = np.random.multivariate_normal(mean=np.zeros(dimensions),\n                                                   cov=np.eye(dimensions))\n      randomVector /= np.linalg.norm(randomVector)\n      projectedToPlane = randomVector - np.dot(randomVector, b1) * b1\n\n      # make sure random vector is not parallel to b1 plane\n      length = np.linalg.norm(projectedToPlane)\n      if length == 0:\n        continue\n\n      b2 = projectedToPlane / length\n\n      # b1 and b2 are two orthogonal vectors on the plane.\n      # To get a 2D displacement, you'll dot the n-dimensional displacement with\n      # each of these vectors.\n      return np.array([b1, b2])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef updatePotentialRadius(sp, newPotentialRadius):\n  oldPotentialRadius = sp._potentialRadius\n  sp._potentialRadius = newPotentialRadius\n  numColumns = np.prod(sp.getColumnDimensions())\n  for columnIndex in xrange(numColumns):\n    potential = sp._mapPotential(columnIndex)\n    sp._potentialPools.replace(columnIndex, potential.nonzero()[0])\n\n  sp._updateInhibitionRadius()", "response": "Update the potential radius for all columns in the specified SFrame"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef computeRatModuleParametersFromCellCount(cellsPerAxis,\n                                            baselineCellsPerAxis=6):\n  \"\"\"\n  Compute 'cellsPerAxis', 'bumpSigma', and 'activeFiringRate' parameters for\n  :class:`ThresholdedGaussian2DLocationModule` given the number of cells per\n  axis. See :func:`createRatModuleFromCellCount`\n  \"\"\"\n  bumpSigma = RAT_BUMP_SIGMA * (baselineCellsPerAxis / float(cellsPerAxis))\n  activeFiringRate = ThresholdedGaussian2DLocationModule.chooseReliableActiveFiringRate(\n    cellsPerAxis, bumpSigma)\n\n  return {\n    \"cellsPerAxis\": cellsPerAxis,\n    \"bumpSigma\": bumpSigma,\n    \"activeFiringRate\": activeFiringRate\n  }", "response": "Compute the parameters for a single cell in a single order cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef computeRatModuleParametersFromReadoutResolution(inverseReadoutResolution,\n                                                    enlargeModuleFactor=1.):\n  \"\"\"\n  Compute 'cellsPerAxis', 'bumpSigma', and 'activeFiringRate' parameters for\n  :class:`ThresholdedGaussian2DLocationModule` given the\n  inverseReadoutResolution. See :func:`createRatModuleFromReadoutResolution`\n  \"\"\"\n  # Give the module enough precision in its learning so that the bump is the\n  # specified diameter when properly accounting for uncertainty.\n  cellsPerAxis = int(math.ceil(2*inverseReadoutResolution*enlargeModuleFactor))\n\n  bumpSigma = RAT_BUMP_SIGMA / enlargeModuleFactor\n\n  readoutResolution = 1. / (enlargeModuleFactor*inverseReadoutResolution)\n  activeFiringRate = ThresholdedGaussian2DLocationModule.chooseReliableActiveFiringRate(\n    cellsPerAxis, bumpSigma, readoutResolution)\n\n  return {\n    \"cellsPerAxis\": cellsPerAxis,\n    \"bumpSigma\": bumpSigma,\n    \"activeFiringRate\": activeFiringRate\n  }", "response": "Compute the parameters for a single resource in the hierarchy for a single resource in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new RatModule from the readout resolution.", "response": "def createRatModuleFromReadoutResolution(inverseReadoutResolution, scale,\n                                         enlargeModuleFactor=1.,\n                                         fixedScale=False, **kwargs):\n  \"\"\"\n  @param inverseReadoutResolution (int or float)\n  Equivalent to 1/readoutResolution, but specified this way as a convenience\n  (because it's easier and less ambiguous to type 3 than to type 0.3333333). The\n  readout resolution specifies the diameter of the circle of phases in the\n  rhombus encoded by a bump. So when a bump of activity is converted into a set\n  of active cells, this circle of active cells will have a diameter of at least\n  this amount.\n\n  @param enlargeModuleFactor (float)\n  A multiplicative factor that's used to simulate the effect of having a larger\n  module, keeping the bump size fixed but making the module larger, so that the\n  bump is smaller relative to the size of the module. Equivalently, this shrinks\n  the bump, increases the precision of the readout, adds more cells, and\n  increases the scale so that the bump is the same size when overlayed on the\n  real world.\n\n  @param fixedScale (bool)\n  By default, the enlargeModuleFactor will increase the scale, effectively\n  holding the bump size constant relative to physical space. Set this to True to\n  hold the scale constant, so enlarging the module causes the bump size to\n  shrink relative to physical space.\n  \"\"\"\n\n  params = computeRatModuleParametersFromReadoutResolution(inverseReadoutResolution,\n                                                           enlargeModuleFactor)\n  params.update(kwargs)\n  params[\"scale\"] = (scale if fixedScale else scale * enlargeModuleFactor)\n  return ThresholdedGaussian2DLocationModule(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef movementCompute(self, displacement, noiseFactor = 0, moduleNoiseFactor = 0):\n\n    if noiseFactor != 0:\n      xdisp = np.random.normal(0, noiseFactor)\n      ydisp = np.random.normal(0, noiseFactor)\n    else:\n      xdisp = 0\n      ydisp = 0\n\n    locationParams = {\n      \"displacement\": [displacement[\"top\"] + ydisp,\n                       displacement[\"left\"] + xdisp],\n      \"noiseFactor\": moduleNoiseFactor\n    }\n\n    for module in self.L6aModules:\n      module.movementCompute(**locationParams)\n\n    return locationParams", "response": "Compute the location parameters for the specified displacement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the sensory model of the given minicolumns.", "response": "def sensoryCompute(self, activeMinicolumns, learn):\n    \"\"\"\n    @param activeMinicolumns (numpy array)\n    List of indices of minicolumns to activate.\n\n    @param learn (bool)\n    If True, the two layers should learn this association.\n\n    @return (tuple of dicts)\n    Data for logging/tracing.\n    \"\"\"\n    inputParams = {\n      \"activeColumns\": activeMinicolumns,\n      \"basalInput\": self.getLocationRepresentation(),\n      \"basalGrowthCandidates\": self.getLearnableLocationRepresentation(),\n      \"learn\": learn\n    }\n    self.L4.compute(**inputParams)\n\n    locationParams = {\n      \"anchorInput\": self.L4.getActiveCells(),\n      \"anchorGrowthCandidates\": self.L4.getWinnerCells(),\n      \"learn\": learn,\n    }\n    for module in self.L6aModules:\n      module.sensoryCompute(**locationParams)\n\n    return (inputParams, locationParams)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset all cell activity.", "response": "def reset(self):\n    \"\"\"\n    Clear all cell activity.\n    \"\"\"\n    self.L4.reset()\n    for module in self.L6aModules:\n      module.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getLocationRepresentation(self):\n    activeCells = np.array([], dtype=\"uint32\")\n\n    totalPrevCells = 0\n    for module in self.L6aModules:\n      activeCells = np.append(activeCells,\n                              module.getActiveCells() + totalPrevCells)\n      totalPrevCells += module.numberOfCells()\n\n    return activeCells", "response": "Get the full population representation of the location layer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the cells in the location layer that should be associated with the sensory input layer representation.", "response": "def getLearnableLocationRepresentation(self):\n    \"\"\"\n    Get the cells in the location layer that should be associated with the\n    sensory input layer representation. In some models, this is identical to the\n    active cells. In others, it's a subset.\n    \"\"\"\n    learnableCells = np.array([], dtype=\"uint32\")\n\n    totalPrevCells = 0\n    for module in self.L6aModules:\n      learnableCells = np.append(learnableCells,\n                                 module.getLearnableCells() + totalPrevCells)\n      totalPrevCells += module.numberOfCells()\n\n    return learnableCells"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getSensoryAssociatedLocationRepresentation(self):\n    cells = np.array([], dtype=\"uint32\")\n\n    totalPrevCells = 0\n    for module in self.L6aModules:\n      cells = np.append(cells,\n                        module.sensoryAssociatedCells + totalPrevCells)\n      totalPrevCells += module.numberOfCells()\n\n    return cells", "response": "Returns the location cells that were driven by the location layer that were driven by this input."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntrains the network to recognize the specified object.", "response": "def learnObject(self,\n                  objectDescription,\n                  randomLocation=False,\n                  useNoise=False,\n                  noisyTrainingTime=1):\n    \"\"\"\n    Train the network to recognize the specified object. Move the sensor to one of\n    its features and activate a random location representation in the location\n    layer. Move the sensor over the object, updating the location representation\n    through path integration. At each point on the object, form reciprocal\n    connections between the represention of the location and the representation\n    of the sensory input.\n\n    @param objectDescription (dict)\n    For example:\n    {\"name\": \"Object 1\",\n     \"features\": [{\"top\": 0, \"left\": 0, \"width\": 10, \"height\": 10, \"name\": \"A\"},\n                  {\"top\": 0, \"left\": 10, \"width\": 10, \"height\": 10, \"name\": \"B\"}]}\n\n    @return locationsAreUnique (bool)\n    True if this object was assigned a unique set of locations. False if a\n    location on this object has the same location representation as another\n    location somewhere else.\n    \"\"\"\n    self.reset()\n    self.column.activateRandomLocation()\n\n    locationsAreUnique = True\n\n    if randomLocation or useNoise:\n      numIters = noisyTrainingTime\n    else:\n      numIters = 1\n    for i in xrange(numIters):\n      for iFeature, feature in enumerate(objectDescription[\"features\"]):\n        self._move(feature, randomLocation=randomLocation, useNoise=useNoise)\n        featureSDR = self.features[feature[\"name\"]]\n        self._sense(featureSDR, learn=True, waitForSettle=False)\n\n        locationRepresentation = self.column.getSensoryAssociatedLocationRepresentation()\n        self.locationRepresentations[(objectDescription[\"name\"],\n                                      iFeature)].append(locationRepresentation)\n        self.inputRepresentations[(objectDescription[\"name\"],\n                                   iFeature, feature[\"name\"])] = (\n                                     self.column.L4.getWinnerCells())\n\n        locationTuple = tuple(locationRepresentation)\n        locationsAreUnique = (locationsAreUnique and\n                              locationTuple not in self.representationSet)\n\n        self.representationSet.add(tuple(locationRepresentation))\n\n    self.learnedObjects.append(objectDescription)\n\n    return locationsAreUnique"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninfer the specified object with the network.", "response": "def inferObjectWithRandomMovements(self,\n                                     objectDescription,\n                                     numSensations=None,\n                                     randomLocation=False,\n                                     checkFalseConvergence=True):\n    \"\"\"\n    Attempt to recognize the specified object with the network. Randomly move\n    the sensor over the object until the object is recognized.\n\n    @param objectDescription (dict)\n    For example:\n    {\"name\": \"Object 1\",\n     \"features\": [{\"top\": 0, \"left\": 0, \"width\": 10, \"height\": 10, \"name\": \"A\"},\n                  {\"top\": 0, \"left\": 10, \"width\": 10, \"height\": 10, \"name\": \"B\"}]}\n\n    @param numSensations (int or None)\n    Set this to run the network for a fixed number of sensations. Otherwise this\n    method will run until the object is recognized or until maxTraversals is\n    reached.\n\n    @return (bool)\n    True if inference succeeded\n    \"\"\"\n    self.reset()\n\n    for monitor in self.monitors.values():\n      monitor.beforeInferObject(objectDescription)\n\n    currentStep = 0\n    finished = False\n    inferred = False\n    inferredStep = None\n    prevTouchSequence = None\n\n    for _ in xrange(self.maxTraversals):\n      # Choose touch sequence.\n      while True:\n        touchSequence = range(len(objectDescription[\"features\"]))\n        random.shuffle(touchSequence)\n\n        # Make sure the first touch will cause a movement.\n        if (prevTouchSequence is not None and\n            touchSequence[0] == prevTouchSequence[-1]):\n          continue\n\n        break\n\n      for iFeature in touchSequence:\n        currentStep += 1\n        feature = objectDescription[\"features\"][iFeature]\n        self._move(feature, randomLocation=randomLocation)\n\n        featureSDR = self.features[feature[\"name\"]]\n        self._sense(featureSDR, learn=False, waitForSettle=False)\n\n        if not inferred:\n          # Use the sensory-activated cells to detect whether the object has been\n          # recognized. In some models, this set of cells is equivalent to the\n          # active cells. In others, a set of cells around the sensory-activated\n          # cells become active. In either case, if these sensory-activated cells\n          # are correct, it implies that the input layer's representation is\n          # classifiable -- the location layer just correctly classified it.\n          representation = self.column.getSensoryAssociatedLocationRepresentation()\n          target_representations = set(np.concatenate(\n            self.locationRepresentations[\n              (objectDescription[\"name\"], iFeature)]))\n          inferred = (set(representation) <= target_representations)\n          if inferred:\n            inferredStep = currentStep\n\n          if not inferred and tuple(representation) in self.representationSet:\n            # We have converged to an incorrect representation - declare failure.\n            print(\"Converged to an incorrect representation!\")\n            return None\n\n        finished = ((inferred and numSensations is None) or\n                    (numSensations is not None and currentStep == numSensations))\n\n        if finished:\n          break\n\n      prevTouchSequence = touchSequence\n\n      if finished:\n        break\n\n    for monitor in self.monitors.values():\n      monitor.afterInferObject(objectDescription, inferredStep)\n\n    return inferredStep"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _move(self, feature, randomLocation = False, useNoise = True):\n\n    if randomLocation:\n      locationOnObject = {\n        \"top\": feature[\"top\"] + np.random.rand()*feature[\"height\"],\n        \"left\": feature[\"left\"] + np.random.rand()*feature[\"width\"],\n      }\n    else:\n      locationOnObject = {\n        \"top\": feature[\"top\"] + feature[\"height\"]/2.,\n        \"left\": feature[\"left\"] + feature[\"width\"]/2.\n      }\n\n    if self.locationOnObject is not None:\n      displacement = {\"top\": locationOnObject[\"top\"] -\n                             self.locationOnObject[\"top\"],\n                      \"left\": locationOnObject[\"left\"] -\n                              self.locationOnObject[\"left\"]}\n      if useNoise:\n        params = self.column.movementCompute(displacement,\n                                             self.noiseFactor,\n                                             self.moduleNoiseFactor)\n      else:\n        params = self.column.movementCompute(displacement, 0, 0)\n\n      for monitor in self.monitors.values():\n        monitor.afterLocationShift(**params)\n    else:\n      for monitor in self.monitors.values():\n        monitor.afterLocationInitialize()\n\n    self.locationOnObject = locationOnObject\n    for monitor in self.monitors.values():\n      monitor.afterLocationChanged(locationOnObject)", "response": "Move the sensor to the center of the specified feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends sensory input into the network. Optionally send it multiple times until the network settles.", "response": "def _sense(self, featureSDR, learn, waitForSettle):\n    \"\"\"\n    Send the sensory input into the network. Optionally, send it multiple times\n    until the network settles.\n    \"\"\"\n\n    for monitor in self.monitors.values():\n      monitor.beforeSense(featureSDR)\n\n    iteration = 0\n    prevCellActivity = None\n    while True:\n      (inputParams,\n       locationParams) = self.column.sensoryCompute(featureSDR, learn)\n\n      if waitForSettle:\n        cellActivity = (set(self.column.getSensoryRepresentation()),\n                        set(self.column.getLocationRepresentation()))\n        if cellActivity == prevCellActivity:\n          # It settled. Don't even log this timestep.\n          break\n\n        prevCellActivity = cellActivity\n\n      for monitor in self.monitors.values():\n        if iteration > 0:\n          monitor.beforeSensoryRepetition()\n        monitor.afterInputCompute(**inputParams)\n        monitor.afterLocationAnchor(**locationParams)\n\n      iteration += 1\n\n      if not waitForSettle or iteration >= self.maxSettlingTime:\n        break"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_kwinners(x, k, dutyCycles, boostStrength):\n\n  k = tf.convert_to_tensor(k, dtype=tf.int32)\n  boostStrength = tf.math.maximum(boostStrength, 0.0, name=\"boostStrength\")\n  targetDensity = tf.cast(k / x.shape[1], tf.float32, name=\"targetDensity\")\n  boostFactors = tf.exp((targetDensity - dutyCycles) * boostStrength,\n                        name=\"boostFactors\")\n  boosted = tf.multiply(x, boostFactors, name=\"boosted\")\n\n  # Take the boosted version of the input x, find the top k winners.\n  # Compute an output that contains the values of x corresponding to the top k\n  # boosted values\n  topk, _ = tf.math.top_k(input=boosted, k=k, sorted=False,\n                          name=\"compute_kwinners\")\n  bottom = tf.reduce_min(topk, axis=1, keepdims=True,\n                         name=\"compute_kwinners\")\n  mask = tf.cast(tf.greater_equal(boosted, bottom), dtype=x.dtype,\n                 name=\"compute_kwinners\")\n\n  return x * mask", "response": "Compute the kwinners of the given activity of each unit at the given activation level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrecovering permamnece for a single column :param columnIndex: index of the column of interest :return: permInfo (dict) with the following traces over time avgPermConnectedSyn: mean permanence for connected synapses avgPermNonConnectedSyn: mean permanence for non-connected synapses numConnectedSyn: number of connected synapses numNonConnectedSyn: number of non-connected synapses", "response": "def recoverPermanence(self, columnIndex, verbose=0, getPermTrace=0):\n    \"\"\"\n    Recover permamnece for a single column\n    :param columnIndex: index of the column of interest\n    :return: permInfo (dict) with the following traces over time\n              avgPermConnectedSyn: mean permanence for connected synapses\n              avgPermNonConnectedSyn: mean permanence for non-connected synapses\n              numConnectedSyn: number of connected synapses\n              numNonConnectedSyn: number of non-connected synapses\n    \"\"\"\n    activeColumns = self.mmGetTraceActiveColumns()\n    activeInputs = self.mmGetTraceActiveInputs()\n    learnTrace = self.mmGetTraceLearn()\n    initialPermanence = self.mmGetDataInitialPermanence()\n\n    (columnNumber, inputSize) = initialPermanence.shape\n\n    # potential mask for the column of interest\n    potential = numpy.zeros((inputSize), dtype=uintType)\n    self.getPotential(columnIndex, potential)\n    maskPotential = numpy.where(potential > 0)[0]\n\n    permChanges = numpy.zeros(inputSize, dtype=realDType)\n    perm = numpy.copy(initialPermanence[columnIndex, :])\n\n    numStep = len(activeColumns.data)\n    numConnectedSyn = []\n    avgPermConnected = []\n    avgPermNonConnected = []\n    activeSteps = []\n\n    permTrace = {}\n\n    for i in range(numStep):\n      if learnTrace.data[i] is False:\n        continue\n\n      if columnIndex in activeColumns.data[i][0]:\n        activeSteps.append(i)\n        if verbose:\n          print \"Column {} active at step {}\".format(columnIndex, i)\n          print \"active inputs: {}\".format(activeInputs.data[i])\n        # the logic here matches _adaptSynapses funciton in SP\n        permChanges.fill(-1 * self._synPermInactiveDec)\n        permChanges[activeInputs.data[i]] = self._synPermActiveInc\n        perm[maskPotential] += permChanges[maskPotential]\n\n        perm[perm < self._synPermTrimThreshold] = 0\n        numpy.clip(perm, self._synPermMin, self._synPermMax, out=perm)\n\n        if getPermTrace:\n          permTrace[i] = perm\n\n      numConnectedSyn.append(numpy.sum(perm > self._synPermConnected))\n      permMask = perm[maskPotential]\n      avgPermConnected.append(numpy.mean(permMask[\n                                         permMask > self._synPermConnected]))\n      avgPermNonConnected.append(numpy.mean(permMask[\n                                            permMask < self._synPermConnected]))\n\n    truePermanence = numpy.zeros((inputSize), dtype=realDType)\n    self.getPermanence(columnIndex, truePermanence)\n    if numpy.max(numpy.abs(perm - truePermanence)) > 1e-10:\n      raise RuntimeError(\"Permamnence reconstruction failed. The reconstructed\"\n                         \"permamnence does not match the real permamnence\")\n\n    numNonConnectedSyn = len(maskPotential) - numpy.array(numConnectedSyn)\n    permInfo = {\"avgPermConnectedSyn\": numpy.array(avgPermConnected),\n                \"avgPermNonConnectedSyn\": numpy.array(avgPermNonConnected),\n                \"numConnectedSyn\": numpy.array(numConnectedSyn),\n                \"numNonConnectedSyn\": numpy.array(numNonConnectedSyn),\n                \"permTrace\": permTrace,\n                \"activeSteps\": activeSteps}\n\n    return permInfo"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an object machine of the appropriate type.", "response": "def createObjectMachine(machineType, **kwargs):\n  \"\"\"\n  Return an object machine of the appropriate type.\n\n  @param machineType (str)  A supported ObjectMachine type\n\n  @param kwargs      (dict) Constructor argument for the class that will be\n                            instantiated. Keyword parameters specific to each\n                            model type should be passed in here.\n  \"\"\"\n\n  if machineType not in ObjectMachineTypes.getTypes():\n    raise RuntimeError(\"Unknown model type: \" + machineType)\n\n  return getattr(ObjectMachineTypes, machineType)(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate OpenAI Gym atari environment suitable for games with atari games.", "response": "def create_atari_environment(name):\n  \"\"\"\n  Create OpenAI Gym atari environment\n  :param name: name of  the atari game\n  :return: Preconfigured environment suitable for atari games\n  \"\"\"\n  env = gym.make(name)\n  env = NoopResetEnv(env, noop_max=30)\n  env = EpisodicLifeEnv(env)\n  env = ClipRewardEnv(env)\n\n  action_names = env.unwrapped.get_action_meanings()\n  if 'FIRE' in action_names:\n    env = FireResetEnv(env)\n\n  env = WarpFrame(env)\n  env = FrameStack(env, k=4)\n  return env"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing no - op action for a number of steps in [ 1 noop_max ).", "response": "def reset(self, **kwargs):\n    \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n    self.env.reset(**kwargs)\n    if self.override_num_noops is not None:\n      noops = self.override_num_noops\n    else:\n      noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  # pylint: disable=E1101\n    assert noops > 0\n    obs = None\n    for _ in range(noops):\n      obs, _, done, _ = self.env.step(self.noop_action)\n      if done:\n        obs = self.env.reset(**kwargs)\n    return obs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction used to update boost strength after each epoch.", "response": "def updateBoostStrength(m):\n  \"\"\"\n  Function used to update KWinner modules boost strength after each epoch.\n\n  Call using :meth:`torch.nn.Module.apply` after each epoch if required\n  For example: ``m.apply(updateBoostStrength)``\n\n  :param m: KWinner module\n  \"\"\"\n  if isinstance(m, KWinnersBase):\n    if m.training:\n      m.boostStrength = m.boostStrength * m.boostStrengthFactor"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates boost strength using given strength factor during training", "response": "def updateBoostStrength(self):\n    \"\"\"\n    Update boost strength using given strength factor during training\n    \"\"\"\n    if self.training:\n      self.boostStrength = self.boostStrength * self.boostStrengthFactor"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the current total entropy of this layer.", "response": "def entropy(self):\n    \"\"\"\n    Returns the current total entropy of this layer\n    \"\"\"\n    if self.k < self.n:\n      _, entropy = binaryEntropy(self.dutyCycle)\n      return entropy\n    else:\n      return 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef greedySensorPositions(numSensors, numLocations):\n\n  locationViewCounts = [0] * numLocations\n  locationViewCountsBySensor = [[0] * numLocations\n                                for _ in xrange(numSensors)]\n\n  placement = random.sample(xrange(numLocations), numSensors)\n\n  while True:\n    yield tuple(placement)\n\n    # Update statistics.\n    for sensor, location in enumerate(placement):\n      locationViewCounts[location] += 1\n      locationViewCountsBySensor[sensor][location] += 1\n\n    # Choose the locations with the lowest view counts. Break ties randomly.\n    nextLocationsRanked = sorted(xrange(numLocations),\n                                 key=lambda x: (locationViewCounts[x],\n                                                random.random()))\n    nextLocations = nextLocationsRanked[:numSensors]\n\n    # For each sensor (in random order), choose the location that has touched\n    # the least, breaking ties randomly.\n    sensors = range(numSensors)\n    random.shuffle(sensors)\n    for sensor in sensors:\n      viewCount = min(locationViewCountsBySensor[sensor][location]\n                      for location in nextLocations)\n      location = random.choice([x for x in nextLocations\n                                if locationViewCountsBySensor[sensor][x]\n                                == viewCount])\n      nextLocations.remove(location)\n      placement[sensor] = location", "response": "Returns an infinite sequence of sensor placements."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getSpec(cls):\n\n    spec = {\n      \"description\": ApicalTMPairRegion.__doc__,\n      \"singleNodeOnly\": True,\n      \"inputs\": {\n        \"activeColumns\": {\n          \"description\": (\"An array of 0's and 1's representing the active \"\n                          \"minicolumns, i.e. the input to the TemporalMemory\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": True,\n          \"regionLevel\": True,\n          \"isDefaultInput\": True,\n          \"requireSplitterMap\": False\n        },\n        \"resetIn\": {\n          \"description\": (\"A boolean flag that indicates whether\"\n                          \" or not the input vector received in this compute cycle\"\n                          \" represents the first presentation in a\"\n                          \" new temporal sequence.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 1,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False\n        },\n        \"basalInput\": {\n          \"description\": \"An array of 0's and 1's representing basal input\",\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False\n        },\n        \"basalGrowthCandidates\": {\n          \"description\": (\"An array of 0's and 1's representing basal input \" +\n                          \"that can be learned on new synapses on basal \" +\n                          \"segments. If this input is a length-0 array, the \" +\n                          \"whole basalInput is used.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False\n        },\n        \"apicalInput\": {\n          \"description\": \"An array of 0's and 1's representing top down input.\"\n          \" The input will be provided to apical dendrites.\",\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False\n        },\n        \"apicalGrowthCandidates\": {\n          \"description\": (\"An array of 0's and 1's representing apical input \" +\n                          \"that can be learned on new synapses on apical \" +\n                          \"segments. If this input is a length-0 array, the \" +\n                          \"whole apicalInput is used.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"required\": False,\n          \"regionLevel\": True,\n          \"isDefaultInput\": False,\n          \"requireSplitterMap\": False},\n      },\n      \"outputs\": {\n        \"predictedCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"cell that was predicted for this timestep.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False\n        },\n\n        \"predictedActiveCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"cell that transitioned from predicted to active.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False\n        },\n\n        \"activeCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"cell that is currently active.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": True\n        },\n\n        \"winnerCells\": {\n          \"description\": (\"A binary output containing a 1 for every \"\n                          \"'winner' cell in the TM.\"),\n          \"dataType\": \"Real32\",\n          \"count\": 0,\n          \"regionLevel\": True,\n          \"isDefaultOutput\": False\n        },\n      },\n\n      \"parameters\": {\n\n        # Input sizes (the network API doesn't provide these during initialize)\n        \"columnCount\": {\n          \"description\": (\"The size of the 'activeColumns' input \"\n                          \"(i.e. the number of columns)\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n\n        \"basalInputWidth\": {\n          \"description\": \"The size of the 'basalInput' input\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n\n        \"apicalInputWidth\": {\n          \"description\": \"The size of the 'apicalInput' input\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n\n        \"learn\": {\n          \"description\": \"True if the TM should learn.\",\n          \"accessMode\": \"ReadWrite\",\n          \"dataType\": \"Bool\",\n          \"count\": 1,\n          \"defaultValue\": \"true\"\n        },\n        \"cellsPerColumn\": {\n          \"description\": \"Number of cells per column\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"activationThreshold\": {\n          \"description\": (\"If the number of active connected synapses on a \"\n                          \"segment is at least this threshold, the segment \"\n                          \"is said to be active.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"reducedBasalThreshold\": {\n          \"description\": (\"Activation threshold of basal segments for cells \"\n                          \"with active apical segments (with apicalTiebreak \"\n                          \"implementation). \"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"initialPermanence\": {\n          \"description\": \"Initial permanence of a new synapse.\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"connectedPermanence\": {\n          \"description\": (\"If the permanence value for a synapse is greater \"\n                          \"than this value, it is said to be connected.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"minThreshold\": {\n          \"description\": (\"If the number of synapses active on a segment is at \"\n                          \"least this threshold, it is selected as the best \"\n                          \"matching cell in a bursting column.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1,\n          \"constraints\": \"\"\n        },\n        \"sampleSize\": {\n          \"description\": (\"The desired number of active synapses for an \"\n                          \"active cell\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1\n        },\n        \"learnOnOneCell\": {\n          \"description\": (\"If True, the winner cell for each column will be\"\n                          \" fixed between resets.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Bool\",\n          \"count\": 1,\n          \"defaultValue\": \"false\"\n        },\n        \"maxSynapsesPerSegment\": {\n          \"description\": \"The maximum number of synapses per segment. Use -1 \"\n                         \"for unlimited.\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Int32\",\n          \"count\": 1\n        },\n        \"maxSegmentsPerCell\": {\n          \"description\": \"The maximum number of segments per cell\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1\n        },\n        \"permanenceIncrement\": {\n          \"description\": (\"Amount by which permanences of synapses are \"\n                          \"incremented during learning.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"permanenceDecrement\": {\n          \"description\": (\"Amount by which permanences of synapses are \"\n                          \"decremented during learning.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"basalPredictedSegmentDecrement\": {\n          \"description\": (\"Amount by which active permanences of synapses of \"\n                          \"previously predicted but inactive segments are \"\n                          \"decremented.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"apicalPredictedSegmentDecrement\": {\n          \"description\": (\"Amount by which active permanences of synapses of \"\n                          \"previously predicted but inactive segments are \"\n                          \"decremented.\"),\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Real32\",\n          \"count\": 1\n        },\n        \"seed\": {\n          \"description\": \"Seed for the random number generator.\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"UInt32\",\n          \"count\": 1\n        },\n        \"implementation\": {\n          \"description\": \"Apical implementation\",\n          \"accessMode\": \"Read\",\n          \"dataType\": \"Byte\",\n          \"count\": 0,\n          \"constraints\": (\"enum: ApicalTiebreak, ApicalTiebreakCPP, ApicalDependent\"),\n          \"defaultValue\": \"ApicalTiebreakCPP\"\n        },\n      },\n    }\n\n    return spec", "response": "Returns the Spec for the given ApicalTMPairRegion object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute(self, inputs, outputs):\n\n    # If there's a reset, don't call compute. In some implementations, an empty\n    # input might cause unwanted effects.\n    if \"resetIn\" in inputs:\n      assert len(inputs[\"resetIn\"]) == 1\n      if inputs[\"resetIn\"][0] != 0:\n        # send empty output\n        self._tm.reset()\n        outputs[\"activeCells\"][:] = 0\n        outputs[\"predictedActiveCells\"][:] = 0\n        outputs[\"winnerCells\"][:] = 0\n        return\n\n    activeColumns = inputs[\"activeColumns\"].nonzero()[0]\n\n    if \"basalInput\" in inputs:\n      basalInput = inputs[\"basalInput\"].nonzero()[0]\n    else:\n      basalInput = np.empty(0, dtype=\"uint32\")\n\n    if \"apicalInput\" in inputs:\n      apicalInput = inputs[\"apicalInput\"].nonzero()[0]\n    else:\n      apicalInput = np.empty(0, dtype=\"uint32\")\n\n    if \"basalGrowthCandidates\" in inputs:\n      basalGrowthCandidates = inputs[\"basalGrowthCandidates\"].nonzero()[0]\n    else:\n      basalGrowthCandidates = basalInput\n\n    if \"apicalGrowthCandidates\" in inputs:\n      apicalGrowthCandidates = inputs[\"apicalGrowthCandidates\"].nonzero()[0]\n    else:\n      apicalGrowthCandidates = apicalInput\n\n    self._tm.compute(activeColumns, basalInput, apicalInput,\n                     basalGrowthCandidates, apicalGrowthCandidates, self.learn)\n\n    # Extract the active / predicted cells and put them into binary arrays.\n    outputs[\"activeCells\"][:] = 0\n    outputs[\"activeCells\"][self._tm.getActiveCells()] = 1\n    outputs[\"predictedCells\"][:] = 0\n    outputs[\"predictedCells\"][\n      self._tm.getPredictedCells()] = 1\n    outputs[\"predictedActiveCells\"][:] = (outputs[\"activeCells\"] *\n                                          outputs[\"predictedCells\"])\n    outputs[\"winnerCells\"][:] = 0\n    outputs[\"winnerCells\"][self._tm.getWinnerCells()] = 1", "response": "Compute one iteration of the TM s compute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getOutputElementCount(self, name):\n    if name in [\"activeCells\", \"predictedCells\", \"predictedActiveCells\",\n                \"winnerCells\"]:\n      return self.cellsPerColumn * self.columnCount\n    else:\n      raise Exception(\"Invalid output name specified: %s\" % name)", "response": "Returns the number of elements for the given output."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_examples(noise = 0.0, maxVal = 1.0, dataDir=\"data\"):\n  testDataDir = os.path.join(dataDir, \"speech_commands\", \"test\")\n  outDir = os.path.join(dataDir, \"noise_examples\")\n  if not os.path.exists(outDir):\n    os.mkdir(outDir)\n\n  # Create noise dataset with noise transform\n  noiseTransform = transforms.Compose([\n    LoadAudio(),\n    FixAudioLength(),\n    AddNoise(noise, maxVal=maxVal),\n  ])\n\n  noiseDataset = SpeechCommandsDataset(\n    testDataDir, noiseTransform, silence_percentage=0,\n  )\n\n  for i in range(0, 2552, 100):\n    d, target = noiseDataset[i]\n    fname = os.path.join(outDir, noiseDataset.classes[target] + \"_\" +\n                         str(i) + \"_\"\n                         + str(int(noise*100)) + \"_\"\n                         + str(int(maxVal*100))\n                         + \".wav\"\n                         )\n    print(d[\"path\"], fname)\n    librosa.output.write_wav(fname, d['samples'], d[\"sample_rate\"])", "response": "Generate sample noise files for listening and debugging."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlearns all provided sequences in the canonical format.", "response": "def learnSequences(self, sequences):\n    \"\"\"\n    Learns all provided sequences. Always reset the network in between\n    sequences.\n\n    Sequences format:\n\n    sequences = [\n      [\n        set([16, 22, 32]),  # S0, position 0\n        set([13, 15, 33])   # S0, position 1\n      ],\n      [\n        set([6, 12, 52]),  # S1, position 0\n        set([6, 2, 15])    # S1, position 1\n      ],\n    ]\n\n    Note that the order of each sequence is important. It denotes the sequence\n    number and will be used during inference to plot accuracy.\n\n    Parameters:\n    ----------------------------\n    @param   sequences (list)\n             Sequences to learn, in the canonical format specified above\n    \"\"\"\n    # This method goes through four phases:\n    #   1) We first train L4 on the sequences, over multiple passes\n    #   2) We then train L2 in one pass.\n    #   3) We then continue training on L4 so the apical segments learn\n    #   4) We run inference to store L2 representations for each sequence\n    # retrieve L2 representations\n\n    # print \"1) Train L4 sequence memory\"\n\n    # We're now using online learning, so both layers should be trying to learn\n    # at all times.\n\n    sequence_order = range(len(sequences))\n    if self.config[\"L2Params\"][\"onlineLearning\"]:\n      # Train L2 and L4\n      self._setLearningMode(l4Learning=True, l2Learning=True)\n      for _ in xrange(self.numLearningPoints):\n        random.shuffle(sequence_order)\n        for i in sequence_order:\n          sequence = sequences[i]\n          for s in sequence:\n            self.sensorInputs[0].addDataToQueue(list(s), 0, 0)\n            self.network.run(1)\n\n          # This is equivalent to, and faster than, giving the network no input\n          # for a period of time.\n          self.sendReset()\n    else:\n      # Train L4\n      self._setLearningMode(l4Learning=True, l2Learning=False)\n      for _ in xrange(self.numLearningPoints):\n        random.shuffle(sequence_order)\n        for i in sequence_order:\n          sequence = sequences[i]\n          for s in sequence:\n            self.sensorInputs[0].addDataToQueue(list(s), 0, 0)\n            self.network.run(1)\n\n          # This is equivalent to, and faster than, giving the network no input\n          # for a period of time.\n          self.sendReset()\n\n      # Train L2\n      self._setLearningMode(l4Learning=False, l2Learning=True)\n      for i in sequence_order:\n        sequence = sequences[i]\n        for s in sequence:\n          self.sensorInputs[0].addDataToQueue(list(s), 0, 0)\n          self.network.run(1)\n        self.sendReset()\n\n      # Train L4 apical segments\n      self._setLearningMode(l4Learning=True, l2Learning=False)\n      for _ in xrange(5):\n        for i in sequence_order:\n          sequence = sequences[i]\n          for s in sequence:\n            self.sensorInputs[0].addDataToQueue(list(s), 0, 0)\n            self.network.run(1)\n          self.sendReset()\n\n    self._setLearningMode(l4Learning=False, l2Learning=False)\n    self.sendReset()\n    for sequenceNum, sequence in enumerate(sequences):\n      for s in sequence:\n        self.sensorInputs[0].addDataToQueue(list(s), 0, 0)\n        self.network.run(1)\n      self.objectL2Representations[sequenceNum] = self.getL2Representations()\n      self.sendReset()\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer on a single sequence in the canonical format.", "response": "def infer(self, sequence, reset=True, sequenceNumber=None, burnIn=2,\n            enableFeedback=True, apicalTiebreak=True,\n            apicalModulationBasalThreshold=True, inertia=True):\n    \"\"\"\n    Infer on a single given sequence. Sequence format:\n\n    sequence = [\n      set([16, 22, 32]),  # Position 0\n      set([13, 15, 33])   # Position 1\n    ]\n\n    Parameters:\n    ----------------------------\n    @param   sequence (list)\n             Sequence to infer, in the canonical format specified above\n\n    @param   reset (bool)\n             If set to True (which is the default value), the network will\n             be reset after inference.\n\n    @param   sequenceNumber (int)\n             Number of the sequence (must match the number given during\n             learning).\n\n    @param   burnIn (int)\n             Number of patterns to wait within a sequence before computing\n             accuracy figures\n\n    \"\"\"\n    if enableFeedback is False:\n      self._disableL2()\n      self.network.regions[\"L4Column_0\"].getSelf()._tm.disableApicalDependence = True\n    else:\n      self._enableL2()\n\n    self._setLearningMode(l4Learning=False, l2Learning=False)\n\n    if sequenceNumber is not None:\n      if sequenceNumber not in self.objectL2Representations:\n        raise ValueError(\"The provided sequence was not given during learning\")\n\n\n    self.network.regions[\"L4Column_0\"].getSelf()._tm.setUseApicalModulationBasalThreshold(apicalModulationBasalThreshold)\n    self.network.regions[\"L4Column_0\"].getSelf()._tm.setUseApicalTiebreak(apicalTiebreak)\n    self.network.regions[\"L2Column_0\"].getSelf()._pooler.setUseInertia(inertia)\n\n    L2Responses=[]\n    L4Responses=[]\n    L4Predicted=[]\n    activityTrace = numpy.zeros(len(sequence))\n\n    totalActiveCells = 0\n    totalPredictedActiveCells = 0\n    for i,s in enumerate(sequence):\n      self.sensorInputs[0].addDataToQueue(list(s), 0, 0)\n      self.network.run(1)\n\n      activityTrace[i] = len(self.getL4Representations()[0])\n      L4Responses.append(self.getL4Representations()[0])\n      L4Predicted.append(self.getL4PredictedCells()[0])\n      L2Responses.append(self.getL2Representations()[0])\n      if i >= burnIn:\n        totalActiveCells += len(self.getL4Representations()[0])\n        totalPredictedActiveCells += len(self.getL4PredictedActiveCells()[0])\n\n    if reset:\n      self.sendReset()\n\n    avgActiveCells = float(totalActiveCells) / len(sequence)\n    avgPredictedActiveCells = float(totalPredictedActiveCells) / len(sequence)\n\n    responses = {\n            \"L2Responses\": L2Responses,\n            \"L4Responses\": L4Responses,\n            \"L4Predicted\": L4Predicted\n            }\n    return avgActiveCells,avgPredictedActiveCells,activityTrace, responses"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a reset signal to all the columns in the current sensor.", "response": "def sendReset(self, sequenceId=0):\n    \"\"\"\n    Sends a reset signal to the network.\n    \"\"\"\n    for col in xrange(self.numColumns):\n      self.sensorInputs[col].addResetToQueue(sequenceId)\n    self.network.run(1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getL4PredictedActiveCells(self):\n    predictedActive = []\n    for i in xrange(self.numColumns):\n      region = self.network.regions[\"L4Column_\" + str(i)]\n      predictedActive.append(\n        region.getOutputData(\"predictedActiveCells\").nonzero()[0])\n    return predictedActive", "response": "Returns the predicted active cells in each column in L4."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setLearningMode(self, l4Learning = False, l2Learning=False):\n    for column in self.L4Columns:\n      column.setParameter(\"learn\", 0, l4Learning)\n    for column in self.L2Columns:\n      column.setParameter(\"learningMode\", 0, l2Learning)", "response": "Sets the learning mode for L4 and L2."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef trainSequences(sequences, exp, idOffset=0):\n  for seqId in sequences:\n    # Make sure we learn enough times to deal with high order sequences and\n    # remove extra predictions.\n    iterations = 3*len(sequences[seqId])\n    for p in range(iterations):\n\n      # Ensure we generate new random location for each sequence presentation\n      s = sequences.provideObjectsToLearn([seqId])\n      objectSDRs = dict()\n      objectSDRs[seqId + idOffset] = s[seqId]\n      exp.learnObjects(objectSDRs, reset=False)\n\n      # TM needs reset between sequences, but not other regions\n      exp.TMColumns[0].reset()\n\n    # L2 needs resets when we switch to new object\n    exp.sendReset()", "response": "Train the network on all the sequences"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntrains the network on all the objects in the object list.", "response": "def trainObjects(objects, exp, numRepeatsPerObject, experimentIdOffset=0):\n  \"\"\"\n  Train the network on all the objects by randomly traversing points on\n  each object.  We offset the id of each object to avoid confusion with\n  any sequences that might have been learned.\n  \"\"\"\n  # We want to traverse the features of each object randomly a few times before\n  # moving on to the next object. Create the SDRs that we need for this.\n  objectsToLearn = objects.provideObjectsToLearn()\n  objectTraversals = {}\n  for objectId in objectsToLearn:\n    objectTraversals[objectId + experimentIdOffset] = objects.randomTraversal(\n      objectsToLearn[objectId], numRepeatsPerObject)\n\n  # Train the network on all the SDRs for all the objects\n  exp.learnObjects(objectTraversals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inferSequence(exp, sequenceId, sequences, objectName=0):\n  # Create the (loc, feat) pairs for this sequence for column 0.\n  objectSensations = {\n    0: [pair for pair in sequences[sequenceId]]\n  }\n\n  inferConfig = {\n    \"object\": sequenceId,\n    \"numSteps\": len(objectSensations[0]),\n    \"pairs\": objectSensations,\n  }\n\n  inferenceSDRs = sequences.provideObjectToInfer(inferConfig)\n  exp.infer(inferenceSDRs, objectName=objectName)", "response": "Run inference on the given sequence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning inference on the given object. objectName is the name of this object in the experiment.", "response": "def inferObject(exp, objectId, objects, objectName):\n  \"\"\"\n  Run inference on the given object.\n  objectName is the name of this object in the experiment.\n  \"\"\"\n\n  # Create sequence of random sensations for this object for one column. The\n  # total number of sensations is equal to the number of points on the object.\n  # No point should be visited more than once.\n  objectSensations = {}\n  objectSensations[0] = []\n  obj = objects[objectId]\n  objectCopy = [pair for pair in obj]\n  random.shuffle(objectCopy)\n  for pair in objectCopy:\n    objectSensations[0].append(pair)\n\n  inferConfig = {\n    \"numSteps\": len(objectSensations[0]),\n    \"pairs\": objectSensations,\n    \"includeRandomLocation\": False,\n  }\n\n  inferenceSDRs = objects.provideObjectToInfer(inferConfig)\n\n  exp.infer(inferenceSDRs, objectName=objectName)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createSuperimposedSensorySDRs(sequenceSensations, objectSensations):\n  assert len(sequenceSensations) == len(objectSensations)\n  superimposedSensations = []\n  for i, objectSensation in enumerate(objectSensations):\n    # print \"sequence loc:\", sequenceSensations[i][0][0]\n    # print \"object loc:  \",objectSensation[0][0]\n    # print\n    # print \"sequence feat:\", sequenceSensations[i][0][1]\n    # print \"object feat:  \",objectSensation[0][1]\n    # print\n    newSensation = {\n      0: (objectSensation[0][0],\n          sequenceSensations[i][0][1].union(objectSensation[0][1]))\n    }\n    # newSensation = {\n    #   0: (objectSensation[0][0],objectSensation[0][1])\n    # }\n    superimposedSensations.append(newSensation)\n    # print newSensation\n    # print\n    # print\n\n  return superimposedSensations", "response": "Given two lists of sensations create a new list where the sensory SDRs are the union of the individual sensory SDRs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning inference on the given sequence.", "response": "def inferSuperimposedSequenceObjects(exp, sequenceId, objectId, sequences, objects):\n  \"\"\"Run inference on the given sequence.\"\"\"\n  # Create the (loc, feat) pairs for this sequence for column 0.\n  objectSensations = {\n    0: [pair for pair in sequences[sequenceId]]\n  }\n\n  inferConfig = {\n    \"object\": sequenceId,\n    \"numSteps\": len(objectSensations[0]),\n    \"pairs\": objectSensations,\n  }\n\n  inferenceSDRSequence = sequences.provideObjectToInfer(inferConfig)\n\n  # Create sequence of random sensations for this object for one column. The\n  # total number of sensations is equal to the number of points on the object.\n  # No point should be visited more than once.\n  objectSensations = {}\n  objectSensations[0] = []\n  obj = objects[objectId]\n  objectCopy = [pair for pair in obj]\n  random.shuffle(objectCopy)\n  for pair in objectCopy:\n    objectSensations[0].append(pair)\n\n  inferConfig = {\n    \"numSteps\": len(objectSensations[0]),\n    \"pairs\": objectSensations,\n    \"includeRandomLocation\": False,\n  }\n\n  inferenceSDRObject = objects.provideObjectToInfer(inferConfig)\n\n  superimposedSDRs = createSuperimposedSDRs(inferenceSDRSequence, inferenceSDRObject)\n\n  # exp.infer(superimposedSDRs, objectName=str(sequenceId) + \"+\" + str(objectId))\n  exp.infer(superimposedSDRs, objectName=sequenceId*len(objects) + objectId)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef trainSuperimposedSequenceObjects(exp, numRepetitions,\n                                     sequences, objects):\n  \"\"\"\n  Train the network on the given object and sequence simultaneously for N\n  repetitions each.  The total number of training inputs is N * seqLength = M\n  (Ideally numPoints = seqLength)\n\n  Create a list of M random training presentations of the object (To)\n  Create a list of M=N*seqLength training presentations of the sequence (Ts).\n\n  Create a list of M training inputs. The i'th training sensory input is created\n  by taking the OR of To and Ts.  The location input comes from the object.\n\n  We only train L4 this way (?).\n\n  The other alternative is to train on one object for N repetitions. For each\n  repetition we choose a random sequence and create OR'ed sensory inputs. Insert\n  reset between each sequence presentation as before.  We need to ensure that\n  each sequence is seen at least seqLength times.\n  \"\"\"\n  trainingSensations = {}\n  objectSDRs = objects.provideObjectsToLearn()\n  sequenceSDRs = sequences.provideObjectsToLearn()\n\n  # Create the order of sequences we will show. Each sequence will be shown\n  # exactly numRepetitions times.\n  sequenceOrder = range(len(sequences)) * numRepetitions\n  random.shuffle(sequenceOrder)\n\n  for objectId,sensations in objectSDRs.iteritems():\n\n    # Create sequence of random sensations for this object, repeated\n    # numRepetitions times. The total number of sensations is equal to the\n    # number of points on the object multiplied by numRepetitions. Each time\n    # an object is visited, we choose a random sequence to show.\n\n    trainingSensations[objectId] = []\n\n    for s in range(numRepetitions):\n      # Get sensations for this object and shuffle them\n      objectSensations = [sensation for sensation in sensations]\n      random.shuffle(objectSensations)\n\n      # Pick a random sequence and get its sensations.\n      sequenceId = sequenceOrder.pop()\n      sequenceSensations = sequenceSDRs[sequenceId]\n\n      # Create superimposed sensory sensations.  We will only use the location\n      # SDRs from the object SDRs.\n      trainingSensations[objectId].extend(createSuperimposedSensorySDRs(\n        sequenceSensations, objectSensations))\n\n  # Train the network on all the SDRs for all the objects\n  exp.learnObjects(trainingSensations)", "response": "Train the network on the given object and sequence simultaneously for each object and object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_traceback(f):\n  @functools.wraps(f)\n  def wrapper(*args, **kwargs):\n    try:\n      return f(*args, **kwargs)\n    except Exception, ex:\n      ret = '#' * 60\n      ret += \"\\nException caught:\"\n      ret += \"\\n\" + '-' * 60\n      ret += \"\\n\" + traceback.format_exc()\n      ret += \"\\n\" + '-' * 60\n      ret += \"\\n\" + \"#\" * 60\n      print sys.stderr, ret\n      sys.stderr.flush()\n      raise ex\n\n  return wrapper", "response": "A decorator that returns a traceback of the given function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the experiment. The code is organized around what we need for specific figures in the paper. args is a dict representing the various parameters. We do it this way to support multiprocessing. The function returns the args dict updated with a number of additional keys containing performance metrics.", "response": "def runExperiment(args):\n  \"\"\"\n  Runs the experiment.  The code is organized around what we need for specific\n  figures in the paper.\n\n  args is a dict representing the various parameters. We do it this way to\n  support multiprocessing. The function returns the args dict updated with a\n  number of additional keys containing performance metrics.\n  \"\"\"\n  numObjects = args.get(\"numObjects\", 10)\n  numSequences = args.get(\"numSequences\", 10)\n  numFeatures = args.get(\"numFeatures\", 10)\n  seqLength = args.get(\"seqLength\", 10)\n  numPoints = args.get(\"numPoints\", 10)\n  trialNum = args.get(\"trialNum\", 42)\n  inputSize = args.get(\"inputSize\", 1024)\n  numLocations = args.get(\"numLocations\", 100000)\n  numInputBits = args.get(\"inputBits\", 20)\n  settlingTime = args.get(\"settlingTime\", 1)\n  numRepetitions = args.get(\"numRepetitions\", 5)\n  figure = args.get(\"figure\", False)\n  synPermProximalDecL2 = args.get(\"synPermProximalDecL2\", 0.001)\n  minThresholdProximalL2 = args.get(\"minThresholdProximalL2\", 10)\n  sampleSizeProximalL2 = args.get(\"sampleSizeProximalL2\", 15)\n  basalPredictedSegmentDecrement = args.get(\n    \"basalPredictedSegmentDecrement\", 0.0006)\n  stripStats = args.get(\"stripStats\", True)\n\n\n  random.seed(trialNum)\n\n  #####################################################\n  #\n  # Create the sequences and objects, and make sure they share the\n  # same features and locations.\n\n  sequences = createObjectMachine(\n    machineType=\"sequence\",\n    numInputBits=numInputBits,\n    sensorInputSize=inputSize,\n    externalInputSize=1024,\n    numCorticalColumns=1,\n    numFeatures=numFeatures,\n    numLocations=numLocations,\n    seed=trialNum\n  )\n  sequences.createRandomSequences(numSequences, seqLength)\n\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=numInputBits,\n    sensorInputSize=inputSize,\n    externalInputSize=1024,\n    numCorticalColumns=1,\n    numFeatures=numFeatures,\n    numLocations=numLocations,\n    seed=trialNum\n  )\n\n  # Make sure they share the same features and locations\n  objects.locations = sequences.locations\n  objects.features = sequences.features\n\n  objects.createRandomObjects(numObjects, numPoints=numPoints,\n                                    numLocations=numLocations,\n                                    numFeatures=numFeatures)\n\n  #####################################################\n  #\n  # Setup experiment and train the network\n  name = \"combined_sequences_S%03d_O%03d_F%03d_L%03d_T%03d\" % (\n    numSequences, numObjects, numFeatures, numLocations, trialNum\n  )\n  exp = L4TMExperiment(\n    name=name,\n    numCorticalColumns=1,\n    inputSize=inputSize,\n    numExternalInputBits=numInputBits,\n    externalInputSize=1024,\n    numInputBits=numInputBits,\n    seed=trialNum,\n    L2Overrides={\"synPermProximalDec\": synPermProximalDecL2,\n           \"minThresholdProximal\": minThresholdProximalL2,\n           \"sampleSizeProximal\": sampleSizeProximalL2,\n           \"initialProximalPermanence\": 0.45,\n           \"synPermProximalDec\": 0.002,\n    },\n    TMOverrides={\n      \"basalPredictedSegmentDecrement\": basalPredictedSegmentDecrement\n    },\n    L4Overrides={\"initialPermanence\": 0.21,\n           \"activationThreshold\": 18,\n           \"minThreshold\": 18,\n           \"basalPredictedSegmentDecrement\": basalPredictedSegmentDecrement,\n    },\n  )\n\n  printDiagnostics(exp, sequences, objects, args, verbosity=0)\n\n  # Train the network on all the sequences and then all the objects.\n  if figure in [\"S\", \"6\", \"7\"]:\n    trainSuperimposedSequenceObjects(exp, numRepetitions, sequences, objects)\n  else:\n    trainObjects(objects, exp, numRepetitions)\n    trainSequences(sequences, exp, numObjects)\n\n  ##########################################################################\n  #\n  # Run inference\n\n  print \"Running inference\"\n  if figure in [\"6\"]:\n    # We have trained the system on both temporal sequences and\n    # objects. We test the system by randomly switching between sequences and\n    # objects. To replicate the graph, we want to run sequences and objects in a\n    # specific order\n    for trial,itemType in enumerate([\"sequence\", \"object\", \"sequence\", \"object\",\n                                     \"sequence\", \"sequence\", \"object\",\n                                     \"sequence\", ]):\n      if itemType == \"sequence\":\n        objectId = random.randint(0, numSequences-1)\n        inferSequence(exp, objectId, sequences, objectId+numObjects)\n\n      else:\n        objectId = random.randint(0, numObjects-1)\n        inferObject(exp, objectId, objects, objectId)\n\n\n  elif figure in [\"7\"]:\n    # For figure 7 we have trained the system on both temporal sequences and\n    # objects. We test the system by superimposing randomly chosen sequences and\n    # objects.\n    for trial in range(10):\n      sequenceId = random.randint(0, numSequences - 1)\n      objectId = random.randint(0, numObjects - 1)\n      inferSuperimposedSequenceObjects(exp, sequenceId=sequenceId,\n                         objectId=objectId, sequences=sequences, objects=objects)\n\n  else:\n    # By default run inference on every sequence and object in order.\n    for objectId in objects:\n      inferObject(exp, objectId, objects, objectId)\n    for seqId in sequences:\n      inferSequence(exp, seqId, sequences, seqId+numObjects)\n\n\n  ##########################################################################\n  #\n  # Debugging diagnostics\n  printDiagnosticsAfterTraining(exp)\n\n  ##########################################################################\n  #\n  # Compute a number of overall inference statistics\n\n  print \"# Sequences {} # features {} trial # {}\\n\".format(\n    numSequences, numFeatures, trialNum)\n\n  convergencePoint, sequenceAccuracyL2 = exp.averageConvergencePoint(\n    \"L2 Representation\", 30, 40, 1, numObjects)\n  print \"L2 accuracy for sequences:\", sequenceAccuracyL2\n\n  convergencePoint, objectAccuracyL2 = exp.averageConvergencePoint(\n    \"L2 Representation\", 30, 40, 1, 0, numObjects)\n  print \"L2 accuracy for objects:\", objectAccuracyL2\n\n  objectCorrectSparsityTM, _ = exp.averageSequenceAccuracy(15, 25, 0, numObjects)\n  print \"TM accuracy for objects:\", objectCorrectSparsityTM\n\n  sequenceCorrectSparsityTM, sequenceCorrectClassificationsTM = \\\n    exp.averageSequenceAccuracy(15, 25, numObjects)\n  print \"TM accuracy for sequences:\", sequenceCorrectClassificationsTM\n\n  infStats = exp.getInferenceStats()\n  predictedActive = numpy.zeros(len(infStats))\n  predicted = numpy.zeros(len(infStats))\n  predictedActiveL4 = numpy.zeros(len(infStats))\n  predictedL4 = numpy.zeros(len(infStats))\n  for i,stat in enumerate(infStats):\n    predictedActive[i] = float(sum(stat[\"TM PredictedActive C0\"][2:])) / len(\n      stat[\"TM PredictedActive C0\"][2:])\n    predicted[i] = float(sum(stat[\"TM NextPredicted C0\"][2:])) / len(\n      stat[\"TM NextPredicted C0\"][2:])\n\n    predictedActiveL4[i] = float(sum(stat[\"L4 PredictedActive C0\"])) / len(\n      stat[\"L4 PredictedActive C0\"])\n    predictedL4[i] = float(sum(stat[\"L4 Predicted C0\"])) / len(\n      stat[\"L4 Predicted C0\"])\n\n  # Return a bunch of metrics we will use in plots\n  args.update({\"sequences\": sequences.getObjects()})\n  args.update({\"objects\": objects.getObjects()})\n  args.update({\"convergencePoint\":convergencePoint})\n  args.update({\"objectAccuracyL2\": objectAccuracyL2})\n  args.update({\"sequenceAccuracyL2\": sequenceAccuracyL2})\n  args.update({\"sequenceCorrectSparsityTM\": sequenceCorrectSparsityTM})\n  args.update({\"sequenceCorrectClassificationsTM\": sequenceCorrectClassificationsTM})\n  args.update({\"objectCorrectSparsityTM\": objectCorrectSparsityTM})\n  args.update({\"averagePredictions\": predicted.mean()})\n  args.update({\"averagePredictedActive\": predictedActive.mean()})\n  args.update({\"averagePredictionsL4\": predictedL4.mean()})\n  args.update({\"averagePredictedActiveL4\": predictedActiveL4.mean()})\n\n  if stripStats:\n    exp.stripStats()\n  args.update({\"name\": exp.name})\n  args.update({\"statistics\": exp.statistics})\n  args.update({\"networkConfig\": exp.config})\n\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef runExperimentPool(numSequences,\n                      numFeatures,\n                      numLocations,\n                      numObjects,\n                      numWorkers=7,\n                      nTrials=1,\n                      seqLength=10,\n                      figure=\"\",\n                      numRepetitions=1,\n                      synPermProximalDecL2=[0.001],\n                      minThresholdProximalL2=[10],\n                      sampleSizeProximalL2=[15],\n                      inputSize=[1024],\n                      basalPredictedSegmentDecrement=[0.0006],\n                      resultsName=\"convergence_results.pkl\"):\n  \"\"\"\n  Run a bunch of experiments using a pool of numWorkers multiple processes. For\n  numSequences, numFeatures, and numLocations pass in a list containing valid\n  values for that parameter. The cross product of everything is run, and each\n  combination is run nTrials times.\n\n  Returns a list of dict containing detailed results from each experiment. Also\n  pickles and saves all the results in resultsName for later analysis.\n\n  If numWorkers == 1, the experiments will be run in a single thread. This makes\n  it easier to debug.\n\n  Example:\n    results = runExperimentPool(\n                          numSequences=[10, 20],\n                          numFeatures=[5, 13],\n                          numWorkers=8,\n                          nTrials=5)\n  \"\"\"\n  # Create function arguments for every possibility\n  args = []\n\n  for bd in basalPredictedSegmentDecrement:\n    for i in inputSize:\n      for thresh in minThresholdProximalL2:\n        for dec in synPermProximalDecL2:\n          for s in sampleSizeProximalL2:\n            for o in reversed(numSequences):\n              for l in numLocations:\n                for f in numFeatures:\n                  for no in numObjects:\n                    for t in range(nTrials):\n                      args.append(\n                        {\"numSequences\": o,\n                         \"numFeatures\": f,\n                         \"numObjects\": no,\n                         \"trialNum\": t,\n                         \"seqLength\": seqLength,\n                         \"numLocations\": l,\n                         \"sampleSizeProximalL2\": s,\n                         \"synPermProximalDecL2\": dec,\n                         \"minThresholdProximalL2\": thresh,\n                         \"numRepetitions\": numRepetitions,\n                         \"figure\": figure,\n                         \"inputSize\": i,\n                         \"basalPredictedSegmentDecrement\": bd,\n                         }\n                      )\n  print \"{} experiments to run, {} workers\".format(len(args), numWorkers)\n\n  # Run the pool\n  if numWorkers > 1:\n    pool = Pool(processes=numWorkers)\n    result = pool.map(runExperiment, args)\n  else:\n    result = []\n    for arg in args:\n      result.append(runExperiment(arg))\n\n  # Pickle results for later use\n  with open(resultsName,\"wb\") as f:\n    cPickle.dump(result,f)\n\n  return result", "response": "This function runs a bunch of experiments using a pool of processes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the second experiment in the section Simulations with Pure Temporal Sequences. Here we check accuracy of the L2 and L4 networks and the could sequences.", "response": "def runExperiment4B(dirName):\n  \"\"\"\n  This runs the second experiment in the section \"Simulations with Pure Temporal\n  Sequences\". Here we check accuracy of the L2/L4 networks in classifying the\n  sequences. This experiment averages over many parameter combinations and could\n  take several minutes.\n  \"\"\"\n  # Results are put into a pkl file which can be used to generate the plots.\n  # dirName is the absolute path where the pkl file will be placed.\n  resultsName = os.path.join(dirName, \"sequence_batch_high_dec_normal_features.pkl\")\n\n  numTrials = 10\n  featureRange = [10, 50, 100, 200]\n  seqRange = [50]\n  locationRange = [10, 100, 200, 300, 400, 500]\n\n  runExperimentPool(\n    numSequences=seqRange,\n    numFeatures=featureRange,\n    numLocations=locationRange,\n    numObjects=[0],\n    seqLength=10,\n    nTrials=numTrials,\n    numWorkers=cpu_count()-1,\n    basalPredictedSegmentDecrement=[0.005],\n    resultsName=resultsName)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef runExperiment5A(dirName):\n  # Results are put into a pkl file which can be used to generate the plots.\n  # dirName is the absolute path where the pkl file will be placed.\n  resultsFilename = os.path.join(dirName, \"sensorimotor_sequence_example.pkl\")\n  results = runExperiment(\n    {\n      \"numSequences\": 0,\n      \"seqLength\": 10,\n      \"numFeatures\": 100,\n      \"trialNum\": 4,\n      \"numObjects\": 50,\n      \"numLocations\": 100,\n    }\n  )\n\n  # Pickle results for plotting and possible later debugging\n  with open(resultsFilename, \"wb\") as f:\n    cPickle.dump(results, f)", "response": "This runs the first experiment in the section Simulations with Sensorimotor\n Sequences an example sensorimotor sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the second experiment in the section Simulations with Sensorimotor Sequences. It averages over many parameter combinations.", "response": "def runExperiment5B(dirName):\n  \"\"\"\n  This runs the second experiment in the section \"Simulations with Sensorimotor\n  Sequences\". It averages over many parameter combinations. This experiment\n  could take several hours.  You can run faster versions by reducing the number\n  of trials.\n  \"\"\"\n  # Results are put into a pkl file which can be used to generate the plots.\n  # dirName is the absolute path where the pkl file will be placed.\n  resultsName = os.path.join(dirName, \"sensorimotor_batch_results_more_objects.pkl\")\n\n  # We run 10 trials for each column number and then analyze results\n  numTrials = 10\n  featureRange = [10, 50, 100, 150, 500]\n  objectRange = [110, 130, 200, 300]\n  locationRange = [100]\n\n  # Comment this out if you  are re-running analysis on already saved results.\n  # Very useful for debugging the plots\n  runExperimentPool(\n    numSequences=[0],\n    numObjects=objectRange,\n    numFeatures=featureRange,\n    numLocations=locationRange,\n    nTrials=numTrials,\n    numWorkers=cpu_count() - 1,\n    numRepetitions=10,\n    resultsName=resultsName)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef runExperiment6(dirName):\n  # Results are put into a pkl file which can be used to generate the plots.\n  # dirName is the absolute path where the pkl file will be placed.\n  resultsFilename = os.path.join(dirName, \"combined_results.pkl\")\n  results = runExperiment(\n    {\n      \"numSequences\": 50,\n      \"seqLength\": 10,\n      \"numObjects\": 50,\n      \"numFeatures\": 500,\n      \"trialNum\": 8,\n      \"numLocations\": 100,\n      \"settlingTime\": 1,\n      \"figure\": \"6\",\n      \"numRepetitions\": 30,\n      \"basalPredictedSegmentDecrement\": 0.001,\n      \"stripStats\": False,\n    }\n  )\n\n  # Pickle results for plotting and possible later debugging\n  with open(resultsFilename, \"wb\") as f:\n    cPickle.dump(results, f)", "response": "This runs the experiment the section Simulations with Combined Sequences and generates a pickle file containing the combined sequences."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef runExperimentSP(dirName):\n  # Results are put into a pkl file which can be used to generate the plots.\n  # dirName is the absolute path where the pkl file will be placed.\n  resultsFilename = os.path.join(dirName, \"superimposed_128mcs.pkl\")\n\n  # We run a bunch of trials with these combinations\n  numTrials = 10\n  featureRange = [1000]\n  objectRange = [50]\n\n  # Comment this out if you  are re-running analysis on already saved results.\n  runExperimentPool(\n    numSequences=objectRange,\n    numObjects=objectRange,\n    numFeatures=featureRange,\n    numLocations=[100],\n    nTrials=numTrials,\n    numWorkers=cpu_count() - 1,\n    resultsName=resultsFilename,\n    figure=\"S\",\n    numRepetitions=30,\n    sampleSizeProximalL2=[15],\n    minThresholdProximalL2=[10],\n    synPermProximalDecL2=[0.001],\n    # basalPredictedSegmentDecrement=[0.0, 0.001, 0.002, 0.003, 0.004, 0.005],\n    basalPredictedSegmentDecrement=[0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.01, 0.02, 0.04, 0.08, 0.12],\n    inputSize=[128],\n  )\n\n  print \"Done with experiments\"", "response": "This function runs a pool of experiments where the network is trained on stream\n. The number of trials is 10 and the number of objects is 50."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset(self,):\n\n    self.activeState['t-1'].fill(0)\n    self.activeState['t'].fill(0)\n    self.predictedState['t-1'].fill(0)\n    self.predictedState['t'].fill(0)\n    self.learnState['t-1'].fill(0)\n    self.learnState['t'].fill(0)\n    self.confidence['t-1'].fill(0)\n    self.confidence['t'].fill(0)\n\n    # Flush the segment update queue\n    self.segmentUpdates = {}\n\n    self._internalStats['nInfersSinceReset'] = 0\n\n    #To be removed\n    self._internalStats['curPredictionScore'] = 0\n    #New prediction score\n    self._internalStats['curPredictionScore2']   = 0\n    self._internalStats['curFalseNegativeScore'] = 0\n    self._internalStats['curFalsePositiveScore'] = 0\n\n    self._internalStats['curMissing'] = 0\n    self._internalStats['curExtra'] = 0\n\n\n    # When a reset occurs, set prevSequenceSignature to the signature of the\n    # just-completed sequence and start accumulating histogram for the next\n    # sequence.\n    self._internalStats['prevSequenceSignature'] = None\n    if self.collectSequenceStats:\n      if self._internalStats['confHistogram'].sum() > 0:\n        sig = self._internalStats['confHistogram'].copy()\n        sig.reshape(self.numberOfCols * self.cellsPerColumn)\n        self._internalStats['prevSequenceSignature'] = sig\n      self._internalStats['confHistogram'].fill(0)\n\n    self.resetCalled = True", "response": "Resets the internal state of all cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets the learning and inference stats.", "response": "def resetStats(self):\n    \"\"\" Reset the learning and inference stats. This will usually be called by\n    user code at the start of each inference run (for a particular data set).\n    \"\"\"\n    self._stats = dict()\n    self._internalStats = dict()\n\n    self._internalStats['nInfersSinceReset'] = 0\n    self._internalStats['nPredictions'] = 0\n\n    #New prediction score\n    self._internalStats['curPredictionScore2']     = 0\n    self._internalStats['predictionScoreTotal2']   = 0\n    self._internalStats['curFalseNegativeScore']   = 0\n    self._internalStats['falseNegativeScoreTotal'] = 0\n    self._internalStats['curFalsePositiveScore']   = 0\n    self._internalStats['falsePositiveScoreTotal'] = 0\n\n    self._internalStats['pctExtraTotal'] = 0\n    self._internalStats['pctMissingTotal'] = 0\n    self._internalStats['curMissing'] = 0\n    self._internalStats['curExtra'] = 0\n    self._internalStats['totalMissing'] = 0\n    self._internalStats['totalExtra'] = 0\n\n    # Sequence signature statistics. Note that we don't reset the sequence\n    # signature list itself.\n    self._internalStats['prevSequenceSignature'] = None\n    if self.collectSequenceStats:\n      self._internalStats['confHistogram'] = \\\n          numpy.zeros((self.numberOfCols, self.cellsPerColumn), dtype=\"float32\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the current learning and inference stats.", "response": "def getStats(self):\n    \"\"\" Return the current learning and inference stats. This returns a dict\n    containing all the learning and inference stats we have collected since the\n    last resetStats(). If collectStats is False, then None is returned.\n\n    The following keys are returned in the dict when collectStats is True:\n\n      nPredictions:           the number of predictions. This is the total number\n                              of inferences excluding burn-in and the last\n                              inference.\n\n      curPredictionScore:     the score for predicting the current input (predicted\n                              during the previous inference)\n\n      curMissing:             the number of bits in the current input that were\n                              not predicted to be on.\n\n      curExtra:               the number of bits in the predicted output that are\n                              not in the next input\n\n      predictionScoreTotal:   the sum of every prediction score to date\n\n      predictionScoreAvg:     predictionScoreTotal / nPredictions\n\n      pctMissingTotal:        the total number of bits that were missed over all\n                              predictions\n\n      pctMissingAvg:          pctMissingTotal / nPredictions\n\n      prevSequenceSignature:  signature for the sequence immediately preceding the\n                              last reset. 'None' if collectSequenceStats is False\n\n    \"\"\"\n\n    if not self.collectStats:\n      return None\n\n    self._stats['nPredictions'] = self._internalStats['nPredictions']\n    self._stats['curMissing'] = self._internalStats['curMissing']\n    self._stats['curExtra'] = self._internalStats['curExtra']\n    self._stats['totalMissing'] = self._internalStats['totalMissing']\n    self._stats['totalExtra'] = self._internalStats['totalExtra']\n\n    nPredictions = max(1, self._stats['nPredictions'])\n\n    #New prediction score\n    self._stats['curPredictionScore2'] = self._internalStats['curPredictionScore2']\n    self._stats['predictionScoreAvg2'] = self._internalStats['predictionScoreTotal2'] \\\n                                         / nPredictions\n    self._stats['curFalseNegativeScore'] = self._internalStats['curFalseNegativeScore']\n    self._stats['falseNegativeAvg'] = self._internalStats['falseNegativeScoreTotal'] \\\n                                      / nPredictions\n    self._stats['curFalsePositiveScore'] = self._internalStats['curFalsePositiveScore']\n    self._stats['falsePositiveAvg'] = self._internalStats['falsePositiveScoreTotal'] \\\n                                      / nPredictions\n\n    self._stats['pctExtraAvg'] = self._internalStats['pctExtraTotal'] \\\n                                   / nPredictions\n    self._stats['pctMissingAvg'] = self._internalStats['pctMissingTotal'] \\\n                                   / nPredictions\n\n    # This will be None if collectSequenceStats is False\n    self._stats['prevSequenceSignature'] = self._internalStats['prevSequenceSignature']\n\n    return self._stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _updateStatsInferEnd(self, stats, bottomUpNZ, predictedState, confidence):\n\n    # Return if not collecting stats\n    if not self.collectStats:\n      return\n    stats['nInfersSinceReset'] += 1\n\n\n    # Compute the prediction score, how well the prediction from the last\n    #  time step predicted the current bottom-up input\n    patternsToCheck = [bottomUpNZ]\n    (numExtra2, numMissing2, confidences2) = self.checkPrediction2(\n                                  patternNZs = [bottomUpNZ],\n                                  output = predictedState,\n                                  confidence = confidence)\n    predictionScore, positivePredictionScore, negativePredictionScore = \\\n                                                          confidences2[0]\n\n    # Store the stats that don't depend on burn-in\n    stats['curPredictionScore2'] = float(predictionScore)\n    stats['curFalseNegativeScore'] = 1.0 - float(positivePredictionScore)\n    stats['curFalsePositiveScore'] = float(negativePredictionScore)\n\n    stats['curMissing'] = numMissing2\n    stats['curExtra'] = numExtra2\n\n\n    # ----------------------------------------------------------------------\n    # If we are passed the burn-in period, update the accumulated stats\n    # Here's what various burn-in values mean:\n    #   0: try to predict the first element of each sequence and all subsequent\n    #   1: try to predict the second element of each sequence and all subsequent\n    #   etc.\n    if stats['nInfersSinceReset'] <= self.burnIn:\n      return\n\n    # Burn-in related stats\n    stats['nPredictions'] += 1\n    numExpected = max(1.0, float(len(bottomUpNZ)))\n\n    stats['totalMissing'] += numMissing2\n    stats['totalExtra'] += numExtra2\n    stats['pctExtraTotal'] += 100.0 * numExtra2 / numExpected\n    stats['pctMissingTotal'] += 100.0 * numMissing2 / numExpected\n    stats['predictionScoreTotal2'] += float(predictionScore)\n    stats['falseNegativeScoreTotal'] += 1.0 - float(positivePredictionScore)\n    stats['falsePositiveScoreTotal'] += float(negativePredictionScore)\n\n    if self.collectSequenceStats:\n      # Collect cell confidences for every cell that correctly predicted current\n      # bottom up input. Normalize confidence across each column\n      cc = self.confidence['t-1'] * self.activeState['t']\n      sconf = cc.sum(axis=1)\n      for c in range(self.numberOfCols):\n        if sconf[c] > 0:\n          cc[c,:] /= sconf[c]\n\n      # Update cell confidence histogram: add column-normalized confidence\n      # scores to the histogram\n      self._internalStats['confHistogram'] += cc", "response": "This routine is called at the end of learning and inference of a single current bottom - up input. This routine updates the internal statistics dictionary with the values of the current prediction score and the predicted state and the confidence of the current prediction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef printActiveIndices(self, state, andValues=False):\n    (cols, cellIdxs) = state.nonzero()\n\n    if len(cols) == 0:\n      print \"NONE\"\n      return\n\n    prevCol = -1\n    for (col,cellIdx) in zip(cols, cellIdxs):\n      if col != prevCol:\n        if prevCol != -1:\n          print \"] \",\n        print \"Col %d: [\" % (col),\n        prevCol = col\n\n      if andValues:\n        print \"%d: %s,\" % (cellIdx, state[col,cellIdx]),\n      else:\n        print \"%d,\" % (cellIdx),\n    print \"]\"", "response": "Print the list of column cellIdx indices for each of the active\n    cells in state."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints out the diagnostic information for the current state of the current segment and the current confidence.", "response": "def printComputeEnd(self, output, learn=False):\n    \"\"\" Called at the end of inference to print out various diagnostic\n    information based on the current verbosity level.\n    \"\"\"\n    if self.verbosity >= 3:\n      print \"----- computeEnd summary: \"\n      print \"numBurstingCols: %s, \" % (self.activeState['t'].min(axis=1).sum()),\n      print \"curPredScore2: %s, \" % (self._internalStats['curPredictionScore2']),\n      print \"curFalsePosScore: %s, \" % (self._internalStats['curFalsePositiveScore']),\n      print \"1-curFalseNegScore: %s, \" % (1-self._internalStats['curFalseNegativeScore']),\n      print \"numPredictedCells[t-1]: %s\" % (self.predictedState['t-1'].sum()),\n      print \"numSegments: \",self.getNumSegments()\n\n      print \"----- activeState (%d on) ------\" \\\n              % (self.activeState['t'].sum())\n      self.printActiveIndices(self.activeState['t'])\n      if self.verbosity >= 5:\n        self.printState(self.activeState['t'])\n\n      print \"----- predictedState (%d on)-----\" \\\n              % (self.predictedState['t'].sum())\n      self.printActiveIndices(self.predictedState['t'])\n      if self.verbosity >= 5:\n        self.printState(self.predictedState['t'])\n\n      print \"----- cell confidence -----\"\n      self.printActiveIndices(self.confidence['t'], andValues=True)\n      if self.verbosity >= 5:\n        self.printConfidence(self.confidence['t'])\n\n      print \"----- confidence[t-1] for currently active cells -----\"\n      cc = self.confidence['t-1'] * self.activeState['t']\n      self.printActiveIndices(cc, andValues=True)\n\n      if self.verbosity == 4:\n        print \"Cells, predicted segments only:\"\n        self.printCells(predictedOnly=True)\n      elif self.verbosity >= 5:\n        print \"Cells, all segments:\"\n        self.printCells(predictedOnly=False)\n      print"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the segment on cell c with index segIdx", "response": "def getSegmentOnCell(self, c, i, segIdx):\n    \"\"\"Return the segment on cell (c,i) with index sidx.\n    Returns the segment as following list:\n      [  [segmentID, sequenceSegmentFlag, frequency],\n         [col1, idx1, perm1],\n         [col2, idx2, perm2], ...\n      ]\n    \"\"\"\n    return self.cells[c][i][segIdx]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove a segment update from the sequence.", "response": "def removeSegmentUpdate(self, updateInfo):\n    \"\"\"Remove a segment update (called when seg update expires or is processed)\n\n    Parameters:\n    --------------------------------------------------------------\n    updateInfo:     (creationDate, SegmentUpdate)\n    \"\"\"\n\n    # An updateInfo contains (creationDate, SegmentUpdate)\n    (creationDate, segUpdate) = updateInfo\n\n    # Key is stored in segUpdate itself...\n    key = (segUpdate.columnIdx, segUpdate.cellIdx)\n\n    self.segmentUpdates[key].remove(updateInfo)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef computeOutput(self):\n\n    \"\"\"Computes output for both learning and inference. In both cases, the\n    output is the boolean OR of activeState and predictedState at t.\n    Stores currentOutput for checkPrediction.\"\"\"\n\n    # todo: This operation can be sped up by:\n    #  1.)  Pre-allocating space for the currentOutput\n    #  2.)  Making predictedState and activeState of type 'float32' up front\n    #  3.)  Using logical_or(self.predictedState['t'], self.activeState['t'],\n    #          self.currentOutput)\n    #\n    self.currentOutput = \\\n        logical_or(self.predictedState['t'], self.activeState['t'])\n\n    return self.currentOutput.reshape(-1).astype('float32')", "response": "Computes the output for both learning and inference."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict(self, nSteps):\n\n    # Save the TP dynamic state, we will use to revert back in the end\n    pristineTPDynamicState = self._getTPDynamicState()\n\n    assert (nSteps>0)\n\n    # multiStepColumnPredictions holds all the future prediction.\n    multiStepColumnPredictions = numpy.zeros((nSteps, self.numberOfCols),\n                                             dtype=\"float32\")\n\n    # This is a (nSteps-1)+half loop. Phase 2 in both learn and infer methods\n    # already predicts for timestep (t+1). We use that prediction for free and save\n    # the half-a-loop of work.\n\n    step = 0\n    while True:\n\n      # We get the prediction for the columns in the next time step from\n      # the topDownCompute method. It internally uses confidences.\n      multiStepColumnPredictions[step,:] = self.topDownCompute()\n\n      # Cleanest way in python to handle one and half loops\n      if step == nSteps-1:\n        break\n      step += 1\n\n      # Copy t-1 into t\n      self.activeState['t-1'][:,:] = self.activeState['t'][:,:]\n      self.predictedState['t-1'][:,:] = self.predictedState['t'][:,:]\n      self.confidence['t-1'][:,:] = self.confidence['t'][:,:]\n\n      # Predicted state at \"t-1\" becomes the active state at \"t\"\n      self.activeState['t'][:,:] = self.predictedState['t-1'][:,:]\n\n      # Predicted state and confidence are set in phase2.\n      self.predictedState['t'].fill(0)\n      self.confidence['t'].fill(0.0)\n      self.computePhase2(doLearn=False)\n\n    # Revert the dynamic state to the saved state\n    self._setTPDynamicState(pristineTPDynamicState)\n\n\n    return multiStepColumnPredictions", "response": "This function computes the prediction for each column in the next time step."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef computePhase2(self, doLearn=False):\n\n    # Phase 2: compute predicted state for each cell\n    # - if a segment has enough horizontal connections firing because of\n    #   bottomUpInput, it's set to be predicting, and we queue up the segment\n    #   for reinforcement,\n    # - if pooling is on, try to find the best weakly activated segment to\n    #   reinforce it, else create a new pooling segment.\n    for c in xrange(self.numberOfCols):\n\n      buPredicted = False # whether any cell in the column is predicted\n      for i in xrange(self.cellsPerColumn):\n        # Iterate over each of the segments of this cell\n        maxConfidence = 0\n        for s in self.cells[c][i]:\n\n          # sum(connected synapses) >= activationThreshold?\n          if self.isSegmentActive(s, self.activeState['t']):\n\n            self.predictedState['t'][c,i] = 1\n            buPredicted = True\n            maxConfidence = max(maxConfidence, s.dutyCycle(readOnly=True))\n\n            if doLearn:\n              s.totalActivations += 1    # increment activationFrequency\n              s.lastActiveIteration = self.iterationIdx\n              # mark this segment for learning\n              activeUpdate = self.getSegmentActiveSynapses(c,i,s,'t')\n              activeUpdate.phase1Flag = False\n              self.addToSegmentUpdates(c, i, activeUpdate)\n\n        # Store the max confidence seen among all the weak and strong segments\n        #  as the cell's confidence.\n        self.confidence['t'][c,i] = maxConfidence", "response": "This function computes the phase 2 of learning inference and multistep prediction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the output for both learning and inference.", "response": "def compute(self, bottomUpInput, enableLearn, computeInfOutput=None):\n    \"\"\"Computes output for both learning and inference. In both cases, the\n    output is the boolean OR of activeState and predictedState at t.\n    Stores currentOutput for checkPrediction.\"\"\"\n\n    if enableLearn:\n      self.learn(bottomUpInput)\n    else:\n      self.infer(bottomUpInput)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninfer the current state of a single segment from the input matrix.", "response": "def infer(self, bottomUpInput):\n    \"\"\"\n    Parameters:\n    --------------------------------------------\n    input:        Current bottom-up input, dense\n    retval:       ?\n    \"\"\"\n\n    self.iterationIdx = self.iterationIdx + 1\n\n    if self.verbosity >= 3:\n      print \"\\n==== Iteration: %d =====\" % (self.iterationIdx)\n      print \"Active cols:\", bottomUpInput.nonzero()[0]\n\n    # Copy t into t-1\n    # Don't need to copy learnState, which is not used in inference\n    self.activeState['t-1'][:,:] = self.activeState['t'][:,:]\n    self.predictedState['t-1'][:,:] = self.predictedState['t'][:,:]\n    self.confidence['t-1'][:,:] = self.confidence['t'][:,:]\n    self.activeState['t'].fill(0)\n    self.predictedState['t'].fill(0)\n    self.confidence['t'].fill(0.0)\n\n    # Phase 1: calculate current state for each cell\n    # For each column (winning in the SP):\n    # - if the bottom up input was predicted by one of the sequence\n    #   segments of a cell, this cell bursts,\n    # - if the bottom up input was not predicted by any cell, the entire\n    #   column bursts.\n    activeColumns = bottomUpInput.nonzero()[0]\n\n    for c in activeColumns:\n\n      buPredicted = False\n      predictingCells = numpy.where(self.predictedState['t-1'][c] == 1)[0]\n\n      for i in predictingCells:\n        buPredicted = True\n        self.activeState['t'][c,i] = 1\n\n      if not buPredicted:\n        self.activeState['t'][c,:] = 1 # whole column bursts\n\n    # Phase 2: calculate predictive output for each cell\n    # A cell turns on its predictedState if any of its segments\n    # has enough horizontal connections currently firing due to\n    # bottom up input.\n    self.computePhase2(doLearn=False)\n\n    # Update the prediction score stats\n    if self.collectStats:\n      # import pdb; pdb.set_trace()\n      self._updateStatsInferEnd(self._internalStats,\n                                activeColumns,\n                                self.predictedState['t-1'],\n                                self.confidence['t-1'])\n\n\n\n    # Get the output\n    output = self.computeOutput()\n\n    # Print diagnostic information based on the current verbosity level\n    self.printComputeEnd(output, learn=False)\n\n    self.resetCalled = False\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef learn(self, bottomUpInput):\n\n    self.lrnIterationIdx = self.lrnIterationIdx + 1\n    self.iterationIdx = self.iterationIdx + 1\n    if self.verbosity >= 3:\n      print \"\\n==== Iteration: %d =====\" % (self.iterationIdx)\n      print \"Active cols:\", bottomUpInput.nonzero()[0]\n\n\n    if self.verbosity >= 4:\n      print len(self.segmentUpdates), \"updates\"\n      for k,v in self.segmentUpdates.iteritems():\n        print 'cell:', k[0] * self.cellsPerColumn + k[1],\n        for vv in v:\n          print 'seg:', vv[1].segment,\n          print 'timeStamp:', vv[0],\n          print '/ src cells:', vv[1].activeSynapses\n\n    # Copy t into t-1\n    # This time also copy over learnState.\n    self.activeState['t-1'][:,:] = self.activeState['t'][:,:]\n    self.activeState['t'].fill(0)\n    self.predictedState['t-1'][:,:] = self.predictedState['t'][:,:]\n    self.predictedState['t'].fill(0)\n    self.learnState['t-1'][:,:] = self.learnState['t'][:,:]\n    self.learnState['t'][:,:] = 0\n    self.confidence['t-1'][:,:] = self.confidence['t'][:,:]\n    self.confidence['t'].fill(0.0)\n\n    # Update segment duty cycles if we are crossing a \"tier\"\n    # We determine if it's time to update the segment duty cycles. Since the\n    # duty cycle calculation is a moving average based on a tiered alpha, it is\n    # important that we update all segments on each tier boundary\n    if self.lrnIterationIdx in Segment.dutyCycleTiers:\n      for c, i in product(xrange(self.numberOfCols),\n                                    xrange(self.cellsPerColumn)):\n        for segment in self.cells[c][i]:\n          segment.dutyCycle()\n\n\n    # Phase 1: compute current state for each cell\n    # For each column (winning in the SP):\n    # - if the bottom up input was predicted by one of the sequence\n    #   segments of a cell, this cell bursts, (those cells also become\n    #   learning cells),\n    # - if the bottom up input was not predicted by any cell, the entire\n    #   column bursts, AND:\n    #   - if a cell had a sequence segment above minThreshold, it becomes\n    #     a learning cell, else\n    #   - we create a new sequence segment for that cell, and it also\n    #     becomes a learning cell.\n    activeColumns = bottomUpInput.nonzero()[0]\n    numUnpredictedColumns = 0\n\n\n    for c in activeColumns:\n\n      # todo: cache this list when building it in iteration at t\n      buPredicted = False  # Was this bottom up input predicted?\n      predictingCells = numpy.where(self.predictedState['t-1'][c] == 1)[0]\n\n      for i in predictingCells:\n        # Convert from Numpy integer structures, for debugging in Komodo\n        c,i = int(c), int(i)\n        buPredicted = True\n        self.activeState['t'][c,i] = 1\n        self.learnState['t'][c,i] = 1\n\n      # Turn on the active state of the whole column if the bottom up input was\n      # not predicted\n      if not buPredicted:\n        numUnpredictedColumns += 1\n\n        self.activeState['t'][c,:] = 1\n\n        # We didn't find a cell to learn on (since no cell was predicted), so\n        # just pick the best matching cell, or, failing that, the least allocated\n        # cell.\n        # sum(all synapses) >= minThreshold, \"weak\" activation\n        i,s = self.getBestMatchingCell(c,self.activeState['t-1'])\n\n        if s is not None:\n          s.totalActivations += 1      # activationFrequency\n          s.lastActiveIteration = self.iterationIdx\n        else:\n          # if best matching cell does not exist, then get least used cell\n          i = self.getLeastUsedCell(c)\n          s = None\n\n\n\n          # Update a possibly weakly matching segment\n          # todo: In here, should we only use learnState to check for\n          # a weak match?\n        self.learnState['t'][c,i] = 1\n        self.activeState['t'][c,i] = 1    # In case we are in PAM mode\n\n        # propose a list of synapse change\n        segUpdate = self.getSegmentActiveSynapses(c,i,s,'t-1',\n                            newSynapses = True)\n        segUpdate.phase1Flag = True\n        self.addToSegmentUpdates(c, i, segUpdate)\n\n    # ----------------------------------------------------------------------\n    # Phase 2: compute predicted state for each cell\n    # - if a segment has enough horizontal connections firing because of\n    #   bottomUpInput, it's set to be predicting, and we queue up the segment\n    #   for reinforcement,\n    self.computePhase2(doLearn=True)\n\n    # ----------------------------------------------------------------------\n    # Phase 3: update synapses for currently active cells (due to bottom up input)\n    # that have queued up segment updates, or cells that just stopped predicting\n    # (those cells that stopped predicting are _negatively_ reinforced).\n    # Also clean up queues by removing seg updates that are too old.\n    self.processSegmentUpdates()\n\n    # ----------------------------------------------------------------------\n    # Phase 4: Apply global decay, and remove synapses and/or segments.\n    # Synapses are removed if their permanence value is <= 0.\n    # Segments are removed when they don't have synapses anymore.\n    # Removal of synapses can trigger removal of whole segments!\n    # todo: isolate the synapse/segment retraction logic so that\n    # it can be called in adaptSegments, in the case where we\n    # do global decay only episodically.\n    if self.globalDecay > 0.0 and ((self.iterationIdx % self.maxAge) == 0):\n      for c, i in product(xrange(self.numberOfCols), xrange(self.cellsPerColumn)):\n\n        segsToDel = [] # collect and remove outside the loop\n        for segment in self.cells[c][i]:\n          age = self.iterationIdx - segment[0][self.kSegLastActiveIteration]\n          if age <= self.maxAge:\n            continue\n\n          #print \"Decrementing seg age %d:\" % (age), c, i, segment\n          synsToDel = [] # collect and remove outside the loop\n          for synapse in segment.syns: # skip sequenceSegment flag\n\n            synapse[2] = synapse[2] - self.globalDecay # decrease permanence\n\n            if synapse[2] <= 0:\n              synsToDel.append(synapse) # add to list to delete\n\n          if len(synsToDel) == len(segment.syns): # 1 for sequenceSegment flag\n            segsToDel.append(segment) # will remove the whole segment\n          elif len(synsToDel) > 0:\n            for syn in synsToDel: # remove some synapses on segment\n              segment.syns.remove(syn)\n\n        for seg in segsToDel: # remove some segments of this cell\n          self.cleanUpdatesList(c,i,seg)\n          self.cells[c][i].remove(seg)\n\n\n    # Update the prediction score stats\n    # Learning always includes inference\n    if self.collectStats:\n      self._updateStatsInferEnd(self._internalStats, activeColumns,\n                                self.predictedState['t-1'],\n                                self.confidence['t-1'])\n\n    # Finally return learning output\n    output = self.computeOutput()\n\n    # Print diagnostic information based on the current verbosity level\n    self.printComputeEnd(output, learn=True)\n\n    self.resetCalled = False\n\n    return self.computeOutput()", "response": "This function learns the segment and updates the state of the segment with the new values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef columnConfidences(self, cellConfidences=None):\n\n    if cellConfidences is None:\n      cellConfidences = self.confidence['t']\n\n    colConfidences = cellConfidences.sum(axis=1)\n\n    # Make the max column confidence 1.0\n    #colConfidences /= colConfidences.max()\n\n    return colConfidences", "response": "Compute the column confidences given the cell confidences."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trimSegments(self, minPermanence=None, minNumSyns=None):\n\n    # Fill in defaults\n    if minPermanence is None:\n      minPermanence = self.connectedPerm\n    if minNumSyns is None:\n      minNumSyns = self.activationThreshold\n\n    # Loop through all cells\n    totalSegsRemoved, totalSynsRemoved = 0, 0\n    for c,i in product(xrange(self.numberOfCols), xrange(self.cellsPerColumn)):\n\n      (segsRemoved, synsRemoved) = self.trimSegmentsInCell(colIdx=c, cellIdx=i,\n                segList=self.cells[c][i], minPermanence=minPermanence,\n                minNumSyns=minNumSyns)\n      totalSegsRemoved += segsRemoved\n      totalSynsRemoved += synsRemoved\n\n\n    return totalSegsRemoved, totalSynsRemoved", "response": "This method deletes all synapses that have less than minPermanence and all synapses that have less than minNumSyns remaining."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves any update that would be for the given col cellIdx segIdx.", "response": "def cleanUpdatesList(self, col, cellIdx, seg):\n    \"\"\"\n    Removes any update that would be for the given col, cellIdx, segIdx.\n    NOTE: logically, we need to do this when we delete segments, so that if\n    an update refers to a segment that was just deleted, we also remove\n    that update from the update list. However, I haven't seen it trigger\n    in any of the unit tests yet, so it might mean that it's not needed\n    and that situation doesn't occur, by construction.\n    todo: check if that situation occurs.\n    \"\"\"\n    for key, updateList in self.segmentUpdates.iteritems():\n      c,i = key[0], key[1]\n      if c == col and i == cellIdx:\n        for update in updateList:\n          if update[1].segment == seg:\n            self.removeSegmentUpdate(update)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isSegmentActive(self, seg, activeState):\n    # Computing in C - *much* faster\n    return isSegmentActive(seg.syns, activeState,\n                           self.connectedPerm, self.activationThreshold)", "response": "A segment is active if it has >= activationThreshold connected synapses that are active due to activeState."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a segmentUpdate data structure containing a list of proposed changes to segment s.", "response": "def getSegmentActiveSynapses(self, c,i,s, timeStep, newSynapses =False):\n\n    \"\"\"Return a segmentUpdate data structure containing a list of proposed changes\n    to segment s. Let activeSynapses be the list of active synapses where the\n    originating cells have their activeState output = 1 at time step t.\n    (This list is empty if s is None since the segment doesn't exist.)\n    newSynapses is an optional argument that defaults to false. If newSynapses\n    is true, then newSynapseCount - len(activeSynapses) synapses are added to\n    activeSynapses. These synapses are randomly chosen from the set of cells\n    that have learnState = 1 at timeStep.\"\"\"\n\n    activeSynapses = []\n    activeState = self.activeState[timeStep]\n\n    if s is not None: # s can be None, if adding a new segment\n\n      # Here we add *integers* to activeSynapses\n      activeSynapses = [idx for idx, syn in enumerate(s.syns) \\\n                        if activeState[syn[0], syn[1]]]\n\n    if newSynapses: # add a few more synapses\n\n      nSynapsesToAdd = self.newSynapseCount - len(activeSynapses)\n\n      # Here we add *pairs* (colIdx, cellIdx) to activeSynapses\n      activeSynapses += self.chooseCellsToLearnFrom(c,i,s, nSynapsesToAdd, timeStep)\n\n    # It's still possible that activeSynapses is empty, and this will\n    # be handled in addToSegmentUpdates\n\n    # NOTE: activeSynapses contains a mixture of integers and pairs of integers\n    # - integers are indices of synapses already existing on the segment,\n    #   that we will need to update.\n    # - pairs represent source (colIdx, cellIdx) of new synapses to create on the\n    #   segment\n    update = TM.SegmentUpdate(c, i, s, activeSynapses)\n\n    return update"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getActiveSegment(self, c, i, timeStep):\n\n    # todo: put back preference for sequence segments.\n\n    nSegments = len(self.cells[c][i])\n\n    bestActivation = self.activationThreshold\n    which = -1\n\n    for j,s in enumerate(self.cells[c][i]):\n\n      activity = self.getSegmentActivityLevel(s, self.activeState[timeStep], connectedSynapsesOnly = True)\n\n      if activity >= bestActivation:\n        bestActivation = activity\n        which = j\n\n    if which != -1:\n      return self.cells[c][i][which]\n    else:\n      return None", "response": "For a given cell return the active segment with the strongest activation of the connected synapses."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chooseCellsToLearnFrom(self, c,i,s, n, timeStep):\n\n    if n <= 0:\n      return []\n\n    tmpCandidates = [] # tmp because we'll refine just below with activeSynapses\n\n    if timeStep == 't-1':\n      tmpCandidates = numpy.where(self.learnState['t-1'] == 1)\n    else:\n      tmpCandidates = numpy.where(self.learnState['t'] == 1)\n\n    # Candidates can be empty at this point, in which case we return\n    # an empty segment list. adaptSegments will do nothing when getting\n    # that list.\n    if len(tmpCandidates[0]) == 0:\n      return []\n\n    if s is None: # new segment\n      cands = [syn for syn in zip(tmpCandidates[0], tmpCandidates[1])]\n    else:\n      # We exclude any synapse that is already in this segment.\n      synapsesAlreadyInSegment = set((syn[0], syn[1]) for syn in s.syns)\n      cands = [syn for syn in zip(tmpCandidates[0], tmpCandidates[1]) \\\n               if (syn[0], syn[1]) not in synapsesAlreadyInSegment]\n\n    if n == 1: # so that we don't shuffle if only one is needed\n      idx = self._random.getUInt32(len(cands))\n      return [cands[idx]]  # col and cell idx in col\n\n    # If we need more than one candidate\n    self._random.getUInt32(10) # this required to line RNG with C++ (??)\n    indices = array([j for j in range(len(cands))], dtype='uint32')\n    tmp = zeros(min(n, len(indices)), dtype='uint32')\n    self._random.getUInt32Sample(indices, tmp, True)\n    return [cands[j] for j in tmp]", "response": "Choose n random cells to learn from."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding weakly activated cell in column. Returns index and segment of most activated segment above minThreshold.", "response": "def getBestMatchingCell(self, c, activeState):\n    \"\"\"Find weakly activated cell in column. Returns index and segment of most\n    activated segment above minThreshold.\n    \"\"\"\n    # Collect all cells in column c that have at least minThreshold in the most\n    # activated segment\n    bestActivityInCol = self.minThreshold\n    bestSegIdxInCol = -1\n    bestCellInCol = -1\n\n    for i in xrange(self.cellsPerColumn):\n\n      maxSegActivity = 0\n      maxSegIdx = 0\n\n      for j,s in enumerate(self.cells[c][i]):\n\n        activity = self.getSegmentActivityLevel(s, activeState, connectedSynapsesOnly =False)\n\n        if self.verbosity >= 6:\n          print \" Segment Activity for column \", c, \" cell \", i, \" segment \", \" j is \", activity\n\n        if activity > maxSegActivity:\n          maxSegActivity = activity\n          maxSegIdx = j\n\n      if maxSegActivity >= bestActivityInCol:\n        bestActivityInCol = maxSegActivity\n        bestSegIdxInCol = maxSegIdx\n        bestCellInCol = i\n\n    if self.verbosity >= 6:\n      print \"Best Matching Cell In Col: \", bestCellInCol\n    if bestCellInCol == -1:\n      return (None, None)\n    else:\n      return bestCellInCol, self.cells[c][bestCellInCol][bestSegIdxInCol]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef updateSynapses(self, segment, synapses, delta):\n\n    reached0 = False\n\n    if delta > 0:\n      for synapse in synapses:\n        segment[synapse][2] = newValue = segment[synapse][2] + delta\n\n        # Cap synapse permanence at permanenceMax\n        if newValue > self.permanenceMax:\n          segment[synapse][2] = self.permanenceMax\n\n    else:\n      for synapse in synapses:\n        segment[synapse][2] = newValue = segment[synapse][2] + delta\n\n        # Cap min synapse permanence to 0 in case there is no global decay\n        if newValue < 0:\n          segment[synapse][2] = 0\n          reached0 = True\n\n    return reached0", "response": "Update a set of synapses of the given segment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef processSegmentUpdates(self):\n\n    # =================================================================\n    # The segmentUpdates dict has keys which are the column,cellIdx of the\n    #  owner cell. The values are lists of segment updates for that cell\n    removeKeys = []\n    trimSegments = []\n    for key, updateList in self.segmentUpdates.iteritems():\n\n      # Get the column number and cell index of the owner cell\n      c,i = key[0], key[1]\n\n      # Should we positively or negatively re-enforce the predicting synapses?\n      positiveReinforcement = None\n      if self.learnState['t'][c,i] == 1:\n        positiveReinforcement = True\n      elif self.predictedState['t'][c,i] == 0 \\\n                                and self.predictedState['t-1'][c,i] == 1:\n        positiveReinforcement = False\n\n      # Process each segment for this cell. Each segment entry contains\n      #  [creationDate, SegmentInfo]\n      updateListKeep = []\n      for (createDate, segUpdate) in updateList:\n\n        # Decide whether to apply the update now. If update has expired, then\n        # delete it. If update was created in Phase1 then apply immediately.\n        # Otherwise, if update was not created during this pass then apply now.\n\n        if self.iterationIdx - createDate > self.segUpdateValidDuration:\n          # This segment has expired. Ignore this update (and hence remove it from list)\n          pass\n        elif (segUpdate.phase1Flag or\n             self.iterationIdx > createDate) and (positiveReinforcement is not None):\n          trimSegment = self.adaptSegment(segUpdate, positiveReinforcement)\n          if trimSegment:\n            trimSegments.append((segUpdate.columnIdx, segUpdate.cellIdx,\n                                    segUpdate.segment))\n        else:\n          # Keep all updates that don't match the above criteria\n          updateListKeep.append((createDate,segUpdate))\n\n      self.segmentUpdates[key] = updateListKeep\n      if len(updateListKeep) == 0:\n        removeKeys.append(key)\n\n\n    # =====================================================================\n    # Clean out empty segment updates\n    for key in removeKeys:\n      self.segmentUpdates.pop(key)\n\n    # =====================================================================\n    # Trim segments that had synapses go to 0\n    for (c, i, segment) in trimSegments:\n      self.trimSegmentsInCell(c, i, [segment], minPermanence = 0.00001,\n              minNumSyns = 0)", "response": "This function processes the segment updates in the segmentUpdates dictionary and returns the list of segment updates that have been applied to the current iteration."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adaptSegment(self, segUpdate, positiveReinforcement):\n\n    # This will be set to True if detect that any syapses were decremented to\n    #  0\n    trimSegment = False\n\n    # segUpdate.segment is None when creating a new segment\n    c, i, segment = segUpdate.columnIdx, segUpdate.cellIdx, segUpdate.segment\n\n    # update.activeSynapses can be empty.\n    # If not, it can contain either or both integers and tuples.\n    # The integers are indices of synapses to update.\n    # The tuples represent new synapses to create (src col, src cell in col).\n    # We pre-process to separate these various element types.\n    # synToCreate is not empty only if positiveReinforcement is True.\n    # NOTE: the synapse indices start at *1* to skip the segment flags.\n    activeSynapses = segUpdate.activeSynapses\n    synToUpdate = set([syn for syn in activeSynapses if type(syn) == int])\n\n    if segment is not None: # modify an existing segment\n\n      if positiveReinforcement:\n\n        if self.verbosity >= 4:\n          print \"Reinforcing segment for cell[%d,%d]\" %(c,i),\n          segment.printSegment()\n\n        # Update frequency and positiveActivations\n        segment.positiveActivations += 1       # positiveActivations += 1\n        segment.dutyCycle(active=True)\n\n        # First, decrement synapses that are not active\n        # s is a synapse *index*, with index 0 in the segment being the tuple\n        # (segId, sequence segment flag). See below, creation of segments.\n        lastSynIndex = len(segment.syns) - 1\n        inactiveSynIndices = [s for s in xrange(0, lastSynIndex+1) \\\n                              if s not in synToUpdate]\n        trimSegment = segment.updateSynapses(inactiveSynIndices,\n                                           -self.permanenceDec)\n\n\n        # Now, increment active synapses\n        activeSynIndices = [syn for syn in synToUpdate if syn <= lastSynIndex]\n        segment.updateSynapses(activeSynIndices, self.permanenceInc)\n\n        # Finally, create new synapses if needed\n        # syn is now a tuple (src col, src cell)\n        synsToAdd = [syn for syn in activeSynapses if type(syn) != int]\n\n        for newSyn in synsToAdd:\n          segment.addSynapse(newSyn[0], newSyn[1], self.initialPerm)\n\n        if self.verbosity >= 4:\n          print \"            after\",\n          segment.printSegment()\n\n      else: # positiveReinforcement is False\n\n        desc = \"\"\n        if self.verbosity >= 4:\n          print \"Negatively Reinforcing %s segment for cell[%d,%d]\" \\\n                  % (desc, c,i),\n          segment.printSegment()\n\n        # Decrease frequency count\n        segment.dutyCycle(active=True)\n\n        # We decrement all the \"active\" that were passed in\n        trimSegment = segment.updateSynapses(synToUpdate,\n                                            -self.permanenceDec)\n\n        if self.verbosity >= 4:\n          print \"            after\",\n          segment.printSegment()\n\n    else: # segment is None: create a new segment\n\n      newSegment = Segment(tp=self, isSequenceSeg=segUpdate.sequenceSegment)\n\n\n      # numpy.float32 important so that we can match with C++\n      for synapse in activeSynapses:\n        newSegment.addSynapse(synapse[0], synapse[1], self.initialPerm)\n\n      if self.verbosity >= 3:\n        print \"New segment for cell[%d,%d]\" %(c,i),\n        newSegment.printSegment()\n\n      self.cells[c][i].append(newSegment)\n\n\n    return trimSegment", "response": "This function is used to adapt a segment in a cell into a new segment in a new cell."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the segment information for verbose messaging and debugging.", "response": "def printSegment(self):\n    \"\"\"Print segment information for verbose messaging and debugging.\n    This uses the following format:\n\n     ID:54413 True 0.64801 (24/36) 101 [9,1]0.75 [10,1]0.75 [11,1]0.75\n\n    where:\n      54413 - is the unique segment id\n      True - is sequence segment\n      0.64801 - moving average duty cycle\n      (24/36) - (numPositiveActivations / numTotalActivations)\n      101 - age, number of iterations since last activated\n      [9,1]0.75 - synapse from column 9, cell #1, strength 0.75\n      [10,1]0.75 - synapse from column 10, cell #1, strength 0.75\n      [11,1]0.75 - synapse from column 11, cell #1, strength 0.75\n    \"\"\"\n    # Segment ID\n    print \"ID:%-5d\" % (self.segID),\n\n    # Sequence segment or pooling segment\n    if self.isSequenceSeg:\n      print \"True\",\n    else:\n      print \"False\",\n\n    # Duty cycle\n    print \"%9.7f\" % (self.dutyCycle(readOnly=True)),\n\n    # numPositive/totalActivations\n    print \"(%4d/%-4d)\" % (self.positiveActivations,\n                          self.totalActivations),\n\n    # Age\n    print \"%4d\" % (self.tp.lrnIterationIdx - self.lastActiveIteration),\n\n    # Print each synapses on this segment as: srcCellCol/srcCellIdx/perm\n    # if the permanence is above connected, put [] around the synapse info\n    # For aid in comparing to the C++ implementation, print them in sorted\n    #  order\n    sortedSyns = sorted(self.syns)\n    for _, synapse in enumerate(sortedSyns):\n      print \"[%d,%d]%4.2f\" % (synapse[0], synapse[1], synapse[2]),\n    print"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef freeNSynapses(self, numToFree, inactiveSynapseIndices, verbosity= 0):\n    # Make sure numToFree isn't larger than the total number of syns we have\n    assert (numToFree <= len(self.syns))\n\n    if (verbosity >= 4):\n      print \"\\nIn PY freeNSynapses with numToFree =\", numToFree,\n      print \"inactiveSynapseIndices =\",\n      for i in inactiveSynapseIndices:\n        print self.syns[i][0:2],\n      print\n\n    # Remove the lowest perm inactive synapses first\n    if len(inactiveSynapseIndices) > 0:\n      perms = numpy.array([self.syns[i][2] for i in inactiveSynapseIndices])\n      candidates = numpy.array(inactiveSynapseIndices)[\n          perms.argsort()[0:numToFree]]\n      candidates = list(candidates)\n    else:\n      candidates = []\n\n    # Do we need more? if so, remove the lowest perm active synapses too\n    if len(candidates) < numToFree:\n      activeSynIndices = [i for i in xrange(len(self.syns))\n                          if i not in inactiveSynapseIndices]\n      perms = numpy.array([self.syns[i][2] for i in activeSynIndices])\n      moreToFree = numToFree - len(candidates)\n      moreCandidates = numpy.array(activeSynIndices)[\n          perms.argsort()[0:moreToFree]]\n      candidates += list(moreCandidates)\n\n    if verbosity >= 4:\n      print \"Deleting %d synapses from segment to make room for new ones:\" % (\n          len(candidates)), candidates\n      print \"BEFORE:\",\n      self.printSegment()\n\n    # Free up all the candidates now\n    synsToDelete = [self.syns[i] for i in candidates]\n    for syn in synsToDelete:\n      self.syns.remove(syn)\n\n    if verbosity >= 4:\n      print \"AFTER:\",\n      self.printSegment()", "response": "Free up some synapses in this segment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the synapses of the current TP object with the given number of synapses. Returns True if synapse reached 0 otherwise.", "response": "def updateSynapses(self, synapses, delta):\n    \"\"\"Update a set of synapses in the segment.\n\n    @param tp       The owner TP\n    @param synapses List of synapse indices to update\n    @param delta    How much to add to each permanence\n\n    @returns   True if synapse reached 0\n    \"\"\"\n    reached0 = False\n\n    if delta > 0:\n      for synapse in synapses:\n        self.syns[synapse][2] = newValue = self.syns[synapse][2] + delta\n\n        # Cap synapse permanence at permanenceMax\n        if newValue > self.tp.permanenceMax:\n          self.syns[synapse][2] = self.tp.permanenceMax\n\n    else:\n      for synapse in synapses:\n        self.syns[synapse][2] = newValue = self.syns[synapse][2] + delta\n\n        # Cap min synapse permanence to 0 in case there is no global decay\n        if newValue <= 0:\n          self.syns[synapse][2] = 0\n          reached0 = True\n\n    return reached0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maxEntropy(n,k):\n\n  s = float(k)/n\n  if s > 0.0 and s < 1.0:\n    entropy = - s * math.log(s,2) - (1 - s) * math.log(1 - s,2)\n  else:\n    entropy = 0\n\n  return n*entropy", "response": "Returns the maximum enropy we could get with n units and k winners."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef binaryEntropy(x):\n  entropy = - x*x.log2() - (1-x)*(1-x).log2()\n  entropy[x*(1 - x) == 0] = 0\n  return entropy, entropy.sum()", "response": "Calculate the entropy of a list of binary random variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating plot showing histogram of duty cycles.", "response": "def plotDutyCycles(dutyCycle, filePath):\n  \"\"\"\n  Create plot showing histogram of duty cycles\n\n  :param dutyCycle: (torch tensor) the duty cycle of each unit\n  :param filePath: (str) Full filename of image file\n  \"\"\"\n  _,entropy = binaryEntropy(dutyCycle)\n  bins = np.linspace(0.0, 0.3, 200)\n  plt.hist(dutyCycle, bins, alpha=0.5, label='All cols')\n  plt.title(\"Histogram of duty cycles, entropy=\" + str(float(entropy)))\n  plt.xlabel(\"Duty cycle\")\n  plt.ylabel(\"Number of units\")\n  plt.savefig(filePath)\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef computeMaxPool(input_width):\n  wout = math.floor((input_width + 2 * PADDING - KERNEL_SIZE) / STRIDE + 1)\n  return int(math.floor(wout / 2.0))", "response": "Compute max pool width. see cnn_sdr. py"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef provideObjectsToLearn(self, objectNames=None):\n    if objectNames is None:\n      objectNames = self.objects.keys()\n\n    objects = {}\n    for name in objectNames:\n      objects[name] = [self._getSDRPairs([pair] * self.numColumns) \\\n                       for pair in self.objects[name]]\n\n    self._checkObjectsToLearn(objects)\n    return objects", "response": "Returns the objects in a canonical format to be sent to an experiment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef provideObjectToInfer(self, inferenceConfig):\n    numSteps = inferenceConfig.get(\"numSteps\",\n                                   len(inferenceConfig[\"pairs\"][0]))\n\n    # some checks\n    if numSteps == 0:\n      raise ValueError(\"No inference steps were provided\")\n    for col in xrange(self.numColumns):\n      if len(inferenceConfig[\"pairs\"][col]) != numSteps:\n        raise ValueError(\"Incompatible numSteps and actual inference steps\")\n\n    sensationSteps = []\n    for step in xrange(numSteps):\n      pairs = [\n        inferenceConfig[\"pairs\"][col][step] for col in xrange(self.numColumns)\n      ]\n      sdrPairs = self._getSDRPairs(\n        pairs,\n        noise=inferenceConfig.get(\"noiseLevel\", None),\n        locationNoise=inferenceConfig.get(\"locationNoise\", None),\n        includeRandomLocation=inferenceConfig.get(\"includeRandomLocation\",\n                                                  False),\n        numAmbiguousLocations=inferenceConfig.get(\"numAmbiguousLocations\", 0))\n      sensationSteps.append(sdrPairs)\n\n    self._checkObjectToInfer(sensationSteps)\n    return sensationSteps", "response": "Returns the sensations in a canonical format to be sent to an experiment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an object to the Machine.", "response": "def addObject(self, pairs, name=None):\n    \"\"\"\n    Adds an object to the Machine.\n    \"\"\"\n    if name is None:\n      name = len(self.objects)\n\n    self.objects[name] = pairs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a set of random objects and adds them to the internal list.", "response": "def createRandomObjects(self,\n                          numObjects,\n                          numPoints,\n                          numLocations=None,\n                          numFeatures=None):\n    \"\"\"\n    Creates a set of random objects and adds them to the machine.\n    If numLocations and numFeatures and not specified, they will be set to the\n    desired number of points.\n    \"\"\"\n    if numObjects > 0:\n      if numLocations is None:\n        numLocations = numPoints\n      if numFeatures is None:\n        numFeatures = numPoints\n\n      assert(numPoints <= numLocations), (\"Number of points in object cannot be \"\n            \"greater than number of locations\")\n\n      locationArray = numpy.array(range(numLocations))\n      numpy.random.seed(self.seed)\n      for _ in xrange(numObjects):\n        # Permute the number of locations and select points from it\n        locationArray = numpy.random.permutation(locationArray)\n        self.addObject(\n          [(locationArray[p],\n            numpy.random.randint(0, numFeatures)) for p in xrange(numPoints)],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getDistinctPairs(self):\n    distinctPairs = set()\n    for pairs in self.objects.itervalues():\n      distinctPairs = distinctPairs.union(set(pairs))\n    return distinctPairs", "response": "Returns a set consisting of unique feature and location pairs across all objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getUniqueFeaturesLocationsInObject(self, name):\n    uniqueFeatures = set()\n    uniqueLocations = set()\n    for pair in self.objects[name]:\n      uniqueLocations = uniqueLocations.union({pair[0]})\n      uniqueFeatures = uniqueFeatures.union({pair[1]})\n\n    return uniqueLocations, uniqueFeatures", "response": "Returns two sets containing the unique locations Ids in the object. The first set contains the unique feature Ids in the object. The second set contains the unique locations Ids in the object. The first set contains the unique feature Ids in the object. The second set contains the unique feature Ids in the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getSDRPairs(self,\n                   pairs,\n                   noise=None,\n                   locationNoise=None,\n                   includeRandomLocation=False,\n                   numAmbiguousLocations=0):\n    \"\"\"\n    This method takes a list of (location, feature) index pairs (one pair per\n    cortical column), and returns a sensation dict in the correct format,\n    adding noise if necessary.\n    \"\"\"\n    sensations = {}\n    numpy.random.seed(self.seed)\n    for col in xrange(self.numColumns):\n      locationID, featureID = pairs[col]\n\n      # generate random location if requested\n      if includeRandomLocation:\n        location = self._generatePattern(self.numInputBits,\n                                         self.externalInputSize)\n\n      elif numAmbiguousLocations > 0:\n        location = self.locations[col][locationID]\n        for _ in range(numAmbiguousLocations):\n          idx = numpy.random.randint(len(self.locations[col]))\n          location = location | self.locations[col][idx]\n\n      # generate union of locations if requested\n      elif isinstance(locationID, tuple):\n        location = set()\n        for idx in list(locationID):\n          location = location | self.locations[col][idx]\n      else:\n        location = self.locations[col][locationID]\n\n      # generate empty feature if requested\n      if featureID == -1:\n        feature = set()\n      # generate union of features if requested\n      elif isinstance(featureID, tuple):\n        feature = set()\n        for idx in list(featureID):\n          feature = feature | self.features[col][idx]\n      else:\n        feature = self.features[col][featureID]\n\n      # Add random noise if requested\n      if noise is not None:\n        feature = self._addNoise(\n          feature, noise, self.sensorInputSize)\n      if locationNoise is not None:\n        location = self._addNoise(\n          location, locationNoise, self.externalInputSize)\n\n      sensations[col] = (location, feature)\n\n    return sensations", "response": "This method generates a sensation dictionary for a list of location and feature indices."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generateLocations(self):\n    size = self.externalInputSize\n    bits = self.numInputBits\n    random.seed(self.seed)\n    self.locations = []\n    for _ in xrange(self.numColumns):\n      self.locations.append(\n        [self._generatePattern(bits, size) for _ in xrange(self.numLocations)]\n      )", "response": "Generates a pool of locations for each SDR."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getDetectorClassConstructors(detectors):\n  detectorConstructors = {\n  d : globals()[detectorNameToClass(d)] for d in detectors}\n\n  return detectorConstructors", "response": "Returns a dict mapping detector name to class names."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nselects the best action for the given state using e - greedy exploration to minimize overfitting", "response": "def select_action(self, state):\n    \"\"\"\n    Select the best action for the given state using e-greedy exploration to\n    minimize overfitting\n\n    :return: tuple(action, value)\n    \"\"\"\n    value = 0\n    if self.steps < self.min_steps:\n      action = np.random.randint(self.actions)\n    else:\n      self.eps = max(self.eps_end, self.eps * self.eps_decay)\n      if random.random() < self.eps:\n        action = np.random.randint(self.actions)\n      else:\n        self.local.eval()\n        with torch.no_grad():\n          state = torch.tensor(state, device=self.device, dtype=torch.float).unsqueeze(0)\n          Q = self.local(state)\n          value, action = torch.max(Q, 1)\n\n    return int(action), float(value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hard_target_update(self):\n    if self.steps % self.target_update == 0:\n      self.target.load_state_dict(self.local.state_dict())", "response": "Update model parameters every target_update time steps\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef soft_target_update(self):\n    for target_param, local_param in zip(self.target.parameters(), self.local.parameters()):\n      target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)", "response": "Soft update model parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef learn(self, state, action, reward, next_state, done):\n    self.steps += 1\n    self.replay.append((state, action, reward, next_state, done))\n    if self.steps > self.min_steps and len(self.replay) > self.batch_size:\n      batch = random.sample(self.replay, self.batch_size)\n      return self.optimize(batch)\n\n    return None", "response": "Learn a random variety of experiences from the replay buffer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlearning each provided object in the object tree.", "response": "def learnObjects(self, bodyPlacement):\n    \"\"\"\n    Learn each provided object.\n\n    This method simultaneously learns 4 sets of synapses:\n    - sensor-to-specific-object -> input\n    - input -> sensor-to-specific-object\n    - input -> object\n    - object -> input\n    \"\"\"\n    for monitor in self.monitors.itervalues():\n      monitor.afterBodyWorldLocationChanged(bodyPlacement)\n\n    for objectName, objectFeatures in self.objects.iteritems():\n      self.reset()\n\n      objectPlacement = self.objectPlacements[objectName]\n\n      for module in self.bodyToSpecificObjectModules:\n        module.activateRandomLocation()\n\n      for iFeatureStart in xrange(len(objectFeatures)):\n        featureIndexByColumn = np.mod(np.arange(iFeatureStart,\n                                                iFeatureStart +\n                                                self.numCorticalColumns),\n                                      len(objectFeatures))\n\n        featureByColumn = [objectFeatures[iFeature]\n                           for iFeature in featureIndexByColumn]\n        featureSDRByColumn = [self.features[(iCol, feature[\"name\"])]\n                              for iCol, feature in enumerate(featureByColumn)]\n        locationOnObjectByColumn = np.array(\n          [[feature[\"top\"] + feature[\"height\"]/2,\n            feature[\"left\"] + feature[\"width\"]/2]\n           for feature in featureByColumn])\n\n        worldLocationByColumn = objectPlacement + locationOnObjectByColumn\n        egocentricLocationByColumn = worldLocationByColumn - bodyPlacement\n\n        for monitor in self.monitors.itervalues():\n          monitor.afterSensorWorldLocationChanged(worldLocationByColumn)\n\n        for t in xrange(2):\n          for monitor in self.monitors.itervalues():\n            monitor.beforeCompute(egocentricLocationByColumn, featureSDRByColumn,\n                                  isRepeat=(t > 0))\n          self.compute(egocentricLocationByColumn, featureSDRByColumn,\n                       learn=True)\n\n        for iFeature, locationOnObject, c in zip(featureIndexByColumn,\n                                                 locationOnObjectByColumn,\n                                                 self.corticalColumns):\n          locationOnObject = tuple(locationOnObject.tolist())\n          c.sensorLocationRepresentations[(objectName, locationOnObject)] = (\n            c.getSensorToSpecificObjectSDR())\n          c.inputRepresentations[(objectName, locationOnObject,\n                                  objectFeatures[iFeature][\"name\"])] = (\n                                    c.inputLayer.getActiveCells())\n          c.objectRepresentations[objectName] = c.objectLayer.getActiveCells()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inferObjects(self, bodyPlacement, maxTouches=2):\n    for monitor in self.monitors.itervalues():\n      monitor.afterBodyWorldLocationChanged(bodyPlacement)\n\n    numTouchesRequired = collections.defaultdict(int)\n    for objectName, objectFeatures in self.objects.iteritems():\n      self.reset()\n\n      objectPlacement = self.objectPlacements[objectName]\n\n      featureIndexByColumnIterator = (\n        greedySensorPositions(self.numCorticalColumns, len(objectFeatures)))\n\n      for touch in xrange(maxTouches):\n        # Choose where to place each sensor.\n        featureIndexByColumn = featureIndexByColumnIterator.next()\n        sensedFeatures = [objectFeatures[i] for i in featureIndexByColumn]\n        featureSDRByColumn = [self.features[(iCol, feature[\"name\"])]\n                              for iCol, feature in enumerate(sensedFeatures)]\n        worldLocationByColumn = np.array([\n          [objectPlacement[0] + feature[\"top\"] + feature[\"height\"]/2,\n           objectPlacement[1] + feature[\"left\"] + feature[\"width\"]/2]\n          for feature in sensedFeatures])\n\n        for monitor in self.monitors.itervalues():\n          monitor.afterSensorWorldLocationChanged(worldLocationByColumn)\n\n        egocentricLocationByColumn = worldLocationByColumn - bodyPlacement\n\n        prevCellActivity = None\n\n        for t in xrange(self.maxSettlingTime):\n          for monitor in self.monitors.itervalues():\n            monitor.beforeCompute(egocentricLocationByColumn, featureSDRByColumn,\n                                  isRepeat=(t > 0))\n          self.compute(egocentricLocationByColumn, featureSDRByColumn, learn=False)\n\n          cellActivity = (\n            tuple(c.getAllCellActivity()\n                  for c in self.corticalColumns),\n            tuple(set(module.activeCells)\n                  for module in self.bodyToSpecificObjectModules))\n\n          if cellActivity == prevCellActivity:\n            # It settled. Cancel logging this timestep.\n            for monitor in self.monitors.itervalues():\n              monitor.clearUnflushedData()\n            break\n          else:\n            prevCellActivity = cellActivity\n            for monitor in self.monitors.itervalues():\n              monitor.flush()\n\n        # Check if the object is narrowed down\n        if self.isObjectClassified(objectName):\n          numTouchesRequired[touch + 1] += 1\n          break\n      else:\n        numTouchesRequired[None] += 1\n\n    return numTouchesRequired", "response": "Infer objects from a body with multiple sensors twice."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload Thing sensation data from a log file.", "response": "def loadThingData(dataDir=\"data\", n=150, w=11):\n  \"\"\"\n  Load Thing sensation data. There is one file per object, each row contains one\n  feature, location pairs. The format is as follows:\n    [(-33.6705, 75.5003, 2.4207)/10] => [[list of active bits of location],\n                                         [list of active bits of feature]]\n  The content before \"=>\" is the true 3D location / sensation\n  We ignore the encoded values after \"=>\" and use :class:`ScalarEncoder` to\n  encode the sensation in a way that is compatible with the experiment network.\n\n  :param dataDir: The location data files\n  :type dataDir: str\n  :param n: The number of bits in the feature SDR. Usually L4 column count\n  :type n: int\n  :param w: Number of 'on' bits in the feature SDR. Usually L4 sample size\n  :type w: int\n  :return: Dictionary mapping objects to sensations that can be used directly by\n           class L246aNetwork 'infer' and 'learn' methods\n  :rtype: dict[str,list]\n  \"\"\"\n  objects = defaultdict(list)\n\n  # Thing features are scalar values ranging from 1-25 inclusive\n  encoder = ScalarEncoder(n=n, w=w, minval=1, maxval=25, forced=True)\n\n  dataPath = os.path.dirname(os.path.realpath(__file__))\n  dataPath = os.path.join(dataPath, dataDir)\n  objFiles = glob.glob1(dataPath, \"*.log\")\n\n  for filename in objFiles:\n    obj, _ = os.path.splitext(filename)\n\n    # Read raw sensations from log file. Ignore SDRs after \"=>\"\n    sensations = []\n    with open(os.path.join(dataPath, filename)) as f:\n      for line in f.readlines():\n        # Parse raw location/feature values\n        line = line.split(\"=>\")[0].translate(None, \"[,]()\")\n        locationStr, featureStr = line.split(\"/\")\n        location = map(float, locationStr.split())\n        feature = encoder.encode(int(featureStr)).nonzero()[0].tolist()\n\n        sensations.append((location, feature))\n\n    # Assume single column\n    objects[obj] = [sensations]\n\n  return objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresetting the object to the initial state", "response": "def reset(self, params, repetition):\n    \"\"\"\n    Take the steps necessary to reset the experiment before each repetition:\n      - Make sure random seed is different for each repetition\n      - Create the L2-L4-L6a network\n      - Load objects used by the experiment\n      - Learn all objects used by the experiment\n    \"\"\"\n    print params[\"name\"], \":\", repetition\n    self.debug = params.get(\"debug\", False)\n    self.numLearningPoints = params[\"num_learning_points\"]\n    self.numOfSensations = params[\"num_sensations\"]\n\n    L2Params = json.loads('{' + params[\"l2_params\"] + '}')\n    L4Params = json.loads('{' + params[\"l4_params\"] + '}')\n    L6aParams = json.loads('{' + params[\"l6a_params\"] + '}')\n\n    self.sdrSize = L2Params[\"sdrSize\"]\n\n    # Make sure random seed is different for each repetition\n    seed = params.get(\"seed\", 42)\n    np.random.seed(seed + repetition)\n    random.seed(seed + repetition)\n    L2Params[\"seed\"] = seed + repetition\n    L4Params[\"seed\"] = seed + repetition\n    L6aParams[\"seed\"] = seed + repetition\n\n    # Configure L6a params\n    numModules = params[\"num_modules\"]\n    L6aParams[\"scale\"] = [params[\"scale\"]] * numModules\n    angle = params[\"angle\"] / numModules\n    orientation = range(angle / 2, angle * numModules, angle)\n    L6aParams[\"orientation\"] = np.radians(orientation).tolist()\n    L6aParams[\"cellsPerAxis\"] = params[\"cells_per_axis\"]\n\n    # Create single column L2-L4-L6a network\n    self.network = L246aNetwork(numColumns=1, L2Params=L2Params,\n                                L4Params=L4Params, L6aParams=L6aParams,\n                                repeat=self.numLearningPoints,\n                                logCalls=self.debug)\n\n    # Load Thing Objects\n    sampleSize = L4Params[\"sampleSize\"]\n    columnCount = L4Params[\"columnCount\"]\n\n    # Make sure w is odd per encoder requirement\n    sampleSize = sampleSize if sampleSize % 2 != 0 else sampleSize + 1\n\n    self.objects = loadThingData(dataDir=params[\"data_path\"],\n                                 w=sampleSize, n=columnCount)\n\n    # Number of iterations must match the number of objects. This will allow us\n    # to execute one iteration per object and use the \"iteration\" parameter as\n    # the object index\n    assert params[\"iterations\"] == len(self.objects)\n\n    # Learn objects\n    self.network.learn(self.objects)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_initialization_experiment(seed,\n                                  num_neurons = 50,\n                                  dim = 40,\n                                  num_bins = 10,\n                                  num_samples = 50*600,\n                                  neuron_size = 10000,\n                                  num_dendrites = 400,\n                                  dendrite_length = 25,\n                                  power = 10,\n                                  ):\n  \"\"\"\n  Runs an experiment testing classifying a binary dataset, based on Poirazi &\n  Mel's original experiment.  Learning is using our modified variant of their\n  rule, and positive and negative neurons compete to classify a datapoint.\n\n  Performance has historically been poor, noticeably worse than what is\n  achieved with only a single neuron using an HTM-style learning rule on\n  datasets of similar size.  It is suspected that the simplifications made\n  to the P&M learning rule are having a negative effect.\n\n  Furthermore, P&M report that they are willing to train for an exceptional\n  amount of time, up to 96,000 iterations per neuron.  We have never even\n  begun to approach this long a training time, so it is possible that our\n  performance would converge with theirs given more time.\n\n  This experiment does not correspond to a figure in the paper, but we report\n  our results across an average of 50 trials, using the settings above.\n  \"\"\"\n\n  numpy.random.seed(seed)\n\n  nonlinearity = power_nonlinearity(power)\n  pos_neurons = [Neuron(size = neuron_size, num_dendrites = num_dendrites, dendrite_length = dendrite_length, nonlinearity = nonlinearity, dim = dim*num_bins) for i in range(num_neurons/2)]\n  neg_neurons = [Neuron(size = neuron_size, num_dendrites = num_dendrites, dendrite_length = dendrite_length, nonlinearity = nonlinearity, dim = dim*num_bins) for i in range(num_neurons/2)]\n  #pos, neg = generate_evenly_distributed_data_sparse(dim = 400, num_active = 40, num_samples = num_samples/2), generate_evenly_distributed_data_sparse(dim = 400, num_active = 40, num_samples = num_samples/2)\n  pos, neg = generate_data(dim = dim, num_bins = num_bins, num_samples = num_samples, sparse = True)\n\n  if (pos.nRows() > num_dendrites*len(pos_neurons)):\n    print \"Too much data to have unique dendrites for positive neurons, clustering\"\n    pos = pos.toDense()\n    model = KMeans(n_clusters = len(pos_neurons), n_jobs=1)\n    clusters = model.fit_predict(pos)\n    neuron_data = [SM32() for i in range(len(pos_neurons))]\n    for datapoint, cluster in zip(pos, clusters):\n      neuron_data[cluster].append(SM32([datapoint]))\n    for i, neuron in enumerate(pos_neurons):\n      neuron.HTM_style_initialize_on_data(neuron_data[i], [1 for i in range(neuron_data[i].nRows())])\n    pos = SM32(pos)\n  else:\n    print \"Directly initializing positive neurons with unique dendrites\"\n    neuron_data = split_sparse_matrix(pos, len(pos_neurons))\n    for neuron, data in zip(pos_neurons, neuron_data):\n      neuron.HTM_style_initialize_on_data(data, [1 for i in range(data.nRows())])\n\n\n  if (neg.nRows() > num_dendrites*len(neg_neurons)):\n    print \"Too much data to have unique dendrites for negative neurons, clustering\"\n    neg = neg.toDense()\n    model = KMeans(n_clusters = len(neg_neurons), n_jobs=1)\n    clusters = model.fit_predict(neg)\n    neuron_data = [SM32() for i in range(len(neg_neurons))]\n    for datapoint, cluster in zip(neg, clusters):\n      neuron_data[cluster].append(SM32([datapoint]))\n    for i, neuron in enumerate(neg_neurons):\n      neuron.HTM_style_initialize_on_data(neuron_data[i], [1 for i in range(neuron_data[i].nRows())])\n    neg = SM32(neg)\n\n  else:\n    print \"Directly initializing negative neurons with unique dendrites\"\n    neuron_data = split_sparse_matrix(neg, len(neg_neurons))\n    for neuron, data in zip(neg_neurons, neuron_data):\n      neuron.HTM_style_initialize_on_data(data, [1 for i in range(data.nRows())])\n\n\n  print \"Calculating error\"\n  labels = [1 for i in range(pos.nRows())] + [-1 for i in range(neg.nRows())]\n  data = pos\n  data.append(neg)\n\n  error, fp, fn = get_error(data, labels, pos_neurons, neg_neurons)\n  print \"Error at initialization is {}, with {} false positives and {} false negatives\".format(error, fp, fn)\n  return error", "response": "Runs an experiment testing classifying a binary dataset based on Poirazi s original experiment."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the error of the given data.", "response": "def get_error(data, labels, pos_neurons, neg_neurons = [], add_noise = True):\n  \"\"\"\n  Calculates error, including number of false positives and false negatives.\n\n  Written to allow the use of multiple neurons, in case we attempt to use a\n  population in the future.\n\n  \"\"\"\n  num_correct = 0\n  num_false_positives = 0\n  num_false_negatives = 0\n  classifications = numpy.zeros(data.nRows())\n  for neuron in pos_neurons:\n    classifications += neuron.calculate_on_entire_dataset(data)\n  for neuron in neg_neurons:\n    classifications -= neuron.calculate_on_entire_dataset(data)\n  if add_noise:\n    classifications += (numpy.random.rand() - 0.5)/1000\n  classifications = numpy.sign(classifications)\n  for classification, label in zip(classifications, labels):\n    if classification > 0 and label > 0:\n      num_correct += 1.0\n    elif classification <= 0 and label <= 0:\n      num_correct += 1.0\n    elif classification > 0 and label <= 0:\n      num_false_positives += 1\n    else:\n      num_false_negatives += 1\n  return (1.*num_false_positives + num_false_negatives)/data.nRows(), num_false_positives, num_false_negatives"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns experiment. What did you think this does? args is a dict representing the parameters. We do it this way to support multiprocessing. args contains one or more of the following keys: @param featureNoise (float) Noise level to add to the features during inference. Default: None @param locationNoise (float) Noise level to add to the locations during inference. Default: None @param numObjects (int) The number of objects we will train. Default: 10 @param numPoints (int) The number of points on each object. Default: 10 @param numLocations (int) For each point, the number of locations to choose from. Default: 10 @param numFeatures (int) For each point, the number of features to choose from. Default: 10 @param numColumns (int) The total number of cortical columns in network. Default: 2 @param networkType (string)The type of network to use. Options are: \"MultipleL4L2Columns\", \"MultipleL4L2ColumnsWithTopology\" and \"MultipleL4L2ColumnsWithRandomTopology\". Default: \"MultipleL4L2Columns\" @param longDistanceConnections (float) The probability that a column will connect to a distant column. Only relevant when using the random topology network type. If > 1, will instead be taken as desired number of long-distance connections per column. @param settlingTime (int) Number of iterations we wait to let columns stabilize. Important for multicolumn experiments with lateral connections. @param includeRandomLocation (bool) If True, a random location SDR will be generated during inference for each feature. @param enableFeedback (bool) If True, enable feedback, default is True @param numAmbiguousLocations (int) number of ambiguous locations. Ambiguous locations will present during inference if this parameter is set to be a positive number The method returns the args dict updated with multiple additional keys representing accuracy metrics.", "response": "def runExperiment(args):\n  \"\"\"\n  Run experiment.  What did you think this does?\n\n  args is a dict representing the parameters. We do it this way to support\n  multiprocessing. args contains one or more of the following keys:\n\n  @param featureNoise (float) Noise level to add to the features\n                             during inference. Default: None\n  @param locationNoise (float) Noise level to add to the locations\n                             during inference. Default: None\n  @param numObjects  (int)   The number of objects we will train.\n                             Default: 10\n  @param numPoints   (int)   The number of points on each object.\n                             Default: 10\n  @param numLocations (int)  For each point, the number of locations to choose\n                             from.  Default: 10\n  @param numFeatures (int)   For each point, the number of features to choose\n                             from.  Default: 10\n  @param numColumns  (int)   The total number of cortical columns in network.\n                             Default: 2\n  @param networkType (string)The type of network to use.  Options are:\n                             \"MultipleL4L2Columns\",\n                             \"MultipleL4L2ColumnsWithTopology\" and\n                             \"MultipleL4L2ColumnsWithRandomTopology\".\n                             Default: \"MultipleL4L2Columns\"\n  @param longDistanceConnections (float) The probability that a column will\n                             connect to a distant column.  Only relevant when\n                             using the random topology network type.\n                             If > 1, will instead be taken as desired number\n                             of long-distance connections per column.\n  @param settlingTime (int)  Number of iterations we wait to let columns\n                             stabilize. Important for multicolumn experiments\n                             with lateral connections.\n  @param includeRandomLocation (bool) If True, a random location SDR will be\n                             generated during inference for each feature.\n  @param enableFeedback (bool) If True, enable feedback, default is True\n  @param numAmbiguousLocations (int) number of ambiguous locations. Ambiguous\n                             locations will present during inference if this\n                             parameter is set to be a positive number\n\n  The method returns the args dict updated with multiple additional keys\n  representing accuracy metrics.\n  \"\"\"\n  numObjects = args.get(\"numObjects\", 10)\n  numLocations = args.get(\"numLocations\", 10)\n  numFeatures = args.get(\"numFeatures\", 10)\n  numColumns = args.get(\"numColumns\", 2)\n  networkType = args.get(\"networkType\", \"MultipleL4L2Columns\")\n  longDistanceConnections = args.get(\"longDistanceConnections\", 0)\n  locationNoise = args.get(\"locationNoise\", 0.0)\n  featureNoise = args.get(\"featureNoise\", 0.0)\n  numPoints = args.get(\"numPoints\", 10)\n  trialNum = args.get(\"trialNum\", 42)\n  plotInferenceStats = args.get(\"plotInferenceStats\", True)\n  settlingTime = args.get(\"settlingTime\", 3)\n  includeRandomLocation = args.get(\"includeRandomLocation\", False)\n  enableFeedback = args.get(\"enableFeedback\", True)\n  numAmbiguousLocations = args.get(\"numAmbiguousLocations\", 0)\n  numInferenceRpts = args.get(\"numInferenceRpts\", 1)\n  l2Params = args.get(\"l2Params\", None)\n  l4Params = args.get(\"l4Params\", None)\n\n  # Create the objects\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=150,\n    externalInputSize=2400,\n    numCorticalColumns=numColumns,\n    numFeatures=numFeatures,\n    numLocations=numLocations,\n    seed=trialNum\n  )\n\n  objects.createRandomObjects(numObjects, numPoints=numPoints,\n                                    numLocations=numLocations,\n                                    numFeatures=numFeatures)\n\n  r = objects.objectConfusion()\n  print \"Average common pairs in objects=\", r[0],\n  print \", locations=\",r[1],\", features=\",r[2]\n\n  # print \"Total number of objects created:\",len(objects.getObjects())\n  # print \"Objects are:\"\n  # for o in objects:\n  #   pairs = objects[o]\n  #   pairs.sort()\n  #   print str(o) + \": \" + str(pairs)\n\n  # Setup experiment and train the network\n  name = \"convergence_O%03d_L%03d_F%03d_C%03d_T%03d\" % (\n    numObjects, numLocations, numFeatures, numColumns, trialNum\n  )\n  exp = L4L2Experiment(\n    name,\n    numCorticalColumns=numColumns,\n    L2Overrides=l2Params,\n    L4Overrides=l4Params,\n    networkType = networkType,\n    longDistanceConnections=longDistanceConnections,\n    inputSize=150,\n    externalInputSize=2400,\n    numInputBits=20,\n    seed=trialNum,\n    enableFeedback=enableFeedback,\n  )\n\n  exp.learnObjects(objects.provideObjectsToLearn())\n\n  # For inference, we will check and plot convergence for each object. For each\n  # object, we create a sequence of random sensations for each column.  We will\n  # present each sensation for settlingTime time steps to let it settle and\n  # ensure it converges.\n  numCorrectClassifications=0\n  classificationPerSensation = numpy.zeros(settlingTime*numPoints)\n  for objectId in objects:\n    exp.sendReset()\n\n    obj = objects[objectId]\n    objectSensations = {}\n    for c in range(numColumns):\n      objectSensations[c] = []\n\n    if numColumns > 1:\n      # Create sequence of random sensations for this object for all columns At\n      # any point in time, ensure each column touches a unique loc,feature pair\n      # on the object.  It is ok for a given column to sense a loc,feature pair\n      # more than once. The total number of sensations is equal to the number of\n      # points on the object.\n      for sensationNumber in range(len(obj)):\n        # Randomly shuffle points for each sensation\n        objectCopy = [pair for pair in obj]\n        random.shuffle(objectCopy)\n        for c in range(numColumns):\n          # stay multiple steps on each sensation\n          for _ in xrange(settlingTime):\n            objectSensations[c].append(objectCopy[c])\n\n    else:\n      # Create sequence of sensations for this object for one column. The total\n      # number of sensations is equal to the number of points on the object. No\n      # point should be visited more than once.\n      objectCopy = [pair for pair in obj]\n      random.shuffle(objectCopy)\n      for pair in objectCopy:\n        # stay multiple steps on each sensation\n        for _ in xrange(settlingTime):\n          objectSensations[0].append(pair)\n\n    inferConfig = {\n      \"object\": objectId,\n      \"numSteps\": len(objectSensations[0]),\n      \"pairs\": objectSensations,\n      \"noiseLevel\": featureNoise,\n      \"locationNoise\": locationNoise,\n      \"includeRandomLocation\": includeRandomLocation,\n      \"numAmbiguousLocations\": numAmbiguousLocations,\n    }\n\n    inferenceSDRs = objects.provideObjectToInfer(inferConfig)\n\n    exp.infer(inferenceSDRs, objectName=objectId, reset=False)\n\n    classificationPerSensation += numpy.array(\n      exp.statistics[objectId][\"Correct classification\"])\n\n    if exp.isObjectClassified(objectId, minOverlap=30):\n      numCorrectClassifications += 1\n\n    if plotInferenceStats:\n      exp.plotInferenceStats(\n        fields=[\"L2 Representation\",\n                \"Overlap L2 with object\",\n                \"L4 Representation\"],\n        experimentID=objectId,\n        onePlot=False,\n      )\n\n\n  convergencePoint, accuracy = exp.averageConvergencePoint(\"L2 Representation\",\n                                                 30, 40, settlingTime)\n  classificationAccuracy = float(numCorrectClassifications) / numObjects\n  classificationPerSensation = classificationPerSensation / numObjects\n\n  print \"# objects {} # features {} # locations {} # columns {} trial # {} network type {}\".format(\n    numObjects, numFeatures, numLocations, numColumns, trialNum, networkType)\n  print \"Average convergence point=\",convergencePoint\n  print \"Classification accuracy=\",classificationAccuracy\n  print\n\n  # Return our convergence point as well as all the parameters and objects\n  args.update({\"objects\": objects.getObjects()})\n  args.update({\"convergencePoint\":convergencePoint})\n  args.update({\"classificationAccuracy\":classificationAccuracy})\n  args.update({\"classificationPerSensation\":classificationPerSensation.tolist()})\n\n  # Can't pickle experiment so can't return it for batch multiprocessing runs.\n  # However this is very useful for debugging when running in a single thread.\n  if plotInferenceStats:\n    args.update({\"experiment\": exp})\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef runExperimentPool(numObjects,\n                      numLocations,\n                      numFeatures,\n                      numColumns,\n                      longDistanceConnectionsRange = [0.0],\n                      numWorkers=7,\n                      nTrials=1,\n                      numPoints=10,\n                      locationNoiseRange=[0.0],\n                      featureNoiseRange=[0.0],\n                      enableFeedback=[True],\n                      ambiguousLocationsRange=[0],\n                      numInferenceRpts=1,\n                      settlingTime=3,\n                      l2Params=None,\n                      l4Params=None,\n                      resultsName=\"convergence_results.pkl\"):\n  \"\"\"\n  Allows you to run a number of experiments using multiple processes.\n  For each parameter except numWorkers, pass in a list containing valid values\n  for that parameter. The cross product of everything is run, and each\n  combination is run nTrials times.\n\n  Returns a list of dict containing detailed results from each experiment.\n  Also pickles and saves the results in resultsName for later analysis.\n\n  Example:\n    results = runExperimentPool(\n                          numObjects=[10],\n                          numLocations=[5],\n                          numFeatures=[5],\n                          numColumns=[2,3,4,5,6],\n                          numWorkers=8,\n                          nTrials=5)\n  \"\"\"\n  # Create function arguments for every possibility\n  args = []\n\n  for c in reversed(numColumns):\n    for o in reversed(numObjects):\n      for l in numLocations:\n        for f in numFeatures:\n          for p in longDistanceConnectionsRange:\n            for t in range(nTrials):\n              for locationNoise in locationNoiseRange:\n                for featureNoise in featureNoiseRange:\n                  for ambiguousLocations in ambiguousLocationsRange:\n                    for feedback in enableFeedback:\n                      args.append(\n                        {\"numObjects\": o,\n                         \"numLocations\": l,\n                         \"numFeatures\": f,\n                         \"numColumns\": c,\n                         \"trialNum\": t,\n                         \"numPoints\": numPoints,\n                         \"longDistanceConnections\" : p,\n                         \"plotInferenceStats\": False,\n                         \"locationNoise\": locationNoise,\n                         \"featureNoise\": featureNoise,\n                         \"enableFeedback\": feedback,\n                         \"numAmbiguousLocations\": ambiguousLocations,\n                         \"numInferenceRpts\": numInferenceRpts,\n                         \"l2Params\": l2Params,\n                         \"l4Params\": l4Params,\n                         \"settlingTime\": settlingTime,\n                         }\n              )\n  numExperiments = len(args)\n  print \"{} experiments to run, {} workers\".format(numExperiments, numWorkers)\n  # Run the pool\n  if numWorkers > 1:\n    pool = Pool(processes=numWorkers)\n    rs = pool.map_async(runExperiment, args, chunksize=1)\n    while not rs.ready():\n      remaining = rs._number_left\n      pctDone = 100.0 - (100.0*remaining) / numExperiments\n      print \"    =>\", remaining, \"experiments remaining, percent complete=\",pctDone\n      time.sleep(5)\n    pool.close()  # No more work\n    pool.join()\n    result = rs.get()\n  else:\n    result = []\n    for arg in args:\n      result.append(runExperiment(arg))\n\n  # print \"Full results:\"\n  # pprint.pprint(result, width=150)\n\n  # Pickle results for later use\n  with open(resultsName,\"wb\") as f:\n    cPickle.dump(result,f)\n\n  return result", "response": "This function runs the experiment pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef euclideanDistance(instance1, instance2, considerDimensions):\n  distance = 0\n  for x in considerDimensions:\n    distance += pow((instance1[x] - instance2[x]), 2)\n  return math.sqrt(distance)", "response": "Calculate Euclidean Distance between two samples\n"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getNeighbors(trainingSet, testInstance, k, considerDimensions=None):\n  if considerDimensions is None:\n    considerDimensions = len(testInstance) - 1\n\n  neighborList = []\n  for x in range(len(trainingSet)):\n    dist = euclideanDistance(testInstance, trainingSet[x], considerDimensions)\n    neighborList.append((trainingSet[x], dist))\n  neighborList.sort(key=operator.itemgetter(1))\n\n  neighbors = []\n  distances = []\n  for x in range(k):\n    neighbors.append(neighborList[x][0])\n    distances.append(neighborList[x][1])\n\n  return neighbors", "response": "get neighbors for a given trainingSet and testInstance"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getResponse(neighbors, weights=None):\n  neighborResponse = []\n  for x in range(len(neighbors)):\n    neighborResponse.append(neighbors[x][-1])\n\n  neighborResponse = np.array(neighborResponse).astype('float')\n  if weights is None:\n    weightedAvg = np.mean(neighborResponse)\n  else:\n    weightedAvg = np.sum(weights * neighborResponse)\n\n  return weightedAvg", "response": "Calculates weighted response based on a list of nearest neighbors\n"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalizeSequence(sequence, considerDimensions=None):\n  seq = np.array(sequence).astype('float64')\n  nSampleDim = seq.shape[1]\n\n  if considerDimensions is None:\n    considerDimensions = range(nSampleDim)\n\n  for dim in considerDimensions:\n    seq[:, dim] = (seq[:, dim] - np.mean(seq[:, dim])) / np.std(seq[:, dim])\n\n  sequence = seq.tolist()\n  return sequence", "response": "normalize sequence by subtracting the mean and\n"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef numenta_local_wmax_extended(self, x):\n    pooler = self\n    W      = pooler.connections.visible_to_hidden\n    H      = pooler.connections.hidden_to_hidden\n    b      = pooler.connections.hidden_bias\n    n, m   = pooler.output_size, pooler.input_size\n    w      = pooler.code_weight\n    s      = pooler.sparsity\n    y      = np.zeros(n)\n\n    a      = pooler.average_activity\n\n    scores = np.exp(-b)*np.dot(W,x)\n\n    for i in range(n): \n\n        estimated_activity = 0.\n        for j in range(n):\n\n            # a_ij = a[i,j]/a[i,i]\n            a_ij = H[i,j]\n\n            if scores[j] >= scores[i]:\n                estimated_activity += a_ij\n\n        if estimated_activity < s:\n            y[i] = 1.\n\n    return y", "response": "r Extends the local w - max overlap procedure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive an input array x it returns its associated encoding y that is the local energy minimum of the hidden units while the visible units are clampled to x.", "response": "def energy_minimum(self, x):\n    \"\"\"\n    Given an input array `x` it returns its associated encoding `y(x)`, that is, \n    a stable configuration (local energy minimum) of the hidden units \n    while the visible units are clampled to `x`.\n    \"\"\"\n    E      = self.energy\n    y_min  = self.find_energy_minimum(E, x)\n    return y_min"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef computeActivity(self, activeInputsBySource, permanenceThreshold=None):\n    overlaps = None\n\n    for source, connections in self.connectionsBySource.iteritems():\n      o = connections.computeActivity(activeInputsBySource[source],\n                                      permanenceThreshold)\n      if overlaps is None:\n        overlaps = o\n      else:\n        overlaps += o\n\n    return overlaps", "response": "Computes the number of active synapses per source."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createSegments(self, cells):\n    segments = None\n\n    for connections in self.connectionsBySource.itervalues():\n      created = connections.createSegments(cells)\n\n      if segments is None:\n        segments = created\n      else:\n        # Sanity-check that the segment numbers are the same.\n        np.testing.assert_equal(segments, created)\n\n    return segments", "response": "Create a segment on each of the specified cells."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngrowing synapses to each of the specified inputs on each segment.", "response": "def growSynapses(self, segments, activeInputsBySource, initialPermanence):\n    \"\"\"\n    Grow synapses to each of the specified inputs on each specified segment.\n\n    @param segments (numpy array)\n    The segments that should add synapses\n\n    @param activeInputsBySource (dict)\n    The active cells in each source. Example:\n      {\"customInputName1\": np.array([42, 69])}\n\n    @param initialPermanence (float)\n    \"\"\"\n    for source, connections in self.connectionsBySource.iteritems():\n      connections.growSynapses(segments, activeInputsBySource[source],\n                               initialPermanence)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setPermanences(self, segments, presynapticCellsBySource, permanence):\n    permanences = np.repeat(np.float32(permanence), len(segments))\n\n    for source, connections in self.connectionsBySource.iteritems():\n      if source in presynapticCellsBySource:\n        connections.matrix.setElements(segments, presynapticCellsBySource[source],\n                                       permanences)", "response": "This method sets the permanence of a set of synapses."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loadImage(t, filename=\"cajal.jpg\"):\n  image = Image.open(\"cajal.jpg\").convert(\"1\")\n  image.load()\n  box = (0, 0, t.inputWidth, t.inputHeight)\n  image = image.crop(box)\n\n  # Here a will be a binary numpy array where True is white. Convert to floating\n  # point numpy array where white is 0.0\n  a = np.asarray(image)\n  im = np.ones((t.inputWidth, t.inputHeight))\n  im[a] = 0\n\n  return im", "response": "Load the given gray scale image. Threshold it to black and white crop it to be the dimensions of the FF input for the thalamus."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inferThalamus(t, l6Input, ffInput):\n  print(\"\\n-----------\")\n  t.reset()\n  t.deInactivateCells(l6Input)\n  ffOutput = t.computeFeedForwardActivity(ffInput)\n  # print(\"L6 input:\", l6Input)\n  # print(\"Active TRN cells: \", t.activeTRNCellIndices)\n  # print(\"Burst ready relay cells: \", t.burstReadyCellIndices)\n  return ffOutput", "response": "Infer the effect of this feed forward input given the specific L6 input."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filtered(w=250):\n  # prepare filter bank kernels\n  kernels = []\n  for theta in range(4):\n    theta = theta / 4. * np.pi\n    for sigma in (1, 3):\n      for frequency in (0.05, 0.25):\n        kernel = np.real(gabor_kernel(frequency, theta=theta,\n                                      sigma_x=sigma, sigma_y=sigma))\n        kernels.append(kernel)\n\n  print(\"Initializing thalamus\")\n\n  t = Thalamus(\n    trnCellShape=(w, w),\n    relayCellShape=(w, w),\n    inputShape=(w, w),\n    l6CellCount=128*128,\n    trnThreshold=15,\n  )\n\n\n  ff = loadImage(t)\n\n  for i,k in enumerate(kernels):\n    plotActivity(k, \"kernel\"+str(i)+\".jpg\", \"Filter kernel\", vmax=k.max(),\n                 vmin=k.min())\n    filtered0 = power(ff, k)\n    ft = np.zeros((w, w))\n    ft[filtered0 > filtered0.mean() + filtered0.std()] = 1.0\n    plotActivity(ft, \"filtered\"+str(i)+\".jpg\", \"Filtered image\", vmax=1.0)\n\n  encoder = createLocationEncoder(t, w=17)\n  trainThalamusLocations(t, encoder)\n\n  filtered0 = power(ff, kernels[3])\n  ft = np.zeros((w, w))\n  ft[filtered0 > filtered0.mean() + filtered0.std()] = 1.0\n\n  # Get a salt and pepper burst ready image\n  print(\"Getting unions\")\n  l6Code = list(getUnionLocations(encoder, 125, 125, 150, step=10))\n  print(\"Num active cells in L6 union:\", len(l6Code),\"out of\", t.l6CellCount)\n  ffOutput = inferThalamus(t, l6Code, ft)\n  plotActivity(t.burstReadyCells, \"relay_burstReady_filtered.jpg\",\n               title=\"Burst-ready cells\",\n               )\n  plotActivity(ffOutput, \"cajal_relay_output_filtered.jpg\",\n               title=\"Filtered activity\",\n               cmap=\"Greys\")\n\n  # Get a more detailed filtered image\n  print(\"Getting unions\")\n  l6Code = list(getUnionLocations(encoder, 125, 125, 150, step=3))\n  print(\"Num active cells in L6 union:\", len(l6Code),\"out of\", t.l6CellCount)\n  ffOutput_all = inferThalamus(t, l6Code, ff)\n  ffOutput_filtered = inferThalamus(t, l6Code, ft)\n  ffOutput3 = ffOutput_all*0.4 + ffOutput_filtered\n  plotActivity(t.burstReadyCells, \"relay_burstReady_all.jpg\",\n               title=\"Burst-ready cells\",\n               )\n  plotActivity(ffOutput3, \"cajal_relay_output_filtered2.jpg\",\n               title=\"Filtered activity\",\n               cmap=\"Greys\")", "response": "This example is used to filter the image into several channels using gabor filters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a delimited protobuf message from the given byte stream.", "response": "def get_delimited_message_bytes(byte_stream, nr=4):\n    ''' Parse a delimited protobuf message. This is done by first getting a protobuf varint from\n    the stream that represents the length of the message, then reading that amount of\n    from the message and then parse it.\n    Since the int can be represented as max 4 bytes, first get 4 bytes and try to decode.\n    The decoder returns the value and the position where the value was found, so we need\n    to rewind the buffer to the position, because the remaining bytes belong to the message\n    after.\n    '''\n\n    (length, pos) = decoder._DecodeVarint32(byte_stream.read(nr), 0)\n    if log.getEffectiveLevel() == logging.DEBUG:\n        log.debug(\"Delimited message length (pos %d): %d\" % (pos, length))\n\n    delimiter_bytes = nr - pos\n\n    byte_stream.rewind(delimiter_bytes)\n    message_bytes = byte_stream.read(length)\n    if log.getEffectiveLevel() == logging.DEBUG:\n        log.debug(\"Delimited message bytes (%d): %s\" % (len(message_bytes), format_bytes(message_bytes)))\n\n    total_len = length + pos\n    return (total_len, message_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, n):\n        '''Reads n bytes into the internal buffer'''\n        bytes_wanted = n - self.buffer_length + self.pos + 1\n        if bytes_wanted > 0:\n            self._buffer_bytes(bytes_wanted)\n\n        end_pos = self.pos + n\n        ret = self.buffer[self.pos + 1:end_pos + 1]\n        self.pos = end_pos\n        return ret", "response": "Reads n bytes into the internal buffer"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrewinds the current buffer to a position.", "response": "def rewind(self, places):\n        '''Rewinds the current buffer to a position. Needed for reading varints,\n        because we might read bytes that belong to the stream after the varint.\n        '''\n        log.debug(\"Rewinding pos %d with %d places\" % (self.pos, places))\n        self.pos -= places\n        log.debug(\"Reset buffer to pos %d\" % self.pos)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a socket connection to a given host and port and writes the Hadoop header and the Hadoop RPC protocol.", "response": "def get_connection(self, host, port):\n        '''Open a socket connection to a given host and port and writes the Hadoop header\n        The Hadoop RPC protocol looks like this when creating a connection:\n\n        +---------------------------------------------------------------------+\n        |  Header, 4 bytes (\"hrpc\")                                           |\n        +---------------------------------------------------------------------+\n        |  Version, 1 byte (default verion 9)                                 |\n        +---------------------------------------------------------------------+\n        |  RPC service class, 1 byte (0x00)                                   |\n        +---------------------------------------------------------------------+\n        |  Auth protocol, 1 byte (Auth method None = 0)                       |\n        +---------------------------------------------------------------------+\n        |  Length of the RpcRequestHeaderProto  + length of the               |\n        |  of the IpcConnectionContextProto (4 bytes/32 bit int)              |\n        +---------------------------------------------------------------------+\n        |  Serialized delimited RpcRequestHeaderProto                         |\n        +---------------------------------------------------------------------+\n        |  Serialized delimited IpcConnectionContextProto                     |\n        +---------------------------------------------------------------------+\n        '''\n\n        log.debug(\"############## CONNECTING ##############\")\n        # Open socket\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        self.sock.settimeout(self.sock_connect_timeout / 1000)\n        # Connect socket to server - defined by host and port arguments\n        self.sock.connect((host, port))\n        self.sock.settimeout(self.sock_request_timeout / 1000)\n\n        # Send RPC headers\n        self.write(self.RPC_HEADER)                             # header\n        self.write(struct.pack('B', self.version))              # version\n        self.write(struct.pack('B', self.RPC_SERVICE_CLASS))    # RPC service class\n        if self.use_sasl:\n            self.write(struct.pack('B', self.AUTH_PROTOCOL_SASL))   # serialization type (protobuf = 0xDF)\n        else:\n            self.write(struct.pack('B', self.AUTH_PROTOCOL_NONE))   # serialization type (protobuf = 0)\n\n        if self.use_sasl:\n            sasl = SaslRpcClient(self, hdfs_namenode_principal=self.hdfs_namenode_principal)\n            sasl_connected = sasl.connect()\n            if not sasl_connected:\n                raise TransientException(\"SASL is configured, but cannot get connected\")\n\n        rpc_header = self.create_rpc_request_header()\n        context = self.create_connection_context()\n\n        header_length = len(rpc_header) + encoder._VarintSize(len(rpc_header)) +len(context) + encoder._VarintSize(len(context))\n\n        if log.getEffectiveLevel() == logging.DEBUG:\n            log.debug(\"Header length: %s (%s)\" % (header_length, format_bytes(struct.pack('!I', header_length))))\n\n        self.write(struct.pack('!I', header_length))\n\n        self.write_delimited(rpc_header)\n        self.write_delimited(context)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_rpc_request_header(self):\n        '''Creates and serializes a delimited RpcRequestHeaderProto message.'''\n        rpcheader = RpcRequestHeaderProto()\n        rpcheader.rpcKind = 2  # rpcheaderproto.RpcKindProto.Value('RPC_PROTOCOL_BUFFER')\n        rpcheader.rpcOp = 0  # rpcheaderproto.RpcPayloadOperationProto.Value('RPC_FINAL_PACKET')\n        rpcheader.callId = self.call_id\n        rpcheader.retryCount = -1\n        rpcheader.clientId = self.client_id[0:16]\n\n        if self.call_id == -3:\n            self.call_id = 0\n        else:\n            self.call_id += 1\n\n        # Serialize delimited\n        s_rpcHeader = rpcheader.SerializeToString()\n        log_protobuf_message(\"RpcRequestHeaderProto (len: %d)\" % (len(s_rpcHeader)), rpcheader)\n        return s_rpcHeader", "response": "Creates and serializes a delimited RpcRequestHeaderProto message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and seriazlies a IpcConnectionContextProto ( not delimited )", "response": "def create_connection_context(self):\n        '''Creates and seriazlies a IpcConnectionContextProto (not delimited)'''\n        context = IpcConnectionContextProto()\n        context.userInfo.effectiveUser = self.effective_user\n        context.protocol = \"org.apache.hadoop.hdfs.protocol.ClientProtocol\"\n\n        s_context = context.SerializeToString()\n        log_protobuf_message(\"RequestContext (len: %d)\" % len(s_context), context)\n        return s_context"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_rpc_message(self, method, request):\n        '''Sends a Hadoop RPC request to the NameNode.\n\n        The IpcConnectionContextProto, RpcPayloadHeaderProto and HadoopRpcRequestProto\n        should already be serialized in the right way (delimited or not) before\n        they are passed in this method.\n\n        The Hadoop RPC protocol looks like this for sending requests:\n\n        When sending requests\n        +---------------------------------------------------------------------+\n        |  Length of the next three parts (4 bytes/32 bit int)                |\n        +---------------------------------------------------------------------+\n        |  Delimited serialized RpcRequestHeaderProto (varint len + header)   |\n        +---------------------------------------------------------------------+\n        |  Delimited serialized RequestHeaderProto (varint len + header)      |\n        +---------------------------------------------------------------------+\n        |  Delimited serialized Request (varint len + request)                |\n        +---------------------------------------------------------------------+\n        '''\n        log.debug(\"############## SENDING ##############\")\n\n        #0. RpcRequestHeaderProto\n        rpc_request_header = self.create_rpc_request_header()\n        #1. RequestHeaderProto\n        request_header = self.create_request_header(method)\n        #2. Param\n        param = request.SerializeToString()\n        if log.getEffectiveLevel() == logging.DEBUG:\n            log_protobuf_message(\"Request\", request)\n\n        rpc_message_length = len(rpc_request_header) + encoder._VarintSize(len(rpc_request_header)) + \\\n                             len(request_header) + encoder._VarintSize(len(request_header)) + \\\n                             len(param) + encoder._VarintSize(len(param))\n\n        if log.getEffectiveLevel() == logging.DEBUG:\n            log.debug(\"RPC message length: %s (%s)\" % (rpc_message_length, format_bytes(struct.pack('!I', rpc_message_length))))\n        self.write(struct.pack('!I', rpc_message_length))\n\n        self.write_delimited(rpc_request_header)\n        self.write_delimited(request_header)\n        self.write_delimited(param)", "response": "Sends a Hadoop RPC request to the NameNode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the length of the delimited part of the protobuf protobuf.", "response": "def get_length(self, byte_stream):\n        ''' In Hadoop protobuf RPC, some parts of the stream are delimited with protobuf varint,\n        while others are delimited with 4 byte integers. This reads 4 bytes from the byte stream\n        and retruns the length of the delimited part that follows, by unpacking the 4 bytes\n        and returning the first element from a tuple. The tuple that is returned from struc.unpack()\n        only contains one element.\n        '''\n        length = struct.unpack(\"!i\", byte_stream.read(4))[0]\n        log.debug(\"4 bytes delimited part length: %d\" % length)\n        return length"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_response(self, byte_stream, response_class):\n        '''Parses a Hadoop RPC response.\n\n        The RpcResponseHeaderProto contains a status field that marks SUCCESS or ERROR.\n        The Hadoop RPC protocol looks like the diagram below for receiving SUCCESS requests.\n        +-----------------------------------------------------------+\n        |  Length of the RPC resonse (4 bytes/32 bit int)           |\n        +-----------------------------------------------------------+\n        |  Delimited serialized RpcResponseHeaderProto              |\n        +-----------------------------------------------------------+\n        |  Serialized delimited RPC response                        |\n        +-----------------------------------------------------------+\n\n        In case of an error, the header status is set to ERROR and the error fields are set.\n        '''\n\n        log.debug(\"############## PARSING ##############\")\n        log.debug(\"Payload class: %s\" % response_class)\n\n        # Read first 4 bytes to get the total length\n        len_bytes = byte_stream.read(4)\n        total_length = struct.unpack(\"!I\", len_bytes)[0]\n        log.debug(\"Total response length: %s\" % total_length)\n\n        header = RpcResponseHeaderProto()\n        (header_len, header_bytes) = get_delimited_message_bytes(byte_stream)\n\n        log.debug(\"Header read %d\" % header_len)\n        header.ParseFromString(header_bytes)\n        log_protobuf_message(\"RpcResponseHeaderProto\", header)\n\n        if header.status == 0:\n            log.debug(\"header: %s, total: %s\" % (header_len, total_length))\n            if header_len >= total_length:\n                return\n            response = response_class()\n            response_bytes = get_delimited_message_bytes(byte_stream, total_length - header_len)[1]\n            if len(response_bytes) > 0:\n                response.ParseFromString(response_bytes)\n                if log.getEffectiveLevel() == logging.DEBUG:\n                    log_protobuf_message(\"Response\", response)\n                return response\n        else:\n            self.handle_error(header)", "response": "Parses a Hadoop RPC response."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the socket and resets the channel.", "response": "def close_socket(self):\n        '''Closes the socket and resets the channel.'''\n        log.debug(\"Closing socket\")\n        if self.sock:\n            try:\n                self.sock.close()\n            except:\n                pass\n\n            self.sock = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling the RPC method. The naming doesn t confirm PEP8 since it s a method called by protobuf", "response": "def CallMethod(self, method, controller, request, response_class, done):\n        '''Call the RPC method. The naming doesn't confirm PEP8, since it's\n        a method called by protobuf\n        '''\n        try:\n            self.validate_request(request)\n\n            if not self.sock:\n                self.get_connection(self.host, self.port)\n\n            self.send_rpc_message(method, request)\n\n            byte_stream = self.recv_rpc_message()\n            return self.parse_response(byte_stream, response_class)\n        except RequestError:  # Raise a request error, but don't close the socket\n            raise\n        except Exception:  # All other errors close the socket\n            self.close_socket()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getLogger(name):\n    ''' Create and return a logger with the specified name. '''\n\n    # Create logger and add a default NULL handler\n    log = logging.getLogger(name)\n    log.addHandler(_NullHandler())\n\n    return log", "response": "Create and return a logger with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if any directory arguments contain hdfs://", "response": "def _read_config_cl(self):\n        ''' Check if any directory arguments contain hdfs://'''\n        dirs_to_check = self.__get_all_directories()\n        hosts, ports = [], []\n        for path in dirs_to_check:\n            if path.startswith('hdfs://'):\n                parse_result = urlparse(path)\n                hosts.append(parse_result.hostname)\n                ports.append(parse_result.port)\n\n        # remove duplicates and None from (hosts + self.args.namenode)\n        hosts = filter(lambda x: x != None, set(hosts + [self.args.namenode]))\n        if len(hosts) > 1:\n            print_error_exit('Conficiting namenode hosts in commandline arguments, hosts: %s' % str(hosts))\n\n        ports = filter(lambda x: x != None, set(ports + [self.args.port]))\n        if len(ports) > 1:\n            print_error_exit('Conflicting namenode ports in commandline arguments, ports: %s' % str(ports))\n\n        # Store port from CL in arguments - CL port has the highest priority\n        if len(ports) == 1:\n            self.args.port = ports[0]\n\n        # do we agree on one namenode?\n        if len(hosts) == 1 and len(ports) <= 1:\n            self.args.namenode = hosts[0]\n            self.args.port = ports[0] if len(ports) == 1 else Namenode.DEFAULT_PORT\n            self.namenodes.append(Namenode(self.args.namenode, self.args.port))\n            # we got the info from CL -> check if use_trash is set - if not use default policy:\n            if self.__usetrash_unset():\n                self.args.usetrash = self.configs['use_trash']\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting configuration out of environment.", "response": "def get_config_from_env(cls):\n        \"\"\"\n        .. deprecated:: 2.5.3\n        Gets configuration out of environment.\n\n        Returns list of dicts - list of namenode representations\n        \"\"\"\n        core_path = os.path.join(os.environ['HADOOP_HOME'], 'conf', 'core-site.xml')\n        core_configs = cls.read_core_config(core_path)\n\n        hdfs_path = os.path.join(os.environ['HADOOP_HOME'], 'conf', 'hdfs-site.xml')\n        hdfs_configs = cls.read_hdfs_config(hdfs_path)\n\n        if (not core_configs) and (not hdfs_configs):\n            raise Exception(\"No config found in %s nor in %s\" % (core_path, hdfs_path))\n\n        configs = {\n            'use_trash': hdfs_configs.get('use_trash', core_configs.get('use_trash', False)),\n            'use_sasl': core_configs.get('use_sasl', False),\n            'hdfs_namenode_principal': hdfs_configs.get('hdfs_namenode_principal', None),\n            'namenodes': hdfs_configs.get('namenodes', []) or core_configs.get('namenodes', [])\n        }\n\n        return configs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuploads a file to HDFS", "response": "def put(self, src, dst):\n        '''Upload a file to HDFS\n\n        This will take a file from the ``testfiles_path`` supplied in the constuctor.\n        '''\n        src = \"%s%s\" % (self._testfiles_path, src)\n        return self._getStdOutCmd([self._hadoop_cmd, 'fs', '-put', src, self._full_hdfs_path(dst)], True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if <path > is a directory False otherwise", "response": "def is_directory(self, path):\n        \"\"\"Return True if <path> is a directory, False if it's NOT a directory\"\"\"\n        return self._getReturnCodeCmd([self._hadoop_cmd, 'fs', '-test', '-d', self._full_hdfs_path(path)]) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if <path > is a file False otherwise", "response": "def is_files(self, path):\n        \"\"\"Return True if <path> is a file, False if it's NOT a file\"\"\"\n        return self._getReturnCodeCmd([self._hadoop_cmd, 'fs', '-test', '-f', self._full_hdfs_path(path)]) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if file <path is greater than zero bytes in size False otherwise.", "response": "def is_greater_then_zero_bytes(self, path):\n        \"\"\"Return True if file <path> is greater than zero bytes in size, False otherwise\"\"\"\n        return self._getReturnCodeCmd([self._hadoop_cmd, 'fs', '-test', '-s', self._full_hdfs_path(path)]) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_zero_bytes_file(self, path):\n        return self._getReturnCodeCmd([self._hadoop_cmd, 'fs', '-test', '-z', self._full_hdfs_path(path)]) == 0", "response": "Return True if file <path is zero bytes in size otherwise return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ls(self, src, extra_args=[]):\n        '''List files in a directory'''\n        src = [self._full_hdfs_path(x) for x in src]\n        output = self._getStdOutCmd([self._hadoop_cmd, 'fs', '-ls'] + extra_args + src, True)\n        return self._transform_ls_output(output, self.hdfs_url)", "response": "List files in a directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming df on a path", "response": "def df(self, src):\n        '''Perform ``df`` on a path'''\n        return self._getStdOutCmd([self._hadoop_cmd, 'fs', '-df', self._full_hdfs_path(src)], True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef du(self, src, extra_args=[]):\n        '''Perform ``du`` on a path'''\n        src = [self._full_hdfs_path(x) for x in src]\n        return self._transform_du_output(self._getStdOutCmd([self._hadoop_cmd, 'fs', '-du'] + extra_args + src, True), self.hdfs_url)", "response": "Perform du on a path"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms count on a path", "response": "def count(self, src):\n        '''Perform ``count`` on a path'''\n        src = [self._full_hdfs_path(x) for x in src]\n        return self._transform_count_output(self._getStdOutCmd([self._hadoop_cmd, 'fs', '-count'] + src, True), self.hdfs_url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handleError(self, error_code, message):\n        '''Log and set the controller state.'''\n        self._fail = True\n        self.reason = error_code\n        self._error = message", "response": "Log and set the controller state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ls(self, paths, recurse=False, include_toplevel=False, include_children=True):\n        ''' Issues 'ls' command and returns a list of maps that contain fileinfo\n\n        :param paths: Paths to list\n        :type paths: list\n        :param recurse: Recursive listing\n        :type recurse: boolean\n        :param include_toplevel: Include the given path in the listing. If the path is a file, include_toplevel is always True.\n        :type include_toplevel: boolean\n        :param include_children: Include child nodes in the listing.\n        :type include_children: boolean\n        :returns: a generator that yields dictionaries\n\n        **Examples:**\n\n        Directory listing\n\n        >>> list(client.ls([\"/\"]))\n        [{'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1367317324982L, 'block_replication': 1, 'modification_time': 1367317325346L, 'length': 6783L, 'blocksize': 134217728L, 'owner': u'wouter', 'path': '/Makefile'}, {'group': u'supergroup', 'permission': 493, 'file_type': 'd', 'access_time': 0L, 'block_replication': 0, 'modification_time': 1367317325431L, 'length': 0L, 'blocksize': 0L, 'owner': u'wouter', 'path': '/build'}, {'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1367317326510L, 'block_replication': 1, 'modification_time': 1367317326522L, 'length': 100L, 'blocksize': 134217728L, 'owner': u'wouter', 'path': '/index.asciidoc'}, {'group': u'supergroup', 'permission': 493, 'file_type': 'd', 'access_time': 0L, 'block_replication': 0, 'modification_time': 1367317326628L, 'length': 0L, 'blocksize': 0L, 'owner': u'wouter', 'path': '/source'}]\n\n        File listing\n\n        >>> list(client.ls([\"/Makefile\"]))\n        [{'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'access_time': 1367317324982L, 'block_replication': 1, 'modification_time': 1367317325346L, 'length': 6783L, 'blocksize': 134217728L, 'owner': u'wouter', 'path': '/Makefile'}]\n\n        Get directory information\n\n        >>> list(client.ls([\"/source\"], include_toplevel=True, include_children=False))\n        [{'group': u'supergroup', 'permission': 493, 'file_type': 'd', 'access_time': 0L, 'block_replication': 0, 'modification_time': 1367317326628L, 'length': 0L, 'blocksize': 0L, 'owner': u'wouter', 'path': '/source'}]\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n\n        for item in self._find_items(paths, self._handle_ls,\n                                     include_toplevel=include_toplevel,\n                                     include_children=include_children,\n                                     recurse=recurse):\n            if item:\n                yield item", "response": "Issues a ls command and returns a list of maps that contain fileinfo\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling every node received for an ls request", "response": "def _handle_ls(self, path, node):\n        ''' Handle every node received for an ls request'''\n        entry = {}\n\n        entry[\"file_type\"] = self.FILETYPES[node.fileType]\n        entry[\"permission\"] = node.permission.perm\n        entry[\"path\"] = path\n\n        for attribute in self.LISTING_ATTRIBUTES:\n            entry[attribute] = node.__getattribute__(attribute)\n\n        return entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chmod(self, paths, mode, recurse=False):\n        ''' Change the mode for paths. This returns a list of maps containing the resut of the operation.\n\n        :param paths: List of paths to chmod\n        :type paths: list\n        :param mode: Octal mode (e.g. 0o755)\n        :type mode: int\n        :param recurse: Recursive chmod\n        :type recurse: boolean\n        :returns: a generator that yields dictionaries\n\n        .. note:: The top level directory is always included when `recurse=True`'''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"chmod: no path given\")\n        if not mode:\n            raise InvalidInputException(\"chmod: no mode given\")\n\n        processor = lambda path, node, mode=mode: self._handle_chmod(path, node, mode)\n        for item in self._find_items(paths, processor, include_toplevel=True,\n                                     include_children=False, recurse=recurse):\n            if item:\n                yield item", "response": "Change the mode for the given paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the owner for paths.", "response": "def chown(self, paths, owner, recurse=False):\n        ''' Change the owner for paths. The owner can be specified as `user` or `user:group`\n\n        :param paths: List of paths to chmod\n        :type paths: list\n        :param owner: New owner\n        :type owner: string\n        :param recurse: Recursive chown\n        :type recurse: boolean\n        :returns: a generator that yields dictionaries\n\n        This always include the toplevel when recursing.'''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"chown: no path given\")\n        if not owner:\n            raise InvalidInputException(\"chown: no owner given\")\n\n        processor = lambda path, node, owner=owner: self._handle_chown(path, node, owner)\n        for item in self._find_items(paths, processor, include_toplevel=True,\n                                     include_children=False, recurse=recurse):\n            if item:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncounts the number of files in a list of paths", "response": "def count(self, paths):\n        ''' Count files in a path\n\n        :param paths: List of paths to count\n        :type paths: list\n        :returns: a generator that yields dictionaries\n\n        **Examples:**\n\n        >>> list(client.count(['/']))\n        [{'spaceConsumed': 260185L, 'quota': 2147483647L, 'spaceQuota': 18446744073709551615L, 'length': 260185L, 'directoryCount': 9L, 'path': '/', 'fileCount': 34L}]\n\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"count: no path given\")\n\n        for item in self._find_items(paths, self._handle_count, include_toplevel=True,\n                                     include_children=False, recurse=False):\n            if item:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef df(self):\n        ''' Get FS information\n\n        :returns: a dictionary\n\n        **Examples:**\n\n        >>> client.df()\n        {'used': 491520L, 'capacity': 120137519104L, 'under_replicated': 0L, 'missing_blocks': 0L, 'filesystem': 'hdfs://localhost:8020', 'remaining': 19669295104L, 'corrupt_blocks': 0L}\n        '''\n        processor = lambda path, node: self._handle_df(path, node)\n        return list(self._find_items(['/'], processor, include_toplevel=True, include_children=False, recurse=False))[0]", "response": "Get FS information\n\n        :returns: a dictionary\n\n        **Examples:**\n\n        >>> client.df()\n        {'used': 491520L, 'capacity': 120137519104L, 'under_replicated': 0L, 'missing_blocks': 0L, 'filesystem': 'hdfs://localhost:8020', 'remaining': 19669295104L, 'corrupt_blocks': 0L}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef du(self, paths, include_toplevel=False, include_children=True):\n        '''Returns size information for paths\n\n        :param paths: Paths to du\n        :type paths: list\n        :param include_toplevel: Include the given path in the result. If the path is a file, include_toplevel is always True.\n        :type include_toplevel: boolean\n        :param include_children: Include child nodes in the result.\n        :type include_children: boolean\n        :returns: a generator that yields dictionaries\n\n        **Examples:**\n\n        Children:\n\n        >>> list(client.du(['/']))\n        [{'path': '/Makefile', 'length': 6783L}, {'path': '/build', 'length': 244778L}, {'path': '/index.asciidoc', 'length': 100L}, {'path': '/source', 'length': 8524L}]\n\n        Directory only:\n\n        >>> list(client.du(['/'], include_toplevel=True, include_children=False))\n        [{'path': '/', 'length': 260185L}]\n\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"du: no path given\")\n\n        processor = lambda path, node: self._handle_du(path, node)\n        for item in self._find_items(paths, processor, include_toplevel=include_toplevel,\n                                     include_children=include_children, recurse=False):\n            if item:\n                yield item", "response": "Returns size information for paths"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename(self, paths, dst):\n        ''' Rename (move) path(s) to a destination\n\n        :param paths: Source paths\n        :type paths: list\n        :param dst: destination\n        :type dst: string\n        :returns: a generator that yields dictionaries\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"rename: no path given\")\n        if not dst:\n            raise InvalidInputException(\"rename: no destination given\")\n\n        processor = lambda path, node, dst=dst: self._handle_rename(path, node, dst)\n        for item in self._find_items(paths, processor, include_toplevel=True):\n            if item:\n                yield item", "response": "Rename path to a destination"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rename2(self, path, dst, overwriteDest=False):\n        ''' Rename (but don't move) path to a destination\n\n        By only renaming, we mean that you can't move a file or folder out or\n        in other folder. The renaming can only happen within the folder the\n        file or folder lies in.\n\n        Note that this operation \"always succeeds\" unless an exception is\n        raised, hence, the dict returned from this function  doesn't have the\n        'result' key.\n\n        Since you can't move with this operation, and only rename, it would\n        not make sense to pass multiple paths to rename to a single\n        destination. This method uses the underlying rename2 method.\n\n        https://github.com/apache/hadoop/blob/ae91b13/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java#L483-L523\n\n        Out of all the different exceptions mentioned in the link above, this\n        method only wraps the FileAlreadyExistsException exception.  You will\n        also get a FileAlreadyExistsException if you have overwriteDest=True\n        and the destination folder is not empty. The other exceptions will just\n        be passed along.\n\n        :param path: Source path\n        :type path: string\n        :param dst: destination\n        :type dst: string\n        :returns: A dictionary or None\n        '''\n        if not path:\n            raise InvalidInputException(\"rename2: no path given\")\n        if not dst:\n            raise InvalidInputException(\"rename2: no destination given\")\n        if not isinstance(path, (str, unicode)):\n            raise InvalidInputException(\"rename2: Path should be a string\")\n\n        processor = lambda path, node, dst=dst, overwriteDest=overwriteDest: self._handle_rename2(path, node, dst, overwriteDest)\n        for item in self._find_items([path], processor, include_toplevel=True):\n            return item", "response": "Rename a file or folder or folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a set of items from the named node.", "response": "def delete(self, paths, recurse=False):\n        ''' Delete paths\n\n        :param paths: Paths to delete\n        :type paths: list\n        :param recurse: Recursive delete (use with care!)\n        :type recurse: boolean\n        :returns: a generator that yields dictionaries\n\n        .. note:: Recursive deletion uses the NameNode recursive deletion functionality\n                 instead of letting the client recurse. Hadoops client recurses\n                 by itself and thus showing all files and directories that are\n                 deleted. Snakebite doesn't.\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"delete: no path given\")\n\n        processor = lambda path, node, recurse=recurse: self._handle_delete(path, node, recurse)\n        for item in self._find_items(paths, processor, include_toplevel=True):\n            if item:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a directory :param paths: Paths to delete :type paths: list :returns: a generator that yields dictionaries .. note: directories have to be empty.", "response": "def rmdir(self, paths):\n        ''' Delete a directory\n\n        :param paths: Paths to delete\n        :type paths: list\n        :returns: a generator that yields dictionaries\n\n        .. note: directories have to be empty.\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"rmdir: no path given\")\n\n        processor = lambda path, node: self._handle_rmdir(path, node)\n        for item in self._find_items(paths, processor, include_toplevel=True):\n            if item:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef touchz(self, paths, replication=None, blocksize=None):\n        ''' Create a zero length file or updates the timestamp on a zero length file\n\n        :param paths: Paths\n        :type paths: list\n        :param replication: Replication factor\n        :type recurse: int\n        :param blocksize: Block size (in bytes) of the newly created file\n        :type blocksize: int\n        :returns: a generator that yields dictionaries\n        '''\n\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"touchz: no path given\")\n\n        # Let's get the blocksize and replication from the server defaults\n        # provided by the namenode if they are not specified\n        if not replication or not blocksize:\n            defaults = self.serverdefaults()\n\n        if not replication:\n            replication = defaults['replication']\n        if not blocksize:\n            blocksize = defaults['blockSize']\n\n        processor = lambda path, node, replication=replication, blocksize=blocksize: self._handle_touchz(path, node, replication, blocksize)\n        for item in self._find_items(paths, processor, include_toplevel=True, check_nonexistence=True, include_children=False):\n            if item:\n                yield item", "response": "Create a zero length file or updates the timestamp on a zero length file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setrep(self, paths, replication, recurse=False):\n        ''' Set the replication factor for paths\n\n        :param paths: Paths\n        :type paths: list\n        :param replication: Replication factor\n        :type recurse: int\n        :param recurse: Apply replication factor recursive\n        :type recurse: boolean\n        :returns: a generator that yields dictionaries\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"setrep: no path given\")\n        if not replication:\n            raise InvalidInputException(\"setrep: no replication given\")\n\n        processor = lambda path, node, replication=replication: self._handle_setrep(path, node, replication)\n        for item in self._find_items(paths, processor, include_toplevel=True,\n                                     include_children=False, recurse=recurse):\n            if item:\n                yield item", "response": "Set the replication factor for paths\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches all files that match the source file pattern and display their content on stdout.", "response": "def cat(self, paths, check_crc=False):\n        ''' Fetch all files that match the source file pattern\n        and display their content on stdout.\n\n        :param paths: Paths to display\n        :type paths: list of strings\n        :param check_crc: Check for checksum errors\n        :type check_crc: boolean\n        :returns: a generator that yields strings\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"cat: no path given\")\n\n        processor = lambda path, node, check_crc=check_crc: self._handle_cat(path, node, check_crc)\n        for item in self._find_items(paths, processor, include_toplevel=True,\n                                     include_children=False, recurse=False):\n            if item:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copyToLocal(self, paths, dst, check_crc=False):\n        ''' Copy files that match the file source pattern\n        to the local name.  Source is kept.  When copying multiple,\n        files, the destination must be a directory.\n\n        :param paths: Paths to copy\n        :type paths: list of strings\n        :param dst: Destination path\n        :type dst: string\n        :param check_crc: Check for checksum errors\n        :type check_crc: boolean\n        :returns: a generator that yields strings\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"copyToLocal: no path given\")\n        if not dst:\n            raise InvalidInputException(\"copyToLocal: no destination given\")\n\n        dst = self._normalize_path(dst)\n\n        processor = lambda path, node, dst=dst, check_crc=check_crc: self._handle_copyToLocal(path, node, dst, check_crc)\n        for path in paths:\n            self.base_source = None\n            for item in self._find_items([path], processor, include_toplevel=True, recurse=True, include_children=True):\n                if item:\n                    yield item", "response": "Copy files that match the file source pattern\n        to the local name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all the files that match the source file pattern and merge them to only one file on local fs.", "response": "def getmerge(self, path, dst, newline=False, check_crc=False):\n        ''' Get all the files in the directories that\n        match the source file pattern and merge and sort them to only\n        one file on local fs.\n\n        :param paths: Directory containing files that will be merged\n        :type paths: string\n        :param dst: Path of file that will be written\n        :type dst: string\n        :param nl: Add a newline character at the end of each file.\n        :type nl: boolean\n        :returns: string content of the merged file at dst\n        '''\n        if not path:\n            raise InvalidInputException(\"getmerge: no path given\")\n        if not dst:\n            raise InvalidInputException(\"getmerge: no destination given\")\n\n        temporary_target = \"%s._COPYING_\" % dst\n        f = open(temporary_target, 'w')\n\n        processor = lambda path, node, dst=dst, check_crc=check_crc: self._handle_getmerge(path, node, dst, check_crc)\n        try:\n            for item in self._find_items([path], processor, include_toplevel=True, recurse=False, include_children=True):\n                for load in item:\n                    if load['result']:\n                        f.write(load['response'])\n                    elif not load['error'] is '':\n                        if os.path.isfile(temporary_target):\n                            os.remove(temporary_target)\n                        raise FatalException(load['error'])\n                if newline and load['response']:\n                    f.write(\"\\n\")\n            yield {\"path\": dst, \"response\": '', \"result\": True, \"error\": load['error'], \"source_path\": path}\n\n        finally:\n            if os.path.isfile(temporary_target):\n                f.close()\n                os.rename(temporary_target, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stat(self, paths):\n        ''' Stat a fileCount\n\n        :param paths: Path\n        :type paths: string\n        :returns: a dictionary\n\n        **Example:**\n\n        >>> client.stat(['/index.asciidoc'])\n        {'blocksize': 134217728L, 'owner': u'wouter', 'length': 100L, 'access_time': 1367317326510L, 'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'path': '/index.asciidoc', 'modification_time': 1367317326522L, 'block_replication': 1}\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"stat: no path given\")\n\n        processor = lambda path, node: self._handle_stat(path, node)\n        return list(self._find_items(paths, processor, include_toplevel=True))[0]", "response": "Stat a fileCount\n\n        :param paths: Path\n        :type paths: string\n        :returns: a dictionary\n\n        **Example:**\n\n        >>> client.stat(['/index.asciidoc'])\n        {'blocksize': 134217728L, 'owner': u'wouter', 'length': 100L, 'access_time': 1367317326510L, 'group': u'supergroup', 'permission': 420, 'file_type': 'f', 'path': '/index.asciidoc', 'modification_time': 1367317326522L, 'block_replication': 1}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tail(self, path, tail_length=1024, append=False):\n        # Note: append is currently not implemented.\n        ''' Show the end of the file - default 1KB, supports up to the Hadoop block size.\n\n        :param path: Path to read\n        :type path: string\n        :param tail_length: The length to read from the end of the file - default 1KB, up to block size.\n        :type tail_length: int\n        :param append: Currently not implemented\n        :type append: bool\n        :returns: a generator that yields strings\n        '''\n        #TODO: Make tail support multiple files at a time, like most other methods do\n\n        if not path:\n            raise InvalidInputException(\"tail: no path given\")\n\n        block_size = self.serverdefaults()['blockSize']\n        if tail_length > block_size:\n            raise InvalidInputException(\"tail: currently supports length up to the block size (%d)\" % (block_size,))\n\n        if tail_length <= 0:\n            raise InvalidInputException(\"tail: tail_length cannot be less than or equal to zero\")\n\n        processor = lambda path, node: self._handle_tail(path, node, tail_length, append)\n        for item in self._find_items([path], processor, include_toplevel=True,\n                                     include_children=False, recurse=False):\n            if item:\n                yield item", "response": "Return a generator that yields the contents of the file at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a directoryCount :param paths: Paths to create :type paths: list of strings :param create_parent: Also create the parent directories :type create_parent: boolean :param mode: Mode the directory should be created with :type mode: int :returns: a generator that yields dictionaries", "response": "def mkdir(self, paths, create_parent=False, mode=0o755):\n        ''' Create a directoryCount\n\n        :param paths: Paths to create\n        :type paths: list of strings\n        :param create_parent: Also create the parent directories\n        :type create_parent: boolean\n        :param mode: Mode the directory should be created with\n        :type mode: int\n        :returns: a generator that yields dictionaries\n        '''\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"mkdirs: no path given\")\n\n        for path in paths:\n            if not path.startswith(\"/\"):\n                path = self._join_user_path(path)\n\n            fileinfo = self._get_file_info(path)\n            if not fileinfo:\n                try:\n                    request = client_proto.MkdirsRequestProto()\n                    request.src = path\n                    request.masked.perm = mode\n                    request.createParent = create_parent\n                    response = self.service.mkdirs(request)\n                    yield {\"path\": path, \"result\": response.result}\n                except RequestError as e:\n                    yield {\"path\": path, \"result\": False, \"error\": str(e)}\n            else:\n                yield {\"path\": path, \"result\": False, \"error\": \"mkdir: `%s': File exists\" % path}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets server defaults for the current user.", "response": "def serverdefaults(self, force_reload=False):\n        '''Get server defaults, caching the results. If there are no results saved, or the force_reload flag is True,\n        it will query the HDFS server for its default parameter values. Otherwise, it will simply return the results\n        it has already queried.\n\n        Note: This function returns a copy of the results loaded from the server, so you can manipulate or change\n        them as you'd like. If for any reason you need to change the results the client saves, you must access\n        the property client._server_defaults directly.\n\n        :param force_reload: Should the server defaults be reloaded even if they already exist?\n        :type force_reload: bool\n        :returns: dictionary with the following keys: blockSize, bytesPerChecksum, writePacketSize, replication, fileBufferSize, encryptDataTransfer, trashInterval, checksumType\n\n        **Example:**\n\n        >>> client.serverdefaults()\n        [{'writePacketSize': 65536, 'fileBufferSize': 4096, 'replication': 1, 'bytesPerChecksum': 512, 'trashInterval': 0L, 'blockSize': 134217728L, 'encryptDataTransfer': False, 'checksumType': 2}]\n\n        '''\n\n        if not self._server_defaults or force_reload:\n            request = client_proto.GetServerDefaultsRequestProto()\n            response = self.service.getServerDefaults(request).serverDefaults\n            self._server_defaults = {'blockSize': response.blockSize, 'bytesPerChecksum': response.bytesPerChecksum,\n                'writePacketSize': response.writePacketSize, 'replication': response.replication,\n                'fileBufferSize': response.fileBufferSize, 'encryptDataTransfer': response.encryptDataTransfer,\n                'trashInterval': response.trashInterval, 'checksumType': response.checksumType}\n\n        # return a copy, so if the user changes any values, they won't be saved in the client\n        return self._server_defaults.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_items(self, paths, processor, include_toplevel=False, include_children=False, recurse=False, check_nonexistence=False):\n        ''' Request file info from the NameNode and call the processor on the node(s) returned\n\n        :param paths:\n            A list of paths that need to be processed\n        :param processor:\n            Method that is called on an node. Method signature should be foo(path, node). For additional\n            (static) params, use a lambda.\n        :param include_toplevel:\n            Boolean to enable the inclusion of the first node found.\n            Example: listing a directory should not include the toplevel, but chmod should\n            only operate on the path that is input, so it should include the toplevel.\n        :param include_children:\n            Include children (when the path is a directory) in processing. Recurse will always\n            include children.\n            Example: listing a directory should include children, but chmod shouldn't.\n        :param recurse:\n            Recurse into children if they are directories.\n        '''\n\n        if not paths:\n            paths = [posixpath.join(\"/user\", get_current_username())]\n\n        # Expand paths if necessary (/foo/{bar,baz} --> ['/foo/bar', '/foo/baz'])\n        paths = glob.expand_paths(paths)\n\n        for path in paths:\n            if not path.startswith(\"/\"):\n                path = self._join_user_path(path)\n\n            # Normalize path (remove double /, handle '..', remove trailing /, etc)\n            path = self._normalize_path(path)\n\n            log.debug(\"Trying to find path %s\" % path)\n\n            if glob.has_magic(path):\n                log.debug(\"Dealing with globs in %s\" % path)\n                for item in self._glob_find(path, processor, include_toplevel):\n                    yield item\n            else:\n                fileinfo = self._get_file_info(path)\n                if not fileinfo and not check_nonexistence:\n                    raise FileNotFoundException(\"`%s': No such file or directory\" % path)\n                elif not fileinfo and check_nonexistence:\n                    yield processor(path, None)\n                    continue\n                elif fileinfo and check_nonexistence:\n                    yield {\"path\": path, \"result\": False, \"error\": \"File already exists\"}\n                    continue\n\n                if (include_toplevel and fileinfo) or not self._is_dir(fileinfo.fs):\n                    # Construct the full path before processing\n                    full_path = self._get_full_path(path, fileinfo.fs)\n                    log.debug(\"Added %s to to result set\" % full_path)\n                    entry = processor(full_path, fileinfo.fs)\n                    yield entry\n\n                if self._is_dir(fileinfo.fs) and (include_children or recurse):\n                    for node in self._get_dir_listing(path):\n                        full_path = self._get_full_path(path, node)\n                        entry = processor(full_path, node)\n                        yield entry\n\n                        # Recurse into directories\n                        if recurse and self._is_dir(node):\n                            # Construct the full path before processing\n                            full_path = posixpath.join(path, node.path)\n                            for item in self._find_items([full_path],\n                                                         processor,\n                                                         include_toplevel=False,\n                                                         include_children=False,\n                                                         recurse=recurse):\n                                yield item", "response": "Find items in the NameNode and call the processor on the items."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _glob_find(self, path, processor, include_toplevel):\n        '''Handle globs in paths.\n        This is done by listing the directory before a glob and checking which\n        node matches the initial glob. If there are more globs in the path,\n        we don't add the found children to the result, but traverse into paths\n        that did have a match.\n        '''\n\n        # Split path elements and check where the first occurence of magic is\n        path_elements = path.split(\"/\")\n        for i, element in enumerate(path_elements):\n            if glob.has_magic(element):\n                first_magic = i\n                break\n\n        # Create path that we check first to get a listing we match all children\n        # against. If the 2nd path element is a glob, we need to check \"/\", and\n        # we hardcode that, since \"/\".join(['']) doesn't return \"/\"\n        if first_magic == 1:\n            check_path = \"/\"\n        else:\n            check_path = \"/\".join(path_elements[:first_magic])\n\n        # Path that we need to match against\n        match_path = \"/\".join(path_elements[:first_magic + 1])\n\n        # Rest of the unmatched path. In case the rest is only one element long\n        # we prepend it with \"/\", since \"/\".join(['x']) doesn't return \"/x\"\n        rest_elements = path_elements[first_magic + 1:]\n        if len(rest_elements) == 1:\n            rest = rest_elements[0]\n        else:\n            rest = \"/\".join(rest_elements)\n        # Check if the path exists and that it's a directory (which it should..)\n        fileinfo = self._get_file_info(check_path)\n        if fileinfo and self._is_dir(fileinfo.fs):\n            # List all child nodes and match them agains the glob\n            for node in self._get_dir_listing(check_path):\n                full_path = self._get_full_path(check_path, node)\n                if fnmatch.fnmatch(full_path, match_path):\n                    # If we have a match, but need to go deeper, we recurse\n                    if rest and glob.has_magic(rest):\n                        traverse_path = \"/\".join([full_path, rest])\n                        for item in self._glob_find(traverse_path, processor, include_toplevel):\n                            yield item\n                    elif rest:\n                        # we have more rest, but it's not magic, which is either a file or a directory\n                        final_path = posixpath.join(full_path, rest)\n                        fi = self._get_file_info(final_path)\n                        if fi and self._is_dir(fi.fs):\n                            for n in self._get_dir_listing(final_path):\n                                full_child_path = self._get_full_path(final_path, n)\n                                yield processor(full_child_path, n)\n                        elif fi:\n                            yield processor(final_path, fi.fs)\n                    else:\n                        # If the matching node is a directory, we list the directory\n                        # This is what the hadoop client does at least.\n                        if self._is_dir(node):\n                            if include_toplevel:\n                                yield processor(full_path, node)\n                            fp = self._get_full_path(check_path, node)\n                            dir_list = self._get_dir_listing(fp)\n                            if dir_list:  # It might happen that the directory above has been removed\n                                for n in dir_list:\n                                    full_child_path = self._get_full_path(fp, n)\n                                    yield processor(full_child_path, n)\n                        else:\n                            yield processor(full_path, node)", "response": "Handle globs in paths. A glob is performed by listing the directory before a glob and checking which node matches the initial glob and adding the found children to the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef properties(self, rel_path=None):\n\n        full_url_or_path = self.__url_or_path\n        if rel_path is not None:\n            full_url_or_path += '/' + rel_path\n\n        result = self.run_command(\n            'proplist',\n            ['--xml', full_url_or_path],\n            do_combine=True)\n\n        # query the proper list of this path\n        root = xml.etree.ElementTree.fromstring(result)\n        target_elem = root.find('target')\n        property_names = [p.attrib[\"name\"]\n                          for p in target_elem.findall('property')]\n\n        # now query the content of each propery\n        property_dict = {}\n\n        for property_name in property_names:\n            result = self.run_command(\n                'propget',\n                ['--xml', property_name, full_url_or_path, ],\n                do_combine=True)\n            root = xml.etree.ElementTree.fromstring(result)\n            target_elem = root.find('target')\n            property_elem = target_elem.find('property')\n            property_dict[property_name] = property_elem.text\n\n        return property_dict", "response": "Returns a dictionary with all properties associated with the a\n            relative path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_default(self, timestamp_from_dt=None, timestamp_to_dt=None,\n                    limit=None, rel_filepath=None, stop_on_copy=False,\n                    revision_from=None, revision_to=None, changelist=False,\n                    use_merge_history=False):\n        \"\"\"Allow for the most-likely kind of log listing: the complete list,\n        a FROM and TO timestamp, a FROM timestamp only, or a quantity limit.\n        \"\"\"\n\n        full_url_or_path = self.__url_or_path\n        if rel_filepath is not None:\n            full_url_or_path += '/' + rel_filepath\n\n        timestamp_from_phrase = ('{' + timestamp_from_dt.isoformat() + '}') \\\n            if timestamp_from_dt \\\n            else ''\n\n        timestamp_to_phrase = ('{' + timestamp_to_dt.isoformat() + '}') \\\n            if timestamp_to_dt \\\n            else ''\n\n        args = []\n\n        if timestamp_from_phrase or timestamp_to_phrase:\n            if not timestamp_from_phrase:\n                raise ValueError(\"The default log retriever can not take a TO \"\n                                 \"timestamp without a FROM timestamp.\")\n\n            if not timestamp_to_phrase:\n                timestamp_to_phrase = 'HEAD'\n\n            args += ['-r', timestamp_from_phrase + ':' + timestamp_to_phrase]\n\n        if revision_from or revision_to:\n            if timestamp_from_phrase or timestamp_to_phrase:\n                raise ValueError(\"The default log retriever can not take both \"\n                                 \"timestamp and revision number ranges.\")\n\n            if not revision_from:\n                revision_from = '1'\n\n            if not revision_to:\n                revision_to = 'HEAD'\n\n            args += ['-r', str(revision_from) + ':' + str(revision_to)]\n\n        if limit is not None:\n            args += ['-l', str(limit)]\n\n        if stop_on_copy is True:\n            args += ['--stop-on-copy']\n\n        if use_merge_history is True:\n            args += ['--use-merge-history']\n\n        if changelist is True:\n            args += ['--verbose']\n\n        result = self.run_command(\n            'log',\n            args + ['--xml', full_url_or_path],\n            do_combine=True)\n\n        root = xml.etree.ElementTree.fromstring(result)\n        named_fields = ['date', 'msg', 'revision', 'author', 'changelist']\n        c = collections.namedtuple(\n            'LogEntry',\n            named_fields)\n\n        # Merge history can create nested log entries, so use iter instead of findall\n        for e in root.iter('logentry'):\n            entry_info = {x.tag: x.text for x in e.getchildren()}\n\n            date = None\n            date_text = entry_info.get('date')\n            if date_text is not None:\n                date = dateutil.parser.parse(date_text)\n\n            log_entry = {\n                'msg': entry_info.get('msg'),\n                'author': entry_info.get('author'),\n                'revision': int(e.get('revision')),\n                'date': date\n            }\n\n            if changelist is True:\n                cl = []\n                for ch in e.findall('paths/path'):\n                    cl.append((ch.attrib['action'], ch.text))\n\n                log_entry['changelist'] = cl\n            else:\n                log_entry['changelist'] = None\n\n            yield c(**log_entry)", "response": "This function returns a default log listing for the most - likely kind of log listing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef diff_summary(self, old, new, rel_path=None):\n        full_url_or_path = self.__url_or_path\n        if rel_path is not None:\n            full_url_or_path += '/' + rel_path\n        result = self.run_command(\n            'diff',\n            ['--old', '{0}@{1}'.format(full_url_or_path, old),\n             '--new', '{0}@{1}'.format(full_url_or_path, new),\n             '--summarize', '--xml'],\n            do_combine=True)\n        root = xml.etree.ElementTree.fromstring(result)\n        diff = []\n        for element in root.findall('paths/path'):\n            diff.append({\n                'path': element.text,\n                'item': element.attrib['item'],\n                'kind': element.attrib['kind']})\n        return diff", "response": "Provides a summarized output of a diff between two revisions\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef diff(self, old, new, rel_path=None):\n        full_url_or_path = self.__url_or_path\n        if rel_path is not None:\n            full_url_or_path += '/' + rel_path\n        diff_result = self.run_command(\n            'diff',\n            ['--old', '{0}@{1}'.format(full_url_or_path, old),\n             '--new', '{0}@{1}'.format(full_url_or_path, new)],\n            do_combine=True)\n        file_to_diff = {}\n        for non_empty_diff in filter(None, diff_result.decode('utf8').split('Index: ')):\n            split_diff = non_empty_diff.split('==')\n            file_to_diff[split_diff[0].strip().strip('/')] = split_diff[-1].strip('=').strip()\n        diff_summaries = self.diff_summary(old, new, rel_path)\n        for diff_summary in diff_summaries:\n            diff_summary['diff'] = \\\n                file_to_diff[diff_summary['path'].split(full_url_or_path)[-1].strip('/')]\n        return diff_summaries", "response": "Provides output of a diff between two revisions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_number_sequence(self, seq, n):\n        if seq is None:\n            return np.zeros(n)\n        if len(seq) is n:\n            try:\n                l = [float(e) for e in seq]\n            except ValueError:\n                raise ValueError(\"One or more elements in sequence <\" + repr(seq) + \"> cannot be interpreted as a real number\")\n            else:\n                return np.asarray(l)\n        elif len(seq) is 0:\n            return np.zeros(n)\n        else:\n            raise ValueError(\"Unexpected number of elements in sequence. Got: \" + str(len(seq)) + \", Expected: \" + str(n) + \".\")", "response": "Validate a sequence to be of a certain length and ensure it s a numpy array of floats."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitialising from matrix representation.", "response": "def _from_matrix(cls, matrix):\n        \"\"\"Initialise from matrix representation\n\n        Create a Quaternion by specifying the 3x3 rotation or 4x4 transformation matrix\n        (as a numpy array) from which the quaternion's rotation should be created.\n\n        \"\"\"\n        try:\n            shape = matrix.shape\n        except AttributeError:\n            raise TypeError(\"Invalid matrix type: Input must be a 3x3 or 4x4 numpy array or matrix\")\n\n        if shape == (3, 3):\n            R = matrix\n        elif shape == (4,4):\n            R = matrix[:-1][:,:-1] # Upper left 3x3 sub-matrix\n        else:\n            raise ValueError(\"Invalid matrix shape: Input must be a 3x3 or 4x4 numpy array or matrix\")\n\n        # Check matrix properties\n        if not np.allclose(np.dot(R, R.conj().transpose()), np.eye(3)):\n            raise ValueError(\"Matrix must be orthogonal, i.e. its transpose should be its inverse\")\n        if not np.isclose(np.linalg.det(R), 1.0):\n            raise ValueError(\"Matrix must be special orthogonal i.e. its determinant must be +1.0\")\n\n        def decomposition_method(matrix):\n            \"\"\" Method supposedly able to deal with non-orthogonal matrices - NON-FUNCTIONAL!\n            Based on this method: http://arc.aiaa.org/doi/abs/10.2514/2.4654\n            \"\"\"\n            x, y, z = 0, 1, 2 # indices\n            K = np.array([\n                [R[x, x]-R[y, y]-R[z, z],  R[y, x]+R[x, y],           R[z, x]+R[x, z],           R[y, z]-R[z, y]],\n                [R[y, x]+R[x, y],          R[y, y]-R[x, x]-R[z, z],   R[z, y]+R[y, z],           R[z, x]-R[x, z]],\n                [R[z, x]+R[x, z],          R[z, y]+R[y, z],           R[z, z]-R[x, x]-R[y, y],   R[x, y]-R[y, x]],\n                [R[y, z]-R[z, y],          R[z, x]-R[x, z],           R[x, y]-R[y, x],           R[x, x]+R[y, y]+R[z, z]]\n            ])\n            K = K / 3.0\n\n            e_vals, e_vecs = np.linalg.eig(K)\n            print('Eigenvalues:', e_vals)\n            print('Eigenvectors:', e_vecs)\n            max_index = np.argmax(e_vals)\n            principal_component = e_vecs[max_index]\n            return principal_component\n\n        def trace_method(matrix):\n            \"\"\"\n            This code uses a modification of the algorithm described in:\n            https://d3cw3dd2w32x2b.cloudfront.net/wp-content/uploads/2015/01/matrix-to-quat.pdf\n            which is itself based on the method described here:\n            http://www.euclideanspace.com/maths/geometry/rotations/conversions/matrixToQuaternion/\n\n            Altered to work with the column vector convention instead of row vectors\n            \"\"\"\n            m = matrix.conj().transpose() # This method assumes row-vector and postmultiplication of that vector\n            if m[2, 2] < 0:\n                if m[0, 0] > m[1, 1]:\n                    t = 1 + m[0, 0] - m[1, 1] - m[2, 2]\n                    q = [m[1, 2]-m[2, 1],  t,  m[0, 1]+m[1, 0],  m[2, 0]+m[0, 2]]\n                else:\n                    t = 1 - m[0, 0] + m[1, 1] - m[2, 2]\n                    q = [m[2, 0]-m[0, 2],  m[0, 1]+m[1, 0],  t,  m[1, 2]+m[2, 1]]\n            else:\n                if m[0, 0] < -m[1, 1]:\n                    t = 1 - m[0, 0] - m[1, 1] + m[2, 2]\n                    q = [m[0, 1]-m[1, 0],  m[2, 0]+m[0, 2],  m[1, 2]+m[2, 1],  t]\n                else:\n                    t = 1 + m[0, 0] + m[1, 1] + m[2, 2]\n                    q = [t,  m[1, 2]-m[2, 1],  m[2, 0]-m[0, 2],  m[0, 1]-m[1, 0]]\n\n            q = np.array(q)\n            q *= 0.5 / sqrt(t);\n            return q\n\n        return cls(array=trace_method(R))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _from_axis_angle(cls, axis, angle):\n        mag_sq = np.dot(axis, axis)\n        if mag_sq == 0.0:\n            raise ZeroDivisionError(\"Provided rotation axis has no length\")\n        # Ensure axis is in unit vector form\n        if (abs(1.0 - mag_sq) > 1e-12):\n            axis = axis / sqrt(mag_sq)\n        theta = angle / 2.0\n        r = cos(theta)\n        i = axis * sin(theta)\n\n        return cls(r, i[0], i[1], i[2])", "response": "Initialise from axis and angle representation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a random unit quaternion.", "response": "def random(cls):\n        \"\"\"Generate a random unit quaternion.\n\n        Uniformly distributed across the rotation space\n        As per: http://planning.cs.uiuc.edu/node198.html\n        \"\"\"\n        r1, r2, r3 = np.random.random(3)\n\n        q1 = sqrt(1.0 - r1) * (sin(2 * pi * r2))\n        q2 = sqrt(1.0 - r1) * (cos(2 * pi * r2))\n        q3 = sqrt(r1)       * (sin(2 * pi * r3))\n        q4 = sqrt(r1)       * (cos(2 * pi * r3))\n\n        return cls(q1, q2, q3, q4)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new instance with the conjugate part of the quaternion.", "response": "def conjugate(self):\n        \"\"\"Quaternion conjugate, encapsulated in a new instance.\n\n        For a unit quaternion, this is the same as the inverse.\n\n        Returns:\n            A new Quaternion object clone with its vector part negated\n        \"\"\"\n        return self.__class__(scalar=self.scalar, vector= -self.vector)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the inverse of the quaternion object.", "response": "def inverse(self):\n        \"\"\"Inverse of the quaternion object, encapsulated in a new instance.\n\n        For a unit quaternion, this is the inverse rotation, i.e. when combined with the original rotation, will result in the null rotation.\n\n        Returns:\n            A new Quaternion object representing the inverse of this object\n        \"\"\"\n        ss = self._sum_of_squares()\n        if ss > 0:\n            return self.__class__(array=(self._vector_conjugate() / ss))\n        else:\n            raise ZeroDivisionError(\"a zero quaternion (0 + 0i + 0j + 0k) cannot be inverted\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalise the object to be a unit quaternion.", "response": "def _normalise(self):\n        \"\"\"Object is guaranteed to be a unit quaternion after calling this\n        operation UNLESS the object is equivalent to Quaternion(0)\n        \"\"\"\n        if not self.is_unit():\n            n = self.norm\n            if n > 0:\n                self.q = self.q / n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalising the object to a unit quaternion using a fast approximation method if appropriate.", "response": "def _fast_normalise(self):\n        \"\"\"Normalise the object to a unit quaternion using a fast approximation method if appropriate.\n\n        Object is guaranteed to be a quaternion of approximately unit length\n        after calling this operation UNLESS the object is equivalent to Quaternion(0)\n        \"\"\"\n        if not self.is_unit():\n            mag_squared = np.dot(self.q, self.q)\n            if (mag_squared == 0):\n                return\n            if (abs(1.0 - mag_squared) < 2.107342e-08):\n                mag =  ((1.0 + mag_squared) / 2.0) # More efficient. Pade approximation valid if error is small\n            else:\n                mag =  sqrt(mag_squared) # Error is too big, take the performance hit to calculate the square root properly\n\n            self.q = self.q / mag"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _q_matrix(self):\n        return np.array([\n            [self.q[0], -self.q[1], -self.q[2], -self.q[3]],\n            [self.q[1],  self.q[0], -self.q[3],  self.q[2]],\n            [self.q[2],  self.q[3],  self.q[0], -self.q[1]],\n            [self.q[3], -self.q[2],  self.q[1],  self.q[0]]])", "response": "Matrix representation of quaternion for multiplication purposes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _q_bar_matrix(self):\n        return np.array([\n            [self.q[0], -self.q[1], -self.q[2], -self.q[3]],\n            [self.q[1],  self.q[0],  self.q[3], -self.q[2]],\n            [self.q[2], -self.q[3],  self.q[0],  self.q[1]],\n            [self.q[3],  self.q[2], -self.q[1],  self.q[0]]])", "response": "Matrix representation of quaternion for multiplication purposes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotate(self, vector):\n        if isinstance(vector, Quaternion):\n            return self._rotate_quaternion(vector)\n        q = Quaternion(vector=vector)\n        a = self._rotate_quaternion(q).vector\n        if isinstance(vector, list):\n            l = [x for x in a]\n            return l\n        elif isinstance(vector, tuple):\n            l = [x for x in a]\n            return tuple(l)\n        else:\n            return a", "response": "Rotate a 3D vector by the rotation stored in the Quaternion object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exp(cls, q):\n        tolerance = 1e-17\n        v_norm = np.linalg.norm(q.vector)\n        vec = q.vector\n        if v_norm > tolerance:\n            vec = vec / v_norm\n        magnitude = exp(q.scalar)\n        return Quaternion(scalar = magnitude * cos(v_norm), vector = magnitude * sin(v_norm) * vec)", "response": "Quaternion Exponential.\n\n        Find the exponential of a quaternion amount.\n\n        Params:\n             q: the input quaternion/argument as a Quaternion object.\n\n        Returns:\n             A quaternion amount representing the exp(q). See [Source](https://math.stackexchange.com/questions/1030737/exponential-function-of-quaternion-derivation for more information and mathematical background).\n\n        Note:\n             The method can compute the exponential of any quaternion."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log(cls, q):\n        v_norm = np.linalg.norm(q.vector)\n        q_norm = q.norm\n        tolerance = 1e-17\n        if q_norm < tolerance:\n            # 0 quaternion - undefined\n            return Quaternion(scalar=-float('inf'), vector=float('nan')*q.vector)\n        if v_norm < tolerance:\n            # real quaternions - no imaginary part\n            return Quaternion(scalar=log(q_norm), vector=[0,0,0])\n        vec = q.vector / v_norm\n        return Quaternion(scalar=log(q_norm), vector=acos(q.scalar/q_norm)*vec)", "response": "Quaternion Logarithm.\n        returns the logarithm of a quaternion amount."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sym_exp_map(cls, q, eta):\n        sqrt_q = q ** 0.5\n        return sqrt_q * Quaternion.exp(eta) * sqrt_q", "response": "Quaternion symmetrized exponential map."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef absolute_distance(cls, q0, q1):\n        q0_minus_q1 = q0 - q1\n        q0_plus_q1  = q0 + q1\n        d_minus = q0_minus_q1.norm\n        d_plus  = q0_plus_q1.norm\n        if (d_minus < d_plus):\n            return d_minus\n        else:\n            return d_plus", "response": "Quaternion absolute distance.\n\n        Find the distance between two quaternions accounting for the sign ambiguity.\n\n        Params:\n            q0: the first quaternion\n            q1: the second quaternion\n\n        Returns:\n           A positive scalar corresponding to the chord of the shortest path/arc that\n           connects q0 to q1.\n\n        Note:\n           This function does not measure the distance on the hypersphere, but\n           it takes into account the fact that q and -q encode the same rotation.\n           It is thus a good indicator for rotation similarities."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sym_distance(cls, q0, q1):\n        q = Quaternion.sym_log_map(q0, q1)\n        return q.norm", "response": "Quaternion symmetrized distance between q0 and q1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef derivative(self, rate):\n        rate = self._validate_number_sequence(rate, 3)\n        return 0.5 * self * Quaternion(vector=rate)", "response": "Returns the instantaneous quaternion derivative representing a rotation at a 3D rate vector rate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadvancing a time varying quaternion to its value at a time timestep in the future.", "response": "def integrate(self, rate, timestep):\n        \"\"\"Advance a time varying quaternion to its value at a time `timestep` in the future.\n\n        The Quaternion object will be modified to its future value.\n        It is guaranteed to remain a unit quaternion.\n\n        Params:\n\n        rate: numpy 3-array (or array-like) describing rotation rates about the\n            global x, y and z axes respectively.\n        timestep: interval over which to integrate into the future.\n            Assuming *now* is `T=0`, the integration occurs over the interval\n            `T=0` to `T=timestep`. Smaller intervals are more accurate when\n            `rate` changes over time.\n\n        Note:\n            The solution is closed form given the assumption that `rate` is constant\n            over the interval of length `timestep`.\n        \"\"\"\n        self._fast_normalise()\n        rate = self._validate_number_sequence(rate, 3)\n\n        rotation_vector = rate * timestep\n        rotation_norm = np.linalg.norm(rotation_vector)\n        if rotation_norm > 0:\n            axis = rotation_vector / rotation_norm\n            angle = rotation_norm\n            q2 = Quaternion(axis=axis, angle=angle)\n            self.q = (self * q2).q\n            self._fast_normalise()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rotation_matrix(self):\n        self._normalise()\n        product_matrix = np.dot(self._q_matrix(), self._q_bar_matrix().conj().transpose())\n        return product_matrix[1:][:,1:]", "response": "Get the 3x3 orthogonal rotation matrix equivalent of the quaternion rotation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the 4x4 homogeneous transformation matrix equivalent of the quaternion rotation.", "response": "def transformation_matrix(self):\n        \"\"\"Get the 4x4 homogeneous transformation matrix equivalent of the quaternion rotation.\n\n        Returns:\n            A 4x4 homogeneous transformation matrix as a 4x4 Numpy array\n\n        Note:\n            This feature only makes sense when referring to a unit quaternion. Calling this method will implicitly normalise the Quaternion object to a unit quaternion if it is not already one.\n        \"\"\"\n        t = np.array([[0.0], [0.0], [0.0]])\n        Rt = np.hstack([self.rotation_matrix, t])\n        return np.vstack([Rt, np.array([0.0, 0.0, 0.0, 1.0])])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef yaw_pitch_roll(self):\n\n        self._normalise()\n        yaw = np.arctan2(2*(self.q[0]*self.q[3] - self.q[1]*self.q[2]),\n            1 - 2*(self.q[2]**2 + self.q[3]**2))\n        pitch = np.arcsin(2*(self.q[0]*self.q[2] + self.q[3]*self.q[1]))\n        roll = np.arctan2(2*(self.q[0]*self.q[1] - self.q[2]*self.q[3]),\n            1 - 2*(self.q[1]**2 + self.q[2]**2))\n\n        return yaw, pitch, roll", "response": "Get the equivalent yaw - pitch - roll angles aka. intrinsic Tait - Bryan angles following the z - y - x convention."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _wrap_angle(self, theta):\n        result = ((theta + pi) % (2*pi)) - pi\n        if result == -pi: result = pi\n        return result", "response": "Wrap any angle to lie between - pi and pi\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the axis or vector about which the quaternion rotation occurs.", "response": "def get_axis(self, undefined=np.zeros(3)):\n        \"\"\"Get the axis or vector about which the quaternion rotation occurs\n\n        For a null rotation (a purely real quaternion), the rotation angle will\n        always be `0`, but the rotation axis is undefined.\n        It is by default assumed to be `[0, 0, 0]`.\n\n        Params:\n            undefined: [optional] specify the axis vector that should define a null rotation.\n                This is geometrically meaningless, and could be any of an infinite set of vectors,\n                but can be specified if the default (`[0, 0, 0]`) causes undesired behaviour.\n\n        Returns:\n            A Numpy unit 3-vector describing the Quaternion object's axis of rotation.\n\n        Note:\n            This feature only makes sense when referring to a unit quaternion.\n            Calling this method will implicitly normalise the Quaternion object to a unit quaternion if it is not already one.\n        \"\"\"\n        tolerance = 1e-17\n        self._normalise()\n        norm = np.linalg.norm(self.vector)\n        if norm < tolerance:\n            # Here there are an infinite set of possible axes, use what has been specified as an undefined axis.\n            return undefined\n        else:\n            return self.vector / norm"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the angle of the rotation about the object.", "response": "def angle(self):\n        \"\"\"Get the angle (in radians) describing the magnitude of the quaternion rotation about its rotation axis.\n\n        This is guaranteed to be within the range (-pi:pi) with the direction of\n        rotation indicated by the sign.\n\n        When a particular rotation describes a 180 degree rotation about an arbitrary\n        axis vector `v`, the conversion to axis / angle representation may jump\n        discontinuously between all permutations of `(-pi, pi)` and `(-v, v)`,\n        each being geometrically equivalent (see Note in documentation).\n\n        Returns:\n            A real number in the range (-pi:pi) describing the angle of rotation\n                in radians about a Quaternion object's axis of rotation.\n\n        Note:\n            This feature only makes sense when referring to a unit quaternion.\n            Calling this method will implicitly normalise the Quaternion object to a unit quaternion if it is not already one.\n        \"\"\"\n        self._normalise()\n        norm = np.linalg.norm(self.vector)\n        return self._wrap_angle(2.0 * atan2(norm,self.scalar))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef req_string(self, model):\n        if model not in self.models:\n            model = self.models\n        if self.min_ykver and self.max_ykver:\n            return \"%s %d.%d..%d.%d\" % (model, \\\n                                           self.min_ykver[0], self.min_ykver[1], \\\n                                           self.max_ykver[0], self.max_ykver[1], \\\n                                           )\n        if self.max_ykver:\n            return \"%s <= %d.%d\" % (model, self.max_ykver[0], self.max_ykver[1])\n\n        return \"%s >= %d.%d\" % (model, self.min_ykver[0], self.min_ykver[1])", "response": "Return string describing model and version requirement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if this flag is compatible with a YubiKey of version ver.", "response": "def is_compatible(self, model, version):\n        \"\"\" Check if this flag is compatible with a YubiKey of version 'ver'. \"\"\"\n        if not model in self.models:\n            return False\n        if self.max_ykver:\n            return (version >= self.min_ykver and\n                    version <= self.max_ykver)\n        else:\n            return version >= self.min_ykver"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the boolean value of flag.", "response": "def get_set(self, flag, new):\n        \"\"\"\n        Return the boolean value of 'flag'. If 'new' is set,\n        the flag is updated, and the value before update is\n        returned.\n        \"\"\"\n        old = self._is_set(flag)\n        if new is True:\n            self._set(flag)\n        elif new is False:\n            self._clear(flag)\n        return old"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_capabilities(self):\n\n        frame = yubikey_frame.YubiKeyFrame(command=SLOT.YK4_CAPABILITIES)\n        self._device._write(frame)\n        response = self._device._read_response()\n        r_len = yubico_util.ord_byte(response[0])\n\n        # 1 byte length, 2 byte CRC.\n        if not yubico_util.validate_crc16(response[:r_len+3]):\n            raise YubiKey4_USBHIDError(\"Read from device failed CRC check\")\n\n        return response[1:r_len+1]", "response": "Read the capabilities list from a YubiKey > = 4. 0. 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the status of the YubiKey.", "response": "def status(self):\n        \"\"\"\n        Poll YubiKey for status.\n        \"\"\"\n        data = self._read()\n        self._status = YubiKeyUSBHIDStatus(data)\n        return self._status"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write_config(self, cfg, slot):\n        old_pgm_seq = self._status.pgm_seq\n        frame = cfg.to_frame(slot=slot)\n        self._debug(\"Writing %s frame :\\n%s\\n\" % \\\n                        (yubikey_config.command2str(frame.command), cfg))\n        self._write(frame)\n        self._waitfor_clear(yubikey_defs.SLOT_WRITE_FLAG)\n        # make sure we have a fresh pgm_seq value\n        self.status()\n        self._debug(\"Programmed slot %i, sequence %i -> %i\\n\" % (slot, old_pgm_seq, self._status.pgm_seq))\n\n        cfgs = self._status.valid_configs()\n        if not cfgs and self._status.pgm_seq == 0:\n            return\n        if self._status.pgm_seq == old_pgm_seq + 1:\n            return\n\n        raise YubiKeyUSBHIDError('YubiKey programming failed (seq %i not increased (%i))' % \\\n                                    (old_pgm_seq, self._status.pgm_seq))", "response": "Write configuration to YubiKey."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_response(self, may_block=False):\n        # wait for response to become available\n        res = self._waitfor_set(yubikey_defs.RESP_PENDING_FLAG, may_block)[:7]\n        # continue reading while response pending is set\n        while True:\n            this = self._read()\n            flags = yubico_util.ord_byte(this[7])\n            if flags & yubikey_defs.RESP_PENDING_FLAG:\n                seq = flags & 0b00011111\n                if res and (seq == 0):\n                    break\n                res += this[:7]\n            else:\n                break\n        self._write_reset()\n        return res", "response": "Read the response from the YubiKey."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a USB HID feature report from the YubiKey.", "response": "def _read(self):\n        \"\"\" Read a USB HID feature report from the YubiKey. \"\"\"\n        request_type = _USB_TYPE_CLASS | _USB_RECIP_INTERFACE | _USB_ENDPOINT_IN\n        value = _REPORT_TYPE_FEATURE << 8    # apparently required for YubiKey 1.3.2, but not 2.2.x\n        recv = self._usb_handle.controlMsg(request_type,\n                                          _HID_GET_REPORT,\n                                          _FEATURE_RPT_SIZE,\n                                          value = value,\n                                          timeout = _USB_TIMEOUT_MS)\n        if len(recv) != _FEATURE_RPT_SIZE:\n            self._debug(\"Failed reading %i bytes (got %i) from USB HID YubiKey.\\n\"\n                        % (_FEATURE_RPT_SIZE, recv))\n            raise YubiKeyUSBHIDError('Failed reading from USB HID YubiKey')\n        data = b''.join(yubico_util.chr_byte(c) for c in recv)\n        self._debug(\"READ  : %s\" % (yubico_util.hexdump(data, colorize=True)))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write(self, frame):\n        for data in frame.to_feature_reports(debug=self.debug):\n            debug_str = None\n            if self.debug:\n                (data, debug_str) = data\n            # first, we ensure the YubiKey will accept a write\n            self._waitfor_clear(yubikey_defs.SLOT_WRITE_FLAG)\n            self._raw_write(data, debug_str)\n        return True", "response": "Write a YubiKeyFrame to the USB HID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset read mode by issuing a dummy write.", "response": "def _write_reset(self):\n        \"\"\"\n        Reset read mode by issuing a dummy write.\n        \"\"\"\n        data = b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8f'\n        self._raw_write(data)\n        self._waitfor_clear(yubikey_defs.SLOT_WRITE_FLAG)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _raw_write(self, data, debug_str = None):\n        if self.debug:\n            if not debug_str:\n                debug_str = ''\n            hexdump = yubico_util.hexdump(data, colorize=True)[:-1] # strip LF\n            self._debug(\"WRITE : %s %s\\n\" % (hexdump, debug_str))\n        request_type = _USB_TYPE_CLASS | _USB_RECIP_INTERFACE | _USB_ENDPOINT_OUT\n        value = _REPORT_TYPE_FEATURE << 8    # apparently required for YubiKey 1.3.2, but not 2.2.x\n        sent = self._usb_handle.controlMsg(request_type,\n                                          _HID_SET_REPORT,\n                                          data,\n                                          value = value,\n                                          timeout = _USB_TIMEOUT_MS)\n        if sent != _FEATURE_RPT_SIZE:\n            self.debug(\"Failed writing %i bytes (wrote %i) to USB HID YubiKey.\\n\"\n                       % (_FEATURE_RPT_SIZE, sent))\n            raise YubiKeyUSBHIDError('Failed talking to USB HID YubiKey')\n        return sent", "response": "Write data to YubiKey."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _waitfor(self, mode, mask, may_block, timeout=2):\n        finished = False\n        sleep = 0.01\n        # After six sleeps, we've slept 0.64 seconds.\n        wait_num = (timeout * 2) - 1 + 6\n        resp_timeout = False    # YubiKey hasn't indicated RESP_TIMEOUT (yet)\n        while not finished:\n            time.sleep(sleep)\n            this = self._read()\n            flags = yubico_util.ord_byte(this[7])\n\n            if flags & yubikey_defs.RESP_TIMEOUT_WAIT_FLAG:\n                if not resp_timeout:\n                    resp_timeout = True\n                    seconds_left = flags & yubikey_defs.RESP_TIMEOUT_WAIT_MASK\n                    self._debug(\"Device indicates RESP_TIMEOUT (%i seconds left)\\n\" \\\n                                    % (seconds_left))\n                    if may_block:\n                        # calculate new wait_num - never more than 20 seconds\n                        seconds_left = min(20, seconds_left)\n                        wait_num = (seconds_left * 2) - 1 + 6\n\n            if mode is 'nand':\n                if not flags & mask == mask:\n                    finished = True\n                else:\n                    self._debug(\"Status %s (0x%x) has not cleared bits %s (0x%x)\\n\"\n                                % (bin(flags), flags, bin(mask), mask))\n            elif mode is 'and':\n                if flags & mask == mask:\n                    finished = True\n                else:\n                    self._debug(\"Status %s (0x%x) has not set bits %s (0x%x)\\n\"\n                                % (bin(flags), flags, bin(mask), mask))\n            else:\n                assert()\n\n            if not finished:\n                wait_num -= 1\n                if wait_num == 0:\n                    if mode is 'nand':\n                        reason = 'Timed out waiting for YubiKey to clear status 0x%x' % mask\n                    else:\n                        reason = 'Timed out waiting for YubiKey to set status 0x%x' % mask\n                    raise yubikey_base.YubiKeyTimeout(reason)\n                sleep = min(sleep + sleep, 0.5)\n            else:\n                return this", "response": "Wait for a YubiKey to turn ON or OFF certain bits in the status byte."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _open(self, skip=0):\n        usb_device = self._get_usb_device(skip)\n\n        if usb_device:\n            usb_conf = usb_device.configurations[0]\n            self._usb_int = usb_conf.interfaces[0][0]\n        else:\n            raise YubiKeyUSBHIDError('No USB YubiKey found')\n\n        try:\n            self._usb_handle = usb_device.open()\n            self._usb_handle.detachKernelDriver(0)\n        except Exception as error:\n            if 'could not detach kernel driver from interface' in str(error):\n                self._debug('The in-kernel-HID driver has already been detached\\n')\n            else:\n                self._debug(\"detachKernelDriver not supported!\\n\")\n\n        try:\n            self._usb_handle.setConfiguration(1)\n        except usb.USBError:\n            self._debug(\"Unable to set configuration, ignoring...\\n\")\n        self._usb_handle.claimInterface(self._usb_int)\n        return True", "response": "Open the USB device and initialize the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncloses the USB interface and the USB handle.", "response": "def _close(self):\n        \"\"\"\n        Release the USB interface again.\n        \"\"\"\n        self._usb_handle.releaseInterface()\n        try:\n            # If we're using PyUSB >= 1.0 we can re-attach the kernel driver here.\n            self._usb_handle.dev.attach_kernel_driver(0)\n        except:\n            pass\n        self._usb_int = None\n        self._usb_handle = None\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_usb_device(self, skip=0):\n        try:\n            # PyUSB >= 1.0, this is a workaround for a problem with libusbx\n            # on Windows.\n            import usb.core\n            import usb.legacy\n            devices = [usb.legacy.Device(d) for d in usb.core.find(\n                find_all=True, idVendor=YUBICO_VID)]\n        except ImportError:\n            # Using PyUsb < 1.0.\n            import usb\n            devices = [d for bus in usb.busses() for d in bus.devices]\n        for device in devices:\n            if device.idVendor == YUBICO_VID:\n                if device.idProduct in PID.all(otp=True):\n                    if skip == 0:\n                        return device\n                    skip -= 1\n        return None", "response": "Get YubiKey USB device.\n\n        Optionally allows you to skip n devices, to support multiple attached YubiKeys."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _debug(self, out, print_prefix=True):\n        if self.debug:\n            if print_prefix:\n                pre = self.__class__.__name__\n                if hasattr(self, 'debug_prefix'):\n                    pre = getattr(self, 'debug_prefix')\n                sys.stderr.write(\"%s: \" % pre)\n            sys.stderr.write(out)", "response": "Print out to stderr if debugging is enabled."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the serial number of the YubiKey.", "response": "def serial(self, may_block=True):\n        \"\"\" Get the YubiKey serial number (requires YubiKey 2.2). \"\"\"\n        if not self.capabilities.have_serial_number():\n            raise yubikey_base.YubiKeyVersionError(\"Serial number unsupported in YubiKey %s\" % self.version() )\n        return self._read_serial(may_block)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nissue a challenge response to the YubiKey and return the response.", "response": "def challenge_response(self, challenge, mode='HMAC', slot=1, variable=True, may_block=True):\n        \"\"\" Issue a challenge to the YubiKey and return the response (requires YubiKey 2.2). \"\"\"\n        if not self.capabilities.have_challenge_response(mode):\n            raise yubikey_base.YubiKeyVersionError(\"%s challenge-response unsupported in YubiKey %s\" % (mode, self.version()) )\n        return self._challenge_response(challenge, mode, slot, variable, may_block)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing a configuration object for this type of YubiKey.", "response": "def init_config(self, **kw):\n        \"\"\" Get a configuration object for this type of YubiKey. \"\"\"\n        return YubiKeyConfigUSBHID(ykver=self.version_num(), \\\n                                       capabilities = self.capabilities, \\\n                                       **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a configuration to the YubiKey.", "response": "def write_config(self, cfg, slot=1):\n        \"\"\" Write a configuration to the YubiKey. \"\"\"\n        cfg_req_ver = cfg.version_required()\n        if cfg_req_ver > self.version_num():\n            raise yubikey_base.YubiKeyVersionError('Configuration requires YubiKey version %i.%i (this is %s)' % \\\n                                                  (cfg_req_ver[0], cfg_req_ver[1], self.version()))\n        if not self.capabilities.have_configuration_slot(slot):\n            raise YubiKeyUSBHIDError(\"Can't write configuration to slot %i\" % (slot))\n        return self._device._write_config(cfg, slot)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_serial(self, may_block):\n\n        frame = yubikey_frame.YubiKeyFrame(command = SLOT.DEVICE_SERIAL)\n        self._device._write(frame)\n        response = self._device._read_response(may_block=may_block)\n        if not yubico_util.validate_crc16(response[:6]):\n            raise YubiKeyUSBHIDError(\"Read from device failed CRC check\")\n        # the serial number is big-endian, although everything else is little-endian\n        serial = struct.unpack('>lxxx', response)\n        return serial[0]", "response": "Read the serial number from a YubiKey > 2. 2."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a challenge - response to the YubiKey.", "response": "def _challenge_response(self, challenge, mode, slot, variable, may_block):\n        \"\"\" Do challenge-response with a YubiKey > 2.0. \"\"\"\n         # Check length and pad challenge if appropriate\n        if mode == 'HMAC':\n            if len(challenge) > yubikey_defs.SHA1_MAX_BLOCK_SIZE:\n                raise yubico_exception.InputError('Mode HMAC challenge too big (%i/%i)' \\\n                                                      % (yubikey_defs.SHA1_MAX_BLOCK_SIZE, len(challenge)))\n            if len(challenge) < yubikey_defs.SHA1_MAX_BLOCK_SIZE:\n                pad_with = b'\\0'\n                if variable and challenge[-1:] == pad_with:\n                    pad_with = b'\\xff'\n                challenge = challenge.ljust(yubikey_defs.SHA1_MAX_BLOCK_SIZE, pad_with)\n            response_len = yubikey_defs.SHA1_DIGEST_SIZE\n        elif mode == 'OTP':\n            if len(challenge) != yubikey_defs.UID_SIZE:\n                raise yubico_exception.InputError('Mode OTP challenge must be %i bytes (got %i)' \\\n                                                      % (yubikey_defs.UID_SIZE, len(challenge)))\n            challenge = challenge.ljust(yubikey_defs.SHA1_MAX_BLOCK_SIZE, b'\\0')\n            response_len = 16\n        else:\n            raise yubico_exception.InputError('Invalid mode supplied (%s, valid values are HMAC and OTP)' \\\n                                                  % (mode))\n\n        try:\n            command = _CMD_CHALLENGE[mode][slot]\n        except:\n            raise yubico_exception.InputError('Invalid slot specified (%s)' % (slot))\n\n        frame = yubikey_frame.YubiKeyFrame(command=command, payload=challenge)\n        self._device._write(frame)\n        response = self._device._read_response(may_block=may_block)\n        if not yubico_util.validate_crc16(response[:response_len + 2]):\n            raise YubiKeyUSBHIDError(\"Read from device failed CRC check\")\n        return response[:response_len]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef valid_configs(self):\n        if self.ykver() < (2,1,0):\n            raise YubiKeyUSBHIDError('Valid configs unsupported in firmware %s' % (self.version()))\n        res = []\n        if self.touch_level & self.CONFIG1_VALID == self.CONFIG1_VALID:\n            res.append(1)\n        if self.touch_level & self.CONFIG2_VALID == self.CONFIG2_VALID:\n            res.append(2)\n        return res", "response": "Return a list of valid configurtion slots. Requires firmware 2. 1."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nturning a command number into a string.", "response": "def command2str(num):\n    \"\"\" Turn command number into name \"\"\"\n    for attr in SLOT.__dict__.keys():\n        if not attr.startswith('_') and attr == attr.upper():\n            if getattr(SLOT, attr) == num:\n                return 'SLOT_%s' % attr\n\n    return \"0x%02x\" % (num)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_flag(which, flags):\n    res = [this for this in flags if this.is_equal(which)]\n    if len(res) == 0:\n        return None\n    if len(res) == 1:\n        return res[0]\n    assert()", "response": "Find which entry in flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fixed_string(self, data=None):\n        old = self.fixed\n        if data != None:\n            new = self._decode_input_string(data)\n            if len(new) <= 16:\n                self.fixed = new\n            else:\n                raise yubico_exception.InputError('The \"fixed\" string must be 0..16 bytes')\n        return old", "response": "This method is used to identify a particular YubiKey class. It is used to identify a particular YubiKey class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nenables or disable extended scan code mode for the YubiKey.", "response": "def enable_extended_scan_code_mode(self):\n        \"\"\"\n        Extended scan code mode means the Yubikey will output the bytes in\n        the 'fixed string' as scan codes, without modhex encoding the data.\n\n        Because of the way this is stored in the config flags, it is not\n        possible to disable this option once it is enabled (of course, you\n        can abort config update or reprogram the YubiKey again).\n\n        Requires YubiKey 2.x.\n        \"\"\"\n        if not self.capabilities.have_extended_scan_code_mode():\n            raise\n        self._require_version(major=2)\n        self.config_flag('SHORT_TICKET', True)\n        self.config_flag('STATIC_TICKET', False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enable_shifted_1(self):\n        self._require_version(major=2)\n        self.config_flag('STRONG_PW2', True)\n        self.config_flag('SEND_REF', True)", "response": "Enables shifted 1 for YubiKey output."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode a 16 byte AES128 key into a YubiKey.", "response": "def aes_key(self, data):\n        \"\"\"\n        AES128 key to program into YubiKey.\n\n        Supply data as either a raw string, or a hexlified string prefixed by 'h:'.\n        The result, after any hex decoding, must be 16 bytes.\n        \"\"\"\n        old = self.key\n        if data:\n            new = self._decode_input_string(data)\n            if len(new) == 16:\n                self.key = new\n            else:\n                raise yubico_exception.InputError('AES128 key must be exactly 16 bytes')\n\n        return old"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unlock_key(self, data):\n        if data.startswith(b'h:'):\n            new = binascii.unhexlify(data[2:])\n        else:\n            new = data\n        if len(new) == 6:\n            self.unlock_code = new\n            if not self.access_code:\n                # Don't reset the access code when programming, unless that seems\n                # to be the intent of the calling program.\n                self.access_code = new\n        else:\n            raise yubico_exception.InputError('Unlock key must be exactly 6 bytes')", "response": "Access code to allow re-programming of your YubiKey.\n\n        Supply data as either a raw bytestring, or a hexlified bytestring prefixed by 'h:'.\n        The result, after any hex decoding, must be 6 bytes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef access_key(self, data):\n        if data.startswith(b'h:'):\n            new = binascii.unhexlify(data[2:])\n        else:\n            new = data\n        if len(new) == 6:\n            self.access_code = new\n        else:\n            raise yubico_exception.InputError('Access key must be exactly 6 bytes')", "response": "Set the access code of the YubiKey."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the YubiKey up for standard OTP validation.", "response": "def mode_yubikey_otp(self, private_uid, aes_key):\n        \"\"\"\n        Set the YubiKey up for standard OTP validation.\n        \"\"\"\n        if not self.capabilities.have_yubico_OTP():\n            raise yubikey_base.YubiKeyVersionError('Yubico OTP not available in %s version %d.%d' \\\n                                                   % (self.capabilities.model, self.ykver[0], self.ykver[1]))\n        if private_uid.startswith(b'h:'):\n            private_uid = binascii.unhexlify(private_uid[2:])\n        if len(private_uid) != yubikey_defs.UID_SIZE:\n            raise yubico_exception.InputError('Private UID must be %i bytes' % (yubikey_defs.UID_SIZE))\n\n        self._change_mode('YUBIKEY_OTP', major=0, minor=9)\n        self.uid = private_uid\n        self.aes_key(aes_key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the YubiKey up for OATH - HOTP operation.", "response": "def mode_oath_hotp(self, secret, digits=6, factor_seed=None, omp=0x0, tt=0x0, mui=''):\n        \"\"\"\n        Set the YubiKey up for OATH-HOTP operation.\n\n        Requires YubiKey 2.1.\n        \"\"\"\n        if not self.capabilities.have_OATH('HOTP'):\n            raise yubikey_base.YubiKeyVersionError('OATH HOTP not available in %s version %d.%d' \\\n                                                   % (self.capabilities.model, self.ykver[0], self.ykver[1]))\n        if digits != 6 and digits != 8:\n            raise yubico_exception.InputError('OATH-HOTP digits must be 6 or 8')\n\n        self._change_mode('OATH_HOTP', major=2, minor=1)\n        self._set_20_bytes_key(secret)\n        if digits == 8:\n            self.config_flag('OATH_HOTP8', True)\n        if omp or tt or mui:\n            decoded_mui = self._decode_input_string(mui)\n            fixed = yubico_util.chr_byte(omp) + yubico_util.chr_byte(tt) + decoded_mui\n            self.fixed_string(fixed)\n        if factor_seed:\n            self.uid = self.uid + struct.pack('<H', factor_seed)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mode_challenge_response(self, secret, type='HMAC', variable=True, require_button=False):\n        if not type.upper() in ['HMAC', 'OTP']:\n            raise yubico_exception.InputError('Invalid \\'type\\' (%s)' % type)\n        if not self.capabilities.have_challenge_response(type.upper()):\n            raise yubikey_base.YubiKeyVersionError('%s Challenge-Response not available in %s version %d.%d' \\\n                                                   % (type.upper(), self.capabilities.model, \\\n                                                          self.ykver[0], self.ykver[1]))\n        self._change_mode('CHAL_RESP', major=2, minor=2)\n        if type.upper() == 'HMAC':\n            self.config_flag('CHAL_HMAC', True)\n            self.config_flag('HMAC_LT64', variable)\n            self._set_20_bytes_key(secret)\n        else:\n            # type is 'OTP', checked above\n            self.config_flag('CHAL_YUBICO', True)\n            self.aes_key(secret)\n        self.config_flag('CHAL_BTN_TRIG', require_button)", "response": "Set the YubiKey up for challenge - response operation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ticket_flag(self, which, new=None):\n        flag = _get_flag(which, TicketFlags)\n        if flag:\n            if not self.capabilities.have_ticket_flag(flag):\n                raise yubikey_base.YubiKeyVersionError('Ticket flag %s requires %s, and this is %s %d.%d'\n                                                       % (which, flag.req_string(self.capabilities.model), \\\n                                                         self.capabilities.model, self.ykver[0], self.ykver[1]))\n            req_major, req_minor = flag.req_version()\n            self._require_version(major=req_major, minor=req_minor)\n            value = flag.to_integer()\n        else:\n            if type(which) is not int:\n                raise yubico_exception.InputError('Unknown non-integer TicketFlag (%s)' % which)\n            value = which\n\n        return self.ticket_flags.get_set(value, new)", "response": "Get or set a ticket flag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget or set a config flag.", "response": "def config_flag(self, which, new=None):\n        \"\"\"\n        Get or set a config flag.\n\n        'which' can be either a string ('PACING_20MS' etc.), or an integer.\n        You should ALWAYS use a string, unless you really know what you are doing.\n        \"\"\"\n        flag = _get_flag(which, ConfigFlags)\n        if flag:\n            if not self.capabilities.have_config_flag(flag):\n                raise yubikey_base.YubiKeyVersionError('Config flag %s requires %s, and this is %s %d.%d'\n                                                       % (which, flag.req_string(self.capabilities.model), \\\n                                                         self.capabilities.model, self.ykver[0], self.ykver[1]))\n            req_major, req_minor = flag.req_version()\n            self._require_version(major=req_major, minor=req_minor)\n            value = flag.to_integer()\n        else:\n            if type(which) is not int:\n                raise yubico_exception.InputError('Unknown non-integer ConfigFlag (%s)' % which)\n            value = which\n\n        return self.config_flags.get_set(value, new)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets or set an extended flag.", "response": "def extended_flag(self, which, new=None):\n        \"\"\"\n        Get or set a extended flag.\n\n        'which' can be either a string ('SERIAL_API_VISIBLE' etc.), or an integer.\n        You should ALWAYS use a string, unless you really know what you are doing.\n        \"\"\"\n        flag = _get_flag(which, ExtendedFlags)\n        if flag:\n            if not self.capabilities.have_extended_flag(flag):\n                raise yubikey_base.YubiKeyVersionError('Extended flag %s requires %s, and this is %s %d.%d'\n                                                       % (which, flag.req_string(self.capabilities.model), \\\n                                                         self.capabilities.model, self.ykver[0], self.ykver[1]))\n            req_major, req_minor = flag.req_version()\n            self._require_version(major=req_major, minor=req_minor)\n            value = flag.to_integer()\n        else:\n            if type(which) is not int:\n                raise yubico_exception.InputError('Unknown non-integer ExtendedFlag (%s)' % which)\n            value = which\n\n        return self.extended_flags.get_set(value, new)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_string(self):\n        #define UID_SIZE\t\t6\t/* Size of secret ID field */\n        #define FIXED_SIZE              16      /* Max size of fixed field */\n        #define KEY_SIZE                16      /* Size of AES key */\n        #define KEY_SIZE_OATH           20      /* Size of OATH-HOTP key (key field + first 4 of UID field) */\n        #define ACC_CODE_SIZE           6       /* Size of access code to re-program device */\n        #\n        #struct config_st {\n        #    unsigned char fixed[FIXED_SIZE];/* Fixed data in binary format */\n        #    unsigned char uid[UID_SIZE];    /* Fixed UID part of ticket */\n        #    unsigned char key[KEY_SIZE];    /* AES key */\n        #    unsigned char accCode[ACC_CODE_SIZE]; /* Access code to re-program device */\n        #    unsigned char fixedSize;        /* Number of bytes in fixed field (0 if not used) */\n        #    unsigned char extFlags;         /* Extended flags */\n        #    unsigned char tktFlags;         /* Ticket configuration flags */\n        #    unsigned char cfgFlags;         /* General configuration flags */\n        #    unsigned char rfu[2];           /* Reserved for future use */\n        #    unsigned short crc;             /* CRC16 value of all fields */\n        #};\n        t_rfu = 0\n\n        first = struct.pack('<16s6s16s6sBBBBH',\n                            self.fixed,\n                            self.uid,\n                            self.key,\n                            self.access_code,\n                            len(self.fixed),\n                            self.extended_flags.to_integer(),\n                            self.ticket_flags.to_integer(),\n                            self.config_flags.to_integer(),\n                            t_rfu\n                            )\n\n        crc = 0xffff - yubico_util.crc16(first)\n\n        second = first + struct.pack('<H', crc) + self.unlock_code\n        return second", "response": "Return the current configuration as a bytestring."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the current configuration as a YubiKeyFrame object.", "response": "def to_frame(self, slot=1):\n        \"\"\"\n        Return the current configuration as a YubiKeyFrame object.\n        \"\"\"\n        data = self.to_string()\n        payload = data.ljust(64, yubico_util.chr_byte(0x0))\n        if slot is 1:\n            if self._update_config:\n                command = SLOT.UPDATE1\n            else:\n                command = SLOT.CONFIG\n        elif slot is 2:\n            if self._update_config:\n                command = SLOT.UPDATE2\n            else:\n                command = SLOT.CONFIG2\n        else:\n            assert()\n\n        if self._swap_slots:\n            command = SLOT.SWAP\n\n        if self._zap:\n            payload = b''\n\n        return yubikey_frame.YubiKeyFrame(command=command, payload=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the minimum version of the YubiKey that this configuration can be applied to.", "response": "def _require_version(self, major, minor=0):\n        \"\"\" Update the minimum version of YubiKey this configuration can be applied to. \"\"\"\n        new_ver = (major, minor)\n        if self.ykver and new_ver > self.ykver:\n            raise yubikey_base.YubiKeyVersionError('Configuration requires YubiKey %d.%d, and this is %d.%d'\n                                              % (major, minor, self.ykver[0], self.ykver[1]))\n        if new_ver > self.yk_req_version:\n            self.yk_req_version = new_ver"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the mode of the current instance.", "response": "def _change_mode(self, mode, major, minor):\n        \"\"\" Change mode of operation, with some sanity checks. \"\"\"\n        if self._mode:\n            if self._mode != mode:\n                raise RuntimeError('Can\\'t change mode (from %s to %s)' % (self._mode, mode))\n        self._require_version(major=major, minor=minor)\n        self._mode = mode\n        # when setting mode, we reset all flags\n        self.ticket_flags = YubiKeyConfigBits(0x0)\n        self.config_flags = YubiKeyConfigBits(0x0)\n        self.extended_flags = YubiKeyConfigBits(0x0)\n        if mode != 'YUBIKEY_OTP':\n            self.ticket_flag(mode, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_20_bytes_key(self, data):\n        if data.startswith(b'h:'):\n            new = binascii.unhexlify(data[2:])\n        else:\n            new = data\n        if len(new) == 20:\n            self.key = new[:16]\n            self.uid = new[16:]\n        else:\n            raise yubico_exception.InputError('HMAC key must be exactly 20 bytes')", "response": "Set a 20 bytes key. This is used in CHAL_HMAC and OATH_HOTP mode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef crc16(data):\n    m_crc = 0xffff\n    for this in data:\n        m_crc ^= ord_byte(this)\n        for _ in range(8):\n            j = m_crc & 1\n            m_crc >>= 1\n            if j:\n                m_crc ^= 0x8408\n    return m_crc", "response": "Calculate an ISO13239 CRC checksum of the input buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nproduces a string hexdump of src.", "response": "def hexdump(src, length=8, colorize=False):\n    \"\"\" Produce a string hexdump of src, for debug output.\n\n    Input: bytestring; output: text string\n    \"\"\"\n    if not src:\n        return str(src)\n    if type(src) is not bytes:\n        raise yubico_exception.InputError('Hexdump \\'src\\' must be bytestring (got %s)' % type(src))\n    offset = 0\n    result = ''\n    for this in group(src, length):\n        if colorize:\n            last, this = this[-1], this[:-1]\n            colors = DumpColors()\n            color = colors.get('RESET')\n            if ord_byte(last) & yubikey_defs.RESP_PENDING_FLAG:\n                # write to key\n                color = colors.get('BLUE')\n            elif ord_byte(last) & yubikey_defs.SLOT_WRITE_FLAG:\n                color = colors.get('GREEN')\n            hex_s = color + ' '.join([\"%02x\" % ord_byte(x) for x in this]) + colors.get('RESET')\n            hex_s += \" %02x\" % ord_byte(last)\n        else:\n            hex_s = ' '.join([\"%02x\" % ord_byte(x) for x in this])\n        result += \"%04X   %s\\n\" % (offset, hex_s)\n        offset += length\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef group(data, num):\n    return [data[i:i+num] for i in range(0, len(data), num)]", "response": "Split data into chunks of num chars each"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modhex_decode(data):\n    try:\n        maketrans = string.maketrans\n    except AttributeError:\n        # Python 3\n        maketrans = bytes.maketrans\n    t_map = maketrans(b\"cbdefghijklnrtuv\", b\"0123456789abcdef\")\n    return data.translate(t_map)", "response": "Convert a modhex bytestring to ordinary hex."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntruncates the HOTP Algorithm.", "response": "def hotp_truncate(hmac_result, length=6):\n    \"\"\" Perform the HOTP Algorithm truncating.\n\n    Input is a bytestring.\n    \"\"\"\n    if len(hmac_result) != 20:\n        raise yubico_exception.YubicoError(\"HMAC-SHA-1 not 20 bytes long\")\n    offset   =  ord_byte(hmac_result[19]) & 0xf\n    bin_code = (ord_byte(hmac_result[offset]) & 0x7f) << 24 \\\n        | (ord_byte(hmac_result[offset+1]) & 0xff) << 16 \\\n        | (ord_byte(hmac_result[offset+2]) & 0xff) <<  8 \\\n        | (ord_byte(hmac_result[offset+3]) & 0xff)\n    return bin_code % (10 ** length)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tlv_parse(data):\n    parsed = {}\n    while data:\n        t, l, data = ord_byte(data[0]), ord_byte(data[1]), data[2:]\n        parsed[t], data = data[:l], data[l:]\n    return parsed", "response": "Parses a bytestring of TLV values into a dict with the tags as keys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, what):\n        if self.enabled:\n            if what in self.colors:\n                return self.colors[what]\n        return ''", "response": "Get the ANSI code for what"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a set of all USB modes with optional filtering.", "response": "def all(cls, otp=False, ccid=False, u2f=False):\n        \"\"\"Returns a set of all USB modes, with optional filtering\"\"\"\n        modes = set([\n            cls.OTP,\n            cls.CCID,\n            cls.OTP_CCID,\n            cls.U2F,\n            cls.OTP_U2F,\n            cls.U2F_CCID,\n            cls.OTP_U2F_CCID\n        ])\n\n        if otp:\n            modes.difference_update(set([\n                cls.CCID,\n                cls.U2F,\n                cls.U2F_CCID\n            ]))\n\n        if ccid:\n            modes.difference_update(set([\n                cls.OTP,\n                cls.U2F,\n                cls.OTP_U2F\n            ]))\n\n        if u2f:\n            modes.difference_update(set([\n                cls.OTP,\n                cls.CCID,\n                cls.OTP_CCID\n            ]))\n\n        return modes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all(cls, otp=False, ccid=False, u2f=False):\n        pids = set([\n            cls.YUBIKEY,\n            cls.NEO_OTP,\n            cls.NEO_OTP_CCID,\n            cls.NEO_CCID,\n            cls.NEO_U2F,\n            cls.NEO_OTP_U2F,\n            cls.NEO_U2F_CCID,\n            cls.NEO_OTP_U2F_CCID,\n            cls.NEO_SKY,\n            cls.YK4_OTP,\n            cls.YK4_U2F,\n            cls.YK4_OTP_U2F,\n            cls.YK4_CCID,\n            cls.YK4_OTP_CCID,\n            cls.YK4_U2F_CCID,\n            cls.YK4_OTP_U2F_CCID,\n            cls.PLUS_U2F_OTP\n        ])\n\n        if otp:\n            pids.difference_update(set([\n                cls.NEO_CCID,\n                cls.NEO_U2F,\n                cls.NEO_U2F_CCID,\n                cls.NEO_SKY,\n                cls.YK4_U2F,\n                cls.YK4_CCID,\n                cls.YK4_U2F_CCID\n            ]))\n\n        if ccid:\n            pids.difference_update(set([\n                cls.YUBIKEY,\n                cls.NEO_OTP,\n                cls.NEO_U2F,\n                cls.NEO_OTP_U2F,\n                cls.NEO_SKY,\n                cls.YK4_OTP,\n                cls.YK4_U2F,\n                cls.YK4_OTP_U2F,\n                cls.PLUS_U2F_OTP\n            ]))\n\n        if u2f:\n            pids.difference_update(set([\n                cls.YUBIKEY,\n                cls.NEO_OTP,\n                cls.NEO_OTP_CCID,\n                cls.NEO_CCID,\n                cls.YK4_OTP,\n                cls.YK4_CCID,\n                cls.YK4_OTP_CCID\n            ]))\n\n        return pids", "response": "Returns a set of all the YubiKey PIDs that are in the YubiKey."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the frame as a 70 byte string.", "response": "def to_string(self):\n        \"\"\"\n        Return the frame as a 70 byte string.\n        \"\"\"\n        # From ykdef.h :\n        #\n        # // Frame structure\n\t# #define SLOT_DATA_SIZE  64\n        # typedef struct {\n        #     unsigned char payload[SLOT_DATA_SIZE];\n        #     unsigned char slot;\n        #     unsigned short crc;\n        #     unsigned char filler[3];\n        # } YKFRAME;\n        filler = b''\n        return struct.pack('<64sBH3s',\n                           self.payload, self.command, self.crc, filler)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_feature_reports(self, debug=False):\n        rest = self.to_string()\n        seq = 0\n        out = []\n        # When sending a frame to the YubiKey, we can (should) remove any\n        # 7-byte serie that only consists of '\\x00', besides the first\n        # and last serie.\n        while rest:\n            this, rest = rest[:7], rest[7:]\n            if seq > 0 and rest:\n                # never skip first or last serie\n                if this != b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00':\n                    this += yubico_util.chr_byte(yubikey_defs.SLOT_WRITE_FLAG + seq)\n                    out.append(self._debug_string(debug, this))\n            else:\n                this += yubico_util.chr_byte(yubikey_defs.SLOT_WRITE_FLAG + seq)\n                out.append(self._debug_string(debug, this))\n            seq += 1\n        return out", "response": "Return the frame as an array of 8 - byte parts ready to be sent to a YubiKey."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string representation of the ISO 1918 - formatted ISO - 8601 data.", "response": "def _debug_string(self, debug, data):\n        \"\"\"\n        Annotate a frames data, if debug is True.\n        \"\"\"\n        if not debug:\n            return data\n        if self.command in [\n            SLOT.CONFIG,\n            SLOT.CONFIG2,\n            SLOT.UPDATE1,\n            SLOT.UPDATE2,\n            SLOT.SWAP,\n        ]:\n            # annotate according to config_st (see ykdef.h)\n            if yubico_util.ord_byte(data[-1]) == 0x80:\n                return (data, \"FFFFFFF\")  # F = Fixed data (16 bytes)\n            if yubico_util.ord_byte(data[-1]) == 0x81:\n                return (data, \"FFFFFFF\")\n            if yubico_util.ord_byte(data[-1]) == 0x82:\n                return (data, \"FFUUUUU\")  # U = UID (6 bytes)\n            if yubico_util.ord_byte(data[-1]) == 0x83:\n                return (data, \"UKKKKKK\")  # K = Key (16 bytes)\n            if yubico_util.ord_byte(data[-1]) == 0x84:\n                return (data, \"KKKKKKK\")\n            if yubico_util.ord_byte(data[-1]) == 0x85:\n                return (data, \"KKKAAAA\")  # A = Access code to set (6 bytes)\n            if yubico_util.ord_byte(data[-1]) == 0x86:\n                return (data, \"AAlETCr\")  # l = Length of fixed field (1 byte)\n                                          # E = extFlags (1 byte)\n                                          # T = tktFlags (1 byte)\n                                          # C = cfgFlags (1 byte)\n                                          # r = RFU (2 bytes)\n            if yubico_util.ord_byte(data[-1]) == 0x87:\n                return (data, \"rCRaaaa\")  # CR = CRC16 checksum (2 bytes)\n                                          # a = Access code to use (6 bytes)\n            if yubico_util.ord_byte(data[-1]) == 0x88:\n                return (data, 'aa')\n            # after payload\n            if yubico_util.ord_byte(data[-1]) == 0x89:\n                return (data, \" Scr\")\n\n        return (data, '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the current version as defined by yubico_version. py.", "response": "def get_version():\n    \"\"\"Return the current version as defined by yubico/yubico_version.py.\"\"\"\n\n    with open('yubico/yubico_version.py', 'r') as f:\n        match = VERSION_PATTERN.search(f.read())\n        return match.group(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites an NDEF tag configuration to the YubiKey NEO.", "response": "def write_ndef(self, ndef, slot=1):\n        \"\"\"\n        Write an NDEF tag configuration to the YubiKey NEO.\n        \"\"\"\n        if not self.capabilities.have_nfc_ndef(slot):\n            raise yubikey_base.YubiKeyVersionError(\"NDEF slot %i unsupported in %s\" % (slot, self))\n\n        return self._device._write_config(ndef, _NDEF_SLOTS[slot])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_device_config(self, device_config):\n        if not self.capabilities.have_usb_mode(device_config._mode):\n            raise yubikey_base.YubiKeyVersionError(\"USB mode: %02x not supported for %s\" % (device_config._mode, self))\n        return self._device._write_config(device_config, SLOT.DEVICE_CONFIG)", "response": "Write a DEVICE_CONFIG to the NEO."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures parameters for NDEF type TEXT.", "response": "def text(self, encoding = 'UTF-8', language = 'en'):\n        \"\"\"\n        Configure parameters for NDEF type TEXT.\n\n        @param encoding: The encoding used. Should be either 'UTF-8' or 'UTF16'.\n        @param language: ISO/IANA language code (see RFC 3066).\n        \"\"\"\n        self.ndef_type = _NDEF_TEXT_TYPE\n        self.ndef_text_lang = language\n        self.ndef_text_enc = encoding\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the NDEF type.", "response": "def type(self, url = False, text = False, other = None):\n        \"\"\"\n        Change the NDEF type.\n        \"\"\"\n        if (url, text, other) == (True, False, None):\n            self.ndef_type = _NDEF_URI_TYPE\n        elif (url, text, other) == (False, True, None):\n            self.ndef_type = _NDEF_TEXT_TYPE\n        elif (url, text, type(other)) == (False, False, int):\n            self.ndef_type = other\n        else:\n            raise YubiKeyNEO_USBHIDError(\"Bad or conflicting NDEF type specified\")\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the current NDEF as a string.", "response": "def to_string(self):\n        \"\"\"\n        Return the current NDEF as a string (always 64 bytes).\n        \"\"\"\n        data = self.ndef_str\n        if self.ndef_type == _NDEF_URI_TYPE:\n            data = self._encode_ndef_uri_type(data)\n        elif self.ndef_type == _NDEF_TEXT_TYPE:\n            data = self._encode_ndef_text_params(data)\n        if len(data) > _NDEF_DATA_SIZE:\n            raise YubiKeyNEO_USBHIDError(\"NDEF payload too long\")\n        # typedef struct {\n        #   unsigned char len;                  // Payload length\n        #   unsigned char type;                 // NDEF type specifier\n        #   unsigned char data[NDEF_DATA_SIZE]; // Payload size\n        #   unsigned char curAccCode[ACC_CODE_SIZE]; // Access code\n        # } YKNDEF;\n        #\n        fmt = '< B B %ss %ss' % (_NDEF_DATA_SIZE, _ACC_CODE_SIZE)\n        first = struct.pack(fmt,\n                            len(data),\n                            self.ndef_type,\n                            data.ljust(_NDEF_DATA_SIZE, b'\\0'),\n                            self.access_code,\n                            )\n        #crc = 0xffff - yubico_util.crc16(first)\n        #second = first + struct.pack('<H', crc) + self.unlock_code\n        return first"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode the NDEF URI Type field.", "response": "def _encode_ndef_uri_type(self, data):\n        \"\"\"\n        Implement NDEF URI Identifier Code.\n\n        This is a small hack to replace some well known prefixes (such as http://)\n        with a one byte code. If the prefix is not known, 0x00 is used.\n        \"\"\"\n        t = 0x0\n        for (code, prefix) in uri_identifiers:\n            if data[:len(prefix)].decode('latin-1').lower() == prefix:\n                t = code\n                data = data[len(prefix):]\n                break\n        data = yubico_util.chr_byte(t) + data\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _encode_ndef_text_params(self, data):\n        status = len(self.ndef_text_lang)\n        if self.ndef_text_enc == 'UTF16':\n            status = status & 0b10000000\n        return yubico_util.chr_byte(status) + self.ndef_text_lang + data", "response": "Encode the ndef text parameters according to the NDEF - TEXT - ENCODED parameter set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the current DEVICE_CONFIG as a string.", "response": "def to_string(self):\n        \"\"\"\n        Return the current DEVICE_CONFIG as a string (always 4 bytes).\n        \"\"\"\n        fmt = '<BBH'\n        first = struct.pack(\n            fmt,\n            self._mode,\n            self._cr_timeout,\n            self._auto_eject_time\n        )\n\n        #crc = 0xffff - yubico_util.crc16(first)\n        #second = first + struct.pack('<H', crc)\n        return first"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_frame(self, slot=SLOT.DEVICE_CONFIG):\n        data = self.to_string()\n        payload = data.ljust(64, b'\\0')\n        return yubikey_frame.YubiKeyFrame(command=slot, payload=payload)", "response": "Return the current configuration as a YubiKeyFrame object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the current configuration as a YubiKeyFrame object.", "response": "def to_frame(self, slot=SLOT.SCAN_MAP):\n        \"\"\"\n        Return the current configuration as a YubiKeyFrame object.\n        \"\"\"\n        payload = self.scanmap.ljust(64, b'\\0')\n        return yubikey_frame.YubiKeyFrame(command=slot, payload=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlocating a connected YubiKey. Throws an exception if none is found.", "response": "def find_key(debug=False, skip=0):\n    \"\"\"\n    Locate a connected YubiKey. Throws an exception if none is found.\n\n    This function is supposed to be possible to extend if any other YubiKeys\n    appear in the future.\n\n    Attributes :\n        skip  -- number of YubiKeys to skip\n        debug -- True or False\n    \"\"\"\n    try:\n        hid_device = YubiKeyHIDDevice(debug, skip)\n        yk_version = hid_device.status().ykver()\n        if (2, 1, 4) <= yk_version <= (2, 1, 9):\n            return YubiKeyNEO_USBHID(debug, skip, hid_device)\n        if yk_version < (3, 0, 0):\n            return YubiKeyUSBHID(debug, skip, hid_device)\n        if yk_version < (4, 0, 0):\n            return YubiKeyNEO_USBHID(debug, skip, hid_device)\n        return YubiKey4_USBHID(debug, skip, hid_device)\n    except YubiKeyUSBHIDError as inst:\n        if 'No USB YubiKey found' in str(inst):\n            # generalize this error\n            raise YubiKeyError('No YubiKey found')\n        else:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef norm_package_version(version):\n    if version:\n        version = ','.join(v.strip() for v in version.split(',')).strip()\n\n        if version.startswith('(') and version.endswith(')'):\n            version = version[1:-1]\n\n        version = ''.join(v for v in version if v.strip())\n    else:\n        version = ''\n\n    return version", "response": "Normalize a version by removing extra spaces and parentheses."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits a spec by separator and return stripped start and end parts.", "response": "def split_spec(spec, sep):\n    \"\"\"Split a spec by separator and return stripped start and end parts.\"\"\"\n    parts = spec.rsplit(sep, 1)\n    spec_start = parts[0].strip()\n    spec_end = ''\n    if len(parts) == 2:\n        spec_end = parts[-1].strip()\n    return spec_start, spec_end"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_specification(spec):\n    name, extras, const = spec, [], ''\n\n    # Remove excess whitespace\n    spec = ' '.join(p for p in spec.split(' ') if p).strip()\n\n    # Extract marker (Assumes that there can only be one ';' inside the spec)\n    spec, marker = split_spec(spec, ';')\n\n    # Extract url (Assumes that there can only be one '@' inside the spec)\n    spec, url = split_spec(spec, '@')\n\n    # Find name, extras and constraints\n    r = PARTIAL_PYPI_SPEC_PATTERN.match(spec)\n    if r:\n        # Normalize name\n        name = r.group('name')\n\n        # Clean extras\n        extras = r.group('extras')\n        extras = [e.strip() for e in extras.split(',') if e] if extras else []\n\n        # Clean constraints\n        const = r.group('constraints')\n        const = ''.join(c for c in const.split(' ') if c).strip()\n        if const.startswith('(') and const.endswith(')'):\n            # Remove parens\n            const = const[1:-1]\n\n    return name, extras, const, marker, url", "response": "Parses a python distribution metadata specification and returns a tuple with name extras constraints marker url components."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting description from metadata file and remove any empty lines at end.", "response": "def get_header_description(filedata):\n    \"\"\"Get description from metadata file and remove any empty lines at end.\"\"\"\n    python_version = sys.version_info.major\n    if python_version == 3:\n        filedata = Parser().parsestr(filedata)\n    else:\n        filedata = Parser().parsestr(filedata.encode(\"UTF-8\", \"replace\"))\n\n    payload = filedata.get_payload()\n    lines = payload.split('\\n')\n    while True:\n        if lines and lines[-1] == '':\n            lines.pop()\n        else:\n            break\n    return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_sdist_header_metadata(data, filename):\n    description = get_header_description(data)\n    config_items = python_version_check(data)\n    attrs = dict(config_items)\n    name = pop_key(attrs, 'Name', None)\n\n    basename = path.basename(filename)\n    if name is None:\n        name = basename.split('-')[0]\n\n    package_data = {\n        'name': name,\n        'summary': pop_key(attrs, 'Summary', None),\n        'license': pop_key(attrs, 'License', None),\n    }\n\n    release_data = {\n        'version': pop_key(attrs, 'Version'),\n        'description': pop_key(attrs, 'Description', description),\n        'home_page': pop_key(attrs, 'Home-page', None),\n    }\n\n    file_data = {\n        'basename': basename,\n        'attrs': {\n            'packagetype': 'sdist',\n            'python_version': 'source',\n        }\n    }\n\n    # Parse multiple keys\n    deps = []\n    exts = {}\n    environments = {}\n    for key, val in config_items:\n        if key in ['Requires-Dist', 'Requires']:\n            name, extras, const, marker, url = parse_specification(val)\n            name = norm_package_name(name)\n            specs = const.split(',')\n            new_specs = []\n            for spec in specs:\n                pos = [i for i, c in enumerate(spec) if c in '0123456789']\n                if pos:\n                    pos = pos[0]\n                    comp, spec_ = spec[:pos].strip(), spec[pos:].strip()\n                    new_specs.append((comp, spec_))\n\n            # TODO: All this is to preserve the format used originally\n            # but is this really needed?\n            if marker:\n                if marker.startswith('extra'):\n                    marker = marker.replace('extra', '')\n                    marker = marker.replace('==', '').strip()\n                    ext = marker.rsplit(' ')[-1]\n                    if '\"' in ext or \"'\" in ext:\n                        ext = ext[1:-1]\n\n                    if ext not in exts:\n                        exts[ext] = [{'name': name, 'specs': new_specs}]\n                    else:\n                        exts[ext].append({'name': name, 'specs': new_specs})\n                else:\n                    if marker not in environments:\n                        environments[marker] = [{'name': name, 'specs': new_specs}]\n                    else:\n                        environments[marker].append({'name': name, 'specs': new_specs})\n            else:\n                deps.append({\n                    'name': name,\n                    'specs': new_specs,\n                })\n\n    deps.sort(key=lambda o: o['name'])\n\n    new_exts = []\n    for key, values in exts.items():\n        new_exts.append({'name': key, 'depends': values})\n\n    new_environments = []\n    for key, values in environments.items():\n        new_environments.append({'name': key, 'depends': values})\n\n    file_data.update(dependencies={\n        'has_dep_errors': False,\n        'depends': deps,\n        'extras': new_exts,\n        'environments': new_environments,\n    })\n\n    return package_data, release_data, file_data", "response": "Format the metadata of pypi packages stored in email header format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the file type from the inspected package or from the package - type argument.", "response": "def determine_package_type(filename, args):\n    \"\"\"\n    return the file type from the inspected package or from the\n    -t/--package-type argument\n    \"\"\"\n    if args.package_type:\n        package_type = args.package_type\n    else:\n        logger.info('Detecting file type...')\n\n        package_type = detect_package_type(filename)\n\n        if package_type is None:\n            message = 'Could not detect package type of file %r please specify package type with option --package-type' % filename\n            logger.error(message)\n            raise errors.BinstarError(message)\n\n        logger.info('File type is \"%s\"', package_type)\n\n    return package_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parameterize(string, separator='-'):\n    string = transliterate(string)\n    # Turn unwanted chars into the separator\n    string = re.sub(r\"(?i)[^a-z0-9\\-_]+\", separator, string)\n    if separator:\n        re_sep = re.escape(separator)\n        # No more than one of the separator in a row.\n        string = re.sub(r'%s{2,}' % re_sep, separator, string)\n        # Remove leading/trailing separator.\n        string = re.sub(r\"(?i)^%(sep)s|%(sep)s$\" % {'sep': re_sep}, '', string)\n\n    return string.lower()", "response": "Translate a string into a valid URL parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transliterate(string):\n    try:\n        normalized = unicodedata.normalize('NFKD', unicode(string))\n    except NameError:\n        normalized = unicodedata.normalize('NFKD', string)\n    return normalized.encode('ascii', 'ignore').decode('ascii')", "response": "Transliterate a string to a non - ASCII character."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting the channels for owner", "response": "def list_channels(self, owner):\n        '''List the channels for owner\n        If owner is none, the currently logged in user is used\n        '''\n        url = '%s/channels/%s' % (self.domain, owner)\n\n        res = self.session.get(url)\n        self._check_response(res, [200])\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_channel(self, channel, owner):\n        '''List the channels for owner\n        If owner is none, the currently logged in user is used\n        '''\n        url = '%s/channels/%s/%s' % (self.domain, owner, channel)\n\n        res = self.session.get(url)\n        self._check_response(res, [200])\n        return res.json()", "response": "List the channels for owner\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_channel(self, channel, owner, package=None, version=None, filename=None):\n        '''\n        Add a channel to the specified files\n        \n        :param channel: channel to add\n        :param owner: The user to add the channel to (all files of all packages for this user)\n        :param package: The package to add the channel to (all files in this package)\n        :param version: The version to add the channel to (all files in this version of the package)\n        :param filename: The exact file to add the channel to\n        \n        '''\n        url = '%s/channels/%s/%s' % (self.domain, owner, channel)\n        data, headers = jencode(package=package, version=version, basename=filename)\n\n        res = self.session.post(url, data=data, headers=headers)\n        self._check_response(res, [201])", "response": "Add a channel to the specified files\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a channel from the specified files", "response": "def remove_channel(self, channel, owner, package=None, version=None, filename=None):\n        '''\n        Remove a channel from the specified files\n        \n        :param channel: channel to remove\n        :param owner: The user to remove the channel from (all files of all packages for this user)\n        :param package: The package to remove the channel from (all files in this package)\n        :param version: The version to remove the channel to (all files in this version of the package)\n        :param filename: The exact file to remove the channel from\n        \n        '''\n        url = '%s/channels/%s/%s' % (self.domain, owner, channel)\n        data, headers = jencode(package=package, version=version, basename=filename)\n\n        res = self.session.delete(url, data=data, headers=headers)\n        self._check_response(res, [201])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy all files in a channel also as a channel to another name", "response": "def copy_channel(self, channel, owner, to_channel):\n        '''\n        Tag all files in channel <channel> also as channel <to_channel> \n        \n        :param channel: channel to copy\n        :param owner: Perform this operation on all packages of this user\n        :param to_channel: Destination name (may be a channel that already exists)\n        \n        '''\n        url = '%s/channels/%s/%s/copy/%s' % (self.domain, owner, channel, to_channel)\n        res = self.session.post(url)\n        self._check_response(res, [201])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lock_channel(self, channel, owner):\n        '''\n        Tag all files in channel <channel> also as channel <to_channel> \n        \n        :param channel: channel to copy\n        :param owner: Perform this operation on all packages of this user\n        :param to_channel: Destination name (may be a channel that already exists)\n        \n        '''\n        url = '%s/channels/%s/%s/lock' % (self.domain, owner, channel)\n        res = self.session.post(url)\n        self._check_response(res, [201])", "response": "Tag all files in channel also as channel <to_channel > \n "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unlock_channel(self, channel, owner):\n        '''\n        Tag all files in channel <channel> also as channel <to_channel> \n        \n        :param channel: channel to copy\n        :param owner: Perform this operation on all packages of this user\n        :param to_channel: Destination name (may be a channel that already exists)\n        \n        '''\n        url = '%s/channels/%s/%s/lock' % (self.domain, owner, channel)\n        res = self.session.delete(url)\n        self._check_response(res, [201])", "response": "Unlock all files in a channel also as a channel to copy the to_channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a string into username and notebooks", "response": "def parse(handle):\n    \"\"\"\n    >>> parse(\"user/notebook\")\n    ('user', 'notebook')\n    >>> parse(\"notebook\")\n    (None, 'notebook')\n\n    :param handle: String\n    :return: username, notebooks\n    \"\"\"\n\n    components = handle.split('/', 1)\n    if len(components) == 1:\n        return None, components[0]\n    elif len(components) == 2:\n        return components[0], components[1]\n    else:\n        raise BinstarError(\"{} can't be parsed\".format(handle))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload a notebook to the notebook.", "response": "def upload(self, force=False):\n        \"\"\"\n        Uploads a notebook\n        :param force: True/False\n        :returns {}\n        \"\"\"\n        self.package and self.release\n        try:\n            return self.aserver_api.upload(self.username, self.project, self.version,\n                                       basename(self.filepath), open(self.filepath, 'rb'),\n                                       self.filepath.split('.')[-1])\n        except errors.Conflict:\n            if force:\n                self.remove()\n                return self.upload()\n            else:\n                msg = \"Conflict: {} already exist in {}/{}\".format(self.filepath,\n                                                                   self.project,\n                                                                   self.version)\n                raise errors.BinstarError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to replace the token with the current one.", "response": "def try_replace_token(authenticate, **kwargs):\n    \"\"\"\n    Authenticates using the given *authenticate*, retrying if the token needs\n    to be replaced.\n    \"\"\"\n\n    try:\n        return authenticate(**kwargs)\n    except errors.BinstarError as err:\n        if kwargs.get('fail_if_already_exists') and len(err.args) > 1 and err.args[1] == 400:\n            logger.warning('It appears you are already logged in from host %s' % socket.gethostname())\n            logger.warning('Logging in again will remove the previous token. (This could cause troubles with virtual '\n                           'machines with the same hostname)')\n            logger.warning('Otherwise you can login again and specify a different hostname with \"--hostname\"')\n\n            if bool_input(\"Would you like to continue\"):\n                kwargs['fail_if_already_exists'] = False\n                return authenticate(**kwargs)\n\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_server(self):\n        msg = 'API server not found. Please check your API url configuration.'\n\n        try:\n            response = self.session.head(self.domain)\n        except Exception as e:\n            raise_from(errors.ServerError(msg), e)\n\n        try:\n            self._check_response(response)\n        except errors.NotFound as e:\n            raise raise_from(errors.ServerError(msg), e)", "response": "Checks if the server is reachable and throws an exception if it isn t."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an authentication token for the user.", "response": "def _authenticate(self,\n                     auth,\n                     application,\n                     application_url=None,\n                     for_user=None,\n                     scopes=None,\n                     created_with=None,\n                     max_age=None,\n                     strength='strong',\n                     fail_if_already_exists=False,\n                     hostname=platform.node()):\n        '''\n        Use basic authentication to create an authentication token using the interface below.\n        With this technique, a username and password need not be stored permanently, and the user can\n        revoke access at any time.\n\n        :param username: The users name\n        :param password: The users password\n        :param application: The application that is requesting access\n        :param application_url: The application's home page\n        :param scopes: Scopes let you specify exactly what type of access you need. Scopes limit access for the tokens.\n        '''\n\n        url = '%s/authentications' % (self.domain)\n        payload = {\"scopes\": scopes, \"note\": application, \"note_url\": application_url,\n                   'hostname': hostname,\n                   'user': for_user,\n                   'max-age': max_age,\n                   'created_with': None,\n                   'strength': strength,\n                   'fail-if-exists': fail_if_already_exists}\n\n        data, headers = jencode(payload)\n        res = self.session.post(url, auth=auth, data=data, headers=headers)\n        self._check_response(res)\n        res = res.json()\n        token = res['token']\n        self.session.headers.update({'Authorization': 'token %s' % (token)})\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving information on the current authentication token", "response": "def authentication(self):\n        '''\n        Retrieve information on the current authentication token\n        '''\n        url = '%s/authentication' % (self.domain)\n        res = self.session.get(url)\n        self._check_response(res)\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the current authentication or the one given by auth_name.", "response": "def remove_authentication(self, auth_name=None, organization=None):\n        \"\"\"\n        Remove the current authentication or the one given by `auth_name`\n        \"\"\"\n        if auth_name:\n            if organization:\n                url = '%s/authentications/org/%s/name/%s' % (self.domain, organization, auth_name)\n            else:\n                url = '%s/authentications/name/%s' % (self.domain, auth_name)\n        else:\n            url = '%s/authentications' % (self.domain,)\n\n        res = self.session.delete(url)\n        self._check_response(res, [201])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets user information. :param login: (optional) the login name of the user or None. If login is None this method will return the information of the authenticated user.", "response": "def user(self, login=None):\n        '''\n        Get user information.\n\n        :param login: (optional) the login name of the user or None. If login is None\n                      this method will return the information of the authenticated user.\n        '''\n        if login:\n            url = '%s/user/%s' % (self.domain, login)\n        else:\n            url = '%s/user' % (self.domain)\n\n        res = self.session.get(url, verify=self.session.verify)\n        self._check_response(res)\n\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user_packages(\n            self,\n            login=None,\n            platform=None,\n            package_type=None,\n            type_=None,\n            access=None):\n        '''\n        Returns a list of packages for a given user and optionally filter\n        by `platform`, `package_type` and `type_`.\n\n        :param login: (optional) the login name of the user or None. If login\n                      is None this method will return the packages for the\n                      authenticated user.\n        :param platform: only find packages that include files for this platform.\n           (e.g. 'linux-64', 'osx-64', 'win-32')\n        :param package_type: only find packages that have this kind of file\n           (e.g. 'env', 'conda', 'pypi')\n        :param type_: only find packages that have this conda `type`\n           (i.e. 'app')\n        :param access: only find packages that have this access level\n           (e.g. 'private', 'authenticated', 'public')\n        '''\n        if login:\n            url = '{0}/packages/{1}'.format(self.domain, login)\n        else:\n            url = '{0}/packages'.format(self.domain)\n\n        arguments = collections.OrderedDict()\n\n        if platform:\n            arguments['platform'] = platform\n        if package_type:\n            arguments['package_type'] = package_type\n        if type_:\n            arguments['type'] = type_\n        if access:\n            arguments['access'] = access\n\n        res = self.session.get(url, params=arguments)\n        self._check_response(res)\n\n        return res.json()", "response": "Returns a list of packages for a given user and optionally filter by platform package_type and type_ and access."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting information about a specific package", "response": "def package(self, login, package_name):\n        '''\n        Get information about a specific package\n\n        :param login: the login of the package owner\n        :param package_name: the name of the package\n        '''\n        url = '%s/package/%s/%s' % (self.domain, login, package_name)\n        res = self.session.get(url)\n        self._check_response(res)\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_package(self, login, package_name,\n                    summary=None,\n                    license=None,\n                    public=True,\n                    license_url=None,\n                    license_family=None,\n                    attrs=None,\n                    package_type=None):\n        '''\n        Add a new package to a users account\n\n        :param login: the login of the package owner\n        :param package_name: the name of the package to be created\n        :param package_type: A type identifier for the package (eg. 'pypi' or 'conda', etc.)\n        :param summary: A short summary about the package\n        :param license: the name of the package license\n        :param license_url: the url of the package license\n        :param public: if true then the package will be hosted publicly\n        :param attrs: A dictionary of extra attributes for this package\n        '''\n        url = '%s/package/%s/%s' % (self.domain, login, package_name)\n\n        attrs = attrs or {}\n        attrs['summary'] = summary\n        attrs['package_types'] = [package_type]\n        attrs['license'] = {\n            'name': license,\n            'url': license_url,\n            'family': license_family,\n        }\n\n        payload = dict(public=bool(public),\n                       publish=False,\n                       public_attrs=dict(attrs or {})\n                       )\n\n        data, headers = jencode(payload)\n        res = self.session.post(url, data=data, headers=headers)\n        self._check_response(res)\n        return res.json()", "response": "Add a new package to a users account"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef release(self, login, package_name, version):\n        '''\n        Get information about a specific release\n\n        :param login: the login of the package owner\n        :param package_name: the name of the package\n        :param version: the name of the package\n        '''\n        url = '%s/release/%s/%s/%s' % (self.domain, login, package_name, version)\n        res = self.session.get(url)\n        self._check_response(res)\n        return res.json()", "response": "Get information about a specific release of a specific package."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a release and all files under it", "response": "def remove_release(self, username, package_name, version):\n        '''\n        remove a release and all files under it\n\n        :param username: the login of the package owner\n        :param package_name: the name of the package\n        :param version: the name of the package\n        '''\n        url = '%s/release/%s/%s/%s' % (self.domain, username, package_name, version)\n        res = self.session.delete(url)\n        self._check_response(res, [201])\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new release to a package.", "response": "def add_release(self, login, package_name, version, requirements, announce, release_attrs):\n        '''\n        Add a new release to a package.\n\n        :param login: the login of the package owner\n        :param package_name: the name of the package\n        :param version: the version string of the release\n        :param requirements: A dict of requirements TODO: describe\n        :param announce: An announcement that will be posted to all package watchers\n        '''\n\n        url = '%s/release/%s/%s/%s' % (self.domain, login, package_name, version)\n\n        if not release_attrs:\n            release_attrs = {}\n\n        payload = {\n            'requirements': requirements,\n            'announce': announce,\n            'description': None,  # Will be updated with the one on release_attrs\n        }\n        payload.update(release_attrs)\n\n        data, headers = jencode(payload)\n        res = self.session.post(url, data=data, headers=headers)\n        self._check_response(res)\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a package from an anaconda. org and return a file like object or None", "response": "def download(self, login, package_name, release, basename, md5=None):\n        '''\n        Download a package distribution\n\n        :param login: the login of the package owner\n        :param package_name: the name of the package\n        :param version: the version string of the release\n        :param basename: the basename of the distribution to download\n        :param md5: (optional) an md5 hash of the download if given and the package has not changed\n                    None will be returned\n\n        :returns: a file like object or None\n        '''\n\n        url = '%s/download/%s/%s/%s/%s' % (self.domain, login, package_name, release, basename)\n        if md5:\n            headers = {'ETag':md5, }\n        else:\n            headers = {}\n\n        res = self.session.get(url, headers=headers, allow_redirects=False)\n        self._check_response(res, allowed=[200, 302, 304])\n\n        if res.status_code == 200:\n            # We received the content directly from anaconda.org\n            return res\n        elif res.status_code == 304:\n            # The content has not changed\n            return None\n        elif res.status_code == 302:\n            # Download from s3:\n            # We need to create a new request (without using session) to avoid\n            # sending the custom headers set on our session to S3 (which causes\n            # a failure).\n            res2 = requests.get(res.headers['location'], stream=True)\n            return res2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading a new distribution to a package release.", "response": "def upload(self, login, package_name, release, basename, fd, distribution_type,\n               description='', md5=None, size=None, dependencies=None, attrs=None, channels=('main',), callback=None):\n        '''\n        Upload a new distribution to a package release.\n\n        :param login: the login of the package owner\n        :param package_name: the name of the package\n        :param version: the version string of the release\n        :param basename: the basename of the distribution to download\n        :param fd: a file like object to upload\n        :param distribution_type: pypi or conda or ipynb, etc\n        :param description: (optional) a short description about the file\n        :param attrs: any extra attributes about the file (eg. build=1, pyversion='2.7', os='osx')\n\n        '''\n        url = '%s/stage/%s/%s/%s/%s' % (self.domain, login, package_name, release, quote(basename))\n        if attrs is None:\n            attrs = {}\n        if not isinstance(attrs, dict):\n            raise TypeError('argument attrs must be a dictionary')\n\n        payload = dict(distribution_type=distribution_type, description=description, attrs=attrs,\n                       dependencies=dependencies, channels=channels)\n\n        data, headers = jencode(payload)\n        res = self.session.post(url, data=data, headers=headers)\n        self._check_response(res)\n        obj = res.json()\n\n        s3url = obj['post_url']\n        s3data = obj['form_data']\n\n        if md5 is None:\n            _hexmd5, b64md5, size = compute_hash(fd, size=size)\n        elif size is None:\n            spos = fd.tell()\n            fd.seek(0, os.SEEK_END)\n            size = fd.tell() - spos\n            fd.seek(spos)\n\n        s3data['Content-Length'] = size\n        s3data['Content-MD5'] = b64md5\n\n        data_stream, headers = stream_multipart(s3data, files={'file':(basename, fd)},\n                                                callback=callback)\n\n        request_method = self.session if s3url.startswith(self.domain) else requests\n        s3res = request_method.post(\n            s3url, data=data_stream,\n            verify=self.session.verify, timeout=10 * 60 * 60,\n            headers=headers\n        )\n\n        if s3res.status_code != 201:\n            logger.info(s3res.text)\n            logger.info('')\n            logger.info('')\n            raise errors.BinstarError('Error uploading package', s3res.status_code)\n\n        url = '%s/commit/%s/%s/%s/%s' % (self.domain, login, package_name, release, quote(basename))\n        payload = dict(dist_id=obj['dist_id'])\n        data, headers = jencode(payload)\n        res = self.session.post(url, data=data, headers=headers)\n        self._check_response(res)\n\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload the user current trial / paid licenses.", "response": "def user_licenses(self):\n        \"\"\"Download the user current trial/paid licenses.\"\"\"\n        url = '{domain}/license'.format(domain=self.domain)\n        res = self.session.get(url)\n        self._check_response(res)\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform a list of dependencies into a common binstar format", "response": "def transform_conda_deps(deps):\n    \"\"\"\n    Format dependencies into a common binstar format\n    \"\"\"\n    depends = []\n    for dep in deps:\n        dep = dep.strip()\n        name_spec = dep.split(' ', 1)\n        if len(name_spec) == 1:\n            name, = name_spec\n            depends.append({'name':name, 'specs': []})\n        elif len(name_spec) == 2:\n            name, spec = name_spec\n            if spec.endswith('*'):  # Star does nothing in semver\n                spec = spec[:-1]\n\n            match = specs_re.match(spec)\n            if match:\n                op, spec = match.groups()\n            else:\n                op = '=='\n\n            depends.append({'name':name, 'specs': [[op, spec]]})\n        elif len(name_spec) == 3:\n            name, spec, build_str = name_spec\n            if spec.endswith('*'):  # Star does nothing in semver\n                spec = spec[:-1]\n\n            match = specs_re.match(spec)\n            if match:\n                op, spec = match.groups()\n            else:\n                op = '=='\n\n            depends.append({'name':name, 'specs': [['==', '%s+%s' % (spec, build_str)]]})\n\n    return {'depends': depends}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the sub - directory given the index dictionary.", "response": "def get_subdir(index):\n    \"\"\"\n    Return the sub-directory given the index dictionary.  The return\n    value is obtained in the following order:\n\n    1. when the 'subdir' key exists, it's value is returned\n    2. if the 'arch' is None, or does not exist, 'noarch' is returned\n    3. otherwise, the return value is constructed from the 'platform' key\n       and the 'arch' key (where 'x86' is replaced by '32',\n       and 'x86_64' by '64')\n    \"\"\"\n    try:\n        return index['subdir']\n    except KeyError:\n        arch = index.get('arch')\n        if arch is None:\n            return 'noarch'\n        intel_map = {'x86': '32', 'x86_64': '64'}\n        return '%s-%s' % (index.get('platform'), intel_map.get(arch, arch))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upload(self):\n        '''\n        * Check if project already exists\n            * if it doesn't, then create it\n        * stage a new revision\n        * upload\n        * commit revision\n        '''\n        if not self.exists():\n            self.create()\n\n        data = self.stage().json()\n        self.file_upload(data['post_url'], data)\n        res = self.commit(data['dist_id'])\n        data = res.json()\n        return data", "response": "Upload a new project to the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the value if value is a file path or a token value.", "response": "def file_or_token(value):\n    \"\"\"\n    If value is a file path and the file exists its contents are stripped and returned,\n    otherwise value is returned.\n    \"\"\"\n    if isfile(value):\n        with open(value) as fd:\n            return fd.read().strip()\n\n    if any(char in value for char in '/\\\\.'):\n        # This chars will never be in a token value, but may be in a path\n        # The error message will be handled by the parser\n        raise ValueError()\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode_multipart_formdata_stream(fields, boundary=None):\n    body = []\n\n    def body_write(item):\n        if isinstance(item, six.binary_type):\n            item = BytesIO(item)\n        elif isinstance(item, six.text_type):\n            item = StringIO(item)\n        body.append(item)\n\n    body_write_encode = lambda item: body.append(BytesIO(item.encode('utf-8')))\n\n    if boundary is None:\n        boundary = choose_boundary()\n\n    for fieldname, value in iter_fields(fields):\n        body_write_encode('--%s\\r\\n' % (boundary))\n\n        if isinstance(value, tuple):\n            if len(value) == 3:\n                filename, data, content_type = value\n            else:\n                filename, data = value\n                from mimetypes import guess_type\n                content_type, _ = guess_type(filename)\n                if content_type is None:\n                    content_type = 'application/octet-stream'\n            body_write_encode('Content-Disposition: form-data; name=\"%s\"; '\n                              'filename=\"%s\"\\r\\n' % (fieldname, filename))\n            body_write_encode('Content-Type: %s\\r\\n\\r\\n' %\n                              (content_type,))\n        else:\n            data = value\n            body_write_encode('Content-Disposition: form-data; name=\"%s\"\\r\\n'\n                               % (fieldname))\n            body_write(b'\\r\\n')\n\n        if isinstance(data, six.integer_types):\n            data = six.text_type(data)  # Backwards compatibility\n\n        if isinstance(data, six.text_type):\n            body_write_encode(data)\n        else:\n            body_write(data)\n\n        body_write(b'\\r\\n')\n\n    body_write_encode('--%s--\\r\\n' % (boundary))\n\n    content_type = 'multipart/form-data; boundary=%s' % (boundary)\n\n    return body, content_type", "response": "Encodes a dictionary of fields or list of fields using the multipart - form - data MIME format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the anaconda server api class", "response": "def get_server_api(token=None, site=None, cls=None, config=None, **kwargs):\n    \"\"\"\n    Get the anaconda server api class\n    \"\"\"\n    if not cls:\n        from binstar_client import Binstar\n        cls = Binstar\n\n    config = config if config is not None else get_config(site=site)\n\n    url = config.get('url', DEFAULT_URL)\n\n    logger.info(\"Using Anaconda API: %s\", url)\n\n    if token:\n        logger.debug(\"Using token from command line args\")\n    elif 'BINSTAR_API_TOKEN' in os.environ:\n        logger.debug(\"Using token from environment variable BINSTAR_API_TOKEN\")\n        token = os.environ['BINSTAR_API_TOKEN']\n    elif 'ANACONDA_API_TOKEN' in os.environ:\n        logger.debug(\"Using token from environment variable ANACONDA_API_TOKEN\")\n        token = os.environ['ANACONDA_API_TOKEN']\n    else:\n        token = load_token(url)\n\n    verify = config.get('ssl_verify', config.get('verify_ssl', True))\n\n    return cls(token, domain=url, verify=verify, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_binstar(args=None, cls=None):\n\n    warnings.warn(\n        'method get_binstar is deprecated, please use `get_server_api`',\n        DeprecationWarning\n    )\n\n    token = getattr(args, 'token', None)\n    log_level = getattr(args, 'log_level', logging.INFO)\n    site = getattr(args, 'site', None)\n\n    aserver_api = get_server_api(token=token, site=site, log_level=log_level, cls=cls)\n    return aserver_api", "response": "get_binstar is deprecated use get_server_api"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the ROOT_PREFIX of the conda installation.", "response": "def get_conda_root():\n    \"\"\"Get the PREFIX of the conda installation.\n\n    Returns:\n        str: the ROOT_PREFIX of the conda installation\n    \"\"\"\n    try:\n        # Fast-path\n        # We're in the root environment\n        conda_root = _import_conda_root()\n    except ImportError:\n        # We're not in the root environment.\n        envs_dir = dirname(CONDA_PREFIX)\n        if basename(envs_dir) == 'envs':\n            # We're in a named environment: `conda create -n <name>`\n            conda_root = dirname(envs_dir)\n        else:\n            # We're in an isolated environment: `conda create -p <path>`\n            # The only way we can find out is by calling conda.\n            conda_root = _conda_root_from_conda_info()\n\n    return conda_root"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef group_spec(spec):\n    '''<organization>/<group_name>/<member>'''\n    org = spec\n    group = member = None\n    if '/' in org:\n        org, group = org.split('/', 1)\n    if group and '/' in group:\n        group, member = group.split('/', 1)\n    if member and '/' in member:\n        raise UserError('Invalid group specification \"%s\" (unexpected \"/\" in %s)' % member)\n\n    return GroupSpec(org, group, member, spec)", "response": "Return a GroupSpec object from a group specification."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists all files in the log that are available for download.", "response": "def list_download_files(self, package_types, output='.', force=False):\n        \"\"\"\n        This additional method was created to better handle the log output\n        as files are downloaded one by one on the commands/download.py.\n        \"\"\"\n        self.output = output\n        self.ensure_output()\n        files = OrderedDict()\n        for f in self.list_files():\n            # Check type\n            pkg_type = f.get('type') or ''\n            if pkg_type in package_types:\n                if self.can_download(f, force):\n                    files[f['basename']] = f\n                else:\n                    raise DestionationPathExists(f['basename'])\n        return files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading file into location.", "response": "def download(self, dist):\n        \"\"\"\n        Download file into location.\n        \"\"\"\n        filename = dist['basename']\n        requests_handle = self.aserver_api.download(\n            self.username, self.notebook, dist['version'], filename\n        )\n\n        if not os.path.exists(os.path.dirname(filename)):\n            try:\n                os.makedirs(os.path.dirname(filename))\n            except OSError:\n                pass\n\n        with open(os.path.join(self.output, filename), 'wb') as fdout:\n            for chunk in requests_handle.iter_content(4096):\n                fdout.write(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if location or file exists and False otherwise.", "response": "def can_download(self, dist, force=False):\n        \"\"\"\n        Can download if location/file does not exist or if force=True\n        :param dist:\n        :param force:\n        :return: True/False\n        \"\"\"\n        return not os.path.exists(os.path.join(self.output, dist['basename'])) or force"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ensure_output(self):\n        if not os.path.exists(self.output):\n            os.makedirs(self.output)", "response": "Ensure output directory exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist available files in a project.", "response": "def list_files(self):\n        \"\"\"\n        List available files in a project (aka notebook).\n        :return: list\n        \"\"\"\n        output = []\n        tmp = {}\n\n        files = self.aserver_api.package(self.username, self.notebook)['files']\n\n        for f in files:\n            if f['basename'] in tmp:\n                tmp[f['basename']].append(f)\n            else:\n                tmp[f['basename']] = [f]\n\n        for basename, versions in tmp.items():\n            try:\n                output.append(max(versions, key=lambda x: int(x['version'])))\n            except ValueError:\n                output.append(\n                    max(versions, key=lambda x: mktime(parse(x['upload_time']).timetuple()))\n                )\n            except:\n                output.append(versions[-1])\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(exif_dict_original):\n    exif_dict = copy.deepcopy(exif_dict_original)\n    header = b\"Exif\\x00\\x00\\x4d\\x4d\\x00\\x2a\\x00\\x00\\x00\\x08\"\n    exif_is = False\n    gps_is = False\n    interop_is = False\n    first_is = False\n\n    if \"0th\" in exif_dict:\n        zeroth_ifd = exif_dict[\"0th\"]\n    else:\n        zeroth_ifd = {}\n\n    if ((\"Exif\" in exif_dict) and len(exif_dict[\"Exif\"]) or\n          (\"Interop\" in exif_dict) and len(exif_dict[\"Interop\"]) ):\n        zeroth_ifd[ImageIFD.ExifTag] = 1\n        exif_is = True\n        exif_ifd = exif_dict[\"Exif\"]\n        if (\"Interop\" in exif_dict) and len(exif_dict[\"Interop\"]):\n            exif_ifd[ExifIFD. InteroperabilityTag] = 1\n            interop_is = True\n            interop_ifd = exif_dict[\"Interop\"]\n        elif ExifIFD. InteroperabilityTag in exif_ifd:\n            exif_ifd.pop(ExifIFD.InteroperabilityTag)\n    elif ImageIFD.ExifTag in zeroth_ifd:\n        zeroth_ifd.pop(ImageIFD.ExifTag)\n\n    if (\"GPS\" in exif_dict) and len(exif_dict[\"GPS\"]):\n        zeroth_ifd[ImageIFD.GPSTag] = 1\n        gps_is = True\n        gps_ifd = exif_dict[\"GPS\"]\n    elif ImageIFD.GPSTag in zeroth_ifd:\n        zeroth_ifd.pop(ImageIFD.GPSTag)\n\n    if ((\"1st\" in exif_dict) and\n            (\"thumbnail\" in exif_dict) and\n            (exif_dict[\"thumbnail\"] is not None)):\n        first_is = True\n        exif_dict[\"1st\"][ImageIFD.JPEGInterchangeFormat] = 1\n        exif_dict[\"1st\"][ImageIFD.JPEGInterchangeFormatLength] = 1\n        first_ifd = exif_dict[\"1st\"]\n\n    zeroth_set = _dict_to_bytes(zeroth_ifd, \"0th\", 0)\n    zeroth_length = (len(zeroth_set[0]) + exif_is * 12 + gps_is * 12 + 4 +\n                     len(zeroth_set[1]))\n\n    if exif_is:\n        exif_set = _dict_to_bytes(exif_ifd, \"Exif\", zeroth_length)\n        exif_length = len(exif_set[0]) + interop_is * 12 + len(exif_set[1])\n    else:\n        exif_bytes = b\"\"\n        exif_length = 0\n    if gps_is:\n        gps_set = _dict_to_bytes(gps_ifd, \"GPS\", zeroth_length + exif_length)\n        gps_bytes = b\"\".join(gps_set)\n        gps_length = len(gps_bytes)\n    else:\n        gps_bytes = b\"\"\n        gps_length = 0\n    if interop_is:\n        offset = zeroth_length + exif_length + gps_length\n        interop_set = _dict_to_bytes(interop_ifd, \"Interop\", offset)\n        interop_bytes = b\"\".join(interop_set)\n        interop_length = len(interop_bytes)\n    else:\n        interop_bytes = b\"\"\n        interop_length = 0\n    if first_is:\n        offset = zeroth_length + exif_length + gps_length + interop_length\n        first_set = _dict_to_bytes(first_ifd, \"1st\", offset)\n        thumbnail = _get_thumbnail(exif_dict[\"thumbnail\"])\n        thumbnail_max_size = 64000\n        if len(thumbnail) > thumbnail_max_size:\n            raise ValueError(\"Given thumbnail is too large. max 64kB\")\n    else:\n        first_bytes = b\"\"\n    if exif_is:\n        pointer_value = TIFF_HEADER_LENGTH + zeroth_length\n        pointer_str = struct.pack(\">I\", pointer_value)\n        key = ImageIFD.ExifTag\n        key_str = struct.pack(\">H\", key)\n        type_str = struct.pack(\">H\", TYPES.Long)\n        length_str = struct.pack(\">I\", 1)\n        exif_pointer = key_str + type_str + length_str + pointer_str\n    else:\n        exif_pointer = b\"\"\n    if gps_is:\n        pointer_value = TIFF_HEADER_LENGTH + zeroth_length + exif_length\n        pointer_str = struct.pack(\">I\", pointer_value)\n        key = ImageIFD.GPSTag\n        key_str = struct.pack(\">H\", key)\n        type_str = struct.pack(\">H\", TYPES.Long)\n        length_str = struct.pack(\">I\", 1)\n        gps_pointer = key_str + type_str + length_str + pointer_str\n    else:\n        gps_pointer = b\"\"\n    if interop_is:\n        pointer_value = (TIFF_HEADER_LENGTH +\n                         zeroth_length + exif_length + gps_length)\n        pointer_str = struct.pack(\">I\", pointer_value)\n        key = ExifIFD.InteroperabilityTag\n        key_str = struct.pack(\">H\", key)\n        type_str = struct.pack(\">H\", TYPES.Long)\n        length_str = struct.pack(\">I\", 1)\n        interop_pointer = key_str + type_str + length_str + pointer_str\n    else:\n        interop_pointer = b\"\"\n    if first_is:\n        pointer_value = (TIFF_HEADER_LENGTH + zeroth_length +\n                         exif_length + gps_length + interop_length)\n        first_ifd_pointer = struct.pack(\">L\", pointer_value)\n        thumbnail_pointer = (pointer_value + len(first_set[0]) + 24 +\n                             4 + len(first_set[1]))\n        thumbnail_p_bytes = (b\"\\x02\\x01\\x00\\x04\\x00\\x00\\x00\\x01\" +\n                             struct.pack(\">L\", thumbnail_pointer))\n        thumbnail_length_bytes = (b\"\\x02\\x02\\x00\\x04\\x00\\x00\\x00\\x01\" +\n                                  struct.pack(\">L\", len(thumbnail)))\n        first_bytes = (first_set[0] + thumbnail_p_bytes +\n                       thumbnail_length_bytes + b\"\\x00\\x00\\x00\\x00\" +\n                       first_set[1] + thumbnail)\n    else:\n        first_ifd_pointer = b\"\\x00\\x00\\x00\\x00\"\n\n    zeroth_bytes = (zeroth_set[0] + exif_pointer + gps_pointer +\n                    first_ifd_pointer + zeroth_set[1])\n    if exif_is:\n        exif_bytes = exif_set[0] + interop_pointer + exif_set[1]\n\n    return (header + zeroth_bytes + exif_bytes + gps_bytes +\n            interop_bytes + first_bytes)", "response": "Dump exif data into a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert(exif, image, new_file=None):\n    if exif[0:6] != b\"\\x45\\x78\\x69\\x66\\x00\\x00\":\n        raise ValueError(\"Given data is not exif data\")\n\n    output_file = False\n    # Prevents \"UnicodeWarning: Unicode equal comparison failed\" warnings on Python 2\n    maybe_image = sys.version_info >= (3,0,0) or isinstance(image, str)\n\n    if maybe_image and image[0:2] == b\"\\xff\\xd8\":\n        image_data = image\n        file_type = \"jpeg\"\n    elif maybe_image and image[0:4] == b\"RIFF\" and image[8:12] == b\"WEBP\":\n        image_data = image\n        file_type = \"webp\"\n    else:\n        with open(image, 'rb') as f:\n            image_data = f.read()\n        if image_data[0:2] == b\"\\xff\\xd8\":\n            file_type = \"jpeg\"\n        elif image_data[0:4] == b\"RIFF\" and image_data[8:12] == b\"WEBP\":\n            file_type = \"webp\"\n        else:\n            raise InvalidImageDataError\n        output_file = True\n\n    if file_type == \"jpeg\":\n        exif = b\"\\xff\\xe1\" + struct.pack(\">H\", len(exif) + 2) + exif\n        segments = split_into_segments(image_data)\n        new_data = merge_segments(segments, exif)\n    elif file_type == \"webp\":\n        exif = exif[6:]\n        new_data = _webp.insert(image_data, exif)\n\n    if isinstance(new_file, io.BytesIO):\n        new_file.write(new_data)\n        new_file.seek(0)\n    elif new_file:\n        with open(new_file, \"wb+\") as f:\n            f.write(new_data)\n    elif output_file:\n        with open(image, \"wb+\") as f:\n            f.write(new_data)\n    else:\n        raise ValueError(\"Give a 3rd argument to 'insert' to output file\")", "response": "Insert image into the internal list of images."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransplanting a single image from filename1 to filename2.", "response": "def transplant(exif_src, image, new_file=None):\n    \"\"\"\n    py:function:: piexif.transplant(filename1, filename2)\n\n    Transplant exif from filename1 to filename2.\n\n    :param str filename1: JPEG\n    :param str filename2: JPEG\n    \"\"\"\n    if exif_src[0:2] == b\"\\xff\\xd8\":\n        src_data = exif_src\n    else:\n        with open(exif_src, 'rb') as f:\n            src_data = f.read()\n    segments = split_into_segments(src_data)\n    exif = get_exif_seg(segments)\n    if exif is None:\n        raise ValueError(\"not found exif in input\")\n\n    output_file = False\n    if image[0:2] == b\"\\xff\\xd8\":\n        image_data = image\n    else:\n        with open(image, 'rb') as f:\n            image_data = f.read()\n        output_file = True\n    segments = split_into_segments(image_data)\n    new_data = merge_segments(segments, exif)\n\n    if isinstance(new_file, io.BytesIO):\n        new_file.write(new_data)\n        new_file.seek(0)\n    elif new_file:\n        with open(new_file, \"wb+\") as f:\n            f.write(new_data)\n    elif output_file:\n        with open(image, \"wb+\") as f:\n            f.write(new_data)\n    else:\n        raise ValueError(\"Give a 3rd argument to 'transplant' to output file\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_into_segments(data):\n    if data[0:2] != b\"\\xff\\xd8\":\n        raise InvalidImageDataError(\"Given data isn't JPEG.\")\n\n    head = 2\n    segments = [b\"\\xff\\xd8\"]\n    while 1:\n        if data[head: head + 2] == b\"\\xff\\xda\":\n            segments.append(data[head:])\n            break\n        else:\n            length = struct.unpack(\">H\", data[head + 2: head + 4])[0]\n            endPoint = head + length + 2\n            seg = data[head: endPoint]\n            segments.append(seg)\n            head = endPoint\n\n        if (head >= len(data)):\n            raise InvalidImageDataError(\"Wrong JPEG data.\")\n    return segments", "response": "Slices JPEG binary data into a list of segments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the EXIF from a JPEG file.", "response": "def read_exif_from_file(filename):\n    \"\"\"Slices JPEG meta data into a list from JPEG binary data.\n    \"\"\"\n    f = open(filename, \"rb\")\n    data = f.read(6)\n\n    if data[0:2] != b\"\\xff\\xd8\":\n        raise InvalidImageDataError(\"Given data isn't JPEG.\")\n\n    head = data[2:6]\n    HEAD_LENGTH = 4\n    exif = None\n    while 1:\n        length = struct.unpack(\">H\", head[2: 4])[0]\n\n        if head[:2] == b\"\\xff\\xe1\":\n            segment_data = f.read(length - 2)\n            if segment_data[:4] != b'Exif':\n                head = f.read(HEAD_LENGTH)\n                continue\n            exif = head + segment_data\n            break\n        elif head[0:1] == b\"\\xff\":\n            f.read(length - 2)\n            head = f.read(HEAD_LENGTH)\n        else:\n            break\n\n    f.close()\n    return exif"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_segments(segments, exif=b\"\"):\n    if segments[1][0:2] == b\"\\xff\\xe0\" and \\\n       segments[2][0:2] == b\"\\xff\\xe1\" and \\\n       segments[2][4:10] == b\"Exif\\x00\\x00\":\n        if exif:\n            segments[2] = exif\n            segments.pop(1)\n        elif exif is None:\n            segments.pop(2)\n        else:\n            segments.pop(1)\n    elif segments[1][0:2] == b\"\\xff\\xe0\":\n        if exif:\n            segments[1] = exif\n    elif segments[1][0:2] == b\"\\xff\\xe1\" and \\\n         segments[1][4:10] == b\"Exif\\x00\\x00\":\n        if exif:\n            segments[1] = exif\n        elif exif is None:\n            segments.pop(1)\n    else:\n        if exif:\n            segments.insert(1, exif)\n    return b\"\".join(segments)", "response": "Merges the given list of segments into a single segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(src, new_file=None):\n    output_is_file = False\n    if src[0:2] == b\"\\xff\\xd8\":\n        src_data = src\n        file_type = \"jpeg\"\n    elif src[0:4] == b\"RIFF\" and src[8:12] == b\"WEBP\":\n        src_data = src\n        file_type = \"webp\"\n    else:\n        with open(src, 'rb') as f:\n            src_data = f.read()\n        output_is_file = True\n        if src_data[0:2] == b\"\\xff\\xd8\":\n            file_type = \"jpeg\"\n        elif src_data[0:4] == b\"RIFF\" and src_data[8:12] == b\"WEBP\":\n            file_type = \"webp\"\n\n    if file_type == \"jpeg\":\n        segments = split_into_segments(src_data)\n        exif = get_exif_seg(segments)\n        if exif:\n            new_data = src_data.replace(exif, b\"\")\n        else:\n            new_data = src_data\n    elif file_type == \"webp\":\n        try:\n            new_data = _webp.remove(src_data)\n        except ValueError:\n            new_data = src_data\n        except e:\n            print(e.args)\n            raise ValueError(\"Error occurred.\")\n\n    if isinstance(new_file, io.BytesIO):\n        new_file.write(new_data)\n        new_file.seek(0)\n    elif new_file:\n        with open(new_file, \"wb+\") as f:\n            f.write(new_data)\n    elif output_is_file:\n        with open(src, \"wb+\") as f:\n            f.write(new_data)\n    else:\n        raise ValueError(\"Give a second argument to 'remove' to output file\")", "response": "Remove exif from JPEG or WEBP."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(input_data, key_is_name=False):\n    exif_dict = {\"0th\":{},\n                 \"Exif\":{},\n                 \"GPS\":{},\n                 \"Interop\":{},\n                 \"1st\":{},\n                 \"thumbnail\":None}\n    exifReader = _ExifReader(input_data)\n    if exifReader.tiftag is None:\n        return exif_dict\n\n    if exifReader.tiftag[0:2] == LITTLE_ENDIAN:\n        exifReader.endian_mark = \"<\"\n    else:\n        exifReader.endian_mark = \">\"\n\n    pointer = struct.unpack(exifReader.endian_mark + \"L\",\n                            exifReader.tiftag[4:8])[0]\n    exif_dict[\"0th\"] = exifReader.get_ifd_dict(pointer, \"0th\")\n    first_ifd_pointer = exif_dict[\"0th\"].pop(\"first_ifd_pointer\")\n    if ImageIFD.ExifTag in exif_dict[\"0th\"]:\n        pointer = exif_dict[\"0th\"][ImageIFD.ExifTag]\n        exif_dict[\"Exif\"] = exifReader.get_ifd_dict(pointer, \"Exif\")\n    if ImageIFD.GPSTag in exif_dict[\"0th\"]:\n        pointer = exif_dict[\"0th\"][ImageIFD.GPSTag]\n        exif_dict[\"GPS\"] = exifReader.get_ifd_dict(pointer, \"GPS\")\n    if ExifIFD.InteroperabilityTag in exif_dict[\"Exif\"]:\n        pointer = exif_dict[\"Exif\"][ExifIFD.InteroperabilityTag]\n        exif_dict[\"Interop\"] = exifReader.get_ifd_dict(pointer, \"Interop\")\n    if first_ifd_pointer != b\"\\x00\\x00\\x00\\x00\":\n        pointer = struct.unpack(exifReader.endian_mark + \"L\",\n                                first_ifd_pointer)[0]\n        exif_dict[\"1st\"] = exifReader.get_ifd_dict(pointer, \"1st\")\n        if (ImageIFD.JPEGInterchangeFormat in exif_dict[\"1st\"] and\n            ImageIFD.JPEGInterchangeFormatLength in exif_dict[\"1st\"]):\n            end = (exif_dict[\"1st\"][ImageIFD.JPEGInterchangeFormat] +\n                   exif_dict[\"1st\"][ImageIFD.JPEGInterchangeFormatLength])\n            thumb = exifReader.tiftag[exif_dict[\"1st\"][ImageIFD.JPEGInterchangeFormat]:end]\n            exif_dict[\"thumbnail\"] = thumb\n\n    if key_is_name:\n        exif_dict = _get_key_name_dict(exif_dict)\n    return exif_dict", "response": "Load the exif data from file into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(cls, data):\n        if len(data) < cls._PREFIX_SIZE:\n            raise ValueError('not enough data to decode UserComment')\n        prefix = data[:cls._PREFIX_SIZE]\n        body = data[cls._PREFIX_SIZE:]\n        if prefix == cls._UNDEFINED_PREFIX:\n            raise ValueError('prefix is UNDEFINED, unable to decode UserComment')\n        try:\n            encoding = {\n                cls._ASCII_PREFIX: cls.ASCII, cls._JIS_PREFIX: cls._JIS, cls._UNICODE_PREFIX: cls._UNICODE,\n            }[prefix]\n        except KeyError:\n            raise ValueError('unable to determine appropriate encoding')\n        return body.decode(encoding, errors='replace')", "response": "Convert the UserComment value in exif format to str."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting str to appropriate format for UserComment.", "response": "def dump(cls, data, encoding=\"ascii\"):\n        \"\"\"\n        Convert str to appropriate format for \"UserComment\".\n\n        :param data: Like u\"foobar\"\n        :param str encoding: \"ascii\", \"jis\", or \"unicode\"\n        :return: b\"ASCII\\x00\\x00\\x00foobar\"\n        :rtype: bytes\n        :raises: ValueError if the encoding is unsupported.\n        \"\"\"\n        if encoding not in cls.ENCODINGS:\n            raise ValueError('encoding {!r} must be one of {!r}'.format(encoding, cls.ENCODINGS))\n        prefix = {cls.ASCII: cls._ASCII_PREFIX, cls.JIS: cls._JIS_PREFIX, cls.UNICODE: cls._UNICODE_PREFIX}[encoding]\n        internal_encoding = {cls.UNICODE: cls._UNICODE, cls.JIS: cls._JIS}.get(encoding, encoding)\n        return prefix + data.encode(internal_encoding, errors='replace')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fs(path):\n    scheme = ''\n    if '://' in path:\n        scheme = path.partition('://')[0]\n\n    for schemes, fs_class in FILE_EXTENSIONS:\n        if scheme in schemes:\n            return fs_class\n\n    return FileSystem", "response": "Find the file system implementation for this path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwait for the context to stop.", "response": "def awaitTermination(self, timeout=None):\n        \"\"\"Wait for context to stop.\n\n        :param float timeout: in seconds\n        \"\"\"\n\n        if timeout is not None:\n            IOLoop.current().call_later(timeout, self.stop)\n\n        IOLoop.current().start()\n        IOLoop.clear_current()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef binaryRecordsStream(self, directory, recordLength=None,\n                            process_all=False):\n        \"\"\"Monitor a directory and process all binary files.\n\n        File names starting with ``.`` are ignored.\n\n        :param string directory: a path\n        :param recordLength: None, int or struct format string\n        :param bool process_all: whether to process pre-existing files\n        :rtype: DStream\n\n        .. warning::\n            Only ``int`` ``recordLength`` are supported in PySpark API.\n            The ``process_all`` parameter does not exist in the PySpark API.\n        \"\"\"\n        deserializer = FileBinaryStreamDeserializer(self._context,\n                                                    recordLength)\n        file_stream = FileStream(directory, process_all)\n        self._on_stop_cb.append(file_stream.stop)\n        return DStream(file_stream, self, deserializer)", "response": "Monitor a directory and process all binary files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef queueStream(self, rdds, oneAtATime=True, default=None):\n        deserializer = QueueStreamDeserializer(self._context)\n        if default is not None:\n            default = deserializer(default)\n\n        if Queue is False:\n            log.error('Run \"pip install tornado\" to install tornado.')\n\n        q = Queue()\n        for i in rdds:\n            q.put(i)\n\n        qstream = QueueStream(q, oneAtATime, default)\n        return DStream(qstream, self, deserializer)", "response": "Create a stream over a list of RDDs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef socketBinaryStream(self, hostname, port, length):\n        deserializer = TCPDeserializer(self._context)\n        tcp_binary_stream = TCPBinaryStream(length)\n        tcp_binary_stream.listen(port, hostname)\n        self._on_stop_cb.append(tcp_binary_stream.stop)\n        return DStream(tcp_binary_stream, self, deserializer)", "response": "Create a TCP socket server for binary input."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a TCP socket server.", "response": "def socketTextStream(self, hostname, port):\n        \"\"\"Create a TCP socket server.\n\n        :param string hostname: Hostname of TCP server.\n        :param int port: Port of TCP server.\n        :rtype: DStream\n        \"\"\"\n        deserializer = TCPDeserializer(self._context)\n        tcp_text_stream = TCPTextStream()\n        tcp_text_stream.listen(port, hostname)\n        self._on_stop_cb.append(tcp_text_stream.stop)\n        return DStream(tcp_text_stream, self, deserializer)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstop processing streams. :param stopSparkContext: stop the SparkContext (NOT IMPLEMENTED) :param stopGracefully: stop gracefully (NOT IMPLEMENTED)", "response": "def stop(self, stopSparkContext=True, stopGraceFully=False):\n        \"\"\"Stop processing streams.\n\n        :param stopSparkContext: stop the SparkContext (NOT IMPLEMENTED)\n        :param stopGracefully: stop gracefully (NOT IMPLEMENTED)\n        \"\"\"\n        while self._on_stop_cb:\n            cb = self._on_stop_cb.pop()\n            log.debug('calling on_stop_cb {}'.format(cb))\n            cb()\n\n        IOLoop.current().stop()\n\n        StreamingContext._activeContext = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef textFileStream(self, directory, process_all=False):\n        deserializer = FileTextStreamDeserializer(self._context)\n        file_stream = FileStream(directory, process_all)\n        self._on_stop_cb.append(file_stream.stop)\n        return DStream(file_stream, self, deserializer)", "response": "Monitor a directory and process all text files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_codec(path):\n    if '.' not in path or path.rfind('/') > path.rfind('.'):\n        return Codec\n\n    for endings, codec_class in FILE_ENDINGS:\n        if any(path.endswith(e) for e in endings):\n            log.debug('Using {0} codec: {1}'.format(endings, path))\n            return codec_class\n\n    return NoCodec", "response": "Find the codec implementation for this path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(self, encoding='utf8', encoding_errors='ignore'):  # pylint: disable=arguments-differ\n        if isinstance(self.codec, codec.NoCodec) and \\\n           self.fs.load_text != FileSystem.load_text:\n            stream = self.fs.load_text(encoding, encoding_errors)\n        else:\n            stream = self.fs.load()\n            stream = self.codec.decompress(stream)\n            stream = TextIOWrapper(stream, encoding, encoding_errors)\n        return stream", "response": "Load the data from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(self, stream=None, encoding='utf8', encoding_errors='ignore'):  # pylint: disable=arguments-differ\n        if stream is None:\n            stream = StringIO()\n\n        if isinstance(stream, string_types):\n            stream = StringIO(stream)\n\n        stream = self.codec.compress(\n            BytesIO(stream.read().encode(encoding, encoding_errors))\n        )\n        self.fs.dump(stream)\n\n        return self", "response": "Writes a stream to a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count(self):\n        return (\n            self\n            .mapPartitions(lambda p: [sum(1 for _ in p)])\n            .reduce(operator.add)\n        )", "response": "Count the number of elements in each RDD."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef countByValue(self):\n        return self.transform(\n            lambda rdd: self._context._context.parallelize(\n                rdd.countByValue().items()))", "response": "Return a new DStream containing the count of every entry in the source RDD."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies function f and flatten the DStream.", "response": "def flatMap(self, f, preservesPartitioning=False):\n        \"\"\"Apply function f and flatten.\n\n        :param f: mapping function\n        :rtype: DStream\n        \"\"\"\n        return self.mapPartitions(\n            lambda p: (e for pp in p for e in f(pp)),\n            preservesPartitioning,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef map(self, f, preservesPartitioning=False):\n        return (\n            self\n            .mapPartitions(lambda p: (f(e) for e in p), preservesPartitioning)\n            .transform(lambda rdd:\n                       rdd.setName('{}:{}'.format(rdd.prev.name(), f)))\n        )", "response": "Apply function f to each element of the sequence returning a new DStream\nAttributeNames."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mapPartitions(self, f, preservesPartitioning=False):\n        return (\n            self\n            .mapPartitionsWithIndex(lambda i, p: f(p), preservesPartitioning)\n            .transform(lambda rdd:\n                       rdd.setName('{}:{}'.format(rdd.prev.name(), f)))\n        )", "response": "Map partitions.\n\n        :param f: mapping function\n        :rtype: DStream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint the first num elements of each RDD.", "response": "def pprint(self, num=10):\n        \"\"\"Print the first ``num`` elements of each RDD.\n\n        :param int num: Set number of elements to be printed.\n        \"\"\"\n\n        def pprint_map(time_, rdd):\n            print('>>> Time: {}'.format(time_))\n            data = rdd.take(num + 1)\n            for d in data[:num]:\n                py_pprint.pprint(d)\n            if len(data) > num:\n                print('...')\n            print('')\n\n        self.foreachRDD(pprint_map)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new DStream where each RDD was reduced with func.", "response": "def reduce(self, func):\n        \"\"\"Return a new DStream where each RDD was reduced with ``func``.\n\n        :rtype: DStream\n        \"\"\"\n\n        # avoid RDD.reduce() which does not return an RDD\n        return self.transform(\n            lambda rdd: (\n                rdd\n                .map(lambda i: (None, i))\n                .reduceByKey(func)\n                .map(lambda none_i: none_i[1])\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reduceByKey(self, func, numPartitions=None):\n        return self.transform(lambda rdd: rdd.reduceByKey(func))", "response": "Apply reduceByKey to every RDD."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef repartition(self, numPartitions):\n        return self.transform(\n            lambda rdd: (rdd.repartition(numPartitions)\n                         if not isinstance(rdd, EmptyRDD) else rdd)\n        )", "response": "Repartition every RDD.\n\n        :rtype: DStream\n\n\n        Example:\n\n        >>> import pysparkling\n        >>> sc = pysparkling.Context()\n        >>> ssc = pysparkling.streaming.StreamingContext(sc, 0.1)\n        >>> (\n        ...     ssc\n        ...     .queueStream([['hello', 'world']])\n        ...     .repartition(2)\n        ...     .foreachRDD(lambda rdd: print(len(rdd.partitions())))\n        ... )\n        >>> ssc.start()\n        >>> ssc.awaitTermination(0.25)\n        2\n        0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave every RDD in this RDD as text files.", "response": "def saveAsTextFiles(self, prefix, suffix=None):\n        \"\"\"Save every RDD as a text file (or sets of text files).\n\n        :param string prefix: path prefix of the output\n        :param string suffix: file suffix (e.g. '.gz' to enable compression)\n\n\n        Example:\n\n        >>> from backports import tempfile\n        >>> import os, pysparkling\n        >>> sc = pysparkling.Context()\n        >>> ssc = pysparkling.streaming.StreamingContext(sc, 0.1)\n        >>> with tempfile.TemporaryDirectory() as tmp_dir:\n        ...     (\n        ...         ssc.queueStream([['hello', 'world'], [1, 2]])\n        ...         .saveAsTextFiles(os.path.join(tmp_dir, 'textout'))\n        ...     )\n        ...     ssc.start()\n        ...     ssc.awaitTermination(0.25)\n        ...     result = sc.textFile(tmp_dir + '*').collect()\n        >>> result\n        ['hello', 'world', '1', '2']\n        \"\"\"\n        self.foreachRDD(\n            lambda time_, rdd:\n            rdd.saveAsTextFile('{}-{:.0f}{}'.format(\n                prefix, time_ * 1000, suffix if suffix is not None else ''))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new DStream containing only the RDDs from begin to end.", "response": "def slice(self, begin, end):\n        \"\"\"Filter RDDs to between begin and end.\n\n        :param datetime.datetime|int begin: datetiem or unix timestamp\n        :param datetime.datetime|int end: datetiem or unix timestamp\n        :rtype: DStream\n        \"\"\"\n        return self.transform(lambda time_, rdd:\n                              rdd if begin <= time_ <= end\n                              else EmptyRDD(self._context._context))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transform(self, func):\n        if func.__code__.co_argcount == 1:\n            one_arg_func = func\n            func = lambda _, rdd: one_arg_func(rdd)\n\n        return TransformedDStream(self, func)", "response": "Return a new DStream where each RDD is transformed by f."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new DStream where each RDD is transformed by func.", "response": "def transformWith(self, func, other, keepSerializer=False):\n        \"\"\"Return a new DStream where each RDD is transformed by ``f``.\n\n        :param f: transformation function\n        :rtype: DStream\n\n        The transformation function can have arguments ``(time, rdd_a, rdd_b)``\n        or ``(rdd_a, rdd_b)``.\n        \"\"\"\n        if func.__code__.co_argcount == 2:\n            two_arg_func = func\n            func = lambda _, rdd_a, rdd_b: two_arg_func(rdd_a, rdd_b)\n\n        return TransformedWithDStream(self, func, other)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_filenames(all_expr):\n        files = []\n        for expr in all_expr.split(','):\n            expr = expr.strip()\n            files += fs.get_fs(expr).resolve_filenames(expr)\n        log.debug('Filenames: {0}'.format(files))\n        return files", "response": "resolve expression for a filename\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self):\n        stream = self.fs.load()\n        stream = self.codec.decompress(stream)\n        return stream", "response": "Load the data from a file and return it as a byte stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a stream to a file.", "response": "def dump(self, stream=None):\n        \"\"\"Writes a stream to a file.\n\n        :param stream:\n            A BytesIO instance. ``bytes`` are also possible and are converted\n            to BytesIO.\n\n        :rtype: File\n        \"\"\"\n        if stream is None:\n            stream = BytesIO()\n\n        if isinstance(stream, bytes):\n            stream = BytesIO(stream)\n\n        stream = self.codec.compress(stream)\n        self.fs.dump(stream)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fake2db_logger():\n    '''creates a logger obj'''\n    # Pull the local ip and username for meaningful logging\n    username = getpass.getuser()\n    \n    # Set the logger\n    FORMAT = '%(asctime)-15s %(user)-8s %(message)s'\n    logging.basicConfig(format=FORMAT)\n    extra_information = {'user': username}\n    logger = logging.getLogger('fake2db_logger')\n    # --------------------\n    return logger, extra_information", "response": "creates a logger object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a mongodb database returning the related connection object which will be later used to spawn the cursor", "response": "def database_caller_creator(self, host, port, name=None):\n        '''creates a mongodb database\n        returns the related connection object\n        which will be later used to spawn the cursor\n        '''\n\n        client = pymongo.MongoClient(host, port)\n\n        if name:\n            db = client[name]\n        else:\n            db = client['mongodb_' + str_generator(self)]\n\n        return db"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fake2db_redis_initiator(self, host, port, number_of_rows, name=None, custom=None):\n        '''Main handler for the operation\n        '''\n\n        client, pipe = self.database_caller_creator(host, port, name)\n\n        if custom:\n            self.custom_db_creator(number_of_rows, pipe, custom)\n            client.save()\n            sys.exit(0)\n\n        self.data_filler_simple_registration(number_of_rows, pipe)\n        self.data_filler_detailed_registration(number_of_rows, pipe)\n        self.data_filler_company(number_of_rows, pipe)\n        self.data_filler_user_agent(number_of_rows, pipe)\n        self.data_filler_customer(number_of_rows, pipe)\n\n        client.save()", "response": "This method is called by the fake2db_redis_initiator method. It creates a redis client and then calls the data filling methods."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a redis connection object which will be later used to modify the db", "response": "def database_caller_creator(self, host, port, name=None):\n        '''creates a redis connection object\n        which will be later used to modify the db\n        '''\n\n        name = name or 0\n        client = redis.StrictRedis(host=host, port=port, db=name)\n        pipe = client.pipeline(transaction=False)\n        return client, pipe"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data_filler_simple_registration(self, number_of_rows, pipe):\n        '''creates keys with simple regis. information\n        '''\n\n        try:\n            for i in range(number_of_rows):\n                pipe.hmset('simple_registration:%s' % i, {\n                    'id': rnd_id_generator(self),\n                    'email': self.faker.safe_email(),\n                    'password': self.faker.md5(raw_output=False)\n                })\n            pipe.execute()\n            logger.warning('simple_registration Commits are successful after write job!', extra=d)\n\n        except Exception as e:\n            logger.error(e, extra=d)", "response": "creates keys with simple regis. information\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_filler_detailed_registration(self, number_of_rows, pipe):\n        '''creates keys with detailed regis. information\n        '''\n\n        try:\n            for i in range(number_of_rows):\n                pipe.hmset('detailed_registration:%s' % i, {\n                    'id': rnd_id_generator(self),\n                    'email': self.faker.safe_email(),\n                    'password': self.faker.md5(raw_output=False),\n                    'lastname': self.faker.last_name(),\n                    'name': self.faker.first_name(),\n                    'address': self.faker.address(),\n                    'phone': self.faker.phone_number()\n                })\n            pipe.execute()\n            logger.warning('detailed_registration Commits are successful after write job!', extra=d)\n\n        except Exception as e:\n            logger.error(e, extra=d)", "response": "creates keys with detailed regis. information\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data_filler_user_agent(self, number_of_rows, pipe):\n        '''creates keys with user agent data\n        '''\n\n        try:\n            for i in range(number_of_rows):\n                pipe.hmset('user_agent:%s' % i, {\n                    'id': rnd_id_generator(self),\n                    'ip': self.faker.ipv4(),\n                    'countrycode': self.faker.country_code(),\n                    'useragent': self.faker.user_agent()\n                })\n            pipe.execute()\n            logger.warning('user_agent Commits are successful after write job!', extra=d)\n\n        except Exception as e:\n            logger.error(e, extra=d)", "response": "creates keys with user agent data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate keys with company data", "response": "def data_filler_company(self, number_of_rows, pipe):\n        '''creates keys with company data\n        '''\n\n        try:\n            for i in range(number_of_rows):\n                pipe.hmset('company:%s' % i, {\n                    'id': rnd_id_generator(self),\n                    'name': self.faker.company(),\n                    'date': self.faker.date(pattern=\"%d-%m-%Y\"),\n                    'email': self.faker.company_email(),\n                    'domain': self.faker.safe_email(),\n                    'city': self.faker.city()\n                })\n            pipe.execute()\n            logger.warning('companies Commits are successful after write job!', extra=d)\n\n        except Exception as e:\n            logger.error(e, extra=d)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate keys with customer data", "response": "def data_filler_customer(self, number_of_rows, pipe):\n        '''creates keys with customer data\n        '''\n\n        try:\n            for i in range(number_of_rows):\n                pipe.hmset('customer:%s' % i, {\n                    'id': rnd_id_generator(self),\n                    'name': self.faker.first_name(),\n                    'lastname': self.faker.last_name(),\n                    'address': self.faker.address(),\n                    'country': self.faker.country(),\n                    'city': self.faker.city(),\n                    'registry_date': self.faker.date(pattern=\"%d-%m-%Y\"),\n                    'birthdate': self.faker.date(pattern=\"%d-%m-%Y\"),\n                    'email': self.faker.safe_email(),\n                    'phone_number': self.faker.phone_number(),\n                    'locale': self.faker.locale()\n                })\n            pipe.execute()\n            logger.warning('customer Commits are successful after write job!', extra=d)\n\n        except Exception as e:\n            logger.error(e, extra=d)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fake2db_initiator(self, number_of_rows, **connection_kwargs):\n        '''Main handler for the operation\n        '''\n        rows = number_of_rows\n        cursor, conn = self.database_caller_creator(number_of_rows, **connection_kwargs)\n\n        self.data_filler_simple_registration(rows, cursor, conn)\n        self.data_filler_detailed_registration(rows, cursor, conn)\n        self.data_filler_company(rows, cursor, conn)\n        self.data_filler_user_agent(rows, cursor, conn)\n        self.data_filler_customer(rows, cursor, conn)\n        cursor.close()\n        conn.close()", "response": "Main handler for the operation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef custom_db_creator(self, number_of_rows, cursor, conn, custom):\n        '''creates and fills the table with simple regis. information\n        '''\n\n        custom_d = faker_options_container()\n        sqlst = \"CREATE TABLE custom (id serial PRIMARY KEY,\"\n        custom_payload = \"INSERT INTO custom (\"\n        \n        # form the sql query that will set the db up\n        for c in custom:\n            if custom_d.get(c):\n                sqlst += \" \" + c + \" \" + custom_d[c] + \",\"\n                custom_payload += \" \" + c + \",\"\n                logger.warning(\"fake2db found valid custom key provided: %s\" % c, extra=d)\n            else:\n                logger.error(\"fake2db does not support the custom key you provided.\", extra=d )\n                sys.exit(1)\n                \n        # the indice thing is for trimming the last extra comma\n        sqlst = sqlst[:-1]\n        sqlst += \");\"\n        custom_payload = custom_payload[:-1]\n        custom_payload += \") VALUES (\"\n\n        for i in range(0, len(custom)):\n            custom_payload += \"%s, \"\n        custom_payload = custom_payload[:-2] + \")\"\n        cursor.execute(sqlst)\n        conn.commit()\n\n        multi_lines = []\n        try:\n            for i in range(0, number_of_rows):\n                multi_lines.append([])\n                for c in custom:\n                    multi_lines[i].append(getattr(self.faker, c)())\n            \n            cursor.executemany(custom_payload, multi_lines)\n            conn.commit()\n            logger.warning('custom Commits are successful after write job!', extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)", "response": "creates and fills the table with simple regis. information\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fake2db_mysql_initiator(self, host, port, password, username, number_of_rows, name=None, custom=None):\n        '''Main handler for the operation\n        '''\n        rows = number_of_rows\n\n        if name:\n            cursor, conn = self.database_caller_creator(host, port, password, username, name)\n        else:\n            cursor, conn = self.database_caller_creator(host, port, password, username)\n\n        if custom:\n            self.custom_db_creator(rows, cursor, conn, custom)\n            cursor.close()\n            conn.close()\n            sys.exit(0)\n\n        tables = self.mysql_table_creator()\n        keys = tables.keys()\n\n        for key in keys:\n            try:\n                cursor.execute(tables[key])\n                conn.commit()\n            except mysql.connector.Error as err:\n                logger.error(err.msg, extra=extra_information)\n            else:\n                logger.info(\"OK\", extra=extra_information)\n\n        logger.warning('Table creation ops finished', extra=extra_information)\n\n        self.data_filler_simple_registration(rows, cursor, conn)\n        self.data_filler_detailed_registration(rows, cursor, conn)\n        self.data_filler_company(rows, cursor, conn)\n        self.data_filler_user_agent(rows, cursor, conn)\n        self.data_filler_customer(rows, cursor, conn)\n        cursor.close()\n        conn.close()", "response": "This method is called by the mysql2db_mysql_initiator function. It creates a fake database connection and creates the table names and company names."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef custom_db_creator(self, number_of_rows, cursor, conn, custom):\n        '''creates and fills the table with simple regis. information\n        '''\n\n        custom_d = faker_options_container()\n        sqlst = \"CREATE TABLE `custom` (`id` varchar(200) NOT NULL,\"\n        custom_payload = \"INSERT INTO custom (id,\"\n        \n        # form the sql query that will set the db up\n        for c in custom:\n            if custom_d.get(c):\n                sqlst += \" `\" + c + \"` \" + custom_d[c] + \",\"\n                custom_payload += \" \" + c + \",\"\n                logger.warning(\"fake2db found valid custom key provided: %s\" % c, extra=extra_information)\n            else:\n                logger.error(\"fake2db does not support the custom key you provided.\", extra=extra_information)\n                sys.exit(1)\n                \n        # the indice thing is for trimming the last extra comma\n        sqlst += \" PRIMARY KEY (`id`)) ENGINE=InnoDB\"\n        custom_payload = custom_payload[:-1]\n        custom_payload += \") VALUES (%s, \"\n        \n        for i in range(0, len(custom)):\n            custom_payload += \"%s, \"\n        custom_payload = custom_payload[:-2] + \")\"\n        \n        try:\n            cursor.execute(sqlst)\n            conn.commit()\n        except mysql.connector.Error as err:\n            logger.error(err.msg, extra=extra_information)\n\n        multi_lines = []\n        try:\n            for i in range(0, number_of_rows):\n                multi_lines.append([rnd_id_generator(self)])\n                for c in custom:\n                    multi_lines[i].append(getattr(self.faker, c)())\n            \n            cursor.executemany(custom_payload, multi_lines)\n            conn.commit()\n            logger.warning('custom Commits are successful after write job!', extra=extra_information)\n        except Exception as e:\n            logger.error(e, extra=extra_information)", "response": "creates and fills the table with simple regis. information\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data_filler_company(self, number_of_rows, cursor, conn):\n        '''creates and fills the table with company data\n        '''\n        companies_data = []\n        \n        try:\n            for i in range(0, number_of_rows):\n                \n                companies_data.append((rnd_id_generator(self), self.faker.company(), self.faker.date(pattern=\"%Y-%m-%d\"),\n                                  self.faker.company_email(), self.faker.safe_email(), self.faker.city()))\n                \n            companies_payload = (\"INSERT INTO company \"\n                                 \"(id, name, sdate, email, domain, city) \"\n                                 \"VALUES (%s, %s, %s, %s, %s, %s)\")\n            \n            cursor.executemany(companies_payload, companies_data)\n            conn.commit()\n            logger.warning('companies Commits are successful after write job!', extra=extra_information)\n        except Exception as e:\n            logger.error(e, extra=extra_information)", "response": "creates and fills the table with company data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef data_filler_customer(self, number_of_rows, cursor, conn):\n        '''creates and fills the table with customer\n        '''\n        customer_data = []\n        \n        try:\n            for i in range(0, number_of_rows):\n                \n                customer_data.append((\n                    rnd_id_generator(self), self.faker.first_name(), self.faker.last_name(), self.faker.address(),\n                    self.faker.country(), self.faker.city(), self.faker.date(pattern=\"%d-%m-%Y\"),\n                    self.faker.date(pattern=\"%d-%m-%Y\"), self.faker.safe_email(), self.faker.phone_number(),\n                    self.faker.locale()))\n                \n            customer_payload = (\"INSERT INTO customer \"\n                                \"(id, name, lastname, address, country, city, registry_date, birthdate, email, \"\n                                \"phone_number, locale)\"\n                                \"VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\")\n            cursor.executemany(customer_payload, customer_data)\n            conn.commit()\n            logger.warning('detailed_registration Commits are successful after write job!', extra=extra_information)\n        except Exception as e:\n            logger.error(e, extra=extra_information)", "response": "creates and fills the table with customer data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if mysql server is available in sys fires up one", "response": "def _mysqld_process_checkpoint():\n    '''this helper method checks if\n    mysql server is available in the sys\n    if not fires up one\n    '''\n    try:\n        subprocess.check_output(\"pgrep mysqld\", shell=True)\n    except Exception:\n        logger.warning(\n            'Your mysql server is offline, fake2db will try to launch it now!',\n            extra=extra_information)\n        # close_fds = True argument is the flag that is responsible\n        # for Popen to launch the process completely independent\n        subprocess.Popen(\"mysqld\", close_fds=True, shell=True)\n        time.sleep(3)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _redis_process_checkpoint(host, port):\n    '''this helper method checks if\n    redis server is available in the sys\n    if not fires up one\n    '''\n    try:\n        subprocess.check_output(\"pgrep redis\", shell=True)\n    except Exception:\n        logger.warning(\n            'Your redis server is offline, fake2db will try to launch it now!',\n            extra=extra_information)\n        # close_fds = True argument is the flag that is responsible\n        # for Popen to launch the process completely independent\n        subprocess.Popen(\"redis-server --bind %s --port %s\" % (host, port),\n                         close_fds=True,\n                         shell=True)\n        time.sleep(3)", "response": "this helper method checks if redis server is available in the sys\n    and launches the process completely"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fake2db_sqlite_initiator(self, number_of_rows, name=None, custom=None):\n        '''Main handler for the operation\n        '''\n        rows = number_of_rows\n        conn = self.database_caller_creator(name)\n\n        if custom:\n            self.custom_db_creator(rows, conn, custom)\n            conn.close()\n            sys.exit(0)\n            \n        self.data_filler_simple_registration(rows, conn)\n        self.data_filler_detailed_registration(rows, conn)\n        self.data_filler_company(rows, conn)\n        self.data_filler_user_agent(rows, conn)\n        self.data_filler_customer(rows, conn)\n        conn.close()", "response": "Main handler for the operation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef database_caller_creator(self, name=None):\n        '''creates a sqlite3 db\n        returns the related connection object\n        which will be later used to spawn the cursor\n        '''\n\n        try:\n            if name:\n                database = name + '.db'\n            else:\n                database = 'sqlite_' + str_generator(self) + '.db'\n\n            conn = sqlite3.connect(database)\n            logger.warning('Database created and opened succesfully: %s' % database, extra=d)\n        except Exception:\n            logger.error('Failed to connect or create database / sqlite3', extra=d)\n            raise DbConnException\n\n        return conn", "response": "creates a sqlite3 db\n            returns the related connection object which will be later used to spawn the cursor object\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_filler_simple_registration(self, number_of_rows, conn):\n        '''creates and fills the table with simple regis. information\n        '''\n        cursor = conn.cursor()\n\n        cursor.execute('''\n        CREATE TABLE simple_registration(id TEXT PRIMARY KEY,\n        email TEXT , password TEXT)\n        ''')\n        conn.commit()\n        multi_lines = []\n        \n        try:\n            for i in range(0, number_of_rows):\n                multi_lines.append((rnd_id_generator(self), self.faker.safe_email(), self.faker.md5(raw_output=False)))\n            cursor.executemany('insert into simple_registration values(?,?,?)',multi_lines)\n            conn.commit()\n            logger.warning('simple_registration Commits are successful after write job!', extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)", "response": "creates and fills the table with simple regis. information\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and fills the table with user agent data", "response": "def data_filler_user_agent(self, number_of_rows, conn):\n        '''creates and fills the table with user agent data\n        '''\n        cursor = conn.cursor()\n\n        cursor.execute('''\n        CREATE TABLE user_agent(id TEXT PRIMARY KEY,\n        ip TEXT, countrycode TEXT, useragent TEXT)\n        ''')\n        conn.commit()\n        multi_lines = []\n        \n        try:\n            for i in range(0, number_of_rows):\n                multi_lines.append((rnd_id_generator(self), self.faker.ipv4(), self.faker.country_code(),\n                                    self.faker.user_agent()))\n                \n            cursor.executemany('insert into user_agent values(?,?,?,?)', multi_lines)\n            conn.commit()\n            logger.warning('user_agent Commits are successful after write job!', extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and fills the table with company data", "response": "def data_filler_company(self, number_of_rows, conn):\n        '''creates and fills the table with company data\n        '''\n        cursor = conn.cursor()\n\n        cursor.execute('''\n        CREATE TABLE company(id TEXT PRIMARY KEY,\n        name TEXT, sdate TEXT, email TEXT, domain TEXT, city TEXT)\n        ''')\n        conn.commit()\n        multi_lines = []\n        \n        try:\n            for i in range(0, number_of_rows):\n                \n                multi_lines.append((rnd_id_generator(self), self.faker.company(),\n                                    self.faker.date(pattern=\"%d-%m-%Y\"),\n                                    self.faker.company_email(), self.faker.safe_email(),\n                                    self.faker.city()))\n                \n            cursor.executemany('insert into company values (?,?,?,?,?,?)', multi_lines)\n            conn.commit()\n            logger.warning('companies Commits are successful after write job!', extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fake2db_couchdb_initiator(self, number_of_rows, name=None, custom=None):\n        '''Main handler for the operation\n        '''\n        rows = number_of_rows\n        db = self.database_caller_creator(name)\n\n        if custom:\n            self.custom_db_creator(rows, db, custom)\n            sys.exit(0)\n            \n        self.data_filler_simple_registration(rows, db)\n        self.data_filler_detailed_registration(rows, db)\n        self.data_filler_company(rows, db)\n        self.data_filler_user_agent(rows, db)\n        self.data_filler_customer(rows, db)", "response": "Main handler for the operation\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a couchdb database returning the related connection object which will be later used to spawn the cursor", "response": "def database_caller_creator(self, name=None):\n        '''creates a couchdb database\n        returns the related connection object\n        which will be later used to spawn the cursor\n        '''\n\n        couch = couchdb.Server()\n\n        if name:\n            db = couch.create(name)\n        else:\n            n = 'couchdb_' + lower_str_generator(self)\n            db = couch.create(n)\n            logger.warning('couchdb database created with the name: %s', n,\n                           extra=d)\n\n        return db"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate and fills the table with simple regis. information", "response": "def data_filler_simple_registration(self, number_of_rows, db):\n        '''creates and fills the table with simple regis. information\n        '''\n\n        try:\n            simple_registration = db\n\n            for i in range(0, number_of_rows):\n                post_simple_reg = {\n                    \"id\": rnd_id_generator(self),\n                    \"email\": self.faker.safe_email(),\n                    \"password\": self.faker.md5(raw_output=False)\n                }\n                simple_registration.save(post_simple_reg)\n\n            logger.warning(\n                'simple_registration Commits are successful after write job!',\n                extra=d)\n\n        except Exception as e:\n            logger.error(e, extra=d)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and fills the table with detailed regis. information", "response": "def data_filler_detailed_registration(self, number_of_rows, db):\n        '''creates and fills the table with detailed regis. information\n        '''\n\n        try:\n            detailed_registration = db\n            data_list = list()\n            for i in range(0, number_of_rows):\n                post_det_reg = {\n                    \"id\": rnd_id_generator(self),\n                    \"email\": self.faker.safe_email(),\n                    \"password\": self.faker.md5(raw_output=False),\n                    \"lastname\": self.faker.last_name(),\n                    \"name\": self.faker.first_name(),\n                    \"adress\": self.faker.address(),\n                    \"phone\": self.faker.phone_number()\n                }\n                detailed_registration.save(post_det_reg)\n\n            logger.warning(\n                'detailed_registration Commits are successful after write job!',\n                extra=d)\n\n        except Exception as e:\n            logger.error(e, extra=d)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and fills the table with user agent data", "response": "def data_filler_user_agent(self, number_of_rows, db):\n        '''creates and fills the table with user agent data\n        '''\n\n        try:\n            user_agent = db\n            data_list = list()\n            for i in range(0, number_of_rows):\n                post_uo_reg = {\n                    \"id\": rnd_id_generator(self),\n                    \"ip\": self.faker.ipv4(),\n                    \"countrycode\": self.faker.country_code(),\n                    \"useragent\": self.faker.user_agent()\n                }\n                user_agent.save(post_uo_reg)\n\n            logger.warning(\n                'user_agent Commits are successful after write job!',\n                extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data_filler_company(self, number_of_rows, db):\n        '''creates and fills the table with company data\n        '''\n\n        try:\n            company = db\n            data_list = list()\n            for i in range(0, number_of_rows):\n                post_comp_reg = {\n                    \"id\": rnd_id_generator(self),\n                    \"name\": self.faker.company(),\n                    \"date\": self.faker.date(pattern=\"%d-%m-%Y\"),\n                    \"email\": self.faker.company_email(),\n                    \"domain\": self.faker.safe_email(),\n                    \"city\": self.faker.city()\n                }\n                company.save(post_comp_reg)\n\n            logger.warning('companies Commits are successful after write job!',\n                           extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)", "response": "creates and fills the table with company data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data_filler_customer(self, number_of_rows, db):\n        '''creates and fills the table with customer data\n        '''\n\n        try:\n            customer = db\n            data_list = list()\n            for i in range(0, number_of_rows):\n                post_cus_reg = {\n                    \"id\": rnd_id_generator(self),\n                    \"name\": self.faker.first_name(),\n                    \"lastname\": self.faker.last_name(),\n                    \"address\": self.faker.address(),\n                    \"country\": self.faker.country(),\n                    \"city\": self.faker.city(),\n                    \"registry_date\": self.faker.date(pattern=\"%d-%m-%Y\"),\n                    \"birthdate\": self.faker.date(pattern=\"%d-%m-%Y\"),\n                    \"email\": self.faker.safe_email(),\n                    \"phone_number\": self.faker.phone_number(),\n                    \"locale\": self.faker.locale()\n                }\n                customer.save(post_cus_reg)\n\n            logger.warning('customer Commits are successful after write job!',\n                           extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)", "response": "creates and fills the table with customer data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit(self, node):\n        method = getattr(self, 'visit_' + node.expr_name, self.generic_visit)\n\n        # Call that method, and show where in the tree it failed if it blows\n        # up.\n        try:\n            return method(node, [self.visit(n) for n in node])\n        except (VisitationError, UndefinedLabel):\n            # Don't catch and re-wrap already-wrapped exceptions.\n            raise\n        except self.unwrapped_exceptions:\n            raise\n        except Exception:\n            # Catch any exception, and tack on a parse tree so it's easier to\n            # see where it went wrong.\n            exc_class, exc, tb = exc_info()\n            reraise(VisitationError, VisitationError(exc, exc_class, node), tb)", "response": "Walk a parse tree transforming it into another representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a parse or match on the default grammar.", "response": "def _parse_or_match(self, text, pos, method_name):\n        \"\"\"Execute a parse or match on the default grammar, followed by a\n        visitation.\n\n        Raise RuntimeError if there is no default grammar specified.\n\n        \"\"\"\n        if not self.grammar:\n            raise RuntimeError(\n                \"The {cls}.{method}() shortcut won't work because {cls} was \"\n                \"never associated with a specific \" \"grammar. Fill out its \"\n                \"`grammar` attribute, and try again.\".format(\n                    cls=self.__class__.__name__,\n                    method=method_name))\n        return self.visit(getattr(self.grammar, method_name)(text, pos=pos))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the rules for parsing the grammar definition syntax.", "response": "def _expressions_from_rules(self, rule_syntax, custom_rules):\n        \"\"\"Return the rules for parsing the grammar definition syntax.\n\n        Return a 2-tuple: a dict of rule names pointing to their expressions,\n        and then the top-level expression for the first rule.\n\n        \"\"\"\n        # Hard-code enough of the rules to parse the grammar that describes the\n        # grammar description language, to bootstrap:\n        comment = Regex(r'#[^\\r\\n]*', name='comment')\n        meaninglessness = OneOf(Regex(r'\\s+'), comment, name='meaninglessness')\n        _ = ZeroOrMore(meaninglessness, name='_')\n        equals = Sequence(Literal('='), _, name='equals')\n        label = Sequence(Regex(r'[a-zA-Z_][a-zA-Z_0-9]*'), _, name='label')\n        reference = Sequence(label, Not(equals), name='reference')\n        quantifier = Sequence(Regex(r'[*+?]'), _, name='quantifier')\n        # This pattern supports empty literals. TODO: A problem?\n        spaceless_literal = Regex(r'u?r?\"[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"',\n                                  ignore_case=True,\n                                  dot_all=True,\n                                  name='spaceless_literal')\n        literal = Sequence(spaceless_literal, _, name='literal')\n        regex = Sequence(Literal('~'),\n                         literal,\n                         Regex('[ilmsuxa]*', ignore_case=True),\n                         _,\n                         name='regex')\n        atom = OneOf(reference, literal, regex, name='atom')\n        quantified = Sequence(atom, quantifier, name='quantified')\n\n        term = OneOf(quantified, atom, name='term')\n        not_term = Sequence(Literal('!'), term, _, name='not_term')\n        term.members = (not_term,) + term.members\n\n        sequence = Sequence(term, OneOrMore(term), name='sequence')\n        or_term = Sequence(Literal('/'), _, term, name='or_term')\n        ored = Sequence(term, OneOrMore(or_term), name='ored')\n        expression = OneOf(ored, sequence, term, name='expression')\n        rule = Sequence(label, equals, expression, name='rule')\n        rules = Sequence(_, OneOrMore(rule), name='rules')\n\n        # Use those hard-coded rules to parse the (more extensive) rule syntax.\n        # (For example, unless I start using parentheses in the rule language\n        # definition itself, I should never have to hard-code expressions for\n        # those above.)\n\n        rule_tree = rules.parse(rule_syntax)\n\n        # Turn the parse tree into a map of expressions:\n        return RuleVisitor().visit(rule_tree)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntreating a parenthesized subexpression as just its contents.", "response": "def visit_parenthesized(self, node, parenthesized):\n        \"\"\"Treat a parenthesized subexpression as just its contents.\n\n        Its position in the tree suffices to maintain its grouping semantics.\n\n        \"\"\"\n        left_paren, _, expression, right_paren, _ = parenthesized\n        return expression"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_rule(self, node, rule):\n        label, equals, expression = rule\n        expression.name = label  # Assign a name to the expr.\n        return expression", "response": "Assign a name to the Expression and return it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_regex(self, node, regex):\n        tilde, literal, flags, _ = regex\n        flags = flags.text.upper()\n        pattern = literal.literal  # Pull the string back out of the Literal\n                                   # object.\n        return Regex(pattern, ignore_case='I' in flags,\n                              locale='L' in flags,\n                              multiline='M' in flags,\n                              dot_all='S' in flags,\n                              unicode='U' in flags,\n                              verbose='X' in flags,\n                              ascii='A' in flags)", "response": "Visit a regex node and return a Regex expression."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _resolve_refs(self, rule_map, expr, done):\n        if isinstance(expr, LazyReference):\n            label = text_type(expr)\n            try:\n                reffed_expr = rule_map[label]\n            except KeyError:\n                raise UndefinedLabel(expr)\n            return self._resolve_refs(rule_map, reffed_expr, done)\n        else:\n            if getattr(expr, 'members', ()) and expr not in done:\n                # Prevents infinite recursion for circular refs. At worst, one\n                # of `expr.members` can refer back to `expr`, but it can't go\n                # any farther.\n                done.add(expr)\n                expr.members = tuple(self._resolve_refs(rule_map, member, done)\n                                     for member in expr.members)\n            return expr", "response": "Resolve any lazy references in the expression expr recursively and all its subexpressions resolved."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncollating all the rules into a map.", "response": "def visit_rules(self, node, rules_list):\n        \"\"\"Collate all the rules into a map. Return (map, default rule).\n\n        The default rule is the first one. Or, if you have more than one rule\n        of that name, it's the last-occurring rule of that name. (This lets you\n        override the default rule when you extend a grammar.) If there are no\n        string-based rules, the default rule is None, because the custom rules,\n        due to being kwarg-based, are unordered.\n\n        \"\"\"\n        _, rules = rules_list\n\n        # Map each rule's name to its Expression. Later rules of the same name\n        # override earlier ones. This lets us define rules multiple times and\n        # have the last declaration win, so you can extend grammars by\n        # concatenation.\n        rule_map = OrderedDict((expr.name, expr) for expr in rules)\n\n        # And custom rules override string-based rules. This is the least\n        # surprising choice when you compare the dict constructor:\n        # dict({'x': 5}, x=6).\n        rule_map.update(self.custom_rules)\n\n        # Resolve references. This tolerates forward references.\n        done = set()\n        rule_map = OrderedDict((expr.name, self._resolve_refs(rule_map, expr, done))\n                               for expr in itervalues(rule_map))\n\n        # isinstance() is a temporary hack around the fact that * rules don't\n        # always get transformed into lists by NodeVisitor. We should fix that;\n        # it's surprising and requires writing lame branches like this.\n        return rule_map, (rule_map[rules[0].name]\n                          if isinstance(rules, list) and rules else None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn a plain callable into an Expression.", "response": "def expression(callable, rule_name, grammar):\n    \"\"\"Turn a plain callable into an Expression.\n\n    The callable can be of this simple form::\n\n        def foo(text, pos):\n            '''If this custom expression matches starting at text[pos], return\n            the index where it stops matching. Otherwise, return None.'''\n            if the expression matched:\n                return end_pos\n\n    If there child nodes to return, return a tuple::\n\n        return end_pos, children\n\n    If the expression doesn't match at the given ``pos`` at all... ::\n\n        return None\n\n    If your callable needs to make sub-calls to other rules in the grammar or\n    do error reporting, it can take this form, gaining additional arguments::\n\n        def foo(text, pos, cache, error, grammar):\n            # Call out to other rules:\n            node = grammar['another_rule'].match_core(text, pos, cache, error)\n            ...\n            # Return values as above.\n\n    The return value of the callable, if an int or a tuple, will be\n    automatically transmuted into a :class:`~parsimonious.Node`. If it returns\n    a Node-like class directly, it will be passed through unchanged.\n\n    :arg rule_name: The rule name to attach to the resulting\n        :class:`~parsimonious.Expression`\n    :arg grammar: The :class:`~parsimonious.Grammar` this expression will be a\n        part of, to make delegating to other rules possible\n\n    \"\"\"\n    num_args = len(getargspec(callable).args)\n    if num_args == 2:\n        is_simple = True\n    elif num_args == 5:\n        is_simple = False\n    else:\n        raise RuntimeError(\"Custom rule functions must take either 2 or 5 \"\n                           \"arguments, not %s.\" % num_args)\n\n    class AdHocExpression(Expression):\n        def _uncached_match(self, text, pos, cache, error):\n            result = (callable(text, pos) if is_simple else\n                      callable(text, pos, cache, error, grammar))\n\n            if isinstance(result, integer_types):\n                end, children = result, None\n            elif isinstance(result, tuple):\n                end, children = result\n            else:\n                # Node or None\n                return result\n            return Node(self, text, pos, end, children=children)\n\n        def _as_rhs(self):\n            return '{custom function \"%s\"}' % callable.__name__\n\n    return AdHocExpression(name=rule_name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _uncached_match(self, text, pos, cache, error):\n        m = self.re.match(text, pos)\n        if m is not None:\n            span = m.span()\n            node = RegexNode(self, text, pos, pos + span[1] - span[0])\n            node.match = m  # TODO: A terrible idea for cache size?\n            return node", "response": "Return length of match or None if no match."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _regex_flags_from_bits(self, bits):\n        flags = 'ilmsuxa'\n        return ''.join(flags[i - 1] if (1 << i) & bits else '' for i in range(1, len(flags) + 1))", "response": "Return the textual equivalent of numerically encoded regex flags."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gegetate_args(self, options):\n        for optkey, optval in self._normalize_options(options):\n            yield optkey\n\n            if isinstance(optval, (list, tuple)):\n                assert len(optval) == 2 and optval[0] and optval[\n                    1], 'Option value can only be either a string or a (tuple, list) of 2 items'\n                yield optval[0]\n                yield optval[1]\n            else:\n                yield optval", "response": "Generator of args parts based on options specification."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _command(self, path=None):\n        options = self._gegetate_args(self.options)\n        options = [x for x in options]\n        # print 'options', options\n        if self.css:\n            self._prepend_css(self.css)\n\n        if '--xvfb' in options:\n            options.remove('--xvfb')\n            yield self.xvfb\n            # auto servernum option to prevent failure on concurrent runs\n            # https://bugs.launchpad.net/ubuntu/+source/xorg-server/+bug/348052\n            yield '-a'\n\n        yield self.wkhtmltoimage\n\n        for argpart in options:\n            if argpart:\n                yield argpart\n\n        if self.cover and self.cover_first:\n            yield 'cover'\n            yield self.cover\n\n        if self.toc:\n            yield 'toc'\n            for argpart in self._gegetate_args(self.toc):\n                if argpart:\n                    yield argpart\n\n        if self.cover and not self.cover_first:\n            yield 'cover'\n            yield self.cover\n\n        # If the source is a string then we will pipe it into wkhtmltoimage\n        # If the source is file-like then we will read from it and pipe it in\n        if self.source.isString() or self.source.isFileObj():\n            yield '-'\n        else:\n            if isinstance(self.source.source, basestring):\n                yield self.source.to_s()\n            else:\n                for s in self.source.source:\n                    yield s\n\n        # If output_path evaluates to False append '-' to end of args\n        # and wkhtmltoimage will pass generated IMG to stdout\n        if path:\n            yield path\n        else:\n            yield '-'", "response": "Generator of all command parts\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _normalize_options(self, options):\n        for key, value in list(options.items()):\n            if '--' in key:\n                normalized_key = self._normalize_arg(key)\n            else:\n                normalized_key = '--%s' % self._normalize_arg(key)\n\n            if isinstance(value, (list, tuple)):\n                for opt_val in value:\n                    yield (normalized_key, opt_val)\n            else:\n                yield (normalized_key, str(value) if value else value)", "response": "Normalizes options and returns iterator of 2 - tuples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_options_in_meta(self, content):\n        if (isinstance(content, io.IOBase)\n            or content.__class__.__name__ == 'StreamReaderWriter'):\n            content = content.read()\n\n        found = {}\n\n        for x in re.findall('<meta [^>]*>', content):\n            if re.search('name=[\"\\']%s' % self.config.meta_tag_prefix, x):\n                name = re.findall('name=[\"\\']%s([^\"\\']*)' %\n                                  self.config.meta_tag_prefix, x)[0]\n                found[name] = re.findall('content=[\"\\']([^\"\\']*)', x)[0]\n\n        return found", "response": "Reads content and extracts options encoded in HTML meta tags\n\n        returns dict of config option and value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_url(url,\n             output_path,\n             options=None,\n             toc=None,\n             cover=None,\n             config=None,\n             cover_first=None):\n    \"\"\"\n    Convert URL/URLs to IMG file/files\n\n    :param url: URL or list of URLs to be saved\n    :param output_path: path to output PDF file/files. False means file will be returned as string\n    :param options: (optional) dict with wkhtmltopdf global and page options, with or w/o '--'\n    :param toc: (optional) dict with toc-specific wkhtmltopdf options, with or w/o '--'\n    :param cover: (optional) string with url/filename with a cover html page\n    :param css: style of input\n    :param config: (optional) instance of imgkit.config.Config()\n    :param cover_first: (optional) if True, cover always precedes TOC\n    :return: True when success\n    \"\"\"\n    rtn = IMGKit(url,\n                 'url',\n                 options=options,\n                 toc=toc, cover=cover,\n                 config=config,\n                 cover_first=cover_first)\n    return rtn.to_img(output_path)", "response": "Convert URL or list of URLs to IMG file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an HTML file to IMG file.", "response": "def from_file(filename,\n              output_path,\n              options=None,\n              toc=None,\n              cover=None,\n              css=None,\n              config=None,\n              cover_first=None):\n    \"\"\"\n    Convert HTML file/files to IMG file/files\n\n    :param filename: path of HTML file or list with paths or file-like object\n    :param output_path: path to output PDF file/files. False means file will be returned as string\n    :param options: (optional) dict with wkhtmltopdf global and page options, with or w/o '--'\n    :param toc: (optional) dict with toc-specific wkhtmltopdf options, with or w/o '--'\n    :param cover: (optional) string with url/filename with a cover html page\n    :param css: style of input\n    :param config: (optional) instance of imgkit.config.Config()\n    :param cover_first: (optional) if True, cover always precedes TOC\n    :return: True when success\n    \"\"\"\n    rtn = IMGKit(filename,\n                 'file',\n                 options=options,\n                 toc=toc,\n                 cover=cover,\n                 css=css,\n                 config=config,\n                 cover_first=cover_first)\n    return rtn.to_img(output_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert given string to IMG file", "response": "def from_string(string,\n                output_path,\n                options=None,\n                toc=None,\n                cover=None,\n                css=None,\n                config=None,\n                cover_first=None):\n    \"\"\"\n    Convert given string/strings to IMG file\n\n    :param string:\n    :param output_path: path to output PDF file/files. False means file will be returned as string\n    :param options: (optional) dict with wkhtmltopdf global and page options, with or w/o '--'\n    :param toc: (optional) dict with toc-specific wkhtmltopdf options, with or w/o '--'\n    :param cover: (optional) string with url/filename with a cover html page\n    :param css: style of input\n    :param config: (optional) instance of imgkit.config.Config()\n    :param cover_first: (optional) if True, cover always precedes TOC\n    :return: True when success\n    \"\"\"\n    rtn = IMGKit(string, 'string', options=options, toc=toc, cover=cover, css=css,\n                 config=config, cover_first=cover_first)\n    return rtn.to_img(output_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nauthenticates the user with the given token.", "response": "def authenticate_credentials(self, token):\n        '''\n        Due to the random nature of hashing a salted value, this must inspect\n        each auth_token individually to find the correct one.\n\n        Tokens that have expired will be deleted and skipped\n        '''\n        msg = _('Invalid token.')\n        token = token.decode(\"utf-8\")\n        for auth_token in AuthToken.objects.filter(\n                token_key=token[:CONSTANTS.TOKEN_KEY_LENGTH]):\n            if self._cleanup_token(auth_token):\n                continue\n\n            try:\n                digest = hash_token(token, auth_token.salt)\n            except (TypeError, binascii.Error):\n                raise exceptions.AuthenticationFailed(msg)\n            if compare_digest(digest, auth_token.digest):\n                if knox_settings.AUTO_REFRESH and auth_token.expiry:\n                    self.renew_token(auth_token)\n                return self.validate_user(auth_token)\n        raise exceptions.AuthenticationFailed(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hash_token(token, salt):\n    '''\n    Calculates the hash of a token and salt.\n    input is unhexlified\n\n    token and salt must contain an even number of hex digits or\n    a binascii.Error exception will be raised\n    '''\n    digest = hashes.Hash(sha(), backend=default_backend())\n    digest.update(binascii.unhexlify(token))\n    digest.update(binascii.unhexlify(salt))\n    return binascii.hexlify(digest.finalize()).decode()", "response": "Calculates the hash of a token and salt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extend(cls, services):\n        service_names = []\n        for service in services:\n            service_names.append(_service_name(service))\n        services = list(services)\n        services.append(cls(reflection_pb2, service_names))\n        services.append(ServerReflectionV1Alpha(reflection_pb2_v1alpha,\n                                                service_names))\n        return services", "response": "Extends services list with reflection service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimplement synchronous periodic checks.", "response": "async def Check(self, stream):\n        \"\"\"Implements synchronous periodic checks\"\"\"\n        request = await stream.recv_message()\n        checks = self._checks.get(request.service)\n        if checks is None:\n            await stream.send_trailing_metadata(status=Status.NOT_FOUND)\n        elif len(checks) == 0:\n            await stream.send_message(HealthCheckResponse(\n                status=HealthCheckResponse.SERVING,\n            ))\n        else:\n            for check in checks:\n                await check.__check__()\n            await stream.send_message(HealthCheckResponse(\n                status=_status(checks),\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, value: Optional[bool]):\n        prev_value = self._value\n        self._value = value\n        if self._value != prev_value:\n            # notify all watchers that this check was changed\n            for event in self._events:\n                event.set()", "response": "Sets the current status of a check\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def recv_message(self):\n        message = await recv_message(self._stream, self._codec, self._recv_type)\n        message, = await self._dispatch.recv_message(message)\n        return message", "response": "Coroutine to receive a message from the client."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def start(self, host=None, port=None, *, path=None,\n                    family=socket.AF_UNSPEC, flags=socket.AI_PASSIVE,\n                    sock=None, backlog=100, ssl=None, reuse_address=None,\n                    reuse_port=None):\n        \"\"\"Coroutine to start the server.\n\n        :param host: can be a string, containing IPv4/v6 address or domain name.\n            If host is None, server will be bound to all available interfaces.\n\n        :param port: port number.\n\n        :param path: UNIX domain socket path. If specified, host and port should\n            be omitted (must be None).\n\n        :param family: can be set to either :py:data:`python:socket.AF_INET` or\n            :py:data:`python:socket.AF_INET6` to force the socket to use IPv4 or\n            IPv6. If not set it will be determined from host.\n\n        :param flags: is a bitmask for\n            :py:meth:`~python:asyncio.AbstractEventLoop.getaddrinfo`.\n\n        :param sock: sock can optionally be specified in order to use a\n            preexisting socket object. If specified, host and port should be\n            omitted (must be None).\n\n        :param backlog: is the maximum number of queued connections passed to\n            listen().\n\n        :param ssl: can be set to an :py:class:`~python:ssl.SSLContext`\n            to enable SSL over the accepted connections.\n\n        :param reuse_address: tells the kernel to reuse a local socket in\n            TIME_WAIT state, without waiting for its natural timeout to expire.\n\n        :param reuse_port: tells the kernel to allow this endpoint to be bound\n            to the same port as other existing endpoints are bound to,\n            so long as they all set this flag when being created.\n        \"\"\"\n        if path is not None and (host is not None or port is not None):\n            raise ValueError(\"The 'path' parameter can not be used with the \"\n                             \"'host' or 'port' parameters.\")\n\n        if self._server is not None:\n            raise RuntimeError('Server is already started')\n\n        if path is not None:\n            self._server = await self._loop.create_unix_server(\n                self._protocol_factory, path, sock=sock, backlog=backlog,\n                ssl=ssl\n            )\n\n        else:\n            self._server = await self._loop.create_server(\n                self._protocol_factory, host, port,\n                family=family, flags=flags, sock=sock, backlog=backlog, ssl=ssl,\n                reuse_address=reuse_address, reuse_port=reuse_port\n            )", "response": "Coroutine to start the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstop accepting new connections cancels all currently running requests and closes all currently running requests.", "response": "def close(self):\n        \"\"\"Stops accepting new connections, cancels all currently running\n        requests. Request handlers are able to handle `CancelledError` and\n        exit properly.\n        \"\"\"\n        if self._server is None:\n            raise RuntimeError('Server is not started')\n        self._server.close()\n        for handler in self._handlers:\n            handler.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def recv_message(self):\n        # TODO: check that messages were sent for non-stream-stream requests\n        if not self._recv_initial_metadata_done:\n            await self.recv_initial_metadata()\n\n        with self._wrapper:\n            message = await recv_message(self._stream, self._codec,\n                                         self._recv_type)\n            self._recv_message_count += 1\n            message, = await self._dispatch.recv_message(message)\n            return message", "response": "Coroutine to receive a message from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def recv_trailing_metadata(self):\n        if not self._end_done:\n            raise ProtocolError('Outgoing stream was not ended')\n\n        if (\n            not self._cardinality.server_streaming\n            and not self._recv_message_count\n        ):\n            raise ProtocolError('No messages were received before waiting '\n                                'for trailing metadata')\n\n        if self._recv_trailing_metadata_done:\n            raise ProtocolError('Trailing metadata was already received')\n\n        with self._wrapper:\n            headers = await self._stream.recv_headers()\n            self._recv_trailing_metadata_done = True\n\n            metadata = decode_metadata(headers)\n            metadata, = await self._dispatch.recv_trailing_metadata(metadata)\n            self.trailing_metadata = metadata\n\n            self._raise_for_grpc_status(dict(headers))", "response": "Coroutine to wait for trailing metadata from the context manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self):\n        if self._protocol is not None:\n            self._protocol.processor.close()\n            del self._protocol", "response": "Closes connection to the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates and returns a new Stream object for the current resource.", "response": "def open(self, *, timeout=None, metadata=None) -> Stream:\n        \"\"\"Creates and returns :py:class:`Stream` object to perform request\n        to the server.\n\n        Nothing will happen to the current underlying HTTP/2 connection during\n        this method call. It just initializes :py:class:`Stream` object for you.\n        Actual request will be sent only during :py:meth:`Stream.send_request`\n        or :py:meth:`Stream.send_message` coroutine call.\n\n        :param float timeout: request timeout (seconds)\n        :param metadata: custom request metadata, dict or list of pairs\n        :return: :py:class:`Stream` object\n        \"\"\"\n        return self.channel.request(self.name, self._cardinality,\n                                    self.request_type, self.reply_type,\n                                    timeout=timeout, metadata=metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_config():\n\n  with open(PYDOCMD_CONFIG) as fp:\n    config = yaml.load(fp)\n  return default_config(config)", "response": "Reads and preprocesses the pydoc - markdown configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_temp_mkdocs_config(inconf):\n  ignored_keys = ('gens_dir', 'pages', 'headers', 'generate', 'loader',\n                  'preprocessor', 'additional_search_paths')\n\n  config = {key: value for key, value in inconf.items() if key not in ignored_keys}\n  config['docs_dir'] = inconf['gens_dir']\n  if 'pages' in inconf:\n      config['nav'] = inconf['pages']\n\n  with open('mkdocs.yml', 'w') as fp:\n    yaml.dump(config, fp)\n\n  atexit.register(lambda: os.remove('mkdocs.yml'))", "response": "Writes a configuration for MkDocs on - the - fly from the pydoc - markdown\n configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying all files from the source directory into the generated files directory.", "response": "def copy_source_files(config, pages_required=True):\n  \"\"\"\n  Copies all files from the `docs_dir` to the `gens_dir` defined in the\n  *config*. It also takes the MkDocs `pages` configuration into account\n  and converts the special `<< INFILE` syntax by copying them to the\n  `gens_dir` as well.\n  \"\"\"\n\n  for path in config['additional_search_paths']:\n    path = os.path.abspath(path)\n    sys.path.append(path)\n\n  # Copy all template files from the source directory into our\n  # generated files directory.\n  log('Started copying source files...')\n  for root, dirs, files in os.walk(config['docs_dir']):\n    rel_root = os.path.relpath(root, config['docs_dir'])\n    for fname in files:\n      dest_fname = os.path.join(config['gens_dir'], rel_root, fname)\n      makedirs(os.path.dirname(dest_fname))\n      shutil.copyfile(os.path.join(root, fname), dest_fname)\n\n  if 'pages' not in config:\n    if pages_required:\n      raise RuntimeError('pydocmd.yml does not have defined pages!')\n\n    return\n\n  for page in config['pages']:\n    process_pages(page, config['gens_dir'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports a Python object by an absolute identifier.", "response": "def import_object_with_scope(name):\n  \"\"\"\n  Imports a Python object by an absolute identifier.\n\n  # Arguments\n  name (str): The name of the Python object to import.\n\n  # Returns\n  (any, Module): The object and the module that contains it. Note that\n    for plain modules loaded with this function, both elements of the\n    tuple may be the same object.\n  \"\"\"\n\n  # Import modules until we can no longer import them. Prefer existing\n  # attributes over importing modules at each step.\n  parts = name.split('.')\n  current_name = parts[0]\n  obj = import_module(current_name)\n  scope = None\n  for part in parts[1:]:\n    current_name += '.' + part\n    try:\n      if hasattr(obj, '__dict__'):\n        # Using directly __dict__ for descriptors, where we want to get the descriptor's instance\n        # and not calling the descriptor's __get__ method.\n        sub_obj = obj.__dict__[part]\n      else:\n        sub_obj = getattr(obj, part)\n\n      scope, obj = obj, sub_obj\n    except (AttributeError, KeyError):\n      try:\n        obj = scope = import_module(current_name)\n      except ImportError as exc:\n        if 'named {}'.format(part) in str(exc):\n          raise ImportError(current_name)\n        raise\n  return obj, scope"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nforce lazy imports of modules off of name.", "response": "def force_lazy_import(name):\n  \"\"\"\n  Import any modules off of \"name\" by iterating a new list rather than a generator so that this\n  library works with lazy imports.\n  \"\"\"\n  obj = import_object(name)\n  module_items = list(getattr(obj, '__dict__', {}).items())\n  for key, value in module_items:\n    if getattr(value, '__module__', None):\n        import_object(name + '.' + key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preprocess_section(self, section):\n    codeblock_opened = False\n    current_section = None\n    content = self._preprocess_refs(section)\n    lines = []\n    for line in content.split('\\n'):\n      if line.startswith(\"```\"):\n        codeblock_opened = (not codeblock_opened)\n      if not line:\n        current_section = None\n      elif not codeblock_opened:\n        line, current_section = self._preprocess_line(line, current_section)\n      lines.append(line)\n    section.content = '\\n'.join(lines)", "response": "Preprocess the contents of section."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse references and replaces them with markdown links", "response": "def _preprocess_refs(self, section):\n    \"\"\"\n    Parses references and replaces them with markdown links\n    Syntax is:\n        `#anchor` for in page reference\n        `#::mod[.submod][#item][+n]` where\n            `mod.submod` is the module, `item` is the module member and n is\n            the number of duplicates in case of headers: markdown conflicts.\n            You can also references class members: `#::mod.submod.cls#method`\n    \"\"\"\n    def handler(match):\n      mod = match.group('mod')\n      ref = match.group('ref')\n      parens = match.group('parens') or ''\n      dup = match.group('dup')\n      has_trailing_dot = False\n      if not parens and ref.endswith('.'):\n        ref = ref[:-1]\n        has_trailing_dot = True\n\n      if '#' in ref:\n        text = ref.split('#')[-1]\n        title = mod_ref = ref.replace('#', '.')\n        anchor = text.lower()\n      else:\n        title = text = anchor = ref\n        anchor = ref.replace('.', '')\n        mod_ref = ref\n\n      if self.config['headers'] == 'html':\n          anchor = mod_ref\n          if not mod and not anchor.startswith(section.identifier):\n              anchor = section.identifier + '.' + anchor\n      elif dup:\n          anchor += '_' + dup[1:]\n\n      result = '`{}`'.format(text + parens)\n      link = self.link_lookup.get(mod_ref, self.link_lookup.get(ref))\n      if mod and link:\n        result = ('[' + result + '](' + link + '#' + anchor + ' \"' + title + '\")')\n      else:\n        result = '[' + result + '](#' + anchor + ')'\n      if has_trailing_dot:\n        result += '.'\n      return (match.group('prefix') or '') + result\n    return re.sub('(?P<prefix>^| |\\t)#(?P<mod>::)?(?P<ref>[\\w\\d\\._\\#]+)(?P<parens>\\(\\))?(?P<dup>\\+\\d+)?', handler, section.content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render(self, stream):\n\n    if self.header_type == 'html':\n      print('<h{depth} id=\"{id}\">{title}</h{depth}>\\n'\n        .format(depth = self.depth, id = self.identifier, title = self.title),\n        file = stream)\n    elif self.header_type == 'markdown':\n      print('\\n' + ('#' * self.depth), self.title, file = stream)\n    else:\n      raise ValueError('Invalid header type: %s' % self.header_type)\n    print(self.content, file=stream)", "response": "Render the section into stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new_document(self, filename, url=None):\n\n    if not filename.endswith('.md'):\n      raise ValueError('filename must end with `.md`')\n    if os.path.isabs(filename):\n      raise ValueError('filename must be relative')\n    if filename in self.documents:\n      raise ValueError('document filename {!r} already used'.format(filename))\n    if not url:\n      url = filename[:-3]\n\n    doc = Document(self, url)\n    self.documents[filename] = doc\n    return doc", "response": "Create a new document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new section in the specified document.", "response": "def new_section(self, doc, *args, **kwargs):\n    \"\"\"\n    Create a new section in the specified document. The arguments for this\n    method match the parameters for the #Section constructor.\n\n    # Raises\n    ValueError: If the section identifier is already used.\n    \"\"\"\n\n    section = Section(doc, *args, **kwargs)\n    if section.identifier:\n      if section.identifier in self.sections:\n        raise ValueError('section identifier {!r} already used'\n          .format(section.identifier))\n      self.sections[section.identifier] = section\n    doc.sections.append(section)\n    return section"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the contents of a section.", "response": "def load_section(self, section):\n    \"\"\"\n    Loads the contents of a #Section. The `section.identifier` is the name\n    of the object that we need to load.\n\n    # Arguments\n      section (Section): The section to load. Fill the `section.title` and\n        `section.content` values. Optionally, `section.loader_context` can\n        be filled with custom arbitrary data to reference at a later point.\n    \"\"\"\n\n    assert section.identifier is not None\n    obj, scope = import_object_with_scope(section.identifier)\n\n    if '.' in section.identifier:\n      default_title = section.identifier.rsplit('.', 1)[1]\n    else:\n      default_title = section.identifier\n\n    section.title = getattr(obj, '__name__', default_title)\n    section.content = trim(get_docstring(obj))\n    section.loader_context = {'obj': obj, 'scope': scope}\n\n    # Add the function signature in a code-block.\n    if callable(obj):\n      sig = get_function_signature(obj, scope if inspect.isclass(scope) else None)\n      sig, _ = FormatCode(sig, style_config='pep8')\n      section.content = '```python\\n{}\\n```\\n'.format(sig.strip()) + section.content"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the files in the specified directories into the paths dictionary.", "response": "def load(self, patterns, dirs, ignore=None):\n        \"\"\"\n        Load objects from the filesystem into the ``paths`` dictionary.\n\n        \"\"\"\n        for _dir in dirs:\n            data = self.read_file(_dir, ignore=ignore)\n            if data:\n                self.paths[_dir] = data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading file into memory returning deserialized objects", "response": "def read_file(self, path, **kwargs):\n        \"\"\"Read file input into memory, returning deserialized objects\n\n        :param path: Path of file to read\n        :param \\**kwargs:\n            * ignore (``list``): List of file patterns to ignore\n        \"\"\"\n        # TODO support JSON here\n        # TODO sphinx way of reporting errors in logs?\n\n        parser_command = [\"godocjson\"]\n\n        _ignore = kwargs.get(\"ignore\")\n        if _ignore:\n            parser_command.extend([\"-e\", \"{0}\".format(\"|\".join(_ignore))])\n\n        parser_command.append(path)\n\n        try:\n            parsed_data = json.loads(subprocess.check_output(parser_command))\n            return parsed_data\n        except IOError:\n            LOGGER.warning(\"Error reading file: {0}\".format(path))\n        except TypeError:\n            LOGGER.warning(\"Error reading file: {0}\".format(path))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new object based on the data.", "response": "def create_class(self, data, options=None, **kwargs):\n        \"\"\"Return instance of class based on Go data\n\n        Data keys handled here:\n\n            _type\n                Set the object class\n\n            consts, types, vars, funcs\n                Recurse into :py:meth:`create_class` to create child object\n                instances\n\n        :param data: dictionary data from godocjson output\n        \"\"\"\n        _type = kwargs.get(\"_type\")\n        obj_map = dict((cls.type, cls) for cls in ALL_CLASSES)\n        try:\n            # Contextual type data from children recursion\n            if _type:\n                LOGGER.debug(\"Forcing Go Type %s\" % _type)\n                cls = obj_map[_type]\n            else:\n                cls = obj_map[data[\"type\"]]\n        except KeyError:\n            LOGGER.warning(\"Unknown Type: %s\" % data)\n        else:\n            if cls.inverted_names and \"names\" in data:\n                # Handle types that have reversed names parameter\n                for name in data[\"names\"]:\n                    data_inv = {}\n                    data_inv.update(data)\n                    data_inv[\"name\"] = name\n                    if \"names\" in data_inv:\n                        del data_inv[\"names\"]\n                    for obj in self.create_class(data_inv):\n                        yield obj\n            else:\n                # Recurse for children\n                obj = cls(data, jinja_env=self.jinja_env)\n                for child_type in [\"consts\", \"types\", \"vars\", \"funcs\"]:\n                    for child_data in data.get(child_type, []):\n                        obj.children += list(\n                            self.create_class(\n                                child_data,\n                                _type=child_type.replace(\"consts\", \"const\")\n                                .replace(\"types\", \"type\")\n                                .replace(\"vars\", \"variable\")\n                                .replace(\"funcs\", \"func\"),\n                            )\n                        )\n                yield obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pathname(self):\n        slug = self.name\n        slug = unidecode.unidecode(slug)\n        slug = slug.replace(\"-\", \"\")\n        slug = re.sub(r\"[^\\w\\.]+\", \"-\", slug).strip(\"-\")\n        return os.path.join(*slug.split(\".\"))", "response": "Returns the path to the file with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef include_dir(self, root):\n        parts = [root]\n        parts.extend(self.pathname.split(os.path.sep))\n        return \"/\".join(parts)", "response": "Return directory of file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef include_path(self):\n        parts = [self.include_dir(root=self.url_root)]\n        parts.append(\"index\")\n        return \"/\".join(parts)", "response": "Return absolute path without regarding OS path separator"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload objects from the filesystem into the paths dictionary.", "response": "def load(self, patterns, dirs, ignore=None):\n        \"\"\"\n        Load objects from the filesystem into the ``paths`` dictionary.\n\n        \"\"\"\n        for path in self.find_files(patterns=patterns, dirs=dirs, ignore=ignore):\n            data = self.read_file(path=path)\n            if data:\n                self.paths[path] = data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_object(self, obj):\n        self.objects[obj.id] = obj\n        self.all_objects[obj.id] = obj\n        child_stack = list(obj.children)\n        while child_stack:\n            child = child_stack.pop()\n            self.all_objects[child.id] = child\n            child_stack.extend(getattr(child, \"children\", ()))", "response": "Add an object to local and app environment storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map(self, options=None):\n        for path, data in self.paths.items():\n            for obj in self.create_class(data, options=options):\n                self.add_object(obj)", "response": "Trigger find of serialized sources and build objects"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading file into memory returning deserialized objects", "response": "def read_file(self, path, **kwargs):\n        \"\"\"Read file input into memory, returning deserialized objects\n\n        :param path: Path of file to read\n        \"\"\"\n        # TODO support JSON here\n        # TODO sphinx way of reporting errors in logs?\n        subcmd = \"jsdoc\"\n        if os.name == \"nt\":\n            subcmd = \".\".join([subcmd, \"cmd\"])\n\n        try:\n            parsed_data = json.loads(subprocess.check_output([subcmd, \"-X\", path]))\n            return parsed_data\n        except IOError:\n            LOGGER.warning(\"Error reading file: {0}\".format(path))\n        except TypeError:\n            LOGGER.warning(\"Error reading file: {0}\".format(path))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntriggering find of serialized sources and build objects", "response": "def map(self, options=None):\n        \"\"\"Trigger find of serialized sources and build objects\"\"\"\n        for path, data in self.paths.items():\n            for item in data:\n                for obj in self.create_class(item, options):\n                    obj.jinja_env = self.jinja_env\n                    self.add_object(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_class(self, data, options=None, **kwargs):\n        obj_map = dict((cls.type, cls) for cls in ALL_CLASSES)\n        try:\n            cls = obj_map[data[\"kind\"]]\n        except (KeyError, TypeError):\n            LOGGER.warning(\"Unknown Type: %s\" % data)\n        else:\n            # Recurse for children\n            obj = cls(data, jinja_env=self.jinja_env)\n            if \"children\" in data:\n                for child_data in data[\"children\"]:\n                    for child_obj in self.create_class(child_data, options=options):\n                        obj.children.append(child_obj)\n            yield obj", "response": "Create a new object based on Javascript data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts to lowercase removes non - word characters underscores and hyphens and converts leading and trailing whitespace to hyphens.", "response": "def slugify(value):\n    \"\"\"\n    Converts to lowercase, removes non-word characters (alphanumerics and\n    underscores) and converts spaces to hyphens. Also strips leading and\n    trailing whitespace.\n    \"\"\"\n    value = re.sub(r\"[^\\w\\s-]\", \"\", value).strip()\n    return re.sub(r\"[-\\s]+\", \"-\", value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self, patterns, dirs, ignore=None, **kwargs):\n        raise_error = kwargs.get(\"raise_error\", True)\n        all_files = set()\n        if not self.app.config.autoapi_file_patterns:\n            all_files = set()\n            for _file in self.find_files(\n                patterns=[\"docfx.json\"], dirs=dirs, ignore=ignore\n            ):\n                all_files.add(_file)\n        if not all_files:\n            for _file in self.find_files(patterns=patterns, dirs=dirs, ignore=ignore):\n                all_files.add(_file)\n        if all_files:\n            try:\n                command = [\"docfx\", \"metadata\", \"--raw\", \"--force\"]\n                command.extend(all_files)\n                proc = subprocess.Popen(\n                    \" \".join(command),\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    shell=True,\n                    env=dict(\n                        (key, os.environ[key])\n                        for key in [\n                            \"PATH\",\n                            \"HOME\",\n                            \"SYSTEMROOT\",\n                            \"USERPROFILE\",\n                            \"WINDIR\",\n                        ]\n                        if key in os.environ\n                    ),\n                )\n                _, error_output = proc.communicate()\n                if error_output:\n                    LOGGER.warning(error_output)\n            except (OSError, subprocess.CalledProcessError) as e:\n                LOGGER.warning(\"Error generating metadata: {0}\".format(e))\n                if raise_error:\n                    raise ExtensionError(\n                        \"Failure in docfx while generating AutoAPI output.\"\n                    )\n        # We now have yaml files\n        for xdoc_path in self.find_files(\n            patterns=[\"*.yml\"], dirs=[self.DOCFX_OUTPUT_PATH], ignore=ignore\n        ):\n            data = self.read_file(path=xdoc_path)\n            if data:\n                self.paths[xdoc_path] = data", "response": "Load objects from the filesystem into the paths dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread file into memory returning deserialized objects", "response": "def read_file(self, path, **kwargs):\n        \"\"\"Read file input into memory, returning deserialized objects\n\n        :param path: Path of file to read\n        \"\"\"\n        try:\n            with open(path, \"r\") as handle:\n                parsed_data = yaml.safe_load(handle)\n                return parsed_data\n        except IOError:\n            LOGGER.warning(\"Error reading file: {0}\".format(path))\n        except TypeError:\n            LOGGER.warning(\"Error reading file: {0}\".format(path))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntriggering find of serialized sources and build objects", "response": "def map(self, options=None):\n        \"\"\"Trigger find of serialized sources and build objects\"\"\"\n        for path, data in self.paths.items():\n            references = data.get(\"references\", [])\n            for item in data[\"items\"]:\n                for obj in self.create_class(item, options, references=references):\n                    self.add_object(obj)\n\n        self.organize_objects()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an object of the appropriate class based on the data passed in.", "response": "def create_class(self, data, options=None, path=None, **kwargs):\n        \"\"\"\n        Return instance of class based on Roslyn type property\n\n        Data keys handled here:\n\n            type\n                Set the object class\n\n            items\n                Recurse into :py:meth:`create_class` to create child object\n                instances\n\n        :param data: dictionary data from Roslyn output artifact\n        \"\"\"\n\n        obj_map = dict((cls.type, cls) for cls in ALL_CLASSES)\n        try:\n            cls = obj_map[data[\"type\"].lower()]\n        except KeyError:\n            LOGGER.warning(\"Unknown type: %s\" % data)\n        else:\n            obj = cls(\n                data,\n                jinja_env=self.jinja_env,\n                options=options,\n                url_root=self.url_root,\n                **kwargs\n            )\n\n            # Append child objects\n            # TODO this should recurse in the case we're getting back more\n            # complex argument listings\n\n            yield obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_object(self, obj):\n        if obj.top_level_object:\n            if isinstance(obj, DotNetNamespace):\n                self.namespaces[obj.name] = obj\n        self.objects[obj.id] = obj", "response": "Add an object to local and app environment storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef organize_objects(self):\n\n        def _render_children(obj):\n            for child in obj.children_strings:\n                child_object = self.objects.get(child)\n                if child_object:\n                    obj.item_map[child_object.plural].append(child_object)\n                    obj.children.append(child_object)\n\n            for key in obj.item_map:\n                obj.item_map[key].sort()\n\n        def _recurse_ns(obj):\n            if not obj:\n                return\n            namespace = obj.top_namespace\n            if namespace is not None:\n                ns_obj = self.top_namespaces.get(namespace)\n                if ns_obj is None or not isinstance(ns_obj, DotNetNamespace):\n                    for ns_obj in self.create_class(\n                        {\"uid\": namespace, \"type\": \"namespace\"}\n                    ):\n                        self.top_namespaces[ns_obj.id] = ns_obj\n                if obj not in ns_obj.children and namespace != obj.id:\n                    ns_obj.children.append(obj)\n\n        for obj in self.objects.values():\n            _render_children(obj)\n            _recurse_ns(obj)\n\n        # Clean out dead namespaces\n        for key, ns in self.top_namespaces.copy().items():\n            if not ns.children:\n                del self.top_namespaces[key]\n\n        for key, ns in self.namespaces.items():\n            if not ns.children:\n                del self.namespaces[key]", "response": "Organize objects and namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform_doc_comments(text):\n        try:\n            while True:\n                found = DOC_COMMENT_SEE_PATTERN.search(text)\n                if found is None:\n                    break\n                ref = found.group(\"attr_value\").replace(\"<\", \"\\<\").replace(\"`\", \"\\`\")\n\n                reftype = \"any\"\n                replacement = \"\"\n                # Given the pattern of `\\w:\\w+`, inspect first letter of\n                # reference for identity type\n                if ref[1] == \":\" and ref[0] in DOC_COMMENT_IDENTITIES:\n                    reftype = DOC_COMMENT_IDENTITIES[ref[:1]]\n                    ref = ref[2:]\n                    replacement = \":{reftype}:`{ref}`\".format(reftype=reftype, ref=ref)\n                elif ref[:2] == \"!:\":\n                    replacement = ref[2:]\n                else:\n                    replacement = \":any:`{ref}`\".format(ref=ref)\n\n                # Escape following text\n                text_end = text[found.end() :]\n                text_start = text[: found.start()]\n                text_end = re.sub(r\"^(\\S)\", r\"\\\\\\1\", text_end)\n                text_start = re.sub(r\"(\\S)$\", r\"\\1 \", text_start)\n\n                text = \"\".join([text_start, replacement, text_end])\n            while True:\n                found = DOC_COMMENT_PARAM_PATTERN.search(text)\n                if found is None:\n                    break\n\n                # Escape following text\n                text_end = text[found.end() :]\n                text_start = text[: found.start()]\n                text_end = re.sub(r\"^(\\S)\", r\"\\\\\\1\", text_end)\n                text_start = re.sub(r\"(\\S)$\", r\"\\1 \", text_start)\n\n                text = \"\".join(\n                    [text_start, \"``\", found.group(\"attr_value\"), \"``\", text_end]\n                )\n        except TypeError:\n            pass\n        return text", "response": "Transform the text of the doc comments into a set of XML elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_spec_identifier(self, obj_name):\n        ref = self.references.get(obj_name)\n        if ref is None:\n            return obj_name\n\n        resolved = ref.get(\"fullName\", obj_name)\n        spec = ref.get(\"spec.csharp\", [])\n        parts = []\n        for part in spec:\n            if part.get(\"name\") == \"<\":\n                parts.append(\"{\")\n            elif part.get(\"name\") == \">\":\n                parts.append(\"}\")\n            elif \"fullName\" in part and \"uid\" in part:\n                parts.append(\"{fullName}<{uid}>\".format(**part))\n            elif \"uid\" in part:\n                parts.append(part[\"uid\"])\n            elif \"fullName\" in part:\n                parts.append(part[\"fullName\"])\n        if parts:\n            resolved = \"\".join(parts)\n        return resolved", "response": "Find reference name based on spec identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef summary(self):\n        for line in self.docstring.splitlines():\n            line = line.strip()\n            if line:\n                return line\n\n        return \"\"", "response": "The summary line of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_import_alias(name, import_names):\n    resolved_name = name\n\n    for import_name, imported_as in import_names:\n        if import_name == name:\n            break\n        if imported_as == name:\n            resolved_name = import_name\n            break\n\n    return resolved_name", "response": "Resolve a name from an aliased import to its original name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_full_import_name(import_from, name):\n    partial_basename = resolve_import_alias(name, import_from.names)\n\n    module_name = import_from.modname\n    if import_from.level:\n        module = import_from.root()\n        assert isinstance(module, astroid.nodes.Module)\n        module_name = module.relative_to_absolute_name(\n            import_from.modname, level=import_from.level\n        )\n\n    return \"{}.{}\".format(module_name, partial_basename)", "response": "Get the full path of a name from an astroid. nodes. ImportFrom object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_full_basename(node, basename):\n    full_basename = basename\n\n    top_level_name = re.sub(r\"\\(.*\\)\", \"\", basename).split(\".\", 1)[0]\n    lookup_node = node\n    while not hasattr(lookup_node, \"lookup\"):\n        lookup_node = lookup_node.parent\n    assigns = lookup_node.lookup(top_level_name)[1]\n    for assignment in assigns:\n        if isinstance(assignment, astroid.nodes.ImportFrom):\n            import_name = get_full_import_name(assignment, top_level_name)\n            full_basename = basename.replace(top_level_name, import_name, 1)\n            break\n        elif isinstance(assignment, astroid.nodes.Import):\n            import_name = resolve_import_alias(top_level_name, assignment.names)\n            full_basename = basename.replace(top_level_name, import_name, 1)\n            break\n        elif isinstance(assignment, astroid.nodes.ClassDef):\n            full_basename = \"{}.{}\".format(assignment.root().name, assignment.name)\n            break\n\n    if isinstance(node, astroid.nodes.Call):\n        full_basename = re.sub(r\"\\(.*\\)\", \"()\", full_basename)\n\n    if full_basename.startswith(\"builtins.\"):\n        return full_basename[len(\"builtins.\") :]\n\n    if full_basename.startswith(\"__builtin__.\"):\n        return full_basename[len(\"__builtin__.\") :]\n\n    return full_basename", "response": "Resolve a partial base name to the full path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving the base nodes and partial names of a class to full names.", "response": "def get_full_basenames(bases, basenames):\n    \"\"\"Resolve the base nodes and partial names of a class to full names.\n\n    :param bases: The astroid node representing something that a class\n        inherits from.\n    :type bases: iterable(astroid.NodeNG)\n    :param basenames: The partial name of something that a class inherits from.\n    :type basenames: iterable(str)\n\n    :returns: The full names.\n    :rtype: iterable(str)\n    \"\"\"\n    for base, basename in zip(bases, basenames):\n        yield get_full_basename(base, basename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_assign_value(node):\n    try:\n        targets = node.targets\n    except AttributeError:\n        targets = [node.target]\n\n    if len(targets) == 1:\n        target = targets[0]\n        if isinstance(target, astroid.nodes.AssignName):\n            name = target.name\n        elif isinstance(target, astroid.nodes.AssignAttr):\n            name = target.attrname\n        else:\n            return None\n        return (name, _get_const_values(node.value))\n\n    return None", "response": "Get the name and value of the assignment of the given node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the type annotation of the assignment of the given node.", "response": "def get_assign_annotation(node):\n    \"\"\"Get the type annotation of the assignment of the given node.\n\n    :param node: The node to get the annotation for.\n    :type node: astroid.nodes.Assign or astroid.nodes.AnnAssign\n\n    :returns: The type annotation as a string, or None if one does not exist.\n    :type: str or None\n    \"\"\"\n    annotation = None\n\n    annotation_node = None\n    try:\n        annotation_node = node.annotation\n    except AttributeError:\n        # Python 2 has no support for type annotations, so use getattr\n        annotation_node = getattr(node, \"type_annotation\", None)\n\n    if annotation_node:\n        if isinstance(annotation_node, astroid.nodes.Const):\n            annotation = node.value\n        else:\n            annotation = annotation_node.as_string()\n\n    return annotation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the function is decorated with a property.", "response": "def is_decorated_with_property(node):\n    \"\"\"Check if the function is decorated as a property.\n\n    :param node: The node to check.\n    :type node: astroid.nodes.FunctionDef\n\n    :returns: True if the function is a property, False otherwise.\n    :rtype: bool\n    \"\"\"\n    if not node.decorators:\n        return False\n\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, astroid.Name):\n            continue\n\n        try:\n            if _is_property_decorator(decorator):\n                return True\n        except astroid.InferenceError:\n            pass\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_decorated_with_property_setter(node):\n    if not node.decorators:\n        return False\n\n    for decorator in node.decorators.nodes:\n        if (\n            isinstance(decorator, astroid.nodes.Attribute)\n            and decorator.attrname == \"setter\"\n        ):\n            return True\n\n    return False", "response": "Checks if the function is decorated with a property setter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the function is a constructor.", "response": "def is_constructor(node):\n    \"\"\"Check if the function is a constructor.\n\n    :param node: The node to check.\n    :type node: astroid.nodes.FunctionDef\n\n    :returns: True if the function is a contructor, False otherwise.\n    :rtype: bool\n    \"\"\"\n    return (\n        node.parent\n        and isinstance(node.parent.scope(), astroid.nodes.ClassDef)\n        and node.name == \"__init__\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a class is an exception.", "response": "def is_exception(node):\n    \"\"\"Check if a class is an exception.\n\n    :param node: The node to check.\n    :type node: astroid.nodes.ClassDef\n\n    :returns: True if the class is an exception, False otherwise.\n    :rtype: bool\n    \"\"\"\n    if (\n        node.name in (\"Exception\", \"BaseException\")\n        and node.root().name == _EXCEPTIONS_MODULE\n    ):\n        return True\n\n    if not hasattr(node, \"ancestors\"):\n        return False\n\n    return any(is_exception(parent) for parent in node.ancestors(recurs=True))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_local_import_from(node, package_name):\n    if not isinstance(node, astroid.ImportFrom):\n        return False\n\n    return (\n        node.level\n        or node.modname == package_name\n        or node.modname.startswith(package_name + \".\")\n    )", "response": "Checks if a node is an import from the local package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_module_all(node):\n    all_ = None\n\n    if \"__all__\" in node.locals:\n        assigned = next(node.igetattr(\"__all__\"))\n        if assigned is not astroid.Uninferable:\n            all_ = []\n            for elt in getattr(assigned, \"elts\", ()):\n                try:\n                    elt_name = next(elt.infer())\n                except astroid.InferenceError:\n                    continue\n\n                if elt_name is astroid.Uninferable:\n                    continue\n\n                if isinstance(elt_name, astroid.Const) and isinstance(\n                    elt_name.value, _STRING_TYPES\n                ):\n                    all_.append(elt_name.value)\n\n    return all_", "response": "Get the contents of the __all__ variable from a module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads AutoAPI data from the filesystem.", "response": "def run_autoapi(app):\n    \"\"\"\n    Load AutoAPI data from the filesystem.\n    \"\"\"\n\n    if not app.config.autoapi_dirs:\n        raise ExtensionError(\"You must configure an autoapi_dirs setting\")\n\n    # Make sure the paths are full\n    normalized_dirs = []\n    autoapi_dirs = app.config.autoapi_dirs\n    if isinstance(autoapi_dirs, str):\n        autoapi_dirs = [autoapi_dirs]\n    for path in autoapi_dirs:\n        if os.path.isabs(path):\n            normalized_dirs.append(path)\n        else:\n            normalized_dirs.append(os.path.normpath(os.path.join(app.confdir, path)))\n\n    for _dir in normalized_dirs:\n        if not os.path.exists(_dir):\n            raise ExtensionError(\n                \"AutoAPI Directory `{dir}` not found. \"\n                \"Please check your `autoapi_dirs` setting.\".format(dir=_dir)\n            )\n\n    normalized_root = os.path.normpath(\n        os.path.join(app.confdir, app.config.autoapi_root)\n    )\n    url_root = os.path.join(\"/\", app.config.autoapi_root)\n\n    sphinx_mapper = default_backend_mapping[app.config.autoapi_type]\n    sphinx_mapper_obj = sphinx_mapper(\n        app, template_dir=app.config.autoapi_template_dir, url_root=url_root\n    )\n    app.env.autoapi_mapper = sphinx_mapper_obj\n\n    if app.config.autoapi_file_patterns:\n        file_patterns = app.config.autoapi_file_patterns\n    else:\n        file_patterns = default_file_mapping.get(app.config.autoapi_type, [])\n\n    if app.config.autoapi_ignore:\n        ignore_patterns = app.config.autoapi_ignore\n    else:\n        ignore_patterns = default_ignore_patterns.get(app.config.autoapi_type, [])\n\n    if \".rst\" in app.config.source_suffix:\n        out_suffix = \".rst\"\n    elif \".txt\" in app.config.source_suffix:\n        out_suffix = \".txt\"\n    else:\n        # Fallback to first suffix listed\n        out_suffix = app.config.source_suffix[0]\n\n    # Actual meat of the run.\n    LOGGER.info(bold(\"[AutoAPI] \") + darkgreen(\"Loading Data\"))\n    sphinx_mapper_obj.load(\n        patterns=file_patterns, dirs=normalized_dirs, ignore=ignore_patterns\n    )\n\n    LOGGER.info(bold(\"[AutoAPI] \") + darkgreen(\"Mapping Data\"))\n    sphinx_mapper_obj.map(options=app.config.autoapi_options)\n\n    if app.config.autoapi_generate_api_docs:\n        LOGGER.info(bold(\"[AutoAPI] \") + darkgreen(\"Rendering Data\"))\n        sphinx_mapper_obj.output_rst(root=normalized_root, source_suffix=out_suffix)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef doctree_read(app, doctree):\n    if app.env.docname == \"index\":\n        all_docs = set()\n        insert = True\n        nodes = doctree.traverse(toctree)\n        toc_entry = \"%s/index\" % app.config.autoapi_root\n        add_entry = (\n            nodes\n            and app.config.autoapi_generate_api_docs\n            and app.config.autoapi_add_toctree_entry\n        )\n        if not add_entry:\n            return\n        # Capture all existing toctree entries\n        for node in nodes:\n            for entry in node[\"entries\"]:\n                all_docs.add(entry[1])\n        # Don't insert autoapi it's already present\n        for doc in all_docs:\n            if doc.find(app.config.autoapi_root) != -1:\n                insert = False\n        if insert and app.config.autoapi_add_toctree_entry:\n            # Insert AutoAPI index\n            nodes[-1][\"entries\"].append((None, u\"%s/index\" % app.config.autoapi_root))\n            nodes[-1][\"includefiles\"].append(u\"%s/index\" % app.config.autoapi_root)\n            message_prefix = bold(\"[AutoAPI] \")\n            message = darkgreen(\n                \"Adding AutoAPI TOCTree [{0}] to index.rst\".format(toc_entry)\n            )\n            LOGGER.info(message_prefix + message)", "response": "Read the TOC Tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexpands a wildcard placeholder to a sequence of named placeholders.", "response": "def _expand_wildcard_placeholder(original_module, originals_map, placeholder):\n    \"\"\"Expand a wildcard placeholder to a sequence of named placeholders.\n\n    :param original_module: The data dictionary of the module\n        that the placeholder is imported from.\n    :type original_module: dict\n    :param originals_map: A map of the names of children under the module\n        to their data dictionaries.\n    :type originals_map: dict(str, dict)\n    :param placeholder: The wildcard placeholder to expand.\n    :type placeholder: dict\n\n    :returns: The placeholders that the wildcard placeholder represents.\n    :rtype: list(dict)\n    \"\"\"\n    originals = originals_map.values()\n    if original_module[\"all\"] is not None:\n        originals = []\n        for name in original_module[\"all\"]:\n            if name == \"__all__\":\n                continue\n\n            if name not in originals_map:\n                msg = \"Invalid __all__ entry {0} in {1}\".format(\n                    name, original_module[\"name\"]\n                )\n                LOGGER.warning(msg)\n                continue\n\n            originals.append(originals_map[name])\n\n    placeholders = []\n    for original in originals:\n        new_full_name = placeholder[\"full_name\"].replace(\"*\", original[\"name\"])\n        new_original_path = placeholder[\"original_path\"].replace(\"*\", original[\"name\"])\n        if \"original_path\" in original:\n            new_original_path = original[\"original_path\"]\n        new_placeholder = dict(\n            placeholder,\n            name=original[\"name\"],\n            full_name=new_full_name,\n            original_path=new_original_path,\n        )\n        placeholders.append(new_placeholder)\n\n    return placeholders"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nresolve all placeholder children under a module.", "response": "def _resolve_module_placeholders(modules, module_name, visit_path, resolved):\n    \"\"\"Resolve all placeholder children under a module.\n\n    :param modules: A mapping of module names to their data dictionary.\n        Placeholders are resolved in place.\n    :type modules: dict(str, dict)\n    :param module_name: The name of the module to resolve.\n    :type module_name: str\n    :param visit_path: An ordered set of visited module names.\n    :type visited: collections.OrderedDict\n    :param resolved: A set of already resolved module names.\n    :type resolved: set(str)\n    \"\"\"\n    if module_name in resolved:\n        return\n\n    visit_path[module_name] = True\n\n    module, children = modules[module_name]\n    for child in list(children.values()):\n        if child[\"type\"] != \"placeholder\":\n            continue\n\n        if child[\"original_path\"] in modules:\n            module[\"children\"].remove(child)\n            children.pop(child[\"name\"])\n            continue\n\n        imported_from, original_name = child[\"original_path\"].rsplit(\".\", 1)\n        if imported_from in visit_path:\n            msg = \"Cannot resolve cyclic import: {0}, {1}\".format(\n                \", \".join(visit_path), imported_from\n            )\n            LOGGER.warning(msg)\n            module[\"children\"].remove(child)\n            children.pop(child[\"name\"])\n            continue\n\n        if imported_from not in modules:\n            msg = \"Cannot resolve import of unknown module {0} in {1}\".format(\n                imported_from, module_name\n            )\n            LOGGER.warning(msg)\n            module[\"children\"].remove(child)\n            children.pop(child[\"name\"])\n            continue\n\n        _resolve_module_placeholders(modules, imported_from, visit_path, resolved)\n\n        if original_name == \"*\":\n            original_module, originals_map = modules[imported_from]\n\n            # Replace the wildcard placeholder\n            # with a list of named placeholders.\n            new_placeholders = _expand_wildcard_placeholder(\n                original_module, originals_map, child\n            )\n            child_index = module[\"children\"].index(child)\n            module[\"children\"][child_index : child_index + 1] = new_placeholders\n            children.pop(child[\"name\"])\n\n            for new_placeholder in new_placeholders:\n                if new_placeholder[\"name\"] not in children:\n                    children[new_placeholder[\"name\"]] = new_placeholder\n                original = originals_map[new_placeholder[\"name\"]]\n                _resolve_placeholder(new_placeholder, original)\n        elif original_name not in modules[imported_from][1]:\n            msg = \"Cannot resolve import of {0} in {1}\".format(\n                child[\"original_path\"], module_name\n            )\n            LOGGER.warning(msg)\n            module[\"children\"].remove(child)\n            children.pop(child[\"name\"])\n            continue\n        else:\n            original = modules[imported_from][1][original_name]\n            _resolve_placeholder(child, original)\n\n    del visit_path[module_name]\n    resolved.add(module_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _resolve_placeholder(placeholder, original):\n    new = copy.deepcopy(original)\n    # The name remains the same.\n    new[\"name\"] = placeholder[\"name\"]\n    new[\"full_name\"] = placeholder[\"full_name\"]\n    # Record where the placeholder originally came from.\n    new[\"original_path\"] = original[\"full_name\"]\n    # The source lines for this placeholder do not exist in this file.\n    # The keys might not exist if original is a resolved placeholder.\n    new.pop(\"from_line_no\", None)\n    new.pop(\"to_line_no\", None)\n\n    # Resolve the children\n    stack = list(new.get(\"children\", ()))\n    while stack:\n        child = stack.pop()\n        # Relocate the child to the new location\n        assert child[\"full_name\"].startswith(original[\"full_name\"])\n        suffix = child[\"full_name\"][len(original[\"full_name\"]) :]\n        child[\"full_name\"] = new[\"full_name\"] + suffix\n        # The source lines for this placeholder do not exist in this file.\n        # The keys might not exist if original is a resolved placeholder.\n        child.pop(\"from_line_no\", None)\n        child.pop(\"to_line_no\", None)\n        # Resolve the remaining children\n        stack.extend(child.get(\"children\", ()))\n\n    placeholder.clear()\n    placeholder.update(new)", "response": "Resolve a placeholder to the given original object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(self, patterns, dirs, ignore=None):\n        for dir_ in dirs:\n            dir_root = dir_\n            if os.path.exists(os.path.join(dir_, \"__init__.py\")):\n                dir_root = os.path.abspath(os.path.join(dir_, os.pardir))\n\n            for path in self.find_files(patterns=patterns, dirs=[dir_], ignore=ignore):\n                data = self.read_file(path=path)\n                if data:\n                    data[\"relative_path\"] = os.path.relpath(path, dir_root)\n                    self.paths[path] = data", "response": "Load objects from the filesystem into the paths dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading file into memory returning deserialized objects", "response": "def read_file(self, path, **kwargs):\n        \"\"\"Read file input into memory, returning deserialized objects\n\n        :param path: Path of file to read\n        \"\"\"\n        try:\n            parsed_data = Parser().parse_file(path)\n            return parsed_data\n        except (IOError, TypeError, ImportError):\n            LOGGER.warning(\"Error reading file: {0}\".format(path))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresolves objects that have been imported from elsewhere.", "response": "def _resolve_placeholders(self):\n        \"\"\"Resolve objects that have been imported from elsewhere.\"\"\"\n        modules = {}\n        for module in self.paths.values():\n            children = {child[\"name\"]: child for child in module[\"children\"]}\n            modules[module[\"name\"]] = (module, children)\n\n        resolved = set()\n        for module_name in modules:\n            visit_path = collections.OrderedDict()\n            _resolve_module_placeholders(modules, module_name, visit_path, resolved)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a class from the passed in data.", "response": "def create_class(self, data, options=None, **kwargs):\n        \"\"\"Create a class from the passed in data\n\n        :param data: dictionary data of parser output\n        \"\"\"\n        obj_map = dict(\n            (cls.type, cls)\n            for cls in [\n                PythonClass,\n                PythonFunction,\n                PythonModule,\n                PythonMethod,\n                PythonPackage,\n                PythonAttribute,\n                PythonData,\n                PythonException,\n            ]\n        )\n        try:\n            cls = obj_map[data[\"type\"]]\n        except KeyError:\n            LOGGER.warning(\"Unknown type: %s\" % data[\"type\"])\n        else:\n            obj = cls(\n                data,\n                class_content=self.app.config.autoapi_python_class_content,\n                options=self.app.config.autoapi_options,\n                jinja_env=self.jinja_env,\n                url_root=self.url_root,\n                **kwargs\n            )\n\n            lines = sphinx.util.docstrings.prepare_docstring(obj.docstring)\n            if lines and \"autodoc-process-docstring\" in self.app.events.events:\n                self.app.emit(\n                    \"autodoc-process-docstring\",\n                    cls.type,\n                    obj.name,\n                    None,  # object\n                    None,  # options\n                    lines,\n                )\n            obj.docstring = \"\\n\".join(lines)\n\n            for child_data in data.get(\"children\", []):\n                for child_obj in self.create_class(\n                    child_data, options=options, **kwargs\n                ):\n                    obj.children.append(child_obj)\n            yield obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a node structure that Sphinx expects for TOC Tree entries.", "response": "def _build_toc_node(docname, anchor=\"anchor\", text=\"test text\", bullet=False):\n    \"\"\"\n    Create the node structure that Sphinx expects for TOC Tree entries.\n\n    The ``bullet`` argument wraps it in a ``nodes.bullet_list``,\n    which is how you nest TOC Tree entries.\n    \"\"\"\n    reference = nodes.reference(\n        \"\",\n        \"\",\n        internal=True,\n        refuri=docname,\n        anchorname=\"#\" + anchor,\n        *[nodes.Text(text, text)]\n    )\n    para = addnodes.compact_paragraph(\"\", \"\", reference)\n    ret_list = nodes.list_item(\"\", para)\n    return nodes.bullet_list(\"\", ret_list) if bullet else ret_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _traverse_parent(node, objtypes):\n    curr_node = node.parent\n    while curr_node is not None:\n        if isinstance(curr_node, objtypes):\n            return curr_node\n        curr_node = curr_node.parent\n    return None", "response": "Traverse up the tree until you hit the objtypes referenced."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_toc_node(toc, ref_id, objtype):\n    for check_node in toc.traverse(nodes.reference):\n        if objtype == nodes.section and (\n            check_node.attributes[\"refuri\"] == ref_id\n            or check_node.attributes[\"anchorname\"] == \"#\" + ref_id\n        ):\n            return check_node\n        if (\n            objtype == addnodes.desc\n            and check_node.attributes[\"anchorname\"] == \"#\" + ref_id\n        ):\n            return check_node\n    return None", "response": "Find the actual TOC node for a ref_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_domain_to_toctree(app, doctree, docname):\n    toc = app.env.tocs[docname]\n    for desc_node in doctree.traverse(addnodes.desc):\n        try:\n            ref_id = desc_node.children[0].attributes[\"ids\"][0]\n        except (KeyError, IndexError) as e:\n            LOGGER.warning(\"Invalid desc node: %s\" % e)\n            continue\n        try:\n            # Python domain object\n            ref_text = desc_node[0].attributes[\"fullname\"].split(\".\")[-1].split(\"(\")[0]\n        except (KeyError, IndexError):\n            # TODO[eric]: Support other Domains and ways of accessing this data\n            # Use `astext` for other types of domain objects\n            ref_text = desc_node[0].astext().split(\".\")[-1].split(\"(\")[0]\n        # This is the actual object that will exist in the TOC Tree\n        # Sections by default, and other Desc nodes that we've previously placed.\n        parent_node = _traverse_parent(\n            node=desc_node, objtypes=(addnodes.desc, nodes.section)\n        )\n\n        if parent_node:\n            toc_reference = _get_toc_reference(app, parent_node, toc, docname)\n            if toc_reference:\n                # Get the last child of our parent's bullet list, this is where \"we\" live.\n                toc_insertion_point = _traverse_parent(\n                    toc_reference, nodes.bullet_list\n                )[-1]\n                # Ensure we're added another bullet list so that we nest inside the parent,\n                # not next to it\n                if toc_insertion_point and isinstance(\n                    toc_insertion_point[0], nodes.bullet_list\n                ):\n                    new_insert = toc_insertion_point[0]\n                    to_add = _build_toc_node(docname, anchor=ref_id, text=ref_text)\n                    new_insert.append(to_add)\n                else:\n                    to_add = _build_toc_node(\n                        docname, anchor=ref_id, text=ref_text, bullet=True\n                    )\n                    toc_insertion_point.append(to_add)", "response": "Add domain objects to the toctree dynamically."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef warn(self, msg):\n        self.warnings.append(\n            self.state.document.reporter.warning(msg, line=self.lineno)\n        )", "response": "Add a warning message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_names(self):\n        for line in self.content:\n            line = line.strip()\n            if line and re.search(\"^[a-zA-Z0-9]\", line):\n                yield line", "response": "Get the names of the objects to include in the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef humanize_duration(duration):\n    days = duration.days\n    hours = int(duration.seconds / 3600)\n    minutes = int(duration.seconds % 3600 / 60)\n    seconds = int(duration.seconds % 3600 % 60)\n\n    parts = []\n    if days > 0:\n        parts.append(u'%s %s' % (days, pluralize(days, _('day,days'))))\n\n    if hours > 0:\n        parts.append(u'%s %s' % (hours, pluralize(hours, _('hour,hours'))))\n\n    if minutes > 0:\n        parts.append(u'%s %s' % (minutes, pluralize(minutes, _('minute,minutes'))))\n\n    if seconds > 0:\n        parts.append(u'%s %s' % (seconds, pluralize(seconds, _('second,seconds'))))\n\n    return ', '.join(parts) if len(parts) != 0 else _('< 1 second')", "response": "Returns a humanized string representing time difference in a sequence of days hours minutes and seconds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the cache and runs the cron or not.", "response": "def run_cron_with_cache_check(cron_class, force=False, silent=False):\n    \"\"\"\n    Checks the cache and runs the cron or not.\n\n    @cron_class - cron class to run.\n    @force      - run job even if not scheduled\n    @silent     - suppress notifications\n    \"\"\"\n\n    with CronJobManager(cron_class, silent) as manager:\n        manager.run(force)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_old_log_entries():\n    if hasattr(settings, 'DJANGO_CRON_DELETE_LOGS_OLDER_THAN'):\n        delta = timedelta(days=settings.DJANGO_CRON_DELETE_LOGS_OLDER_THAN)\n        CronJobLog.objects.filter(end_time__lt=get_current_time() - delta).delete()", "response": "Removes old log entries if the appropriate setting has been set"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the cron command line arguments.", "response": "def handle(self, *args, **options):\n        \"\"\"\n        Iterates over all the CRON_CLASSES (or if passed in as a commandline argument)\n        and runs them.\n        \"\"\"\n        cron_classes = options['cron_classes']\n        if cron_classes:\n            cron_class_names = cron_classes\n        else:\n            cron_class_names = getattr(settings, 'CRON_CLASSES', [])\n\n        try:\n            crons_to_run = [get_class(x) for x in cron_class_names]\n        except Exception:\n            error = traceback.format_exc()\n            self.stdout.write('Make sure these are valid cron class names: %s\\n%s' % (cron_class_names, error))\n            return\n\n        for cron_class in crons_to_run:\n            run_cron_with_cache_check(\n                cron_class,\n                force=options['force'],\n                silent=options['silent']\n            )\n\n        clear_old_log_entries()\n        close_old_connections()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a boolean determining whether this cron should run now or not.", "response": "def should_run_now(self, force=False):\n        from django_cron.models import CronJobLog\n        cron_job = self.cron_job\n        \"\"\"\n        Returns a boolean determining whether this cron should run now or not!\n        \"\"\"\n\n        self.user_time = None\n        self.previously_ran_successful_cron = None\n\n        # If we pass --force options, we force cron run\n        if force:\n            return True\n        if cron_job.schedule.run_every_mins is not None:\n\n            # We check last job - success or not\n            last_job = None\n            try:\n                last_job = CronJobLog.objects.filter(code=cron_job.code).latest('start_time')\n            except CronJobLog.DoesNotExist:\n                pass\n            if last_job:\n                if not last_job.is_success and cron_job.schedule.retry_after_failure_mins:\n                    if get_current_time() > last_job.start_time + timedelta(minutes=cron_job.schedule.retry_after_failure_mins):\n                        return True\n                    else:\n                        return False\n\n            try:\n                self.previously_ran_successful_cron = CronJobLog.objects.filter(\n                    code=cron_job.code,\n                    is_success=True,\n                    ran_at_time__isnull=True\n                ).latest('start_time')\n            except CronJobLog.DoesNotExist:\n                pass\n\n            if self.previously_ran_successful_cron:\n                if get_current_time() > self.previously_ran_successful_cron.start_time + timedelta(minutes=cron_job.schedule.run_every_mins):\n                    return True\n            else:\n                return True\n\n        if cron_job.schedule.run_at_times:\n            for time_data in cron_job.schedule.run_at_times:\n                user_time = time.strptime(time_data, \"%H:%M\")\n                now = get_current_time()\n                actual_time = time.strptime(\"%s:%s\" % (now.hour, now.minute), \"%H:%M\")\n                if actual_time >= user_time:\n                    qset = CronJobLog.objects.filter(\n                        code=cron_job.code,\n                        ran_at_time=time_data,\n                        is_success=True\n                    ).filter(\n                        Q(start_time__gt=now) | Q(end_time__gte=now.replace(hour=0, minute=0, second=0, microsecond=0))\n                    )\n                    if not qset:\n                        self.user_time = time_data\n                        return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the cron job.", "response": "def run(self, force=False):\n        \"\"\"\n        apply the logic of the schedule and call do() on the CronJobBase class\n        \"\"\"\n        cron_job_class = self.cron_job_class\n        if not issubclass(cron_job_class, CronJobBase):\n            raise Exception('The cron_job to be run must be a subclass of %s' % CronJobBase.__name__)\n\n        with self.lock_class(cron_job_class, self.silent):\n            self.cron_job = cron_job_class()\n\n            if self.should_run_now(force):\n                logger.debug(\"Running cron: %s code %s\", cron_job_class.__name__, self.cron_job.code)\n                self.msg = self.cron_job.do()\n                self.make_log(self.msg, success=True)\n                self.cron_job.set_prev_success_cron(self.previously_ran_successful_cron)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lock(self):\n        if self.cache.get(self.lock_name):\n            return False\n        else:\n            self.cache.set(self.lock_name, timezone.now(), self.timeout)\n            return True", "response": "This method locks the current job in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_invitation_model():\n    path = app_settings.INVITATION_MODEL\n    try:\n        return django_apps.get_model(path)\n    except ValueError:\n        raise ImproperlyConfigured(\n            \"path must be of the form 'app_label.model_name'\"\n        )\n    except LookupError:\n        raise ImproperlyConfigured(\n            \"path refers to model '%s' that\\\n             has not been installed\" % app_settings.INVITATION_MODEL\n        )", "response": "Returns the Invitation model that is active in this project."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering an e - mail to the given email.", "response": "def render_mail(self, template_prefix, email, context):\n        \"\"\"\n        Renders an e-mail to `email`.  `template_prefix` identifies the\n        e-mail that is to be sent, e.g. \"account/email/email_confirmation\"\n        \"\"\"\n        subject = render_to_string('{0}_subject.txt'.format(template_prefix),\n                                   context)\n        # remove superfluous line breaks\n        subject = \" \".join(subject.splitlines()).strip()\n        subject = self.format_email_subject(subject)\n\n        bodies = {}\n        for ext in ['html', 'txt']:\n            try:\n                template_name = '{0}_message.{1}'.format(template_prefix, ext)\n                bodies[ext] = render_to_string(template_name,\n                                               context).strip()\n            except TemplateDoesNotExist:\n                if ext == 'txt' and not bodies:\n                    # We need at least one body\n                    raise\n        if 'txt' in bodies:\n            msg = EmailMultiAlternatives(subject,\n                                         bodies['txt'],\n                                         settings.DEFAULT_FROM_EMAIL,\n                                         [email])\n            if 'html' in bodies:\n                msg.attach_alternative(bodies['html'], 'text/html')\n        else:\n            msg = EmailMessage(subject,\n                               bodies['html'],\n                               settings.DEFAULT_FROM_EMAIL,\n                               [email])\n            msg.content_subtype = 'html'  # Main content is now text/html\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all images in the registry.", "response": "async def list(self, **params) -> Mapping:\n        \"\"\"\n        List of images\n        \"\"\"\n        response = await self.docker._query_json(\"images/json\", \"GET\", params=params)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def inspect(self, name: str) -> Mapping:\n        response = await self.docker._query_json(\"images/{name}/json\".format(name=name))\n        return response", "response": "Return low - level information about an image"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def pull(\n        self,\n        from_image: str,\n        *,\n        auth: Optional[Union[MutableMapping, str, bytes]] = None,\n        tag: str = None,\n        repo: str = None,\n        stream: bool = False\n    ) -> Mapping:\n        \"\"\"\n        Similar to `docker pull`, pull an image locally\n\n        Args:\n            fromImage: name of the image to pull\n            repo: repository name given to an image when it is imported\n            tag: if empty when pulling an image all tags\n                 for the given image to be pulled\n            auth: special {'auth': base64} pull private repo\n        \"\"\"\n        image = from_image  # TODO: clean up\n        params = {\"fromImage\": image}\n        headers = {}\n        if repo:\n            params[\"repo\"] = repo\n        if tag:\n            params[\"tag\"] = tag\n        if auth is not None:\n            registry, has_registry_host, _ = image.partition(\"/\")\n            if not has_registry_host:\n                raise ValueError(\n                    \"Image should have registry host \"\n                    \"when auth information is provided\"\n                )\n            # TODO: assert registry == repo?\n            headers[\"X-Registry-Auth\"] = compose_auth_header(auth, registry)\n        response = await self.docker._query(\n            \"images/create\", \"POST\", params=params, headers=headers\n        )\n        return await json_stream_result(response, stream=stream)", "response": "Pull an image locally"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def tag(self, name: str, repo: str, *, tag: str = None) -> bool:\n        params = {\"repo\": repo}\n\n        if tag:\n            params[\"tag\"] = tag\n\n        await self.docker._query(\n            \"images/{name}/tag\".format(name=name),\n            \"POST\",\n            params=params,\n            headers={\"content-type\": \"application/json\"},\n        )\n        return True", "response": "Tag the given image so that it becomes part of a repository."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete an image along with any untagged parent images that are referenced by that image.", "response": "async def delete(\n        self, name: str, *, force: bool = False, noprune: bool = False\n    ) -> List:\n        \"\"\"\n        Remove an image along with any untagged parent\n        images that were referenced by that image\n\n        Args:\n            name: name/id of the image to delete\n            force: remove the image even if it is being used\n                   by stopped containers or has other tags\n            noprune: don't delete untagged parent images\n\n        Returns:\n            List of deleted images\n        \"\"\"\n        params = {\"force\": force, \"noprune\": noprune}\n        response = await self.docker._query_json(\n            \"images/{name}\".format(name=name), \"DELETE\", params=params\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def build(\n        self,\n        *,\n        remote: str = None,\n        fileobj: BinaryIO = None,\n        path_dockerfile: str = None,\n        tag: str = None,\n        quiet: bool = False,\n        nocache: bool = False,\n        buildargs: Mapping = None,\n        pull: bool = False,\n        rm: bool = True,\n        forcerm: bool = False,\n        labels: Mapping = None,\n        stream: bool = False,\n        encoding: str = None\n    ) -> Mapping:\n        \"\"\"\n        Build an image given a remote Dockerfile\n        or a file object with a Dockerfile inside\n\n        Args:\n            path_dockerfile: path within the build context to the Dockerfile\n            remote: a Git repository URI or HTTP/HTTPS context URI\n            quiet: suppress verbose build output\n            nocache: do not use the cache when building the image\n            rm: remove intermediate containers after a successful build\n            pull: downloads any updates to the FROM image in Dockerfiles\n            encoding: set `Content-Encoding` for the file object your send\n            forcerm: always remove intermediate containers, even upon failure\n            labels: arbitrary key/value labels to set on the image\n            fileobj: a tar archive compressed or not\n        \"\"\"\n\n        local_context = None\n\n        headers = {}\n\n        params = {\n            \"t\": tag,\n            \"rm\": rm,\n            \"q\": quiet,\n            \"pull\": pull,\n            \"remote\": remote,\n            \"nocache\": nocache,\n            \"forcerm\": forcerm,\n            \"dockerfile\": path_dockerfile,\n        }\n\n        if remote is None and fileobj is None:\n            raise ValueError(\"You need to specify either remote or fileobj\")\n\n        if fileobj and remote:\n            raise ValueError(\"You cannot specify both fileobj and remote\")\n\n        if fileobj and not encoding:\n            raise ValueError(\"You need to specify an encoding\")\n\n        if remote is None and fileobj is None:\n            raise ValueError(\"Either remote or fileobj needs to be provided.\")\n\n        if fileobj:\n            local_context = fileobj.read()\n            headers[\"content-type\"] = \"application/x-tar\"\n\n        if fileobj and encoding:\n            headers[\"Content-Encoding\"] = encoding\n\n        if buildargs:\n            params.update({\"buildargs\": json.dumps(buildargs)})\n\n        if labels:\n            params.update({\"labels\": json.dumps(labels)})\n\n        response = await self.docker._query(\n            \"build\",\n            \"POST\",\n            params=clean_map(params),\n            headers=headers,\n            data=local_context,\n        )\n\n        return await json_stream_result(response, stream=stream)", "response": "Build an image from a Dockerfile or a file object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a tarball of an image by name or id.", "response": "async def export_image(self, name: str):\n        \"\"\"\n        Get a tarball of an image by name or id.\n\n        Args:\n            name: name/id of the image to be exported\n\n        Returns:\n            Streamreader of tarball image\n        \"\"\"\n        response = await self.docker._query(\n            \"images/{name}/get\".format(name=name), \"GET\"\n        )\n        return response.content"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def import_image(self, data, stream: bool = False):\n        headers = {\"Content-Type\": \"application/x-tar\"}\n        response = await self.docker._query_chunked_post(\n            \"images/load\", \"POST\", data=data, headers=headers\n        )\n        return await json_stream_result(response, stream=stream)", "response": "Imports the tarball of an image to the docker."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def parse_result(response, response_type=None, *, encoding=\"utf-8\"):\n    if response_type is None:\n        ct = response.headers.get(\"content-type\")\n        if ct is None:\n            cl = response.headers.get(\"content-length\")\n            if cl is None or cl == \"0\":\n                return \"\"\n            raise TypeError(\n                \"Cannot auto-detect response type \"\n                \"due to missing Content-Type header.\"\n            )\n        main_type, sub_type, extras = parse_content_type(ct)\n        if sub_type == \"json\":\n            response_type = \"json\"\n        elif sub_type == \"x-tar\":\n            response_type = \"tar\"\n        elif (main_type, sub_type) == (\"text\", \"plain\"):\n            response_type = \"text\"\n            encoding = extras.get(\"charset\", encoding)\n        else:\n            raise TypeError(\"Unrecognized response type: {ct}\".format(ct=ct))\n    if \"tar\" == response_type:\n        what = await response.read()\n        return tarfile.open(mode=\"r\", fileobj=BytesIO(what))\n    if \"json\" == response_type:\n        data = await response.json(encoding=encoding)\n    elif \"text\" == response_type:\n        data = await response.text(encoding=encoding)\n    else:\n        data = await response.read()\n    return data", "response": "Convert the response to native objects by the given response type\nFormula or the auto - detected HTTP content - type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_content_type(ct: str) -> Tuple[str, str, Mapping[str, str]]:\n    pieces = ct.split(\";\")\n    try:\n        main_type, sub_type = pieces[0].split(\"/\")\n    except ValueError:\n        msg = 'Invalid mime-type component: \"{0}\"'.format(pieces[0])\n        raise ValueError(msg)\n    if len(pieces) > 1:\n        options = {}\n        for opt in pieces[1:]:\n            opt = opt.strip()\n            if not opt:\n                continue\n            try:\n                k, v = opt.split(\"=\", 1)\n            except ValueError:\n                msg = 'Invalid option component: \"{0}\"'.format(opt)\n                raise ValueError(msg)\n            else:\n                options[k.lower()] = v.lower()\n    else:\n        options = {}\n    return main_type.lower(), sub_type.lower(), options", "response": "Parses the value of HTTP Content - Type header into main sub MIME types and other extra options as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new copy of the given Mapping object with the keys with None values from the given Mapping object.", "response": "def clean_map(obj: Mapping[Any, Any]) -> Mapping[Any, Any]:\n    \"\"\"\n    Return a new copied dictionary without the keys with ``None`` values from\n    the given Mapping object.\n    \"\"\"\n    return {k: v for k, v in obj.items() if v is not None}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_env(key, value: Union[None, bytes, str]) -> str:\n    if value is None:\n        return key\n    if isinstance(value, bytes):\n        value = value.decode(\"utf-8\")\n\n    return \"{key}={value}\".format(key=key, value=value)", "response": "Formats envs from key = value to [ key = value ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_networks(networks: Iterable[str] = None) -> Optional[Iterable[str]]:\n    if not networks:\n        return networks\n    if not isinstance(networks, list):\n        raise TypeError(\"networks parameter must be a list.\")\n\n    result = []\n    for n in networks:\n        if isinstance(n, str):\n            n = {\"Target\": n}\n        result.append(n)\n    return result", "response": "Cleans the values inside networks Returns a new list of dicts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean_filters(filters: Mapping = None) -> str:\n\n    if filters and isinstance(filters, dict):\n        for k, v in filters.items():\n            if not isinstance(v, list):\n                v = [v]\n            filters[k] = v\n\n    return json.dumps(filters)", "response": "Clean the filters dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a zipped tar archive from a Dockerfile", "response": "def mktar_from_dockerfile(fileobject: BinaryIO) -> IO:\n    \"\"\"\n    Create a zipped tar archive from a Dockerfile\n    **Remember to close the file object**\n    Args:\n        fileobj: a Dockerfile\n    Returns:\n        a NamedTemporaryFile() object\n    \"\"\"\n\n    f = tempfile.NamedTemporaryFile()\n    t = tarfile.open(mode=\"w:gz\", fileobj=f)\n\n    if isinstance(fileobject, BytesIO):\n        dfinfo = tarfile.TarInfo(\"Dockerfile\")\n        dfinfo.size = len(fileobject.getvalue())\n        fileobject.seek(0)\n    else:\n        dfinfo = t.gettarinfo(fileobj=fileobject, arcname=\"Dockerfile\")\n\n    t.addfile(dfinfo, fileobject)\n    t.close()\n    f.seek(0)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compose_auth_header(\n    auth: Union[MutableMapping, str, bytes], registry_addr: str = None\n) -> str:\n    \"\"\"\n    Validate and compose base64-encoded authentication header\n    with an optional support for parsing legacy-style \"user:password\"\n    strings.\n\n    Args:\n        auth: Authentication information\n        registry_addr: An address of the registry server\n\n    Returns:\n        A base64-encoded X-Registry-Auth header value\n    \"\"\"\n    if isinstance(auth, Mapping):\n        # Validate the JSON format only.\n        if \"identitytoken\" in auth:\n            pass\n        elif \"auth\" in auth:\n            return compose_auth_header(auth[\"auth\"], registry_addr)\n        else:\n            if registry_addr:\n                auth[\"serveraddress\"] = registry_addr\n        auth_json = json.dumps(auth).encode(\"utf-8\")\n    elif isinstance(auth, (str, bytes)):\n        # Parse simple \"username:password\"-formatted strings\n        # and attach the server address specified.\n        if isinstance(auth, bytes):\n            auth = auth.decode(\"utf-8\")\n        s = base64.b64decode(auth)\n        username, passwd = s.split(b\":\", 1)\n        config = {\n            \"username\": username.decode(\"utf-8\"),\n            \"password\": passwd.decode(\"utf-8\"),\n            \"email\": None,\n            \"serveraddress\": registry_addr,\n        }\n        auth_json = json.dumps(config).encode(\"utf-8\")\n    else:\n        raise TypeError(\"auth must be base64 encoded string/bytes or a dictionary\")\n    auth = base64.b64encode(auth_json).decode(\"ascii\")\n    return auth", "response": "Validate and compose base64 - encoded authentication header value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def inspect(self, task_id: str) -> Mapping[str, Any]:\n\n        response = await self.docker._query_json(\n            \"tasks/{task_id}\".format(task_id=task_id), method=\"GET\"\n        )\n        return response", "response": "Return info about a task."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and start a container.", "response": "async def run(self, config, *, name=None):\n        \"\"\"\n        Create and start a container.\n\n        If container.start() will raise an error the exception will contain\n        a `container_id` attribute with the id of the container.\n        \"\"\"\n        try:\n            container = await self.create(config, name=name)\n        except DockerError as err:\n            # image not find, try pull it\n            if err.status == 404 and \"Image\" in config:\n                await self.docker.pull(config[\"Image\"])\n                container = await self.create(config, name=name)\n            else:\n                raise err\n\n        try:\n            await container.start()\n        except DockerError as err:\n            raise DockerContainerError(\n                err.status, {\"message\": err.message}, container[\"id\"]\n            )\n\n        return container"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subscribe(self, *, create_task=True, **params):\n        if create_task and not self.task:\n            self.task = asyncio.ensure_future(self.run(**params))\n        return self.channel.subscribe()", "response": "Subscribes to the Docker events channel. Use the keyword argument create_task = False to prevent automatically spawning the background tasks that listen to the events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying the events endpoint of the Docker daemon and publish messages inside the asyncio queue.", "response": "async def run(self, **params):\n        \"\"\"\n        Query the events endpoint of the Docker daemon.\n\n        Publish messages inside the asyncio queue.\n        \"\"\"\n        if self.json_stream:\n            warnings.warn(\"already running\", RuntimeWarning, stackelevel=2)\n            return\n        forced_params = {\"stream\": True}\n        params = ChainMap(forced_params, params)\n        try:\n            # timeout has to be set to 0, None is not passed\n            # Otherwise after 5 minutes the client\n            # will close the connection\n            # http://aiohttp.readthedocs.io/en/stable/client_reference.html#aiohttp.ClientSession.request\n            response = await self.docker._query(\n                \"events\", method=\"GET\", params=params, timeout=0\n            )\n            self.json_stream = await json_stream_result(\n                response, self._transform_event, human_bool(params[\"stream\"])\n            )\n            try:\n                async for data in self.json_stream:\n                    await self.channel.publish(data)\n            finally:\n                if self.json_stream is not None:\n                    await self.json_stream._close()\n                self.json_stream = None\n        finally:\n            # signal termination to subscribers\n            await self.channel.publish(None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def inspect(self, *, node_id: str) -> Mapping[str, Any]:\n\n        response = await self.docker._query_json(\n            \"nodes/{node_id}\".format(node_id=node_id), method=\"GET\"\n        )\n        return response", "response": "Inspect a node s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the spec of a node.", "response": "async def update(\n        self, *, node_id: str, version: int, spec: Mapping[str, Any]\n    ) -> Mapping[str, Any]:\n        \"\"\"\n        Update the spec of a node.\n\n        Args:\n            node_id: The ID or name of the node\n            version: version number of the node being updated\n            spec: fields to be updated\n        \"\"\"\n\n        params = {\"version\": version}\n\n        if \"Role\" in spec:\n            assert spec[\"Role\"] in {\"worker\", \"manager\"}\n\n        if \"Availability\" in spec:\n            assert spec[\"Availability\"] in {\"active\", \"pause\", \"drain\"}\n\n        response = await self.docker._query_json(\n            \"nodes/{node_id}/update\".format(node_id=node_id),\n            method=\"POST\",\n            params=params,\n            data=spec,\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a node from a swarm.", "response": "async def remove(self, *, node_id: str, force: bool = False) -> Mapping[str, Any]:\n        \"\"\"\n        Remove a node from a swarm.\n\n        Args:\n            node_id: The ID or name of the node\n        \"\"\"\n\n        params = {\"force\": force}\n\n        response = await self.docker._query_json(\n            \"nodes/{node_id}\".format(node_id=node_id), method=\"DELETE\", params=params\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the response object by performing the HTTP request. The caller is responsible to finalize the response object.", "response": "async def _query(\n        self,\n        path,\n        method=\"GET\",\n        *,\n        params=None,\n        data=None,\n        headers=None,\n        timeout=None,\n        chunked=None\n    ):\n        \"\"\"\n        Get the response object by performing the HTTP request.\n        The caller is responsible to finalize the response object.\n        \"\"\"\n        url = self._canonicalize_url(path)\n        if headers and \"content-type\" not in headers:\n            headers[\"content-type\"] = \"application/json\"\n        try:\n            response = await self.session.request(\n                method,\n                url,\n                params=httpize(params),\n                headers=headers,\n                data=data,\n                timeout=timeout,\n                chunked=chunked,\n            )\n        except asyncio.TimeoutError:\n            raise\n        if (response.status // 100) in [4, 5]:\n            what = await response.read()\n            content_type = response.headers.get(\"content-type\", \"\")\n            response.close()\n            if content_type == \"application/json\":\n                raise DockerError(response.status, json.loads(what.decode(\"utf8\")))\n            else:\n                raise DockerError(response.status, {\"message\": what.decode(\"utf8\")})\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _query_json(\n        self, path, method=\"GET\", *, params=None, data=None, headers=None, timeout=None\n    ):\n        \"\"\"\n        A shorthand of _query() that treats the input as JSON.\n        \"\"\"\n        if headers is None:\n            headers = {}\n        headers[\"content-type\"] = \"application/json\"\n        if not isinstance(data, (str, bytes)):\n            data = json.dumps(data)\n        response = await self._query(\n            path, method, params=params, data=data, headers=headers, timeout=timeout\n        )\n        data = await parse_result(response)\n        return data", "response": "A shorthand of _query that treats the input as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _query_chunked_post(\n        self, path, method=\"POST\", *, params=None, data=None, headers=None, timeout=None\n    ):\n        \"\"\"\n        A shorthand for uploading data by chunks\n        \"\"\"\n        if headers is None:\n            headers = {}\n        if headers and \"content-type\" not in headers:\n            headers[\"content-type\"] = \"application/octet-stream\"\n        response = await self._query(\n            path,\n            method,\n            params=params,\n            data=data,\n            headers=headers,\n            timeout=timeout,\n            chunked=True,\n        )\n        return response", "response": "A shorthand for uploading data by chunks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _docker_machine_ssl_context():\n        context = ssl.SSLContext(ssl.PROTOCOL_TLS)\n        context.set_ciphers(ssl._RESTRICTED_SERVER_CIPHERS)\n        certs_path = os.environ.get(\"DOCKER_CERT_PATH\", None)\n        if certs_path is None:\n            raise ValueError(\n                \"Cannot create ssl context, \" \"DOCKER_CERT_PATH is not set!\"\n            )\n        certs_path = Path(certs_path)\n        context.load_verify_locations(cafile=certs_path / \"ca.pem\")\n        context.load_cert_chain(\n            certfile=certs_path / \"cert.pem\", keyfile=certs_path / \"key.pem\"\n        )\n        return context", "response": "Create an SSL context object using the DOCKER_CERT_PATH environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a new node with the given attributes.", "response": "async def init(\n        self,\n        *,\n        advertise_addr: str = None,\n        listen_addr: str = \"0.0.0.0:2377\",\n        force_new_cluster: bool = False,\n        swarm_spec: Mapping = None\n    ) -> str:\n        \"\"\"\n        Initialize a new swarm.\n\n        Args:\n            ListenAddr: listen address used for inter-manager communication\n            AdvertiseAddr: address advertised to other nodes.\n            ForceNewCluster: Force creation of a new swarm.\n            SwarmSpec: User modifiable swarm configuration.\n\n        Returns:\n            id of the swarm node\n        \"\"\"\n\n        data = {\n            \"AdvertiseAddr\": advertise_addr,\n            \"ListenAddr\": listen_addr,\n            \"ForceNewCluster\": force_new_cluster,\n            \"Spec\": swarm_spec,\n        }\n\n        response = await self.docker._query_json(\"swarm/init\", method=\"POST\", data=data)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\njoins a manager node.", "response": "async def join(\n        self,\n        *,\n        remote_addrs: Iterable[str],\n        listen_addr: str = \"0.0.0.0:2377\",\n        join_token: str,\n        advertise_addr: str = None,\n        data_path_addr: str = None\n    ) -> bool:\n        \"\"\"\n        Join a swarm.\n\n        Args:\n            listen_addr\n                Used for inter-manager communication\n\n            advertise_addr\n                Externally reachable address advertised to other nodes.\n\n            data_path_addr\n                Address or interface to use for data path traffic.\n\n            remote_addrs\n                Addresses of manager nodes already participating in the swarm.\n\n            join_token\n                Secret token for joining this swarm.\n        \"\"\"\n\n        data = {\n            \"RemoteAddrs\": list(remote_addrs),\n            \"JoinToken\": join_token,\n            \"ListenAddr\": listen_addr,\n            \"AdvertiseAddr\": advertise_addr,\n            \"DataPathAddr\": data_path_addr,\n        }\n\n        await self.docker._query(\"swarm/join\", method=\"POST\", data=clean_map(data))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def leave(self, *, force: bool = False) -> bool:\n\n        params = {\"force\": force}\n\n        await self.docker._query(\"swarm/leave\", method=\"POST\", params=params)\n\n        return True", "response": "Leave a swarm.\n\n        Args:\n            force: force to leave the swarm even if the node is a master"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of services in the cluster.", "response": "async def list(self, *, filters: Mapping = None) -> List[Mapping]:\n        \"\"\"\n        Return a list of services\n\n        Args:\n            filters: a dict with a list of filters\n\n        Available filters:\n            id=<service id>\n            label=<service label>\n            mode=[\"replicated\"|\"global\"]\n            name=<service name>\n        \"\"\"\n\n        params = {\"filters\": clean_filters(filters)}\n\n        response = await self.docker._query_json(\n            \"services\", method=\"GET\", params=params\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new service in the specified registry.", "response": "async def create(\n        self,\n        task_template: Mapping[str, Any],\n        *,\n        name: str = None,\n        labels: List = None,\n        mode: Mapping = None,\n        update_config: Mapping = None,\n        rollback_config: Mapping = None,\n        networks: List = None,\n        endpoint_spec: Mapping = None,\n        auth: Optional[Union[MutableMapping, str, bytes]] = None,\n        registry: str = None\n    ) -> Mapping[str, Any]:\n        \"\"\"\n        Create a service\n\n        Args:\n            task_template: user modifiable task configuration\n            name: name of the service\n            labels: user-defined key/value metadata\n            mode: scheduling mode for the service\n            update_config: update strategy of the service\n            rollback_config: rollback strategy of the service\n            networks: array of network names/IDs to attach the service to\n            endpoint_spec: ports to expose\n            auth: authentication information, can be a string, dict or bytes\n            registry: used when auth is specified, it provides domain/IP of\n                the registry without a protocol\n\n        Returns:\n            a dict with info of the created service\n        \"\"\"\n\n        if \"Image\" not in task_template[\"ContainerSpec\"]:\n            raise KeyError(\"Missing mandatory Image key in ContainerSpec\")\n\n        if auth and registry is None:\n            raise KeyError(\n                \"When auth is specified you need to specifiy also the registry\"\n            )\n\n        # from {\"key\":\"value\"} to [\"key=value\"]\n        if \"Env\" in task_template[\"ContainerSpec\"]:\n            task_template[\"ContainerSpec\"][\"Env\"] = [\n                format_env(k, v)\n                for k, v in task_template[\"ContainerSpec\"][\"Env\"].items()\n            ]\n\n        headers = None\n        if auth:\n            headers = {\"X-Registry-Auth\": compose_auth_header(auth, registry)}\n\n        config = {\n            \"TaskTemplate\": task_template,\n            \"Name\": name,\n            \"Labels\": labels,\n            \"Mode\": mode,\n            \"UpdateConfig\": update_config,\n            \"RollbackConfig\": rollback_config,\n            \"Networks\": clean_networks(networks),\n            \"EndpointSpec\": endpoint_spec,\n        }\n\n        data = json.dumps(clean_map(config))\n\n        response = await self.docker._query_json(\n            \"services/create\", method=\"POST\", data=data, headers=headers\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating a service s metadata.", "response": "async def update(\n        self,\n        service_id: str,\n        version: str,\n        *,\n        image: str = None,\n        rollback: bool = False\n    ) -> bool:\n        \"\"\"\n        Update a service.\n        If rollback is True image will be ignored.\n\n        Args:\n            service_id: ID or name of the service.\n            version: Version of the service that you want to update.\n            rollback: Rollback the service to the previous service spec.\n\n        Returns:\n            True if successful.\n        \"\"\"\n        if image is None and rollback is False:\n            raise ValueError(\"You need to specify an image.\")\n\n        inspect_service = await self.inspect(service_id)\n        spec = inspect_service[\"Spec\"]\n\n        if image is not None:\n            spec[\"TaskTemplate\"][\"ContainerSpec\"][\"Image\"] = image\n\n        params = {\"version\": version}\n        if rollback is True:\n            params[\"rollback\"] = \"previous\"\n\n        data = json.dumps(clean_map(spec))\n\n        await self.docker._query_json(\n            \"services/{service_id}/update\".format(service_id=service_id),\n            method=\"POST\",\n            data=data,\n            params=params,\n        )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a service from the cache", "response": "async def delete(self, service_id: str) -> bool:\n        \"\"\"\n        Remove a service\n\n        Args:\n            service_id: ID or name of the service\n\n        Returns:\n            True if successful\n        \"\"\"\n\n        await self.docker._query(\n            \"services/{service_id}\".format(service_id=service_id), method=\"DELETE\"\n        )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninspect a service by ID or name", "response": "async def inspect(self, service_id: str) -> Mapping[str, Any]:\n        \"\"\"\n        Inspect a service\n\n        Args:\n            service_id: ID or name of the service\n\n        Returns:\n            a dict with info about a service\n        \"\"\"\n\n        response = await self.docker._query_json(\n            \"services/{service_id}\".format(service_id=service_id), method=\"GET\"\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def logs(\n        self,\n        service_id: str,\n        *,\n        details: bool = False,\n        follow: bool = False,\n        stdout: bool = False,\n        stderr: bool = False,\n        since: int = 0,\n        timestamps: bool = False,\n        is_tty: bool = False,\n        tail: str = \"all\"\n    ) -> Union[str, AsyncIterator[str]]:\n        \"\"\"\n        Retrieve logs of the given service\n\n        Args:\n            details: show service context and extra details provided to logs\n            follow: return the logs as a stream.\n            stdout: return logs from stdout\n            stderr: return logs from stderr\n            since: return logs since this time, as a UNIX timestamp\n            timestamps: add timestamps to every log line\n            is_tty: the service has a pseudo-TTY allocated\n            tail: only return this number of log lines\n                  from the end of the logs, specify as an integer\n                  or `all` to output all log lines.\n\n\n        \"\"\"\n        if stdout is False and stderr is False:\n            raise TypeError(\"Need one of stdout or stderr\")\n\n        params = {\n            \"details\": details,\n            \"follow\": follow,\n            \"stdout\": stdout,\n            \"stderr\": stderr,\n            \"since\": since,\n            \"timestamps\": timestamps,\n            \"tail\": tail,\n        }\n\n        response = await self.docker._query(\n            \"services/{service_id}/logs\".format(service_id=service_id),\n            method=\"GET\",\n            params=params,\n        )\n        return await multiplexed_result(response, follow, is_tty=is_tty)", "response": "Retrieve logs of a given service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore vector and JSON - serializable data in bucket with specified key.", "response": "def store_vector(self, hash_name, bucket_key, v, data):\n        \"\"\"\n        Stores vector and JSON-serializable data in bucket with specified key.\n        \"\"\"\n\n        if not hash_name in self.buckets:\n            self.buckets[hash_name] = {}\n\n        if not bucket_key in self.buckets[hash_name]:\n            self.buckets[hash_name][bucket_key] = []\n        self.buckets[hash_name][bucket_key].append((v, data))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef store_many_vectors(self, hash_name, bucket_keys, vs, data):\n        if data is None:\n            data = itertools.repeat(data)\n        for v, k, d in zip(vs, bucket_keys, data):\n            self.store_vector(hash_name, k, v, d)", "response": "Stores a batch of vectors and JSON - serializable data in bucket with specified key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes vector and JSON - serializable data in buckets with specified keys.", "response": "def delete_vector(self, hash_name, bucket_keys, data):\n        \"\"\"\n        Deletes vector and JSON-serializable data in buckets with specified keys.\n        \"\"\"\n        for key in bucket_keys:\n            bucket = self.get_bucket(hash_name, key)\n            bucket[:] = [(v, id_data) for v, id_data\n                         in bucket if id_data != data]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bucket(self, hash_name, bucket_key):\n        if hash_name in self.buckets:\n            if bucket_key in self.buckets[hash_name]:\n                return self.buckets[hash_name][bucket_key]\n        return []", "response": "Returns the content of a given bucket as list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert a new entry for a bucket_key into the tree.", "response": "def insert_entry_for_bucket(self, bucket_key, tree_depth):\n        \"\"\"\n        Increases counter for specified bucket_key (leaf of tree) and\n        also increases counters along the way from the root to the leaf.\n        \"\"\"\n\n        # First increase vector count of this subtree\n        self.vector_count = self.vector_count + 1\n\n        if tree_depth < len(bucket_key):\n            hash_char = bucket_key[tree_depth]\n\n            # This is not the leaf so continue down into the tree towards the leafes\n\n            #print 'hash_char=%c' % hash_char\n\n            # If child node for this character ('0' or '1') is not existing yet, create it\n            if not hash_char in self.childs:\n                #print 'Creating child for %c' % hash_char\n                self.childs[hash_char] = RandomBinaryProjectionTreeNode()\n\n            # Continue on the child\n            self.childs[hash_char].insert_entry_for_bucket(bucket_key, tree_depth+1)\n        else:\n            # This is a leaf, so keep the bucket key\n            #print 'Inserting leaf %s(%s, %d), count is %d' % (bucket_key, self.bucket_key, tree_depth, self.vector_count)\n            if not self.bucket_key is None:\n                if self.bucket_key != bucket_key:\n                    raise AttributeError\n            self.bucket_key = bucket_key"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collect_all_bucket_keys(self):\n        if len(self.childs) == 0:\n            # This is a leaf so just return the bucket key (we reached the bucket leaf)\n            #print 'Returning (collect) leaf bucket key %s with %d vectors' % (self.bucket_key, self.vector_count)\n            return [self.bucket_key]\n\n        # Not leaf, return results of childs\n        result = []\n        for child in self.childs.values():\n            result = result + child.collect_all_bucket_keys()\n\n        return result", "response": "Collect all bucket keys from this node and all its children."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of bucket keys based on the specified bucket key and minimum result size N.", "response": "def bucket_keys_to_guarantee_result_set_size(self, bucket_key, N, tree_depth):\n        \"\"\"\n        Returns list of bucket keys based on the specified bucket key\n        and minimum result size N.\n        \"\"\"\n\n        if tree_depth == len(bucket_key):\n            #print 'Returning leaf bucket key %s with %d vectors' % (self.bucket_key, self.vector_count)\n            # This is a leaf so just return the bucket key (we reached the bucket leaf)\n            return [self.bucket_key]\n\n        # If not leaf, this is a subtree node.\n\n        hash_char = bucket_key[tree_depth]\n        if hash_char == '0':\n            other_hash_char = '1'\n        else:\n            other_hash_char = '0'\n\n        # Check if child has enough results\n        if hash_char in self.childs:\n            if self.childs[hash_char].vector_count < N:\n                # If not combine buckets of both child subtrees\n                listA = self.childs[hash_char].collect_all_bucket_keys()\n                listB = self.childs[other_hash_char].collect_all_bucket_keys()\n                return listA + listB\n            else:\n                # Child subtree has enough results, so call method on child\n                return self.childs[hash_char].bucket_keys_to_guarantee_result_set_size(bucket_key, N, tree_depth+1)\n        else:\n            # That subtree is not existing, so just follow the other side\n            return self.childs[other_hash_char].bucket_keys_to_guarantee_result_set_size(bucket_key, N, tree_depth+1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the hash for the specified dimension.", "response": "def reset(self, dim):\n        \"\"\" Resets / Initializes the hash for the specified dimension. \"\"\"\n        if self.dim != dim:\n            self.dim = dim\n            self.normals = self.rand.randn(self.projection_count, dim)\n            self.tree_root = RandomBinaryProjectionTreeNode()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhashes the vector and returns the binary bucket key as string.", "response": "def hash_vector(self, v, querying=False):\n        \"\"\"\n        Hashes the vector and returns the binary bucket key as string.\n        \"\"\"\n        if scipy.sparse.issparse(v):\n            # If vector is sparse, make sure we have the CSR representation\n            # of the projection matrix\n            if self.normals_csr == None:\n                self.normals_csr = scipy.sparse.csr_matrix(self.normals)\n            # Make sure that we are using CSR format for multiplication\n            if not scipy.sparse.isspmatrix_csr(v):\n                v = scipy.sparse.csr_matrix(v)\n            # Project vector onto all hyperplane normals\n            projection = self.normals_csr.dot(v)\n        else:\n            # Project vector onto all hyperplane normals\n            projection = numpy.dot(self.normals, v)\n\n        # Build binary key\n        binary_key = ''.join(['1' if x > 0.0 else '0' for x in projection])\n\n        if querying:\n            #print 'Querying...'\n            # Make sure returned buckets keys contain at least N results\n            return self.tree_root.bucket_keys_to_guarantee_result_set_size(binary_key, self.minimum_result_size, 0)\n        else:\n            # We are indexing, so adapt bucket key counter in binary tree\n            self.tree_root.insert_entry_for_bucket(binary_key, 0)\n\n            # Return binary key\n            return [binary_key]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_config(self):\n        # Fill this dict with config data\n        return {\n            'hash_name': self.hash_name,\n            'dim': self.dim,\n            'projection_count': self.projection_count,\n            'normals': self.normals,\n            'tree_root': self.tree_root,\n            'minimum_result_size': self.minimum_result_size\n        }", "response": "Returns a pickle - serializable configuration struct for storage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying config to this object", "response": "def apply_config(self, config):\n        \"\"\"\n        Applies config\n        \"\"\"\n        self.hash_name = config['hash_name']\n        self.dim = config['dim']\n        self.projection_count = config['projection_count']\n        self.normals = config['normals']\n        self.tree_root = config['tree_root']\n        self.minimum_result_size = config['minimum_result_size']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring vector and JSON - serializable data in MongoDB with specified key.", "response": "def store_vector(self, hash_name, bucket_key, v, data):\n        \"\"\"\n        Stores vector and JSON-serializable data in MongoDB with specified key.\n        \"\"\"\n        mongo_key = self._format_mongo_key(hash_name, bucket_key)\n\n        val_dict = {}\n        val_dict['lsh'] = mongo_key\n\n        # Depending on type (sparse or not) fill value dict\n        if scipy.sparse.issparse(v):\n            # Make sure that we are using COO format (easy to handle)\n            if not scipy.sparse.isspmatrix_coo(v):\n                v = scipy.sparse.coo_matrix(v)\n\n            # Construct list of [index, value] items,\n            # one for each non-zero element of the sparse vector\n            encoded_values = []\n\n            for k in range(v.data.size):\n                row_index = v.row[k]\n                value = v.data[k]\n                encoded_values.append([int(row_index), value])\n\n            val_dict['sparse'] = 1\n            val_dict['nonzeros'] = encoded_values\n            val_dict['dim'] = v.shape[0]\n        else:\n            # Make sure it is a 1d vector\n            v = numpy.reshape(v, v.shape[0])\n            val_dict['vector'] = v.tostring()\n\n        val_dict['dtype'] = v.dtype.name\n\n        # Add data if set\n        if data is not None:\n            val_dict['data'] = data\n\n        # Push JSON representation of dict to end of bucket list\n        self.mongo_object.insert_one(val_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_vector(self, hash_name, bucket_keys, data):\n        lsh_keys = [self._format_mongo_key(hash_name, key)\n                    for key in bucket_keys]\n        self.mongo_object.remove({'lsh': {'$in': lsh_keys},\n                                  'data': data})", "response": "Deletes vector and JSON - serializable data in buckets with specified keys."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_hash_configuration(self, lshash):\n        self.mongo_object.insert_one(\n            {'hash_conf_name': lshash.hash_name+'_conf',\n             'hash_configuration': pickle.dumps(lshash.get_config())\n             }\n        )", "response": "Stores the hash configuration in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_hash_configuration(self, hash_name):\n        conf = self.mongo_object.find_one(\n            {'hash_conf_name': hash_name + '_conf'}\n        )\n        return pickle.loads(conf['hash_configuration']) if conf is not None\\\n            else None", "response": "Loads and returns the hash configuration for the given hash name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef numpy_array_from_list_or_numpy_array(vectors):\n    # If vectors is not a numpy matrix, create one\n    if not isinstance(vectors, numpy.ndarray):\n        V = numpy.zeros((vectors[0].shape[0], len(vectors)))\n        for index in range(len(vectors)):\n            vector = vectors[index]\n            V[:, index] = vector\n        return V\n\n    return vectors", "response": "Returns numpy array representation of argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscales a vector to unit length.", "response": "def unitvec(vec):\n    \"\"\"\n    Scale a vector to unit length. The only exception is the zero vector, which\n    is returned back unchanged.\n    \"\"\"\n    if scipy.sparse.issparse(vec): # convert scipy.sparse to standard numpy array\n        vec = vec.tocsr()\n        veclen = numpy.sqrt(numpy.sum(vec.data ** 2))\n        if veclen > 0.0:\n            return vec / veclen\n        else:\n            return vec\n\n    if isinstance(vec, numpy.ndarray):\n        vec = numpy.asarray(vec, dtype=float)\n        veclen = numpy.linalg.norm(vec)\n        if veclen > 0.0:\n            return vec / veclen\n        else:\n            return vec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef perform_pca(A):\n    # First subtract the mean\n    M = (A-numpy.mean(A.T, axis=1)).T\n    # Get eigenvectors and values of covariance matrix\n    return numpy.linalg.eig(numpy.cov(M))", "response": "Compute the eigenvalues and eigenvectors of covariance matrix of A."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npermutes the bitarray ba inplace.", "response": "def permute(self, ba):\n        \"\"\"\n        Permute the bitarray ba inplace.\n        \"\"\"\n        c = ba.copy()\n        for i in xrange(len(self.mapping)):\n            ba[i] = c[self.mapping[i]]\n        return ba"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreverts the bitarray to the original bitarray", "response": "def search_revert(self, bas, ba, beam_size):\n        \"\"\"\n        ba: query bitarray\n        bas: a sorted list of tuples of (permuted bitarray, original bitarray)\n        return : query bitarray's beam-size neighbours (unpermuted bitarray)\n        \"\"\"\n        pba = ba.copy()\n        self.permute(pba)\n        assert(beam_size % 2 == 0)\n        half_beam = beam_size / 2\n\n        # binary search (pba,ba) in bas\n        idx = bisect_left(bas, (pba, ba))\n\n        start = int(max(0, idx - half_beam))\n        end = int(min(len(bas), idx + half_beam))\n\n        res = bas[start:end]\n\n        # return the original(unpermuted) keys\n        res = [x[1] for x in res]\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstores the vector v in all matching buckets in the storage.", "response": "def store_vector(self, v, data=None):\n        \"\"\"\n        Hashes vector v and stores it in all matching buckets in the storage.\n        The data argument must be JSON-serializable. It is stored with the\n        vector and will be returned in search results.\n        \"\"\"\n        # We will store the normalized vector (used during retrieval)\n        nv = unitvec(v)\n        # Store vector in each bucket of all hashes\n        for lshash in self.lshashes:\n            for bucket_key in lshash.hash_vector(v):\n                #print 'Storying in bucket %s one vector' % bucket_key\n                self.storage.store_vector(lshash.hash_name, bucket_key,\n                                          nv, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstores a batch of vectors in all matching buckets.", "response": "def store_many_vectors(self, vs, data=None):\n        \"\"\"\n        Store a batch of vectors.\n        Hashes vector vs and stores them in all matching buckets in the storage.\n        The data argument must be either None or a list of JSON-serializable\n        object. It is stored with the vector and will be returned in search\n        results.\n        \"\"\"\n        # We will store the normalized vector (used during retrieval)\n        nvs = [unitvec(i) for i in vs]\n        # Store vector in each bucket of all hashes\n        for lshash in self.lshashes:\n            bucket_keys = [lshash.hash_vector(i)[0] for i in vs]\n            self.storage.store_many_vectors(lshash.hash_name, bucket_keys,\n                                            nvs, data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting vector v and his id in all matching buckets in the storage.", "response": "def delete_vector(self, data, v=None):\n        \"\"\"\n        Deletes vector v and his id (data) in all matching buckets in the storage.\n        The data argument must be JSON-serializable.\n        \"\"\"\n\n        # Delete data id in each hashes\n        for lshash in self.lshashes:\n            if v is None:\n                keys = self.storage.get_all_bucket_keys(lshash.hash_name)\n            else:\n                keys = lshash.hash_vector(v)\n            self.storage.delete_vector(lshash.hash_name, keys, data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef neighbours(self, v,\n                   distance=None,\n                   fetch_vector_filters=None,\n                   vector_filters=None):\n        \"\"\"\n        Hashes vector v, collects all candidate vectors from the matching\n        buckets in storage, applys the (optional) distance function and\n        finally the (optional) filter function to construct the returned list\n        of either (vector, data, distance) tuples or (vector, data) tuples.\n        \"\"\"\n\n        # Collect candidates from all buckets from all hashes\n        candidates = self._get_candidates(v)\n        # print 'Candidate count is %d' % len(candidates)\n\n        # Apply fetch vector filters if specified and return filtered list\n        if fetch_vector_filters:\n            candidates = self._apply_filter(fetch_vector_filters,\n                                            candidates)\n\n        # Apply distance implementation if specified\n        if not distance:\n            distance = self.distance\n        candidates = self._append_distances(v, distance, candidates)\n\n        # Apply vector filters if specified and return filtered list\n        if not vector_filters:\n            vector_filters = self.vector_filters\n        candidates = self._apply_filter(vector_filters, candidates)\n\n        # If there is no vector filter, just return list of candidates\n        return candidates", "response": "Returns a list of all candidates that are neighbours of the specified hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncollecting candidates from all buckets from all hashes", "response": "def _get_candidates(self, v):\n        \"\"\" Collect candidates from all buckets from all hashes \"\"\"\n        candidates = []\n        for lshash in self.lshashes:\n            for bucket_key in lshash.hash_vector(v, querying=True):\n                bucket_content = self.storage.get_bucket(\n                    lshash.hash_name,\n                    bucket_key,\n                )\n                #print 'Bucket %s size %d' % (bucket_key, len(bucket_content))\n                candidates.extend(bucket_content)\n        return candidates"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _apply_filter(self, filters, candidates):\n        if filters:\n            filter_input = candidates\n            for fetch_vector_filter in filters:\n                filter_input = fetch_vector_filter.filter_vectors(filter_input)\n\n            return filter_input\n        else:\n            return candidates", "response": "Apply vector filters if specified and return filtered list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _append_distances(self, v, distance, candidates):\n        if distance:\n            # Normalize vector (stored vectors are normalized)\n            nv = unitvec(v)\n            candidates = [(x[0], x[1], self.distance.distance(x[0], nv)) for x\n                            in candidates]\n\n        return candidates", "response": "Append distances to the list of candidates."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute distance measure between vectors x and y. Returns float.", "response": "def distance(self, x, y):\n        \"\"\"\n        Computes distance measure between vectors x and y. Returns float.\n        \"\"\"\n\n        if scipy.sparse.issparse(x):\n            x = x.toarray().ravel()\n            y = y.toarray().ravel()\n        return 1.0 - numpy.dot(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms nearest neighbour recall experiments with custom vector data", "response": "def perform_experiment(self, engine_list):\n        \"\"\"\n        Performs nearest neighbour recall experiments with custom vector data\n        for all engines in the specified list.\n\n        Returns self.result contains list of (recall, precision, search_time)\n        tuple. All are the averaged values over all request vectors.\n        search_time is the average retrieval/search time compared to the\n        average exact search time.\n        \"\"\"\n        # We will fill this array with measures for all the engines.\n        result = []\n\n        # For each engine, first index vectors and then retrieve neighbours\n        for endine_idx, engine in enumerate(engine_list):\n            print('Engine %d / %d' % (endine_idx, len(engine_list)))\n\n            # Clean storage\n            engine.clean_all_buckets()\n            # Use this to compute average recall\n            avg_recall = 0.0\n            # Use this to compute average precision\n            avg_precision = 0.0\n            # Use this to compute average search time\n            avg_search_time = 0.0\n\n            # Index all vectors and store them\n            for index, v in enumerate(self.vectors):\n                engine.store_vector(v, 'data_%d' % index)\n\n            # Look for N nearest neighbours for query vectors\n            for index in self.query_indices:\n                # Get indices of the real nearest as set\n                real_nearest = set(self.closest[index])\n\n                # We have to time the search\n                search_time_start = time.time()\n\n                # Get nearest N according to engine\n                nearest = engine.neighbours(self.vectors[index])\n\n                # Get search time\n                search_time = time.time() - search_time_start\n\n                # For comparance we need their indices (as set)\n                nearest = set([self.__index_of_vector(x[0]) for x in nearest])\n\n                # Remove query index from search result to make sure that\n                # recall and precision make sense in terms of \"neighbours\".\n                # If ONLY the query vector is retrieved, we want recall to be\n                # zero!\n                nearest.remove(index)\n\n                # If the result list is empty, recall and precision are 0.0\n                if len(nearest) == 0:\n                    recall = 0.0\n                    precision = 0.0\n                else:\n                    # Get intersection count\n                    inter_count = float(len(real_nearest & nearest))\n\n                    # Normalize recall for this vector\n                    recall = inter_count/float(len(real_nearest))\n\n                    # Normalize precision for this vector\n                    precision = inter_count/float(len(nearest))\n\n                # Add to accumulator\n                avg_recall += recall\n\n                # Add to accumulator\n                avg_precision += precision\n\n                # Add to accumulator\n                avg_search_time += search_time\n\n            # Normalize recall over query set\n            avg_recall /= float(len(self.query_indices))\n\n            # Normalize precision over query set\n            avg_precision /= float(len(self.query_indices))\n\n            # Normalize search time over query set\n            avg_search_time = avg_search_time / float(len(self.query_indices))\n\n            # Normalize search time with respect to exact search\n            avg_search_time /= self.exact_search_time_per_vector\n\n            print('  recall=%f, precision=%f, time=%f' % (avg_recall,\n                                                          avg_precision,\n                                                          avg_search_time))\n\n            result.append((avg_recall, avg_precision, avg_search_time))\n\n        # Return (recall, precision, search_time) tuple\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __vector_to_string(self, vector):\n        return numpy.array_str(numpy.round(unitvec(vector), decimals=3))", "response": "Returns string representation of vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning subset of specified input list.", "response": "def filter_vectors(self, input_list):\n        \"\"\"\n        Returns subset of specified input list.\n        \"\"\"\n        unique_dict = {}\n        for v in input_list:\n            unique_dict[v[1]] = v\n        return list(unique_dict.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the hash for the specified dimension.", "response": "def reset(self, dim):\n        \"\"\" Resets / Initializes the hash for the specified dimension. \"\"\"\n        self.dim = dim\n        # Reset all child hashes\n        for child_hash in self.child_hashes:\n            child_hash['hash'].reset(dim)\n            child_hash['bucket_keys'] = {}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hash_vector(self, v, querying=False):\n\n        bucket_keys = []\n\n        if querying:\n            # If we are querying, use the permuted indexes to get bucket keys\n            for child_hash in self.child_hashes:\n                lshash = child_hash['hash']\n                # Make sure the permuted index for this hash is existing\n                if not lshash.hash_name in self.permutation.permutedIndexs:\n                    raise AttributeError('Permuted index is not existing for hash with name %s' % lshash.hash_name)\n\n                # Get regular bucket keys from hash\n                for bucket_key in lshash.hash_vector(v, querying):\n                    #print 'Regular bucket key %s' % bucket_key\n                    # Get neighbour keys from permuted index\n                    neighbour_keys = self.permutation.get_neighbour_keys(lshash.hash_name,bucket_key)\n                    # Add them to result, but prefix with hash name\n                    for n in neighbour_keys:\n                        bucket_keys.append(lshash.hash_name+'_'+n)\n\n        else:\n            # If we are indexing (storing) just use child hashes without permuted index\n            for child_hash in self.child_hashes:\n                lshash = child_hash['hash']\n\n                # Get regular bucket keys from hash\n                for bucket_key in lshash.hash_vector(v, querying):\n                    # Register bucket key in child hash dict\n                    child_hash['bucket_keys'][bucket_key] = bucket_key\n                    # Append bucket key to result prefixed with child hash name\n                    bucket_keys.append(lshash.hash_name+'_'+bucket_key)\n\n        # Return all the bucket keys\n        return bucket_keys", "response": "Hashes the vector and returns the bucket key as string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild the PermutedIndex for all your binary hashings.", "response": "def build_permuted_index(self):\n        \"\"\"\n        Build PermutedIndex for all your binary hashings.\n        PermutedIndex would be used to find the neighbour bucket key\n        in terms of Hamming distance. Permute_configs is nested dict\n        in the following format:\n        permuted_config = {\"<hash_name>\":\n                           { \"num_permutation\":50,\n                             \"beam_size\":10,\n                             \"num_neighbour\":100 }\n                          }\n        \"\"\"\n\n        for child_hash in self.child_hashes:\n            # Get config values for child hash\n            config = child_hash['config']\n            num_permutation = config['num_permutation']\n            beam_size = config['beam_size']\n            num_neighbour = config['num_neighbour']\n\n            # Get used buckets keys for child hash\n            bucket_keys = child_hash['bucket_keys'].keys()\n\n            # Get actual child hash\n            lshash = child_hash['hash']\n\n            # Compute permuted index for this hash\n            self.permutation.build_permuted_index(lshash,bucket_keys,num_permutation,beam_size,num_neighbour)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the Manhattan distance between vectors x and y. Returns float.", "response": "def distance(self, x, y):\n        \"\"\"\n        Computes the Manhattan distance between vectors x and y. Returns float.\n        \"\"\"\n        if scipy.sparse.issparse(x):\n            return numpy.sum(numpy.absolute((x-y).toarray().ravel()))\n        else:\n            return numpy.sum(numpy.absolute(x-y))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute distance measure between vectors x and y. Returns float.", "response": "def distance(self, x, y):\n        \"\"\"\n        Computes distance measure between vectors x and y. Returns float.\n        \"\"\"\n        if scipy.sparse.issparse(x):\n            return numpy.linalg.norm((x-y).toarray().ravel())\n        else:\n            return numpy.linalg.norm(x-y)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring vector and JSON - serializable data in bucket with specified key.", "response": "def store_vector(self, hash_name, bucket_key, v, data):\n        \"\"\"\n        Stores vector and JSON-serializable data in bucket with specified key.\n        \"\"\"\n        self._add_vector(hash_name, bucket_key, v, data, self.redis_object)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef store_many_vectors(self, hash_name, bucket_keys, vs, data):\n        with self.redis_object.pipeline() as pipeline:\n            if data is None:\n                data = [None] * len(vs)\n            for bucket_key, data, v in zip(bucket_keys, data, vs):\n                self._add_vector(hash_name, bucket_key, v, data, pipeline)\n            pipeline.execute()", "response": "Stores a batch of vectors in Redis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_vector(self, hash_name, bucket_key, v, data, redis_object):\n        '''\n        Store vector and JSON-serializable data in bucket with specified key.\n        '''\n        redis_key = self._format_redis_key(hash_name, bucket_key)\n\n        val_dict = {}\n\n        # Depending on type (sparse or not) fill value dict\n        if scipy.sparse.issparse(v):\n            # Make sure that we are using COO format (easy to handle)\n            if not scipy.sparse.isspmatrix_coo(v):\n                v = scipy.sparse.coo_matrix(v)\n\n            # Construct list of [index, value] items,\n            # one for each non-zero element of the sparse vector\n            encoded_values = []\n\n            for k in range(v.data.size):\n                row_index = v.row[k]\n                value = v.data[k]\n                encoded_values.append([int(row_index), value])\n\n            val_dict['sparse'] = 1\n            val_dict['nonzeros'] = encoded_values\n            val_dict['dim'] = v.shape[0]\n        else:\n            # Make sure it is a 1d vector\n            v = numpy.reshape(v, v.shape[0])\n            val_dict['vector'] = v.tostring()\n\n        val_dict['dtype'] = v.dtype.name\n\n        # Add data if set\n        if data is not None:\n            val_dict['data'] = data\n\n        # Push JSON representation of dict to end of bucket list\n        self.redis_object.rpush(redis_key, pickle.dumps(val_dict, protocol=2))", "response": "Add vector and JSON - serializable data to the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_vector(self, hash_name, bucket_keys, data):\n        with self.redis_object.pipeline() as pipeline:\n            for key in bucket_keys:\n                redis_key = self._format_redis_key(hash_name, key)\n                rows = [(row, pickle.loads(row).get('data'))\n                        for row in self._get_bucket_rows(hash_name, key)]\n                for _, id_data in rows:\n                    if id_data == data:\n                        break\n                else:\n                    # Deleted data is not present in this bucket\n                    continue\n                pipeline.delete(redis_key)\n                pipeline.rpush(redis_key, *(row for row, id_data in rows\n                                            if id_data != data))\n            pipeline.execute()", "response": "Deletes vector and JSON - serializable data in buckets with specified keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_bucket(self, hash_name, bucket_key):\n        results = []\n        for row in self._get_bucket_rows(hash_name, bucket_key):\n            val_dict = pickle.loads(row)\n            # Depending on type (sparse or not) reconstruct vector\n            if 'sparse' in val_dict:\n\n                # Fill these for COO creation\n                row = []\n                col = []\n                data = []\n\n                # For each non-zero element, append values\n                for e in val_dict['nonzeros']:\n                    row.append(e[0])  # Row index\n                    data.append(e[1])  # Value\n                    col.append(0)  # Column index (always 0)\n\n                # Create numpy arrays for COO creation\n                coo_row = numpy.array(row, dtype=numpy.int32)\n                coo_col = numpy.array(col, dtype=numpy.int32)\n                coo_data = numpy.array(data)\n\n                # Create COO sparse vector\n                vector = scipy.sparse.coo_matrix((coo_data, (coo_row, coo_col)), shape=(val_dict['dim'], 1))\n\n            else:\n                vector = numpy.fromstring(val_dict['vector'],\n                                          dtype=val_dict['dtype'])\n\n            # Add data to result tuple, if present\n            results.append((vector, val_dict.get('data')))\n\n        return results", "response": "Returns the content of a bucket as list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_buckets(self, hash_name):\n        bucket_keys = self._iter_bucket_keys(hash_name)\n        self.redis_object.delete(*bucket_keys)", "response": "Removes all buckets and their content for the specified hash."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean_all_buckets(self):\n        bucket_keys = self.redis_object.keys(pattern='nearpy_*')\n        if len(bucket_keys) > 0:\n            self.redis_object.delete(*bucket_keys)", "response": "Removes all buckets from all hashes and their content."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstore the hash configuration in Redis", "response": "def store_hash_configuration(self, lshash):\n        \"\"\"\n        Stores hash configuration\n        \"\"\"\n        self.redis_object.set(lshash.hash_name+'_conf', pickle.dumps(lshash.get_config()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading and returns the hash configuration for the given hash name.", "response": "def load_hash_configuration(self, hash_name):\n        \"\"\"\n        Loads and returns hash configuration\n        \"\"\"\n        conf = self.redis_object.get(hash_name+'_conf')\n\n        return pickle.loads(conf) if conf is not None else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhashing the vector and returns the bucket key as string.", "response": "def hash_vector(self, v, querying=False):\n        \"\"\"\n        Hashes the vector and returns the bucket key as string.\n        \"\"\"\n\n        bucket_keys = []\n\n        if querying:\n            # If we are querying, use the bucket key map\n            for lshash in self.child_hashes:\n                # Get regular bucket keys from hash\n                for bucket_key in lshash.hash_vector(v, querying):\n                    prefixed_key = lshash.hash_name+'_'+bucket_key\n                    # Get entries from map (bucket keys with hamming distance of 1)\n                    if prefixed_key in self.bucket_key_map:\n                        bucket_keys.extend(self.bucket_key_map[prefixed_key].keys())\n        else:\n            # If we are indexing (storing) just use child hashes without permuted index\n            for lshash in self.child_hashes:\n                # Get regular bucket keys from hash\n                for bucket_key in lshash.hash_vector(v, querying):\n                    # Get permuted keys\n                    perm_keys = self.permuted_keys(bucket_key)\n                    # Put extact hit key into list\n                    perm_keys.append(bucket_key)\n\n                    # Append key for storage (not the permutations)\n                    bucket_keys.append(lshash.hash_name+'_'+bucket_key)\n\n                    # For every permutation register all the variants\n                    for perm_key in perm_keys:\n                        prefixed_key = lshash.hash_name+'_'+perm_key\n\n                        # Make sure dictionary exists\n                        if not prefixed_key in self.bucket_key_map:\n                            self.bucket_key_map[prefixed_key] = {}\n\n                        for variant in perm_keys:\n                            prefixed_variant = lshash.hash_name+'_'+variant\n                            self.bucket_key_map[prefixed_key][prefixed_variant] = 1\n\n        # Return all the bucket keys\n        return bucket_keys"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_config(self, config):\n        self.hash_name = config['hash_name']\n        self.dim = config['dim']\n        self.bucket_key_map = config['bucket_key_map']", "response": "Applies config to this object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd specified child hash to the internal list of child hashes.", "response": "def add_child_hash(self, child_hash):\n        \"\"\"\n        Adds specified child hash.\n\n        The hash must be one of the binary types.\n        \"\"\"\n\n        # Hash must generate binary keys\n        if not (isinstance(child_hash,PCABinaryProjections) or isinstance(child_hash,RandomBinaryProjections) or isinstance(child_hash,RandomBinaryProjectionTree)):\n            raise ValueError('Child hashes must generate binary keys')\n\n        # Add both hash and config to array of child hashes. Also we are going to\n        # accumulate used bucket keys for every hash in order to build the permuted index\n        self.child_hashes.append(child_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_neighbour_keys(self, bucket_key, k):\n        # convert query_key into bitarray\n        query_key = bitarray(bucket_key)\n\n        topk = set()\n        for i in xrange(len(self.permutes)):\n            p = self.permutes[i]\n            plist = self.permuted_lists[i]\n            candidates = p.search_revert(plist, query_key, self.beam_size)\n            topk = topk.union(set(candidates))\n        topk = list(topk)\n\n        # sort the topk neighbour keys according to the Hamming distance to qurey key\n        topk = sorted(topk, key=lambda x: self.hamming_distance(x, query_key))\n        # return the top k items\n        topk_bin = [x.to01() for x in topk[:k]]\n        return topk_bin", "response": "Get the neighbour keys for a given bucket key and key k."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_vectors(self, input_list):\n        try:\n            # Return filtered (vector, data, distance )tuple list. Will fail\n            # if input is list of (vector, data) tuples.\n            return [x for x in input_list if x[2] < self.distance_threshold]\n        except:\n            # Otherwise just return input list\n            return input_list", "response": "Filter the list of vectors based on distance threshold."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef perform_experiment(self, engine_list):\n        # We will fill this array with measures for all the engines.\n        result = []\n\n        # For each engine, first index vectors and then retrieve neighbours\n        for engine in engine_list:\n            print('Engine %d / %d' % (engine_list.index(engine),\n                                      len(engine_list)))\n\n            # Clean storage\n            engine.clean_all_buckets()\n            # Use this to compute average distance_ratio\n            avg_distance_ratio = 0.0\n            # Use this to compute average result set size\n            avg_result_size = 0.0\n            # Use this to compute average search time\n            avg_search_time = 0.0\n\n            # Index all vectors and store them\n            for index in range(self.vectors.shape[1]):\n                engine.store_vector(self.vectors[:, index],\n                                    'data_%d' % index)\n\n            # Look for N nearest neighbours for query vectors\n            for index in self.query_indices:\n                # We have to time the search\n                search_time_start = time.time()\n\n                # Get nearest N according to engine\n                nearest = engine.neighbours(self.vectors[:, index])\n\n                # Get search time\n                search_time = time.time() - search_time_start\n\n                # Get average distance ratio (with respect to radius\n                # of real N closest neighbours)\n                distance_ratio = 0.0\n                for n in nearest:\n                    # If the vector is outside the real neighbour radius\n                    if n[2] > self.nearest_radius[index]:\n                        # Compute distance to real neighbour radius\n                        d = (n[2] - self.nearest_radius[index])\n                        # And normalize it. 1.0 means: distance to\n                        # real neighbour radius is identical to radius\n                        d /= self.nearest_radius[index]\n                        # If all neighbours are in the radius, the\n                        # distance ratio is 0.0\n                        distance_ratio += d\n                # Normalize distance ratio over all neighbours\n                distance_ratio /= len(nearest)\n\n                # Add to accumulator\n                avg_distance_ratio += distance_ratio\n\n                # Add to accumulator\n                avg_result_size += len(nearest)\n\n                # Add to accumulator\n                avg_search_time += search_time\n\n            # Normalize distance ratio over query set\n            avg_distance_ratio /= float(len(self.query_indices))\n\n            # Normalize avg result size\n            avg_result_size /= float(len(self.query_indices))\n\n            # Normalize search time over query set\n            avg_search_time = avg_search_time / float(len(self.query_indices))\n\n            # Normalize search time with respect to exact search\n            avg_search_time /= self.exact_search_time_per_vector\n\n            print('  distance_ratio=%f, result_size=%f, time=%f' % (avg_distance_ratio,\n                                                                    avg_result_size,\n                                                                    avg_search_time))\n\n            result.append((avg_distance_ratio, avg_result_size, avg_search_time))\n\n        return result", "response": "Perform nearest neighbour experiments with custom vector data\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset(self, dim):\n        if self.dim != dim:\n            self.dim = dim\n            self.normals = self.rand.randn(self.projection_count, dim)", "response": "Resets the hash for the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hash_vector(self, v, querying=False):\n        if scipy.sparse.issparse(v):\n            # If vector is sparse, make sure we have the CSR representation\n            # of the projection matrix\n            if self.normals_csr == None:\n                self.normals_csr = scipy.sparse.csr_matrix(self.normals)\n            # Make sure that we are using CSR format for multiplication\n            if not scipy.sparse.isspmatrix_csr(v):\n                v = scipy.sparse.csr_matrix(v)\n            # Project vector onto all hyperplane normals\n            projection = self.normals_csr.dot(v)\n        else:\n            # Project vector onto all hyperplane normals\n            projection = numpy.dot(self.normals, v)\n        # Return binary key\n        return [''.join(['1' if x > 0.0 else '0' for x in projection])]", "response": "Hashes the vector and returns the binary bucket key as string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_vectors(self, input_list):\n        try:\n            # Return filtered (vector, data, distance )tuple list. Will fail\n            # if input is list of (vector, data) tuples.\n            sorted_list = sorted(input_list, key=lambda x: x[2])\n            return sorted_list[:self.N]\n        except:\n            # Otherwise just return input list\n            return input_list", "response": "Filter the input list of the specified number of elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhash the vector and returns the binary bucket key as string.", "response": "def hash_vector(self, v, querying=False):\n        \"\"\"\n        Hashes the vector and returns the binary bucket key as string.\n        \"\"\"\n        if scipy.sparse.issparse(v):\n            # If vector is sparse, make sure we have the CSR representation\n            # of the projection matrix\n            if self.components_csr == None:\n                self.components_csr = scipy.sparse.csr_matrix(self.components)\n            # Make sure that we are using CSR format for multiplication\n            if not scipy.sparse.isspmatrix_csr(v):\n                v = scipy.sparse.csr_matrix(v)\n            # Project vector onto all hyperplane normals\n            projection = (self.components_csr.dot(v) / self.bin_width).floor().toarray()\n        else:\n            # Project vector onto components\n            projection = numpy.dot(self.components, v)\n            projection = numpy.floor(projection / self.bin_width)\n        # Return key\n        return ['_'.join([str(int(x)) for x in projection])]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_config(self):\n        # Fill this dict with config data\n        return {\n            'hash_name': self.hash_name,\n            'dim': self.dim,\n            'bin_width': self.bin_width,\n            'projection_count': self.projection_count,\n            'components': self.components\n        }", "response": "Returns a dict with the configuration data for storage."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply config to the current object", "response": "def apply_config(self, config):\n        \"\"\"\n        Applies config\n        \"\"\"\n        self.hash_name = config['hash_name']\n        self.dim = config['dim']\n        self.bin_width = config['bin_width']\n        self.projection_count = config['projection_count']\n        self.components = config['components']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(self):\n        # Fill this dict with config data\n        return {\n            'hash_name': self.hash_name,\n            'dim': self.dim,\n            'bin_width': self.bin_width,\n            'projection_count': self.projection_count,\n            'normals': self.normals\n        }", "response": "Returns a dict with the configuration data for the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies config to the current object", "response": "def apply_config(self, config):\n        \"\"\"\n        Applies config\n        \"\"\"\n        self.hash_name = config['hash_name']\n        self.dim = config['dim']\n        self.bin_width = config['bin_width']\n        self.projection_count = config['projection_count']\n        self.normals = config['normals']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a PermutedIndex object and stores it into the self. permutedIndexs.", "response": "def build_permuted_index(\n            self,\n            lshash,\n            buckets,\n            num_permutation,\n            beam_size,\n            num_neighbour):\n        \"\"\"\n        Build a permutedIndex and store it into the dict self.permutedIndexs.\n        lshash: the binary lshash object (nearpy.hashes.lshash).\n        buckets: the buckets object corresponding to lshash. It's a dict object \n                 which can get from nearpy.storage.buckets[lshash.hash_name]\n        num_permutation: the number of sorted randomly-permuted bucket key lists (SRPBKL).\n        beam_size: beam size, details please refer to __init__() in nearpy.hashes.permutation.PermutedIndex \n        num_neighbour: the number of neighbour bucket keys needed to return in self.get_neighbour_keys().\n        \"\"\"\n        # Init a PermutedIndex\n        pi = PermutedIndex(\n            lshash,\n            buckets,\n            num_permutation,\n            beam_size,\n            num_neighbour)\n        # get hash_name\n        hash_name = lshash.hash_name\n        self.permutedIndexs[hash_name] = pi"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the neighbour buckets given hash_name and query bucket key.", "response": "def get_neighbour_keys(self, hash_name, bucket_key):\n        \"\"\"\n        Return the neighbour buckets given hash_name and query bucket key.\n        \"\"\"\n        # get the permutedIndex given hash_name\n        permutedIndex = self.permutedIndexs[hash_name]\n        # return neighbour bucket keys of query bucket key\n        return permutedIndex.get_neighbour_keys(\n            bucket_key,\n            permutedIndex.num_neighbour)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_rc_file(editor, rc_file):\n    assert isinstance(editor, Editor)\n    assert isinstance(rc_file, six.string_types)\n\n    # Expand tildes.\n    rc_file = os.path.expanduser(rc_file)\n\n    # Check whether this file exists.\n    if not os.path.exists(rc_file):\n        print('Impossible to read %r' % rc_file)\n        _press_enter_to_continue()\n        return\n\n    # Run the rc file in an empty namespace.\n    try:\n        namespace = {}\n\n        with open(rc_file, 'r') as f:\n            code = compile(f.read(), rc_file, 'exec')\n            six.exec_(code, namespace, namespace)\n\n        # Now we should have a 'configure' method in this namespace. We call this\n        # method with editor as an argument.\n        if 'configure' in namespace:\n            namespace['configure'](editor)\n\n    except Exception as e:\n        # Handle possible exceptions in rc file.\n        traceback.print_exc()\n        _press_enter_to_continue()", "response": "Run the rc file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to decode a character using sys. stdout if it can t be encoded using sys. stdout else return the backup character.", "response": "def _try_char(character, backup, encoding=sys.stdout.encoding):\n    \"\"\"\n    Return `character` if it can be encoded using sys.stdout, else return the\n    backup character.\n    \"\"\"\n    if character.encode(encoding, 'replace') == b'?':\n        return backup\n    else:\n        return character"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that returns True when the buffer list overlay should be displayed.", "response": "def _bufferlist_overlay_visible(editor):\n    \"\"\"\n    True when the buffer list overlay should be displayed.\n    (This is when someone starts typing ':b' or ':buffer' in the command line.)\n    \"\"\"\n    @Condition\n    def overlay_is_visible():\n        app = get_app()\n\n        text = editor.command_buffer.text.lstrip()\n        return app.layout.has_focus(editor.command_buffer) and (\n                any(text.startswith(p) for p in ['b ', 'b! ', 'buffer', 'buffer!']))\n    return overlay_is_visible"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self):\n        # Start with an empty frames list everytime, to avoid memory leaks.\n        existing_frames = self._frames\n        self._frames = {}\n\n        def create_layout_from_node(node):\n            if isinstance(node, window_arrangement.Window):\n                # Create frame for Window, or reuse it, if we had one already.\n                key = (node, node.editor_buffer)\n                frame = existing_frames.get(key)\n                if frame is None:\n                    frame, pt_window = self._create_window_frame(node.editor_buffer)\n\n                    # Link layout Window to arrangement.\n                    node.pt_window = pt_window\n\n                self._frames[key] = frame\n                return frame\n\n            elif isinstance(node, window_arrangement.VSplit):\n                return VSplit(\n                    [create_layout_from_node(n) for n in node],\n                    padding=1,\n                    padding_char=self.get_vertical_border_char(),\n                    padding_style='class:frameborder')\n\n            if isinstance(node, window_arrangement.HSplit):\n                return HSplit([create_layout_from_node(n) for n in node])\n\n        layout = create_layout_from_node(self.window_arrangement.active_tab.root)\n        self._fc.content = layout", "response": "Update the layout to match the layout as described in the WindowArrangement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Window for the buffer.", "response": "def _create_window_frame(self, editor_buffer):\n        \"\"\"\n        Create a Window for the buffer, with underneat a status bar.\n        \"\"\"\n        @Condition\n        def wrap_lines():\n            return self.editor.wrap_lines\n\n        window = Window(\n            self._create_buffer_control(editor_buffer),\n            allow_scroll_beyond_bottom=True,\n            scroll_offsets=ScrollOffsets(\n                left=0, right=0,\n                top=(lambda: self.editor.scroll_offset),\n                bottom=(lambda: self.editor.scroll_offset)),\n            wrap_lines=wrap_lines,\n            left_margins=[ConditionalMargin(\n                    margin=NumberedMargin(\n                        display_tildes=True,\n                        relative=Condition(lambda: self.editor.relative_number)),\n                    filter=Condition(lambda: self.editor.show_line_numbers))],\n            cursorline=Condition(lambda: self.editor.cursorline),\n            cursorcolumn=Condition(lambda: self.editor.cursorcolumn),\n            colorcolumns=(\n                lambda: [ColorColumn(pos) for pos in self.editor.colorcolumn]),\n            ignore_content_width=True,\n            ignore_content_height=True,\n            get_line_prefix=partial(self._get_line_prefix, editor_buffer.buffer))\n\n        return HSplit([\n            window,\n            VSplit([\n                WindowStatusBar(self.editor, editor_buffer),\n                WindowStatusBarRuler(self.editor, window, editor_buffer.buffer),\n            ], width=Dimension()),  # Ignore actual status bar width.\n        ]), window"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new BufferControl for a given location.", "response": "def _create_buffer_control(self, editor_buffer):\n        \"\"\"\n        Create a new BufferControl for a given location.\n        \"\"\"\n        @Condition\n        def preview_search():\n            return self.editor.incsearch\n\n        input_processors = [\n            # Processor for visualising spaces. (should come before the\n            # selection processor, otherwise, we won't see these spaces\n            # selected.)\n            ConditionalProcessor(\n                ShowTrailingWhiteSpaceProcessor(),\n                Condition(lambda: self.editor.display_unprintable_characters)),\n\n            # Replace tabs by spaces.\n            TabsProcessor(\n                tabstop=(lambda: self.editor.tabstop),\n                char1=(lambda: '|' if self.editor.display_unprintable_characters else ' '),\n                char2=(lambda: _try_char('\\u2508', '.', get_app().output.encoding())\n                                       if self.editor.display_unprintable_characters else ' '),\n            ),\n\n            # Reporting of errors, for Pyflakes.\n            ReportingProcessor(editor_buffer),\n            HighlightSelectionProcessor(),\n            ConditionalProcessor(\n                HighlightSearchProcessor(),\n                Condition(lambda: self.editor.highlight_search)),\n            ConditionalProcessor(\n                HighlightIncrementalSearchProcessor(),\n                Condition(lambda: self.editor.highlight_search) & preview_search),\n            HighlightMatchingBracketProcessor(),\n            DisplayMultipleCursors(),\n        ]\n\n        return BufferControl(\n            lexer=DocumentLexer(editor_buffer),\n            include_default_input_processors=False,\n            input_processors=input_processors,\n            buffer=editor_buffer.buffer,\n            preview_search=preview_search,\n            search_buffer_control=self.search_control,\n            focus_on_click=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_command(editor, input_string):\n    # Match with grammar and extract variables.\n    m = COMMAND_GRAMMAR.match(input_string)\n    if m is None:\n        return\n\n    variables = m.variables()\n    command = variables.get('command')\n    go_to_line = variables.get('go_to_line')\n    shell_command = variables.get('shell_command')\n\n    # Call command handler.\n\n    if go_to_line is not None:\n        # Handle go-to-line.\n        _go_to_line(editor, go_to_line)\n\n    elif shell_command is not None:\n        # Handle shell commands.\n        editor.application.run_system_command(shell_command)\n\n    elif has_command_handler(command):\n        # Handle other 'normal' commands.\n        call_command_handler(command, editor, variables)\n    else:\n        # For unknown commands, show error message.\n        editor.show_message('Not an editor command: %s' % input_string)\n        return\n\n    # After execution of commands, make sure to update the layout and focus\n    # stack.\n    editor.sync_with_prompt_toolkit()", "response": "Handles the given command line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove cursor to the given line in the current buffer.", "response": "def _go_to_line(editor, line):\n    \"\"\"\n    Move cursor to this line in the current buffer.\n    \"\"\"\n    b = editor.application.current_buffer\n    b.cursor_position = b.document.translate_row_col_to_index(max(0, int(line) - 1), 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef report(location, document):\n    assert isinstance(location, six.string_types)\n\n    if location.endswith('.py'):\n        return report_pyflakes(document)\n    else:\n        return []", "response": "Run reporter on document and return list of ReporterError instances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun pyflakes on input and return list of ReporterError instances.", "response": "def report_pyflakes(document):\n    \"\"\"\n    Run pyflakes on document and return list of ReporterError instances.\n    \"\"\"\n    # Run pyflakes on input.\n    reporter = _FlakesReporter()\n    pyflakes.api.check(document.text, '', reporter=reporter)\n\n    def format_flake_message(message):\n        return [\n            ('class:flakemessage.prefix', 'pyflakes:'),\n            ('', ' '),\n            ('class:flakemessage', message.message % message.message_args)\n        ]\n\n    def message_to_reporter_error(message):\n        \"\"\" Turn pyflakes message into ReporterError. \"\"\"\n        start_index = document.translate_row_col_to_index(message.lineno - 1, message.col)\n        end_index = start_index\n        while end_index < len(document.text) and document.text[end_index] in WORD_CHARACTERS:\n            end_index += 1\n\n        return ReporterError(lineno=message.lineno - 1,\n                             start_column=message.col,\n                             end_column=message.col + end_index - start_index,\n                             formatted_text=format_flake_message(message))\n\n    # Construct list of ReporterError instances.\n    return [message_to_reporter_error(m) for m in reporter.messages]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_initial_files(self, locations, in_tab_pages=False, hsplit=False, vsplit=False):\n        assert in_tab_pages + hsplit + vsplit <= 1  # Max one of these options.\n\n        # When no files were given, open at least one empty buffer.\n        locations2 = locations or [None]\n\n        # First file\n        self.window_arrangement.open_buffer(locations2[0])\n\n        for f in locations2[1:]:\n            if in_tab_pages:\n                self.window_arrangement.create_tab(f)\n            elif hsplit:\n                self.window_arrangement.hsplit(location=f)\n            elif vsplit:\n                self.window_arrangement.vsplit(location=f)\n            else:\n                self.window_arrangement.open_buffer(f)\n\n        self.window_arrangement.active_tab_index = 0\n\n        if locations and len(locations) > 1:\n            self.show_message('%i files loaded.' % len(locations))", "response": "Load a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the EditorBuffer that is currently active.", "response": "def current_editor_buffer(self):\n        \"\"\"\n        Return the `EditorBuffer` that is currently active.\n        \"\"\"\n        current_buffer = self.application.current_buffer\n\n        # Find/return the EditorBuffer with this name.\n        for b in self.window_arrangement.editor_buffers:\n            if b.buffer == current_buffer:\n                return b"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef use_colorscheme(self, name='default'):\n        try:\n            self.current_style = get_editor_style_by_name(name)\n        except pygments.util.ClassNotFound:\n            pass", "response": "Apply new colorscheme. (By name.)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the prompt - toolkit Layout and FocusStack.", "response": "def sync_with_prompt_toolkit(self):\n        \"\"\"\n        Update the prompt-toolkit Layout and FocusStack.\n        \"\"\"\n        # After executing a command, make sure that the layout of\n        # prompt-toolkit matches our WindowArrangement.\n        self.editor_layout.update()\n\n        # Make sure that the focus stack of prompt-toolkit has the current\n        # page.\n        window = self.window_arrangement.active_pt_window\n        if window:\n            self.application.layout.focus(window)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        # Make sure everything is in sync, before starting.\n        self.sync_with_prompt_toolkit()\n\n        def pre_run():\n            # Start in navigation mode.\n            self.application.vi_state.input_mode = InputMode.NAVIGATION\n\n        # Run eventloop of prompt_toolkit.\n        self.application.run(pre_run=pre_run)", "response": "Run the event loop for the interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngo into command mode.", "response": "def enter_command_mode(self):\n        \"\"\"\n        Go into command mode.\n        \"\"\"\n        self.application.layout.focus(self.command_buffer)\n        self.application.vi_state.input_mode = InputMode.INSERT\n\n        self.previewer.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef leave_command_mode(self, append_to_history=False):\n        self.previewer.restore()\n\n        self.application.layout.focus_last()\n        self.application.vi_state.input_mode = InputMode.NAVIGATION\n\n        self.command_buffer.reset(append_to_history=append_to_history)", "response": "Leave command mode. Focus document window again."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_editor_style_by_name(name):\n    if name == 'vim':\n        vim_style = Style.from_dict(default_vim_style)\n    else:\n        vim_style = style_from_pygments_cls(get_style_by_name(name))\n\n    return merge_styles([\n        vim_style,\n        Style.from_dict(style_extensions),\n    ])", "response": "Get Style class.\n    This raises `pygments.util.ClassNotFound` when there is no style with this\n    name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall the lexer and return a get_tokens_for_line function.", "response": "def lex_document(self, document):\n        \"\"\"\n        Call the lexer and return a get_tokens_for_line function.\n        \"\"\"\n        location = self.editor_buffer.location\n\n        if location:\n            if self.editor_buffer.in_file_explorer_mode:\n                return PygmentsLexer(DirectoryListingLexer, sync_from_start=False).lex_document(document)\n\n            return PygmentsLexer.from_filename(location, sync_from_start=False).lex_document(document)\n\n        return SimpleLexer().lex_document(document)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding bytes. Return a tuple.", "response": "def _auto_decode(data):\n    \"\"\"\n    Decode bytes. Return a (text, encoding) tuple.\n    \"\"\"\n    assert isinstance(data, six.binary_type)\n\n    for e in ENCODINGS:\n        try:\n            return data.decode(e), e\n        except UnicodeDecodeError:\n            pass\n\n    return data.decode('utf-8', 'ignore')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, location):\n        location = os.path.expanduser(location)\n\n        # Try to open this file, using different encodings.\n        for e in ENCODINGS:\n            try:\n                with codecs.open(location, 'r', e) as f:\n                    return f.read(), e\n            except UnicodeDecodeError:\n                pass  # Try next codec.\n\n        # Unable to open.\n        raise Exception('Unable to open file: %r' % location)", "response": "Read file from disk."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, location, text, encoding):\n        location = os.path.expanduser(location)\n\n        with codecs.open(location, 'w', encoding) as f:\n            f.write(text)", "response": "Write text to disk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the given text to disk.", "response": "def write(self, location, text, encoding):\n        \"\"\"\n        Write file to disk.\n        \"\"\"\n        location = os.path.expanduser(location)\n\n        with gzip.open(location, 'wb') as f:\n            f.write(text.encode(encoding))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read(self, location):\n        for io in self.editor.io_backends:\n            if io.can_open_location(location):\n                # Found an I/O backend.\n                exists = io.exists(location)\n                self.isdir = io.isdir(location)\n\n                if exists in (True, NotImplemented):\n                    # File could exist. Read it.\n                    self.is_new = False\n                    try:\n                        text, self.encoding = io.read(location)\n\n                        # Replace \\r\\n by \\n.\n                        text = text.replace('\\r\\n', '\\n')\n\n                        # Drop trailing newline while editing.\n                        # (prompt-toolkit doesn't enforce the trailing newline.)\n                        if text.endswith('\\n'):\n                            text = text[:-1]\n                    except Exception as e:\n                        self.editor.show_message('Cannot read %r: %r' % (location, e))\n                        return ''\n                    else:\n                        return text\n                else:\n                    # File doesn't exist.\n                    self.is_new = True\n                    return ''\n\n        self.editor.show_message('Cannot read: %r' % location)\n        return ''", "response": "Read file I / O backend."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreloading file from storage.", "response": "def reload(self):\n        \"\"\"\n        Reload file again from storage.\n        \"\"\"\n        text = self._read(self.location)\n        cursor_position = min(self.buffer.cursor_position, len(text))\n\n        self.buffer.document = Document(text, cursor_position)\n        self._file_content = text"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, location=None):\n        # Take location and expand tilde.\n        if location is not None:\n            self.location = location\n        assert self.location\n\n        # Find I/O backend that handles this location.\n        for io in self.editor.io_backends:\n            if io.can_open_location(self.location):\n                break\n        else:\n            self.editor.show_message('Unknown location: %r' % location)\n\n        # Write it.\n        try:\n            io.write(self.location, self.buffer.text + '\\n', self.encoding)\n            self.is_new = False\n        except Exception as e:\n            # E.g. \"No such file or directory.\"\n            self.editor.show_message('%s' % e)\n        else:\n            # When the save succeeds: update: _file_content.\n            self._file_content = self.buffer.text", "response": "Write file to I/O backend."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning name as displayed.", "response": "def get_display_name(self, short=False):\n        \"\"\"\n        Return name as displayed.\n        \"\"\"\n        if self.location is None:\n            return '[New file]'\n        elif short:\n            return os.path.basename(self.location)\n        else:\n            return self.location"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves current editor state.", "response": "def save(self):\n        \"\"\"\n        Back up current editor state.\n        \"\"\"\n        e = self.editor\n\n        self._style = e.current_style\n        self._show_line_numbers = e.show_line_numbers\n        self._highlight_search = e.highlight_search\n        self._show_ruler = e.show_ruler\n        self._relative_number = e.relative_number\n        self._cursorcolumn = e.cursorcolumn\n        self._cursorline = e.cursorline\n        self._colorcolumn = e.colorcolumn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply(self, input_string):\n        e = self.editor\n\n        # Parse command.\n        m = COMMAND_GRAMMAR.match(input_string)\n        if m is None:\n            return\n\n        variables = m.variables()\n\n        command = variables.get('command')\n        set_option = variables.get('set_option')\n\n        # Preview colorschemes.\n        if command == 'colorscheme':\n            colorscheme = variables.get('colorscheme')\n            if colorscheme:\n                e.use_colorscheme(colorscheme)\n\n        # Preview some set commands.\n        if command == 'set':\n            if set_option in ('hlsearch', 'hls'):\n                e.highlight_search = True\n            elif set_option in ('nohlsearch', 'nohls'):\n                e.highlight_search = False\n            elif set_option in ('nu', 'number'):\n                e.show_line_numbers = True\n            elif set_option in ('nonu', 'nonumber'):\n                e.show_line_numbers = False\n            elif set_option in ('ruler', 'ru'):\n                e.show_ruler = True\n            elif set_option in ('noruler', 'noru'):\n                e.show_ruler = False\n            elif set_option in ('relativenumber', 'rnu'):\n                e.relative_number = True\n            elif set_option in ('norelativenumber', 'nornu'):\n                e.relative_number = False\n            elif set_option in ('cursorline', 'cul'):\n                e.cursorline = True\n            elif set_option in ('cursorcolumn', 'cuc'):\n                e.cursorcolumn = True\n            elif set_option in ('nocursorline', 'nocul'):\n                e.cursorline = False\n            elif set_option in ('nocursorcolumn', 'nocuc'):\n                e.cursorcolumn = False\n            elif set_option in ('colorcolumn', 'cc'):\n                value = variables.get('set_value', '')\n                if value:\n                    e.colorcolumn = [\n                        int(v) for v in value.split(',') if v.isdigit()]", "response": "Apply the command to the current state of the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef location_cmd(name, accepts_force=False):\n    COMMANDS_TAKING_LOCATIONS.add(name)\n\n    def decorator(func):\n        @_cmd(name)\n        def command_wrapper(editor, variables):\n            location = variables.get('location')\n            force = bool(variables['force'])\n\n            if force and not accepts_force:\n                editor.show_message('No ! allowed')\n            elif accepts_force:\n                func(editor, location, force=force)\n            else:\n                func(editor, location)\n        return func\n    return decorator", "response": "Decorator that registers a command that takes a location as optional )\n    parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmd(name, accepts_force=False):\n    def decorator(func):\n        @_cmd(name)\n        def command_wrapper(editor, variables):\n            force = bool(variables['force'])\n\n            if force and not accepts_force:\n                editor.show_message('No ! allowed')\n            elif accepts_force:\n                func(editor, force=force)\n            else:\n                func(editor)\n        return func\n    return decorator", "response": "Decorator that registers a command that doesn t take any parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a : set command.", "response": "def set_command_execute(editor, variables):\n    \"\"\"\n    Execute a ':set' command.\n    \"\"\"\n    option = variables.get('set_option')\n    value = variables.get('set_value')\n\n    if option in SET_COMMANDS:\n        # Call the correct handler.\n        if option in SET_COMMANDS_TAKING_VALUE:\n            SET_COMMANDS[option](editor, value)\n        else:\n            SET_COMMANDS[option](editor)\n    else:\n        editor.show_message('Unknown option: %s' % option)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bn(editor, force=False):\n    eb = editor.window_arrangement.active_editor_buffer\n\n    if not force and eb.has_unsaved_changes:\n        editor.show_message(_NO_WRITE_SINCE_LAST_CHANGE_TEXT)\n    else:\n        editor.window_arrangement.go_to_next_buffer()", "response": "Go to next buffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bp(editor, force=False):\n    eb = editor.window_arrangement.active_editor_buffer\n\n    if not force and eb.has_unsaved_changes:\n        editor.show_message(_NO_WRITE_SINCE_LAST_CHANGE_TEXT)\n    else:\n        editor.window_arrangement.go_to_previous_buffer()", "response": "Go to previous buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _buffer(editor, variables, force=False):\n    eb = editor.window_arrangement.active_editor_buffer\n    force = bool(variables['force'])\n\n    buffer_name = variables.get('buffer_name')\n    if buffer_name:\n        if not force and eb.has_unsaved_changes:\n            editor.show_message(_NO_WRITE_SINCE_LAST_CHANGE_TEXT)\n        else:\n            editor.window_arrangement.go_to_buffer(buffer_name)", "response": "Go to a buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_and_quit(editor, location, force=False):\n    write(editor, location, force=force)\n    editor.application.exit()", "response": "Write file and quit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_and_quit_all(editor):\n    eb = editor.window_arrangement.active_editor_buffer\n    if eb.location is None:\n        editor.show_message(_NO_FILE_NAME)\n    else:\n        eb.write()\n        quit(editor, all_=True, force=False)", "response": "Write current buffer and quit all."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef color_scheme(editor, variables):\n    colorscheme = variables.get('colorscheme')\n    if colorscheme:\n        editor.use_colorscheme(colorscheme)", "response": "Set colorscheme for the current open buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_key_bindings(editor):\n    kb = KeyBindings()\n\n    # Filters.\n    @Condition\n    def vi_buffer_focussed():\n        app = get_app()\n        if app.layout.has_focus(editor.search_buffer) or app.layout.has_focus(editor.command_buffer):\n            return False\n        return True\n\n    in_insert_mode = vi_insert_mode & vi_buffer_focussed\n    in_navigation_mode = vi_navigation_mode & vi_buffer_focussed\n\n    @kb.add('c-t')\n    def _(event):\n        \"\"\"\n        Override default behaviour of prompt-toolkit.\n        (Control-T will swap the last two characters before the cursor, because\n        that's what readline does.)\n        \"\"\"\n        pass\n\n    @kb.add('c-t', filter=in_insert_mode)\n    def indent_line(event):\n        \"\"\"\n        Indent current line.\n        \"\"\"\n        b = event.application.current_buffer\n\n        # Move to start of line.\n        pos = b.document.get_start_of_line_position(after_whitespace=True)\n        b.cursor_position += pos\n\n        # Insert tab.\n        if editor.expand_tab:\n            b.insert_text('    ')\n        else:\n            b.insert_text('\\t')\n\n        # Restore cursor.\n        b.cursor_position -= pos\n\n    @kb.add('c-r', filter=in_navigation_mode, save_before=(lambda e: False))\n    def redo(event):\n        \"\"\"\n        Redo.\n        \"\"\"\n        event.app.current_buffer.redo()\n\n    @kb.add(':', filter=in_navigation_mode)\n    def enter_command_mode(event):\n        \"\"\"\n        Entering command mode.\n        \"\"\"\n        editor.enter_command_mode()\n\n    @kb.add('tab', filter=vi_insert_mode &\n            ~has_focus(editor.command_buffer) & whitespace_before_cursor_on_line)\n    def autocomplete_or_indent(event):\n        \"\"\"\n        When the 'tab' key is pressed with only whitespace character before the\n        cursor, do autocompletion. Otherwise, insert indentation.\n        \"\"\"\n        b = event.app.current_buffer\n        if editor.expand_tab:\n            b.insert_text('    ')\n        else:\n            b.insert_text('\\t')\n\n    @kb.add('escape', filter=has_focus(editor.command_buffer))\n    @kb.add('c-c', filter=has_focus(editor.command_buffer))\n    @kb.add('backspace',\n        filter=has_focus(editor.command_buffer) & Condition(lambda: editor.command_buffer.text == ''))\n    def leave_command_mode(event):\n        \"\"\"\n        Leaving command mode.\n        \"\"\"\n        editor.leave_command_mode()\n\n    @kb.add('c-w', 'c-w', filter=in_navigation_mode)\n    def focus_next_window(event):\n        editor.window_arrangement.cycle_focus()\n        editor.sync_with_prompt_toolkit()\n\n    @kb.add('c-w', 'n', filter=in_navigation_mode)\n    def horizontal_split(event):\n        \"\"\"\n        Split horizontally.\n        \"\"\"\n        editor.window_arrangement.hsplit(None)\n        editor.sync_with_prompt_toolkit()\n\n    @kb.add('c-w', 'v', filter=in_navigation_mode)\n    def vertical_split(event):\n        \"\"\"\n        Split vertically.\n        \"\"\"\n        editor.window_arrangement.vsplit(None)\n        editor.sync_with_prompt_toolkit()\n\n    @kb.add('g', 't', filter=in_navigation_mode)\n    def focus_next_tab(event):\n        editor.window_arrangement.go_to_next_tab()\n        editor.sync_with_prompt_toolkit()\n\n    @kb.add('g', 'T', filter=in_navigation_mode)\n    def focus_previous_tab(event):\n        editor.window_arrangement.go_to_previous_tab()\n        editor.sync_with_prompt_toolkit()\n\n    @kb.add('f1')\n    def show_help(event):\n        editor.show_help()\n\n    @Condition\n    def in_file_explorer_mode():\n        return bool(editor.current_editor_buffer and\n            editor.current_editor_buffer.in_file_explorer_mode)\n\n    @kb.add('enter', filter=in_file_explorer_mode)\n    def open_path(event):\n        \"\"\"\n        Open file/directory in file explorer mode.\n        \"\"\"\n        name_under_cursor = event.current_buffer.document.current_line\n        new_path = os.path.normpath(os.path.join(\n            editor.current_editor_buffer.location, name_under_cursor))\n\n        editor.window_arrangement.open_buffer(\n            new_path, show_in_current_window=True)\n        editor.sync_with_prompt_toolkit()\n\n    @kb.add('-', filter=in_file_explorer_mode)\n    def to_parent_directory(event):\n        new_path = os.path.normpath(os.path.join(\n            editor.current_editor_buffer.location, '..'))\n\n        editor.window_arrangement.open_buffer(\n            new_path, show_in_current_window=True)\n        editor.sync_with_prompt_toolkit()\n\n    return kb", "response": "Create custom key bindings for prompt - toolkit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef whitespace_before_cursor_on_line():\n    b = get_app().current_buffer\n    before_cursor = b.document.current_line_before_cursor\n\n    return bool(not before_cursor or before_cursor[-1].isspace())", "response": "Filter which evaluates to True when the characters before the cursor are whitespace or we are at the start of te line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _walk_through_windows(self):\n        def walk(split):\n            for c in split:\n                if isinstance(c, (HSplit, VSplit)):\n                    for i in walk(c):\n                        yield i\n                elif isinstance(c, Window):\n                    yield split, c\n\n        return walk(self.root)", "response": "Yields the split and window tuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding parent_split and child_split tuples.", "response": "def _walk_through_splits(self):\n        \"\"\"\n        Yields (parent_split, child_plit) tuples.\n        \"\"\"\n        def walk(split):\n            for c in split:\n                if isinstance(c, (HSplit, VSplit)):\n                    yield split, c\n                    for i in walk(c):\n                        yield i\n\n        return walk(self.root)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _split(self, split_cls, editor_buffer=None):\n        if editor_buffer is None:\n            editor_buffer = self.active_window.editor_buffer\n\n        active_split = self._get_active_split()\n        index = active_split.index(self.active_window)\n        new_window = Window(editor_buffer)\n\n        if isinstance(active_split, split_cls):\n            # Add new window to active split.\n            active_split.insert(index, new_window)\n        else:\n            # Split in the other direction.\n            active_split[index] = split_cls([active_split[index], new_window])\n\n        # Focus new window.\n        self.active_window = new_window", "response": "Split horizontal or vertical."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show_editor_buffer(self, editor_buffer):\n        assert isinstance(editor_buffer, EditorBuffer)\n        self.active_window.editor_buffer = editor_buffer", "response": "Show this EditorBuffer in the active window."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close_editor_buffer(self, editor_buffer):\n        for split, window in self._walk_through_windows():\n            if window.editor_buffer == editor_buffer:\n                self._close_window(window)", "response": "Closes all the windows that have this editor buffer open."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclosing the active window.", "response": "def close_active_window(self):\n        \"\"\"\n        Close active window.\n        \"\"\"\n        active_split = self._get_active_split()\n\n        # First remove the active window from its split.\n        index = active_split.index(self.active_window)\n        del active_split[index]\n\n        # Move focus.\n        if len(active_split):\n            new_active_window = active_split[max(0, index - 1)]\n            while isinstance(new_active_window, (HSplit, VSplit)):\n                new_active_window = new_active_window[0]\n            self.active_window = new_active_window\n        else:\n            self.active_window = None  # No windows left.\n\n        # When there is exactly on item left, move this back into the parent\n        # split. (We don't want to keep a split with one item around -- exept\n        # for the root.)\n        if len(active_split) == 1 and active_split != self.root:\n            parent = self._get_split_parent(active_split)\n            index = parent.index(active_split)\n            parent[index] = active_split[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncycle through all windows and set the active window to the next one.", "response": "def cycle_focus(self):\n        \"\"\"\n        Cycle through all windows.\n        \"\"\"\n        windows = self.windows()\n        new_index = (windows.index(self.active_window) + 1) % len(windows)\n        self.active_window = windows[new_index]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntruing when any of the visible buffers in this tab has unsaved changes.", "response": "def has_unsaved_changes(self):\n        \"\"\"\n        True when any of the visible buffers in this tab has unsaved changes.\n        \"\"\"\n        for w in self.windows():\n            if w.editor_buffer.has_unsaved_changes:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef active_editor_buffer(self):\n        if self.active_tab and self.active_tab.active_window:\n            return self.active_tab.active_window.editor_buffer", "response": "The active EditorBuffer or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef active_pt_window(self):\n        \" The active prompt_toolkit layout Window. \"\n        if self.active_tab:\n            w = self.active_tab.active_window\n            if w:\n                return w.pt_window", "response": "The active prompt_toolkit layout Window."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_editor_buffer_for_location(self, location):\n        for eb in self.editor_buffers:\n            if eb.location == location:\n                return eb", "response": "Returns the EditorBuffer for this location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_editor_buffer_for_buffer_name(self, buffer_name):\n        for eb in self.editor_buffers:\n            if eb.buffer_name == buffer_name:\n                return eb", "response": "Return the EditorBuffer for the given buffer_name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing all other windows except the current one.", "response": "def keep_only_current_window(self):\n        \"\"\"\n        Close all other windows, except the current one.\n        \"\"\"\n        self.tab_pages = [TabPage(self.active_tab.active_window)]\n        self.active_tab_index = 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngoing to next buffer in active window.", "response": "def go_to_next_buffer(self, _previous=False):\n        \"\"\"\n        Open next buffer in active window.\n        \"\"\"\n        if self.active_editor_buffer:\n            # Find the active opened buffer.\n            index = self.editor_buffers.index(self.active_editor_buffer)\n\n            # Get index of new buffer.\n            if _previous:\n                new_index = (len(self.editor_buffers) + index - 1) % len(self.editor_buffers)\n            else:\n                new_index = (index + 1) % len(self.editor_buffers)\n\n            # Open new buffer in active tab.\n            self.active_tab.show_editor_buffer(self.editor_buffers[new_index])\n\n            # Clean up buffers.\n            self._auto_close_new_empty_buffers()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngoing to the previous tab.", "response": "def go_to_previous_tab(self):\n        \"\"\"\n        Focus the previous tab.\n        \"\"\"\n        self.active_tab_index = (self.active_tab_index - 1 +\n                                 len(self.tab_pages)) % len(self.tab_pages)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngoing to one of the open buffers.", "response": "def go_to_buffer(self, buffer_name):\n        \"\"\"\n        Go to one of the open buffers.\n        \"\"\"\n        assert isinstance(buffer_name, string_types)\n\n        for i, eb in enumerate(self.editor_buffers):\n            if (eb.location == buffer_name or\n                    (buffer_name.isdigit() and int(buffer_name) == i)):\n                self.show_editor_buffer(eb)\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_editor_buffer(self, editor_buffer, show_in_current_window=False):\n        assert isinstance(editor_buffer, EditorBuffer) and editor_buffer not in self.editor_buffers\n\n        # Add to list of EditorBuffers\n        eb = self.active_editor_buffer\n        if eb is None:\n            self.editor_buffers.append(editor_buffer)\n        else:\n            # Append right after the currently active one.\n            try:\n                index = self.editor_buffers.index(self.active_editor_buffer)\n            except ValueError:\n                index = 0\n            self.editor_buffers.insert(index, editor_buffer)\n\n        # When there are no tabs/windows yet, create one for this buffer.\n        if self.tab_pages == []:\n            self.tab_pages.append(TabPage(Window(editor_buffer)))\n            self.active_tab_index = 0\n\n        # To be shown?\n        if show_in_current_window and self.active_tab:\n            self.active_tab.show_editor_buffer(editor_buffer)\n\n        # Start reporter.\n        editor_buffer.run_reporter()", "response": "Add this new EditorBuffer to the list of EditorBuffers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a location return the EditorBuffer instance that we have if it already exists or create a new one.", "response": "def _get_or_create_editor_buffer(self, location=None, text=None):\n        \"\"\"\n        Given a location, return the `EditorBuffer` instance that we have if\n        the file is already open, or create a new one.\n\n        When location is None, this creates a new buffer.\n        \"\"\"\n        assert location is None or text is None  # Don't pass two of them.\n        assert location is None or isinstance(location, string_types)\n\n        if location is None:\n            # Create and add an empty EditorBuffer\n            eb = EditorBuffer(self.editor, text=text)\n            self._add_editor_buffer(eb)\n\n            return eb\n        else:\n            # When a location is given, first look whether the file was already\n            # opened.\n            eb = self.get_editor_buffer_for_location(location)\n\n            # Not found? Create one.\n            if eb is None:\n                # Create and add EditorBuffer\n                eb = EditorBuffer(self.editor, location)\n                self._add_editor_buffer(eb)\n\n                return eb\n            else:\n                # Found! Return it.\n                return eb"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_buffer(self, location=None, show_in_current_window=False):\n        eb = self._get_or_create_editor_buffer(location)\n\n        if show_in_current_window:\n            self.show_editor_buffer(eb)", "response": "Open a file load it and show it in a new buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new tab page.", "response": "def create_tab(self, location=None):\n        \"\"\"\n        Create a new tab page.\n        \"\"\"\n        eb = self._get_or_create_editor_buffer(location)\n\n        self.tab_pages.insert(self.active_tab_index + 1, TabPage(Window(eb)))\n        self.active_tab_index += 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_open_buffers(self):\n        active_eb = self.active_editor_buffer\n        visible_ebs = self.active_tab.visible_editor_buffers()\n\n        def make_info(i, eb):\n            return OpenBufferInfo(\n                index=i,\n                editor_buffer=eb,\n                is_active=(eb == active_eb),\n                is_visible=(eb in visible_ebs))\n\n        return [make_info(i, eb) for i, eb in enumerate(self.editor_buffers)]", "response": "Return a list that gives information about the available open buffers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking the source code for errors and return a generator of errors.", "response": "def check(source,\n          filename='<string>',\n          report_level=docutils.utils.Reporter.INFO_LEVEL,\n          ignore=None,\n          debug=False):\n    \"\"\"Yield errors.\n\n    Use lower report_level for noisier error output.\n\n    Each yielded error is a tuple of the form:\n\n        (line_number, message)\n\n    Line numbers are indexed at 1 and are with respect to the full RST file.\n\n    Each code block is checked asynchronously in a subprocess.\n\n    Note that this function mutates state by calling the ``docutils``\n    ``register_*()`` functions.\n\n    \"\"\"\n    # Do this at call time rather than import time to avoid unnecessarily\n    # mutating state.\n    register_code_directive()\n    ignore_sphinx()\n\n    ignore = ignore or {}\n\n    try:\n        ignore.setdefault('languages', []).extend(\n            find_ignored_languages(source)\n        )\n    except Error as error:\n        yield (error.line_number, '{}'.format(error))\n\n    writer = CheckWriter(source, filename, ignore=ignore)\n\n    string_io = io.StringIO()\n\n    # This is a hack to avoid false positive from docutils (#23). docutils\n    # mistakes BOMs for actual visible letters. This results in the \"underline\n    # too short\" warning firing.\n    source = strip_byte_order_mark(source)\n\n    try:\n        docutils.core.publish_string(\n            source, writer=writer,\n            source_path=filename,\n            settings_overrides={'halt_level': report_level,\n                                'report_level': report_level,\n                                'warning_stream': string_io})\n    except docutils.utils.SystemMessage:\n        pass\n    except AttributeError:\n        # Sphinx will sometimes throw an exception trying to access\n        # \"self.state.document.settings.env\". Ignore this for now until we\n        # figure out a better approach.\n        if debug:\n            raise\n\n    for checker in writer.checkers:\n        for error in checker():\n            yield error\n\n    rst_errors = string_io.getvalue().strip()\n    if rst_errors:\n        for message in rst_errors.splitlines():\n            try:\n                ignore_regex = ignore.get('messages', '')\n                if ignore_regex and re.search(ignore_regex, message):\n                    continue\n                yield parse_gcc_style_error_message(message,\n                                                    filename=filename,\n                                                    has_column=False)\n            except ValueError:\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_ignored_languages(source):\n    for (index, line) in enumerate(source.splitlines()):\n        match = RSTCHECK_COMMENT_RE.match(line)\n        if match:\n            key_and_value = line[match.end():].strip().split('=')\n            if len(key_and_value) != 2:\n                raise Error('Expected \"key=value\" syntax',\n                            line_number=index + 1)\n\n            if key_and_value[0] == 'ignore-language':\n                for language in key_and_value[1].split(','):\n                    yield language.strip()", "response": "Yields all ignored languages in source."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_file(parameters):\n    (filename, args) = parameters\n\n    if filename == '-':\n        contents = sys.stdin.read()\n    else:\n        with contextlib.closing(\n            docutils.io.FileInput(source_path=filename)\n        ) as input_file:\n            contents = input_file.read()\n\n    args = load_configuration_from_file(\n        os.path.dirname(os.path.realpath(filename)), args)\n\n    ignore_directives_and_roles(args.ignore_directives, args.ignore_roles)\n\n    for substitution in args.ignore_substitutions:\n        contents = contents.replace('|{}|'.format(substitution), 'None')\n\n    ignore = {\n        'languages': args.ignore_language,\n        'messages': args.ignore_messages,\n    }\n    all_errors = []\n    for error in check(contents,\n                       filename=filename,\n                       report_level=args.report,\n                       ignore=ignore,\n                       debug=args.debug):\n        all_errors.append(error)\n    return (filename, all_errors)", "response": "Check the contents of a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields errors in nested RST code.", "response": "def check_rst(code, ignore):\n    \"\"\"Yield errors in nested RST code.\"\"\"\n    filename = '<string>'\n\n    for result in check(code,\n                        filename=filename,\n                        ignore=ignore):\n        yield result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_directives_and_roles_from_sphinx():\n    if SPHINX_INSTALLED:\n        sphinx_directives = list(sphinx.domains.std.StandardDomain.directives)\n        sphinx_roles = list(sphinx.domains.std.StandardDomain.roles)\n\n        for domain in [sphinx.domains.c.CDomain,\n                       sphinx.domains.cpp.CPPDomain,\n                       sphinx.domains.javascript.JavaScriptDomain,\n                       sphinx.domains.python.PythonDomain]:\n\n            sphinx_directives += list(domain.directives) + [\n                '{}:{}'.format(domain.name, item)\n                for item in list(domain.directives)]\n\n            sphinx_roles += list(domain.roles) + [\n                '{}:{}'.format(domain.name, item)\n                for item in list(domain.roles)]\n    else:\n        sphinx_roles = [\n            'abbr',\n            'command',\n            'dfn',\n            'doc',\n            'download',\n            'envvar',\n            'file',\n            'guilabel',\n            'kbd',\n            'keyword',\n            'mailheader',\n            'makevar',\n            'manpage',\n            'menuselection',\n            'mimetype',\n            'newsgroup',\n            'option',\n            'program',\n            'py:func',\n            'ref',\n            'regexp',\n            'samp',\n            'term',\n            'token']\n\n        sphinx_directives = [\n            'autosummary',\n            'currentmodule',\n            'centered',\n            'c:function',\n            'c:type',\n            'include',\n            'deprecated',\n            'envvar',\n            'glossary',\n            'index',\n            'no-code-block',\n            'literalinclude',\n            'hlist',\n            'option',\n            'productionlist',\n            'py:function',\n            'seealso',\n            'toctree',\n            'todo',\n            'versionadded',\n            'versionchanged']\n\n    return (sphinx_directives, sphinx_roles)", "response": "Return a tuple of Sphinx directive and roles."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ignore_sphinx():\n    (directives, roles) = _get_directives_and_roles_from_sphinx()\n\n    directives += [\n        'centered',\n        'include',\n        'deprecated',\n        'index',\n        'no-code-block',\n        'literalinclude',\n        'hlist',\n        'seealso',\n        'toctree',\n        'todo',\n        'versionadded',\n        'versionchanged']\n\n    ext_autosummary = [\n        'autosummary',\n        'currentmodule',\n    ]\n\n    ignore_directives_and_roles(directives + ext_autosummary,\n                                roles + ['ctype'])", "response": "Register Sphinx directives and roles to ignore."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the configuration filename.", "response": "def find_config(directory_or_file, debug=False):\n    \"\"\"Return configuration filename.\n\n    If `directory_or_file` is a file, return the real-path of that file. If it\n    is a directory, find the configuration (any file name in CONFIG_FILES) in\n    that directory or its ancestors.\n    \"\"\"\n    directory_or_file = os.path.realpath(directory_or_file)\n    if os.path.isfile(directory_or_file):\n        if debug:\n            print('using config file {}'.format(directory_or_file),\n                  file=sys.stderr)\n        return directory_or_file\n    directory = directory_or_file\n\n    while directory:\n        for filename in CONFIG_FILES:\n            candidate = os.path.join(directory, filename)\n            if os.path.exists(candidate):\n                if debug:\n                    print('using config file {}'.format(candidate),\n                          file=sys.stderr)\n                return candidate\n\n        parent_directory = os.path.dirname(directory)\n        if parent_directory == directory:\n            break\n        else:\n            directory = parent_directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads configuration from file.", "response": "def load_configuration_from_file(directory, args):\n    \"\"\"Return new ``args`` with configuration loaded from file.\"\"\"\n    args = copy.copy(args)\n\n    directory_or_file = directory\n    if args.config is not None:\n        directory_or_file = args.config\n\n    options = _get_options(directory_or_file, debug=args.debug)\n\n    args.report = options.get('report', args.report)\n    threshold_dictionary = docutils.frontend.OptionParser.thresholds\n    args.report = int(threshold_dictionary.get(args.report, args.report))\n\n    args.ignore_language = get_and_split(\n        options, 'ignore_language', args.ignore_language)\n\n    args.ignore_messages = options.get(\n        'ignore_messages', args.ignore_messages)\n\n    args.ignore_directives = get_and_split(\n        options, 'ignore_directives', args.ignore_directives)\n\n    args.ignore_substitutions = get_and_split(\n        options, 'ignore_substitutions', args.ignore_substitutions)\n\n    args.ignore_roles = get_and_split(\n        options, 'ignore_roles', args.ignore_roles)\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ignore_directives_and_roles(directives, roles):\n    for directive in directives:\n        docutils.parsers.rst.directives.register_directive(directive,\n                                                           IgnoredDirective)\n\n    for role in roles:\n        docutils.parsers.rst.roles.register_local_role(role, _ignore_role)", "response": "Ignore directives and roles in docutils."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a function that runs a bash command and returns a generator that yields the number of errors.", "response": "def bash_checker(code, working_directory):\n    \"\"\"Return checker.\"\"\"\n    run = run_in_subprocess(code, '.bash', ['bash', '-n'],\n                            working_directory=working_directory)\n\n    def run_check():\n        \"\"\"Yield errors.\"\"\"\n        result = run()\n        if result:\n            (output, filename) = result\n            prefix = filename + ': line '\n            for line in output.splitlines():\n                if not line.startswith(prefix):\n                    continue\n                message = line[len(prefix):]\n                split_message = message.split(':', 1)\n                yield (int(split_message[0]) - 1,\n                       split_message[1].strip())\n    return run_check"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a function that runs the gcc code and returns a generator that yields the errors.", "response": "def gcc_checker(code, filename_suffix, arguments, working_directory):\n    \"\"\"Return checker.\"\"\"\n    run = run_in_subprocess(code,\n                            filename_suffix,\n                            arguments + ['-pedantic', '-fsyntax-only'],\n                            working_directory=working_directory)\n\n    def run_check():\n        \"\"\"Yield errors.\"\"\"\n        result = run()\n        if result:\n            (output, filename) = result\n            for line in output.splitlines():\n                try:\n                    yield parse_gcc_style_error_message(line,\n                                                        filename=filename)\n                except ValueError:\n                    continue\n\n    return run_check"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse GCC - style error message.", "response": "def parse_gcc_style_error_message(message, filename, has_column=True):\n    \"\"\"Parse GCC-style error message.\n\n    Return (line_number, message). Raise ValueError if message cannot be\n    parsed.\n\n    \"\"\"\n    colons = 2 if has_column else 1\n    prefix = filename + ':'\n    if not message.startswith(prefix):\n        raise ValueError()\n    message = message[len(prefix):]\n    split_message = message.split(':', colons)\n    line_number = int(split_message[0])\n    return (line_number,\n            split_message[colons].strip())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a code in a subprocess.", "response": "def run_in_subprocess(code, filename_suffix, arguments, working_directory):\n    \"\"\"Return None on success.\"\"\"\n    temporary_file = tempfile.NamedTemporaryFile(mode='wb',\n                                                 suffix=filename_suffix)\n    temporary_file.write(code.encode('utf-8'))\n    temporary_file.flush()\n\n    process = subprocess.Popen(arguments + [temporary_file.name],\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               cwd=working_directory)\n\n    def run():\n        \"\"\"Yield errors.\"\"\"\n        raw_result = process.communicate()\n        if process.returncode != 0:\n            return (raw_result[1].decode(get_encoding()),\n                    temporary_file.name)\n\n    return run"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef beginning_of_code_block(node, line_number, full_contents, is_code_node):\n    if SPHINX_INSTALLED and not is_code_node:\n        delta = len(node.non_default_attributes())\n        current_line_contents = full_contents.splitlines()[line_number:]\n        blank_lines = next(\n            (i for (i, x) in enumerate(current_line_contents) if x),\n            0)\n        return (\n            line_number +\n            delta - 1 +\n            blank_lines - 1 +\n            SPHINX_CODE_BLOCK_DELTA)\n    else:\n        lines = full_contents.splitlines()\n        code_block_length = len(node.rawsource.splitlines())\n\n        try:\n            # Case where there are no extra spaces.\n            if lines[line_number - 1].strip():\n                return line_number - code_block_length + 1\n        except IndexError:\n            pass\n\n        # The offsets are wrong if the RST text has multiple blank lines after\n        # the code block. This is a workaround.\n        for line_number in range(line_number, 1, -1):\n            if lines[line_number - 2].strip():\n                break\n\n        return line_number - code_block_length", "response": "Return line number of beginning of code block."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_args():\n    threshold_choices = docutils.frontend.OptionParser.threshold_choices\n\n    parser = argparse.ArgumentParser(\n        description=__doc__ + (' Sphinx is enabled.'\n                               if SPHINX_INSTALLED else ''),\n        prog='rstcheck')\n\n    parser.add_argument('files', nargs='+', type=decode_filename,\n                        help='files to check')\n    parser.add_argument('--config', metavar='CONFIG', default=None,\n                        help='location of config file')\n    parser.add_argument('-r', '--recursive', action='store_true',\n                        help='run recursively over directories')\n    parser.add_argument('--report', metavar='level',\n                        choices=threshold_choices,\n                        default='info',\n                        help='report system messages at or higher than '\n                             'level; ' +\n                             ', '.join(choice for choice in threshold_choices\n                                       if not choice.isdigit()) +\n                             ' (default: %(default)s)')\n    parser.add_argument('--ignore-language', '--ignore',\n                        metavar='language', default='',\n                        help='comma-separated list of languages to ignore')\n    parser.add_argument('--ignore-messages',\n                        metavar='messages', default='',\n                        help='python regex that match the messages to ignore')\n    parser.add_argument('--ignore-directives',\n                        metavar='directives', default='',\n                        help='comma-separated list of directives to ignore')\n    parser.add_argument('--ignore-substitutions',\n                        metavar='substitutions', default='',\n                        help='comma-separated list of substitutions to ignore')\n    parser.add_argument('--ignore-roles',\n                        metavar='roles', default='',\n                        help='comma-separated list of roles to ignore')\n    parser.add_argument('--debug', action='store_true',\n                        help='show messages helpful for debugging')\n    parser.add_argument('--version', action='version',\n                        version='%(prog)s ' + __version__)\n\n    args = parser.parse_args()\n\n    if '-' in args.files:\n        if len(args.files) > 1:\n            parser.error(\"'-' for standard in can only be checked alone\")\n    else:\n        args.files = list(find_files(filenames=args.files,\n                                     recursive=args.recursive))\n\n    return args", "response": "Parse command - line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef output_message(text, file=sys.stderr):\n    if file.encoding is None:\n        # If the output file does not support Unicode, encode it to a byte\n        # string. On some machines, this occurs when Python is redirecting to\n        # file (or piping to something like Vim).\n        text = text.encode('utf-8')\n\n    print(text, file=file)", "response": "Output a message to terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_sphinx_if_possible():\n    if SPHINX_INSTALLED:\n        srcdir = tempfile.mkdtemp()\n        outdir = os.path.join(srcdir, '_build')\n        try:\n            sphinx.application.Sphinx(srcdir=srcdir,\n                                      confdir=None,\n                                      outdir=outdir,\n                                      doctreedir=outdir,\n                                      buildername='dummy',\n                                      status=None)\n            yield\n        finally:\n            shutil.rmtree(srcdir)\n    else:\n        yield", "response": "Register Sphinx directives and roles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if file is okay for modifying or recursing.", "response": "def match_file(filename):\n    \"\"\"Return True if file is okay for modifying/recursing.\"\"\"\n    base_name = os.path.basename(filename)\n\n    if base_name.startswith('.'):\n        return False\n\n    if not os.path.isdir(filename) and not filename.lower().endswith('.rst'):\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    args = parse_args()\n\n    if not args.files:\n        return 0\n\n    with enable_sphinx_if_possible():\n        status = 0\n        pool = multiprocessing.Pool(multiprocessing.cpu_count())\n        try:\n            if len(args.files) > 1:\n                results = pool.map(\n                    _check_file,\n                    [(name, args) for name in args.files])\n            else:\n                # This is for the case where we read from standard in.\n                results = [_check_file((args.files[0], args))]\n\n            for (filename, errors) in results:\n                for error in errors:\n                    line_number = error[0]\n                    message = error[1]\n\n                    if not re.match(r'\\([A-Z]+/[0-9]+\\)', message):\n                        message = '(ERROR/3) ' + message\n\n                    output_message('{}:{}: {}'.format(filename,\n                                                      line_number,\n                                                      message))\n\n                    status = 1\n        except (IOError, UnicodeError) as exception:\n            output_message(exception)\n            status = 1\n\n        return status", "response": "Check the contents of the file list and return 0 on success."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_literal_block(self, node):\n        # For \"..code-block:: language\"\n        language = node.get('language', None)\n        is_code_node = False\n        if not language:\n            # For \"..code:: language\"\n            is_code_node = True\n            classes = node.get('classes')\n            if 'code' in classes:\n                language = classes[-1]\n            else:\n                return\n\n        if language in self.ignore['languages']:\n            return\n\n        if language == 'doctest' or (\n                language == 'python' and\n                node.rawsource.lstrip().startswith('>>> ')):\n            self.visit_doctest_block(node)\n            raise docutils.nodes.SkipNode\n\n        checker = {\n            'bash': bash_checker,\n            'c': c_checker,\n            'cpp': cpp_checker,\n            'json': lambda source, _: lambda: check_json(source),\n            'xml': lambda source, _: lambda: check_xml(source),\n            'python': lambda source, _: lambda: check_python(source),\n            'rst': lambda source, _: lambda: check_rst(source,\n                                                       ignore=self.ignore)\n        }.get(language)\n\n        if checker:\n            run = checker(node.rawsource, self.working_directory)\n            self._add_check(node=node,\n                            run=run,\n                            language=language,\n                            is_code_node=is_code_node)\n\n        raise docutils.nodes.SkipNode", "response": "Check syntax of code block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck syntax of reStructuredText.", "response": "def visit_paragraph(self, node):\n        \"\"\"Check syntax of reStructuredText.\"\"\"\n        find = re.search(r'\\[[^\\]]+\\]\\([^\\)]+\\)', node.rawsource)\n        if find is not None:\n            self.document.reporter.warning(\n                '(rst) Link is formatted in Markdown style.', base_node=node)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_check(self, node, run, language, is_code_node):\n        def run_check():\n            \"\"\"Yield errors.\"\"\"\n            all_results = run()\n            if all_results is not None:\n                if all_results:\n                    for result in all_results:\n                        error_offset = result[0] - 1\n\n                        line_number = getattr(node, 'line', None)\n                        if line_number is not None:\n                            yield (\n                                beginning_of_code_block(\n                                    node=node,\n                                    line_number=line_number,\n                                    full_contents=self.contents,\n                                    is_code_node=is_code_node) +\n                                error_offset,\n                                '({}) {}'.format(language, result[1]))\n                else:\n                    yield (self.filename, 0, 'unknown error')\n        self.checkers.append(run_check)", "response": "Add checker that will be run."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_url(url):\n    parsed = urlparse(url)\n    kwargs = parse_qsl(parsed.query)\n\n    # TCP redis connection\n    if parsed.scheme == 'redis':\n        details = {'host': parsed.hostname}\n        if parsed.port:\n            details['port'] = parsed.port\n        if parsed.password:\n            details['password'] = parsed.password\n        db = parsed.path.lstrip('/')\n        if db and db.isdigit():\n            details['db'] = db\n\n    # Unix socket redis connection\n    elif parsed.scheme == 'redis+socket':\n        details = {'unix_socket_path': parsed.path}\n    else:\n        raise ValueError('Unsupported protocol %s' % (parsed.scheme))\n\n    # Add kwargs to the details and convert them to the appropriate type, if needed\n    details.update(kwargs)\n    if 'socket_timeout' in details:\n        details['socket_timeout'] = float(details['socket_timeout'])\n    if 'db' in details:\n        details['db'] = int(details['db'])\n\n    return details", "response": "Parses the argument url and returns a redis connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raise_or_lock(self, key, timeout):\n        acquired = Lock(\n            self.redis,\n            key,\n            timeout=timeout,\n            blocking=self.blocking,\n            blocking_timeout=self.blocking_timeout\n        ).acquire()\n\n        if not acquired:\n            # Time remaining in milliseconds\n            # https://redis.io/commands/pttl\n            ttl = self.redis.pttl(key)\n            raise AlreadyQueued(ttl / 1000.)", "response": "Checks if the task is locked and raises an exception if it is not."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_backend(config):\n    backend_name = config['backend']\n    path = backend_name.split('.')\n    backend_mod_name, backend_class_name = '.'.join(path[:-1]), path[-1]\n    backend_mod = importlib.import_module(backend_mod_name)\n    backend_class = getattr(backend_mod, backend_class_name)\n    return backend_class(config['settings'])", "response": "Imports and initializes the Backend class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nforces key in dict or list to a string.", "response": "def force_string(kwargs):\n    \"\"\"\n    Force key in dict or list to a string.\n    Fixes: https://github.com/cameronmaske/celery-once/issues/11\n    \"\"\"\n    if isinstance(kwargs, dict):\n        # Context: https://github.com/cameronmaske/celery-once/issues/58\n        # Keep equivalent to string of dict for backwards compatibility.\n        return order_dict_to_string(OrderedDict(\n            (force_string(key), force_string(value))\n            for key, value in items_sorted_by_key(kwargs)\n        ))\n    elif isinstance(kwargs, list):\n        return [force_string(element) for element in kwargs]\n    elif six.PY2 and isinstance(kwargs, unicode):\n        return kwargs.encode('utf-8')\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nturns a dictionary of keyword arguments into a list of strings.", "response": "def kwargs_to_list(kwargs):\n    \"\"\"\n    Turns {'a': 1, 'b': 2} into [\"a-1\", \"b-2\"]\n    \"\"\"\n    kwargs_list = []\n    # Kwargs are sorted in alphabetic order by their keys.\n    # Taken from http://www.saltycrane.com/blog/2007/09/how-to-sort-python-dictionary-by-keys/\n    for k, v in items_sorted_by_key(kwargs):\n        kwargs_list.append(str(k) + '-' + str(force_string(v)))\n    return kwargs_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef queue_once_key(name, kwargs, restrict_to=None):\n    keys = ['qo', force_string(name)]\n    # Restrict to only the keys allowed in keys.\n    if restrict_to is not None:\n        restrict_kwargs = {key: kwargs[key] for key in restrict_to}\n        keys += kwargs_to_list(restrict_kwargs)\n    else:\n        keys += kwargs_to_list(kwargs)\n    key = \"_\".join(keys)\n    return key", "response": "Returns a redis key that can be used to store the task once."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef raise_or_lock(self, key, timeout):\n        lock_path = self._get_lock_path(key)\n        try:\n            # Create lock file, raise exception if it exists\n            fd = os.open(lock_path, os.O_CREAT | os.O_EXCL)\n        except OSError as error:\n            if error.errno == errno.EEXIST:\n                # File already exists, check its modification time\n                mtime = os.path.getmtime(lock_path)\n                ttl = mtime + timeout - time.time()\n                if ttl > 0:\n                    raise AlreadyQueued(ttl)\n                else:\n                    # Update modification time if timeout happens\n                    os.utime(lock_path, None)\n                    return\n            else:\n                # Re-raise unexpected OSError\n                raise\n        else:\n            os.close(fd)", "response": "Raise an exception if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the lock file for the given key.", "response": "def clear_lock(self, key):\n        \"\"\"\n        Remove the lock file.\n        \"\"\"\n        lock_path = self._get_lock_path(key)\n        os.remove(lock_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_async(self, args=None, kwargs=None, **options):\n        once_options = options.get('once', {})\n        once_graceful = once_options.get(\n            'graceful', self.once.get('graceful', False))\n        once_timeout = once_options.get(\n            'timeout', self.once.get('timeout', self.default_timeout))\n\n        if not options.get('retries'):\n            key = self.get_key(args, kwargs)\n            try:\n                self.once_backend.raise_or_lock(key, timeout=once_timeout)\n            except AlreadyQueued as e:\n                if once_graceful:\n                    return EagerResult(None, None, states.REJECTED)\n                raise e\n        return super(QueueOnce, self).apply_async(args, kwargs, **options)", "response": "Attempts to queue a task."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_key(self, args=None, kwargs=None):\n        restrict_to = self.once.get('keys', None)\n        args = args or {}\n        kwargs = kwargs or {}\n        call_args = getcallargs(\n                getattr(self, '_orig_run', self.run), *args, **kwargs)\n        # Remove the task instance from the kwargs. This only happens when the\n        # task has the 'bind' attribute set to True. We remove it, as the task\n        # has a memory pointer in its repr, that will change between the task\n        # caller and the celery worker\n        if isinstance(call_args.get('self'), Task):\n            del call_args['self']\n        key = queue_once_key(self.name, call_args, restrict_to)\n        return key", "response": "Generate the key for this task and the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef after_return(self, status, retval, task_id, args, kwargs, einfo):\n        # Only clear the lock after the task's execution if the\n        # \"unlock_before_run\" option is False\n        if not self.unlock_before_run():\n            key = self.get_key(args, kwargs)\n            self.once_backend.clear_lock(key)", "response": "Clear the lock after a task has finished execution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_csv(self, df, csv_path, chunksize=10**5):\n        logger.info(\"Saving DataFrame to %s\", csv_path)\n        df.to_csv(csv_path, index=False, chunksize=chunksize)", "response": "Writes a dataframe to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cached_dataframe(self, csv_path, compute_fn):\n        if not csv_path.endswith(\".csv\"):\n            raise ValueError(\"Invalid path '%s', must be a CSV file\" % csv_path)\n\n        if csv_path in self._memory_cache:\n            return self._memory_cache[csv_path]\n\n        if exists(csv_path) and not self.is_empty(csv_path):\n            df = self._read_csv(csv_path)\n        else:\n            df = compute_fn()\n            if not isinstance(df, pd.DataFrame):\n                raise TypeError(\n                    \"Expected compute_fn to return DataFrame, got %s : %s\" % (\n                        df, type(df)))\n            self._write_csv(df, csv_path)\n        self._memory_cache[csv_path] = df\n        return df", "response": "Load a CSV file and return a Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cached_object(self, path, compute_fn):\n        if path in self._memory_cache:\n            return self._memory_cache[path]\n\n        if exists(path) and not self.is_empty(path):\n            obj = load_pickle(path)\n        else:\n            obj = compute_fn()\n            dump_pickle(obj, path)\n        self._memory_cache[path] = obj\n        return obj", "response": "Returns the cached object for the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_header_id(line):\n    if type(line) is not binary_type:\n        raise TypeError(\"Expected header line to be of type %s but got %s\" % (\n            binary_type, type(line)))\n\n    if len(line) <= 1:\n        raise ValueError(\"No identifier on FASTA line\")\n\n    # split line at first space to get the unique identifier for\n    # this sequence\n    space_index = line.find(b\" \")\n    if space_index >= 0:\n        identifier = line[1:space_index]\n    else:\n        identifier = line[1:]\n\n    # annoyingly Ensembl83 reformatted the transcript IDs of its\n    # cDNA FASTA to include sequence version numbers\n    # .e.g.\n    # \"ENST00000448914.1\" instead of \"ENST00000448914\"\n    # So now we have to parse out the identifier\n    dot_index = identifier.find(b\".\")\n    if dot_index >= 0:\n        identifier = identifier[:dot_index]\n\n    return identifier.decode(\"ascii\")", "response": "Parse the header line which starts with '>' and return the transcript or protein identifier"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_file(self, fasta_path):\n        fasta_dictionary = {}\n        for (identifier, sequence) in self.iterate_over_file(fasta_path):\n            fasta_dictionary[identifier] = sequence\n        return fasta_dictionary", "response": "Read the contents of a FASTA file into a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _open(self, fasta_path):\n        if fasta_path.endswith(\"gz\") or fasta_path.endswith(\"gzip\"):\n            return GzipFile(fasta_path, 'rb')\n        else:\n            return open(fasta_path, 'rb')", "response": "Open either a text file or compressed gzip file as a stream of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_reference_name(name):\n    lower_name = name.strip().lower()\n    for reference in Species._reference_names_to_species.keys():\n        if reference.lower() == lower_name:\n            return reference\n    raise ValueError(\"Reference genome '%s' not found\" % name)", "response": "Normalizes a reference name to a species - specific reference."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef genome_for_reference_name(\n        reference_name,\n        allow_older_downloaded_release=True):\n    \"\"\"\n    Given a genome reference name, such as \"GRCh38\", returns the\n    corresponding Ensembl Release object.\n\n    If `allow_older_downloaded_release` is True, and some older releases have\n    been downloaded, then return the most recent locally available release.\n\n\n    Otherwise, return the newest release of Ensembl (even if its data hasn't\n    already been downloaded).\n    \"\"\"\n    reference_name = normalize_reference_name(reference_name)\n    species = find_species_by_reference(reference_name)\n    (min_ensembl_release, max_ensembl_release) = \\\n        species.reference_assemblies[reference_name]\n    if allow_older_downloaded_release:\n        # go through candidate releases in descending order\n        for release in reversed(range(min_ensembl_release, max_ensembl_release + 1)):\n            # check if release has been locally downloaded\n            candidate = EnsemblRelease.cached(release=release, species=species)\n            if candidate.required_local_files_exist():\n                return candidate\n        # see if any of the releases between [max, min] are already locally\n        # available\n    return EnsemblRelease.cached(release=max_ensembl_release, species=species)", "response": "Given a genome reference name returns the corresponding Ensembl Release object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_species_name(name):\n    lower_name = name.lower().strip()\n\n    # if given a common name such as \"human\", look up its latin equivalent\n    if lower_name in Species._common_names_to_species:\n        return Species._common_names_to_species[lower_name].latin_name\n\n    return lower_name.replace(\" \", \"_\")", "response": "Normalizes a species name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(cls, latin_name, synonyms, reference_assemblies):\n        species = Species(\n            latin_name=latin_name,\n            synonyms=synonyms,\n            reference_assemblies=reference_assemblies)\n        cls._latin_names_to_species[species.latin_name] = species\n        for synonym in synonyms:\n            if synonym in cls._common_names_to_species:\n                raise ValueError(\"Can't use synonym '%s' for both %s and %s\" % (\n                    synonym,\n                    species,\n                    cls._common_names_to_species[synonym]))\n            cls._common_names_to_species[synonym] = species\n        for reference_name in reference_assemblies:\n            if reference_name in cls._reference_names_to_species:\n                raise ValueError(\"Can't use reference '%s' for both %s and %s\" % (\n                    reference_name,\n                    species,\n                    cls._reference_names_to_species[reference_name]))\n            cls._reference_names_to_species[reference_name] = species\n        return species", "response": "Create a Species object from the given arguments and enter into the class s _latin_names_to_species and _reference_names_to_species dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef offset(self, position):\n        if position > self.end or position < self.start:\n            raise ValueError(\n                \"Position %d outside valid range %d..%d of %s\" % (\n                    position, self.start, self.end, self))\n        elif self.on_forward_strand:\n            return position - self.start\n        else:\n            return self.end - position", "response": "Return the offset of the given position from stranded start of this locus."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef can_overlap(self, contig, strand=None):\n        return (self.on_contig(contig) and\n                (strand is None or self.on_strand(strand)))", "response": "Is this locus on the same contig and optionally on the same strand?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the distance between start and end of a set of exon and interval.", "response": "def distance_to_interval(self, start, end):\n        \"\"\"\n        Find the distance between intervals [start1, end1] and [start2, end2].\n        If the intervals overlap then the distance is 0.\n        \"\"\"\n        if self.start > end:\n            # interval is before this exon\n            return self.start - end\n        elif self.end < start:\n            # exon is before the interval\n            return start - self.end\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef overlaps(self, contig, start, end, strand=None):\n        return (\n            self.can_overlap(contig, strand) and\n            self.distance_to_interval(start, end) == 0)", "response": "Does this locus overlap with a given range of positions?"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _memoize_cache_key(args, kwargs):\n    cache_key_list = []\n\n    # hack to get around the unhashability of lists,\n    # add a special case to convert them to tuples\n    for arg in args:\n        if type(arg) is list:\n            cache_key_list.append(tuple(arg))\n        else:\n            cache_key_list.append(arg)\n    for (k, v) in sorted(kwargs.items()):\n        if type(v) is list:\n            cache_key_list.append((k, tuple(v)))\n        else:\n            cache_key_list.append((k, v))\n    return tuple(cache_key_list)", "response": "Turn args tuple and kwargs dictionary into a hashable key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize the arguments which uniquely specify an EnsemblRelease genome.", "response": "def normalize_init_values(cls, release, species, server):\n        \"\"\"\n        Normalizes the arguments which uniquely specify an EnsemblRelease\n        genome.\n        \"\"\"\n        release = check_release_number(release)\n        species = check_species_object(species)\n        return (release, species, server)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing an EnsemblRelease instance if it s not already made before. Otherwise return an old instance.", "response": "def cached(\n            cls,\n            release=MAX_ENSEMBL_RELEASE,\n            species=human,\n            server=ENSEMBL_FTP_SERVER):\n        \"\"\"\n        Construct EnsemblRelease if it's never been made before, otherwise\n        return an old instance.\n        \"\"\"\n        init_args_tuple = cls.normalize_init_values(release, species, server)\n        if init_args_tuple in cls._genome_cache:\n            genome = cls._genome_cache[init_args_tuple]\n        else:\n            genome = cls._genome_cache[init_args_tuple] = cls(*init_args_tuple)\n        return genome"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cache_subdirectory(\n        reference_name=None,\n        annotation_name=None,\n        annotation_version=None):\n    \"\"\"\n    Which cache subdirectory to use for a given annotation database\n    over a particular reference. All arguments can be omitted to just get\n    the base subdirectory for all pyensembl cached datasets.\n    \"\"\"\n    if reference_name is None:\n        reference_name = \"\"\n    if annotation_name is None:\n        annotation_name = \"\"\n    if annotation_version is None:\n        annotation_version = \"\"\n    reference_dir = join(CACHE_BASE_SUBDIR, reference_name)\n    annotation_dir = \"%s%s\" % (annotation_name, annotation_version)\n    return join(reference_dir, annotation_dir)", "response": "Returns the full path to the cache directory for a given annotation database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all the fields in this object.", "response": "def _fields(self):\n        \"\"\"\n        Fields used for hashing, string representation, equality comparison\n        \"\"\"\n        return (\n            ('reference_name', self.reference_name,),\n            ('annotation_name', self.annotation_name),\n            ('annotation_version', self.annotation_version),\n            ('cache_directory_path', self.cache_directory_path),\n            ('decompress_on_download', self.decompress_on_download),\n            ('copy_local_files_to_cache', self.copy_local_files_to_cache)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove the compression suffix if present in the given filename.", "response": "def _remove_compression_suffix_if_present(self, filename):\n        \"\"\"\n        If the given filename ends in one of the compression suffixes that\n        datacache knows how to deal with, remove the suffix (since we expect\n        the result of downloading to be a decompressed file)\n        \"\"\"\n        for ext in [\".gz\", \".gzip\", \".zip\"]:\n            if filename.endswith(ext):\n                return filename[:-len(ext)]\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the path to the local cache file for the given path_or_url.", "response": "def cached_path(self, path_or_url):\n        \"\"\"\n        When downloading remote files, the default behavior is to name local\n        files the same as their remote counterparts.\n        \"\"\"\n        assert path_or_url, \"Expected non-empty string for path_or_url\"\n        remote_filename = split(path_or_url)[1]\n        if self.is_url_format(path_or_url):\n            # passing `decompress=False` since there is logic below\n            # for stripping decompression extensions for both local\n            # and remote files\n            local_filename = datacache.build_local_filename(\n                download_url=path_or_url,\n                filename=remote_filename,\n                decompress=False)\n        else:\n            local_filename = remote_filename\n\n        # if we expect the download function to decompress this file then\n        # we should use its name without the compression extension\n        if self.decompress_on_download:\n            local_filename = self._remove_compression_suffix_if_present(\n                local_filename)\n\n        if len(local_filename) == 0:\n            raise ValueError(\"Can't determine local filename for %s\" % (\n                path_or_url,))\n\n        return join(self.cache_directory_path, local_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a remote file if necessary.", "response": "def _download_if_necessary(self, url, download_if_missing, overwrite):\n        \"\"\"\n        Return local cached path to a remote file, download it if necessary.\n        \"\"\"\n        cached_path = self.cached_path(url)\n        missing = not exists(cached_path)\n        if (missing or overwrite) and download_if_missing:\n            logger.info(\"Fetching %s from URL %s\", cached_path, url)\n            datacache.download._download_and_decompress_if_necessary(\n                full_path=cached_path,\n                download_url=url,\n                timeout=3600)\n        elif missing:\n            raise MissingRemoteFile(url)\n        return cached_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying the local file to the cache if necessary.", "response": "def _copy_if_necessary(self, local_path, overwrite):\n        \"\"\"\n        Return cached path to local file, copying it to the cache if necessary.\n        \"\"\"\n        local_path = abspath(local_path)\n        if not exists(local_path):\n            raise MissingLocalFile(local_path)\n        elif not self.copy_local_files_to_cache:\n            return local_path\n        else:\n            cached_path = self.cached_path(local_path)\n            if exists(cached_path) and not overwrite:\n                return cached_path\n            copy2(local_path, cached_path)\n            return cached_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download_or_copy_if_necessary(\n            self,\n            path_or_url,\n            download_if_missing=False,\n            overwrite=False):\n        \"\"\"\n        Download a remote file or copy\n        Get the local path to a possibly remote file.\n\n        Download if file is missing from the cache directory and\n        `download_if_missing` is True. Download even if local file exists if\n        both `download_if_missing` and `overwrite` are True.\n\n        If the file is on the local file system then return its path, unless\n        self.copy_local_to_cache is True, and then copy it to the cache first.\n\n        Parameters\n        ----------\n        path_or_url : str\n\n        download_if_missing : bool, optional\n            Download files if missing from local cache\n\n        overwrite : bool, optional\n            Overwrite existing copy if it exists\n        \"\"\"\n        assert path_or_url, \"Expected non-empty string for path_or_url\"\n        if self.is_url_format(path_or_url):\n            return self._download_if_necessary(\n                path_or_url,\n                download_if_missing,\n                overwrite)\n        else:\n            return self._copy_if_necessary(path_or_url, overwrite)", "response": "Download a remote file or copy it to the local cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete any cached files matching the prefixes or suffixes given", "response": "def delete_cached_files(self, prefixes=[], suffixes=[]):\n        \"\"\"\n        Deletes any cached files matching the prefixes or suffixes given\n        \"\"\"\n        for filename in listdir(self.cache_directory_path):\n            delete = (\n                any([filename.endswith(ext) for ext in suffixes]) or\n                any([filename.startswith(pre) for pre in prefixes]))\n            if delete:\n                path = join(self.cache_directory_path, filename)\n                logger.info(\"Deleting %s\", path)\n                remove(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of the essential fields of this Genome.", "response": "def to_dict(self):\n        \"\"\"\n        Returns a dictionary of the essential fields of this Genome.\n        \"\"\"\n        return dict(\n            reference_name=self.reference_name,\n            annotation_name=self.annotation_name,\n            annotation_version=self.annotation_version,\n            gtf_path_or_url=self._gtf_path_or_url,\n            transcript_fasta_paths_or_urls=self._transcript_fasta_paths_or_urls,\n            protein_fasta_paths_or_urls=self._protein_fasta_paths_or_urls,\n            decompress_on_download=self.decompress_on_download,\n            copy_local_files_to_cache=self.copy_local_files_to_cache,\n            cache_directory_path=self.cache_directory_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_lazy_fields(self):\n        self.gtf_path = None\n        self._protein_sequences = None\n        self._transcript_sequences = None\n        self._db = None\n        self.protein_fasta_paths = None\n        self.transcript_fasta_paths = None\n\n        # only memoizing the Gene, Transcript, and Exon objects\n        self._genes = {}\n        self._transcripts = {}\n        self._exons = {}", "response": "Initialize the lazy fields of the object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_cached_path(\n            self,\n            field_name,\n            path_or_url,\n            download_if_missing=False,\n            overwrite=False):\n        \"\"\"\n        Get the local path for a possibly remote file, invoking either\n        a download or install error message if it's missing.\n        \"\"\"\n        if len(field_name) == 0:\n            raise ValueError(\"Expected non-empty field name\")\n        if len(path_or_url) == 0:\n            raise ValueError(\"Expected non-empty path_or_url\")\n        return self.download_cache.local_path_or_install_error(\n            field_name=field_name,\n            path_or_url=path_or_url,\n            download_if_missing=download_if_missing,\n            overwrite=overwrite)", "response": "Get the local path for a possibly remote file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload data files needed by this Genome instance.", "response": "def download(self, overwrite=False):\n        \"\"\"\n        Download data files needed by this Genome instance.\n\n        Parameters\n        ----------\n        overwrite : bool, optional\n            Download files regardless whether local copy already exists.\n        \"\"\"\n        self._set_local_paths(download_if_missing=True, overwrite=overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindex all necessary data for this Genome.", "response": "def index(self, overwrite=False):\n        \"\"\"\n        Assuming that all necessary data for this Genome has been downloaded,\n        generate the GTF database and save efficient representation of\n        FASTA sequence files.\n        \"\"\"\n        if self.requires_gtf:\n            self.db.connect_or_create(overwrite=overwrite)\n        if self.requires_transcript_fasta:\n            self.transcript_sequences.index(overwrite=overwrite)\n        if self.requires_protein_fasta:\n            self.protein_sequences.index(overwrite=overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the string to install the user - specified missing files.", "response": "def install_string(self):\n        \"\"\"\n        Add every missing file to the install string shown to the user\n        in an error message.\n        \"\"\"\n        args = [\n            \"--reference-name\", self.reference_name,\n            \"--annotation-name\", self.annotation_name]\n        if self.annotation_version:\n            args.extend([\"--annotation-version\", str(self.annotation_version)])\n        if self.requires_gtf:\n            args.append(\"--gtf\")\n            args.append(\"\\\"%s\\\"\" % self._gtf_path_or_url)\n        if self.requires_protein_fasta:\n            args += [\n                \"--protein-fasta \\\"%s\\\"\" %\n                path for path in self._protein_fasta_paths_or_urls]\n        if self.requires_transcript_fasta:\n            args += [\n                \"--transcript-fasta \\\"%s\\\"\" %\n                path for path in self._transcript_fasta_paths_or_urls]\n        return \"pyensembl install %s\" % \" \".join(args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclear any in - memory cached values and short - lived on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk on - disk.", "response": "def clear_cache(self):\n        \"\"\"\n        Clear any in-memory cached values and short-lived on-disk\n        materializations from MemoryCache\n        \"\"\"\n        for maybe_fn in self.__dict__.values():\n            # clear cache associated with all memoization decorators,\n            # GTF and SequenceData objects\n            if hasattr(maybe_fn, \"clear_cache\"):\n                maybe_fn.clear_cache()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete all data aside from source GTF and FASTA files", "response": "def delete_index_files(self):\n        \"\"\"\n        Delete all data aside from source GTF and FASTA files\n        \"\"\"\n        self.clear_cache()\n        db_path = self.db.local_db_path()\n        if exists(db_path):\n            remove(db_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all feature values for a particular feature property from the database.", "response": "def _all_feature_values(\n            self,\n            column,\n            feature,\n            distinct=True,\n            contig=None,\n            strand=None):\n        \"\"\"\n        Cached lookup of all values for a particular feature property from\n        the database, caches repeated queries in memory and\n        stores them as a CSV.\n\n        Parameters\n        ----------\n\n        column : str\n            Name of property (e.g. exon_id)\n\n        feature : str\n            Type of entry (e.g. exon)\n\n        distinct : bool, optional\n            Keep only unique values\n\n        contig : str, optional\n            Restrict query to particular contig\n\n        strand : str, optional\n            Restrict results to \"+\" or \"-\" strands\n\n        Returns a list constructed from query results.\n        \"\"\"\n        return self.db.query_feature_values(\n            column=column,\n            feature=feature,\n            distinct=distinct,\n            contig=contig,\n            strand=strand)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the cDNA nucleotide sequence of a transcript", "response": "def transcript_sequence(self, transcript_id):\n        \"\"\"Return cDNA nucleotide sequence of transcript, or None if\n        transcript doesn't have cDNA sequence.\n        \"\"\"\n        if self.transcript_sequences is None:\n            raise ValueError(\n                \"No transcript FASTA supplied to this Genome: %s\" % self)\n        return self.transcript_sequences.get(transcript_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef protein_sequence(self, protein_id):\n        if self.protein_sequences is None:\n            raise ValueError(\n                \"No protein FASTA supplied to this Genome: %s\" % self)\n        return self.protein_sequences.get(protein_id)", "response": "Return the cDNA nucleotide sequence of a given protein ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of locus for a given gene ID.", "response": "def locus_of_gene_id(self, gene_id):\n        \"\"\"\n        Given a gene ID returns Locus with: chromosome, start, stop, strand\n        \"\"\"\n        return self.db.query_locus(\n            filter_column=\"gene_id\",\n            filter_value=gene_id,\n            feature=\"gene\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all Gene objects in the database.", "response": "def genes(self, contig=None, strand=None):\n        \"\"\"\n        Returns all Gene objects in the database. Can be restricted to a\n        particular contig/chromosome and strand by the following arguments:\n\n        Parameters\n        ----------\n        contig : str\n            Only return genes on the given contig.\n\n        strand : str\n            Only return genes on this strand.\n        \"\"\"\n        gene_ids = self.gene_ids(contig=contig, strand=strand)\n        return [self.gene_by_id(gene_id) for gene_id in gene_ids]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Gene object for the given gene ID.", "response": "def gene_by_id(self, gene_id):\n        \"\"\"\n        Construct a Gene object for the given gene ID.\n        \"\"\"\n        if gene_id not in self._genes:\n            field_names = [\n                \"seqname\",\n                \"start\",\n                \"end\",\n                \"strand\",\n            ]\n            optional_field_names = [\n                \"gene_name\",\n                \"gene_biotype\",\n            ]\n            # Do not look for gene_name and gene_biotype if they are\n            # not in the database.\n            field_names.extend([\n                name for name in optional_field_names\n                if self.db.column_exists(\"gene\", name)\n            ])\n            result = self.db.query_one(\n                field_names,\n                filter_column=\"gene_id\",\n                filter_value=gene_id,\n                feature=\"gene\")\n            if not result:\n                raise ValueError(\"Gene not found: %s\" % (gene_id,))\n\n            gene_name, gene_biotype = None, None\n            assert len(result) >= 4 and len(result) <= 6, \\\n                \"Result is not the expected length: %d\" % len(result)\n            contig, start, end, strand = result[:4]\n            if len(result) == 5:\n                if \"gene_name\" in field_names:\n                    gene_name = result[4]\n                else:\n                    gene_biotype = result[4]\n            elif len(result) == 6:\n                gene_name, gene_biotype = result[4:]\n\n            self._genes[gene_id] = Gene(\n                gene_id=gene_id,\n                gene_name=gene_name,\n                contig=contig,\n                start=start,\n                end=end,\n                strand=strand,\n                biotype=gene_biotype,\n                genome=self)\n\n        return self._genes[gene_id]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of Gene objects for the given gene name.", "response": "def genes_by_name(self, gene_name):\n        \"\"\"\n        Get all the unqiue genes with the given name (there might be multiple\n        due to copies in the genome), return a list containing a Gene object\n        for each distinct ID.\n        \"\"\"\n        gene_ids = self.gene_ids_of_gene_name(gene_name)\n        return [self.gene_by_id(gene_id) for gene_id in gene_ids]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the gene ID associated with the given protein ID and return its Gene object", "response": "def gene_by_protein_id(self, protein_id):\n        \"\"\"\n        Get the gene ID associated with the given protein ID,\n        return its Gene object\n        \"\"\"\n        gene_id = self.gene_id_of_protein_id(protein_id)\n        return self.gene_by_id(gene_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gene_names(self, contig=None, strand=None):\n        return self._all_feature_values(\n            column=\"gene_name\",\n            feature=\"gene\",\n            contig=contig,\n            strand=strand)", "response": "Return all the gene names in the database optionally restrict to a chromosome and or strand."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all the gene IDs in the cache.", "response": "def gene_ids(self, contig=None, strand=None):\n        \"\"\"\n        What are all the gene IDs\n        (optionally restrict to a given chromosome/contig and/or strand)\n        \"\"\"\n        return self._all_feature_values(\n            column=\"gene_id\",\n            feature=\"gene\",\n            contig=contig,\n            strand=strand)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of gene IDs associated with a given gene name.", "response": "def gene_ids_of_gene_name(self, gene_name):\n        \"\"\"\n        What are the gene IDs associated with a given gene name?\n        (due to copy events, there might be multiple genes per name)\n        \"\"\"\n        results = self._query_gene_ids(\"gene_name\", gene_name)\n        if len(results) == 0:\n            raise ValueError(\"Gene name not found: %s\" % gene_name)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the gene ID associated with a given protein ID.", "response": "def gene_id_of_protein_id(self, protein_id):\n        \"\"\"\n        What is the gene ID associated with a given protein ID?\n        \"\"\"\n        results = self._query_gene_ids(\n            \"protein_id\",\n            protein_id,\n            feature=\"CDS\")\n        if len(results) == 0:\n            raise ValueError(\"Protein ID not found: %s\" % protein_id)\n        assert len(results) == 1, \\\n            (\"Should have only one gene ID for a given protein ID, \"\n             \"but found %d: %s\" % (len(results), results))\n        return results[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transcripts(self, contig=None, strand=None):\n        transcript_ids = self.transcript_ids(contig=contig, strand=strand)\n        return [\n            self.transcript_by_id(transcript_id)\n            for transcript_id in transcript_ids\n        ]", "response": "Returns a list of Transcript objects for every transcript in the database optionally restrict to a particular\n        chromosome using the strand argument."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing Transcript object with given transcript ID", "response": "def transcript_by_id(self, transcript_id):\n        \"\"\"Construct Transcript object with given transcript ID\"\"\"\n        if transcript_id not in self._transcripts:\n            optional_field_names = [\n                \"transcript_name\",\n                \"transcript_biotype\",\n                \"transcript_support_level\",\n            ]\n            field_names = [\n                \"seqname\",\n                \"start\",\n                \"end\",\n                \"strand\",\n                \"gene_id\",\n            ]\n            # Do not look for the optional fields if they are not in the database.\n            field_names.extend([\n                name for name in optional_field_names\n                if self.db.column_exists(\"transcript\", name)\n            ])\n            result = self.db.query_one(\n                select_column_names=field_names,\n                filter_column=\"transcript_id\",\n                filter_value=transcript_id,\n                feature=\"transcript\",\n                distinct=True)\n            if not result:\n                raise ValueError(\"Transcript not found: %s\" % (transcript_id,))\n\n            transcript_name, transcript_biotype, tsl = None, None, None\n            assert 5 <= len(result) <= 5 + len(optional_field_names), \\\n                \"Result is not the expected length: %d\" % len(result)\n            contig, start, end, strand, gene_id = result[:5]\n            if len(result) > 5:\n                extra_field_names = [f for f in optional_field_names if f in field_names]\n                extra_data = dict(zip(extra_field_names, result[5:]))\n                transcript_name = extra_data.get(\"transcript_name\")\n                transcript_biotype = extra_data.get(\"transcript_biotype\")\n                tsl = extra_data.get(\"transcript_support_level\")\n                if not tsl or tsl == 'NA':\n                    tsl = None\n                else:\n                    tsl = int(tsl)\n\n            self._transcripts[transcript_id] = Transcript(\n                transcript_id=transcript_id,\n                transcript_name=transcript_name,\n                contig=contig,\n                start=start,\n                end=end,\n                strand=strand,\n                biotype=transcript_biotype,\n                gene_id=gene_id,\n                genome=self,\n                support_level=tsl)\n\n        return self._transcripts[transcript_id]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all the transcript names in the database.", "response": "def transcript_names(self, contig=None, strand=None):\n        \"\"\"\n        What are all the transcript names in the database\n        (optionally, restrict to a given chromosome and/or strand)\n        \"\"\"\n        return self._all_feature_values(\n            column=\"transcript_name\",\n            feature=\"transcript\",\n            contig=contig,\n            strand=strand)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transcript_id_of_protein_id(self, protein_id):\n        results = self._query_transcript_ids(\n            \"protein_id\",\n            protein_id,\n            feature=\"CDS\")\n        if len(results) == 0:\n            raise ValueError(\"Protein ID not found: %s\" % protein_id)\n        assert len(results) == 1, \\\n            (\"Should have only one transcript ID for a given protein ID, \"\n             \"but found %d: %s\" % (len(results), results))\n        return results[0]", "response": "Returns the transcript ID associated with a given protein ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all exons in the database optionally restrict to a particular chromosome using the strand argument.", "response": "def exons(self, contig=None, strand=None):\n        \"\"\"\n        Create exon object for all exons in the database, optionally\n        restrict to a particular chromosome using the `contig` argument.\n        \"\"\"\n        # DataFrame with single column called \"exon_id\"\n        exon_ids = self.exon_ids(contig=contig, strand=strand)\n        return [\n            self.exon_by_id(exon_id)\n            for exon_id in exon_ids\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exon_by_id(self, exon_id):\n        if exon_id not in self._exons:\n            field_names = [\n                \"seqname\",\n                \"start\",\n                \"end\",\n                \"strand\",\n                \"gene_name\",\n                \"gene_id\",\n            ]\n\n            contig, start, end, strand, gene_name, gene_id = self.db.query_one(\n                select_column_names=field_names,\n                filter_column=\"exon_id\",\n                filter_value=exon_id,\n                feature=\"exon\",\n                distinct=True)\n\n            self._exons[exon_id] = Exon(\n                exon_id=exon_id,\n                contig=contig,\n                start=start,\n                end=end,\n                strand=strand,\n                gene_name=gene_name,\n                gene_id=gene_id)\n\n        return self._exons[exon_id]", "response": "Construct an Exon object from its ID by looking up the exon s\n        properties in the Database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all the protein IDs in the cache.", "response": "def protein_ids(self, contig=None, strand=None):\n        \"\"\"\n        What are all the protein IDs\n        (optionally restrict to a given chromosome and/or strand)\n        \"\"\"\n        protein_ids = self._all_feature_values(\n            column=\"protein_id\",\n            feature=\"CDS\",\n            contig=contig,\n            strand=strand,\n            distinct=True)\n        # drop None values\n        return [protein_id for protein_id in protein_ids if protein_id]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck to make sure a release number is in the valid range of ensembl releases.", "response": "def check_release_number(release):\n    \"\"\"\n    Check to make sure a release is in the valid range of\n    Ensembl releases.\n    \"\"\"\n    try:\n        release = int(release)\n    except:\n        raise ValueError(\"Invalid Ensembl release: %s\" % release)\n\n    if release < MIN_ENSEMBL_RELEASE or release > MAX_ENSEMBL_RELEASE:\n        raise ValueError(\n            \"Invalid Ensembl releases %d, must be between %d and %d\" % (\n                release, MIN_ENSEMBL_RELEASE, MAX_ENSEMBL_RELEASE))\n    return release"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_combinations_of_ensembl_genomes(args):\n    species_list = args.species if args.species else [\"human\"]\n    release_list = args.release if args.release else [MAX_ENSEMBL_RELEASE]\n    genomes = []\n    for species in species_list:\n        # Otherwise, use Ensembl release information\n        for version in release_list:\n            ensembl_release = EnsemblRelease(version, species=species)\n\n            if not args.custom_mirror:\n                genomes.append(ensembl_release)\n            else:\n                # if we're using a custom mirror then we expect the provided\n                # URL to be a directory with all the same filenames as\n                # would be provided by Ensembl\n                gtf_url = os.path.join(\n                    args.custom_mirror,\n                    os.path.basename(ensembl_release.gtf_url))\n                transcript_fasta_urls = [\n                    os.path.join(\n                        args.custom_mirror,\n                        os.path.basename(transcript_fasta_url))\n                    for transcript_fasta_url in ensembl_release.transcript_fasta_urls\n                ]\n                protein_fasta_urls = [\n                    os.path.join(\n                        args.custom_mirror,\n                        os.path.basename(protein_fasta_url))\n                    for protein_fasta_url in\n                    ensembl_release.protein_fasta_urls\n                ]\n                reference_name = ensembl_release.reference_name\n                genome = Genome(\n                    reference_name=reference_name,\n                    annotation_name=\"ensembl\",\n                    annotation_version=version,\n                    gtf_path_or_url=gtf_url,\n                    transcript_fasta_paths_or_urls=transcript_fasta_urls,\n                    protein_fasta_paths_or_urls=protein_fasta_urls)\n                genomes.append(genome)\n    return genomes", "response": "Returns a list of all combinations of species and release versions specified by the given commandline arguments to return a list of EnsemblRelease or Genome objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _species_subdir(\n        ensembl_release,\n        species=\"homo_sapiens\",\n        filetype=\"gtf\",\n        server=ENSEMBL_FTP_SERVER):\n    \"\"\"\n    Assume ensembl_release has already been normalize by calling function\n    but species might be either a common name or latin name.\n    \"\"\"\n    return SPECIES_SUBDIR_TEMPLATE % {\n        \"release\": ensembl_release,\n        \"filetype\": filetype,\n        \"species\": species,\n    }", "response": "Return the path to the species directory for the given ensembl release."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_release_properties(ensembl_release, species):\n    ensembl_release = check_release_number(ensembl_release)\n    if not isinstance(species, Species):\n        species = find_species_by_name(species)\n    reference_name = species.which_reference(ensembl_release)\n    return ensembl_release, species.latin_name, reference_name", "response": "Normalize the ensembl release to be an integer species name and reference name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a GTF filename expect on Ensembl FTP server for a specific species and release combination", "response": "def make_gtf_filename(ensembl_release, species):\n    \"\"\"\n    Return GTF filename expect on Ensembl FTP server for a specific\n    species/release combination\n    \"\"\"\n    ensembl_release, species, reference_name = normalize_release_properties(\n        ensembl_release, species)\n    return GTF_FILENAME_TEMPLATE % {\n        \"Species\": species.capitalize(),\n        \"reference\": reference_name,\n        \"release\": ensembl_release,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_gtf_url(ensembl_release, species, server=ENSEMBL_FTP_SERVER):\n    ensembl_release, species, _ = \\\n        normalize_release_properties(ensembl_release, species)\n    subdir = _species_subdir(\n        ensembl_release,\n        species=species,\n        filetype=\"gtf\",\n        server=server)\n    url_subdir = urllib_parse.urljoin(server, subdir)\n    filename = make_gtf_filename(\n        ensembl_release=ensembl_release,\n        species=species)\n    return join(url_subdir, filename)", "response": "Returns a URL and a filename which can be joined together."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing URL to FASTA file with full sequence of a particular chromosome.", "response": "def make_fasta_dna_url(\n        ensembl_release,\n        species,\n        contig,\n        server=ENSEMBL_FTP_SERVER):\n    \"\"\"\n    Construct URL to FASTA file with full sequence of a particular chromosome.\n    Returns server_url/subdir and filename as tuple result.\n    \"\"\"\n    ensembl_release, species, _ = \\\n        normalize_release_properties(ensembl_release, species)\n    subdir = _species_subdir(\n        ensembl_release,\n        species=species,\n        filetype=\"fasta\",\n        server=server,)\n    server_subdir = urllib_parse.urljoin(server, subdir)\n    server_sequence_subdir = join(server_subdir, \"dna\")\n    filename = make_fasta_dna_filename(\n        ensembl_release=ensembl_release,\n        species=species,\n        contig=contig)\n    return join(server_sequence_subdir, filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct URL to FASTA file with cDNA transcript or protein sequences", "response": "def make_fasta_url(\n        ensembl_release,\n        species,\n        sequence_type,\n        server=ENSEMBL_FTP_SERVER):\n    \"\"\"Construct URL to FASTA file with cDNA transcript or protein sequences\n\n    Parameter examples:\n        ensembl_release = 75\n        species = \"Homo_sapiens\"\n        sequence_type = \"cdna\" (other option: \"pep\")\n    \"\"\"\n    ensembl_release, species, reference_name = normalize_release_properties(\n        ensembl_release, species)\n    subdir = _species_subdir(\n        ensembl_release,\n        species=species,\n        filetype=\"fasta\",\n        server=server)\n    server_subdir = urllib_parse.urljoin(server, subdir)\n    server_sequence_subdir = join(server_subdir, sequence_type)\n    filename = make_fasta_filename(\n        ensembl_release=ensembl_release,\n        species=species,\n        sequence_type=sequence_type)\n    return join(server_sequence_subdir, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _transcript_feature_position_ranges(self, feature, required=True):\n        if feature not in self._TRANSCRIPT_FEATURES:\n            raise ValueError(\"Invalid transcript feature: %s\" % feature)\n\n        results = self.db.query(\n            select_column_names=[\"start\", \"end\"],\n            filter_column=\"transcript_id\",\n            filter_value=self.id,\n            feature=feature)\n\n        if required and len(results) == 0:\n            raise ValueError(\n                \"Transcript %s does not contain feature %s\" % (\n                    self.id, feature))\n        return results", "response": "Find start and end chromosomal position ranges of features in this transcript."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _transcript_feature_positions(self, feature):\n        ranges = self._transcript_feature_position_ranges(\n            feature, required=True)\n        results = []\n        # a feature (such as a stop codon), maybe be split over multiple\n        # contiguous ranges. Collect all the nucleotide positions into a\n        # single list.\n        for (start, end) in ranges:\n            # since ranges are [inclusive, inclusive] and\n            # Python ranges are [inclusive, exclusive) we have to increment\n            # the end position\n            for position in range(start, end + 1):\n                assert position not in results, \\\n                    \"Repeated position %d for %s\" % (position, feature)\n                results.append(position)\n        return results", "response": "Get unique positions for a transcript feature."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of three chromosomal positions for the specified feature.", "response": "def _codon_positions(self, feature):\n        \"\"\"\n        Parameters\n        ----------\n        feature : str\n            Possible values are \"start_codon\" or \"stop_codon\"\n\n        Returns list of three chromosomal positions.\n        \"\"\"\n        results = self._transcript_feature_positions(feature)\n        if len(results) != 3:\n            raise ValueError(\n                \"Expected 3 positions for %s of %s but got %d\" % (\n                    feature,\n                    self.id,\n                    len(results)))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exon_intervals(self):\n        results = self.db.query(\n            select_column_names=[\"exon_number\", \"start\", \"end\"],\n            filter_column=\"transcript_id\",\n            filter_value=self.id,\n            feature=\"exon\")\n        sorted_intervals = [None] * len(results)\n        for (exon_number, start, end) in results:\n            sorted_intervals[int(exon_number) - 1] = (start, end)\n        return sorted_intervals", "response": "List of start and end tuples for each exon of this transcript."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spliced_offset(self, position):\n        # this code is performance sensitive, so switching from\n        # typechecks.require_integer to a simpler assertion\n        assert type(position) == int, \\\n            \"Position argument must be an integer, got %s : %s\" % (\n                position, type(position))\n\n        if position < self.start or position > self.end:\n            raise ValueError(\n                \"Invalid position: %d (must be between %d and %d)\" % (\n                    position,\n                    self.start,\n                    self.end))\n\n        # offset from beginning of unspliced transcript (including introns)\n        unspliced_offset = self.offset(position)\n        total_spliced_offset = 0\n\n        # traverse exons in order of their appearance on the strand\n        # Since absolute positions may decrease if on the negative strand,\n        # we instead use unspliced offsets to get always increasing indices.\n        #\n        # Example:\n        #\n        # Exon Name:                exon 1                exon 2\n        # Spliced Offset:           123456                789...\n        # Intron vs. Exon: ...iiiiiieeeeeeiiiiiiiiiiiiiiiieeeeeeiiiiiiiiiii...\n        for exon in self.exons:\n            exon_unspliced_start, exon_unspliced_end = self.offset_range(\n                exon.start, exon.end)\n            # If the relative position is not within this exon, keep a running\n            # total of the total exonic length-so-far.\n            #\n            # Otherwise, if the relative position is within an exon, get its\n            # offset into that exon by subtracting the exon\"s relative start\n            # position from the relative position. Add that to the total exonic\n            # length-so-far.\n            if exon_unspliced_start <= unspliced_offset <= exon_unspliced_end:\n                # all offsets are base 0, can be used as indices into\n                # sequence string\n                exon_offset = unspliced_offset - exon_unspliced_start\n                return total_spliced_offset + exon_offset\n            else:\n                exon_length = len(exon)  # exon_end_position - exon_start_position + 1\n                total_spliced_offset += exon_length\n        raise ValueError(\n            \"Couldn't find position %d on any exon of %s\" % (\n                position, self.id))", "response": "Convert from an absolute chromosomal position to the offset into the unspliced mRNA."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _contiguous_offsets(self, offsets):\n        offsets.sort()\n        for i in range(len(offsets) - 1):\n            assert offsets[i] + 1 == offsets[i + 1], \\\n                \"Offsets not contiguous: %s\" % (offsets,)\n        return offsets", "response": "Sorts the input list of integer offsets ensures that values are contiguous."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_codon_spliced_offsets(self):\n        offsets = [\n            self.spliced_offset(position)\n            for position\n            in self.start_codon_positions\n        ]\n        return self._contiguous_offsets(offsets)", "response": "Returns the offsets of spliced mRNA nucleotides in start codon."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the set of offsets from start of spliced mRNA transcript of nucleotides in stop codon.", "response": "def stop_codon_spliced_offsets(self):\n        \"\"\"\n        Offsets from start of spliced mRNA transcript\n        of nucleotides in stop codon.\n        \"\"\"\n        offsets = [\n            self.spliced_offset(position)\n            for position\n            in self.stop_codon_positions\n        ]\n        return self._contiguous_offsets(offsets)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the entry is complete.", "response": "def complete(self):\n        \"\"\"\n        Consider a transcript complete if it has start and stop codons and\n        a coding sequence whose length is divisible by 3\n        \"\"\"\n        return (\n            self.contains_start_codon and\n            self.contains_stop_codon and\n            self.coding_sequence is not None and\n            len(self.coding_sequence) % 3 == 0\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coding_sequence(self):\n        if self.sequence is None:\n            return None\n\n        start = self.first_start_codon_spliced_offset\n        end = self.last_stop_codon_spliced_offset\n\n        # If start codon is the at nucleotide offsets [3,4,5] and\n        # stop codon is at nucleotide offsets  [20,21,22]\n        # then start = 3 and end = 22.\n        #\n        # Adding 1 to end since Python uses non-inclusive ends in slices/ranges.\n\n        # pylint: disable=invalid-slice-index\n        # TODO(tavi) Figure out pylint is not happy with this slice\n        return self.sequence[start:end + 1]", "response": "Returns the cDNA coding sequence of the sequence in the nucleotide."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds nearest locus to the given start and end positions.", "response": "def find_nearest_locus(start, end, loci):\n    \"\"\"\n    Finds nearest locus (object with method `distance_to_interval`) to the\n    interval defined by the given `start` and `end` positions.\n    Returns the distance to that locus, along with the locus object itself.\n    \"\"\"\n    best_distance = float(\"inf\")\n    best_locus = None\n    for locus in loci:\n        distance = locus.distance_to_interval(start, end)\n        if best_distance > distance:\n            best_distance = distance\n            best_locus = locus\n    return best_distance, best_locus"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _all_possible_indices(self, column_names):\n        candidate_column_groups = [\n            ['seqname', 'start', 'end'],\n            ['gene_name'],\n            ['gene_id'],\n            ['transcript_id'],\n            ['transcript_name'],\n            ['exon_id'],\n            ['protein_id'],\n            ['ccds_id'],\n        ]\n        indices = []\n        column_set = set(column_names)\n        # Since queries are often restricted by feature type\n        # we should include that column in combination with all\n        # other indices we anticipate might improve performance\n        for column_group in candidate_column_groups:\n            skip = False\n            for column_name in column_group:\n                # some columns, such as 'exon_id',\n                # are not available in all releases of Ensembl (or\n                # other GTFs)\n                if column_name not in column_set:\n                    logger.info(\n                        \"Skipping database index for {%s}\",\n                        \", \".join(column_group))\n                    skip = True\n            if skip:\n                continue\n            indices.append(column_group)\n        return indices", "response": "Create list of tuples containing all possible indices for all possible tables in this database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the name of the primary key for a feature table.", "response": "def _get_primary_key(self, feature_name, feature_df):\n        \"\"\"Name of primary key for a feature table (e.g. \"gene\" -> \"gene_id\")\n\n        Since we're potentially going to run this code over unseen data,\n        make sure that the primary is unique and never null.\n\n        If a feature doesn't have a primary key, return None.\n        \"\"\"\n        if feature_name not in self.PRIMARY_KEY_COLUMNS:\n            return None\n        primary_key = self.PRIMARY_KEY_COLUMNS[feature_name]\n        primary_key_values = feature_df[primary_key]\n        if primary_key_values.isnull().any():\n            raise ValueError(\n                \"Column '%s' can't be primary key of table '%s'\"\n                \" because it contains nulls values\" % (\n                    primary_key, feature_name))\n        elif len(primary_key_values.unique()) < len(primary_key_values):\n            raise ValueError(\n                \"Column '%s' can't be primary key of table '%s'\"\n                \" because it contains repeated values\" % (\n                    primary_key, feature_name))\n        else:\n            return primary_key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchoosing subset of index group tuples from all_index_groups which are the same as the primary key and have non - null values in the feature.", "response": "def _feature_indices(self, all_index_groups, primary_key, feature_df):\n        \"\"\"Choose subset of index group tuples from `all_index_groups` which are\n        applicable to a particular feature (not same as its primary key, have\n        non-null values).\n        \"\"\"\n        # each feature only gets indices if they're *not* the\n        # primary key and have non-null values in the feature's\n        # subset of data\n        result = []\n        for index_group in all_index_groups:\n            # is the index group just a primary key?\n            if len(index_group) == 1 and index_group[0] == primary_key:\n                continue\n            index_column_values = feature_df[index_group]\n            if len(index_column_values.dropna()) == 0:\n                continue\n            result.append(index_group)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the local database and returns a connection to it.", "response": "def create(\n            self,\n            overwrite=False):\n        \"\"\"\n        Create the local database (including indexing) if it's not\n        already set up. If `overwrite` is True, always re-create\n        the database from scratch.\n\n        Returns a connection to the database.\n        \"\"\"\n        logger.info(\"Creating database: %s\", self.local_db_path)\n        df = self._load_gtf_as_dataframe(\n            usecols=self.restrict_gtf_columns,\n            features=self.restrict_gtf_features)\n        all_index_groups = self._all_possible_indices(df.columns)\n\n        if self.restrict_gtf_features:\n            feature_names = self.restrict_gtf_features\n        else:\n            # split single DataFrame into dictionary mapping each unique\n            # feature name onto that subset of the data\n            feature_names = df['feature'].unique()\n        dataframes = {}\n        # every table gets the same set of indices\n        indices_dict = {}\n        # if a feature has an ID then make it that table's primary key\n        primary_keys = {}\n\n        for feature in feature_names:\n            df_subset = df[df.feature == feature]\n            if len(df_subset) == 0:\n                continue\n            dataframes[feature] = df_subset\n\n            primary_key = self._get_primary_key(feature, df_subset)\n            if primary_key:\n                primary_keys[feature] = primary_key\n\n            indices_dict[feature] = self._feature_indices(\n                all_index_groups,\n                primary_key,\n                df_subset)\n\n        self._connection = datacache.db_from_dataframes_with_absolute_path(\n            db_path=self.local_db_path,\n            table_names_to_dataframes=dataframes,\n            table_names_to_primary_keys=primary_keys,\n            table_names_to_indices=indices_dict,\n            overwrite=overwrite,\n            version=DATABASE_SCHEMA_VERSION)\n        return self._connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connection(self):\n        connection = self._get_connection()\n        if connection:\n            return connection\n        else:\n            message = \"GTF database needs to be created\"\n            if self.install_string:\n                message += \", run: %s\" % self.install_string\n            raise ValueError(message)", "response": "Get a connection to the database or raise an exception"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to the database if it does not exist otherwise create it.", "response": "def connect_or_create(self, overwrite=False):\n        \"\"\"\n        Return a connection to the database if it exists, otherwise create it.\n        Overwrite the existing database if `overwrite` is True.\n        \"\"\"\n        connection = self._get_connection()\n        if connection:\n            return connection\n        else:\n            return self.create(overwrite=overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef column_values_at_locus(\n            self,\n            column_name,\n            feature,\n            contig,\n            position,\n            end=None,\n            strand=None,\n            distinct=False,\n            sorted=False):\n        \"\"\"\n        Get the non-null values of a column from the database\n        at a particular range of loci\n        \"\"\"\n\n        # TODO: combine with the query method, since they overlap\n        # significantly\n        require_string(column_name, \"column_name\", nonempty=True)\n\n        contig = normalize_chromosome(contig)\n\n        require_integer(position, \"position\")\n\n        if end is None:\n            end = position\n\n        require_integer(end, \"end\")\n\n        if not self.column_exists(feature, column_name):\n            raise ValueError(\"Table %s doesn't have column %s\" % (\n                feature, column_name,))\n\n        if distinct:\n            distinct_string = \"DISTINCT \"\n        else:\n            distinct_string = \"\"\n\n        query = \"\"\"\n            SELECT %s%s\n            FROM %s\n            WHERE seqname = ?\n            AND start <= ?\n            AND end >= ?\n\n        \"\"\" % (distinct_string, column_name, feature)\n\n        query_params = [contig, end, position]\n\n        if strand:\n            query += \" AND strand = ?\"\n            query_params.append(strand)\n\n        tuples = self.connection.execute(query, query_params).fetchall()\n\n        # each result is a tuple, so pull out its first element\n        results = [t[0] for t in tuples if t[0] is not None]\n\n        if sorted:\n            results.sort()\n        return results", "response": "Get the non - null values of a column from the database at a particular loci."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distinct_column_values_at_locus(\n            self,\n            column,\n            feature,\n            contig,\n            position,\n            end=None,\n            strand=None):\n        \"\"\"\n        Gather all the distinct values for a property/column at some specified\n        locus.\n\n        Parameters\n        ----------\n        column : str\n            Which property are we getting the values of.\n\n        feature : str\n            Which type of entry (e.g. transcript, exon, gene) is the property\n            associated with?\n\n        contig : str\n            Chromosome or unplaced contig name\n\n        position : int\n            Chromosomal position\n\n        end : int, optional\n            End position of a range, if unspecified assume we're only looking\n            at the single given position.\n\n        strand : str, optional\n            Either the positive ('+') or negative strand ('-'). If unspecified\n            then check for values on either strand.\n        \"\"\"\n        return self.column_values_at_locus(\n            column,\n            feature,\n            contig,\n            position,\n            end=end,\n            strand=strand,\n            distinct=True,\n            sorted=True)", "response": "Returns a list of distinct column values for a property or column at some specified locus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning an arbitrary SQL query against the database and return the results.", "response": "def run_sql_query(self, sql, required=False, query_params=[]):\n        \"\"\"\n        Given an arbitrary SQL query, run it against the database\n        and return the results.\n\n        Parameters\n        ----------\n        sql : str\n            SQL query\n\n        required : bool\n            Raise an error if no results found in the database\n\n        query_params : list\n            For each '?' in the query there must be a corresponding value in\n            this list.\n        \"\"\"\n        try:\n            cursor = self.connection.execute(sql, query_params)\n        except sqlite3.OperationalError as e:\n            error_message = e.message if hasattr(e, 'message') else str(e)\n            logger.warn(\n                \"Encountered error \\\"%s\\\" from query \\\"%s\\\" with parameters %s\",\n                error_message,\n                sql,\n                query_params)\n            raise\n        results = cursor.fetchall()\n        if required and not results:\n            raise ValueError(\n                \"No results found for query:\\n%s\\nwith parameters: %s\" % (\n                    sql, query_params))\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery the database for a specific entry in the cache.", "response": "def query(\n            self,\n            select_column_names,\n            filter_column,\n            filter_value,\n            feature,\n            distinct=False,\n            required=False):\n        \"\"\"\n        Construct a SQL query and run against the sqlite3 database,\n        filtered both by the feature type and a user-provided column/value.\n        \"\"\"\n        sql = \"\"\"\n            SELECT %s%s\n            FROM %s\n            WHERE %s = ?\n        \"\"\" % (\"distinct \" if distinct else \"\",\n               \", \".join(select_column_names),\n               feature,\n               filter_column)\n        query_params = [filter_value]\n        return self.run_sql_query(\n            sql, required=required, query_params=query_params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query_feature_values(\n            self,\n            column,\n            feature,\n            distinct=True,\n            contig=None,\n            strand=None):\n        \"\"\"\n        Run a SQL query against the sqlite3 database, filtered\n        only on the feature type.\n        \"\"\"\n        query = \"\"\"\n            SELECT %s%s\n            FROM %s\n            WHERE 1=1\n        \"\"\" % (\"DISTINCT \" if distinct else \"\", column, feature)\n        query_params = []\n\n        if contig:\n            contig = normalize_chromosome(contig)\n            query += \" AND seqname = ?\"\n            query_params.append(contig)\n\n        if strand:\n            strand = normalize_strand(strand)\n            query += \" AND strand = ?\"\n            query_params.append(strand)\n\n        rows = self.run_sql_query(query, query_params=query_params)\n        return [row[0] for row in rows if row is not None]", "response": "Query the feature values of the specified feature type from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries for loci satisfying a given filter and feature type.", "response": "def query_loci(self, filter_column, filter_value, feature):\n        \"\"\"\n        Query for loci satisfying a given filter and feature type.\n\n\n        Parameters\n        ----------\n        filter_column : str\n            Name of column to filter results by.\n\n        filter_value : str\n            Only return loci which have this value in the their filter_column.\n\n        feature : str\n            Feature names such as 'transcript', 'gene', and 'exon'\n\n        Returns list of Locus objects\n        \"\"\"\n        # list of values containing (contig, start, stop, strand)\n        result_tuples = self.query(\n            select_column_names=[\"seqname\", \"start\", \"end\", \"strand\"],\n            filter_column=filter_column,\n            filter_value=filter_value,\n            feature=feature,\n            distinct=True,\n            required=True)\n        return [\n            Locus(contig, start, end, strand)\n            for (contig, start, end, strand)\n            in result_tuples\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries for unique locus in the database.", "response": "def query_locus(self, filter_column, filter_value, feature):\n        \"\"\"\n        Query for unique locus, raises error if missing or more than\n        one locus in the database.\n\n        Parameters\n        ----------\n        filter_column : str\n            Name of column to filter results by.\n\n        filter_value : str\n            Only return loci which have this value in the their filter_column.\n\n        feature : str\n            Feature names such as 'transcript', 'gene', and 'exon'\n\n        Returns single Locus object.\n        \"\"\"\n        loci = self.query_loci(\n            filter_column=filter_column,\n            filter_value=filter_value,\n            feature=feature)\n\n        if len(loci) == 0:\n            raise ValueError(\"Couldn't find locus for %s with %s = %s\" % (\n                feature, filter_column, filter_value))\n        elif len(loci) > 1:\n            raise ValueError(\"Too many loci for %s with %s = %s: %s\" % (\n                feature, filter_column, filter_value, loci))\n        return loci[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses this genome source s GTF file and load it as a Pandas DataFrame.", "response": "def _load_gtf_as_dataframe(self, usecols=None, features=None):\n        \"\"\"\n        Parse this genome source's GTF file and load it as a Pandas DataFrame\n        \"\"\"\n        logger.info(\"Reading GTF from %s\", self.gtf_path)\n        df = read_gtf(\n            self.gtf_path,\n            column_converters={\n                \"seqname\": normalize_chromosome,\n                \"strand\": normalize_strand,\n            },\n            infer_biotype_column=True,\n            usecols=usecols,\n            features=features)\n\n        column_names = set(df.keys())\n        expect_gene_feature = features is None or \"gene\" in features\n        expect_transcript_feature = features is None or \"transcript\" in features\n        observed_features = set(df[\"feature\"])\n\n        # older Ensembl releases don't have \"gene\" or \"transcript\"\n        # features, so fill in those rows if they're missing\n        if expect_gene_feature and \"gene\" not in observed_features:\n            # if we have to reconstruct gene feature rows then\n            # fill in values for 'gene_name' and 'gene_biotype'\n            # but only if they're actually present in the GTF\n            logger.info(\"Creating missing gene features...\")\n            df = create_missing_features(\n                dataframe=df,\n                unique_keys={\"gene\": \"gene_id\"},\n                extra_columns={\n                    \"gene\": {\n                        \"gene_name\",\n                        \"gene_biotype\"\n                    }.intersection(column_names),\n                },\n                missing_value=\"\")\n            logger.info(\"Done.\")\n\n        if expect_transcript_feature and \"transcript\" not in observed_features:\n            logger.info(\"Creating missing transcript features...\")\n            df = create_missing_features(\n                dataframe=df,\n                unique_keys={\"transcript\": \"transcript_id\"},\n                extra_columns={\n                    \"transcript\": {\n                        \"gene_id\",\n                        \"gene_name\",\n                        \"gene_biotype\",\n                        \"transcript_name\",\n                        \"transcript_biotype\",\n                        \"protein_id\",\n                    }.intersection(column_names)\n                },\n                missing_value=\"\")\n            logger.info(\"Done.\")\n\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transcripts(self):\n        transcript_id_results = self.db.query(\n            select_column_names=['transcript_id'],\n            filter_column='gene_id',\n            filter_value=self.id,\n            feature='transcript',\n            distinct=False,\n            required=False)\n\n        # We're doing a SQL query for each transcript ID to fetch\n        # its particular information, might be more efficient if we\n        # just get all the columns here, but how do we keep that modular?\n        return [\n            self.genome.transcript_by_id(result[0])\n            for result in transcript_id_results\n        ]", "response": "Property which dynamically construct transcript objects for all the transcript IDs associated with this gene."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvotes an item in a single object", "response": "def vote(self, something, vote='up'):\n        \"\"\"\u7ed9\u7b54\u6848\u6216\u6587\u7ae0\u70b9\u8d5e\u6216\u53d6\u6d88\u70b9\u8d5e\n\n        :param Answer/Post something: \u9700\u8981\u70b9\u8d5e\u7684\u7b54\u6848\u6216\u6587\u7ae0\u5bf9\u8c61\n        :param str vote:\n            ===== ================ ======\n            \u53d6\u503c        \u8bf4\u660e       \u9ed8\u8ba4\u503c\n            ===== ================ ======\n            up    \u8d5e\u540c              \u221a\n            down  \u53cd\u5bf9              X\n            clear \u65e2\u4e0d\u8d5e\u540c\u4e5f\u4e0d\u53cd\u5bf9  X\n            ===== ================ ======\n\n        :return: \u6210\u529f\u8fd4\u56deTrue\uff0c\u5931\u8d25\u8fd4\u56deFalse\n        :rtype: bool\n        \"\"\"\n        from .answer import Answer\n        from zhihu import Post\n        if isinstance(something, Answer):\n            mapping = {\n                'up': 'vote_up',\n                'clear': 'vote_neutral',\n                'down': 'vote_down'\n            }\n            if vote not in mapping.keys():\n                raise ValueError('Invalid vote value: {0}'.format(vote))\n            if something.author.url == self.url:\n                return False\n            params = {'answer_id': str(something.aid)}\n            data = {\n                '_xsrf': something.xsrf,\n                'method': mapping[vote],\n                'params': json.dumps(params)\n            }\n            headers = dict(Default_Header)\n            headers['Referer'] = something.question.url[:-1]\n            res = self._session.post(Upvote_Answer_Url,\n                                     headers=headers, data=data)\n            return res.json()['r'] == 0\n        elif isinstance(something, Post):\n            mapping = {\n                'up': 'like',\n                'clear': 'none',\n                'down': 'dislike'\n            }\n            if vote not in mapping.keys():\n                raise ValueError('Invalid vote value: {0}'.format(vote))\n            if something.author.url == self.url:\n                return False\n            put_url = Upvote_Article_Url.format(\n                something.column_in_name, something.slug)\n            data = {'value': mapping[vote]}\n            headers = {\n                'Content-Type': 'application/json;charset=utf-8',\n                'Host': 'zhuanlan.zhihu.com',\n                'Referer': something.url[:-1],\n                'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; '\n                              'rv:39.0) Gecko/20100101 Firefox/39.0',\n                'X-XSRF-TOKEN': self._session.cookies.get('XSRF-TOKEN')\n            }\n            res = self._session.put(put_url, json.dumps(data), headers=headers)\n            return res.status_code == 204\n        else:\n            raise ValueError('argument something need to be '\n                             'zhihu.Answer or zhihu.Post object.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfollowing an author question or topic.", "response": "def follow(self, something, follow=True):\n        \"\"\"\u5173\u6ce8\u7528\u6237\u3001\u95ee\u9898\u3001\u8bdd\u9898\u6216\u6536\u85cf\u5939\n\n        :param Author/Question/Topic something: \u9700\u8981\u5173\u6ce8\u7684\u5bf9\u8c61\n        :param bool follow: True-->\u5173\u6ce8\uff0cFalse-->\u53d6\u6d88\u5173\u6ce8\n        :return: \u6210\u529f\u8fd4\u56deTrue\uff0c\u5931\u8d25\u8fd4\u56deFalse\n        :rtype: bool\n        \"\"\"\n        from .question import Question\n        from .topic import Topic\n        from .collection import Collection\n        if isinstance(something, Author):\n            if something.url == self.url:\n                return False\n            data = {\n                '_xsrf': something.xsrf,\n                'method': '\tfollow_member' if follow else 'unfollow_member',\n                'params': json.dumps({'hash_id': something.hash_id})\n            }\n            res = self._session.post(Follow_Author_Url, data=data)\n            return res.json()['r'] == 0\n        elif isinstance(something, Question):\n            data = {\n                '_xsrf': something.xsrf,\n                'method': 'follow_question' if follow else 'unfollow_question',\n                'params': json.dumps({'question_id': str(something.qid)})\n            }\n            res = self._session.post(Follow_Question_Url, data=data)\n            return res.json()['r'] == 0\n        elif isinstance(something, Topic):\n            data = {\n                '_xsrf': something.xsrf,\n                'method': 'follow_topic' if follow else 'unfollow_topic',\n                'params': json.dumps({'topic_id': something.tid})\n            }\n            res = self._session.post(Follow_Topic_Url, data=data)\n            return res.json()['r'] == 0\n        elif isinstance(something, Collection):\n            data = {\n                '_xsrf': something.xsrf,\n                'favlist_id': something.cid\n            }\n            res = self._session.post(\n                Follow_Collection_Url if follow else Unfollow_Collection_Url,\n                data=data)\n            return res.json()['r'] == 0\n        else:\n            raise ValueError('argument something need to be '\n                             'zhihu.Author, zhihu.Question'\n                             ', Zhihu.Topic or Zhihu.Collection object.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd comment to an answer", "response": "def add_comment(self, answer, content):\n        \"\"\"\u7ed9\u6307\u5b9a\u7b54\u6848\u6dfb\u52a0\u8bc4\u8bba\n\n        :param Answer answer: \u7b54\u6848\u5bf9\u8c61\n        :param string content: \u8bc4\u8bba\u5185\u5bb9\n        :return: \u6210\u529f\u8fd4\u56de True\uff0c\u5931\u8d25\u8fd4\u56de False\n        :rtype: bool\n        \"\"\"\n\n        from .answer import Answer\n        if isinstance(answer, Answer) is False:\n            raise ValueError('argument answer need to be Zhihu.Answer object.')\n        if not content:\n            raise ValueError('answer content cannot be empty')\n        data = {\n            'method': 'add_comment',\n            'params': json.dumps({'answer_id': answer.aid, 'content': content}),\n            '_xsrf': answer.xsrf\n            }\n        res = self._session.post(Answer_Add_Comment_URL,\n                                 data=data)\n        return res.json()['r'] == 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_message(self, author, content):\n        if isinstance(author, Author) is False:\n            raise ValueError('argument answer need to be Zhihu.Author object.')\n        if not content:\n            raise ValueError('answer content cannot be empty')\n        if author.url == self.url:\n                return False\n        data = {\n            'member_id': author.hash_id,\n            'content': content,\n            'token': '',\n            '_xsrf': author.xsrf\n        }\n        res = self._session.post(Send_Message_Url,\n                                 data=data)\n        return res.json()['r'] == 0", "response": "Send a message to the specified author."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef block(self, something, block=True):\n        from .topic import Topic\n\n        if isinstance(something, Author):\n\n            if something.url == self.url:\n                return False\n            data = {\n                '_xsrf': something.xsrf,\n                'action': 'add' if block else 'cancel',\n            }\n            block_author_url = something.url + 'block'\n            res = self._session.post(block_author_url, data=data)\n            return res.json()['r'] == 0\n        elif isinstance(something, Topic):\n            tid = something.tid\n            data = {\n                '_xsrf': something.xsrf,\n                'method': 'add' if block else 'del',\n                'tid': tid,\n            }\n            block_topic_url = 'http://www.zhihu.com/topic/ignore'\n            res = self._session.post(block_topic_url, data=data)\n            return res.status_code == 200\n        else:\n            raise ValueError('argument something need to be '\n                             'Zhihu.Author or Zhihu.Topic object.')", "response": "Block the Zhihu instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef column(self):\n        from .column import Column\n\n        if 'column' in self.soup:\n            url = Column_Url + '/' + self.soup['column']['slug']\n            name = self.soup['column']['name']\n            return Column(url, name, session=self._session)\n        else:\n            return None", "response": "\u83b7\u53d6\u6587\u7ae0\u6240\u5728\u4e13\u680f.\n\n        :return: \u6587\u7ae0\u6240\u5728\u4e13\u680f\n        :rtype: Column"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef author(self):\n        from .author import Author\n\n        url = self.soup['author']['profileUrl']\n        name = self.soup['author']['name']\n        motto = self.soup['author']['bio']\n        template = self.soup['author']['avatar']['template']\n        photo_id = self.soup['author']['avatar']['id']\n        photo_url = template.format(id=photo_id, size='r')\n        return Author(url, name, motto, photo_url=photo_url,\n                      session=self._session)", "response": "\u83b7\u53d6\u6587\u7ae0\u4f5c\u8005.\n\n        :return: \u6587\u7ae0\u4f5c\u8005\n        :rtype: Author"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, filepath=None, filename=None, mode=\"md\"):\n        if mode not in [\"html\", \"md\", \"markdown\"]:\n            raise ValueError(\"`mode` must be 'html', 'markdown' or 'md',\"\n                             \" got {0}\".format(mode))\n        self._make_soup()\n        file = get_path(filepath, filename, mode, self.column.name,\n                        self.title + '-' + self.author.name)\n        with open(file, 'wb') as f:\n            if mode == \"html\":\n                f.write(self.soup['content'].encode('utf-8'))\n            else:\n                import html2text\n                h2t = html2text.HTML2Text()\n                h2t.body_width = 0\n                f.write(h2t.handle(self.soup['content']).encode('utf-8'))", "response": "Save the current state of the object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef author(self):\n        from .author import Author\n\n        author = self.soup.find('div', class_='zm-item-answer-author-info')\n        url, name, motto, photo = parser_author_from_tag(author)\n        if name == '\u533f\u540d\u7528\u6237':\n            return ANONYMOUS\n        else:\n            return Author(url, name, motto, photo_url=photo,\n                          session=self._session)", "response": "Get the Author object from the BeautifulSoup object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upvoters(self):\n        self._make_soup()\n        next_req = '/answer/' + str(self.aid) + '/voters_profile'\n        while next_req != '':\n            data = self._session.get(Zhihu_URL + next_req).json()\n            next_req = data['paging']['next']\n            for html in data['payload']:\n                soup = BeautifulSoup(html)\n                yield self._parse_author_soup(soup)", "response": "Get all voters from Zhihu and yield author. Iterable"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting content of answer", "response": "def content(self):\n        \"\"\"\u4ee5\u5904\u7406\u8fc7\u7684Html\u4ee3\u7801\u5f62\u5f0f\u8fd4\u56de\u7b54\u6848\u5185\u5bb9.\n\n        :return: \u7b54\u6848\u5185\u5bb9\n        :rtype: str\n        \"\"\"\n        answer_wrap = self.soup.find('div', id='zh-question-answer-wrap')\n        content = answer_wrap.find('div', class_='zm-editable-content')\n        content = answer_content_process(content)\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting num of click answer", "response": "def collect_num(self):\n        \"\"\"\u83b7\u53d6\u7b54\u6848\u6536\u85cf\u6570\n\n        :return:  \u7b54\u6848\u6536\u85cf\u6570\u91cf\n        :rtype: int\n        \"\"\"\n        element = self.soup.find(\"a\", {\n            \"data-za-a\": \"click_answer_collected_count\"\n        })\n        if element is None:\n            return 0\n        else:\n            return int(element.get_text())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of all the available collections.", "response": "def collections(self):\n        \"\"\"\u83b7\u53d6\u5305\u542b\u8be5\u7b54\u6848\u7684\u6536\u85cf\u5939\n\n        :return: \u5305\u542b\u8be5\u7b54\u6848\u7684\u6536\u85cf\u5939\n        :rtype: Collection.Iterable\n\n        collect_num \u672a\u5fc5\u7b49\u4e8e len(collections)\uff0c\u6bd4\u5982:\n        https://www.zhihu.com/question/20064699/answer/13855720\n        \u663e\u793a\u88ab\u6536\u85cf 38 \u6b21\uff0c\u4f46\u53ea\u6709 30 \u4e2a\u6536\u85cf\u5939\n        \"\"\"\n        import time\n        gotten_feed_num = 20\n        offset = 0\n        data = {\n            'method':'next',\n            '_xsrf': self.xsrf\n        }\n        while gotten_feed_num >= 10:\n            data['params'] = \"{\\\"answer_url\\\": %d,\\\"offset\\\": %d}\" % (self.id, offset)\n            res = self._session.post(url=Get_Collection_Url, data=data)\n            gotten_feed_num = len(res.json()['msg'])\n            offset += gotten_feed_num\n            soup = BeautifulSoup(''.join(res.json()['msg']))\n            for zm_item in soup.find_all('div', class_='zm-item'):\n                url = Zhihu_URL + zm_item.h2.a['href']\n                name = zm_item.h2.a.text\n                links = zm_item.div.find_all('a')\n                owner = Author(links[0]['href'], session=self._session)\n                follower_num = int(links[1].text.split()[0])\n                yield Collection(url, owner=owner, name=name,\n                                 follower_num=follower_num,\n                                 session=self._session)\n\n            time.sleep(0.2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting comments from the Answer.", "response": "def comments(self):\n        \"\"\"\u83b7\u53d6\u7b54\u6848\u4e0b\u7684\u6240\u6709\u8bc4\u8bba.\n\n        :return: \u7b54\u6848\u4e0b\u7684\u6240\u6709\u8bc4\u8bba\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Comments.Iterable\n        \"\"\"\n        import math\n        from .author import Author, ANONYMOUS\n        from .comment import Comment\n\n        api_url = Get_Answer_Comment_URL.format(self.aid)\n        page = pages = 1\n        while page <= pages:\n            res = self._session.get(api_url + '?page=' + str(page))\n            if page == 1:\n                total = int(res.json()['paging']['totalCount'])\n                if total == 0:\n                    return\n                pages = math.ceil(total / 30) \n            page += 1\n\n            comment_items = res.json()['data']\n            for comment_item in comment_items:\n                comment_id = comment_item['id']\n                content = comment_item['content']\n                upvote_num = comment_item['likesCount']\n                time_string = comment_item['createdTime'][:19]\n                time = datetime.strptime(time_string, \"%Y-%m-%dT%H:%M:%S\")\n\n                if comment_item['author'].get('url') is not None:\n                    a_url = comment_item['author']['url']\n                    a_name = comment_item['author']['name']\n                    photo_url_tmp = comment_item['author']['avatar']['template']\n                    photo_url_id = comment_item['author']['avatar']['id']\n                    a_photo_url = photo_url_tmp.replace(\n                            '{id}', photo_url_id).replace('_{size}', '')\n                    author_obj = Author(a_url, a_name, photo_url=a_photo_url,\n                                        session=self._session)\n                else:\n                    author_obj = ANONYMOUS\n\n                yield Comment(comment_id, self, author_obj, upvote_num, content, time)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef refresh(self):\n        super().refresh()\n        self._html = None\n        self._upvote_num = None\n        self._content = None\n        self._collect_num = None\n        self._comment_num = None", "response": "\u5237\u65b0 Answer object \u7684\u5c5e\u6027. \n        \u4f8b\u5982\u8d5e\u540c\u6570\u589e\u52a0\u4e86, \u5148\u8c03\u7528 ``refresh()`` \n        \u518d\u8bbf\u95ee upvote_num\u5c5e\u6027, \u53ef\u83b7\u5f97\u66f4\u65b0\u540e\u7684\u8d5e\u540c\u6570.\n        \n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef follower_num(self):\n        follower_num_block = self.soup.find('div', class_='zg-gray-normal')\n        # \u65e0\u4eba\u5173\u6ce8\u65f6 \u627e\u4e0d\u5230\u5bf9\u5e94block\uff0c\u76f4\u63a5\u8fd4\u56de0 \uff08\u611f\u8c22\u77e5\u4e4e\u7528\u6237 \u6bb5\u6653\u6668 \u63d0\u51fa\u6b64\u95ee\u9898\uff09\n        if follower_num_block is None or follower_num_block.strong is None:\n            return 0\n        return int(follower_num_block.strong.text)", "response": "\u83b7\u53d6\u95ee\u9898\u5173\u6ce8\u4eba\u6570.\n\n        :return: \u95ee\u9898\u5173\u6ce8\u4eba\u6570\n        :rtype: int"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of topics", "response": "def topics(self):\n        \"\"\"\u83b7\u53d6\u95ee\u9898\u6240\u5c5e\u8bdd\u9898.\n\n        :return: \u95ee\u9898\u6240\u5c5e\u8bdd\u9898\n        :rtype: Topic.Iterable\n        \"\"\"\n        from .topic import Topic\n\n        for topic in self.soup.find_all('a', class_='zm-item-tag'):\n            yield Topic(Zhihu_URL + topic['href'], topic.text.replace('\\n', ''),\n                        session=self._session)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef followers(self):\n        self._make_soup()\n        followers_url = self.url + 'followers'\n        for x in common_follower(followers_url, self.xsrf, self._session):\n            yield x", "response": "\u83b7\u53d6\u5173\u6ce8\u6b64\u95ee\u9898\u7684\u7528\u6237\n\n        :return: \u5173\u6ce8\u6b64\u95ee\u9898\u7684\u7528\u6237\n        :rtype: Author.Iterable\n        :\u95ee\u9898: \u8981\u6ce8\u610f\u82e5\u6267\u884c\u8fc7\u7a0b\u4e2d\u53e6\u5916\u6709\u4eba\u5173\u6ce8\uff0c\u53ef\u80fd\u9020\u6210\u91cd\u590d\u83b7\u53d6\u5230\u67d0\u4e9b\u7528\u6237"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting answers from a url.", "response": "def answers(self):\n        \"\"\"\u83b7\u53d6\u95ee\u9898\u7684\u6240\u6709\u7b54\u6848.\n\n        :return: \u95ee\u9898\u7684\u6240\u6709\u7b54\u6848\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Answer.Iterable\n        \"\"\"\n        from .author import Author\n        from .answer import Answer\n\n        self._make_soup()\n\n        # TODO: \u7edf\u4e00\u903b\u8f91. \u5b8c\u5168\u53ef\u4ee5\u90fd\u7528 _parse_answer_html \u7684\u903b\u8f91\u66ff\u6362\n        if self._url.endswith('sort=created'):\n            pager = self.soup.find('div', class_='zm-invite-pager')\n            if pager is None:\n                max_page = 1\n            else:\n                max_page = int(pager.find_all('span')[-2].a.text)\n\n            for page in range(1, max_page + 1):\n                if page == 1:\n                    soup = self.soup\n                else:\n                    url = self._url + '&page=%d' % page\n                    soup = BeautifulSoup(self._session.get(url).content)\n                error_answers = soup.find_all('div', id='answer-status')\n                for each in error_answers:\n                    each['class'] = 'zm-editable-content'\n                answers_wrap = soup.find('div', id='zh-question-answer-wrap')\n                # \u6b63\u5f0f\u5904\u7406\n                authors = answers_wrap.find_all(\n                    'div', class_='zm-item-answer-author-info')\n                urls = answers_wrap.find_all('a', class_='answer-date-link')\n                up_num = answers_wrap.find_all('div',\n                                               class_='zm-item-vote-info')\n                contents = answers_wrap.find_all(\n                    'div', class_='zm-editable-content')\n                assert len(authors) == len(urls) == len(up_num) == len(\n                    contents)\n                for author, url, up_num, content in \\\n                        zip(authors, urls, up_num, contents):\n                    a_url, name, motto, photo = parser_author_from_tag(author)\n                    author_obj = Author(a_url, name, motto, photo_url=photo,\n                                        session=self._session)\n                    url = Zhihu_URL + url['href']\n                    up_num = int(up_num['data-votecount'])\n                    content = answer_content_process(content)\n                    yield Answer(url, self, author_obj, up_num, content,\n                                 session=self._session)\n        else:\n            pagesize = 10\n            new_header = dict(Default_Header)\n            new_header['Referer'] = self.url\n            params = {\"url_token\": self.id,\n                      'pagesize': pagesize,\n                      'offset': 0}\n            data = {'_xsrf': self.xsrf,\n                    'method': 'next',\n                    'params': ''}\n            for i in range(0, (self.answer_num - 1) // pagesize + 1):\n                if i == 0:\n                    # \u4fee\u6b63\u5404\u79cd\u5efa\u8bae\u4fee\u6539\u7684\u56de\u7b54\u2026\u2026\n                    error_answers = self.soup.find_all('div',\n                                                       id='answer-status')\n                    for each in error_answers:\n                        each['class'] = 'zm-editable-content'\n                    answers_wrap = self.soup.find('div',\n                                                  id='zh-question-answer-wrap')\n                    # \u6b63\u5f0f\u5904\u7406\n                    authors = answers_wrap.find_all(\n                        'div', class_='zm-item-answer-author-info')\n                    urls = answers_wrap.find_all('a', class_='answer-date-link')\n                    up_num = answers_wrap.find_all('div',\n                                                   class_='zm-item-vote-info')\n                    contents = answers_wrap.find_all(\n                        'div', class_='zm-editable-content')\n                    assert len(authors) == len(urls) == len(up_num) == len(\n                        contents)\n                    for author, url, up_num, content in \\\n                            zip(authors, urls, up_num, contents):\n                        a_url, name, motto, photo = parser_author_from_tag(\n                            author)\n                        author_obj = Author(a_url, name, motto, photo_url=photo,\n                                            session=self._session)\n                        url = Zhihu_URL + url['href']\n                        up_num = int(up_num['data-votecount'])\n                        content = answer_content_process(content)\n                        yield Answer(url, self, author_obj, up_num, content,\n                                     session=self._session)\n                else:\n                    params['offset'] = i * pagesize\n                    data['params'] = json.dumps(params)\n                    r = self._session.post(Question_Get_More_Answer_URL,\n                                           data=data,\n                                           headers=new_header)\n                    answer_list = r.json()['msg']\n                    for answer_html in answer_list:\n                        yield self._parse_answer_html(answer_html)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the top i answer", "response": "def top_i_answer(self, i):\n        \"\"\"\u83b7\u53d6\u6392\u540d\u67d0\u4e00\u4f4d\u7684\u7b54\u6848.\n\n        :param int i: \u8981\u83b7\u53d6\u7684\u7b54\u6848\u7684\u6392\u540d\n        :return: \u7b54\u6848\u5bf9\u8c61\uff0c\u80fd\u76f4\u63a5\u83b7\u53d6\u7684\u5c5e\u6027\u53c2\u89c1answers\u65b9\u6cd5\n        :rtype: Answer\n        \"\"\"\n        for j, a in enumerate(self.answers):\n            if j == i - 1:\n                return a"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef top_i_answers(self, i):\n        for j, a in enumerate(self.answers):\n            if j <= i - 1:\n                yield a\n            else:\n                return", "response": "return an iterable of all the top i \u83b7\u53d6 \u6392\u540d\u5728\u524d\u51e0\u4f4d\u7684\u7b54\u6848."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the author object.", "response": "def author(self):\n        \"\"\"\u83b7\u53d6\u95ee\u9898\u7684\u63d0\u95ee\u8005.\n        \n        :return: \u63d0\u95ee\u8005\n        :rtype: Author or zhihu.ANONYMOUS\n        \"\"\"\n        from .author import Author, ANONYMOUS\n\n        logs = self._query_logs()\n        author_a = logs[-1].find_all('div')[0].a\n        if author_a.text == '\u533f\u540d\u7528\u6237':\n            return ANONYMOUS\n        else:\n            url = Zhihu_URL + author_a['href']\n            return Author(url, name=author_a.text, session=self._session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the creation time of the item.", "response": "def creation_time(self):\n        \"\"\"\n        :return: \u95ee\u9898\u521b\u5efa\u65f6\u95f4\n        :rtype: datetime.datetime\n        \"\"\"\n        logs = self._query_logs()\n        time_string = logs[-1].find('div', class_='zm-item-meta').time[\n            'datetime']\n        return datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef last_edit_time(self):\n        data = {'_xsrf': self.xsrf, 'offset': '1'}\n        res = self._session.post(self.url + 'log', data=data)\n        _, content = res.json()['msg']\n        soup = BeautifulSoup(content)\n        time_string = soup.find_all('time')[0]['datetime']\n        return datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")", "response": "Get last edit time"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting Id \u83b7\u53d6\u6536\u85cf\u5185\u90e8Id\uff08\u7528\u4e0d\u5230\u5ffd\u89c6\u5c31\u597d\uff09 Id = > \u5185\u90e8Id", "response": "def cid(self):\n        \"\"\"\u83b7\u53d6\u6536\u85cf\u5939\u5185\u90e8Id\uff08\u7528\u4e0d\u5230\u5ffd\u89c6\u5c31\u597d\uff09\n\n        :return: \u5185\u90e8Id\n        :rtype: int\n        \"\"\"\n        return int(re_get_number.match(\n            self.soup.find('a', attrs={'name': 'focus'})['id']).group(1))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the name of the object", "response": "def name(self):\n        \"\"\"\u83b7\u53d6\u6536\u85cf\u5939\u540d\u5b57.\n\n        :return: \u6536\u85cf\u5939\u540d\u5b57\n        :rtype: str\n        \"\"\"\n        return re_del_empty_line.match(\n            self.soup.find('h2', id='zh-fav-head-title').text).group(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef owner(self):\n        from .author import Author\n\n        a = self.soup.find('h2', class_='zm-list-content-title').a\n        name = a.text\n        url = Zhihu_URL + a['href']\n        motto = self.soup.find(\n            'div', id='zh-single-answer-author-info').div.text\n        photo_url = PROTOCOL + self.soup.find(\n            'img', class_='zm-list-avatar-medium')['src'].replace('_m', '_r')\n        return Author(url, name, motto, photo_url=photo_url,\n                      session=self._session)", "response": "Get the author object from the soup."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef follower_num(self):\n        href = re_collection_url_split.match(self.url).group(1)\n        return int(self.soup.find('a', href=href + 'followers').text)", "response": "get n th follower"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting questions from page", "response": "def questions(self):\n        \"\"\"\u83b7\u53d6\u6536\u85cf\u5939\u5185\u6240\u6709\u95ee\u9898\u5bf9\u8c61.\n\n        :return: \u6536\u85cf\u5939\u5185\u6240\u6709\u95ee\u9898\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Question.Iterable\n        \"\"\"\n        self._make_soup()\n        # noinspection PyTypeChecker\n        for question in self._page_get_questions(self.soup):\n            yield question\n        i = 2\n        while True:\n            soup = BeautifulSoup(self._session.get(\n                self.url[:-1] + '?page=' + str(i)).text)\n            for question in self._page_get_questions(soup):\n                if question == 0:\n                    return\n                yield question\n            i += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef answers(self):\n        self._make_soup()\n        # noinspection PyTypeChecker\n        for answer in self._page_get_answers(self.soup):\n            yield answer\n        i = 2\n        while True:\n            soup = BeautifulSoup(self._session.get(\n                self.url[:-1] + '?page=' + str(i)).text)\n            for answer in self._page_get_answers(soup):\n                if answer == 0:\n                    return\n                yield answer\n            i += 1", "response": "\u83b7\u53d6\u6536\u85cf\u5939\u5185\u6240\u6709\u7b54\u6848\u5bf9\u8c61.\n\n        :return: \u6536\u85cf\u5939\u5185\u6240\u6709\u7b54\u6848\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Answer.Iterable"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logs(self):\n        import time\n        from datetime import datetime\n        from .answer import Answer\n        from .question import Question\n        from .acttype import CollectActType\n\n        self._make_soup()\n        gotten_feed_num = 20\n        offset = 0\n        data = {\n            'start': 0,\n            '_xsrf': self.xsrf\n        }\n        api_url = self.url + 'log'\n        while gotten_feed_num == 20:\n            data['offset'] = offset\n            res = self._session.post(url=api_url, data=data)\n            gotten_feed_num = res.json()['msg'][0]\n            soup = BeautifulSoup(res.json()['msg'][1])\n            offset += gotten_feed_num\n            zm_items = soup.find_all('div', class_='zm-item')\n\n            for zm_item in zm_items:\n                act_time = datetime.strptime(zm_item.find('time').text, \"%Y-%m-%d %H:%M:%S\")\n                if zm_item.find('ins'):\n                    link = zm_item.find('ins').a\n                    act_type = CollectActType.INSERT_ANSWER\n                elif zm_item.find('del'):\n                    link = zm_item.find('del').a\n                    act_type = CollectActType.DELETE_ANSWER\n                else:\n                    continue\n                try:\n                    answer_url = Zhihu_URL + link['href']\n                    question_url = re_a2q.match(answer_url).group(1)\n                    question = Question(question_url, link.text)\n                    answer = Answer(\n                        answer_url, question, session=self._session)\n                    yield CollectActivity(\n                        act_type, act_time, self.owner, self, answer)\n                except AttributeError:\n                    act_type = CollectActType.CREATE_COLLECTION\n                    yield CollectActivity(\n                        act_type, act_time, self.owner, self)\n            data['start'] = zm_items[-1]['id'][8:]\n            time.sleep(0.5)", "response": "Get all the logs in the current session."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef posts(self):\n        origin_host = self._session.headers.get('Host')\n        for offset in range(0, (self.post_num - 1) // 10 + 1):\n            self._session.headers.update(Host='zhuanlan.zhihu.com')\n            res = self._session.get(\n                Column_Posts_Data.format(self._in_name, offset * 10))\n            soup = res.json()\n            self._session.headers.update(Host=origin_host)\n            for post in soup:\n                yield self._parse_post_data(post)", "response": "get posts from \u4e13\u680f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove invalid characters from the text.", "response": "def remove_invalid_char(text):\n    \"\"\"\u53bb\u9664\u5b57\u7b26\u4e32\u4e2d\u7684\u65e0\u6548\u5b57\u7b26\uff0c\u4e00\u822c\u7528\u4e8e\u4fdd\u5b58\u6587\u4ef6\u65f6\u4fdd\u8bc1\u6587\u4ef6\u540d\u7684\u6709\u6548\u6027.\n\n    :param str text: \u5f85\u5904\u7406\u7684\u5b57\u7b26\u4e32\n    :return: \u5904\u7406\u540e\u7684\u5b57\u7b26\u4e32\n    :rtype: str\n    \"\"\"\n    invalid_char_list = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', '\\n']\n    res = ''\n    for char in text:\n        if char not in invalid_char_list:\n            res += char\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclones a bs4 tag before modifying it.", "response": "def clone_bs4_elem(el):\n    \"\"\"Clone a bs4 tag before modifying it.\n\n    Code from `http://stackoverflow.com/questions/23057631/clone-element-with\n    -beautifulsoup`\n    \"\"\"\n    if isinstance(el, NavigableString):\n        return type(el)(el)\n\n    copy = Tag(None, el.builder, el.name, el.namespace, el.nsprefix)\n    # work around bug where there is no builder set\n    # https://bugs.launchpad.net/beautifulsoup/+bug/1307471\n    copy.attrs = dict(el.attrs)\n    for attr in ('can_be_empty_element', 'hidden'):\n        setattr(copy, attr, getattr(el, attr))\n    for child in el.contents:\n        copy.append(clone_bs4_elem(child))\n    return copy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the captcha content", "response": "def get_captcha(self):\n        \"\"\"\u83b7\u53d6\u9a8c\u8bc1\u7801\u6570\u636e\u3002\n\n        :return: \u9a8c\u8bc1\u7801\u56fe\u7247\u6570\u636e\u3002\n        :rtype: bytes\n        \"\"\"\n        self._session.get(Zhihu_URL)\n        r = self._session.get(self._get_captcha_url())\n        return r.content"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs-in to the TeleSign API.", "response": "def login(self, email, password, captcha=None):\n        \"\"\"\u767b\u9646\u77e5\u4e4e.\n\n        :param str email: \u90ae\u7bb1\n        :param str password: \u5bc6\u7801\n        :param str captcha: \u9a8c\u8bc1\u7801, \u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a\u4e0d\u63d0\u4ea4\u9a8c\u8bc1\u7801\n        :return:\n            ======== ======== ============== ====================\n            \u5143\u7d20\u5e8f\u53f7 \u5143\u7d20\u7c7b\u578b \u610f\u4e49           \u8bf4\u660e\n            ======== ======== ============== ====================\n            0        int      \u662f\u5426\u6210\u529f       0\u4e3a\u6210\u529f\uff0c1\u4e3a\u5931\u8d25\n            1        str      \u5931\u8d25\u539f\u56e0       \u767b\u5f55\u6210\u529f\u5219\u4e3a\u7a7a\u5b57\u7b26\u4e32\n            2        str       cookies\u5b57\u7b26\u4e32 \u767b\u5f55\u5931\u8d25\u5219\u4e3a\u7a7a\u5b57\u7b26\u4e32\n            ======== ======== ============== ====================\n\n        :rtype: (int, str, str)\n        \"\"\"\n        data = {'email': email, 'password': password,\n                'remember_me': 'true'}\n        if captcha is not None:\n            data['captcha'] = captcha\n        r = self._session.post(Login_URL, data=data)\n        j = r.json()\n        code = int(j['r'])\n        message = j['msg']\n        cookies_str = json.dumps(self._session.cookies.get_dict()) \\\n            if code == 0 else ''\n        return code, message, cookies_str"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef login_with_cookies(self, cookies):\n        if os.path.isfile(cookies):\n            with open(cookies) as f:\n                cookies = f.read()\n        cookies_dict = json.loads(cookies)\n        self._session.cookies.update(cookies_dict)", "response": "\u4f7f\u7528cookies\u6587\u4ef6\u6216\u5b57\u7b26\u4e32\u767b\u5f55\u77e5\u4e4e\n\n        :param str cookies:\n            ============== ===========================\n            \u53c2\u6570\u5f62\u5f0f       \u4f5c\u7528\n            ============== ===========================\n            \u6587\u4ef6\u540d         \u5c06\u6587\u4ef6\u5185\u5bb9\u4f5c\u4e3acookies\u5b57\u7b26\u4e32\n            cookies \u5b57\u7b26\u4e32  \u76f4\u63a5\u63d0\u4f9bcookies\u5b57\u7b26\u4e32\n            ============== ===========================\n        :return: \u65e0\n        :rtype: None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_proxy_pool(self):\n        self.proxies = None\n        self._session.get = self._session.original_get\n        self._session.post = self._session.original_post\n        del self._session.original_get\n        del self._session.original_post", "response": "Removes the proxy pool from the session."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef me(self):\n        from .me import Me\n        headers = dict(Default_Header)\n        headers['Host'] = 'zhuanlan.zhihu.com'\n        res = self._session.get(Get_Me_Info_Url, headers=headers)\n        json_data = res.json()\n        url = json_data['profileUrl']\n        name = json_data['name']\n        motto = json_data['bio']\n        photo = json_data['avatar']['template'].format(\n            id=json_data['avatar']['id'], size='r')\n        return Me(url, name, motto, photo, session=self._session)", "response": "\u83b7\u53d6\u4f7f\u7528\u7279\u5b9a cookies \u7684 Me \u5b9e\u4f8b\n\n        :return: cookies\u5bf9\u5e94\u7684Me\u5bf9\u8c61\n        :rtype: Me"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parents(self):\n        self._make_soup()\n        parent_topic_tag = self.soup.find('div', class_='parent-topic')\n        if parent_topic_tag is None:\n            yield []\n        else:\n            for topic_tag in parent_topic_tag.find_all('a'):\n                yield Topic(Zhihu_URL + topic_tag['href'],\n                            topic_tag.text.strip(),\n                            session=self._session)", "response": "\u83b7\u53d6\u6b64\u8bdd\u9898\u7684\u7236\u8bdd\u9898\u3002\n        \u6ce8\u610f\uff1a\u7531\u4e8e\u6ca1\u627e\u5230\u6709\u5f88\u591a\u7236\u8bdd\u9898\u7684\u8bdd\u9898\u6765\u6d4b\u8bd5\uff0c\n        \u6240\u4ee5\u672c\u65b9\u6cd5\u53ef\u80fd\u518d\u67d0\u4e9b\u65f6\u5019\u51fa\u73b0\u95ee\u9898\uff0c\u8bf7\u4e0d\u541d\u53cd\u9988\u3002\n\n        :return: \u6b64\u8bdd\u9898\u7684\u7236\u8bdd\u9898\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Topic.Iterable"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef children(self):\n        self._make_soup()\n        child_topic_tag = self.soup.find('div', class_='child-topic')\n        if child_topic_tag is None:\n            return []\n        elif '\u5171\u6709' not in child_topic_tag.contents[-2].text:\n            for topic_tag in child_topic_tag.div.find_all('a'):\n                yield Topic(Zhihu_URL + topic_tag['href'],\n                            topic_tag.text.strip(),\n                            session=self._session)\n        else:\n            flag = 'load'\n            child = ''\n            data = {'_xsrf': self.xsrf}\n            params = {\n                'parent': self.id\n            }\n            while flag == 'load':\n                params['child'] = child\n                res = self._session.post(Topic_Get_Children_Url,\n                                         params=params, data=data)\n                j = map(lambda x: x[0], res.json()['msg'][1])\n                *topics, last = j\n                for topic in topics:\n                    yield Topic(Zhihu_URL + '/topic/' + topic[2], topic[1],\n                                session=self._session)\n                flag = last[0]\n                child = last[2]\n                if flag == 'topic':\n                    yield Topic(Zhihu_URL + '/topic/' + last[2], last[1],\n                                session=self._session)", "response": "get all children of a topic"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the followers of the topic.", "response": "def followers(self):\n        \"\"\"\u83b7\u53d6\u8bdd\u9898\u5173\u6ce8\u8005\n\n        :return: \u8bdd\u9898\u5173\u6ce8\u8005\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Author.Iterable\n        \"\"\"\n        from .author import Author, ANONYMOUS\n        self._make_soup()\n        gotten_data_num = 20\n        data = {\n            '_xsrf': self.xsrf,\n            'start': '',\n            'offset': 0\n        }\n        while gotten_data_num == 20:\n            res = self._session.post(\n                Topic_Get_More_Follower_Url.format(self.id), data=data)\n            j = res.json()['msg']\n            gotten_data_num = j[0]\n            data['offset'] += gotten_data_num\n            soup = BeautifulSoup(j[1])\n            divs = soup.find_all('div', class_='zm-person-item')\n            for div in divs:\n                h2 = div.h2\n                url = Zhihu_URL + h2.a['href']\n                name = h2.a.text\n                motto = h2.parent.div.text.strip()\n                try:\n                    yield Author(url, name, motto, session=self._session)\n                except ValueError:  # invalid url\n                    yield ANONYMOUS\n            data['start'] = int(re_get_number.match(divs[-1]['id']).group(1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef photo_url(self):\n        img = self.soup.find('a', id='zh-avartar-edit-form').img['src']\n        return img.replace('_m', '_r')", "response": "\u83b7\u53d6\u8bdd\u9898\u5934\u50cf\u56fe\u7247\u5730\u5740.\n\n        :return: \u8bdd\u9898\u5934\u50cfurl\n        :rtype: str"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all questions in the topic.", "response": "def questions(self):\n        \"\"\"\u83b7\u53d6\u8bdd\u9898\u4e0b\u7684\u6240\u6709\u95ee\u9898\uff08\u6309\u65f6\u95f4\u964d\u5e8f\u6392\u5217\uff09\n\n        :return: \u8bdd\u9898\u4e0b\u6240\u6709\u95ee\u9898\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Question.Iterable\n        \"\"\"\n        from .question import Question\n        question_url = Topic_Questions_Url.format(self.id)\n        params = {'page': 1}\n        older_time_stamp = int(time.time()) * 1000\n        while True:\n            res = self._session.get(question_url, params=params)\n            soup = BeautifulSoup(res.content)\n            if soup.find('div', class_='error') is not None:\n                return\n            questions = soup.find_all('div', class_='question-item')\n            questions = list(filter(\n                lambda x: int(x.h2.span['data-timestamp']) < older_time_stamp,\n                questions))\n            for qu_div in questions:\n                url = Zhihu_URL + qu_div.h2.a['href']\n                title = qu_div.h2.a.text.strip()\n                creation_time = datetime.fromtimestamp(\n                        int(qu_div.h2.span['data-timestamp']) // 1000)\n                yield Question(url, title, creation_time=creation_time,\n                               session=self._session)\n            older_time_stamp = int(questions[-1].h2.span['data-timestamp'])\n            params['page'] += 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unanswered_questions(self):\n        from .question import Question\n        question_url = Topic_Unanswered_Question_Url.format(self.id)\n        params = {'page': 1}\n        while True:\n            res = self._session.get(question_url, params=params)\n            soup = BeautifulSoup(res.content)\n            if soup.find('div', class_='error') is not None:\n                return\n            questions = soup.find_all('div', class_='question-item')\n            for qu_div in questions:\n                url = Zhihu_URL + qu_div.h2.a['href']\n                title = qu_div.h2.a.text.strip()\n                yield Question(url, title, session=self._session)\n            params['page'] += 1", "response": "\u83b7\u53d6\u8bdd\u9898\u4e0b\u7684\u7b49\u5f85\u56de\u7b54\u7684\u95ee\u9898\n\n        \u4ec0\u4e48\u662f\u300c\u7b49\u5f85\u56de\u7b54\u300d\u7684\u95ee\u9898\uff1ahttps://www.zhihu.com/question/40470324\n\n        :return: \u8bdd\u9898\u4e0b\u7b49\u5f85\u56de\u7b54\u7684\u95ee\u9898\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Question.Iterable"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef answers(self):\n        from .question import Question\n        from .answer import Answer\n        from .author import Author, ANONYMOUS\n\n        newest_url = Topic_Newest_Url.format(self.id)\n        params = {'start': 0, '_xsrf': self.xsrf}\n        res = self._session.get(newest_url)\n        soup = BeautifulSoup(res.content)\n        while True:\n            divs = soup.find_all('div', class_='folding')\n            # \u5982\u679c\u8bdd\u9898\u4e0b\u65e0\u7b54\u6848\uff0c\u5219\u76f4\u63a5\u8fd4\u56de\n            if len(divs) == 0:\n                return\n            last_score = divs[-1]['data-score']\n            for div in divs:\n                q = div.find('a', class_=\"question_link\")\n                question_url = Zhihu_URL + q['href']\n                question_title = q.text.strip()\n                question = Question(question_url, question_title,\n                                    session=self._session)\n\n                ans = div.find('a', class_='answer-date-link')\n                answer_url = Zhihu_URL + ans['href']\n\n                upvote = div.find('a', class_='zm-item-vote-count').text\n                if upvote.isdigit():\n                    upvote = int(upvote)\n                else:\n                    upvote = None\n\n                au = div.find('div', class_='zm-item-answer-author-info')\n                if au.a is None:\n                    author = ANONYMOUS\n                else:\n                    author_url = Zhihu_URL + au.a['href']\n                    author_name = au.a.text\n                    author_motto = au.strong['title'] if au.strong else ''\n                    author = Author(author_url, author_name, author_motto,\n                                    session=self._session)\n                yield Answer(answer_url, question, author, upvote,\n                             session=self._session)\n\n            params['offset'] = last_score\n            res = self._session.post(newest_url, data=params)\n            gotten_feed_num = res.json()['msg'][0]\n            # \u5982\u679c\u5f97\u5230\u5185\u5bb9\u6570\u91cf\u4e3a0\u5219\u8fd4\u56de\n            if gotten_feed_num == 0:\n                return\n            soup = BeautifulSoup(res.json()['msg'][1])", "response": "returns an iterable of Answer. Iterable"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hot_questions(self):\n        from .question import Question\n        hot_questions_url = Topic_Hot_Questions_Url.format(self.id)\n        params = {'start': 0, '_xsrf': self.xsrf}\n        res = self._session.get(hot_questions_url)\n        soup = BeautifulSoup(res.content)\n        while True:\n            questions_duplicate = soup.find_all('a', class_='question_link')\n            # \u5982\u679c\u8bdd\u9898\u4e0b\u65e0\u95ee\u9898\uff0c\u5219\u76f4\u63a5\u8fd4\u56de\n            if len(questions_duplicate) == 0:\n                return\n                # \u53bb\u9664\u91cd\u590d\u7684\u95ee\u9898\n            questions = list(set(questions_duplicate))\n            questions.sort(key=self._get_score, reverse=True)\n            last_score = soup.find_all(\n                'div', class_='feed-item')[-1]['data-score']\n            for q in questions:\n                question_url = Zhihu_URL + q['href']\n                question_title = q.text.strip()\n                question = Question(question_url, question_title,\n                                    session=self._session)\n                yield question\n            params['offset'] = last_score\n            res = self._session.post(hot_questions_url, data=params)\n            gotten_feed_num = res.json()['msg'][0]\n            # \u5982\u679c\u5f97\u5230\u95ee\u9898\u6570\u91cf\u4e3a0\u5219\u8fd4\u56de\n            if gotten_feed_num == 0:\n                return\n            soup = BeautifulSoup(res.json()['msg'][1])", "response": "Get a list of all the questions in the topic."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hot_answers(self):\n        from .question import Question\n        from .author import Author\n        from .answer import Answer\n        hot_questions_url = Topic_Hot_Questions_Url.format(self.id)\n        params = {'start': 0, '_xsrf': self.xsrf}\n        res = self._session.get(hot_questions_url)\n        soup = BeautifulSoup(res.content)\n        while True:\n            answers_div = soup.find_all('div', class_='feed-item')\n            last_score = answers_div[-1]['data-score']\n            for div in answers_div:\n                # \u6ca1\u6709 text area \u7684\u60c5\u51b5\u662f\uff1a\u7b54\u6848\u88ab\u548c\u8c10\u3002\n                if not div.textarea:\n                    continue\n                question_url = Zhihu_URL + div.h2.a['href']\n                question_title = div.h2.a.text.strip()\n                question = Question(question_url, question_title,\n                                    session=self._session)\n                author_link = div.find('a', class_='author-link')\n                if not author_link:\n                    author_url = None\n                    author_name = '\u533f\u540d\u7528\u6237'\n                    author_motto = ''\n                else:\n                    author_url = Zhihu_URL + author_link['href']\n                    author_name = author_link.text\n                    author_motto_span = div.find('span', class_='bio')\n                    author_motto = author_motto_span['title'] \\\n                        if author_motto_span else ''\n                author = Author(author_url, author_name, author_motto,\n                                session=self._session)\n\n                body = div.find('div', class_='zm-item-rich-text')\n                answer_url = Zhihu_URL + body['data-entry-url']\n                upvote_num = int(div.find(\n                    'div', class_='zm-item-vote-info')['data-votecount'])\n\n                yield Answer(answer_url, question, author, upvote_num,\n                             session=self._session)\n\n            params['offset'] = last_score\n            res = self._session.post(hot_questions_url, data=params)\n            gotten_feed_num = res.json()['msg'][0]\n            # \u5982\u679c\u5f97\u5230\u95ee\u9898\u6570\u91cf\u4e3a0\u5219\u8fd4\u56de\n            if gotten_feed_num == 0:\n                return\n            soup = BeautifulSoup(res.json()['msg'][1])", "response": "Get a list of all the answers in the topic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef id(self):\n        return re.match(r'^.*/([^/]+)/$', self.url).group(1) \\\n            if self.url is not None else ''", "response": "get the id of the current resource"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting user hash id", "response": "def hash_id(self):\n        \"\"\"\u83b7\u53d6\u4f5c\u8005\u7684\u5185\u90e8hash id\uff08\u7528\u4e0d\u5230\u5c31\u5ffd\u89c6\u5427~\uff09\n\n        :return: \u7528\u6237hash id\n        :rtype: str\n        \"\"\"\n        div = self.soup.find('div', class_='zm-profile-header-op-btns')\n        if div is not None:\n            return div.button['data-id']\n        else:\n            ga = self.soup.find('script', attrs={'data-name': 'ga_vars'})\n            return json.loads(ga.text)['user_hash']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget name of the object", "response": "def name(self):\n        \"\"\"\u83b7\u53d6\u7528\u6237\u540d\u5b57.\n\n        :return: \u7528\u6237\u540d\u5b57\n        :rtype: str\n        \"\"\"\n        if self.url is None:\n            return '\u533f\u540d\u7528\u6237'\n        if self.soup is not None:\n            return self.soup.find('div', class_='title-section').span.text\n        else:\n            assert self.card is not None\n            return self.card.find('span', class_='name').text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef motto(self):\n        if self.url is None:\n            return ''\n        else:\n            if self.soup is not None:\n                bar = self.soup.find(\n                    'div', class_='title-section')\n                if len(bar.contents) < 4:\n                    return ''\n                else:\n                    return bar.contents[3].text\n            else:\n                assert self.card is not None\n                motto = self.card.find('div', class_='tagline')\n                return motto.text if motto is not None else ''", "response": "\u83b7\u53d6\u7528\u6237\u81ea\u6211\u4ecb\u7ecd\uff0c\u7531\u4e8e\u5386\u53f2\u539f\u56e0\uff0c\u6211\u8fd8\u662f\u628a\u8fd9\u4e2a\u5c5e\u6027\u53eb\u505amotto\u5427.\n\n        :return: \u7528\u6237\u81ea\u6211\u4ecb\u7ecd\n        :rtype: str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef photo_url(self):\n        if self.url is not None:\n            if self.soup is not None:\n                img = self.soup.find('img', class_='Avatar Avatar--l')['src']\n                return img.replace('_l', '_r')\n            else:\n                assert (self.card is not None)\n                return PROTOCOL + self.card.img['src'].replace('_xs', '_r')\n        else:\n            return 'http://pic1.zhimg.com/da8e974dc_r.jpg'", "response": "\u83b7\u53d6\u7528\u6237\u5934\u50cf\u56fe\u7247\u5730\u5740.\n\n        :return: \u7528\u6237\u5934\u50cfurl\n        :rtype: str"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef followee_num(self):\n        if self.url is None:\n            return 0\n        else:\n            number = int(self.soup.find(\n                'div', class_='zm-profile-side-following').a.strong.text)\n            return number", "response": "\u83b7\u53d6\u5173\u6ce8\u4e86\u591a\u5c11\u4eba.\n\n        :return: \u5173\u6ce8\u7684\u4eba\u6570\n        :rtype: int"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the number of follower", "response": "def follower_num(self):\n        \"\"\"\u83b7\u53d6\u8ffd\u968f\u8005\u6570\u91cf\uff0c\u5c31\u662f\u5173\u6ce8\u6b64\u4eba\u7684\u4eba\u6570.\n\n        :return: \u8ffd\u968f\u8005\u6570\u91cf\n        :rtype: int\n        \"\"\"\n        if self.url is None:\n            return 0\n        else:\n            number = int(self.soup.find(\n                'div', class_='zm-profile-side-following zg-clear').find_all(\n                'a')[1].strong.text)\n            return number"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upvote_num(self):\n        if self.url is None:\n            return 0\n        else:\n            number = int(self.soup.find(\n                'span', class_='zm-profile-header-user-agree').strong.text)\n            return number", "response": "\u83b7\u53d6\u6536\u5230\u7684\u7684\u8d5e\u540c\u6570\u91cf.\n\n        :return: \u6536\u5230\u7684\u7684\u8d5e\u540c\u6570\u91cf\n        :rtype: int"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef followed_topic_num(self):\n        if self.url is not None:\n            tag = self.soup.find('div', class_='zm-profile-side-topics')\n            if tag is not None:\n                return int(re_get_number.match(\n                    tag.parent.strong.text).group(1))\n        return 0", "response": "get followed topic number"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all questions in the order of the last 20 questions.", "response": "def questions(self):\n        \"\"\"\u83b7\u53d6\u7528\u6237\u7684\u6240\u6709\u95ee\u9898.\n\n        :return: \u7528\u6237\u7684\u6240\u6709\u95ee\u9898\uff0c\u8fd4\u56de\u751f\u6210\u5668.\n        :rtype: Question.Iterable\n        \"\"\"\n        from .question import Question\n\n        if self.url is None or self.question_num == 0:\n            return\n        for page_index in range(1, (self.question_num - 1) // 20 + 2):\n            html = self._session.get(\n                self.url + 'asks?page=' + str(page_index)).text\n            soup = BeautifulSoup(html)\n            question_links = soup.find_all('a', class_='question_link')\n            question_datas = soup.find_all(\n                'div', class_='zm-profile-section-main')\n            for link, data in zip(question_links, question_datas):\n                url = Zhihu_URL + link['href']\n                title = link.text.strip()\n                answer_num = int(\n                    re_get_number.match(data.div.contents[4]).group(1))\n                follower_num = int(\n                    re_get_number.match(data.div.contents[6]).group(1))\n                q = Question(url, title, follower_num, answer_num,\n                             session=self._session)\n                yield q"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef answers(self):\n        from .question import Question\n        from .answer import Answer\n\n        if self.url is None or self.answer_num == 0:\n            return\n        for page_index in range(1, (self.answer_num - 1) // 20 + 2):\n            html = self._session.get(\n                self.url + 'answers?page=' + str(page_index)).text\n            soup = BeautifulSoup(html)\n            questions = soup.find_all('a', class_='question_link')\n            upvotes = soup.find_all('a', class_='zm-item-vote-count')\n            for q, upvote in zip(questions, upvotes):\n                answer_url = Zhihu_URL + q['href']\n                question_url = Zhihu_URL + re_a2q.match(q['href']).group(1)\n                question_title = q.text\n                upvote_num = upvote.text\n                if upvote_num.isdigit():\n                    upvote_num = int(upvote_num)\n                else:\n                    upvote_num = None\n                question = Question(question_url, question_title,\n                                    session=self._session)\n                yield Answer(answer_url, question, self, upvote_num,\n                             session=self._session)", "response": "Get answers from Zhihu"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collections(self):\n        from .collection import Collection\n\n        if self.url is None or self.collection_num == 0:\n            return\n        else:\n            collection_num = self.collection_num\n            for page_index in range(1, (collection_num - 1) // 20 + 2):\n                html = self._session.get(\n                    self.url + 'collections?page=' + str(page_index)).text\n                soup = BeautifulSoup(html)\n                collections_names = soup.find_all(\n                    'a', class_='zm-profile-fav-item-title')\n                collection_follower_nums = soup.find_all(\n                    'div', class_='zm-profile-fav-bio')\n                for c, f in zip(collections_names, collection_follower_nums):\n                    c_url = Zhihu_URL + c['href']\n                    c_name = c.text\n                    c_fn = int(re_get_number.match(f.contents[2]).group(1))\n                    yield Collection(c_url, self, c_name, c_fn,\n                                     session=self._session)", "response": "\u83b7\u53d6\u7528\u6237\u6536\u85cf\u5939.\n\n        :return: \u7528\u6237\u6536\u85cf\u5939\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Collection.Iterable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all the topics that are following this topic.", "response": "def followed_topics(self):\n        \"\"\"\u83b7\u53d6\u7528\u6237\u5173\u6ce8\u7684\u8bdd\u9898.\n\n        :return: \u7528\u6237\u5173\u6ce8\u7684\u8bdd\u9898\uff0c\u8fd4\u56de\u751f\u6210\u5668\n        :rtype: Topic.Iterable\n        \"\"\"\n        from .topic import Topic\n        if self.url is None:\n            return\n        if self.followed_topic_num > 0:\n            tag = self.soup.find('div', class_='zm-profile-side-topics')\n            if tag is not None:\n                for a in tag.find_all('a'):\n                    yield Topic(Zhihu_URL + a['href'], a.img['alt'],\n                                session=self._session)\n            if self.followed_topic_num > 7:\n                offset = 7\n                gotten_data_num = 20\n                while gotten_data_num == 20:\n                    data = {'start': 0, 'offset': offset, '_xsrf': self.xsrf}\n                    j = self._session.post(\n                        Author_Get_More_Follow_Topic_URL.format(self.id),\n                        data=data).json()\n                    gotten_data_num = j['msg'][0]\n                    offset += gotten_data_num\n                    topic_item = BeautifulSoup(j['msg'][1]).find_all(\n                        'div', class_='zm-profile-section-item')\n                    for div in topic_item:\n                        name = div.strong.text\n                        url = Zhihu_URL + div.a['href']\n                        yield Topic(url, name, session=self._session)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a generator of all activities in the order they appear.", "response": "def activities(self):\n        \"\"\"\u83b7\u53d6\u7528\u6237\u7684\u6700\u8fd1\u52a8\u6001.\n\n        :return: \u6700\u8fd1\u52a8\u6001\uff0c\u8fd4\u56de\u751f\u6210\u5668\uff0c\u5177\u4f53\u8bf4\u660e\u89c1 :class:`.Activity`\n        :rtype: Activity.Iterable\n        \"\"\"\n        from .activity import Activity\n\n        if self.url is None:\n            return\n\n        gotten_feed_num = 20\n        start = '0'\n        api_url = self.url + 'activities'\n        while gotten_feed_num == 20:\n            data = {'_xsrf': self.xsrf, 'start': start}\n            res = self._session.post(api_url, data=data)\n            gotten_feed_num = res.json()['msg'][0]\n            soup = BeautifulSoup(res.json()['msg'][1])\n            acts = soup.find_all(\n                'div', class_='zm-profile-section-item zm-item clearfix')\n\n            start = acts[-1]['data-time'] if len(acts) > 0 else 0\n            for act in acts:\n                # --- ignore Round Table temporarily ---\n                if act.attrs['data-type-detail'] == \"member_follow_roundtable\":\n                    continue\n                # --- --- --- --- -- --- --- --- --- ---\n                yield Activity(act, self._session, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef last_activity_time(self):\n        self._make_soup()\n        act = self.soup.find(\n            'div', class_='zm-profile-section-item zm-item clearfix')\n        return int(act['data-time']) if act is not None else -1", "response": "get last activity time"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_zero_user(self):\n        return self.upvote_num + self.thank_num + \\\n               self.question_num + self.answer_num == 0", "response": "is_zero_user - Check if the user is zero."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef memoize(f, refresh_keyword='mrefresh'):\n    f.mcache = {}\n    f.mrefresh_keyword = refresh_keyword\n    return decorator.decorator(_memoize, f)", "response": "Memoize decorator. The refresh keyword is the keyword\n    used to bypass the cache (in the function call)."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_arg(arg):\n    # handle string input\n    if type(arg) == str:\n        arg = arg.strip()\n        # parse csv as tickers and create children\n        if ',' in arg:\n            arg = arg.split(',')\n            arg = [x.strip() for x in arg]\n        # assume single string - create single item list\n        else:\n            arg = [arg]\n\n    return arg", "response": "Parses arguments for convenience."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean_ticker(ticker):\n    pattern = re.compile('[\\W_]+')\n    res = pattern.sub('', ticker.split(' ')[0])\n    return res.lower()", "response": "Cleans a ticker for easier use throughout MoneyTree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scale(val, src, dst):\n    if val < src[0]:\n        return dst[0]\n    if val > src[1]:\n        return dst[1]\n\n    return ((val - src[0]) / (src[1] - src[0])) * (dst[1] - dst[0]) + dst[0]", "response": "Scale value from src range to dst range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_format(item, format_str='.2f'):\n    if isinstance(item, pd.Series):\n        return item.map(lambda x: format(x, format_str))\n    elif isinstance(item, pd.DataFrame):\n        return item.applymap(lambda x: format(x, format_str))", "response": "Map a format string over a pandas object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a price index given a series of returns.", "response": "def to_price_index(returns, start=100):\n    \"\"\"\n    Returns a price index given a series of returns.\n\n    Args:\n        * returns: Expects a return series\n        * start (number): Starting level\n\n    Assumes arithmetic returns.\n\n    Formula is: cumprod (1+r)\n    \"\"\"\n    return (returns.replace(to_replace=np.nan, value=0) + 1).cumprod() * start"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates performance stats of a given object.", "response": "def calc_stats(prices):\n    \"\"\"\n    Calculates performance stats of a given object.\n\n    If object is Series, a PerformanceStats object is\n    returned. If object is DataFrame, a GroupStats object\n    is returned.\n\n    Args:\n        * prices (Series, DataFrame): Set of prices\n    \"\"\"\n    if isinstance(prices, pd.Series):\n        return PerformanceStats(prices)\n    elif isinstance(prices, pd.DataFrame):\n        return GroupStats(*[prices[x] for x in prices.columns])\n    else:\n        raise NotImplementedError('Unsupported type')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a series of prices to a drawdown series.", "response": "def to_drawdown_series(prices):\n    \"\"\"\n    Calculates the `drawdown <https://www.investopedia.com/terms/d/drawdown.asp>`_ series.\n\n    This returns a series representing a drawdown.\n    When the price is at all time highs, the drawdown\n    is 0. However, when prices are below high water marks,\n    the drawdown series = current / hwm - 1\n\n    The max drawdown can be obtained by simply calling .min()\n    on the result (since the drawdown series is negative)\n\n    Method ignores all gaps of NaN's in the price series.\n\n    Args:\n        * prices (Series or DataFrame): Series of prices.\n\n    \"\"\"\n    # make a copy so that we don't modify original data\n    drawdown = prices.copy()\n\n    # Fill NaN's with previous values\n    drawdown = drawdown.fillna(method='ffill')\n\n    # Ignore problems with NaN's in the beginning\n    drawdown[np.isnan(drawdown)] = -np.Inf\n\n    # Rolling maximum\n    roll_max = np.maximum.accumulate(drawdown)\n    drawdown = drawdown / roll_max - 1.\n    return drawdown"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drawdown_details(drawdown, index_type=pd.DatetimeIndex):\n\n    is_zero = drawdown == 0\n    # find start dates (first day where dd is non-zero after a zero)\n    start = ~is_zero & is_zero.shift(1)\n    start = list(start[start == True].index)  # NOQA\n\n    # find end dates (first day where dd is 0 after non-zero)\n    end = is_zero & (~is_zero).shift(1)\n    end = list(end[end == True].index)  # NOQA\n\n    if len(start) is 0:\n        return None\n\n    # drawdown has no end (end period in dd)\n    if len(end) is 0:\n        end.append(drawdown.index[-1])\n\n    # if the first drawdown start is larger than the first drawdown end it\n    # means the drawdown series begins in a drawdown and therefore we must add\n    # the first index to the start series\n    if start[0] > end[0]:\n        start.insert(0, drawdown.index[0])\n\n    # if the last start is greater than the end then we must add the last index\n    # to the end series since the drawdown series must finish with a drawdown\n    if start[-1] > end[-1]:\n        end.append(drawdown.index[-1])\n\n    result = pd.DataFrame(\n        columns=('Start', 'End', 'Length', 'drawdown'),\n        index=range(0, len(start))\n    )\n\n    for i in range(0, len(start)):\n        dd = drawdown[start[i]:end[i]].min()\n\n        if index_type is pd.DatetimeIndex:\n            result.iloc[i] = (start[i], end[i], (end[i] - start[i]).days, dd)\n        else:\n            result.iloc[i] = (start[i], end[i], (end[i] - start[i]), dd)\n\n    return result", "response": "Returns a pandas. DataFrame with start end days and drawdown for each drawdown in a drawdown series."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_cagr(prices):\n    start = prices.index[0]\n    end = prices.index[-1]\n    return (prices.iloc[-1] / prices.iloc[0]) ** (1 / year_frac(start, end)) - 1", "response": "Calculates the CAGR of a given price series."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_information_ratio(returns, benchmark_returns):\n    diff_rets = returns - benchmark_returns\n    diff_std = np.std(diff_rets, ddof=1)\n\n    if np.isnan(diff_std) or diff_std == 0:\n        return 0.0\n\n    return np.divide(diff_rets.mean(), diff_std)", "response": "Calculates the information ratio of a single page."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_prob_mom(returns, other_returns):\n    return t.cdf(returns.calc_information_ratio(other_returns),\n                 len(returns) - 1)", "response": "Calculate the probability of one asset\n    is going to outperform the other."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge(*series):\n    dfs = []\n    for s in series:\n        if isinstance(s, pd.DataFrame):\n            dfs.append(s)\n        elif isinstance(s, pd.Series):\n            tmpdf = pd.DataFrame({s.name: s})\n            dfs.append(tmpdf)\n        else:\n            raise NotImplementedError('Unsupported merge type')\n\n    return pd.concat(dfs, axis=1)", "response": "Merge Series and or DataFrames together."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_duplicate_cols(df):\n    names = set(df.columns)\n    for n in names:\n        if len(df[n].shape) > 1:\n            # get subset of df w/ colname n\n            sub = df[n]\n            # make unique colnames\n            sub.columns = ['%s-%s' % (n, x) for x in range(sub.shape[1])]\n            # get colname w/ max # of data\n            keep = sub.count().idxmax()\n            # drop all columns of name n from original df\n            del df[n]\n            # update original df w/ longest col with name n\n            df[n] = sub[keep]\n\n    return df", "response": "Removes duplicate columns from a dataframe\n            and keeps column w/ longest history\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a TimeSeries to a daily series.", "response": "def to_monthly(series, method='ffill', how='end'):\n    \"\"\"\n    Convenience method that wraps asfreq_actual\n    with 'M' param (method='ffill', how='end').\n    \"\"\"\n    return series.asfreq_actual('M', method=method, how=how)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef asfreq_actual(series, freq, method='ffill', how='end', normalize=False):\n    orig = series\n    is_series = False\n    if isinstance(series, pd.Series):\n        is_series = True\n        name = series.name if series.name else 'data'\n        orig = pd.DataFrame({name: series})\n\n    # add date column\n    t = pd.concat([orig, pd.DataFrame({'dt': orig.index.values},\n                                      index=orig.index.values)], axis=1)\n    # fetch dates\n    dts = t.asfreq(freq=freq, method=method, how=how,\n                   normalize=normalize)['dt']\n\n    res = orig.loc[dts.values]\n\n    if is_series:\n        return res[name]\n    else:\n        return res", "response": "Similar to pandas. asfreq but keeps the actual dates."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_inv_vol_weights(returns):\n    # calc vols\n    vol = np.divide(1., np.std(returns, ddof=1))\n    vol[np.isinf(vol)] = np.NaN\n    volsum = vol.sum()\n    return np.divide(vol, volsum)", "response": "Calculates the inverse volatility of each column."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_mean_var_weights(returns, weight_bounds=(0., 1.),\n                          rf=0.,\n                          covar_method='ledoit-wolf',\n                          options=None):\n    \"\"\"\n    Calculates the mean-variance weights given a DataFrame of returns.\n\n    Args:\n        * returns (DataFrame): Returns for multiple securities.\n        * weight_bounds ((low, high)): Weigh limits for optimization.\n        * rf (float): `Risk-free rate <https://www.investopedia.com/terms/r/risk-freerate.asp>`_ used in utility calculation\n        * covar_method (str): Covariance matrix estimation method.\n            Currently supported:\n                - `ledoit-wolf <http://www.ledoit.net/honey.pdf>`_\n                - standard\n        * options (dict): options for minimizing, e.g. {'maxiter': 10000 }\n\n    Returns:\n        Series {col_name: weight}\n\n    \"\"\"\n    def fitness(weights, exp_rets, covar, rf):\n        # portfolio mean\n        mean = sum(exp_rets * weights)\n        # portfolio var\n        var = np.dot(np.dot(weights, covar), weights)\n        # utility - i.e. sharpe ratio\n        util = (mean - rf) / np.sqrt(var)\n        # negative because we want to maximize and optimizer\n        # minimizes metric\n        return -util\n\n    n = len(returns.columns)\n\n    # expected return defaults to mean return by default\n    exp_rets = returns.mean()\n\n    # calc covariance matrix\n    if covar_method == 'ledoit-wolf':\n        covar = sklearn.covariance.ledoit_wolf(returns)[0]\n    elif covar_method == 'standard':\n        covar = returns.cov()\n    else:\n        raise NotImplementedError('covar_method not implemented')\n\n    weights = np.ones([n]) / n\n    bounds = [weight_bounds for i in range(n)]\n    # sum of weights must be equal to 1\n    constraints = ({'type': 'eq', 'fun': lambda W: sum(W) - 1.})\n    optimized = minimize(fitness, weights, (exp_rets, covar, rf),\n                         method='SLSQP', constraints=constraints,\n                         bounds=bounds, options=options)\n    # check if success\n    if not optimized.success:\n        raise Exception(optimized.message)\n\n    # return weight vector\n    return pd.Series({returns.columns[i]: optimized.x[i] for i in range(n)})", "response": "Calculates the mean - variance weights given a DataFrame of returns."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the equal risk contribution / risk parity weights given a DataFrame of returns.", "response": "def _erc_weights_slsqp(\n        x0,\n        cov,\n        b,\n        maximum_iterations,\n        tolerance\n):\n    \"\"\"\n    Calculates the equal risk contribution / risk parity weights given\n        a DataFrame of returns.\n\n    Args:\n    * x0 (np.array): Starting asset weights.\n    * cov (np.array): covariance matrix.\n    * b (np.array): Risk target weights. By definition target total risk contributions are all equal which makes this redundant.\n    * maximum_iterations (int): Maximum iterations in iterative solutions.\n    * tolerance (float): Tolerance level in iterative solutions.\n\n    Returns:\n    np.array {weight}\n\n    You can read more about ERC at\n    http://thierry-roncalli.com/download/erc.pdf\n\n    \"\"\"\n\n    def fitness(weights, covar):\n        # total risk contributions\n        # trc = weights*np.matmul(covar,weights)/np.sqrt(np.matmul(weights.T,np.matmul(covar,weights)))\n\n        # instead of using the true definition for trc we will use the optimization on page 5\n        trc = weights * np.matmul(covar, weights)\n        \n        n = len(trc)\n        # sum of squared differences of total risk contributions\n        sse = 0.\n        for i in range(n):\n            for j in range(n):\n                #switched from squared deviations to absolute deviations to avoid numerical instability\n                sse += np.abs(trc[i] - trc[j])\n        # minimizes metric\n        return sse\n\n    #nonnegative\n    bounds = [(0,None) for i in range(len(x0))]\n    # sum of weights must be equal to 1\n    constraints = (\n        {\n            'type': 'eq',\n            'fun': lambda W: sum(W) - 1.\n        }\n    )\n    options = {\n        'maxiter':maximum_iterations\n    }\n\n    optimized = minimize(\n        fitness,\n        x0,\n        (cov),\n        method='SLSQP',\n        constraints=constraints,\n        bounds=bounds,\n        options=options,\n        tol=tolerance\n    )\n    # check if success\n    if not optimized.success:\n        raise Exception(optimized.message)\n\n    # return weight vector\n    return optimized.x"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _erc_weights_ccd(x0,\n                     cov,\n                     b,\n                     maximum_iterations,\n                     tolerance):\n    \"\"\"\n    Calculates the equal risk contribution / risk parity weights given\n    a DataFrame of returns.\n\n    Args:\n        * x0 (np.array): Starting asset weights.\n        * cov (np.array): covariance matrix.\n        * b (np.array): Risk target weights.\n        * maximum_iterations (int): Maximum iterations in iterative solutions.\n        * tolerance (float): Tolerance level in iterative solutions.\n\n    Returns:\n        np.array {weight}\n\n    Reference:\n        Griveau-Billion, Theophile and Richard, Jean-Charles and Roncalli,\n        Thierry, A Fast Algorithm for Computing High-Dimensional Risk Parity\n        Portfolios (2013).\n        Available at SSRN: https://ssrn.com/abstract=2325255\n\n    \"\"\"\n    n = len(x0)\n    x = x0.copy()\n    var = np.diagonal(cov)\n    ctr = cov.dot(x)\n    sigma_x = np.sqrt(x.T.dot(ctr))\n\n    for iteration in range(maximum_iterations):\n\n        for i in range(n):\n            alpha = var[i]\n            beta = ctr[i] - x[i] * alpha\n            gamma = -b[i] * sigma_x\n\n            x_tilde = (-beta + np.sqrt(\n                beta * beta - 4 * alpha * gamma)) / (2 * alpha)\n            x_i = x[i]\n\n            ctr = ctr - cov[i] * x_i + cov[i] * x_tilde\n            sigma_x = sigma_x * sigma_x - 2 * x_i * cov[i].dot(\n                x) + x_i * x_i * var[i]\n            x[i] = x_tilde\n            sigma_x = np.sqrt(sigma_x + 2 * x_tilde * cov[i].dot(\n                x) - x_tilde * x_tilde * var[i])\n\n        # check convergence\n        if np.power((x - x0) / x.sum(), 2).sum() < tolerance:\n            return x / x.sum()\n\n        x0 = x.copy()\n\n    # no solution found\n    raise ValueError('No solution found after {0} iterations.'.format(\n        maximum_iterations))", "response": "Calculates the equal risk contribution / risk parity weights given a DataFrame of returns."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_erc_weights(returns,\n                     initial_weights=None,\n                     risk_weights=None,\n                     covar_method='ledoit-wolf',\n                     risk_parity_method='ccd',\n                     maximum_iterations=100,\n                     tolerance=1E-8):\n    \"\"\"\n    Calculates the equal risk contribution / risk parity weights given a\n    DataFrame of returns.\n\n    Args:\n        * returns (DataFrame): Returns for multiple securities.\n        * initial_weights (list): Starting asset weights [default inverse vol].\n        * risk_weights (list): Risk target weights [default equal weight].\n        * covar_method (str): Covariance matrix estimation method.\n            Currently supported:\n                - `ledoit-wolf <http://www.ledoit.net/honey.pdf>`_ [default]\n                - standard\n        * risk_parity_method (str): Risk parity estimation method.\n            Currently supported:\n                - ccd (cyclical coordinate descent)[default]\n        * maximum_iterations (int): Maximum iterations in iterative solutions.\n        * tolerance (float): Tolerance level in iterative solutions.\n\n    Returns:\n        Series {col_name: weight}\n\n    \"\"\"\n    n = len(returns.columns)\n\n    # calc covariance matrix\n    if covar_method == 'ledoit-wolf':\n        covar = sklearn.covariance.ledoit_wolf(returns)[0]\n    elif covar_method == 'standard':\n        covar = returns.cov().values\n    else:\n        raise NotImplementedError('covar_method not implemented')\n\n    # initial weights (default to inverse vol)\n    if initial_weights is None:\n        inv_vol = 1. / np.sqrt(np.diagonal(covar))\n        initial_weights = inv_vol / inv_vol.sum()\n\n    # default to equal risk weight\n    if risk_weights is None:\n        risk_weights = np.ones(n) / n\n\n    # calc risk parity weights matrix\n    if risk_parity_method == 'ccd':\n        # cyclical coordinate descent implementation\n        erc_weights = _erc_weights_ccd(\n            initial_weights,\n            covar,\n            risk_weights,\n            maximum_iterations,\n            tolerance\n        )\n    elif risk_parity_method == 'slsqp':\n        #scipys slsqp optimizer\n        erc_weights = _erc_weights_slsqp(\n            initial_weights,\n            covar,\n            risk_weights,\n            maximum_iterations,\n            tolerance\n        )\n\n    else:\n        raise NotImplementedError('risk_parity_method not implemented')\n\n    # return erc weights vector\n    return pd.Series(erc_weights, index=returns.columns, name='erc')", "response": "Calculates the equal risk contribution and risk parity weights given a DataFrame of returns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of days required to assume that data is OK.", "response": "def get_num_days_required(offset, period='d', perc_required=0.90):\n    \"\"\"\n    Estimates the number of days required to assume that data is OK.\n\n    Helper function used to determine if there are enough \"good\" data\n    days over a given period.\n\n    Args:\n        * offset (DateOffset): Offset (lookback) period.\n        * period (str): Period string.\n        * perc_required (float): percentage of number of days\n            expected required.\n\n    \"\"\"\n    x = pd.to_datetime('2010-01-01')\n    delta = x - (x - offset)\n    # convert to 'trading days' - rough guestimate\n    days = delta.days * 0.69\n\n    if period == 'd':\n        req = days * perc_required\n    elif period == 'm':\n        req = (days / 20) * perc_required\n    elif period == 'y':\n        req = (days / 252) * perc_required\n    else:\n        raise NotImplementedError(\n            'period not supported. Supported periods are d, m, y')\n\n    return req"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_clusters(returns, n=None, plot=False):\n    # calculate correlation\n    corr = returns.corr()\n\n    # calculate dissimilarity matrix\n    diss = 1 - corr\n\n    # scale down to 2 dimensions using MDS\n    # (multi-dimensional scaling) using the\n    # dissimilarity matrix\n    mds = sklearn.manifold.MDS(dissimilarity='precomputed')\n    xy = mds.fit_transform(diss)\n\n    def routine(k):\n        # fit KMeans\n        km = sklearn.cluster.KMeans(n_clusters=k)\n        km_fit = km.fit(xy)\n        labels = km_fit.labels_\n        centers = km_fit.cluster_centers_\n\n        # get {ticker: label} mappings\n        mappings = dict(zip(returns.columns, labels))\n\n        # print % of var explained\n        totss = 0\n        withinss = 0\n        # column average fot totss\n        avg = np.array([np.mean(xy[:, 0]), np.mean(xy[:, 1])])\n        for idx, lbl in enumerate(labels):\n            withinss += sum((xy[idx] - centers[lbl]) ** 2)\n            totss += sum((xy[idx] - avg) ** 2)\n        pvar_expl = 1.0 - withinss / totss\n\n        return mappings, pvar_expl, labels\n\n    if n:\n        result = routine(n)\n    else:\n        n = len(returns.columns)\n        n1 = int(np.ceil(n * 0.6666666666))\n        for i in range(2, n1 + 1):\n            result = routine(i)\n            if result[1] > 0.9:\n                break\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.scatter(xy[:, 0], xy[:, 1], c=result[2], s=90)\n        for i, txt in enumerate(returns.columns):\n            ax.annotate(txt, (xy[i, 0], xy[i, 1]), size=14)\n\n    # sanitize return value\n    tmp = result[0]\n    # map as such {cluster: [list of tickers], cluster2: [...]}\n    inv_map = {}\n    for k, v in iteritems(tmp):\n        inv_map[v] = inv_map.get(v, [])\n        inv_map[v].append(k)\n\n    return inv_map", "response": "Calculates the cluster centers for the k - means cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the FTCA algorithm for the given returns.", "response": "def calc_ftca(returns, threshold=0.5):\n    \"\"\"\n    Implementation of David Varadi's `Fast Threshold Clustering Algorithm (FTCA) <http://cssanalytics.wordpress.com/2013/11/26/fast-threshold-clustering-algorithm-ftca/>`_.\n\n    http://cssanalytics.wordpress.com/2013/11/26/fast-threshold-clustering-algorithm-ftca/  # NOQA\n\n    More stable than k-means for clustering purposes.\n    If you want more clusters, use a higher threshold.\n\n    Args:\n        * returns - expects a pandas dataframe of returns where\n            each column is the name of a given security.\n        * threshold (float): Threshold parameter - use higher value\n            for more clusters. Basically controls how similar\n            (correlated) series have to be.\n    Returns:\n        dict of cluster name (a number) and list of securities in cluster\n\n    \"\"\"\n    # cluster index (name)\n    i = 0\n    # correlation matrix\n    corr = returns.corr()\n    # remaining securities to cluster\n    remain = list(corr.index.copy())\n    n = len(remain)\n    res = {}\n\n    while n > 0:\n        # if only one left then create cluster and finish\n        if n == 1:\n            i += 1\n            res[i] = remain\n            n = 0\n        # if not then we have some work to do\n        else:\n            # filter down correlation matrix to current remain\n            cur_corr = corr[remain].loc[remain]\n            # get mean correlations, ordered\n            mc = cur_corr.mean().sort_values()\n            # get lowest and highest mean correlation\n            low = mc.index[0]\n            high = mc.index[-1]\n\n            # case if corr(high,low) > threshold\n            if corr[high][low] > threshold:\n                i += 1\n\n                # new cluster for high and low\n                res[i] = [low, high]\n                remain.remove(low)\n                remain.remove(high)\n\n                rmv = []\n                for x in remain:\n                    avg_corr = (corr[x][high] + corr[x][low]) / 2.0\n                    if avg_corr > threshold:\n                        res[i].append(x)\n                        rmv.append(x)\n                [remain.remove(x) for x in rmv]\n\n                n = len(remain)\n\n            # otherwise we are creating two clusters - one for high\n            # and one for low\n            else:\n                # add cluster with HC\n                i += 1\n                res[i] = [high]\n                remain.remove(high)\n                remain.remove(low)\n\n                rmv = []\n                for x in remain:\n                    if corr[x][high] > threshold:\n                        res[i].append(x)\n                        rmv.append(x)\n                [remain.remove(x) for x in rmv]\n\n                i += 1\n                res[i] = [low]\n\n                rmv = []\n                for x in remain:\n                    if corr[x][low] > threshold:\n                        res[i].append(x)\n                        rmv.append(x)\n                [remain.remove(x) for x in rmv]\n\n                n = len(remain)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef limit_weights(weights, limit=0.1):\n    if 1.0 / limit > len(weights):\n        raise ValueError('invalid limit -> 1 / limit must be <= len(weights)')\n\n    if isinstance(weights, dict):\n        weights = pd.Series(weights)\n\n    if np.round(weights.sum(), 1) != 1.0:\n        raise ValueError('Expecting weights (that sum to 1) - sum is %s'\n                         % weights.sum())\n\n    res = np.round(weights.copy(), 4)\n    to_rebalance = (res[res > limit] - limit).sum()\n\n    ok = res[res < limit]\n    ok += (ok / ok.sum()) * to_rebalance\n\n    res[res > limit] = limit\n    res[res < limit] = ok\n\n    if any(x > limit for x in res):\n        return limit_weights(res, limit=limit)\n\n    return res", "response": "Returns a new centric version of the given weighted list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef random_weights(n, bounds=(0., 1.), total=1.0):\n    low = bounds[0]\n    high = bounds[1]\n\n    if high < low:\n        raise ValueError('Higher bound must be greater or '\n                         'equal to lower bound')\n\n    if n * high < total or n * low > total:\n        raise ValueError('solution not possible with given n and bounds')\n\n    w = [0] * n\n    tgt = -float(total)\n\n    for i in range(n):\n        rn = n - i - 1\n        rhigh = rn * high\n        rlow = rn * low\n\n        lowb = max(-rhigh - tgt, low)\n        highb = min(-rlow - tgt, high)\n\n        rw = random.uniform(lowb, highb)\n        w[i] = rw\n\n        tgt += rw\n\n    random.shuffle(w)\n    return w", "response": "Generates pseudo - random weights for the n - grams."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_heatmap(data, title='Heatmap', show_legend=True,\n                 show_labels=True, label_fmt='.2f',\n                 vmin=None, vmax=None,\n                 figsize=None, label_color='w',\n                 cmap='RdBu', **kwargs):\n    \"\"\"\n    Plot a heatmap using matplotlib's pcolor.\n\n    Args:\n        * data (DataFrame): DataFrame to plot. Usually small matrix (ex.\n            correlation matrix).\n        * title (string): Plot title\n        * show_legend (bool): Show color legend\n        * show_labels (bool): Show value labels\n        * label_fmt (str): Label format string\n        * vmin (float): Min value for scale\n        * vmax (float): Max value for scale\n        * cmap (string): Color map\n        * kwargs: Passed to matplotlib's pcolor\n\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n\n    heatmap = ax.pcolor(data, vmin=vmin, vmax=vmax, cmap=cmap)\n    # for some reason heatmap has the y values backwards....\n    ax.invert_yaxis()\n\n    if title is not None:\n        plt.title(title)\n\n    if show_legend:\n        fig.colorbar(heatmap)\n\n    if show_labels:\n        vals = data.values\n        for x in range(data.shape[0]):\n            for y in range(data.shape[1]):\n                plt.text(x + 0.5, y + 0.5, format(vals[y, x], label_fmt),\n                         horizontalalignment='center',\n                         verticalalignment='center',\n                         color=label_color)\n\n    plt.yticks(np.arange(0.5, len(data.index), 1), data.index)\n    plt.xticks(np.arange(0.5, len(data.columns), 1), data.columns)\n\n    return plt", "response": "Plots a heatmap using matplotlib s pcolor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rollapply(data, window, fn):\n    res = data.copy()\n    res[:] = np.nan\n    n = len(data)\n\n    if window > n:\n        return res\n\n    for i in range(window - 1, n):\n        res.iloc[i] = fn(data.iloc[i - window + 1:i + 1])\n\n    return res", "response": "Apply a function fn over a rolling window of size window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping scipy winsorize function to drop na s Taxonomy", "response": "def _winsorize_wrapper(x, limits):\n    \"\"\"\n    Wraps scipy winsorize function to drop na's\n    \"\"\"\n    if isinstance(x, pd.Series):\n        if x.count() == 0:\n            return x\n\n        notnanx = ~np.isnan(x)\n        x[notnanx] = scipy.stats.mstats.winsorize(x[notnanx],\n                                                  limits=limits)\n        return x\n    else:\n        return scipy.stats.mstats.winsorize(x, limits=limits)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rescale(x, min=0., max=1., axis=0):\n    def innerfn(x, min, max):\n        return np.interp(x, [np.min(x), np.max(x)], [min, max])\n\n    if isinstance(x, pd.DataFrame):\n        return x.apply(innerfn, axis=axis, args=(min, max,))\n    else:\n        return pd.Series(innerfn(x, min, max), index=x.index)", "response": "Rescale a series of dataframes to fit a certain range [ min max )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the Sortino ratio of a series of returns.", "response": "def calc_sortino_ratio(returns, rf=0., nperiods=None, annualize=True):\n    \"\"\"\n    Calculates the `Sortino ratio <https://www.investopedia.com/terms/s/sortinoratio.asp>`_ given a series of returns (see `Sharpe vs. Sortino <https://www.investopedia.com/ask/answers/010815/what-difference-between-sharpe-ratio-and-sortino-ratio.asp>`_).\n\n    Args:\n        * returns (Series or DataFrame): Returns\n        * rf (float, Series): `Risk-free rate <https://www.investopedia.com/terms/r/risk-freerate.asp>`_ expressed in yearly (annualized) terms or return series.\n        * nperiods (int): Number of periods used for annualization. Must be\n            provided if rf is non-zero and rf is not a price series\n\n    \"\"\"\n    if type(rf) is float and rf != 0 and nperiods is None:\n        raise Exception('nperiods must be set if rf != 0 and rf is not a price series')\n\n    er = returns.to_excess_returns(rf, nperiods=nperiods)\n\n    negative_returns = np.minimum(returns[1:], 0.)\n    std = np.std(negative_returns, ddof=1)\n    res = np.divide(er.mean(), std)\n\n    if annualize:\n        if nperiods is None:\n            nperiods = 1\n        return res * np.sqrt(nperiods)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a series of returns the excess returns over rf.", "response": "def to_excess_returns(returns, rf, nperiods=None):\n    \"\"\"\n    Given a series of returns, it will return the excess returns over rf.\n\n    Args:\n        * returns (Series, DataFrame): Returns\n        * rf (float, Series): `Risk-Free rate(s) <https://www.investopedia.com/terms/r/risk-freerate.asp>`_ expressed in annualized term or return series\n        * nperiods (int): Optional. If provided, will convert rf to different\n            frequency using deannualize only if rf is a float\n    Returns:\n        * excess_returns (Series, DataFrame): Returns - rf\n\n    \"\"\"\n    if type(rf) is float and nperiods is not None:\n        _rf = deannualize(rf, nperiods)\n    else:\n        _rf = rf\n\n    return returns - _rf"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts from prices -> Ulcer index <https://www. investopedia. com / terms / u / ulcerindex.asp > _", "response": "def to_ulcer_index(prices):\n    \"\"\"\n    Converts from prices -> `Ulcer index <https://www.investopedia.com/terms/u/ulcerindex.asp>`_\n\n    See https://en.wikipedia.org/wiki/Ulcer_index\n\n    Args:\n        * prices (Series, DataFrame): Prices\n\n    \"\"\"\n    dd = prices.to_drawdown_series()\n    return np.divide(np.sqrt(np.sum(np.power(dd, 2))), dd.count())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_ulcer_performance_index(prices, rf=0., nperiods=None):\n    if type(rf) is float and rf != 0 and nperiods is None:\n        raise Exception('nperiods must be set if rf != 0 and rf is not a price series')\n\n    er = prices.to_returns().to_excess_returns(rf, nperiods=nperiods)\n\n    return np.divide(er.mean(), prices.to_ulcer_index())", "response": "Converts from prices to ulcer index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextending pandas object with some functions defined in this file.", "response": "def extend_pandas():\n    \"\"\"\n    Extends pandas' PandasObject (Series, Series,\n    DataFrame) with some functions defined in this file.\n\n    This facilitates common functional composition used in quant\n    finance.\n\n    Ex:\n        prices.to_returns().dropna().calc_clusters()\n        (where prices would be a DataFrame)\n    \"\"\"\n    PandasObject.to_returns = to_returns\n    PandasObject.to_log_returns = to_log_returns\n    PandasObject.to_price_index = to_price_index\n    PandasObject.rebase = rebase\n    PandasObject.calc_perf_stats = calc_perf_stats\n    PandasObject.to_drawdown_series = to_drawdown_series\n    PandasObject.calc_max_drawdown = calc_max_drawdown\n    PandasObject.calc_cagr = calc_cagr\n    PandasObject.calc_total_return = calc_total_return\n    PandasObject.as_percent = utils.as_percent\n    PandasObject.as_format = utils.as_format\n    PandasObject.to_monthly = to_monthly\n    PandasObject.asfreq_actual = asfreq_actual\n    PandasObject.drop_duplicate_cols = drop_duplicate_cols\n    PandasObject.calc_information_ratio = calc_information_ratio\n    PandasObject.calc_prob_mom = calc_prob_mom\n    PandasObject.calc_risk_return_ratio = calc_risk_return_ratio\n    PandasObject.calc_erc_weights = calc_erc_weights\n    PandasObject.calc_inv_vol_weights = calc_inv_vol_weights\n    PandasObject.calc_mean_var_weights = calc_mean_var_weights\n    PandasObject.calc_clusters = calc_clusters\n    PandasObject.calc_ftca = calc_ftca\n    PandasObject.calc_stats = calc_stats\n    PandasObject.plot_heatmap = plot_heatmap\n    PandasObject.plot_corr_heatmap = plot_corr_heatmap\n    PandasObject.rollapply = rollapply\n    PandasObject.winsorize = winsorize\n    PandasObject.rescale = rescale\n    PandasObject.calc_sortino_ratio = calc_sortino_ratio\n    PandasObject.calc_calmar_ratio = calc_calmar_ratio\n    PandasObject.calc_sharpe = calc_sharpe\n    PandasObject.to_excess_returns = to_excess_returns\n    PandasObject.to_ulcer_index = to_ulcer_index\n    PandasObject.to_ulcer_performance_index = to_ulcer_performance_index"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_riskfree_rate(self, rf):\n\n        \"\"\"\n        Set annual risk-free rate property and calculate properly annualized\n        monthly and daily rates. Then performance stats are recalculated.\n        Affects only this instance of the PerformanceStats.\n\n        Args:\n            * rf (float): Annual `risk-free rate <https://www.investopedia.com/terms/r/risk-freerate.asp>`_\n        \"\"\"\n        self.rf = rf\n\n        # Note, that we recalculate everything.\n        self._update(self.prices)", "response": "Set annual risk - free rate property and calculate properly annualized\n        monthly and daily rates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay an overview of the Series containing the basic statistics for the Series.", "response": "def display(self):\n        \"\"\"\n        Displays an overview containing descriptive stats for the Series\n        provided.\n        \"\"\"\n        print('Stats for %s from %s - %s' % (self.name, self.start, self.end))\n        if type(self.rf) is float:\n            print('Annual risk-free rate considered: %s' % (fmtp(self.rf)))\n        print('Summary:')\n        data = [[fmtp(self.total_return), fmtn(self.daily_sharpe),\n                 fmtp(self.cagr), fmtp(self.max_drawdown)]]\n        print(tabulate(data, headers=['Total Return', 'Sharpe',\n                                      'CAGR', 'Max Drawdown']))\n\n        print('\\nAnnualized Returns:')\n        data = [[fmtp(self.mtd), fmtp(self.three_month), fmtp(self.six_month),\n                 fmtp(self.ytd), fmtp(self.one_year), fmtp(self.three_year),\n                 fmtp(self.five_year), fmtp(self.ten_year),\n                 fmtp(self.incep)]]\n        print(tabulate(data,\n                       headers=['mtd', '3m', '6m', 'ytd', '1y',\n                                '3y', '5y', '10y', 'incep.']))\n\n        print('\\nPeriodic:')\n        data = [\n            ['sharpe', fmtn(self.daily_sharpe), fmtn(self.monthly_sharpe),\n             fmtn(self.yearly_sharpe)],\n            ['mean', fmtp(self.daily_mean), fmtp(self.monthly_mean),\n             fmtp(self.yearly_mean)],\n            ['vol', fmtp(self.daily_vol), fmtp(self.monthly_vol),\n             fmtp(self.yearly_vol)],\n            ['skew', fmtn(self.daily_skew), fmtn(self.monthly_skew),\n             fmtn(self.yearly_skew)],\n            ['kurt', fmtn(self.daily_kurt), fmtn(self.monthly_kurt),\n             fmtn(self.yearly_kurt)],\n            ['best', fmtp(self.best_day), fmtp(self.best_month),\n             fmtp(self.best_year)],\n            ['worst', fmtp(self.worst_day), fmtp(self.worst_month),\n             fmtp(self.worst_year)]]\n        print(tabulate(data, headers=['daily', 'monthly', 'yearly']))\n\n        print('\\nDrawdowns:')\n        data = [\n            [fmtp(self.max_drawdown), fmtp(self.avg_drawdown),\n             fmtn(self.avg_drawdown_days)]]\n        print(tabulate(data, headers=['max', 'avg', '# days']))\n\n        print('\\nMisc:')\n        data = [['avg. up month', fmtp(self.avg_up_month)],\n                ['avg. down month', fmtp(self.avg_down_month)],\n                ['up year %', fmtp(self.win_year_perc)],\n                ['12m up %', fmtp(self.twelve_month_win_perc)]]\n        print(tabulate(data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays a table containing monthly returns and ytd returns for every year in range.", "response": "def display_monthly_returns(self):\n        \"\"\"\n        Display a table containing monthly returns and ytd returns\n        for every year in range.\n        \"\"\"\n        data = [['Year', 'Jan', 'Feb', 'Mar', 'Apr', 'May',\n                 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'YTD']]\n        for k in self.return_table.index:\n            r = self.return_table.loc[k].values\n            data.append([k] + [fmtpn(x) for x in r])\n        print(tabulate(data, headers='firstrow'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot a histogram of returns given a return frequency.", "response": "def plot_histogram(self, freq=None, figsize=(15, 5), title=None,\n                       bins=20, **kwargs):\n        \"\"\"\n        Plots a histogram of returns given a return frequency.\n\n        Args:\n            * freq (str): Data frequency used for display purposes.\n                This will dictate the type of returns\n                (daily returns, monthly, ...)\n                Refer to pandas docs for valid period strings.\n            * figsize ((x,y)): figure size\n            * title (str): Title if default not appropriate\n            * bins (int): number of bins for the histogram\n            * kwargs: passed to pandas' hist method\n        \"\"\"\n        if title is None:\n            title = self._get_default_plot_title(\n                self.name, freq, 'Return Histogram')\n\n        ser = self._get_series(freq).to_returns().dropna()\n\n        plt.figure(figsize=figsize)\n        ax = ser.hist(bins=bins, figsize=figsize, normed=True, **kwargs)\n        ax.set_title(title)\n        plt.axvline(0, linewidth=4)\n        return ser.plot(kind='kde')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a CSV string with appropriate formatting.", "response": "def to_csv(self, sep=',', path=None):\n        \"\"\"\n        Returns a CSV string with appropriate formatting.\n        If path is not None, the string will be saved to file\n        at path.\n\n        Args:\n            * sep (char): Separator\n            * path (str): If None, CSV string returned. Else file written\n                to specified path.\n        \"\"\"\n        stats = self._stats()\n\n        data = []\n        first_row = ['Stat', self.name]\n        data.append(sep.join(first_row))\n\n        for stat in stats:\n            k, n, f = stat\n\n            # blank row\n            if k is None:\n                row = [''] * len(data[0])\n                data.append(sep.join(row))\n                continue\n            elif k == 'rf' and not type(self.rf) == float:\n                continue\n\n            row = [n]\n            raw = getattr(self, k)\n            if f is None:\n                row.append(raw)\n            elif f == 'p':\n                row.append(fmtp(raw))\n            elif f == 'n':\n                row.append(fmtn(raw))\n            elif f == 'dt':\n                row.append(raw.strftime('%Y-%m-%d'))\n            else:\n                raise NotImplementedError('unsupported format %s' % f)\n\n            data.append(sep.join(row))\n\n        res = '\\n'.join(data)\n\n        if path is not None:\n            with open(path, 'w') as fl:\n                fl.write(res)\n        else:\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the risk - free rate of the entry in this group.", "response": "def set_riskfree_rate(self, rf):\n\n        \"\"\"\n        Set annual `risk-free rate <https://www.investopedia.com/terms/r/risk-freerate.asp>`_ property and calculate properly annualized\n        monthly and daily rates. Then performance stats are recalculated.\n        Affects only those instances of PerformanceStats that are children of\n        this GroupStats object.\n\n        Args:\n            * rf (float, Series): Annual risk-free rate or risk-free rate price series\n        \"\"\"\n\n        for key in self._names:\n            self[key].set_riskfree_rate(rf)\n\n        # calculate stats for entire series\n        self._update_stats()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_date_range(self, start=None, end=None):\n        start = self._start if start is None else pd.to_datetime(start)\n        end = self._end if end is None else pd.to_datetime(end)\n        self._update(self._prices.loc[start:end])", "response": "Update the date range of the entries in the key - value table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display(self):\n        data = []\n        first_row = ['Stat']\n        first_row.extend(self._names)\n        data.append(first_row)\n\n        stats = self._stats()\n\n        for stat in stats:\n            k, n, f = stat\n            # blank row\n            if k is None:\n                row = [''] * len(data[0])\n                data.append(row)\n                continue\n\n            row = [n]\n            for key in self._names:\n                raw = getattr(self[key], k)\n\n                # if rf is a series print nan\n                if k == 'rf' and not type(raw) == float:\n                    row.append(np.nan)\n                elif f is None:\n                    row.append(raw)\n                elif f == 'p':\n                    row.append(fmtp(raw))\n                elif f == 'n':\n                    row.append(fmtn(raw))\n                elif f == 'dt':\n                    row.append(raw.strftime('%Y-%m-%d'))\n                else:\n                    raise NotImplementedError('unsupported format %s' % f)\n            data.append(row)\n\n        print(tabulate(data, headers='firstrow'))", "response": "Display summary stats table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef display_lookback_returns(self):\n        return self.lookback_returns.apply(\n            lambda x: x.map('{:,.2%}'.format), axis=1)", "response": "Displays the current lookback returns for each series."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the data in the specified frequency.", "response": "def plot(self, freq=None, figsize=(15, 5), title=None,\n             logy=False, **kwargs):\n        \"\"\"\n        Helper function for plotting the series.\n\n        Args:\n            * freq (str): Data frequency used for display purposes.\n                Refer to pandas docs for valid freq strings.\n            * figsize ((x,y)): figure size\n            * title (str): Title if default not appropriate\n            * logy (bool): log-scale for y axis\n            * kwargs: passed to pandas' plot method\n\n        \"\"\"\n\n        if title is None:\n            title = self._get_default_plot_title(\n                freq, 'Equity Progression')\n\n        ser = self._get_series(freq).rebase()\n        return ser.plot(figsize=figsize, logy=logy,\n                        title=title, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_scatter_matrix(self, freq=None, title=None,\n                            figsize=(10, 10), **kwargs):\n        \"\"\"\n        Wrapper around pandas' scatter_matrix.\n\n        Args:\n            * freq (str): Data frequency used for display purposes.\n                Refer to pandas docs for valid freq strings.\n            * figsize ((x,y)): figure size\n            * title (str): Title if default not appropriate\n            * kwargs: passed to pandas' scatter_matrix method\n\n        \"\"\"\n        if title is None:\n            title = self._get_default_plot_title(\n                freq, 'Return Scatter Matrix')\n\n        plt.figure()\n        ser = self._get_series(freq).to_returns().dropna()\n        pd.scatter_matrix(ser, figsize=figsize, **kwargs)\n        return plt.suptitle(title)", "response": "Wrapper around pandas scatter_matrix method that returns a scatter matrix of the data in the specified frequency."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap around pandas hist method.", "response": "def plot_histograms(self, freq=None, title=None,\n                        figsize=(10, 10), **kwargs):\n        \"\"\"\n        Wrapper around pandas' hist.\n\n        Args:\n            * freq (str): Data frequency used for display purposes.\n                Refer to pandas docs for valid freq strings.\n            * figsize ((x,y)): figure size\n            * title (str): Title if default not appropriate\n            * kwargs: passed to pandas' hist method\n\n        \"\"\"\n        if title is None:\n            title = self._get_default_plot_title(\n                freq, 'Return Histogram Matrix')\n\n        plt.figure()\n        ser = self._get_series(freq).to_returns().dropna()\n        ser.hist(figsize=figsize, **kwargs)\n        return plt.suptitle(title)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_correlation(self, freq=None, title=None,\n                         figsize=(12, 6), **kwargs):\n        \"\"\"\n        Utility function to plot correlations.\n\n        Args:\n            * freq (str): Pandas data frequency alias string\n            * title (str): Plot title\n            * figsize (tuple (x,y)): figure size\n            * kwargs: passed to Pandas' plot_corr_heatmap function\n\n        \"\"\"\n        if title is None:\n            title = self._get_default_plot_title(\n                freq, 'Return Correlation Matrix')\n\n        rets = self._get_series(freq).to_returns().dropna()\n        return rets.plot_corr_heatmap(title=title, figsize=figsize, **kwargs)", "response": "Utility function to plot correlation matrix of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_csv(self, sep=',', path=None):\n        data = []\n        first_row = ['Stat']\n        first_row.extend(self._names)\n        data.append(sep.join(first_row))\n\n        stats = self._stats()\n\n        for stat in stats:\n            k, n, f = stat\n            # blank row\n            if k is None:\n                row = [''] * len(data[0])\n                data.append(sep.join(row))\n                continue\n\n            row = [n]\n            for key in self._names:\n                raw = getattr(self[key], k)\n                if f is None:\n                    row.append(raw)\n                elif f == 'p':\n                    row.append(fmtp(raw))\n                elif f == 'n':\n                    row.append(fmtn(raw))\n                elif f == 'dt':\n                    row.append(raw.strftime('%Y-%m-%d'))\n                else:\n                    raise NotImplementedError('unsupported format %s' % f)\n            data.append(sep.join(row))\n\n        res = '\\n'.join(data)\n\n        if path is not None:\n            with open(path, 'w') as fl:\n                fl.write(res)\n        else:\n            return res", "response": "Returns a CSV string with appropriate formatting."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(tickers, provider=None, common_dates=True, forward_fill=False,\n        clean_tickers=True, column_names=None, ticker_field_sep=':',\n        mrefresh=False, existing=None, **kwargs):\n    \"\"\"\n    Helper function for retrieving data as a DataFrame.\n\n    Args:\n        * tickers (list, string, csv string): Tickers to download.\n        * provider (function): Provider to use for downloading data.\n            By default it will be ffn.DEFAULT_PROVIDER if not provided.\n        * common_dates (bool): Keep common dates only? Drop na's.\n        * forward_fill (bool): forward fill values if missing. Only works\n            if common_dates is False, since common_dates will remove\n            all nan's, so no filling forward necessary.\n        * clean_tickers (bool): Should the tickers be 'cleaned' using\n            ffn.utils.clean_tickers? Basically remove non-standard\n            characters (^VIX -> vix) and standardize to lower case.\n        * column_names (list): List of column names if clean_tickers\n            is not satisfactory.\n        * ticker_field_sep (char): separator used to determine the\n            ticker and field. This is in case we want to specify\n            particular, non-default fields. For example, we might\n            want: AAPL:Low,AAPL:High,AAPL:Close. ':' is the separator.\n        * mrefresh (bool): Ignore memoization.\n        * existing (DataFrame): Existing DataFrame to append returns\n            to - used when we download from multiple sources\n        * kwargs: passed to provider\n\n    \"\"\"\n\n    if provider is None:\n        provider = DEFAULT_PROVIDER\n\n    tickers = utils.parse_arg(tickers)\n\n    data = {}\n    for ticker in tickers:\n        t = ticker\n        f = None\n\n        # check for field\n        bits = ticker.split(ticker_field_sep, 1)\n        if len(bits) == 2:\n            t = bits[0]\n            f = bits[1]\n\n        # call provider - check if supports memoization\n        if hasattr(provider, 'mcache'):\n            data[ticker] = provider(ticker=t, field=f,\n                                    mrefresh=mrefresh, **kwargs)\n        else:\n            data[ticker] = provider(ticker=t, field=f, **kwargs)\n\n    df = pd.DataFrame(data)\n    # ensure same order as provided\n    df = df[tickers]\n\n    if existing is not None:\n        df = ffn.merge(existing, df)\n\n    if common_dates:\n        df = df.dropna()\n\n    if forward_fill:\n        df = df.fillna(method='ffill')\n\n    if column_names:\n        cnames = utils.parse_arg(column_names)\n        if len(cnames) != len(df.columns):\n            raise ValueError(\n                'column_names must be of same length as tickers')\n        df.columns = cnames\n    elif clean_tickers:\n        df.columns = map(utils.clean_ticker, df.columns)\n\n    return df", "response": "Helper function for retrieving data from a list of tickers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a single item from the Yahoo Seasonal Web API.", "response": "def web(ticker, field=None, start=None, end=None,\n        mrefresh=False, source='yahoo'):\n    \"\"\"\n    Data provider wrapper around pandas.io.data provider. Provides\n    memoization.\n    \"\"\"\n    if source == 'yahoo' and field is None:\n        field = 'Adj Close'\n\n    tmp = _download_web(ticker, data_source=source,\n                        start=start, end=end)\n\n    if tmp is None:\n        raise ValueError('failed to retrieve data for %s:%s' % (ticker, field))\n\n    if field:\n        return tmp[field]\n    else:\n        return tmp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading in a DataFrame from a csv file.", "response": "def csv(ticker, path='data.csv', field='', mrefresh=False, **kwargs):\n    \"\"\"\n    Data provider wrapper around pandas' read_csv. Provides memoization.\n    \"\"\"\n    # set defaults if not specified\n    if 'index_col' not in kwargs:\n        kwargs['index_col'] = 0\n    if 'parse_dates' not in kwargs:\n        kwargs['parse_dates'] = True\n\n    # read in dataframe from csv file\n    df = pd.read_csv(path, **kwargs)\n\n    tf = ticker\n    if field is not '' and field is not None:\n        tf = '%s:%s' % (tf, field)\n\n    # check that required column exists\n    if tf not in df:\n        raise ValueError('Ticker(field) not present in csv file!')\n\n    return df[tf]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef look(table, limit=0, vrepr=None, index_header=None, style=None,\n         truncate=None, width=None):\n    \"\"\"\n    Format a portion of the table as text for inspection in an interactive\n    session. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2]]\n        >>> etl.look(table1)\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'a' |   1 |\n        +-----+-----+\n        | 'b' |   2 |\n        +-----+-----+\n\n        >>> # alternative formatting styles\n        ... etl.look(table1, style='simple')\n        ===  ===\n        foo  bar\n        ===  ===\n        'a'    1\n        'b'    2\n        ===  ===\n\n        >>> etl.look(table1, style='minimal')\n        foo  bar\n        'a'    1\n        'b'    2\n\n        >>> # any irregularities in the length of header and/or data\n        ... # rows will appear as blank cells\n        ... table2 = [['foo', 'bar'],\n        ...           ['a'],\n        ...           ['b', 2, True]]\n        >>> etl.look(table2)\n        +-----+-----+------+\n        | foo | bar |      |\n        +=====+=====+======+\n        | 'a' |     |      |\n        +-----+-----+------+\n        | 'b' |   2 | True |\n        +-----+-----+------+\n\n    Three alternative presentation styles are available: 'grid', 'simple' and\n    'minimal', where 'grid' is the default. A different style can be specified\n    using the `style` keyword argument. The default style can also be changed\n    by setting ``petl.config.look_style``.\n\n    \"\"\"\n\n    # determine defaults\n    if limit == 0:\n        limit = config.look_limit\n    if vrepr is None:\n        vrepr = config.look_vrepr\n    if index_header is None:\n        index_header = config.look_index_header\n    if style is None:\n        style = config.look_style\n    if width is None:\n        width = config.look_width\n\n    return Look(table, limit=limit, vrepr=vrepr, index_header=index_header,\n                style=style, truncate=truncate, width=width)", "response": "Look for a portion of a table in an interactive sequence of unique items."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookstr(table, limit=0, **kwargs):\n\n    kwargs['vrepr'] = str\n    return look(table, limit=limit, **kwargs)", "response": "Like petl. util. vis. look but use str instead of repr for data\n    values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees a portion of a table.", "response": "def see(table, limit=0, vrepr=None, index_header=None):\n    \"\"\"\n    Format a portion of a table as text in a column-oriented layout for\n    inspection in an interactive session. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar'], ['a', 1], ['b', 2]]\n        >>> etl.see(table)\n        foo: 'a', 'b'\n        bar: 1, 2\n\n    Useful for tables with a larger number of fields.\n\n\n    \"\"\"\n\n    # determine defaults\n    if limit == 0:\n        limit = config.see_limit\n    if vrepr is None:\n        vrepr = config.see_vrepr\n    if index_header is None:\n        index_header = config.see_index_header\n\n    return See(table, limit=limit, vrepr=vrepr, index_header=index_header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef display(table, limit=0, vrepr=None, index_header=None, caption=None,\n            tr_style=None, td_styles=None, encoding=None, truncate=None,\n            epilogue=None):\n    \"\"\"\n    Display a table inline within an IPython notebook.\n\n    \"\"\"\n\n    from IPython.core.display import display_html\n    html = _display_html(table, limit=limit, vrepr=vrepr,\n                         index_header=index_header, caption=caption,\n                         tr_style=tr_style, td_styles=td_styles,\n                         encoding=encoding, truncate=truncate,\n                         epilogue=epilogue)\n    display_html(html, raw=True)", "response": "Display a table inline within an IPython notebook."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fromxlsx(filename, sheet=None, range_string=None, row_offset=0,\n             column_offset=0, **kwargs):\n    \"\"\"\n    Extract a table from a sheet in an Excel .xlsx file.\n\n    N.B., the sheet name is case sensitive.\n\n    The `sheet` argument can be omitted, in which case the first sheet in\n    the workbook is used by default.\n\n    The `range_string` argument can be used to provide a range string\n    specifying a range of cells to extract.\n\n    The `row_offset` and `column_offset` arguments can be used to\n    specify offsets.\n\n    Any other keyword arguments are passed through to\n    :func:`openpyxl.load_workbook()`.\n\n    \"\"\"\n\n    return XLSXView(filename, sheet=sheet, range_string=range_string,\n                    row_offset=row_offset, column_offset=column_offset,\n                    **kwargs)", "response": "Extract a table from an Excel. xlsx file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a table to a new Excel. xlsx file.", "response": "def toxlsx(tbl, filename, sheet=None, encoding=None):\n    \"\"\"\n    Write a table to a new Excel .xlsx file.\n\n    \"\"\"\n\n    import openpyxl\n    if encoding is None:\n        encoding = locale.getpreferredencoding()\n    wb = openpyxl.Workbook(write_only=True)\n    ws = wb.create_sheet(title=sheet)\n    for row in tbl:\n        ws.append(row)\n    wb.save(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide access to data from any DB - API 2. 0 connection via a given query.", "response": "def fromdb(dbo, query, *args, **kwargs):\n    \"\"\"Provides access to data from any DB-API 2.0 connection via a given query.\n    E.g., using :mod:`sqlite3`::\n\n        >>> import petl as etl\n        >>> import sqlite3\n        >>> connection = sqlite3.connect('example.db')\n        >>> table = etl.fromdb(connection, 'SELECT * FROM example')\n\n    E.g., using :mod:`psycopg2` (assuming you've installed it first)::\n\n        >>> import petl as etl\n        >>> import psycopg2\n        >>> connection = psycopg2.connect('dbname=example user=postgres')\n        >>> table = etl.fromdb(connection, 'SELECT * FROM example')\n\n    E.g., using :mod:`pymysql` (assuming you've installed it first)::\n\n        >>> import petl as etl\n        >>> import pymysql\n        >>> connection = pymysql.connect(password='moonpie', database='thangs')\n        >>> table = etl.fromdb(connection, 'SELECT * FROM example')\n\n    The `dbo` argument may also be a function that creates a cursor. N.B., each\n    call to the function should return a new cursor. E.g.::\n\n        >>> import petl as etl\n        >>> import psycopg2\n        >>> connection = psycopg2.connect('dbname=example user=postgres')\n        >>> mkcursor = lambda: connection.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        >>> table = etl.fromdb(mkcursor, 'SELECT * FROM example')\n\n    The parameter `dbo` may also be an SQLAlchemy engine, session or\n    connection object.\n\n    The parameter `dbo` may also be a string, in which case it is interpreted as\n    the name of a file containing an :mod:`sqlite3` database.\n\n    Note that the default behaviour of most database servers and clients is for\n    the entire result set for each query to be sent from the server to the\n    client. If your query returns a large result set this can result in\n    significant memory usage at the client. Some databases support server-side\n    cursors which provide a means for client libraries to fetch result sets\n    incrementally, reducing memory usage at the client.\n\n    To use a server-side cursor with a PostgreSQL database, e.g.::\n\n        >>> import petl as etl\n        >>> import psycopg2\n        >>> connection = psycopg2.connect('dbname=example user=postgres')\n        >>> table = etl.fromdb(lambda: connection.cursor(name='arbitrary'),\n        ...                    'SELECT * FROM example')\n\n    For more information on server-side cursors see the following links:\n\n        * http://initd.org/psycopg/docs/usage.html#server-side-cursors\n        * http://mysql-python.sourceforge.net/MySQLdb.html#using-and-extending\n\n    \"\"\"\n\n    # convenience for working with sqlite3\n    if isinstance(dbo, string_types):\n        import sqlite3\n        dbo = sqlite3.connect(dbo)\n\n    return DbView(dbo, query, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading data into a database table.", "response": "def todb(table, dbo, tablename, schema=None, commit=True,\n         create=False, drop=False, constraints=True, metadata=None,\n         dialect=None, sample=1000):\n    \"\"\"\n    Load data into an existing database table via a DB-API 2.0\n    connection or cursor. Note that the database table will be truncated,\n    i.e., all existing rows will be deleted prior to inserting the new data.\n    E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar'],\n        ...          ['a', 1],\n        ...          ['b', 2],\n        ...          ['c', 2]]\n        >>> # using sqlite3\n        ... import sqlite3\n        >>> connection = sqlite3.connect('example.db')\n        >>> # assuming table \"foobar\" already exists in the database\n        ... etl.todb(table, connection, 'foobar')\n        >>> # using psycopg2\n        >>> import psycopg2\n        >>> connection = psycopg2.connect('dbname=example user=postgres')\n        >>> # assuming table \"foobar\" already exists in the database\n        ... etl.todb(table, connection, 'foobar')\n        >>> # using pymysql\n        >>> import pymysql\n        >>> connection = pymysql.connect(password='moonpie', database='thangs')\n        >>> # tell MySQL to use standard quote character\n        ... connection.cursor().execute('SET SQL_MODE=ANSI_QUOTES')\n        >>> # load data, assuming table \"foobar\" already exists in the database\n        ... etl.todb(table, connection, 'foobar')\n\n    N.B., for MySQL the statement ``SET SQL_MODE=ANSI_QUOTES`` is required to\n    ensure MySQL uses SQL-92 standard quote characters.\n\n    A cursor can also be provided instead of a connection, e.g.::\n\n        >>> import psycopg2\n        >>> connection = psycopg2.connect('dbname=example user=postgres')\n        >>> cursor = connection.cursor()\n        >>> etl.todb(table, cursor, 'foobar')\n\n    The parameter `dbo` may also be an SQLAlchemy engine, session or\n    connection object.\n\n    The parameter `dbo` may also be a string, in which case it is interpreted\n    as the name of a file containing an :mod:`sqlite3` database.\n\n    If ``create=True`` this function will attempt to automatically create a\n    database table before loading the data. This functionality requires\n    `SQLAlchemy <http://www.sqlalchemy.org/>`_ to be installed.\n\n    **Keyword arguments:**\n\n    table : table container\n        Table data to load\n    dbo : database object\n        DB-API 2.0 connection, callable returning a DB-API 2.0 cursor, or\n        SQLAlchemy connection, engine or session\n    tablename : string\n        Name of the table in the database\n    schema : string\n        Name of the database schema to find the table in\n    commit : bool\n        If True commit the changes\n    create : bool\n        If True attempt to create the table before loading, inferring types\n        from a sample of the data (requires SQLAlchemy)\n    drop : bool\n        If True attempt to drop the table before recreating (only relevant if\n        create=True)\n    constraints : bool\n        If True use length and nullable constraints (only relevant if\n        create=True)\n    metadata : sqlalchemy.MetaData\n        Custom table metadata (only relevant if create=True)\n    dialect : string\n        One of {'access', 'sybase', 'sqlite', 'informix', 'firebird', 'mysql',\n        'oracle', 'maxdb', 'postgresql', 'mssql'} (only relevant if\n        create=True)\n    sample : int\n        Number of rows to sample when inferring types etc. Set to 0 to use the\n        whole table (only relevant if create=True)\n\n    .. note::\n\n        This function is in principle compatible with any DB-API 2.0\n        compliant database driver. However, at the time of writing some DB-API\n        2.0 implementations, including cx_Oracle and MySQL's\n        Connector/Python, are not compatible with this function, because they\n        only accept a list argument to the cursor.executemany() function\n        called internally by :mod:`petl`. This can be worked around by\n        proxying the cursor objects, e.g.::\n\n            >>> import cx_Oracle\n            >>> connection = cx_Oracle.Connection(...)\n            >>> class CursorProxy(object):\n            ...     def __init__(self, cursor):\n            ...         self._cursor = cursor\n            ...     def executemany(self, statement, parameters, **kwargs):\n            ...         # convert parameters to a list\n            ...         parameters = list(parameters)\n            ...         # pass through to proxied cursor\n            ...         return self._cursor.executemany(statement, parameters, **kwargs)\n            ...     def __getattr__(self, item):\n            ...         return getattr(self._cursor, item)\n            ...\n            >>> def get_cursor():\n            ...     return CursorProxy(connection.cursor())\n            ...\n            >>> import petl as etl\n            >>> etl.todb(tbl, get_cursor, ...)\n\n        Note however that this does imply loading the entire table into\n        memory as a list prior to inserting into the database.\n\n    \"\"\"\n\n    needs_closing = False\n\n    # convenience for working with sqlite3\n    if isinstance(dbo, string_types):\n        import sqlite3\n        dbo = sqlite3.connect(dbo)\n        needs_closing = True\n\n    try:\n        if create:\n            if drop:\n                drop_table(dbo, tablename, schema=schema, commit=commit)\n            create_table(table, dbo, tablename, schema=schema, commit=commit,\n                         constraints=constraints, metadata=metadata,\n                         dialect=dialect, sample=sample)\n        _todb(table, dbo, tablename, schema=schema, commit=commit,\n              truncate=True)\n\n    finally:\n        if needs_closing:\n            dbo.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload data into an existing database table.", "response": "def appenddb(table, dbo, tablename, schema=None, commit=True):\n    \"\"\"\n    Load data into an existing database table via a DB-API 2.0\n    connection or cursor. As :func:`petl.io.db.todb` except that the database\n    table will be appended, i.e., the new data will be inserted into the\n    table, and any existing rows will remain.\n\n    \"\"\"\n\n    needs_closing = False\n\n    # convenience for working with sqlite3\n    if isinstance(dbo, string_types):\n        import sqlite3\n        dbo = sqlite3.connect(dbo)\n        needs_closing = True\n\n    try:\n        _todb(table, dbo, tablename, schema=schema, commit=commit,\n              truncate=False)\n\n    finally:\n        if needs_closing:\n            dbo.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef columns(table, missing=None):\n\n    cols = OrderedDict()\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    for f in flds:\n        cols[f] = list()\n    for row in it:\n        for f, v in izip_longest(flds, row, fillvalue=missing):\n            if f in cols:\n                cols[f].append(v)\n    return cols", "response": "Construct a dict mapping field names to lists of values. Eg. ::\n    Construct a : class : dict mapping field names to lists of values. Eg. ::\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of the faceted columns of the given table.", "response": "def facetcolumns(table, key, missing=None):\n    \"\"\"\n    Like :func:`petl.util.materialise.columns` but stratified by values of the\n    given key field. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar', 'baz'],\n        ...          ['a', 1, True],\n        ...          ['b', 2, True],\n        ...          ['b', 3]]\n        >>> fc = etl.facetcolumns(table, 'foo')\n        >>> fc['a']\n        {'foo': ['a'], 'bar': [1], 'baz': [True]}\n        >>> fc['b']\n        {'foo': ['b', 'b'], 'bar': [2, 3], 'baz': [True, None]}\n\n    \"\"\"\n\n    fct = dict()\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    indices = asindices(hdr, key)\n    assert len(indices) > 0, 'no key field selected'\n    getkey = operator.itemgetter(*indices)\n\n    for row in it:\n        kv = getkey(row)\n        if kv not in fct:\n            cols = dict()\n            for f in flds:\n                cols[f] = list()\n            fct[kv] = cols\n        else:\n            cols = fct[kv]\n        for f, v in izip_longest(flds, row, fillvalue=missing):\n            if f in cols:\n                cols[f].append(v)\n\n    return fct"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef topickle(table, source=None, protocol=-1, write_header=True):\n\n    _writepickle(table, source=source, mode='wb', protocol=protocol,\n                 write_header=write_header)", "response": "Write the given table to a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend data to an existing pickle file.", "response": "def appendpickle(table, source=None, protocol=-1, write_header=False):\n    \"\"\"\n    Append data to an existing pickle file. I.e.,\n    as :func:`petl.io.pickle.topickle` but the file is opened in append mode.\n\n    Note that no attempt is made to check that the fields or row lengths are\n    consistent with the existing data, the data rows from the table are simply\n    appended to the file.\n\n    \"\"\"\n\n    _writepickle(table, source=source, mode='ab', protocol=protocol,\n                 write_header=write_header)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a table that writes rows to a pickle file as they are iterated over.", "response": "def teepickle(table, source=None, protocol=-1, write_header=True):\n    \"\"\"\n    Return a table that writes rows to a pickle file as they are iterated\n    over.\n\n    \"\"\"\n\n    return TeePickleView(table, source=source, protocol=protocol,\n                         write_header=write_header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts data from a JSON file.", "response": "def fromjson(source, *args, **kwargs):\n    \"\"\"\n    Extract data from a JSON file. The file must contain a JSON array as\n    the top level object, and each member of the array will be treated as a\n    row of data. E.g.::\n\n        >>> import petl as etl\n        >>> data = '''\n        ... [{\"foo\": \"a\", \"bar\": 1},\n        ... {\"foo\": \"b\", \"bar\": 2},\n        ... {\"foo\": \"c\", \"bar\": 2}]\n        ... '''\n        >>> with open('example.json', 'w') as f:\n        ...     f.write(data)\n        ...\n        74\n        >>> table1 = etl.fromjson('example.json', header=['foo', 'bar'])\n        >>> table1\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'a' |   1 |\n        +-----+-----+\n        | 'b' |   2 |\n        +-----+-----+\n        | 'c' |   2 |\n        +-----+-----+\n\n    If your JSON file does not fit this structure, you will need to parse it\n    via :func:`json.load` and select the array to treat as the data, see also\n    :func:`petl.io.json.fromdicts`.\n\n    .. versionchanged:: 1.1.0\n\n    If no `header` is specified, fields will be discovered by sampling keys\n    from the first `sample` objects in `source`. The header will be\n    constructed from keys in the order discovered. Note that this\n    ordering may not be stable, and therefore it may be advisable to specify\n    an explicit `header` or to use another function like\n    :func:`petl.transform.headers.sortheader` on the resulting table to\n    guarantee stability.\n\n    \"\"\"\n\n    source = read_source_from_arg(source)\n    return JsonView(source, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a DictsView with a list of dicts.", "response": "def fromdicts(dicts, header=None, sample=1000, missing=None):\n    \"\"\"\n    View a sequence of Python :class:`dict` as a table. E.g.::\n\n        >>> import petl as etl\n        >>> dicts = [{\"foo\": \"a\", \"bar\": 1},\n        ...          {\"foo\": \"b\", \"bar\": 2},\n        ...          {\"foo\": \"c\", \"bar\": 2}]\n        >>> table1 = etl.fromdicts(dicts, header=['foo', 'bar'])\n        >>> table1\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'a' |   1 |\n        +-----+-----+\n        | 'b' |   2 |\n        +-----+-----+\n        | 'c' |   2 |\n        +-----+-----+\n\n    If `header` is not specified, `sample` items from `dicts` will be\n    inspected to discovery dictionary keys. Note that the order in which\n    dictionary keys are discovered may not be stable,\n\n    See also :func:`petl.io.json.fromjson`.\n\n    .. versionchanged:: 1.1.0\n\n    If no `header` is specified, fields will be discovered by sampling keys\n    from the first `sample` dictionaries in `dicts`. The header will be\n    constructed from keys in the order discovered. Note that this\n    ordering may not be stable, and therefore it may be advisable to specify\n    an explicit `header` or to use another function like\n    :func:`petl.transform.headers.sortheader` on the resulting table to\n    guarantee stability.\n\n    \"\"\"\n\n    return DictsView(dicts, header=header, sample=sample, missing=missing)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a table in JSON format with rows output as JSON objects.", "response": "def tojson(table, source=None, prefix=None, suffix=None, *args, **kwargs):\n    \"\"\"\n    Write a table in JSON format, with rows output as JSON objects. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2],\n        ...           ['c', 2]]\n        >>> etl.tojson(table1, 'example.json', sort_keys=True)\n        >>> # check what it did\n        ... print(open('example.json').read())\n        [{\"bar\": 1, \"foo\": \"a\"}, {\"bar\": 2, \"foo\": \"b\"}, {\"bar\": 2, \"foo\": \"c\"}]\n\n    Note that this is currently not streaming, all data is loaded into memory\n    before being written to the file.\n\n    \"\"\"\n\n    obj = list(_dicts(table))\n    _writejson(source, obj, prefix, suffix, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tojsonarrays(table, source=None, prefix=None, suffix=None,\n                 output_header=False, *args, **kwargs):\n    \"\"\"\n    Write a table in JSON format, with rows output as JSON arrays. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2],\n        ...           ['c', 2]]\n        >>> etl.tojsonarrays(table1, 'example.json')\n        >>> # check what it did\n        ... print(open('example.json').read())\n        [[\"a\", 1], [\"b\", 2], [\"c\", 2]]\n\n    Note that this is currently not streaming, all data is loaded into memory\n    before being written to the file.\n\n    \"\"\"\n\n    if output_header:\n        obj = list(table)\n    else:\n        obj = list(data(table))\n    _writejson(source, obj, prefix, suffix, *args, **kwargs)", "response": "Write a table in JSON format with rows output as JSON arrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading data from the given table into a numpy array. E. g.", "response": "def toarray(table, dtype=None, count=-1, sample=1000):\n    \"\"\"\n    Load data from the given `table` into a\n    `numpy <http://www.numpy.org/>`_ structured array. E.g.::\n\n        >>> import petl as etl\n        >>> table = [('foo', 'bar', 'baz'),\n        ...          ('apples', 1, 2.5),\n        ...          ('oranges', 3, 4.4),\n        ...          ('pears', 7, .1)]\n        >>> a = etl.toarray(table)\n        >>> a\n        array([('apples', 1, 2.5), ('oranges', 3, 4.4), ('pears', 7, 0.1)],\n              dtype=(numpy.record, [('foo', '<U7'), ('bar', '<i8'), ('baz', '<f8')]))\n        >>> # the dtype can be specified as a string\n        ... a = etl.toarray(table, dtype='a4, i2, f4')\n        >>> a\n        array([(b'appl', 1, 2.5), (b'oran', 3, 4.4), (b'pear', 7, 0.1)],\n              dtype=[('foo', 'S4'), ('bar', '<i2'), ('baz', '<f4')])\n        >>> # the dtype can also be partially specified\n        ... a = etl.toarray(table, dtype={'foo': 'a4'})\n        >>> a\n        array([(b'appl', 1, 2.5), (b'oran', 3, 4.4), (b'pear', 7, 0.1)],\n              dtype=[('foo', 'S4'), ('bar', '<i8'), ('baz', '<f8')])\n\n    If the dtype is not completely specified, `sample` rows will be\n    examined to infer an appropriate dtype.\n\n    \"\"\"\n\n    import numpy as np\n    it = iter(table)\n    peek, it = iterpeek(it, sample)\n    hdr = next(it)\n    flds = list(map(str, hdr))\n    dtype = construct_dtype(flds, peek, dtype)\n\n    # numpy is fussy about having tuples, need to make sure\n    it = (tuple(row) for row in it)\n    sa = np.fromiter(it, dtype=dtype, count=count)\n\n    return sa"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef torecarray(*args, **kwargs):\n\n    import numpy as np\n    return toarray(*args, **kwargs).view(np.recarray)", "response": "A convenience method for creating a new sequence of numpy. recarray objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads values from a table column into a numpy array.", "response": "def valuestoarray(vals, dtype=None, count=-1, sample=1000):\n    \"\"\"\n    Load values from a table column into a `numpy <http://www.numpy.org/>`_\n    array, e.g.::\n\n        >>> import petl as etl\n        >>> table = [('foo', 'bar', 'baz'),\n        ...          ('apples', 1, 2.5),\n        ...          ('oranges', 3, 4.4),\n        ...          ('pears', 7, .1)]\n        >>> table = etl.wrap(table)\n        >>> table.values('bar').array()\n        array([1, 3, 7])\n        >>> # specify dtype\n        ... table.values('bar').array(dtype='i4')\n        array([1, 3, 7], dtype=int32)\n\n    \"\"\"\n\n    import numpy as np\n    it = iter(vals)\n    if dtype is None:\n        peek, it = iterpeek(it, sample)\n        dtype = np.array(peek).dtype\n    a = np.fromiter(it, dtype=dtype, count=count)\n    return a"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fromxml(source, *args, **kwargs):\n\n    source = read_source_from_arg(source)\n    return XmlView(source, *args, **kwargs)", "response": "Extract data from an XML file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert values under one or more fields via arbitrary functions.", "response": "def convert(table, *args, **kwargs):\n    \"\"\"Transform values under one or more fields via arbitrary functions, method\n    invocations or dictionary translations. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', '2.4', 12],\n        ...           ['B', '5.7', 34],\n        ...           ['C', '1.2', 56]]\n        >>> # using a built-in function:\n        ... table2 = etl.convert(table1, 'bar', float)\n        >>> table2\n        +-----+-----+-----+\n        | foo | bar | baz |\n        +=====+=====+=====+\n        | 'A' | 2.4 |  12 |\n        +-----+-----+-----+\n        | 'B' | 5.7 |  34 |\n        +-----+-----+-----+\n        | 'C' | 1.2 |  56 |\n        +-----+-----+-----+\n\n        >>> # using a lambda function::\n        ... table3 = etl.convert(table1, 'baz', lambda v: v*2)\n        >>> table3\n        +-----+-------+-----+\n        | foo | bar   | baz |\n        +=====+=======+=====+\n        | 'A' | '2.4' |  24 |\n        +-----+-------+-----+\n        | 'B' | '5.7' |  68 |\n        +-----+-------+-----+\n        | 'C' | '1.2' | 112 |\n        +-----+-------+-----+\n\n        >>> # a method of the data value can also be invoked by passing\n        ... # the method name\n        ... table4 = etl.convert(table1, 'foo', 'lower')\n        >>> table4\n        +-----+-------+-----+\n        | foo | bar   | baz |\n        +=====+=======+=====+\n        | 'a' | '2.4' |  12 |\n        +-----+-------+-----+\n        | 'b' | '5.7' |  34 |\n        +-----+-------+-----+\n        | 'c' | '1.2' |  56 |\n        +-----+-------+-----+\n\n        >>> # arguments to the method invocation can also be given\n        ... table5 = etl.convert(table1, 'foo', 'replace', 'A', 'AA')\n        >>> table5\n        +------+-------+-----+\n        | foo  | bar   | baz |\n        +======+=======+=====+\n        | 'AA' | '2.4' |  12 |\n        +------+-------+-----+\n        | 'B'  | '5.7' |  34 |\n        +------+-------+-----+\n        | 'C'  | '1.2' |  56 |\n        +------+-------+-----+\n\n        >>> # values can also be translated via a dictionary\n        ... table7 = etl.convert(table1, 'foo', {'A': 'Z', 'B': 'Y'})\n        >>> table7\n        +-----+-------+-----+\n        | foo | bar   | baz |\n        +=====+=======+=====+\n        | 'Z' | '2.4' |  12 |\n        +-----+-------+-----+\n        | 'Y' | '5.7' |  34 |\n        +-----+-------+-----+\n        | 'C' | '1.2' |  56 |\n        +-----+-------+-----+\n\n        >>> # the same conversion can be applied to multiple fields\n        ... table8 = etl.convert(table1, ('foo', 'bar', 'baz'), str)\n        >>> table8\n        +-----+-------+------+\n        | foo | bar   | baz  |\n        +=====+=======+======+\n        | 'A' | '2.4' | '12' |\n        +-----+-------+------+\n        | 'B' | '5.7' | '34' |\n        +-----+-------+------+\n        | 'C' | '1.2' | '56' |\n        +-----+-------+------+\n\n        >>> # multiple conversions can be specified at the same time\n        ... table9 = etl.convert(table1, {'foo': 'lower',\n        ...                               'bar': float,\n        ...                               'baz': lambda v: v * 2})\n        >>> table9\n        +-----+-----+-----+\n        | foo | bar | baz |\n        +=====+=====+=====+\n        | 'a' | 2.4 |  24 |\n        +-----+-----+-----+\n        | 'b' | 5.7 |  68 |\n        +-----+-----+-----+\n        | 'c' | 1.2 | 112 |\n        +-----+-----+-----+\n\n        >>> # ...or alternatively via a list\n        ... table10 = etl.convert(table1, ['lower', float, lambda v: v*2])\n        >>> table10\n        +-----+-----+-----+\n        | foo | bar | baz |\n        +=====+=====+=====+\n        | 'a' | 2.4 |  24 |\n        +-----+-----+-----+\n        | 'b' | 5.7 |  68 |\n        +-----+-----+-----+\n        | 'c' | 1.2 | 112 |\n        +-----+-----+-----+\n\n        >>> # conversion can be conditional\n        ... table11 = etl.convert(table1, 'baz', lambda v: v * 2,\n        ...                       where=lambda r: r.foo == 'B')\n        >>> table11\n        +-----+-------+-----+\n        | foo | bar   | baz |\n        +=====+=======+=====+\n        | 'A' | '2.4' |  12 |\n        +-----+-------+-----+\n        | 'B' | '5.7' |  68 |\n        +-----+-------+-----+\n        | 'C' | '1.2' |  56 |\n        +-----+-------+-----+\n\n        >>> # conversion can access other values from the same row\n        ... table12 = etl.convert(table1, 'baz',\n        ...                       lambda v, row: v * float(row.bar),\n        ...                       pass_row=True)\n        >>> table12\n        +-----+-------+--------------------+\n        | foo | bar   | baz                |\n        +=====+=======+====================+\n        | 'A' | '2.4' | 28.799999999999997 |\n        +-----+-------+--------------------+\n        | 'B' | '5.7' |              193.8 |\n        +-----+-------+--------------------+\n        | 'C' | '1.2' |               67.2 |\n        +-----+-------+--------------------+\n\n    Note that either field names or indexes can be given.\n\n    The ``where`` keyword argument can be given with a callable or expression\n    which is evaluated on each row and which should return True if the\n    conversion should be applied on that row, else False.\n\n    The ``pass_row`` keyword argument can be given, which if True will mean\n    that both the value and the containing row will be passed as\n    arguments to the conversion function (so, i.e., the conversion function\n    should accept two arguments).\n\n    \"\"\"\n\n    converters = None\n    if len(args) == 0:\n        # no conversion specified, can be set afterwards via suffix notation\n        pass\n    elif len(args) == 1:\n        converters = args[0]\n    elif len(args) > 1:\n        converters = dict()\n        # assume first arg is field name or spec\n        field = args[0]\n        if len(args) == 2:\n            conv = args[1]\n        else:\n            conv = args[1:]\n        if isinstance(field, (list, tuple)):  # allow for multiple fields\n            for f in field:\n                converters[f] = conv\n        else:\n            converters[field] = conv\n    return FieldConvertView(table, converters, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting all field values to numbers where is possible.", "response": "def convertnumbers(table, strict=False, **kwargs):\n    \"\"\"\n    Convenience function to convert all field values to numbers where\n    possible. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz', 'quux'],\n        ...           ['1', '3.0', '9+3j', 'aaa'],\n        ...           ['2', '1.3', '7+2j', None]]\n        >>> table2 = etl.convertnumbers(table1)\n        >>> table2\n        +-----+-----+--------+-------+\n        | foo | bar | baz    | quux  |\n        +=====+=====+========+=======+\n        |   1 | 3.0 | (9+3j) | 'aaa' |\n        +-----+-----+--------+-------+\n        |   2 | 1.3 | (7+2j) | None  |\n        +-----+-----+--------+-------+\n\n    \"\"\"\n\n    return convertall(table, numparser(strict), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format(table, field, fmt, **kwargs):\n\n    conv = lambda v: fmt.format(v)\n    return convert(table, field, conv, **kwargs)", "response": "Convenience function to format all values in the given field using the given format string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef formatall(table, fmt, **kwargs):\n\n    conv = lambda v: fmt.format(v)\n    return convertall(table, conv, **kwargs)", "response": "Convenience function to format all values in all fields using the\n    format string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interpolateall(table, fmt, **kwargs):\n\n    conv = lambda v: fmt % v\n    return convertall(table, conv, **kwargs)", "response": "Helper function to interpolate all values in all fields using the fmt string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sort(table, key=None, reverse=False, buffersize=None, tempdir=None,\n         cache=True):\n    \"\"\"\n    Sort the table. Field names or indices (from zero) can be used to specify\n    the key. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['C', 2],\n        ...           ['A', 9],\n        ...           ['A', 6],\n        ...           ['F', 1],\n        ...           ['D', 10]]\n        >>> table2 = etl.sort(table1, 'foo')\n        >>> table2\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'A' |   9 |\n        +-----+-----+\n        | 'A' |   6 |\n        +-----+-----+\n        | 'C' |   2 |\n        +-----+-----+\n        | 'D' |  10 |\n        +-----+-----+\n        | 'F' |   1 |\n        +-----+-----+\n\n        >>> # sorting by compound key is supported\n        ... table3 = etl.sort(table1, key=['foo', 'bar'])\n        >>> table3\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'A' |   6 |\n        +-----+-----+\n        | 'A' |   9 |\n        +-----+-----+\n        | 'C' |   2 |\n        +-----+-----+\n        | 'D' |  10 |\n        +-----+-----+\n        | 'F' |   1 |\n        +-----+-----+\n\n        >>> # if no key is specified, the default is a lexical sort\n        ... table4 = etl.sort(table1)\n        >>> table4\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'A' |   6 |\n        +-----+-----+\n        | 'A' |   9 |\n        +-----+-----+\n        | 'C' |   2 |\n        +-----+-----+\n        | 'D' |  10 |\n        +-----+-----+\n        | 'F' |   1 |\n        +-----+-----+\n\n    The `buffersize` argument should be an `int` or `None`.\n\n    If the number of rows in the table is less than `buffersize`, the table\n    will be sorted in memory. Otherwise, the table is sorted in chunks of\n    no more than `buffersize` rows, each chunk is written to a temporary file,\n    and then a merge sort is performed on the temporary files.\n\n    If `buffersize` is `None`, the value of\n    `petl.config.sort_buffersize` will be used. By default this is\n    set to 100000 rows, but can be changed, e.g.::\n\n        >>> import petl.config\n        >>> petl.config.sort_buffersize = 500000\n\n    If `petl.config.sort_buffersize` is set to `None`, this forces\n    all sorting to be done entirely in memory.\n\n    By default the results of the sort will be cached, and so a second pass over\n    the sorted table will yield rows from the cache and will not repeat the\n    sort operation. To turn off caching, set the `cache` argument to `False`.\n\n    \"\"\"\n\n    return SortView(table, key=key, reverse=reverse, buffersize=buffersize,\n                    tempdir=tempdir, cache=cache)", "response": "Sort the table by the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a single iterator over the given iterables sorted by the given the key function.", "response": "def _heapqmergesorted(key=None, *iterables):\n    \"\"\"Return a single iterator over the given iterables, sorted by the\n    given `key` function, assuming the input iterables are already sorted by\n    the same function. (I.e., the merge part of a general merge sort.) Uses\n    :func:`heapq.merge` for the underlying implementation.\"\"\"\n\n    if key is None:\n        keyed_iterables = iterables\n        for element in heapq.merge(*keyed_iterables):\n            yield element\n    else:\n        keyed_iterables = [(_Keyed(key(obj), obj) for obj in iterable)\n                           for iterable in iterables]\n        for element in heapq.merge(*keyed_iterables):\n            yield element.obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a single iterator over the given iterables sorted by the given key function.", "response": "def _shortlistmergesorted(key=None, reverse=False, *iterables):\n    \"\"\"Return a single iterator over the given iterables, sorted by the\n    given `key` function, assuming the input iterables are already sorted by\n    the same function. (I.e., the merge part of a general merge sort.) Uses\n    :func:`min` (or :func:`max` if ``reverse=True``) for the underlying\n    implementation.\"\"\"\n\n    if reverse:\n        op = max\n    else:\n        op = min\n    if key is not None:\n        opkwargs = {'key': key}\n    else:\n        opkwargs = dict()\n    # populate initial shortlist\n    # (remember some iterables might be empty)\n    iterators = list()\n    shortlist = list()\n    for iterable in iterables:\n        it = iter(iterable)\n        try:\n            first = next(it)\n            iterators.append(it)\n            shortlist.append(first)\n        except StopIteration:\n            pass\n    # do the mergesort\n    while iterators:\n        nxt = op(shortlist, **opkwargs)\n        yield nxt\n        nextidx = shortlist.index(nxt)\n        try:\n            shortlist[nextidx] = next(iterators[nextidx])\n        except StopIteration:\n            del shortlist[nextidx]\n            del iterators[nextidx]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the table is ordered by the given key. E. g. table 1 2 3 4 5 6 5 5", "response": "def issorted(table, key=None, reverse=False, strict=False):\n    \"\"\"\n    Return True if the table is ordered (i.e., sorted) by the given key. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['a', 1, True],\n        ...           ['b', 3, True],\n        ...           ['b', 2]]\n        >>> etl.issorted(table1, key='foo')\n        True\n        >>> etl.issorted(table1, key='bar')\n        False\n        >>> etl.issorted(table1, key='foo', strict=True)\n        False\n        >>> etl.issorted(table1, key='foo', reverse=True)\n        False\n\n    \"\"\"\n\n    # determine the operator to use when comparing rows\n    if reverse and strict:\n        op = operator.lt\n    elif reverse and not strict:\n        op = operator.le\n    elif strict:\n        op = operator.gt\n    else:\n        op = operator.ge\n\n    it = iter(table)\n    flds = [text_type(f) for f in next(it)]\n    if key is None:\n        prev = next(it)\n        for curr in it:\n            if not op(curr, prev):\n                return False\n            prev = curr\n    else:\n        getkey = comparable_itemgetter(*asindices(flds, key))\n        prev = next(it)\n        prevkey = getkey(prev)\n        for curr in it:\n            currkey = getkey(curr)\n            if not op(currkey, prevkey):\n                return False\n            prevkey = currkey\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup(table, key, value=None, dictionary=None):\n\n    if dictionary is None:\n        dictionary = dict()\n\n    # setup\n    it, getkey, getvalue = _setup_lookup(table, key, value)\n\n    # build lookup dictionary\n    for row in it:\n        k = getkey(row)\n        v = getvalue(row)\n        if k in dictionary:\n            # work properly with shelve\n            l = dictionary[k]\n            l.append(v)\n            dictionary[k] = l\n        else:\n            dictionary[k] = [v]\n\n    return dictionary", "response": "Load a dictionary with data from the given table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookupone(table, key, value=None, dictionary=None, strict=False):\n\n    if dictionary is None:\n        dictionary = dict()\n\n    # setup\n    it, getkey, getvalue = _setup_lookup(table, key, value)\n\n    # build lookup dictionary\n    for row in it:\n        k = getkey(row)\n        if strict and k in dictionary:\n            raise DuplicateKeyError(k)\n        elif k not in dictionary:\n            v = getvalue(row)\n            dictionary[k] = v\n\n    return dictionary", "response": "Load a dictionary with data from the given table assuming there is at most one value for each key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a dictionary with data from the given table mapping to dicts", "response": "def dictlookupone(table, key, dictionary=None, strict=False):\n    \"\"\"\n    Load a dictionary with data from the given table, mapping to dicts,\n    assuming there is at most one row for each key. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2],\n        ...           ['b', 3]]\n        >>> # if the specified key is not unique and strict=False (default),\n        ... # the first value wins\n        ... lkp = etl.dictlookupone(table1, 'foo')\n        >>> lkp['a']\n        {'foo': 'a', 'bar': 1}\n        >>> lkp['b']\n        {'foo': 'b', 'bar': 2}\n        >>> # if the specified key is not unique and strict=True, will raise\n        ... # DuplicateKeyError\n        ... try:\n        ...     lkp = etl.dictlookupone(table1, 'foo', strict=True)\n        ... except etl.errors.DuplicateKeyError as e:\n        ...     print(e)\n        ...\n        duplicate key: 'b'\n        >>> # compound keys are supported\n        ... table2 = [['foo', 'bar', 'baz'],\n        ...           ['a', 1, True],\n        ...           ['b', 2, False],\n        ...           ['b', 3, True],\n        ...           ['b', 3, False]]\n        >>> lkp = etl.dictlookupone(table2, ('foo', 'bar'))\n        >>> lkp[('a', 1)]\n        {'foo': 'a', 'bar': 1, 'baz': True}\n        >>> lkp[('b', 2)]\n        {'foo': 'b', 'bar': 2, 'baz': False}\n        >>> lkp[('b', 3)]\n        {'foo': 'b', 'bar': 3, 'baz': True}\n        >>> # data can be loaded into an existing dictionary-like\n        ... # object, including persistent dictionaries created via the\n        ... # shelve module\n        ... import shelve\n        >>> lkp = shelve.open('example.dat', flag='n')\n        >>> lkp = etl.dictlookupone(table1, 'foo', lkp)\n        >>> lkp.close()\n        >>> lkp = shelve.open('example.dat', flag='r')\n        >>> lkp['a']\n        {'foo': 'a', 'bar': 1}\n        >>> lkp['b']\n        {'foo': 'b', 'bar': 2}\n\n    \"\"\"\n\n    if dictionary is None:\n        dictionary = dict()\n\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    keyindices = asindices(hdr, key)\n    assert len(keyindices) > 0, 'no key selected'\n    getkey = operator.itemgetter(*keyindices)\n    for row in it:\n        k = getkey(row)\n        if strict and k in dictionary:\n            raise DuplicateKeyError(k)\n        elif k not in dictionary:\n            d = asdict(flds, row)\n            dictionary[k] = d\n    return dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recordlookup(table, key, dictionary=None):\n\n    if dictionary is None:\n        dictionary = dict()\n\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    keyindices = asindices(hdr, key)\n    assert len(keyindices) > 0, 'no key selected'\n    getkey = operator.itemgetter(*keyindices)\n    for row in it:\n        k = getkey(row)\n        rec = Record(row, flds)\n        if k in dictionary:\n            # work properly with shelve\n            l = dictionary[k]\n            l.append(rec)\n            dictionary[k] = l\n        else:\n            dictionary[k] = [rec]\n    return dictionary", "response": "Load a dictionary with data from the given table mapping to record objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting a table from a bcolz file.", "response": "def frombcolz(source, expression=None, outcols=None, limit=None, skip=0):\n    \"\"\"Extract a table from a bcolz ctable, e.g.::\n\n        >>> import petl as etl\n        >>> import bcolz\n        >>> cols = [\n        ...     ['apples', 'oranges', 'pears'],\n        ...     [1, 3, 7],\n        ...     [2.5, 4.4, .1]\n        ... ]\n        >>> names = ('foo', 'bar', 'baz')\n        >>> ctbl = bcolz.ctable(cols, names=names)\n        >>> tbl = etl.frombcolz(ctbl)\n        >>> tbl\n        +-----------+-----+-----+\n        | foo       | bar | baz |\n        +===========+=====+=====+\n        | 'apples'  |   1 | 2.5 |\n        +-----------+-----+-----+\n        | 'oranges' |   3 | 4.4 |\n        +-----------+-----+-----+\n        | 'pears'   |   7 | 0.1 |\n        +-----------+-----+-----+\n\n    If `expression` is provided it will be executed by bcolz and only\n    matching rows returned, e.g.::\n\n        >>> tbl2 = etl.frombcolz(ctbl, expression='bar > 1')\n        >>> tbl2\n        +-----------+-----+-----+\n        | foo       | bar | baz |\n        +===========+=====+=====+\n        | 'oranges' |   3 | 4.4 |\n        +-----------+-----+-----+\n        | 'pears'   |   7 | 0.1 |\n        +-----------+-----+-----+\n\n    .. versionadded:: 1.1.0\n\n    \"\"\"\n\n    return BcolzView(source, expression=expression, outcols=outcols,\n                     limit=limit, skip=skip)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads data into a bcolz ctable.", "response": "def tobcolz(table, dtype=None, sample=1000, **kwargs):\n    \"\"\"Load data into a bcolz ctable, e.g.::\n\n        >>> import petl as etl\n        >>> table = [('foo', 'bar', 'baz'),\n        ...          ('apples', 1, 2.5),\n        ...          ('oranges', 3, 4.4),\n        ...          ('pears', 7, .1)]\n        >>> ctbl = etl.tobcolz(table)\n        >>> ctbl\n        ctable((3,), [('foo', '<U7'), ('bar', '<i8'), ('baz', '<f8')])\n          nbytes: 132; cbytes: 1023.98 KB; ratio: 0.00\n          cparams := cparams(clevel=5, shuffle=1, cname='lz4', quantize=0)\n        [('apples', 1, 2.5) ('oranges', 3, 4.4) ('pears', 7, 0.1)]\n        >>> ctbl.names\n        ['foo', 'bar', 'baz']\n        >>> ctbl['foo']\n        carray((3,), <U7)\n          nbytes := 84; cbytes := 511.98 KB; ratio: 0.00\n          cparams := cparams(clevel=5, shuffle=1, cname='lz4', quantize=0)\n          chunklen := 18724; chunksize: 524272; blocksize: 0\n        ['apples' 'oranges' 'pears']\n\n    Other keyword arguments are passed through to the ctable constructor.\n\n    .. versionadded:: 1.1.0\n\n    \"\"\"\n\n    import bcolz\n    import numpy as np\n\n    it = iter(table)\n    peek, it = iterpeek(it, sample)\n    hdr = next(it)\n    # numpy is fussy about having tuples, need to make sure\n    it = (tuple(row) for row in it)\n    flds = list(map(text_type, hdr))\n    dtype = construct_dtype(flds, peek, dtype)\n\n    # create ctable\n    kwargs.setdefault('expectedlen', 1000000)\n    kwargs.setdefault('mode', 'w')\n    ctbl = bcolz.ctable(np.array([], dtype=dtype), **kwargs)\n\n    # fill chunk-wise\n    chunklen = sum(ctbl.cols[name].chunklen\n                   for name in ctbl.names) // len(ctbl.names)\n    while True:\n        data = list(itertools.islice(it, chunklen))\n        data = np.array(data, dtype=dtype)\n        ctbl.append(data)\n        if len(data) < chunklen:\n            break\n\n    ctbl.flush()\n    return ctbl"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef appendbcolz(table, obj, check_names=True):\n\n    import bcolz\n    import numpy as np\n\n    if isinstance(obj, string_types):\n        ctbl = bcolz.open(obj, mode='a')\n    else:\n        assert hasattr(obj, 'append') and hasattr(obj, 'names'), \\\n            'expected rootdir or ctable, found %r' % obj\n        ctbl = obj\n\n    # setup\n    dtype = ctbl.dtype\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n\n    # check names match\n    if check_names:\n        assert tuple(flds) == tuple(ctbl.names), 'column names do not match'\n\n    # fill chunk-wise\n    chunklen = sum(ctbl.cols[name].chunklen\n                   for name in ctbl.names) // len(ctbl.names)\n    while True:\n        data = list(itertools.islice(it, chunklen))\n        data = np.array(data, dtype=dtype)\n        ctbl.append(data)\n        if len(data) < chunklen:\n            break\n\n    ctbl.flush()\n    return ctbl", "response": "Append data into a bcolz ctable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef melt(table, key=None, variables=None, variablefield='variable',\n         valuefield='value'):\n    \"\"\"\n    Reshape a table, melting fields into data. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'gender', 'age'],\n        ...           [1, 'F', 12],\n        ...           [2, 'M', 17],\n        ...           [3, 'M', 16]]\n        >>> table2 = etl.melt(table1, 'id')\n        >>> table2.lookall()\n        +----+----------+-------+\n        | id | variable | value |\n        +====+==========+=======+\n        |  1 | 'gender' | 'F'   |\n        +----+----------+-------+\n        |  1 | 'age'    |    12 |\n        +----+----------+-------+\n        |  2 | 'gender' | 'M'   |\n        +----+----------+-------+\n        |  2 | 'age'    |    17 |\n        +----+----------+-------+\n        |  3 | 'gender' | 'M'   |\n        +----+----------+-------+\n        |  3 | 'age'    |    16 |\n        +----+----------+-------+\n\n        >>> # compound keys are supported\n        ... table3 = [['id', 'time', 'height', 'weight'],\n        ...           [1, 11, 66.4, 12.2],\n        ...           [2, 16, 53.2, 17.3],\n        ...           [3, 12, 34.5, 9.4]]\n        >>> table4 = etl.melt(table3, key=['id', 'time'])\n        >>> table4.lookall()\n        +----+------+----------+-------+\n        | id | time | variable | value |\n        +====+======+==========+=======+\n        |  1 |   11 | 'height' |  66.4 |\n        +----+------+----------+-------+\n        |  1 |   11 | 'weight' |  12.2 |\n        +----+------+----------+-------+\n        |  2 |   16 | 'height' |  53.2 |\n        +----+------+----------+-------+\n        |  2 |   16 | 'weight' |  17.3 |\n        +----+------+----------+-------+\n        |  3 |   12 | 'height' |  34.5 |\n        +----+------+----------+-------+\n        |  3 |   12 | 'weight' |   9.4 |\n        +----+------+----------+-------+\n\n        >>> # a subset of variable fields can be selected\n        ... table5 = etl.melt(table3, key=['id', 'time'],\n        ...                   variables=['height'])\n        >>> table5.lookall()\n        +----+------+----------+-------+\n        | id | time | variable | value |\n        +====+======+==========+=======+\n        |  1 |   11 | 'height' |  66.4 |\n        +----+------+----------+-------+\n        |  2 |   16 | 'height' |  53.2 |\n        +----+------+----------+-------+\n        |  3 |   12 | 'height' |  34.5 |\n        +----+------+----------+-------+\n\n    See also :func:`petl.transform.reshape.recast`.\n\n    \"\"\"\n\n    return MeltView(table, key=key, variables=variables,\n                    variablefield=variablefield,\n                    valuefield=valuefield)", "response": "Reshape a table into a single tree of melting fields into data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recast(table, key=None, variablefield='variable', valuefield='value',\n           samplesize=1000, reducers=None, missing=None):\n    \"\"\"\n    Recast molten data. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'variable', 'value'],\n        ...           [3, 'age', 16],\n        ...           [1, 'gender', 'F'],\n        ...           [2, 'gender', 'M'],\n        ...           [2, 'age', 17],\n        ...           [1, 'age', 12],\n        ...           [3, 'gender', 'M']]\n        >>> table2 = etl.recast(table1)\n        >>> table2\n        +----+-----+--------+\n        | id | age | gender |\n        +====+=====+========+\n        |  1 |  12 | 'F'    |\n        +----+-----+--------+\n        |  2 |  17 | 'M'    |\n        +----+-----+--------+\n        |  3 |  16 | 'M'    |\n        +----+-----+--------+\n\n        >>> # specifying variable and value fields\n        ... table3 = [['id', 'vars', 'vals'],\n        ...           [3, 'age', 16],\n        ...           [1, 'gender', 'F'],\n        ...           [2, 'gender', 'M'],\n        ...           [2, 'age', 17],\n        ...           [1, 'age', 12],\n        ...           [3, 'gender', 'M']]\n        >>> table4 = etl.recast(table3, variablefield='vars', valuefield='vals')\n        >>> table4\n        +----+-----+--------+\n        | id | age | gender |\n        +====+=====+========+\n        |  1 |  12 | 'F'    |\n        +----+-----+--------+\n        |  2 |  17 | 'M'    |\n        +----+-----+--------+\n        |  3 |  16 | 'M'    |\n        +----+-----+--------+\n\n        >>> # if there are multiple values for each key/variable pair, and no\n        ... # reducers function is provided, then all values will be listed\n        ... table6 = [['id', 'time', 'variable', 'value'],\n        ...           [1, 11, 'weight', 66.4],\n        ...           [1, 14, 'weight', 55.2],\n        ...           [2, 12, 'weight', 53.2],\n        ...           [2, 16, 'weight', 43.3],\n        ...           [3, 12, 'weight', 34.5],\n        ...           [3, 17, 'weight', 49.4]]\n        >>> table7 = etl.recast(table6, key='id')\n        >>> table7\n        +----+--------------+\n        | id | weight       |\n        +====+==============+\n        |  1 | [66.4, 55.2] |\n        +----+--------------+\n        |  2 | [53.2, 43.3] |\n        +----+--------------+\n        |  3 | [34.5, 49.4] |\n        +----+--------------+\n\n        >>> # multiple values can be reduced via an aggregation function\n        ... def mean(values):\n        ...     return float(sum(values)) / len(values)\n        ...\n        >>> table8 = etl.recast(table6, key='id', reducers={'weight': mean})\n        >>> table8\n        +----+--------------------+\n        | id | weight             |\n        +====+====================+\n        |  1 | 60.800000000000004 |\n        +----+--------------------+\n        |  2 |              48.25 |\n        +----+--------------------+\n        |  3 |              41.95 |\n        +----+--------------------+\n\n        >>> # missing values are padded with whatever is provided via the\n        ... # missing keyword argument (None by default)\n        ... table9 = [['id', 'variable', 'value'],\n        ...           [1, 'gender', 'F'],\n        ...           [2, 'age', 17],\n        ...           [1, 'age', 12],\n        ...           [3, 'gender', 'M']]\n        >>> table10 = etl.recast(table9, key='id')\n        >>> table10\n        +----+------+--------+\n        | id | age  | gender |\n        +====+======+========+\n        |  1 |   12 | 'F'    |\n        +----+------+--------+\n        |  2 |   17 | None   |\n        +----+------+--------+\n        |  3 | None | 'M'    |\n        +----+------+--------+\n\n    Note that the table is scanned once to discover variables, then a second\n    time to reshape the data and recast variables as fields. How many rows are\n    scanned in the first pass is determined by the `samplesize` argument.\n\n    See also :func:`petl.transform.reshape.melt`.\n\n    \"\"\"\n\n    return RecastView(table, key=key, variablefield=variablefield,\n                      valuefield=valuefield, samplesize=samplesize,\n                      reducers=reducers, missing=missing)", "response": "Recast molten data. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'variable', 'value'],\n        ...           [3, 'age', 16],\n        ...           [1, 'gender', 'F'],\n        ...           [2, 'gender', 'M'],\n        ...           [2, 'age', 17],\n        ...           [1, 'age', 12],\n        ...           [3, 'gender', 'M']]\n        >>> table2 = etl.recast(table1)\n        >>> table2\n        +----+-----+--------+\n        | id | age | gender |\n        +====+=====+========+\n        |  1 |  12 | 'F'    |\n        +----+-----+--------+\n        |  2 |  17 | 'M'    |\n        +----+-----+--------+\n        |  3 |  16 | 'M'    |\n        +----+-----+--------+\n\n        >>> # specifying variable and value fields\n        ... table3 = [['id', 'vars', 'vals'],\n        ...           [3, 'age', 16],\n        ...           [1, 'gender', 'F'],\n        ...           [2, 'gender', 'M'],\n        ...           [2, 'age', 17],\n        ...           [1, 'age', 12],\n        ...           [3, 'gender', 'M']]\n        >>> table4 = etl.recast(table3, variablefield='vars', valuefield='vals')\n        >>> table4\n        +----+-----+--------+\n        | id | age | gender |\n        +====+=====+========+\n        |  1 |  12 | 'F'    |\n        +----+-----+--------+\n        |  2 |  17 | 'M'    |\n        +----+-----+--------+\n        |  3 |  16 | 'M'    |\n        +----+-----+--------+\n\n        >>> # if there are multiple values for each key/variable pair, and no\n        ... # reducers function is provided, then all values will be listed\n        ... table6 = [['id', 'time', 'variable', 'value'],\n        ...           [1, 11, 'weight', 66.4],\n        ...           [1, 14, 'weight', 55.2],\n        ...           [2, 12, 'weight', 53.2],\n        ...           [2, 16, 'weight', 43.3],\n        ...           [3, 12, 'weight', 34.5],\n        ...           [3, 17, 'weight', 49.4]]\n        >>> table7 = etl.recast(table6, key='id')\n        >>> table7\n        +----+--------------+\n        | id | weight       |\n        +====+==============+\n        |  1 | [66.4, 55.2] |\n        +----+--------------+\n        |  2 | [53.2, 43.3] |\n        +----+--------------+\n        |  3 | [34.5, 49.4] |\n        +----+--------------+\n\n        >>> # multiple values can be reduced via an aggregation function\n        ... def mean(values):\n        ...     return float(sum(values)) / len(values)\n        ...\n        >>> table8 = etl.recast(table6, key='id', reducers={'weight': mean})\n        >>> table8\n        +----+--------------------+\n        | id | weight             |\n        +====+====================+\n        |  1 | 60.800000000000004 |\n        +----+--------------------+\n        |  2 |              48.25 |\n        +----+--------------------+\n        |  3 |              41.95 |\n        +----+--------------------+\n\n        >>> # missing values are padded with whatever is provided via the\n        ... # missing keyword argument (None by default)\n        ... table9 = [['id', 'variable', 'value'],\n        ...           [1, 'gender', 'F'],\n        ...           [2, 'age', 17],\n        ...           [1, 'age', 12],\n        ...           [3, 'gender', 'M']]\n        >>> table10 = etl.recast(table9, key='id')\n        >>> table10\n        +----+------+--------+\n        | id | age  | gender |\n        +====+======+========+\n        |  1 |   12 | 'F'    |\n        +----+------+--------+\n        |  2 |   17 | None   |\n        +----+------+--------+\n        |  3 | None | 'M'    |\n        +----+------+--------+\n\n    Note that the table is scanned once to discover variables, then a second\n    time to reshape the data and recast variables as fields. How many rows are\n    scanned in the first pass is determined by the `samplesize` argument.\n\n    See also :func:`petl.transform.reshape.melt`."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npivots a table of the same size and return the new ones.", "response": "def pivot(table, f1, f2, f3, aggfun, missing=None,\n          presorted=False, buffersize=None, tempdir=None, cache=True):\n    \"\"\"\n    Construct a pivot table. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['region', 'gender', 'style', 'units'],\n        ...           ['east', 'boy', 'tee', 12],\n        ...           ['east', 'boy', 'golf', 14],\n        ...           ['east', 'boy', 'fancy', 7],\n        ...           ['east', 'girl', 'tee', 3],\n        ...           ['east', 'girl', 'golf', 8],\n        ...           ['east', 'girl', 'fancy', 18],\n        ...           ['west', 'boy', 'tee', 12],\n        ...           ['west', 'boy', 'golf', 15],\n        ...           ['west', 'boy', 'fancy', 8],\n        ...           ['west', 'girl', 'tee', 6],\n        ...           ['west', 'girl', 'golf', 16],\n        ...           ['west', 'girl', 'fancy', 1]]\n        >>> table2 = etl.pivot(table1, 'region', 'gender', 'units', sum)\n        >>> table2\n        +--------+-----+------+\n        | region | boy | girl |\n        +========+=====+======+\n        | 'east' |  33 |   29 |\n        +--------+-----+------+\n        | 'west' |  35 |   23 |\n        +--------+-----+------+\n\n        >>> table3 = etl.pivot(table1, 'region', 'style', 'units', sum)\n        >>> table3\n        +--------+-------+------+-----+\n        | region | fancy | golf | tee |\n        +========+=======+======+=====+\n        | 'east' |    25 |   22 |  15 |\n        +--------+-------+------+-----+\n        | 'west' |     9 |   31 |  18 |\n        +--------+-------+------+-----+\n\n        >>> table4 = etl.pivot(table1, 'gender', 'style', 'units', sum)\n        >>> table4\n        +--------+-------+------+-----+\n        | gender | fancy | golf | tee |\n        +========+=======+======+=====+\n        | 'boy'  |    15 |   29 |  24 |\n        +--------+-------+------+-----+\n        | 'girl' |    19 |   24 |   9 |\n        +--------+-------+------+-----+\n\n    See also :func:`petl.transform.reshape.recast`.\n\n    \"\"\"\n\n    return PivotView(table, f1, f2, f3, aggfun, missing=missing,\n                     presorted=presorted, buffersize=buffersize,\n                     tempdir=tempdir, cache=cache)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fromtext(source=None, encoding=None, errors='strict', strip=None,\n             header=('lines',)):\n    \"\"\"\n    Extract a table from lines in the given text file. E.g.::\n\n        >>> import petl as etl\n        >>> # setup example file\n        ... text = 'a,1\\\\nb,2\\\\nc,2\\\\n'\n        >>> with open('example.txt', 'w') as f:\n        ...     f.write(text)\n        ...\n        12\n        >>> table1 = etl.fromtext('example.txt')\n        >>> table1\n        +-------+\n        | lines |\n        +=======+\n        | 'a,1' |\n        +-------+\n        | 'b,2' |\n        +-------+\n        | 'c,2' |\n        +-------+\n\n        >>> # post-process, e.g., with capture()\n        ... table2 = table1.capture('lines', '(.*),(.*)$', ['foo', 'bar'])\n        >>> table2\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'a' | '1' |\n        +-----+-----+\n        | 'b' | '2' |\n        +-----+-----+\n        | 'c' | '2' |\n        +-----+-----+\n\n    Note that the strip() function is called on each line, which by default\n    will remove leading and trailing whitespace, including the end-of-line\n    character - use the `strip` keyword argument to specify alternative\n    characters to strip. Set the `strip` argument to `False` to disable this\n    behaviour and leave line endings in place.\n\n    \"\"\"\n\n    source = read_source_from_arg(source)\n    return TextView(source, header=header, encoding=encoding,\n                    errors=errors, strip=strip)", "response": "Extract a table from a text file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the table to a text file.", "response": "def totext(table, source=None, encoding=None, errors='strict', template=None,\n           prologue=None, epilogue=None):\n    \"\"\"\n    Write the table to a text file. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2],\n        ...           ['c', 2]]\n        >>> prologue = '''{| class=\"wikitable\"\n        ... |-\n        ... ! foo\n        ... ! bar\n        ... '''\n        >>> template = '''|-\n        ... | {foo}\n        ... | {bar}\n        ... '''\n        >>> epilogue = '|}'\n        >>> etl.totext(table1, 'example.txt', template=template,\n        ... prologue=prologue, epilogue=epilogue)\n        >>> # see what we did\n        ... print(open('example.txt').read())\n        {| class=\"wikitable\"\n        |-\n        ! foo\n        ! bar\n        |-\n        | a\n        | 1\n        |-\n        | b\n        | 2\n        |-\n        | c\n        | 2\n        |}\n\n    The `template` will be used to format each row via\n    `str.format <http://docs.python.org/library/stdtypes.html#str.format>`_.\n\n    \"\"\"\n\n    _writetext(table, source=source, mode='wb', encoding=encoding,\n               errors=errors, template=template, prologue=prologue,\n               epilogue=epilogue)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a table that writes rows to a text file.", "response": "def teetext(table, source=None, encoding=None, errors='strict', template=None,\n            prologue=None, epilogue=None):\n    \"\"\"\n    Return a table that writes rows to a text file as they are iterated over.\n\n    \"\"\"\n\n    assert template is not None, 'template is required'\n    return TeeTextView(table, source=source, encoding=encoding, errors=errors,\n                       template=template, prologue=prologue, epilogue=epilogue)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(table, constraints=None, header=None):\n\n    return ProblemsView(table, constraints=constraints, header=header)", "response": "Validate a table against a set of constraints and or an expected header."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_constraints(constraints, flds):\n    local_constraints = constraints or []\n    local_constraints = [dict(**c) for c in local_constraints]\n    local_constraints = [\n        c for c in local_constraints\n        if c.get('field') in flds or\n        not c.get('optional')\n    ]\n    return local_constraints", "response": "This method returns a list of constraints that are not None and that return value is a list of dicts where the key is the key of the key and the value is the value of the key of the key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef join(left, right, key=None, lkey=None, rkey=None, presorted=False,\n         buffersize=None, tempdir=None, cache=True, lprefix=None, rprefix=None):\n    \"\"\"\n    Perform an equi-join on the given tables. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'colour'],\n        ...           [1, 'blue'],\n        ...           [2, 'red'],\n        ...           [3, 'purple']]\n        >>> table2 = [['id', 'shape'],\n        ...           [1, 'circle'],\n        ...           [3, 'square'],\n        ...           [4, 'ellipse']]\n        >>> table3 = etl.join(table1, table2, key='id')\n        >>> table3\n        +----+----------+----------+\n        | id | colour   | shape    |\n        +====+==========+==========+\n        |  1 | 'blue'   | 'circle' |\n        +----+----------+----------+\n        |  3 | 'purple' | 'square' |\n        +----+----------+----------+\n\n        >>> # if no key is given, a natural join is tried\n        ... table4 = etl.join(table1, table2)\n        >>> table4\n        +----+----------+----------+\n        | id | colour   | shape    |\n        +====+==========+==========+\n        |  1 | 'blue'   | 'circle' |\n        +----+----------+----------+\n        |  3 | 'purple' | 'square' |\n        +----+----------+----------+\n\n        >>> # note behaviour if the key is not unique in either or both tables\n        ... table5 = [['id', 'colour'],\n        ...           [1, 'blue'],\n        ...           [1, 'red'],\n        ...           [2, 'purple']]\n        >>> table6 = [['id', 'shape'],\n        ...           [1, 'circle'],\n        ...           [1, 'square'],\n        ...           [2, 'ellipse']]\n        >>> table7 = etl.join(table5, table6, key='id')\n        >>> table7\n        +----+----------+-----------+\n        | id | colour   | shape     |\n        +====+==========+===========+\n        |  1 | 'blue'   | 'circle'  |\n        +----+----------+-----------+\n        |  1 | 'blue'   | 'square'  |\n        +----+----------+-----------+\n        |  1 | 'red'    | 'circle'  |\n        +----+----------+-----------+\n        |  1 | 'red'    | 'square'  |\n        +----+----------+-----------+\n        |  2 | 'purple' | 'ellipse' |\n        +----+----------+-----------+\n\n        >>> # compound keys are supported\n        ... table8 = [['id', 'time', 'height'],\n        ...           [1, 1, 12.3],\n        ...           [1, 2, 34.5],\n        ...           [2, 1, 56.7]]\n        >>> table9 = [['id', 'time', 'weight'],\n        ...           [1, 2, 4.5],\n        ...           [2, 1, 6.7],\n        ...           [2, 2, 8.9]]\n        >>> table10 = etl.join(table8, table9, key=['id', 'time'])\n        >>> table10\n        +----+------+--------+--------+\n        | id | time | height | weight |\n        +====+======+========+========+\n        |  1 |    2 |   34.5 |    4.5 |\n        +----+------+--------+--------+\n        |  2 |    1 |   56.7 |    6.7 |\n        +----+------+--------+--------+\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    Left and right tables with different key fields can be handled via the\n    `lkey` and `rkey` arguments.\n\n    \"\"\"\n\n    # TODO don't read data twice (occurs if using natural key)\n    lkey, rkey = keys_from_args(left, right, key, lkey, rkey)\n    return JoinView(left, right, lkey=lkey, rkey=rkey,\n                    presorted=presorted, buffersize=buffersize, tempdir=tempdir,\n                    cache=cache, lprefix=lprefix, rprefix=rprefix)", "response": "Perform an equi - join on the given tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rightjoin(left, right, key=None, lkey=None, rkey=None, missing=None,\n              presorted=False, buffersize=None, tempdir=None, cache=True,\n              lprefix=None, rprefix=None):\n    \"\"\"\n    Perform a right outer join on the given tables. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'colour'],\n        ...           [1, 'blue'],\n        ...           [2, 'red'],\n        ...           [3, 'purple']]\n        >>> table2 = [['id', 'shape'],\n        ...           [1, 'circle'],\n        ...           [3, 'square'],\n        ...           [4, 'ellipse']]\n        >>> table3 = etl.rightjoin(table1, table2, key='id')\n        >>> table3\n        +----+----------+-----------+\n        | id | colour   | shape     |\n        +====+==========+===========+\n        |  1 | 'blue'   | 'circle'  |\n        +----+----------+-----------+\n        |  3 | 'purple' | 'square'  |\n        +----+----------+-----------+\n        |  4 | None     | 'ellipse' |\n        +----+----------+-----------+\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    Left and right tables with different key fields can be handled via the\n    `lkey` and `rkey` arguments.\n\n    \"\"\"\n\n    # TODO don't read data twice (occurs if using natural key)\n    lkey, rkey = keys_from_args(left, right, key, lkey, rkey)\n    return JoinView(left, right, lkey=lkey, rkey=rkey,\n                    presorted=presorted, leftouter=False, rightouter=True,\n                    missing=missing, buffersize=buffersize,\n                    tempdir=tempdir, cache=cache, lprefix=lprefix,\n                    rprefix=rprefix)", "response": "Perform a right outer join on the given tables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef antijoin(left, right, key=None, lkey=None, rkey=None, presorted=False,\n             buffersize=None, tempdir=None, cache=True):\n    \"\"\"\n    Return rows from the `left` table where the key value does not occur in\n    the `right` table. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'colour'],\n        ...           [0, 'black'],\n        ...           [1, 'blue'],\n        ...           [2, 'red'],\n        ...           [4, 'yellow'],\n        ...           [5, 'white']]\n        >>> table2 = [['id', 'shape'],\n        ...           [1, 'circle'],\n        ...           [3, 'square']]\n        >>> table3 = etl.antijoin(table1, table2, key='id')\n        >>> table3\n        +----+----------+\n        | id | colour   |\n        +====+==========+\n        |  0 | 'black'  |\n        +----+----------+\n        |  2 | 'red'    |\n        +----+----------+\n        |  4 | 'yellow' |\n        +----+----------+\n        |  5 | 'white'  |\n        +----+----------+\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    Left and right tables with different key fields can be handled via the\n    `lkey` and `rkey` arguments.\n\n    \"\"\"\n\n    lkey, rkey = keys_from_args(left, right, key, lkey, rkey)\n    return AntiJoinView(left=left, right=right, lkey=lkey, rkey=rkey,\n                        presorted=presorted, buffersize=buffersize,\n                        tempdir=tempdir, cache=cache)", "response": "AntiJoinView Returns rows from the left table where the key value does not occur in the right table."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms a left join on two tables and return a LookupJoinView object.", "response": "def lookupjoin(left, right, key=None, lkey=None, rkey=None, missing=None,\n               presorted=False, buffersize=None, tempdir=None, cache=True,\n               lprefix=None, rprefix=None):\n    \"\"\"\n    Perform a left join, but where the key is not unique in the right-hand\n    table, arbitrarily choose the first row and ignore others. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'color', 'cost'],\n        ...           [1, 'blue', 12],\n        ...           [2, 'red', 8],\n        ...           [3, 'purple', 4]]\n        >>> table2 = [['id', 'shape', 'size'],\n        ...           [1, 'circle', 'big'],\n        ...           [1, 'circle', 'small'],\n        ...           [2, 'square', 'tiny'],\n        ...           [2, 'square', 'big'],\n        ...           [3, 'ellipse', 'small'],\n        ...           [3, 'ellipse', 'tiny']]\n        >>> table3 = etl.lookupjoin(table1, table2, key='id')\n        >>> table3\n        +----+----------+------+-----------+---------+\n        | id | color    | cost | shape     | size    |\n        +====+==========+======+===========+=========+\n        |  1 | 'blue'   |   12 | 'circle'  | 'big'   |\n        +----+----------+------+-----------+---------+\n        |  2 | 'red'    |    8 | 'square'  | 'tiny'  |\n        +----+----------+------+-----------+---------+\n        |  3 | 'purple' |    4 | 'ellipse' | 'small' |\n        +----+----------+------+-----------+---------+\n\n    See also :func:`petl.transform.joins.leftjoin`.\n\n    \"\"\"\n\n    lkey, rkey = keys_from_args(left, right, key, lkey, rkey)\n    return LookupJoinView(left, right, lkey, rkey, presorted=presorted,\n                          missing=missing, buffersize=buffersize,\n                          tempdir=tempdir, cache=cache,\n                          lprefix=lprefix, rprefix=rprefix)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unjoin(table, value, key=None, autoincrement=(1, 1), presorted=False,\n           buffersize=None, tempdir=None, cache=True):\n    \"\"\"\n    Split a table into two tables by reversing an inner join. E.g.::\n\n        >>> import petl as etl\n        >>> # join key is present in the table\n        ... table1 = (('foo', 'bar', 'baz'),\n        ...           ('A', 1, 'apple'),\n        ...           ('B', 1, 'apple'),\n        ...           ('C', 2, 'orange'))\n        >>> table2, table3 = etl.unjoin(table1, 'baz', key='bar')\n        >>> table2\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'A' |   1 |\n        +-----+-----+\n        | 'B' |   1 |\n        +-----+-----+\n        | 'C' |   2 |\n        +-----+-----+\n\n        >>> table3\n        +-----+----------+\n        | bar | baz      |\n        +=====+==========+\n        |   1 | 'apple'  |\n        +-----+----------+\n        |   2 | 'orange' |\n        +-----+----------+\n\n        >>> # an integer join key can also be reconstructed\n        ... table4 = (('foo', 'bar'),\n        ...           ('A', 'apple'),\n        ...           ('B', 'apple'),\n        ...           ('C', 'orange'))\n        >>> table5, table6 = etl.unjoin(table4, 'bar')\n        >>> table5\n        +-----+--------+\n        | foo | bar_id |\n        +=====+========+\n        | 'A' |      1 |\n        +-----+--------+\n        | 'B' |      1 |\n        +-----+--------+\n        | 'C' |      2 |\n        +-----+--------+\n\n        >>> table6\n        +----+----------+\n        | id | bar      |\n        +====+==========+\n        |  1 | 'apple'  |\n        +----+----------+\n        |  2 | 'orange' |\n        +----+----------+\n\n    The `autoincrement` parameter controls how an integer join key is\n    reconstructed, and should be a tuple of (`start`, `step`).\n\n    \"\"\"\n\n    if key is None:\n        # first sort the table by the value field\n        if presorted:\n            tbl_sorted = table\n        else:\n            tbl_sorted = sort(table, value, buffersize=buffersize,\n                              tempdir=tempdir, cache=cache)\n        # on the left, return the original table but with the value field\n        # replaced by an incrementing integer\n        left = ConvertToIncrementingCounterView(tbl_sorted, value,\n                                                autoincrement)\n        # on the right, return a new table with distinct values from the\n        # given field\n        right = EnumerateDistinctView(tbl_sorted, value, autoincrement)\n    else:\n        # on the left, return distinct rows from the original table\n        # with the value field cut out\n        left = distinct(cutout(table, value))\n        # on the right, return distinct rows from the original table\n        # with all fields but the key and value cut out\n        right = distinct(cut(table, key, value))\n    return left, right", "response": "Unjoin a table into two tables where the value is given."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rowreduce(table, key, reducer, header=None, presorted=False,\n              buffersize=None, tempdir=None, cache=True):\n    \"\"\"\n    Group rows under the given key then apply `reducer` to produce a single\n    output row for each input group of rows. E.g.::\n    \n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 3],\n        ...           ['a', 7],\n        ...           ['b', 2],\n        ...           ['b', 1],\n        ...           ['b', 9],\n        ...           ['c', 4]]\n        >>> def sumbar(key, rows):\n        ...     return [key, sum(row[1] for row in rows)]\n        ...\n        >>> table2 = etl.rowreduce(table1, key='foo', reducer=sumbar,\n        ...                        header=['foo', 'barsum'])\n        >>> table2\n        +-----+--------+\n        | foo | barsum |\n        +=====+========+\n        | 'a' |     10 |\n        +-----+--------+\n        | 'b' |     12 |\n        +-----+--------+\n        | 'c' |      4 |\n        +-----+--------+\n    \n    N.B., this is not strictly a \"reduce\" in the sense of the standard Python\n    :func:`reduce` function, i.e., the `reducer` function is *not* applied \n    recursively to values within a group, rather it is applied once to each row \n    group as a whole.\n    \n    See also :func:`petl.transform.reductions.aggregate` and\n    :func:`petl.transform.reductions.fold`.\n    \n    \"\"\"\n\n    return RowReduceView(table, key, reducer, header=header,\n                         presorted=presorted, \n                         buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Reduce a table by applying a reducer function to each row of the input table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef aggregate(table, key, aggregation=None, value=None, presorted=False,\n              buffersize=None, tempdir=None, cache=True):\n    \"\"\"Group rows under the given key then apply aggregation functions.\n    E.g.::\n\n        >>> import petl as etl\n        >>>\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['a', 3, True],\n        ...           ['a', 7, False],\n        ...           ['b', 2, True],\n        ...           ['b', 2, False],\n        ...           ['b', 9, False],\n        ...           ['c', 4, True]]\n        >>> # aggregate whole rows\n        ... table2 = etl.aggregate(table1, 'foo', len)\n        >>> table2\n        +-----+-------+\n        | foo | value |\n        +=====+=======+\n        | 'a' |     2 |\n        +-----+-------+\n        | 'b' |     3 |\n        +-----+-------+\n        | 'c' |     1 |\n        +-----+-------+\n\n        >>> # aggregate single field\n        ... table3 = etl.aggregate(table1, 'foo', sum, 'bar')\n        >>> table3\n        +-----+-------+\n        | foo | value |\n        +=====+=======+\n        | 'a' |    10 |\n        +-----+-------+\n        | 'b' |    13 |\n        +-----+-------+\n        | 'c' |     4 |\n        +-----+-------+\n\n        >>> # alternative signature using keyword args\n        ... table4 = etl.aggregate(table1, key=('foo', 'bar'),\n        ...                        aggregation=list, value=('bar', 'baz'))\n        >>> table4\n        +-----+-----+-------------------------+\n        | foo | bar | value                   |\n        +=====+=====+=========================+\n        | 'a' |   3 | [(3, True)]             |\n        +-----+-----+-------------------------+\n        | 'a' |   7 | [(7, False)]            |\n        +-----+-----+-------------------------+\n        | 'b' |   2 | [(2, True), (2, False)] |\n        +-----+-----+-------------------------+\n        | 'b' |   9 | [(9, False)]            |\n        +-----+-----+-------------------------+\n        | 'c' |   4 | [(4, True)]             |\n        +-----+-----+-------------------------+\n\n        >>> # aggregate multiple fields\n        ... from collections import OrderedDict\n        >>> import petl as etl\n        >>>\n        >>> aggregation = OrderedDict()\n        >>> aggregation['count'] = len\n        >>> aggregation['minbar'] = 'bar', min\n        >>> aggregation['maxbar'] = 'bar', max\n        >>> aggregation['sumbar'] = 'bar', sum\n        >>> # default aggregation function is list\n        ... aggregation['listbar'] = 'bar'\n        >>> aggregation['listbarbaz'] = ('bar', 'baz'), list\n        >>> aggregation['bars'] = 'bar', etl.strjoin(', ')\n        >>> table5 = etl.aggregate(table1, 'foo', aggregation)\n        >>> table5\n        +-----+-------+--------+--------+--------+-----------+-------------------------------------+-----------+\n        | foo | count | minbar | maxbar | sumbar | listbar   | listbarbaz                          | bars      |\n        +=====+=======+========+========+========+===========+=====================================+===========+\n        | 'a' |     2 |      3 |      7 |     10 | [3, 7]    | [(3, True), (7, False)]             | '3, 7'    |\n        +-----+-------+--------+--------+--------+-----------+-------------------------------------+-----------+\n        | 'b' |     3 |      2 |      9 |     13 | [2, 2, 9] | [(2, True), (2, False), (9, False)] | '2, 2, 9' |\n        +-----+-------+--------+--------+--------+-----------+-------------------------------------+-----------+\n        | 'c' |     1 |      4 |      4 |      4 | [4]       | [(4, True)]                         | '4'       |\n        +-----+-------+--------+--------+--------+-----------+-------------------------------------+-----------+\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are \n    ignored. Otherwise, the data are sorted, see also the discussion of the \n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    \"\"\"\n\n    if callable(aggregation):\n        return SimpleAggregateView(table, key, aggregation=aggregation, \n                                   value=value, presorted=presorted, \n                                   buffersize=buffersize, tempdir=tempdir, \n                                   cache=cache)\n    elif aggregation is None or isinstance(aggregation, (list, tuple, dict)):\n        # ignore value arg\n        return MultiAggregateView(table, key, aggregation=aggregation,  \n                                  presorted=presorted, buffersize=buffersize, \n                                  tempdir=tempdir, cache=cache)\n    else:\n        raise ArgumentError('expected aggregation is callable, list, tuple, dict '\n                        'or None')", "response": "Aggregate the rows under the given key and apply aggregation functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngroups by the key field then count the number of distinct values in the value field.", "response": "def groupcountdistinctvalues(table, key, value):\n    \"\"\"Group by the `key` field then count the number of distinct values in the\n    `value` field.\"\"\"\n    \n    s1 = cut(table, key, value)\n    s2 = distinct(s1)\n    s3 = aggregate(s2, key, len)\n    return s3"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef groupselectfirst(table, key, presorted=False, buffersize=None,\n                     tempdir=None, cache=True):\n    \"\"\"Group by the `key` field then return the first row within each group.\n    E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', 1, True],\n        ...           ['C', 7, False],\n        ...           ['B', 2, False],\n        ...           ['C', 9, True]]\n        >>> table2 = etl.groupselectfirst(table1, key='foo')\n        >>> table2\n        +-----+-----+-------+\n        | foo | bar | baz   |\n        +=====+=====+=======+\n        | 'A' |   1 | True  |\n        +-----+-----+-------+\n        | 'B' |   2 | False |\n        +-----+-----+-------+\n        | 'C' |   7 | False |\n        +-----+-----+-------+\n\n    See also :func:`petl.transform.reductions.groupselectlast`,\n    :func:`petl.transform.dedup.distinct`.\n\n    \"\"\"\n\n    def _reducer(k, rows):\n        return next(rows)\n\n    return rowreduce(table, key, reducer=_reducer, presorted=presorted,\n                     buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Group by the key then return the first row within each group."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngroups by the key then return the last row within each group.", "response": "def groupselectlast(table, key, presorted=False, buffersize=None,\n                    tempdir=None, cache=True):\n    \"\"\"Group by the `key` field then return the last row within each group.\n    E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', 1, True],\n        ...           ['C', 7, False],\n        ...           ['B', 2, False],\n        ...           ['C', 9, True]]\n        >>> table2 = etl.groupselectlast(table1, key='foo')\n        >>> table2\n        +-----+-----+-------+\n        | foo | bar | baz   |\n        +=====+=====+=======+\n        | 'A' |   1 | True  |\n        +-----+-----+-------+\n        | 'B' |   2 | False |\n        +-----+-----+-------+\n        | 'C' |   9 | True  |\n        +-----+-----+-------+\n\n    See also :func:`petl.transform.reductions.groupselectfirst`,\n    :func:`petl.transform.dedup.distinct`.\n\n    .. versionadded:: 1.1.0\n\n    \"\"\"\n\n    def _reducer(k, rows):\n        row = None\n        for row in rows:\n            pass\n        return row\n\n    return rowreduce(table, key, reducer=_reducer, presorted=presorted,\n                     buffersize=buffersize, tempdir=tempdir, cache=cache)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef groupselectmin(table, key, value, presorted=False, buffersize=None,\n                   tempdir=None, cache=True):\n    \"\"\"Group by the `key` field then return the row with the minimum of the\n    `value` field within each group. N.B., will only return one row for each\n    group, even if multiple rows have the same (minimum) value.\"\"\"\n\n    return groupselectfirst(sort(table, value, reverse=False), key,\n                            presorted=presorted, buffersize=buffersize,\n                            tempdir=tempdir, cache=cache)", "response": "Group by the key then return the row with the minimum of the\n    value field within each group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging duplicate rows under the given key.", "response": "def mergeduplicates(table, key, missing=None, presorted=False, buffersize=None,\n                    tempdir=None, cache=True):\n    \"\"\"\n    Merge duplicate rows under the given key. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', 1, 2.7],\n        ...           ['B', 2, None],\n        ...           ['D', 3, 9.4],\n        ...           ['B', None, 7.8],\n        ...           ['E', None, 42.],\n        ...           ['D', 3, 12.3],\n        ...           ['A', 2, None]]\n        >>> table2 = etl.mergeduplicates(table1, 'foo')\n        >>> table2\n        +-----+------------------+-----------------------+\n        | foo | bar              | baz                   |\n        +=====+==================+=======================+\n        | 'A' | Conflict({1, 2}) |                   2.7 |\n        +-----+------------------+-----------------------+\n        | 'B' |                2 |                   7.8 |\n        +-----+------------------+-----------------------+\n        | 'D' |                3 | Conflict({9.4, 12.3}) |\n        +-----+------------------+-----------------------+\n        | 'E' | None             |                  42.0 |\n        +-----+------------------+-----------------------+\n\n    Missing values are overridden by non-missing values. Conflicting values are\n    reported as an instance of the Conflict class (sub-class of frozenset).\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    See also :func:`petl.transform.dedup.conflicts`.\n\n    \"\"\"\n\n    return MergeDuplicatesView(table, key, missing=missing, presorted=presorted,\n                               buffersize=buffersize, tempdir=tempdir,\n                               cache=cache)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncombine multiple tables into a single record.", "response": "def merge(*tables, **kwargs):\n    \"\"\"\n    Convenience function to combine multiple tables (via\n    :func:`petl.transform.sorts.mergesort`) then combine duplicate rows by\n    merging under the given key (via\n    :func:`petl.transform.reductions.mergeduplicates`). E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           [1, 'A', True],\n        ...           [2, 'B', None],\n        ...           [4, 'C', True]]\n        >>> table2 = [['bar', 'baz', 'quux'],\n        ...           ['A', True, 42.0],\n        ...           ['B', False, 79.3],\n        ...           ['C', False, 12.4]]\n        >>> table3 = etl.merge(table1, table2, key='bar')\n        >>> table3\n        +-----+-----+-------------------------+------+\n        | bar | foo | baz                     | quux |\n        +=====+=====+=========================+======+\n        | 'A' |   1 | True                    | 42.0 |\n        +-----+-----+-------------------------+------+\n        | 'B' |   2 | False                   | 79.3 |\n        +-----+-----+-------------------------+------+\n        | 'C' |   4 | Conflict({False, True}) | 12.4 |\n        +-----+-----+-------------------------+------+\n\n    Keyword arguments are the same as for\n    :func:`petl.transform.sorts.mergesort`, except `key` is required.\n\n    \"\"\"\n\n    assert 'key' in kwargs, 'keyword argument \"key\" is required'\n    key = kwargs['key']\n    t1 = mergesort(*tables, **kwargs)\n    t2 = mergeduplicates(t1, key=key, presorted=True)\n    return t2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fold(table, key, f, value=None, presorted=False, buffersize=None,\n         tempdir=None, cache=True):\n    \"\"\"\n    Reduce rows recursively via the Python standard :func:`reduce` function.\n    E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'count'],\n        ...           [1, 3],\n        ...           [1, 5],\n        ...           [2, 4],\n        ...           [2, 8]]\n        >>> import operator\n        >>> table2 = etl.fold(table1, 'id', operator.add, 'count',\n        ...                   presorted=True)\n        >>> table2\n        +-----+-------+\n        | key | value |\n        +=====+=======+\n        |   1 |     8 |\n        +-----+-------+\n        |   2 |    12 |\n        +-----+-------+\n\n    See also :func:`petl.transform.reductions.aggregate`,\n    :func:`petl.transform.reductions.rowreduce`.\n\n    \"\"\"\n\n    return FoldView(table, key, f, value=value, presorted=presorted,\n                    buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Folds the given table with the given key and value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fromtextindex(index_or_dirname, indexname=None, docnum_field=None):\n\n    return TextIndexView(index_or_dirname, indexname=indexname,\n                         docnum_field=docnum_field)", "response": "Create a TextIndexView from a Whoosh index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading all rows from table into an index.", "response": "def totextindex(table, index_or_dirname, schema=None, indexname=None,\n                merge=False, optimize=False):\n    \"\"\"\n    Load all rows from `table` into a Whoosh index. N.B., this will clear any\n    existing data in the index before loading. E.g.::\n\n        >>> import petl as etl\n        >>> import datetime\n        >>> import os\n        >>> # here is the table we want to load into an index\n        ... table = (('f0', 'f1', 'f2', 'f3', 'f4'),\n        ...          ('AAA', 12, 4.3, True, datetime.datetime.now()),\n        ...          ('BBB', 6, 3.4, False, datetime.datetime(1900, 1, 31)),\n        ...          ('CCC', 42, 7.8, True, datetime.datetime(2100, 12, 25)))\n        >>> # define a schema for the index\n        ... from whoosh.fields import *\n        >>> schema = Schema(f0=TEXT(stored=True),\n        ...                 f1=NUMERIC(int, stored=True),\n        ...                 f2=NUMERIC(float, stored=True),\n        ...                 f3=BOOLEAN(stored=True),\n        ...                 f4=DATETIME(stored=True))\n        >>> # load index\n        ... dirname = 'example.whoosh'\n        >>> if not os.path.exists(dirname):\n        ...     os.mkdir(dirname)\n        ...\n        >>> etl.totextindex(table, dirname, schema=schema)\n\n    Keyword arguments:\n\n    table\n        A table container with the data to be loaded.\n    index_or_dirname\n        Either an instance of `whoosh.index.Index` or a string containing the\n        directory path where the index is to be stored.\n    schema\n        Index schema to use if creating the index.\n    indexname\n        String containing the name of the index, if multiple indexes are stored\n        in the same directory.\n    merge\n        Merge small segments during commit?\n    optimize\n        Merge all segments together?\n\n    \"\"\"\n    import whoosh.index\n    import whoosh.writing\n\n    # deal with polymorphic argument\n    if isinstance(index_or_dirname, string_types):\n        dirname = index_or_dirname\n        index = whoosh.index.create_in(dirname, schema,\n                                       indexname=indexname)\n        needs_closing = True\n    elif isinstance(index_or_dirname, whoosh.index.Index):\n        index = index_or_dirname\n        needs_closing = False\n    else:\n        raise ArgumentError('expected string or index, found %r'\n                            % index_or_dirname)\n\n    writer = index.writer()\n    try:\n\n        for d in dicts(table):\n            writer.add_document(**d)\n        writer.commit(merge=merge, optimize=optimize,\n                      mergetype=whoosh.writing.CLEAR)\n\n    except:\n        writer.cancel()\n        raise\n\n    finally:\n        if needs_closing:\n            index.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef appendtextindex(table, index_or_dirname, indexname=None, merge=True,\n                    optimize=False):\n    \"\"\"\n    Load all rows from `table` into a Whoosh index, adding them to any existing\n    data in the index.\n\n    Keyword arguments:\n\n    table\n        A table container with the data to be loaded.\n    index_or_dirname\n        Either an instance of `whoosh.index.Index` or a string containing the\n        directory path where the index is to be stored.\n    indexname\n        String containing the name of the index, if multiple indexes are stored\n        in the same directory.\n    merge\n        Merge small segments during commit?\n    optimize\n        Merge all segments together?\n\n    \"\"\"\n    import whoosh.index\n\n    # deal with polymorphic argument\n    if isinstance(index_or_dirname, string_types):\n        dirname = index_or_dirname\n        index = whoosh.index.open_dir(dirname, indexname=indexname,\n                                      readonly=False)\n        needs_closing = True\n    elif isinstance(index_or_dirname, whoosh.index.Index):\n        index = index_or_dirname\n        needs_closing = False\n    else:\n        raise ArgumentError('expected string or index, found %r'\n                            % index_or_dirname)\n\n    writer = index.writer()\n    try:\n\n        for d in dicts(table):\n            writer.add_document(**d)\n        writer.commit(merge=merge, optimize=optimize)\n\n    except Exception:\n        writer.cancel()\n        raise\n\n    finally:\n        if needs_closing:\n            index.close()", "response": "Load all rows from a table into a Whoosh index and add them to any existing Whoosh index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef searchtextindex(index_or_dirname, query, limit=10, indexname=None,\n                    docnum_field=None, score_field=None, fieldboosts=None,\n                    search_kwargs=None):\n    \"\"\"\n    Search a Whoosh index using a query. E.g.::\n\n        >>> import petl as etl\n        >>> import os\n        >>> # set up an index and load some documents via the Whoosh API\n        ... from whoosh.index import create_in\n        >>> from whoosh.fields import *\n        >>> schema = Schema(title=TEXT(stored=True), path=ID(stored=True),\n        ...                            content=TEXT)\n        >>> dirname = 'example.whoosh'\n        >>> if not os.path.exists(dirname):\n        ...     os.mkdir(dirname)\n        ...\n        >>> index = create_in('example.whoosh', schema)\n        >>> writer = index.writer()\n        >>> writer.add_document(title=u\"Oranges\", path=u\"/a\",\n        ...                     content=u\"This is the first document we've added!\")\n        >>> writer.add_document(title=u\"Apples\", path=u\"/b\",\n        ...                     content=u\"The second document is even more \"\n        ...                             u\"interesting!\")\n        >>> writer.commit()\n        >>> # demonstrate the use of searchtextindex()\n        ... table1 = etl.searchtextindex('example.whoosh', 'oranges')\n        >>> table1\n        +------+-----------+\n        | path | title     |\n        +======+===========+\n        | '/a' | 'Oranges' |\n        +------+-----------+\n\n        >>> table2 = etl.searchtextindex('example.whoosh', 'doc*')\n        >>> table2\n        +------+-----------+\n        | path | title     |\n        +======+===========+\n        | '/a' | 'Oranges' |\n        +------+-----------+\n        | '/b' | 'Apples'  |\n        +------+-----------+\n\n    Keyword arguments:\n\n    index_or_dirname\n        Either an instance of `whoosh.index.Index` or a string containing the\n        directory path where the index is to be stored.\n    query\n        Either a string or an instance of `whoosh.query.Query`. If a string,\n        it will be parsed as a multi-field query, i.e., any terms not bound\n        to a specific field will match **any** field.\n    limit\n        Return at most `limit` results.\n    indexname\n        String containing the name of the index, if multiple indexes are stored\n        in the same directory.\n    docnum_field\n        If not None, an extra field will be added to the output table containing\n        the internal document number stored in the index. The name of the field\n        will be the value of this argument.\n    score_field\n        If not None, an extra field will be added to the output table containing\n        the score of the result. The name of the field will be the value of this\n        argument.\n    fieldboosts\n        An optional dictionary mapping field names to boosts.\n    search_kwargs\n        Any extra keyword arguments to be passed through to the Whoosh\n        `search()` method.\n\n    \"\"\"\n\n    return SearchTextIndexView(index_or_dirname, query, limit=limit,\n                               indexname=indexname, docnum_field=docnum_field,\n                               score_field=score_field, fieldboosts=fieldboosts,\n                               search_kwargs=search_kwargs)", "response": "Search a Whoosh index using a query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching an index using a query returning a result page.", "response": "def searchtextindexpage(index_or_dirname, query, pagenum, pagelen=10,\n                        indexname=None, docnum_field=None, score_field=None,\n                        fieldboosts=None, search_kwargs=None):\n    \"\"\"\n    Search an index using a query, returning a result page.\n\n    Keyword arguments:\n\n    index_or_dirname\n        Either an instance of `whoosh.index.Index` or a string containing the\n        directory path where the index is to be stored.\n    query\n        Either a string or an instance of `whoosh.query.Query`. If a string,\n        it will be parsed as a multi-field query, i.e., any terms not bound\n        to a specific field will match **any** field.\n    pagenum\n        Number of the page to return (e.g., 1 = first page).\n    pagelen\n        Number of results per page.\n    indexname\n        String containing the name of the index, if multiple indexes are stored\n        in the same directory.\n    docnum_field\n        If not None, an extra field will be added to the output table containing\n        the internal document number stored in the index. The name of the field\n        will be the value of this argument.\n    score_field\n        If not None, an extra field will be added to the output table containing\n        the score of the result. The name of the field will be the value of this\n        argument.\n    fieldboosts\n        An optional dictionary mapping field names to boosts.\n    search_kwargs\n        Any extra keyword arguments to be passed through to the Whoosh\n        `search()` method.\n\n    \"\"\"\n\n    return SearchTextIndexView(index_or_dirname, query, pagenum=pagenum,\n                               pagelen=pagelen, indexname=indexname,\n                               docnum_field=docnum_field,\n                               score_field=score_field, fieldboosts=fieldboosts,\n                               search_kwargs=search_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hashrightjoin(left, right, key=None, lkey=None, rkey=None, missing=None,\n                  cache=True, lprefix=None, rprefix=None):\n    \"\"\"Alternative implementation of :func:`petl.transform.joins.rightjoin`,\n    where the join is executed by constructing an in-memory lookup for the\n    left hand table, then iterating over rows from the right hand table.\n    \n    May be faster and/or more resource efficient where the left table is small\n    and the right table is large.\n    \n    By default data from right hand table is cached to improve performance\n    (only available when `key` is given).\n\n    Left and right tables with different key fields can be handled via the\n    `lkey` and `rkey` arguments.\n\n    \"\"\"\n\n    lkey, rkey = keys_from_args(left, right, key, lkey, rkey)\n    return HashRightJoinView(left, right, lkey, rkey, missing=missing,\n                             cache=cache, lprefix=lprefix, rprefix=rprefix)", "response": "Alternative implementation of petl. transform. joins. rightjoin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hashantijoin(left, right, key=None, lkey=None, rkey=None):\n    \n    lkey, rkey = keys_from_args(left, right, key, lkey, rkey)\n    return HashAntiJoinView(left, right, lkey, rkey)", "response": "Alternative implementation of petl. transform. joins. antijoin."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fromxls(filename, sheet=None, use_view=True):\n    \n    return XLSView(filename, sheet=sheet, use_view=use_view)", "response": "Extract a table from an Excel. xls file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a table to a new Excel. xls file.", "response": "def toxls(tbl, filename, sheet, encoding=None, style_compression=0,\n          styles=None):\n    \"\"\"\n    Write a table to a new Excel .xls file.\n\n    \"\"\"\n\n    import xlwt\n    if encoding is None:\n        encoding = locale.getpreferredencoding()\n    wb = xlwt.Workbook(encoding=encoding, style_compression=style_compression)\n    ws = wb.add_sheet(sheet)\n\n    if styles is None:\n        # simple version, don't worry about styles\n        for r, row in enumerate(tbl):\n            for c, v in enumerate(row):\n                ws.write(r, c, label=v)\n    else:\n        # handle styles\n        it = iter(tbl)\n        hdr = next(it)\n        flds = list(map(str, hdr))\n        for c, f in enumerate(flds):\n            ws.write(0, c, label=f)\n            if f not in styles or styles[f] is None:\n                styles[f] = xlwt.Style.default_style\n        # convert to list for easy zipping\n        styles = [styles[f] for f in flds]\n        for r, row in enumerate(it):\n            for c, (v, style) in enumerate(izip_longest(row, styles,\n                                                        fillvalue=None)):\n                ws.write(r+1, c, label=v, style=style)\n\n    wb.save(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef asindices(hdr, spec):\n\n    flds = list(map(text_type, hdr))\n    indices = list()\n    if not isinstance(spec, (list, tuple)):\n        spec = (spec,)\n    for s in spec:\n        # spec could be a field index (takes priority)\n        if isinstance(s, int) and s < len(hdr):\n            indices.append(s)  # index fields from 0\n        # spec could be a field\n        elif s in flds:\n            indices.append(flds.index(s))\n        else:\n            raise FieldSelectionError(s)\n    return indices", "response": "Convert the given field spec into a list of field indices."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expr(s):\n\n    prog = re.compile('\\{([^}]+)\\}')\n\n    def repl(matchobj):\n        return \"rec['%s']\" % matchobj.group(1)\n\n    return eval(\"lambda rec: \" + prog.sub(repl, s))", "response": "Convert a string into a lambda function operating on a table record."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of lists of items grouped by a given key and optional value.", "response": "def rowgroupby(table, key, value=None):\n    \"\"\"Convenient adapter for :func:`itertools.groupby`. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['a', 1, True],\n        ...           ['b', 3, True],\n        ...           ['b', 2]]\n        >>> # group entire rows\n        ... for key, group in etl.rowgroupby(table1, 'foo'):\n        ...     print(key, list(group))\n        ...\n        a [('a', 1, True)]\n        b [('b', 3, True), ('b', 2)]\n        >>> # group specific values\n        ... for key, group in etl.rowgroupby(table1, 'foo', 'bar'):\n        ...     print(key, list(group))\n        ...\n        a [1]\n        b [3, 2]\n\n    N.B., assumes the input table is already sorted by the given key.\n\n    \"\"\"\n\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    # wrap rows as records\n    it = (Record(row, flds) for row in it)\n\n    # determine key function\n    if callable(key):\n        getkey = key\n        native_key = True\n    else:\n        kindices = asindices(hdr, key)\n        getkey = comparable_itemgetter(*kindices)\n        native_key = False\n\n    git = groupby(it, key=getkey)\n    if value is None:\n        if native_key:\n            return git\n        else:\n            return ((k.inner, vals) for (k, vals) in git)\n    else:\n        if callable(value):\n            getval = value\n        else:\n            vindices = asindices(hdr, value)\n            getval = operator.itemgetter(*vindices)\n        if native_key:\n            return ((k, (getval(v) for v in vals))\n                    for (k, vals) in git)\n        else:\n            return ((k.inner, (getval(v) for v in vals))\n                    for (k, vals) in git)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts a table from a delimited CSV file.", "response": "def fromcsv(source=None, encoding=None, errors='strict', header=None, \n            **csvargs):\n    \"\"\"\n    Extract a table from a delimited file. E.g.::\n\n        >>> import petl as etl\n        >>> import csv\n        >>> # set up a CSV file to demonstrate with\n        ... table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2],\n        ...           ['c', 2]]\n        >>> with open('example.csv', 'w') as f:\n        ...     writer = csv.writer(f)\n        ...     writer.writerows(table1)\n        ...\n        >>> # now demonstrate the use of fromcsv()\n        ... table2 = etl.fromcsv('example.csv')\n        >>> table2\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'a' | '1' |\n        +-----+-----+\n        | 'b' | '2' |\n        +-----+-----+\n        | 'c' | '2' |\n        +-----+-----+\n\n    The `source` argument is the path of the delimited file, all other keyword\n    arguments are passed to :func:`csv.reader`. So, e.g., to override the\n    delimiter from the default CSV dialect, provide the `delimiter` keyword\n    argument.\n\n    Note that all data values are strings, and any intended numeric values will\n    need to be converted, see also :func:`petl.transform.conversions.convert`.\n\n    \"\"\"\n\n    source = read_source_from_arg(source)\n    csvargs.setdefault('dialect', 'excel')\n    return fromcsv_impl(source=source, encoding=encoding, errors=errors, \n                        header=header, **csvargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fromtsv(source=None, encoding=None, errors='strict', header=None, \n            **csvargs):\n    \"\"\"\n    Convenience function, as :func:`petl.io.csv.fromcsv` but with different\n    default dialect (tab delimited).\n\n    \"\"\"\n\n    csvargs.setdefault('dialect', 'excel-tab')\n    return fromcsv(source, encoding=encoding, errors=errors, **csvargs)", "response": "Returns a petl. io. CSV object from a tab - delimited text file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tocsv(table, source=None, encoding=None, errors='strict', write_header=True,\n          **csvargs):\n    \"\"\"\n    Write the table to a CSV file. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2],\n        ...           ['c', 2]]\n        >>> etl.tocsv(table1, 'example.csv')\n        >>> # look what it did\n        ... print(open('example.csv').read())\n        foo,bar\n        a,1\n        b,2\n        c,2\n\n    The `source` argument is the path of the delimited file, and the optional\n    `write_header` argument specifies whether to include the field names in the\n    delimited file.  All other keyword arguments are passed to\n    :func:`csv.writer`. So, e.g., to override the delimiter from the default\n    CSV dialect, provide the `delimiter` keyword argument.\n\n    Note that if a file already exists at the given location, it will be\n    overwritten.\n\n    \"\"\"\n\n    source = write_source_from_arg(source)\n    csvargs.setdefault('dialect', 'excel')\n    tocsv_impl(table, source=source, encoding=encoding, errors=errors,\n               write_header=write_header, **csvargs)", "response": "Writes a table to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends data rows to an existing CSV file.", "response": "def appendcsv(table, source=None, encoding=None, errors='strict',\n              write_header=False, **csvargs):\n    \"\"\"\n    Append data rows to an existing CSV file. As :func:`petl.io.csv.tocsv`\n    but the file is opened in append mode and the table header is not written by\n    default.\n\n    Note that no attempt is made to check that the fields or row lengths are\n    consistent with the existing data, the data rows from the table are simply\n    appended to the file.\n\n    \"\"\"\n\n    source = write_source_from_arg(source)\n    csvargs.setdefault('dialect', 'excel')\n    appendcsv_impl(table, source=source, encoding=encoding, errors=errors,\n                   write_header=write_header, **csvargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a table to a CSV file.", "response": "def totsv(table, source=None, encoding=None, errors='strict',\n          write_header=True, **csvargs):\n    \"\"\"\n    Convenience function, as :func:`petl.io.csv.tocsv` but with different\n    default dialect (tab delimited).\n\n    \"\"\"\n\n    csvargs.setdefault('dialect', 'excel-tab')\n    return tocsv(table, source=source, encoding=encoding, errors=errors,\n                 write_header=write_header, **csvargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef appendtsv(table, source=None, encoding=None, errors='strict',\n              write_header=False, **csvargs):\n    \"\"\"\n    Convenience function, as :func:`petl.io.csv.appendcsv` but with different\n    default dialect (tab delimited).\n\n    \"\"\"\n\n    csvargs.setdefault('dialect', 'excel-tab')\n    return appendcsv(table, source=source, encoding=encoding, errors=errors,\n                     write_header=write_header, **csvargs)", "response": "Convenience function to append a table to the end of a petl. io. csv. csv file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef teecsv(table, source=None, encoding=None, errors='strict', write_header=True,\n           **csvargs):\n    \"\"\"\n    Returns a table that writes rows to a CSV file as they are iterated over.\n\n    \"\"\"\n\n    source = write_source_from_arg(source)\n    csvargs.setdefault('dialect', 'excel')\n    return teecsv_impl(table, source=source, encoding=encoding,\n                       errors=errors, write_header=write_header,\n                       **csvargs)", "response": "Writes rows to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef teetsv(table, source=None, encoding=None, errors='strict', write_header=True,\n           **csvargs):\n    \"\"\"\n    Convenience function, as :func:`petl.io.csv.teecsv` but with different\n    default dialect (tab delimited).\n\n    \"\"\"\n\n    csvargs.setdefault('dialect', 'excel-tab')\n    return teecsv(table, source=source, encoding=encoding, errors=errors,\n                  write_header=write_header, **csvargs)", "response": "Convenience function to convert a table into a Television Series."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef capture(table, field, pattern, newfields=None, include_original=False,\n            flags=0, fill=None):\n    \"\"\"\n    Add one or more new fields with values captured from an existing field\n    searched via a regular expression. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'variable', 'value'],\n        ...           ['1', 'A1', '12'],\n        ...           ['2', 'A2', '15'],\n        ...           ['3', 'B1', '18'],\n        ...           ['4', 'C12', '19']]\n        >>> table2 = etl.capture(table1, 'variable', '(\\\\w)(\\\\d+)',\n        ...                      ['treat', 'time'])\n        >>> table2\n        +-----+-------+-------+------+\n        | id  | value | treat | time |\n        +=====+=======+=======+======+\n        | '1' | '12'  | 'A'   | '1'  |\n        +-----+-------+-------+------+\n        | '2' | '15'  | 'A'   | '2'  |\n        +-----+-------+-------+------+\n        | '3' | '18'  | 'B'   | '1'  |\n        +-----+-------+-------+------+\n        | '4' | '19'  | 'C'   | '12' |\n        +-----+-------+-------+------+\n\n        >>> # using the include_original argument\n        ... table3 = etl.capture(table1, 'variable', '(\\\\w)(\\\\d+)',\n        ...                      ['treat', 'time'],\n        ...                      include_original=True)\n        >>> table3\n        +-----+----------+-------+-------+------+\n        | id  | variable | value | treat | time |\n        +=====+==========+=======+=======+======+\n        | '1' | 'A1'     | '12'  | 'A'   | '1'  |\n        +-----+----------+-------+-------+------+\n        | '2' | 'A2'     | '15'  | 'A'   | '2'  |\n        +-----+----------+-------+-------+------+\n        | '3' | 'B1'     | '18'  | 'B'   | '1'  |\n        +-----+----------+-------+-------+------+\n        | '4' | 'C12'    | '19'  | 'C'   | '12' |\n        +-----+----------+-------+-------+------+\n\n    By default the field on which the capture is performed is omitted. It can\n    be included using the `include_original` argument.\n\n    The ``fill`` parameter can be used to provide a list or tuple of values to\n    use if the regular expression does not match. The ``fill`` parameter\n    should contain as many values as there are capturing groups in the regular\n    expression. If ``fill`` is ``None`` (default) then a\n    ``petl.transform.TransformError`` will be raised on the first non-matching\n    value.\n\n    \"\"\"\n\n    return CaptureView(table, field, pattern,\n                       newfields=newfields,\n                       include_original=include_original,\n                       flags=flags,\n                       fill=fill)", "response": "Capture a single field from an existing field."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits a table into two new fields with values generated by splitting the existing ones around occurrences of a regular expression.", "response": "def split(table, field, pattern, newfields=None, include_original=False,\n          maxsplit=0, flags=0):\n    \"\"\"\n    Add one or more new fields with values generated by splitting an\n    existing value around occurrences of a regular expression. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'variable', 'value'],\n        ...           ['1', 'parad1', '12'],\n        ...           ['2', 'parad2', '15'],\n        ...           ['3', 'tempd1', '18'],\n        ...           ['4', 'tempd2', '19']]\n        >>> table2 = etl.split(table1, 'variable', 'd', ['variable', 'day'])\n        >>> table2\n        +-----+-------+----------+-----+\n        | id  | value | variable | day |\n        +=====+=======+==========+=====+\n        | '1' | '12'  | 'para'   | '1' |\n        +-----+-------+----------+-----+\n        | '2' | '15'  | 'para'   | '2' |\n        +-----+-------+----------+-----+\n        | '3' | '18'  | 'temp'   | '1' |\n        +-----+-------+----------+-----+\n        | '4' | '19'  | 'temp'   | '2' |\n        +-----+-------+----------+-----+\n\n    By default the field on which the split is performed is omitted. It can\n    be included using the `include_original` argument.\n\n    \"\"\"\n\n    return SplitView(table, field, pattern, newfields, include_original,\n                     maxsplit, flags)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sub(table, field, pattern, repl, count=0, flags=0):\n\n    prog = re.compile(pattern, flags)\n    conv = lambda v: prog.sub(repl, v, count=count)\n    return convert(table, field, conv)", "response": "Convert values under the given field using a regular expression substitution."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search(table, *args, **kwargs):\n\n    if len(args) == 1:\n        field = None\n        pattern = args[0]\n    elif len(args) == 2:\n        field = args[0]\n        pattern = args[1]\n    else:\n        raise ArgumentError('expected 1 or 2 positional arguments')\n    return SearchView(table, pattern, field=field, **kwargs)", "response": "Perform a regular expression search on a given table and return rows that match a given pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef searchcomplement(table, *args, **kwargs):\n\n    return search(table, *args, complement=True, **kwargs)", "response": "Search complement of a regular expression search."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fromcolumns(cols, header=None, missing=None):\n\n    return ColumnsView(cols, header=header, missing=missing)", "response": "View a sequence of columns as a table e. g."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cut(table, *args, **kwargs):\n\n    # support passing a single list or tuple of fields\n    if len(args) == 1 and isinstance(args[0], (list, tuple)):\n        args = args[0]\n\n    return CutView(table, args, **kwargs)", "response": "Returns a new version of the given table with the specified fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a field to the specified table with a fixed or calculated value.", "response": "def addfield(table, field, value=None, index=None, missing=None):\n    \"\"\"\n    Add a field with a fixed or calculated value. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['M', 12],\n        ...           ['F', 34],\n        ...           ['-', 56]]\n        >>> # using a fixed value\n        ... table2 = etl.addfield(table1, 'baz', 42)\n        >>> table2\n        +-----+-----+-----+\n        | foo | bar | baz |\n        +=====+=====+=====+\n        | 'M' |  12 |  42 |\n        +-----+-----+-----+\n        | 'F' |  34 |  42 |\n        +-----+-----+-----+\n        | '-' |  56 |  42 |\n        +-----+-----+-----+\n\n        >>> # calculating the value\n        ... table2 = etl.addfield(table1, 'baz', lambda rec: rec['bar'] * 2)\n        >>> table2\n        +-----+-----+-----+\n        | foo | bar | baz |\n        +=====+=====+=====+\n        | 'M' |  12 |  24 |\n        +-----+-----+-----+\n        | 'F' |  34 |  68 |\n        +-----+-----+-----+\n        | '-' |  56 | 112 |\n        +-----+-----+-----+\n\n    Use the `index` parameter to control the position of the inserted field.\n\n    \"\"\"\n\n    return AddFieldView(table, field, value=value, index=index,\n                        missing=missing)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a field of row numbers. Eg.", "response": "def addrownumbers(table, start=1, step=1, field='row'):\n    \"\"\"\n    Add a field of row numbers. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['A', 9],\n        ...           ['C', 2],\n        ...           ['F', 1]]\n        >>> table2 = etl.addrownumbers(table1)\n        >>> table2\n        +-----+-----+-----+\n        | row | foo | bar |\n        +=====+=====+=====+\n        |   1 | 'A' |   9 |\n        +-----+-----+-----+\n        |   2 | 'C' |   2 |\n        +-----+-----+-----+\n        |   3 | 'F' |   1 |\n        +-----+-----+-----+\n\n    Parameters `start` and `step` control the numbering.\n\n    \"\"\"\n\n    return AddRowNumbersView(table, start, step, field)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef addcolumn(table, field, col, index=None, missing=None):\n\n    return AddColumnView(table, field, col, index=index, missing=missing)", "response": "Add a column to the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a set containing all Python types found for values in the given field. Eg. :: Returns a set containing all Python types found for values in the given field.", "response": "def typeset(table, field):\n    \"\"\"\n    Return a set containing all Python types found for values in the given\n    field. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar', 'baz'],\n        ...          ['A', 1, '2'],\n        ...          ['B', u'2', '3.4'],\n        ...          [u'B', u'3', '7.8', True],\n        ...          ['D', u'xyz', 9.0],\n        ...          ['E', 42]]\n        >>> sorted(etl.typeset(table, 'foo'))\n        ['str']\n        >>> sorted(etl.typeset(table, 'bar'))\n        ['int', 'str']\n        >>> sorted(etl.typeset(table, 'baz'))\n        ['NoneType', 'float', 'str']\n\n    The `field` argument can be a field name or index (starting from zero).\n\n    \"\"\"\n\n    s = set()\n    for v in values(table, field):\n        try:\n            s.add(type(v).__name__)\n        except IndexError:\n            pass  # ignore short rows\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef diffheaders(t1, t2):\n\n    t1h = set(header(t1))\n    t2h = set(header(t2))\n    return t2h - t1h, t1h - t2h", "response": "Return the difference between the headers of two tables as a pair of\n    sets. E. g."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef diffvalues(t1, t2, f):\n\n    t1v = set(values(t1, f))\n    t2v = set(values(t2, f))\n    return t2v - t1v, t1v - t2v", "response": "Return the difference between the values t1 and t2 under the given field in the two\n    tables e. g."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a function which accepts a row and returns the first non - missing value from the specified fields.", "response": "def coalesce(*fields, **kwargs):\n    \"\"\"\n    Return a function which accepts a row and returns the first non-missing\n    value from the specified fields. Intended for use with\n    :func:`petl.transform.basics.addfield`.\n\n    \"\"\"\n    missing = kwargs.get('missing', None)\n    default = kwargs.get('default', None)\n\n    def _coalesce(row):\n        for f in fields:\n            v = row[f]\n            if v is not missing:\n                return v\n        return default\n\n    return _coalesce"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef datetimeparser(fmt, strict=True):\n\n    def parser(value):\n        try:\n            return datetime.datetime.strptime(value.strip(), fmt)\n        except Exception as e:\n            if strict:\n                raise e\n            else:\n                return value\n    return parser", "response": "Return a function to parse strings as datetime. datetime objects as a list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a function to parse strings as datetime. date objects using a given format. E. g. datetime. date.", "response": "def dateparser(fmt, strict=True):\n    \"\"\"Return a function to parse strings as :class:`datetime.date` objects\n    using a given format. E.g.::\n\n        >>> from petl import dateparser\n        >>> isodate = dateparser('%Y-%m-%d')\n        >>> isodate('2002-12-25')\n        datetime.date(2002, 12, 25)\n        >>> try:\n        ...     isodate('2002-02-30')\n        ... except ValueError as e:\n        ...     print(e)\n        ...\n        day is out of range for month\n\n    If ``strict=False`` then if an error occurs when parsing, the original\n    value will be returned as-is, and no error will be raised.\n\n    \"\"\"\n\n    def parser(value):\n        try:\n            return datetime.datetime.strptime(value.strip(), fmt).date()\n        except Exception as e:\n            if strict:\n                raise e\n            else:\n                return value\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a function to parse strings as datetime. time objects using a given format. E. g. datetime. time. strftime is used to parse the time data.", "response": "def timeparser(fmt, strict=True):\n    \"\"\"Return a function to parse strings as :class:`datetime.time` objects\n    using a given format. E.g.::\n\n        >>> from petl import timeparser\n        >>> isotime = timeparser('%H:%M:%S')\n        >>> isotime('00:00:00')\n        datetime.time(0, 0)\n        >>> isotime('13:00:00')\n        datetime.time(13, 0)\n        >>> try:\n        ...     isotime('12:00:99')\n        ... except ValueError as e:\n        ...     print(e)\n        ...\n        unconverted data remains: 9\n        >>> try:\n        ...     isotime('25:00:00')\n        ... except ValueError as e:\n        ...     print(e)\n        ...\n        time data '25:00:00' does not match format '%H:%M:%S'\n\n    If ``strict=False`` then if an error occurs when parsing, the original\n    value will be returned as-is, and no error will be raised.\n\n    \"\"\"\n\n    def parser(value):\n        try:\n            return datetime.datetime.strptime(value.strip(), fmt).time()\n        except Exception as e:\n            if strict:\n                raise e\n            else:\n                return value\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function to parse strings as bool objects using a base class.", "response": "def boolparser(true_strings=('true', 't', 'yes', 'y', '1'),\n               false_strings=('false', 'f', 'no', 'n', '0'),\n               case_sensitive=False,\n               strict=True):\n    \"\"\"Return a function to parse strings as :class:`bool` objects using a\n    given set of string representations for `True` and `False`. E.g.::\n\n        >>> from petl import boolparser\n        >>> mybool = boolparser(true_strings=['yes', 'y'], false_strings=['no', 'n'])\n        >>> mybool('y')\n        True\n        >>> mybool('yes')\n        True\n        >>> mybool('Y')\n        True\n        >>> mybool('No')\n        False\n        >>> try:\n        ...     mybool('foo')\n        ... except ValueError as e:\n        ...     print(e)\n        ...\n        value is not one of recognised boolean strings: 'foo'\n        >>> try:\n        ...     mybool('True')\n        ... except ValueError as e:\n        ...     print(e)\n        ...\n        value is not one of recognised boolean strings: 'true'\n\n    If ``strict=False`` then if an error occurs when parsing, the original\n    value will be returned as-is, and no error will be raised.\n\n    \"\"\"\n\n    if not case_sensitive:\n        true_strings = [s.lower() for s in true_strings]\n        false_strings = [s.lower() for s in false_strings]\n\n    def parser(value):\n        value = value.strip()\n        if not case_sensitive:\n            value = value.lower()\n        if value in true_strings:\n            return True\n        elif value in false_strings:\n            return False\n        elif strict:\n            raise ValueError('value is not one of recognised boolean strings: '\n                             '%r' % value)\n        else:\n            return value\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef numparser(strict=False):\n\n    def f(v):\n        try:\n            return int(v)\n        except (ValueError, TypeError):\n            pass\n        try:\n            return long(v)\n        except (ValueError, TypeError):\n            pass\n        try:\n            return float(v)\n        except (ValueError, TypeError):\n            pass\n        try:\n            return complex(v)\n        except (ValueError, TypeError) as e:\n            if strict:\n                raise e\n        return v\n\n    return f", "response": "Return a function that will attempt to parse the value as a number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreports progress on rows passing through. Eg.", "response": "def progress(table, batchsize=1000, prefix=\"\", out=sys.stderr):\n    \"\"\"\n    Report progress on rows passing through. E.g.::\n\n        >>> import petl as etl\n        >>> table = etl.dummytable(100000)\n        >>> table.progress(10000).tocsv('example.csv')\n        10000 rows in 0.13s (78363 row/s); batch in 0.13s (78363 row/s)\n        20000 rows in 0.22s (91679 row/s); batch in 0.09s (110448 row/s)\n        30000 rows in 0.31s (96573 row/s); batch in 0.09s (108114 row/s)\n        40000 rows in 0.40s (99535 row/s); batch in 0.09s (109625 row/s)\n        50000 rows in 0.49s (101396 row/s); batch in 0.09s (109591 row/s)\n        60000 rows in 0.59s (102245 row/s); batch in 0.09s (106709 row/s)\n        70000 rows in 0.68s (103221 row/s); batch in 0.09s (109498 row/s)\n        80000 rows in 0.77s (103810 row/s); batch in 0.09s (108126 row/s)\n        90000 rows in 0.90s (99465 row/s); batch in 0.13s (74516 row/s)\n        100000 rows in 1.02s (98409 row/s); batch in 0.11s (89821 row/s)\n        100000 rows in 1.02s (98402 row/s); batches in 0.10 +/- 0.02s [0.09-0.13] (100481 +/- 13340 rows/s [74516-110448])\n\n    See also :func:`petl.util.timing.clock`.\n\n    \"\"\"\n\n    return ProgressView(table, batchsize, prefix, out)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning rows in `a` that are not in `b`. E.g.:: >>> import petl as etl >>> a = [['foo', 'bar', 'baz'], ... ['A', 1, True], ... ['C', 7, False], ... ['B', 2, False], ... ['C', 9, True]] >>> b = [['x', 'y', 'z'], ... ['B', 2, False], ... ['A', 9, False], ... ['B', 3, True], ... ['C', 9, True]] >>> aminusb = etl.complement(a, b) >>> aminusb +-----+-----+-------+ | foo | bar | baz | +=====+=====+=======+ | 'A' | 1 | True | +-----+-----+-------+ | 'C' | 7 | False | +-----+-----+-------+ >>> bminusa = etl.complement(b, a) >>> bminusa +-----+---+-------+ | x | y | z | +=====+===+=======+ | 'A' | 9 | False | +-----+---+-------+ | 'B' | 3 | True | +-----+---+-------+ Note that the field names of each table are ignored - rows are simply compared following a lexical sort. See also the :func:`petl.transform.setops.recordcomplement` function. If `presorted` is True, it is assumed that the data are already sorted by the given key, and the `buffersize`, `tempdir` and `cache` arguments are ignored. Otherwise, the data are sorted, see also the discussion of the `buffersize`, `tempdir` and `cache` arguments under the :func:`petl.transform.sorts.sort` function. Note that the default behaviour is not strictly set-like, because duplicate rows are counted separately, e.g.:: >>> a = [['foo', 'bar'], ... ['A', 1], ... ['B', 2], ... ['B', 2], ... ['C', 7]] >>> b = [['foo', 'bar'], ... ['B', 2]] >>> aminusb = etl.complement(a, b) >>> aminusb +-----+-----+ | foo | bar | +=====+=====+ | 'A' | 1 | +-----+-----+ | 'B' | 2 | +-----+-----+ | 'C' | 7 | +-----+-----+ This behaviour can be changed with the `strict` keyword argument, e.g.:: >>> aminusb = etl.complement(a, b, strict=True) >>> aminusb +-----+-----+ | foo | bar | +=====+=====+ | 'A' | 1 | +-----+-----+ | 'C' | 7 | +-----+-----+ .. versionchanged:: 1.1.0 If `strict` is `True` then strict set-like behaviour is used, i.e., only rows in `a` not found in `b` are returned.", "response": "def complement(a, b, presorted=False, buffersize=None, tempdir=None,\n               cache=True, strict=False):\n    \"\"\"\n    Return rows in `a` that are not in `b`. E.g.::\n\n        >>> import petl as etl\n        >>> a = [['foo', 'bar', 'baz'],\n        ...      ['A', 1, True],\n        ...      ['C', 7, False],\n        ...      ['B', 2, False],\n        ...      ['C', 9, True]]\n        >>> b = [['x', 'y', 'z'],\n        ...      ['B', 2, False],\n        ...      ['A', 9, False],\n        ...      ['B', 3, True],\n        ...      ['C', 9, True]]\n        >>> aminusb = etl.complement(a, b)\n        >>> aminusb\n        +-----+-----+-------+\n        | foo | bar | baz   |\n        +=====+=====+=======+\n        | 'A' |   1 | True  |\n        +-----+-----+-------+\n        | 'C' |   7 | False |\n        +-----+-----+-------+\n\n        >>> bminusa = etl.complement(b, a)\n        >>> bminusa\n        +-----+---+-------+\n        | x   | y | z     |\n        +=====+===+=======+\n        | 'A' | 9 | False |\n        +-----+---+-------+\n        | 'B' | 3 | True  |\n        +-----+---+-------+\n\n    Note that the field names of each table are ignored - rows are simply\n    compared following a lexical sort. See also the\n    :func:`petl.transform.setops.recordcomplement` function.\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    Note that the default behaviour is not strictly set-like, because \n    duplicate rows are counted separately, e.g.::\n    \n        >>> a = [['foo', 'bar'],\n        ...      ['A', 1],\n        ...      ['B', 2],\n        ...      ['B', 2],\n        ...      ['C', 7]]\n        >>> b = [['foo', 'bar'],\n        ...      ['B', 2]]\n        >>> aminusb = etl.complement(a, b)\n        >>> aminusb\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'A' |   1 |\n        +-----+-----+\n        | 'B' |   2 |\n        +-----+-----+\n        | 'C' |   7 |\n        +-----+-----+\n\n    This behaviour can be changed with the `strict` keyword argument, e.g.::\n\n        >>> aminusb = etl.complement(a, b, strict=True)\n        >>> aminusb\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'A' |   1 |\n        +-----+-----+\n        | 'C' |   7 |\n        +-----+-----+\n\n    .. versionchanged:: 1.1.0\n    \n    If `strict` is `True` then strict set-like behaviour is used, i.e., \n    only rows in `a` not found in `b` are returned.\n\n    \"\"\"\n\n    return ComplementView(a, b, presorted=presorted, buffersize=buffersize,\n                          tempdir=tempdir, cache=cache, strict=strict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recordcomplement(a, b, buffersize=None, tempdir=None, cache=True,\n                     strict=False):\n    \"\"\"\n    Find records in `a` that are not in `b`. E.g.::\n\n        >>> import petl as etl\n        >>> a = [['foo', 'bar', 'baz'],\n        ...      ['A', 1, True],\n        ...      ['C', 7, False],\n        ...      ['B', 2, False],\n        ...      ['C', 9, True]]\n        >>> b = [['bar', 'foo', 'baz'],\n        ...      [2, 'B', False],\n        ...      [9, 'A', False],\n        ...      [3, 'B', True],\n        ...      [9, 'C', True]]\n        >>> aminusb = etl.recordcomplement(a, b)\n        >>> aminusb\n        +-----+-----+-------+\n        | foo | bar | baz   |\n        +=====+=====+=======+\n        | 'A' |   1 | True  |\n        +-----+-----+-------+\n        | 'C' |   7 | False |\n        +-----+-----+-------+\n\n        >>> bminusa = etl.recordcomplement(b, a)\n        >>> bminusa\n        +-----+-----+-------+\n        | bar | foo | baz   |\n        +=====+=====+=======+\n        |   3 | 'B' | True  |\n        +-----+-----+-------+\n        |   9 | 'A' | False |\n        +-----+-----+-------+\n\n    Note that both tables must have the same set of fields, but that the order\n    of the fields does not matter. See also the\n    :func:`petl.transform.setops.complement` function.\n\n    See also the discussion of the `buffersize`, `tempdir` and `cache` arguments\n    under the :func:`petl.transform.sorts.sort` function.\n\n    \"\"\"\n\n    # TODO possible with only one pass?\n\n    ha = header(a)\n    hb = header(b)\n    assert set(ha) == set(hb), 'both tables must have the same set of fields'\n    # make sure fields are in the same order\n    bv = cut(b, *ha)\n    return complement(a, bv, buffersize=buffersize, tempdir=tempdir,\n                      cache=cache, strict=strict)", "response": "This function is used to record complements two tables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a pair of tables that are the difference between two tables.", "response": "def diff(a, b, presorted=False, buffersize=None, tempdir=None, cache=True,\n         strict=False):\n    \"\"\"\n    Find the difference between rows in two tables. Returns a pair of tables.\n    E.g.::\n\n        >>> import petl as etl\n        >>> a = [['foo', 'bar', 'baz'],\n        ...      ['A', 1, True],\n        ...      ['C', 7, False],\n        ...      ['B', 2, False],\n        ...      ['C', 9, True]]\n        >>> b = [['x', 'y', 'z'],\n        ...      ['B', 2, False],\n        ...      ['A', 9, False],\n        ...      ['B', 3, True],\n        ...      ['C', 9, True]]\n        >>> added, subtracted = etl.diff(a, b)\n        >>> # rows in b not in a\n        ... added\n        +-----+---+-------+\n        | x   | y | z     |\n        +=====+===+=======+\n        | 'A' | 9 | False |\n        +-----+---+-------+\n        | 'B' | 3 | True  |\n        +-----+---+-------+\n\n        >>> # rows in a not in b\n        ... subtracted\n        +-----+-----+-------+\n        | foo | bar | baz   |\n        +=====+=====+=======+\n        | 'A' |   1 | True  |\n        +-----+-----+-------+\n        | 'C' |   7 | False |\n        +-----+-----+-------+\n\n    Convenient shorthand for ``(complement(b, a), complement(a, b))``. See also\n    :func:`petl.transform.setops.complement`.\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    .. versionchanged:: 1.1.0\n\n    If `strict` is `True` then strict set-like behaviour is used.\n\n    \"\"\"\n\n    if not presorted:\n        a = sort(a)\n        b = sort(b)\n    added = complement(b, a, presorted=True, buffersize=buffersize,\n                       tempdir=tempdir, cache=cache, strict=strict)\n    subtracted = complement(a, b, presorted=True, buffersize=buffersize,\n                            tempdir=tempdir, cache=cache, strict=strict)\n    return added, subtracted"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recorddiff(a, b, buffersize=None, tempdir=None, cache=True, strict=False):\n\n    added = recordcomplement(b, a, buffersize=buffersize, tempdir=tempdir,\n                             cache=cache, strict=strict)\n    subtracted = recordcomplement(a, b, buffersize=buffersize, tempdir=tempdir,\n                                  cache=cache, strict=strict)\n    return added, subtracted", "response": "Find the difference between two records in two tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns rows in a that are also in b.", "response": "def intersection(a, b, presorted=False, buffersize=None, tempdir=None,\n                 cache=True):\n    \"\"\"\n    Return rows in `a` that are also in `b`. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', 1, True],\n        ...           ['C', 7, False],\n        ...           ['B', 2, False],\n        ...           ['C', 9, True]]\n        >>> table2 = [['x', 'y', 'z'],\n        ...           ['B', 2, False],\n        ...           ['A', 9, False],\n        ...           ['B', 3, True],\n        ...           ['C', 9, True]]\n        >>> table3 = etl.intersection(table1, table2)\n        >>> table3\n        +-----+-----+-------+\n        | foo | bar | baz   |\n        +=====+=====+=======+\n        | 'B' |   2 | False |\n        +-----+-----+-------+\n        | 'C' |   9 | True  |\n        +-----+-----+-------+\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    \"\"\"\n\n    return IntersectionView(a, b, presorted=presorted, buffersize=buffersize,\n                            tempdir=tempdir, cache=cache)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hashcomplement(a, b, strict=False):\n\n    return HashComplementView(a, b, strict=strict)", "response": "A convenience method that returns a complement of a set of rows in b."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef duplicates(table, key=None, presorted=False, buffersize=None, tempdir=None, \n               cache=True):\n    \"\"\"\n    Select rows with duplicate values under a given key (or duplicate\n    rows where no key is given). E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', 1, 2.0],\n        ...           ['B', 2, 3.4],\n        ...           ['D', 6, 9.3],\n        ...           ['B', 3, 7.8],\n        ...           ['B', 2, 12.3],\n        ...           ['E', None, 1.3],\n        ...           ['D', 4, 14.5]]\n        >>> table2 = etl.duplicates(table1, 'foo')\n        >>> table2\n        +-----+-----+------+\n        | foo | bar | baz  |\n        +=====+=====+======+\n        | 'B' |   2 |  3.4 |\n        +-----+-----+------+\n        | 'B' |   3 |  7.8 |\n        +-----+-----+------+\n        | 'B' |   2 | 12.3 |\n        +-----+-----+------+\n        | 'D' |   6 |  9.3 |\n        +-----+-----+------+\n        | 'D' |   4 | 14.5 |\n        +-----+-----+------+\n\n        >>> # compound keys are supported\n        ... table3 = etl.duplicates(table1, key=['foo', 'bar'])\n        >>> table3\n        +-----+-----+------+\n        | foo | bar | baz  |\n        +=====+=====+======+\n        | 'B' |   2 |  3.4 |\n        +-----+-----+------+\n        | 'B' |   2 | 12.3 |\n        +-----+-----+------+\n        \n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are \n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n    \n    See also :func:`petl.transform.dedup.unique` and\n    :func:`petl.transform.dedup.distinct`.\n    \n    \"\"\"\n\n    return DuplicatesView(table, key=key, presorted=presorted, \n                          buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Return a list of duplicate tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unique(table, key=None, presorted=False, buffersize=None, tempdir=None,\n           cache=True):\n    \"\"\"\n    Select rows with unique values under a given key (or unique rows\n    if no key is given). E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', 1, 2],\n        ...           ['B', '2', '3.4'],\n        ...           ['D', 'xyz', 9.0],\n        ...           ['B', u'3', u'7.8'],\n        ...           ['B', '2', 42],\n        ...           ['E', None, None],\n        ...           ['D', 4, 12.3],\n        ...           ['F', 7, 2.3]]\n        >>> table2 = etl.unique(table1, 'foo')\n        >>> table2\n        +-----+------+------+\n        | foo | bar  | baz  |\n        +=====+======+======+\n        | 'A' |    1 |    2 |\n        +-----+------+------+\n        | 'E' | None | None |\n        +-----+------+------+\n        | 'F' |    7 |  2.3 |\n        +-----+------+------+\n        \n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n\n    See also :func:`petl.transform.dedup.duplicates` and\n    :func:`petl.transform.dedup.distinct`.\n    \n    \"\"\"\n\n    return UniqueView(table, key=key, presorted=presorted, \n                      buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Return a unique view of the given table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef conflicts(table, key, missing=None, include=None, exclude=None, \n              presorted=False, buffersize=None, tempdir=None, cache=True):\n    \"\"\"\n    Select rows with the same key value but differing in some other field.\n    E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['A', 1, 2.7],\n        ...           ['B', 2, None],\n        ...           ['D', 3, 9.4],\n        ...           ['B', None, 7.8],\n        ...           ['E', None],\n        ...           ['D', 3, 12.3],\n        ...           ['A', 2, None]]\n        >>> table2 = etl.conflicts(table1, 'foo')\n        >>> table2\n        +-----+-----+------+\n        | foo | bar | baz  |\n        +=====+=====+======+\n        | 'A' |   1 |  2.7 |\n        +-----+-----+------+\n        | 'A' |   2 | None |\n        +-----+-----+------+\n        | 'D' |   3 |  9.4 |\n        +-----+-----+------+\n        | 'D' |   3 | 12.3 |\n        +-----+-----+------+\n        \n    Missing values are not considered conflicts. By default, `None` is treated\n    as the missing value, this can be changed via the `missing` keyword \n    argument.\n\n    One or more fields can be ignored when determining conflicts by providing\n    the `exclude` keyword argument. Alternatively, fields to use when\n    determining conflicts can be specified explicitly with the `include`\n    keyword argument. This provides a simple mechanism for analysing the\n    source of conflicting rows from multiple tables, e.g.::\n\n        >>> table1 = [['foo', 'bar'], [1, 'a'], [2, 'b']]\n        >>> table2 = [['foo', 'bar'], [1, 'a'], [2, 'c']]\n        >>> table3 = etl.cat(etl.addfield(table1, 'source', 1),\n        ...                  etl.addfield(table2, 'source', 2))\n        >>> table4 = etl.conflicts(table3, key='foo', exclude='source')\n        >>> table4\n        +-----+-----+--------+\n        | foo | bar | source |\n        +=====+=====+========+\n        |   2 | 'b' |      1 |\n        +-----+-----+--------+\n        |   2 | 'c' |      2 |\n        +-----+-----+--------+\n\n    If `presorted` is True, it is assumed that the data are already sorted by\n    the given key, and the `buffersize`, `tempdir` and `cache` arguments are\n    ignored. Otherwise, the data are sorted, see also the discussion of the\n    `buffersize`, `tempdir` and `cache` arguments under the\n    :func:`petl.transform.sorts.sort` function.\n    \n    \"\"\"\n    \n    return ConflictsView(table, key, missing=missing, exclude=exclude,\n                         include=include, presorted=presorted,\n                         buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Return a list of tables that conflict with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef distinct(table, key=None, count=None, presorted=False, buffersize=None,\n             tempdir=None, cache=True):\n    \"\"\"\n    Return only distinct rows in the table.\n\n    If the `count` argument is not None, it will be used as the name for an\n    additional field, and the values of the field will be the number of\n    duplicate rows.\n\n    If the `key` keyword argument is passed, the comparison is done on the\n    given key instead of the full row.\n\n    See also :func:`petl.transform.dedup.duplicates`,\n    :func:`petl.transform.dedup.unique`,\n    :func:`petl.transform.reductions.groupselectfirst`,\n    :func:`petl.transform.reductions.groupselectlast`.\n\n    \"\"\"\n\n    return DistinctView(table, key=key, count=count, presorted=presorted,\n                        buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Return only distinct rows in the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if there are no duplicate values for the given field.", "response": "def isunique(table, field):\n    \"\"\"\n    Return True if there are no duplicate values for the given field(s),\n    otherwise False. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b'],\n        ...           ['b', 2],\n        ...           ['c', 3, True]]\n        >>> etl.isunique(table1, 'foo')\n        False\n        >>> etl.isunique(table1, 'bar')\n        True\n\n    The `field` argument can be a single field name or index (starting from\n    zero) or a tuple of field names and/or indexes.\n\n    \"\"\"\n\n    vals = set()\n    for v in itervalues(table, field):\n        if v in vals:\n            return False\n        else:\n            vals.add(v)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninfer an appropriate SQLAlchemy column type based on a sequence of values.", "response": "def make_sqlalchemy_column(col, colname, constraints=True):\n    \"\"\"\n    Infer an appropriate SQLAlchemy column type based on a sequence of values.\n\n    Keyword arguments:\n\n    col : sequence\n        A sequence of values to use to infer type, length etc.\n    colname : string\n        Name of column\n    constraints : bool\n        If True use length and nullable constraints\n\n    \"\"\"\n\n    import sqlalchemy\n\n    col_not_none = [v for v in col if v is not None]\n    sql_column_kwargs = {}\n    sql_type_kwargs = {}\n\n    if len(col_not_none) == 0:\n        sql_column_type = sqlalchemy.String\n        if constraints:\n            sql_type_kwargs['length'] = NULL_COLUMN_MAX_LENGTH\n\n    elif all(isinstance(v, bool) for v in col_not_none):\n        sql_column_type = sqlalchemy.Boolean\n\n    elif all(isinstance(v, int) for v in col_not_none):\n        if max(col_not_none) > SQL_INTEGER_MAX \\\n                or min(col_not_none) < SQL_INTEGER_MIN:\n            sql_column_type = sqlalchemy.BigInteger\n        else:\n            sql_column_type = sqlalchemy.Integer\n\n    elif all(isinstance(v, long) for v in col_not_none):\n        sql_column_type = sqlalchemy.BigInteger\n\n    elif all(isinstance(v, (int, long)) for v in col_not_none):\n        sql_column_type = sqlalchemy.BigInteger\n\n    elif all(isinstance(v, (int, long, float)) for v in col_not_none):\n        sql_column_type = sqlalchemy.Float\n\n    elif all(isinstance(v, datetime.datetime) for v in col_not_none):\n        sql_column_type = sqlalchemy.DateTime\n\n    elif all(isinstance(v, datetime.date) for v in col_not_none):\n        sql_column_type = sqlalchemy.Date\n\n    elif all(isinstance(v, datetime.time) for v in col_not_none):\n        sql_column_type = sqlalchemy.Time\n\n    else:\n        sql_column_type = sqlalchemy.String\n        if constraints:\n            sql_type_kwargs['length'] = max([len(text_type(v)) for v in col])\n\n    if constraints:\n        sql_column_kwargs['nullable'] = len(col_not_none) < len(col)\n\n    return sqlalchemy.Column(colname, sql_column_type(**sql_type_kwargs),\n                             **sql_column_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an SQLAlchemy table definition based on data in table.", "response": "def make_sqlalchemy_table(table, tablename, schema=None, constraints=True,\n                          metadata=None):\n    \"\"\"\n    Create an SQLAlchemy table definition based on data in `table`.\n\n    Keyword arguments:\n\n    table : table container\n        Table data to use to infer types etc.\n    tablename : text\n        Name of the table\n    schema : text\n        Name of the database schema to create the table in\n    constraints : bool\n        If True use length and nullable constraints\n    metadata : sqlalchemy.MetaData\n        Custom table metadata\n\n    \"\"\"\n\n    import sqlalchemy\n\n    if not metadata:\n        metadata = sqlalchemy.MetaData()\n\n    sql_table = sqlalchemy.Table(tablename, metadata, schema=schema)\n    cols = columns(table)\n    flds = list(cols.keys())\n    for f in flds:\n        sql_column = make_sqlalchemy_column(cols[f], f,\n                                            constraints=constraints)\n        sql_table.append_column(sql_column)\n\n    return sql_table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a CREATE TABLE statement based on data in table.", "response": "def make_create_table_statement(table, tablename, schema=None,\n                                constraints=True, metadata=None, dialect=None):\n    \"\"\"\n    Generate a CREATE TABLE statement based on data in `table`.\n\n    Keyword arguments:\n\n    table : table container\n        Table data to use to infer types etc.\n    tablename : text\n        Name of the table\n    schema : text\n        Name of the database schema to create the table in\n    constraints : bool\n        If True use length and nullable constraints\n    metadata : sqlalchemy.MetaData\n        Custom table metadata\n    dialect : text\n        One of {'access', 'sybase', 'sqlite', 'informix', 'firebird', 'mysql',\n        'oracle', 'maxdb', 'postgresql', 'mssql'}\n\n    \"\"\"\n\n    import sqlalchemy\n    sql_table = make_sqlalchemy_table(table, tablename, schema=schema,\n                                      constraints=constraints,\n                                      metadata=metadata)\n\n    if dialect:\n        module = __import__('sqlalchemy.dialects.%s' % DIALECTS[dialect],\n                            fromlist=['dialect'])\n        sql_dialect = module.dialect()\n    else:\n        sql_dialect = None\n\n    return text_type(sqlalchemy.schema.CreateTable(sql_table)\n                     .compile(dialect=sql_dialect)).strip()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_table(table, dbo, tablename, schema=None, commit=True,\n                 constraints=True, metadata=None, dialect=None, sample=1000):\n    \"\"\"\n    Create a database table based on a sample of data in the given `table`.\n\n    Keyword arguments:\n\n    table : table container\n        Table data to load\n    dbo : database object\n        DB-API 2.0 connection, callable returning a DB-API 2.0 cursor, or\n        SQLAlchemy connection, engine or session\n    tablename : text\n        Name of the table\n    schema : text\n        Name of the database schema to create the table in\n    commit : bool\n        If True commit the changes\n    constraints : bool\n        If True use length and nullable constraints\n    metadata : sqlalchemy.MetaData\n        Custom table metadata\n    dialect : text\n        One of {'access', 'sybase', 'sqlite', 'informix', 'firebird', 'mysql',\n        'oracle', 'maxdb', 'postgresql', 'mssql'}\n    sample : int\n        Number of rows to sample when inferring types etc., set to 0 to use\n        the whole table\n\n    \"\"\"\n\n    if sample > 0:\n        table = head(table, sample)\n    sql = make_create_table_statement(table, tablename, schema=schema,\n                                      constraints=constraints,\n                                      metadata=metadata, dialect=dialect)\n    _execute(sql, dbo, commit=commit)", "response": "Create a new table in a DB - API 2. 0 database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drop_table(dbo, tablename, schema=None, commit=True):\n\n    # sanitise table name\n    tablename = _quote(tablename)\n    if schema is not None:\n        tablename = _quote(schema) + '.' + tablename\n\n    sql = u'DROP TABLE %s' % tablename\n    _execute(sql, dbo, commit)", "response": "Drop a database table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide access to an HDF5 table. E.g.:: >>> import petl as etl >>> import tables >>> # set up a new hdf5 table to demonstrate with ... h5file = tables.open_file('example.h5', mode='w', ... title='Example file') >>> h5file.create_group('/', 'testgroup', 'Test Group') /testgroup (Group) 'Test Group' children := [] >>> class FooBar(tables.IsDescription): ... foo = tables.Int32Col(pos=0) ... bar = tables.StringCol(6, pos=2) ... >>> h5table = h5file.create_table('/testgroup', 'testtable', FooBar, ... 'Test Table') >>> # load some data into the table ... table1 = (('foo', 'bar'), ... (1, b'asdfgh'), ... (2, b'qwerty'), ... (3, b'zxcvbn')) >>> for row in table1[1:]: ... for i, f in enumerate(table1[0]): ... h5table.row[f] = row[i] ... h5table.row.append() ... >>> h5file.flush() >>> h5file.close() >>> # ... # now demonstrate use of fromhdf5 ... table1 = etl.fromhdf5('example.h5', '/testgroup', 'testtable') >>> table1 +-----+-----------+ | foo | bar | +=====+===========+ | 1 | b'asdfgh' | +-----+-----------+ | 2 | b'qwerty' | +-----+-----------+ | 3 | b'zxcvbn' | +-----+-----------+ >>> # alternatively just specify path to table node ... table1 = etl.fromhdf5('example.h5', '/testgroup/testtable') >>> # ...or use an existing tables.File object ... h5file = tables.open_file('example.h5') >>> table1 = etl.fromhdf5(h5file, '/testgroup/testtable') >>> # ...or use an existing tables.Table object ... h5tbl = h5file.get_node('/testgroup/testtable') >>> table1 = etl.fromhdf5(h5tbl) >>> # use a condition to filter data ... table2 = etl.fromhdf5(h5tbl, condition='foo < 3') >>> table2 +-----+-----------+ | foo | bar | +=====+===========+ | 1 | b'asdfgh' | +-----+-----------+ | 2 | b'qwerty' | +-----+-----------+ >>> h5file.close()", "response": "def fromhdf5(source, where=None, name=None, condition=None,\n             condvars=None, start=None, stop=None, step=None):\n    \"\"\"\n    Provides access to an HDF5 table. E.g.::\n\n        >>> import petl as etl\n        >>> import tables\n        >>> # set up a new hdf5 table to demonstrate with\n        ... h5file = tables.open_file('example.h5', mode='w',\n        ...                           title='Example file')\n        >>> h5file.create_group('/', 'testgroup', 'Test Group')\n        /testgroup (Group) 'Test Group'\n          children := []\n        >>> class FooBar(tables.IsDescription):\n        ...     foo = tables.Int32Col(pos=0)\n        ...     bar = tables.StringCol(6, pos=2)\n        ...\n        >>> h5table = h5file.create_table('/testgroup', 'testtable', FooBar,\n        ...                               'Test Table')\n        >>> # load some data into the table\n        ... table1 = (('foo', 'bar'),\n        ...           (1, b'asdfgh'),\n        ...           (2, b'qwerty'),\n        ...           (3, b'zxcvbn'))\n        >>> for row in table1[1:]:\n        ...     for i, f in enumerate(table1[0]):\n        ...         h5table.row[f] = row[i]\n        ...     h5table.row.append()\n        ...\n        >>> h5file.flush()\n        >>> h5file.close()\n        >>> #\n        ... # now demonstrate use of fromhdf5\n        ... table1 = etl.fromhdf5('example.h5', '/testgroup', 'testtable')\n        >>> table1\n        +-----+-----------+\n        | foo | bar       |\n        +=====+===========+\n        |   1 | b'asdfgh' |\n        +-----+-----------+\n        |   2 | b'qwerty' |\n        +-----+-----------+\n        |   3 | b'zxcvbn' |\n        +-----+-----------+\n\n        >>> # alternatively just specify path to table node\n        ... table1 = etl.fromhdf5('example.h5', '/testgroup/testtable')\n        >>> # ...or use an existing tables.File object\n        ... h5file = tables.open_file('example.h5')\n        >>> table1 = etl.fromhdf5(h5file, '/testgroup/testtable')\n        >>> # ...or use an existing tables.Table object\n        ... h5tbl = h5file.get_node('/testgroup/testtable')\n        >>> table1 = etl.fromhdf5(h5tbl)\n        >>> # use a condition to filter data\n        ... table2 = etl.fromhdf5(h5tbl, condition='foo < 3')\n        >>> table2\n        +-----+-----------+\n        | foo | bar       |\n        +=====+===========+\n        |   1 | b'asdfgh' |\n        +-----+-----------+\n        |   2 | b'qwerty' |\n        +-----+-----------+\n\n        >>> h5file.close()\n\n    \"\"\"\n\n    return HDF5View(source, where=where, name=name,\n                    condition=condition, condvars=condvars,\n                    start=start, stop=stop, step=step)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromhdf5sorted(source, where=None, name=None, sortby=None, checkCSI=False,\n                   start=None, stop=None, step=None):\n    \"\"\"\n    Provides access to an HDF5 table, sorted by an indexed column, e.g.::\n\n        >>> import petl as etl\n        >>> import tables\n        >>> # set up a new hdf5 table to demonstrate with\n        ... h5file = tables.open_file('example.h5', mode='w', title='Test file')\n        >>> h5file.create_group('/', 'testgroup', 'Test Group')\n        /testgroup (Group) 'Test Group'\n          children := []\n        >>> class FooBar(tables.IsDescription):\n        ...     foo = tables.Int32Col(pos=0)\n        ...     bar = tables.StringCol(6, pos=2)\n        ...\n        >>> h5table = h5file.create_table('/testgroup', 'testtable', FooBar, 'Test Table')\n        >>> # load some data into the table\n        ... table1 = (('foo', 'bar'),\n        ...           (3, b'asdfgh'),\n        ...           (2, b'qwerty'),\n        ...           (1, b'zxcvbn'))\n        >>> for row in table1[1:]:\n        ...     for i, f in enumerate(table1[0]):\n        ...         h5table.row[f] = row[i]\n        ...     h5table.row.append()\n        ...\n        >>> h5table.cols.foo.create_csindex()  # CS index is required\n        0\n        >>> h5file.flush()\n        >>> h5file.close()\n        >>> #\n        ... # access the data, sorted by the indexed column\n        ... table2 = etl.fromhdf5sorted('example.h5', '/testgroup', 'testtable',\n        ...                             sortby='foo')\n        >>> table2\n        +-----+-----------+\n        | foo | bar       |\n        +=====+===========+\n        |   1 | b'zxcvbn' |\n        +-----+-----------+\n        |   2 | b'qwerty' |\n        +-----+-----------+\n        |   3 | b'asdfgh' |\n        +-----+-----------+\n\n    \"\"\"\n\n    assert sortby is not None, 'no column specified to sort by'\n    return HDF5SortedView(source, where=where, name=name,\n                          sortby=sortby, checkCSI=checkCSI,\n                          start=start, stop=stop, step=step)", "response": "A simple implementation of the fromhdf5sorted function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tohdf5(table, source, where=None, name=None, create=False, drop=False,\n           description=None, title='', filters=None, expectedrows=10000,\n           chunkshape=None, byteorder=None, createparents=False,\n           sample=1000):\n    \"\"\"\n    Write to an HDF5 table. If `create` is `False`, assumes the table\n    already exists, and attempts to truncate it before loading. If `create`\n    is `True`, a new table will be created, and if `drop` is True,\n    any existing table will be dropped first. If `description` is `None`,\n    the description will be guessed. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = (('foo', 'bar'),\n        ...           (1, b'asdfgh'),\n        ...           (2, b'qwerty'),\n        ...           (3, b'zxcvbn'))\n        >>> etl.tohdf5(table1, 'example.h5', '/testgroup', 'testtable',\n        ...            drop=True, create=True, createparents=True)\n        >>> etl.fromhdf5('example.h5', '/testgroup', 'testtable')\n        +-----+-----------+\n        | foo | bar       |\n        +=====+===========+\n        |   1 | b'asdfgh' |\n        +-----+-----------+\n        |   2 | b'qwerty' |\n        +-----+-----------+\n        |   3 | b'zxcvbn' |\n        +-----+-----------+\n\n    \"\"\"\n\n    import tables\n    it = iter(table)\n\n    if create:\n        with _get_hdf5_file(source, mode='a') as h5file:\n\n            if drop:\n                try:\n                    h5file.get_node(where, name)\n                except tables.NoSuchNodeError:\n                    pass\n                else:\n                    h5file.remove_node(where, name)\n\n            # determine datatype\n            if description is None:\n                peek, it = iterpeek(it, sample)\n                # use a numpy dtype\n                description = infer_dtype(peek)\n\n            # create the table\n            h5file.create_table(where, name, description,\n                                title=title,\n                                filters=filters,\n                                expectedrows=expectedrows,\n                                chunkshape=chunkshape,\n                                byteorder=byteorder,\n                                createparents=createparents)\n\n    with _get_hdf5_table(source, where, name, mode='a') as h5table:\n\n        # truncate the existing table\n        h5table.truncate(0)\n\n        # load the data\n        _insert(it, h5table)", "response": "Write to an HDF5 table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending an HDF5 table to the end of the table.", "response": "def appendhdf5(table, source, where=None, name=None):\n    \"\"\"\n    As :func:`petl.io.hdf5.tohdf5` but don't truncate the target table before\n    loading.\n\n    \"\"\"\n\n    with _get_hdf5_table(source, where, name, mode='a') as h5table:\n\n        # load the data\n        _insert(table, h5table)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef todataframe(table, index=None, exclude=None, columns=None,\n                coerce_float=False, nrows=None):\n    \"\"\"\n    Load data from the given `table` into a\n    `pandas <http://pandas.pydata.org/>`_ DataFrame. E.g.::\n\n        >>> import petl as etl\n        >>> table = [('foo', 'bar', 'baz'),\n        ...          ('apples', 1, 2.5),\n        ...          ('oranges', 3, 4.4),\n        ...          ('pears', 7, .1)]\n        >>> df = etl.todataframe(table)\n        >>> df\n               foo  bar  baz\n        0   apples    1  2.5\n        1  oranges    3  4.4\n        2    pears    7  0.1\n\n    \"\"\"\n    import pandas as pd\n    l = list(table)\n    data = l[1:]\n    if columns is None:\n        columns = l[0]\n    return pd.DataFrame.from_records(data, index=index, exclude=exclude,\n                                     columns=columns, coerce_float=coerce_float,\n                                     nrows=nrows)", "response": "Load data from a given table into a pandas. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds minimum and maximum values under the given field. E. g.", "response": "def limits(table, field):\n    \"\"\"\n    Find minimum and maximum values under the given field. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar'], ['a', 1], ['b', 2], ['b', 3]]\n        >>> minv, maxv = etl.limits(table, 'bar')\n        >>> minv\n        1\n        >>> maxv\n        3\n\n    The `field` argument can be a field name or index (starting from zero).\n\n    \"\"\"\n\n    vals = iter(values(table, field))\n    try:\n        minv = maxv = next(vals)\n    except StopIteration:\n        return None, None\n    else:\n        for v in vals:\n            if v < minv:\n                minv = v\n            if v > maxv:\n                maxv = v\n        return minv, maxv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate basic descriptive statistics on a given field.", "response": "def stats(table, field):\n    \"\"\"\n    Calculate basic descriptive statistics on a given field. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar', 'baz'],\n        ...          ['A', 1, 2],\n        ...          ['B', '2', '3.4'],\n        ...          [u'B', u'3', u'7.8', True],\n        ...          ['D', 'xyz', 9.0],\n        ...          ['E', None]]\n        >>> etl.stats(table, 'bar')\n        stats(count=3, errors=2, sum=6.0, min=1.0, max=3.0, mean=2.0, pvariance=0.6666666666666666, pstdev=0.816496580927726)\n\n    The `field` argument can be a field name or index (starting from zero).\n\n    \"\"\"\n\n    _min = None\n    _max = None\n    _sum = 0\n    _mean = 0\n    _var = 0\n    _count = 0\n    _errors = 0\n    for v in values(table, field):\n        try:\n            v = float(v)\n        except (ValueError, TypeError):\n            _errors += 1\n        else:\n            _count += 1\n            if _min is None or v < _min:\n                _min = v\n            if _max is None or v > _max:\n                _max = v\n            _sum += v\n            _mean, _var = onlinestats(v, _count, mean=_mean, variance=_var)\n    _std = _var**.5\n    return _stats(_count, _errors, _sum, _min, _max, _mean, _var, _std)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncount the number of occurrences of value under the given field.", "response": "def valuecount(table, field, value, missing=None):\n    \"\"\"\n    Count the number of occurrences of `value` under the given field. Returns\n    the absolute count and relative frequency as a pair. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar'],\n        ...          ['a', 1],\n        ...          ['b', 2],\n        ...          ['b', 7]]\n        >>> etl.valuecount(table, 'foo', 'b')\n        (2, 0.6666666666666666)\n\n    The `field` argument can be a single field name or index (starting from\n    zero) or a tuple of field names and/or indexes.\n\n    \"\"\"\n\n    total = 0\n    vs = 0\n    for v in values(table, field, missing=missing):\n        total += 1\n        if v == value:\n            vs += 1\n    return vs, float(vs)/total"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef valuecounter(table, *field, **kwargs):\n\n    missing = kwargs.get('missing', None)\n    counter = Counter()\n    for v in values(table, field, missing=missing):\n        try:\n            counter[v] += 1\n        except IndexError:\n            pass  # short row\n    return counter", "response": "Return a dict mapping values to counts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the given table field and return a Counter object that represents the number of times parsed.", "response": "def parsecounter(table, field, parsers=(('int', int), ('float', float))):\n    \"\"\"\n    Count the number of `str` or `unicode` values under the given fields that\n    can be parsed as ints, floats or via custom parser functions. Return a\n    pair of `Counter` objects, the first mapping parser names to the number of\n    strings successfully parsed, the second mapping parser names to the\n    number of errors. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar', 'baz'],\n        ...          ['A', 'aaa', 2],\n        ...          ['B', u'2', '3.4'],\n        ...          [u'B', u'3', u'7.8', True],\n        ...          ['D', '3.7', 9.0],\n        ...          ['E', 42]]\n        >>> counter, errors = etl.parsecounter(table, 'bar')\n        >>> counter\n        Counter({'float': 3, 'int': 2})\n        >>> errors\n        Counter({'int': 2, 'float': 1})\n\n    The `field` argument can be a field name or index (starting from zero).\n\n    \"\"\"\n\n    if isinstance(parsers, (list, tuple)):\n        parsers = dict(parsers)\n    counter, errors = Counter(), Counter()\n    # need to initialise\n    for n in parsers.keys():\n        counter[n] = 0\n        errors[n] = 0\n    for v in values(table, field):\n        if isinstance(v, string_types):\n            for name, parser in parsers.items():\n                try:\n                    parser(v)\n                except:\n                    errors[name] += 1\n                else:\n                    counter[name] += 1\n    return counter, errors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parsecounts(table, field, parsers=(('int', int), ('float', float))):\n\n    return ParseCountsView(table, field, parsers=parsers)", "response": "Return a table mapping parser names to the number of values successfully parsed and the number of errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef typecounter(table, field):\n\n    counter = Counter()\n    for v in values(table, field):\n        try:\n            counter[v.__class__.__name__] += 1\n        except IndexError:\n            pass  # ignore short rows\n    return counter", "response": "Return a Counter object that counts the number of values found for each Python type in the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stringpatterncounter(table, field):\n\n    trans = maketrans(\n        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789',\n        'AAAAAAAAAAAAAAAAAAAAAAAAAAaaaaaaaaaaaaaaaaaaaaaaaaaa9999999999'\n    )\n    counter = Counter()\n    for v in values(table, field):\n        p = str(v).translate(trans)\n        counter[p] += 1\n    return counter", "response": "Returns a Counter mapping patterns to counts for each profile string pattern in the given field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a table of patterns counts and frequencies for the given field.", "response": "def stringpatterns(table, field):\n    \"\"\"\n    Profile string patterns in the given field, returning a table of patterns,\n    counts and frequencies. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['foo', 'bar'],\n        ...          ['Mr. Foo', '123-1254'],\n        ...          ['Mrs. Bar', '234-1123'],\n        ...          ['Mr. Spo', '123-1254'],\n        ...          [u'Mr. Baz', u'321 1434'],\n        ...          [u'Mrs. Baz', u'321 1434'],\n        ...          ['Mr. Quux', '123-1254-XX']]\n        >>> etl.stringpatterns(table, 'foo')\n        +------------+-------+---------------------+\n        | pattern    | count | frequency           |\n        +============+=======+=====================+\n        | 'Aa. Aaa'  |     3 |                 0.5 |\n        +------------+-------+---------------------+\n        | 'Aaa. Aaa' |     2 |  0.3333333333333333 |\n        +------------+-------+---------------------+\n        | 'Aa. Aaaa' |     1 | 0.16666666666666666 |\n        +------------+-------+---------------------+\n\n        >>> etl.stringpatterns(table, 'bar')\n        +---------------+-------+---------------------+\n        | pattern       | count | frequency           |\n        +===============+=======+=====================+\n        | '999-9999'    |     3 |                 0.5 |\n        +---------------+-------+---------------------+\n        | '999 9999'    |     2 |  0.3333333333333333 |\n        +---------------+-------+---------------------+\n        | '999-9999-AA' |     1 | 0.16666666666666666 |\n        +---------------+-------+---------------------+\n\n    \"\"\"\n\n    counter = stringpatterncounter(table, field)\n    output = [('pattern', 'count', 'frequency')]\n    counter = counter.most_common()\n    total = sum(c[1] for c in counter)\n    cnts = [(c[0], c[1], float(c[1])/total) for c in counter]\n    output.extend(cnts)\n    return wrap(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rowlengths(table):\n\n    counter = Counter()\n    for row in data(table):\n        counter[len(row)] += 1\n    output = [('length', 'count')]\n    output.extend(counter.most_common())\n    return wrap(output)", "response": "Report on row lengths found in the table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a table that writes rows to a Unicode HTML file as they are iterated over.", "response": "def teehtml(table, source=None, encoding=None, errors='strict', caption=None,\n            vrepr=text_type, lineterminator='\\n', index_header=False,\n            tr_style=None, td_styles=None, truncate=None):\n    \"\"\"\n    Return a table that writes rows to a Unicode HTML file as they are\n    iterated over.\n\n    \"\"\"\n\n    source = write_source_from_arg(source)\n    return TeeHTMLView(table, source=source, encoding=encoding, errors=errors,\n                       caption=caption, vrepr=vrepr,\n                       lineterminator=lineterminator, index_header=index_header,\n                       tr_style=tr_style, td_styles=td_styles,\n                       truncate=truncate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef randomtable(numflds=5, numrows=100, wait=0, seed=None):\n\n    return RandomTable(numflds, numrows, wait=wait, seed=seed)", "response": "Return a random table with random numerical data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dummytable(numrows=100,\n               fields=(('foo', partial(random.randint, 0, 100)),\n                       ('bar', partial(random.choice, ('apples', 'pears',\n                                                       'bananas', 'oranges'))),\n                       ('baz', random.random)),\n               wait=0, seed=None):\n    \"\"\"\n    Construct a table with dummy data. Use `numrows` to specify the number of\n    rows. Set `wait` to a float greater than zero to simulate a delay on each\n    row generation (number of seconds per row). E.g.::\n\n        >>> import petl as etl\n        >>> table1 = etl.dummytable(100, seed=42)\n        >>> table1\n        +-----+----------+----------------------+\n        | foo | bar      | baz                  |\n        +=====+==========+======================+\n        |  81 | 'apples' | 0.025010755222666936 |\n        +-----+----------+----------------------+\n        |  35 | 'pears'  |  0.22321073814882275 |\n        +-----+----------+----------------------+\n        |  94 | 'apples' |   0.6766994874229113 |\n        +-----+----------+----------------------+\n        |  69 | 'apples' |   0.5904925124490397 |\n        +-----+----------+----------------------+\n        |   4 | 'apples' |  0.09369523986159245 |\n        +-----+----------+----------------------+\n        ...\n\n        >>> # customise fields\n        ... import random\n        >>> from functools import partial\n        >>> fields = [('foo', random.random),\n        ...           ('bar', partial(random.randint, 0, 500)),\n        ...           ('baz', partial(random.choice,\n        ...                           ['chocolate', 'strawberry', 'vanilla']))]\n        >>> table2 = etl.dummytable(100, fields=fields, seed=42)\n        >>> table2\n        +---------------------+-----+-------------+\n        | foo                 | bar | baz         |\n        +=====================+=====+=============+\n        |  0.6394267984578837 |  12 | 'vanilla'   |\n        +---------------------+-----+-------------+\n        | 0.27502931836911926 | 114 | 'chocolate' |\n        +---------------------+-----+-------------+\n        |  0.7364712141640124 | 346 | 'vanilla'   |\n        +---------------------+-----+-------------+\n        |  0.8921795677048454 |  44 | 'vanilla'   |\n        +---------------------+-----+-------------+\n        |  0.4219218196852704 |  15 | 'chocolate' |\n        +---------------------+-----+-------------+\n        ...\n\n    Data generation functions can be specified via the `fields` keyword\n    argument.\n\n    Note that the data are generated on the fly and are not stored in memory,\n    so this function can be used to simulate very large tables.\n\n    \"\"\"\n\n    return DummyTable(numrows=numrows, fields=fields, wait=wait, seed=seed)", "response": "Construct a dummy table with dummy data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tupletree(table, start='start', stop='stop', value=None):\n\n    import intervaltree\n    tree = intervaltree.IntervalTree()\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    assert start in flds, 'start field not recognised'\n    assert stop in flds, 'stop field not recognised'\n    getstart = itemgetter(flds.index(start))\n    getstop = itemgetter(flds.index(stop))\n    if value is None:\n        getvalue = tuple\n    else:\n        valueindices = asindices(hdr, value)\n        assert len(valueindices) > 0, 'invalid value field specification'\n        getvalue = itemgetter(*valueindices)\n    for row in it:\n        tree.addi(getstart(row), getstop(row), getvalue(row))\n    return tree", "response": "Construct an interval tree for the given table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef facettupletrees(table, key, start='start', stop='stop', value=None):\n\n    import intervaltree\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    assert start in flds, 'start field not recognised'\n    assert stop in flds, 'stop field not recognised'\n    getstart = itemgetter(flds.index(start))\n    getstop = itemgetter(flds.index(stop))\n    if value is None:\n        getvalue = tuple\n    else:\n        valueindices = asindices(hdr, value)\n        assert len(valueindices) > 0, 'invalid value field specification'\n        getvalue = itemgetter(*valueindices)\n    keyindices = asindices(hdr, key)\n    assert len(keyindices) > 0, 'invalid key'\n    getkey = itemgetter(*keyindices)\n\n    trees = dict()\n    for row in it:\n        k = getkey(row)\n        if k not in trees:\n            trees[k] = intervaltree.IntervalTree()\n        trees[k].addi(getstart(row), getstop(row), getvalue(row))\n    return trees", "response": "Construct faceted interval trees for the given table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recordtree(table, start='start', stop='stop'):\n\n    import intervaltree\n    getstart = attrgetter(start)\n    getstop = attrgetter(stop)\n    tree = intervaltree.IntervalTree()\n    for rec in records(table):\n        tree.addi(getstart(rec), getstop(rec), rec)\n    return tree", "response": "Construct an interval tree for the given table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs faceted interval trees for the given table where each node in is a record.", "response": "def facetrecordtrees(table, key, start='start', stop='stop'):\n    \"\"\"\n    Construct faceted interval trees for the given table, where each node in \n    the tree is a record.\n\n    \"\"\"\n\n    import intervaltree\n    getstart = attrgetter(start)\n    getstop = attrgetter(stop)\n    getkey = attrgetter(key)\n    trees = dict()\n    for rec in records(table):\n        k = getkey(rec)\n        if k not in trees:\n            trees[k] = intervaltree.IntervalTree()\n        trees[k].addi(getstart(rec), getstop(rec), rec)\n    return trees"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intervallookup(table, start='start', stop='stop', value=None,\n                   include_stop=False):\n    \"\"\"\n    Construct an interval lookup for the given table. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['start', 'stop', 'value'],\n        ...          [1, 4, 'foo'],\n        ...          [3, 7, 'bar'],\n        ...          [4, 9, 'baz']]\n        >>> lkp = etl.intervallookup(table, 'start', 'stop')\n        >>> lkp.search(0, 1)\n        []\n        >>> lkp.search(1, 2)\n        [(1, 4, 'foo')]\n        >>> lkp.search(2, 4)\n        [(1, 4, 'foo'), (3, 7, 'bar')]\n        >>> lkp.search(2, 5)\n        [(1, 4, 'foo'), (3, 7, 'bar'), (4, 9, 'baz')]\n        >>> lkp.search(9, 14)\n        []\n        >>> lkp.search(19, 140)\n        []\n        >>> lkp.search(0)\n        []\n        >>> lkp.search(1)\n        [(1, 4, 'foo')]\n        >>> lkp.search(2)\n        [(1, 4, 'foo')]\n        >>> lkp.search(4)\n        [(3, 7, 'bar'), (4, 9, 'baz')]\n        >>> lkp.search(5)\n        [(3, 7, 'bar'), (4, 9, 'baz')]\n\n    Note start coordinates are included and stop coordinates are excluded\n    from the interval. Use the `include_stop` keyword argument to include the\n    upper bound of the interval when finding overlaps.\n\n    Some examples using the `include_stop` and `value` keyword arguments::\n    \n        >>> import petl as etl\n        >>> table = [['start', 'stop', 'value'],\n        ...          [1, 4, 'foo'],\n        ...          [3, 7, 'bar'],\n        ...          [4, 9, 'baz']]\n        >>> lkp = etl.intervallookup(table, 'start', 'stop', include_stop=True,\n        ...                          value='value')\n        >>> lkp.search(0, 1)\n        ['foo']\n        >>> lkp.search(1, 2)\n        ['foo']\n        >>> lkp.search(2, 4)\n        ['foo', 'bar', 'baz']\n        >>> lkp.search(2, 5)\n        ['foo', 'bar', 'baz']\n        >>> lkp.search(9, 14)\n        ['baz']\n        >>> lkp.search(19, 140)\n        []\n        >>> lkp.search(0)\n        []\n        >>> lkp.search(1)\n        ['foo']\n        >>> lkp.search(2)\n        ['foo']\n        >>> lkp.search(4)\n        ['foo', 'bar', 'baz']\n        >>> lkp.search(5)\n        ['bar', 'baz']\n\n    \"\"\"\n\n    tree = tupletree(table, start=start, stop=stop, value=value)\n    return IntervalTreeLookup(tree, include_stop=include_stop)", "response": "Construct an interval lookup for the given table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intervallookupone(table, start='start', stop='stop', value=None,\n                      include_stop=False, strict=True):\n    \"\"\"\n    Construct an interval lookup for the given table, returning at most one\n    result for each query. E.g.::\n\n        >>> import petl as etl\n        >>> table = [['start', 'stop', 'value'],\n        ...          [1, 4, 'foo'],\n        ...          [3, 7, 'bar'],\n        ...          [4, 9, 'baz']]\n        >>> lkp = etl.intervallookupone(table, 'start', 'stop', strict=False)\n        >>> lkp.search(0, 1)\n        >>> lkp.search(1, 2)\n        (1, 4, 'foo')\n        >>> lkp.search(2, 4)\n        (1, 4, 'foo')\n        >>> lkp.search(2, 5)\n        (1, 4, 'foo')\n        >>> lkp.search(9, 14)\n        >>> lkp.search(19, 140)\n        >>> lkp.search(0)\n        >>> lkp.search(1)\n        (1, 4, 'foo')\n        >>> lkp.search(2)\n        (1, 4, 'foo')\n        >>> lkp.search(4)\n        (3, 7, 'bar')\n        >>> lkp.search(5)\n        (3, 7, 'bar')\n\n    If ``strict=True``, queries returning more than one result will\n    raise a `DuplicateKeyError`. If ``strict=False`` and there is\n    more than one result, the first result is returned.\n\n    Note start coordinates are included and stop coordinates are excluded\n    from the interval. Use the `include_stop` keyword argument to include the\n    upper bound of the interval when finding overlaps.\n\n    \"\"\"\n\n    tree = tupletree(table, start=start, stop=stop, value=value)\n    return IntervalTreeLookupOne(tree, strict=strict, include_stop=include_stop)", "response": "Construct an interval lookup for the given table returning at most one result for each query."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intervalrecordlookupone(table, start='start', stop='stop',\n                            include_stop=False, strict=True):\n    \"\"\"\n    As :func:`petl.transform.intervals.intervallookupone` but return records\n    instead of tuples.\n\n    \"\"\"\n\n    tree = recordtree(table, start=start, stop=stop)\n    return IntervalTreeLookupOne(tree, include_stop=include_stop, strict=strict)", "response": "As petl. transform. intervals. intervallookupone but return records\n    instead of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef facetintervallookupone(table, key, start='start', stop='stop',\n                           value=None, include_stop=False, strict=True):\n    \"\"\"\n    Construct a faceted interval lookup for the given table, returning at most\n    one result for each query.\n    \n    If ``strict=True``, queries returning more than one result will\n    raise a `DuplicateKeyError`. If ``strict=False`` and there is\n    more than one result, the first result is returned.\n\n    \"\"\"\n    \n    trees = facettupletrees(table, key, start=start, stop=stop, value=value)\n    out = dict()\n    for k in trees:\n        out[k] = IntervalTreeLookupOne(trees[k], include_stop=include_stop,\n                                       strict=strict)\n    return out", "response": "Construct a faceted interval lookup for the given table returning at most\n    one result for each query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\njoining two tables by overlapping intervals.", "response": "def intervaljoin(left, right, lstart='start', lstop='stop', rstart='start',\n                 rstop='stop', lkey=None, rkey=None, include_stop=False,\n                 lprefix=None, rprefix=None):\n    \"\"\"\n    Join two tables by overlapping intervals. E.g.::\n\n        >>> import petl as etl\n        >>> left = [['begin', 'end', 'quux'],\n        ...         [1, 2, 'a'],\n        ...         [2, 4, 'b'],\n        ...         [2, 5, 'c'],\n        ...         [9, 14, 'd'],\n        ...         [1, 1, 'e'],\n        ...         [10, 10, 'f']]\n        >>> right = [['start', 'stop', 'value'],\n        ...          [1, 4, 'foo'],\n        ...          [3, 7, 'bar'],\n        ...          [4, 9, 'baz']]\n        >>> table1 = etl.intervaljoin(left, right,\n        ...                           lstart='begin', lstop='end',\n        ...                           rstart='start', rstop='stop')\n        >>> table1.lookall()\n        +-------+-----+------+-------+------+-------+\n        | begin | end | quux | start | stop | value |\n        +=======+=====+======+=======+======+=======+\n        |     1 |   2 | 'a'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   4 | 'b'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   4 | 'b'  |     3 |    7 | 'bar' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     3 |    7 | 'bar' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     4 |    9 | 'baz' |\n        +-------+-----+------+-------+------+-------+\n\n        >>> # include stop coordinate in intervals\n        ... table2 = etl.intervaljoin(left, right,\n        ...                           lstart='begin', lstop='end',\n        ...                           rstart='start', rstop='stop',\n        ...                           include_stop=True)\n        >>> table2.lookall()\n        +-------+-----+------+-------+------+-------+\n        | begin | end | quux | start | stop | value |\n        +=======+=====+======+=======+======+=======+\n        |     1 |   2 | 'a'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   4 | 'b'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   4 | 'b'  |     3 |    7 | 'bar' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   4 | 'b'  |     4 |    9 | 'baz' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     3 |    7 | 'bar' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     4 |    9 | 'baz' |\n        +-------+-----+------+-------+------+-------+\n        |     9 |  14 | 'd'  |     4 |    9 | 'baz' |\n        +-------+-----+------+-------+------+-------+\n        |     1 |   1 | 'e'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n\n    Note start coordinates are included and stop coordinates are excluded\n    from the interval. Use the `include_stop` keyword argument to include the\n    upper bound of the interval when finding overlaps.\n\n    An additional key comparison can be made, e.g.::\n    \n        >>> import petl as etl\n        >>> left = (('fruit', 'begin', 'end'),\n        ...         ('apple', 1, 2),\n        ...         ('apple', 2, 4),\n        ...         ('apple', 2, 5),\n        ...         ('orange', 2, 5),\n        ...         ('orange', 9, 14),\n        ...         ('orange', 19, 140),\n        ...         ('apple', 1, 1))\n        >>> right = (('type', 'start', 'stop', 'value'),\n        ...          ('apple', 1, 4, 'foo'),\n        ...          ('apple', 3, 7, 'bar'),\n        ...          ('orange', 4, 9, 'baz'))\n        >>> table3 = etl.intervaljoin(left, right,\n        ...                           lstart='begin', lstop='end', lkey='fruit',\n        ...                           rstart='start', rstop='stop', rkey='type')\n        >>> table3.lookall()\n        +----------+-------+-----+----------+-------+------+-------+\n        | fruit    | begin | end | type     | start | stop | value |\n        +==========+=======+=====+==========+=======+======+=======+\n        | 'apple'  |     1 |   2 | 'apple'  |     1 |    4 | 'foo' |\n        +----------+-------+-----+----------+-------+------+-------+\n        | 'apple'  |     2 |   4 | 'apple'  |     1 |    4 | 'foo' |\n        +----------+-------+-----+----------+-------+------+-------+\n        | 'apple'  |     2 |   4 | 'apple'  |     3 |    7 | 'bar' |\n        +----------+-------+-----+----------+-------+------+-------+\n        | 'apple'  |     2 |   5 | 'apple'  |     1 |    4 | 'foo' |\n        +----------+-------+-----+----------+-------+------+-------+\n        | 'apple'  |     2 |   5 | 'apple'  |     3 |    7 | 'bar' |\n        +----------+-------+-----+----------+-------+------+-------+\n        | 'orange' |     2 |   5 | 'orange' |     4 |    9 | 'baz' |\n        +----------+-------+-----+----------+-------+------+-------+\n\n    \"\"\"\n    \n    assert (lkey is None) == (rkey is None), \\\n        'facet key field must be provided for both or neither table'\n    return IntervalJoinView(left, right, lstart=lstart, lstop=lstop,\n                            rstart=rstart, rstop=rstop, lkey=lkey,\n                            rkey=rkey, include_stop=include_stop,\n                            lprefix=lprefix, rprefix=rprefix)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlikes :func:`petl.transform.intervals.intervaljoin` but rows from the left table without a match in the right table are also included. E.g.:: >>> import petl as etl >>> left = [['begin', 'end', 'quux'], ... [1, 2, 'a'], ... [2, 4, 'b'], ... [2, 5, 'c'], ... [9, 14, 'd'], ... [1, 1, 'e'], ... [10, 10, 'f']] >>> right = [['start', 'stop', 'value'], ... [1, 4, 'foo'], ... [3, 7, 'bar'], ... [4, 9, 'baz']] >>> table1 = etl.intervalleftjoin(left, right, ... lstart='begin', lstop='end', ... rstart='start', rstop='stop') >>> table1.lookall() +-------+-----+------+-------+------+-------+ | begin | end | quux | start | stop | value | +=======+=====+======+=======+======+=======+ | 1 | 2 | 'a' | 1 | 4 | 'foo' | +-------+-----+------+-------+------+-------+ | 2 | 4 | 'b' | 1 | 4 | 'foo' | +-------+-----+------+-------+------+-------+ | 2 | 4 | 'b' | 3 | 7 | 'bar' | +-------+-----+------+-------+------+-------+ | 2 | 5 | 'c' | 1 | 4 | 'foo' | +-------+-----+------+-------+------+-------+ | 2 | 5 | 'c' | 3 | 7 | 'bar' | +-------+-----+------+-------+------+-------+ | 2 | 5 | 'c' | 4 | 9 | 'baz' | +-------+-----+------+-------+------+-------+ | 9 | 14 | 'd' | None | None | None | +-------+-----+------+-------+------+-------+ | 1 | 1 | 'e' | None | None | None | +-------+-----+------+-------+------+-------+ | 10 | 10 | 'f' | None | None | None | +-------+-----+------+-------+------+-------+ Note start coordinates are included and stop coordinates are excluded from the interval. Use the `include_stop` keyword argument to include the upper bound of the interval when finding overlaps.", "response": "def intervalleftjoin(left, right, lstart='start', lstop='stop', rstart='start',\n                     rstop='stop', lkey=None, rkey=None, include_stop=False,\n                     missing=None, lprefix=None, rprefix=None):\n    \"\"\"\n    Like :func:`petl.transform.intervals.intervaljoin` but rows from the left \n    table without a match in the right table are also included. E.g.::\n\n        >>> import petl as etl\n        >>> left = [['begin', 'end', 'quux'],\n        ...         [1, 2, 'a'],\n        ...         [2, 4, 'b'],\n        ...         [2, 5, 'c'],\n        ...         [9, 14, 'd'],\n        ...         [1, 1, 'e'],\n        ...         [10, 10, 'f']]\n        >>> right = [['start', 'stop', 'value'],\n        ...          [1, 4, 'foo'],\n        ...          [3, 7, 'bar'],\n        ...          [4, 9, 'baz']]\n        >>> table1 = etl.intervalleftjoin(left, right,\n        ...                               lstart='begin', lstop='end',\n        ...                               rstart='start', rstop='stop')\n        >>> table1.lookall()\n        +-------+-----+------+-------+------+-------+\n        | begin | end | quux | start | stop | value |\n        +=======+=====+======+=======+======+=======+\n        |     1 |   2 | 'a'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   4 | 'b'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   4 | 'b'  |     3 |    7 | 'bar' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     1 |    4 | 'foo' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     3 |    7 | 'bar' |\n        +-------+-----+------+-------+------+-------+\n        |     2 |   5 | 'c'  |     4 |    9 | 'baz' |\n        +-------+-----+------+-------+------+-------+\n        |     9 |  14 | 'd'  | None  | None | None  |\n        +-------+-----+------+-------+------+-------+\n        |     1 |   1 | 'e'  | None  | None | None  |\n        +-------+-----+------+-------+------+-------+\n        |    10 |  10 | 'f'  | None  | None | None  |\n        +-------+-----+------+-------+------+-------+\n\n    Note start coordinates are included and stop coordinates are excluded\n    from the interval. Use the `include_stop` keyword argument to include the\n    upper bound of the interval when finding overlaps.\n\n    \"\"\"\n    \n    assert (lkey is None) == (rkey is None), \\\n        'facet key field must be provided for both or neither table'\n    return IntervalLeftJoinView(left, right, lstart=lstart, lstop=lstop,\n                                rstart=rstart, rstop=rstop, lkey=lkey,\n                                rkey=rkey, include_stop=include_stop,\n                                missing=missing, lprefix=lprefix,\n                                rprefix=rprefix)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn rows from the `left` table with no overlapping rows from the `right` table. Note start coordinates are included and stop coordinates are excluded from the interval. Use the `include_stop` keyword argument to include the upper bound of the interval when finding overlaps.", "response": "def intervalantijoin(left, right, lstart='start', lstop='stop', rstart='start',\n                     rstop='stop', lkey=None, rkey=None, include_stop=False,\n                     missing=None):\n    \"\"\"\n    Return rows from the `left` table with no overlapping rows from the `right`\n    table.\n\n    Note start coordinates are included and stop coordinates are excluded\n    from the interval. Use the `include_stop` keyword argument to include the\n    upper bound of the interval when finding overlaps.\n\n    \"\"\"\n\n    assert (lkey is None) == (rkey is None), \\\n        'facet key field must be provided for both or neither table'\n    return IntervalAntiJoinView(left, right, lstart=lstart, lstop=lstop,\n                                rstart=rstart, rstop=rstop, lkey=lkey,\n                                rkey=rkey, include_stop=include_stop,\n                                missing=missing)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef intervalsubtract(left, right, lstart='start', lstop='stop', rstart='start',\n                     rstop='stop', lkey=None, rkey=None, include_stop=False):\n    \"\"\"\n    Subtract intervals in the right hand table from intervals in the left hand \n    table.\n    \n    \"\"\"\n\n    assert (lkey is None) == (rkey is None), \\\n        'facet key field must be provided for both or neither table'\n    return IntervalSubtractView(left, right, lstart=lstart, lstop=lstop,\n                                rstart=rstart, rstop=rstop, lkey=lkey,\n                                rkey=rkey, include_stop=include_stop)", "response": "Subtract intervals in the left hand table from intervals in the right hand table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _collapse(intervals):\n    span = None\n    for start, stop in intervals:\n        if span is None:\n            span = _Interval(start, stop)\n        elif start <= span.stop < stop:\n            span = _Interval(span.start, stop)\n        elif start > span.stop:\n            yield span\n            span = _Interval(start, stop)\n    if span is not None:\n        yield span", "response": "Collapse an iterable of intervals sorted by start coord."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubtracts intervals from a spanning interval.", "response": "def _subtract(start, stop, intervals):\n    \"\"\"\n    Subtract intervals from a spanning interval.\n    \n    \"\"\"\n    remainder_start = start\n    sub_stop = None\n    for sub_start, sub_stop in _collapse(intervals):\n        if remainder_start < sub_start:\n            yield _Interval(remainder_start, sub_start)\n        remainder_start = sub_stop\n    if sub_stop is not None and sub_stop < stop:\n        yield _Interval(sub_stop, stop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransforms a table to mapping fields arbitrarily between input and output.", "response": "def fieldmap(table, mappings=None, failonerror=False, errorvalue=None):\n    \"\"\"\n    Transform a table, mapping fields arbitrarily between input and output.\n    E.g.::\n\n        >>> import petl as etl\n        >>> from collections import OrderedDict\n        >>> table1 = [['id', 'sex', 'age', 'height', 'weight'],\n        ...           [1, 'male', 16, 1.45, 62.0],\n        ...           [2, 'female', 19, 1.34, 55.4],\n        ...           [3, 'female', 17, 1.78, 74.4],\n        ...           [4, 'male', 21, 1.33, 45.2],\n        ...           [5, '-', 25, 1.65, 51.9]]\n        >>> mappings = OrderedDict()\n        >>> # rename a field\n        ... mappings['subject_id'] = 'id'\n        >>> # translate a field\n        ... mappings['gender'] = 'sex', {'male': 'M', 'female': 'F'}\n        >>> # apply a calculation to a field\n        ... mappings['age_months'] = 'age', lambda v: v * 12\n        >>> # apply a calculation to a combination of fields\n        ... mappings['bmi'] = lambda rec: rec['weight'] / rec['height']**2\n        >>> # transform and inspect the output\n        ... table2 = etl.fieldmap(table1, mappings)\n        >>> table2\n        +------------+--------+------------+--------------------+\n        | subject_id | gender | age_months | bmi                |\n        +============+========+============+====================+\n        |          1 | 'M'    |        192 |  29.48870392390012 |\n        +------------+--------+------------+--------------------+\n        |          2 | 'F'    |        228 |   30.8531967030519 |\n        +------------+--------+------------+--------------------+\n        |          3 | 'F'    |        204 | 23.481883600555488 |\n        +------------+--------+------------+--------------------+\n        |          4 | 'M'    |        252 |  25.55260331279326 |\n        +------------+--------+------------+--------------------+\n        |          5 | '-'    |        300 |   19.0633608815427 |\n        +------------+--------+------------+--------------------+\n\n    Note also that the mapping value can be an expression string, which will be\n    converted to a lambda function via :func:`petl.util.base.expr`.\n\n    \"\"\"\n\n    return FieldMapView(table, mappings=mappings, failonerror=failonerror,\n                        errorvalue=errorvalue)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rowmap(table, rowmapper, header, failonerror=False):\n\n    return RowMapView(table, rowmapper, header, failonerror=failonerror)", "response": "Transform a table of data into a new tree of the same size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap each input row to any number of output rows.", "response": "def rowmapmany(table, rowgenerator, header, failonerror=False):\n    \"\"\"\n    Map each input row to any number of output rows via an arbitrary\n    function. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['id', 'sex', 'age', 'height', 'weight'],\n        ...           [1, 'male', 16, 1.45, 62.0],\n        ...           [2, 'female', 19, 1.34, 55.4],\n        ...           [3, '-', 17, 1.78, 74.4],\n        ...           [4, 'male', 21, 1.33]]\n        >>> def rowgenerator(row):\n        ...     transmf = {'male': 'M', 'female': 'F'}\n        ...     yield [row[0], 'gender',\n        ...            transmf[row['sex']] if row['sex'] in transmf else None]\n        ...     yield [row[0], 'age_months', row.age * 12]\n        ...     yield [row[0], 'bmi', row.height / row.weight ** 2]\n        ...\n        >>> table2 = etl.rowmapmany(table1, rowgenerator,\n        ...                         header=['subject_id', 'variable', 'value'])\n        >>> table2.lookall()\n        +------------+--------------+-----------------------+\n        | subject_id | variable     | value                 |\n        +============+==============+=======================+\n        |          1 | 'gender'     | 'M'                   |\n        +------------+--------------+-----------------------+\n        |          1 | 'age_months' |                   192 |\n        +------------+--------------+-----------------------+\n        |          1 | 'bmi'        | 0.0003772112382934443 |\n        +------------+--------------+-----------------------+\n        |          2 | 'gender'     | 'F'                   |\n        +------------+--------------+-----------------------+\n        |          2 | 'age_months' |                   228 |\n        +------------+--------------+-----------------------+\n        |          2 | 'bmi'        | 0.0004366015456998006 |\n        +------------+--------------+-----------------------+\n        |          3 | 'gender'     | None                  |\n        +------------+--------------+-----------------------+\n        |          3 | 'age_months' |                   204 |\n        +------------+--------------+-----------------------+\n        |          3 | 'bmi'        | 0.0003215689675106949 |\n        +------------+--------------+-----------------------+\n        |          4 | 'gender'     | 'M'                   |\n        +------------+--------------+-----------------------+\n        |          4 | 'age_months' |                   252 |\n        +------------+--------------+-----------------------+\n\n    The `rowgenerator` function should accept a single row and yield zero or\n    more rows (lists or tuples).\n\n    See also the :func:`petl.transform.reshape.melt` function.\n\n    \"\"\"\n\n    return RowMapManyView(table, rowgenerator, header, failonerror=failonerror)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rowgroupmap(table, key, mapper, header=None, presorted=False,\n                buffersize=None, tempdir=None, cache=True):\n    \"\"\"\n    Group rows under the given key then apply `mapper` to yield zero or more\n    output rows for each input group of rows.\n\n    \"\"\"\n\n    return RowGroupMapView(table, key, mapper, header=header,\n                           presorted=presorted,\n                           buffersize=buffersize, tempdir=tempdir, cache=cache)", "response": "Group rows under the given key then apply mapper to yield zero or more\n    output rows for each input group of rows."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nunpack a list of data values that are lists or tuples.", "response": "def unpack(table, field, newfields=None, include_original=False, missing=None):\n    \"\"\"\n    Unpack data values that are lists or tuples. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           [1, ['a', 'b']],\n        ...           [2, ['c', 'd']],\n        ...           [3, ['e', 'f']]]\n        >>> table2 = etl.unpack(table1, 'bar', ['baz', 'quux'])\n        >>> table2\n        +-----+-----+------+\n        | foo | baz | quux |\n        +=====+=====+======+\n        |   1 | 'a' | 'b'  |\n        +-----+-----+------+\n        |   2 | 'c' | 'd'  |\n        +-----+-----+------+\n        |   3 | 'e' | 'f'  |\n        +-----+-----+------+\n\n    This function will attempt to unpack exactly the number of values as\n    given by the number of new fields specified. If there are more values\n    than new fields, remaining values will not be unpacked. If there are less\n    values than new fields, `missing` values will be added.\n\n    See also :func:`petl.transform.unpacks.unpackdict`.\n\n    \"\"\"\n\n    return UnpackView(table, field, newfields=newfields,\n                      include_original=include_original, missing=missing)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unpackdict(table, field, keys=None, includeoriginal=False,\n               samplesize=1000, missing=None):\n    \"\"\"\n    Unpack dictionary values into separate fields. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           [1, {'baz': 'a', 'quux': 'b'}],\n        ...           [2, {'baz': 'c', 'quux': 'd'}],\n        ...           [3, {'baz': 'e', 'quux': 'f'}]]\n        >>> table2 = etl.unpackdict(table1, 'bar')\n        >>> table2\n        +-----+-----+------+\n        | foo | baz | quux |\n        +=====+=====+======+\n        |   1 | 'a' | 'b'  |\n        +-----+-----+------+\n        |   2 | 'c' | 'd'  |\n        +-----+-----+------+\n        |   3 | 'e' | 'f'  |\n        +-----+-----+------+\n\n    See also :func:`petl.transform.unpacks.unpack`.\n\n    \"\"\"\n\n    return UnpackDictView(table, field, keys=keys,\n                          includeoriginal=includeoriginal,\n                          samplesize=samplesize, missing=missing)", "response": "Unpacks a dictionary into separate fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting rows meeting a condition.", "response": "def select(table, *args, **kwargs):\n    \"\"\"\n    Select rows meeting a condition. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar', 'baz'],\n        ...           ['a', 4, 9.3],\n        ...           ['a', 2, 88.2],\n        ...           ['b', 1, 23.3],\n        ...           ['c', 8, 42.0],\n        ...           ['d', 7, 100.9],\n        ...           ['c', 2]]\n        >>> # the second positional argument can be a function accepting\n        ... # a row\n        ... table2 = etl.select(table1,\n        ...                     lambda rec: rec.foo == 'a' and rec.baz > 88.1)\n        >>> table2\n        +-----+-----+------+\n        | foo | bar | baz  |\n        +=====+=====+======+\n        | 'a' |   2 | 88.2 |\n        +-----+-----+------+\n\n        >>> # the second positional argument can also be an expression\n        ... # string, which will be converted to a function using petl.expr()\n        ... table3 = etl.select(table1, \"{foo} == 'a' and {baz} > 88.1\")\n        >>> table3\n        +-----+-----+------+\n        | foo | bar | baz  |\n        +=====+=====+======+\n        | 'a' |   2 | 88.2 |\n        +-----+-----+------+\n\n        >>> # the condition can also be applied to a single field\n        ... table4 = etl.select(table1, 'foo', lambda v: v == 'a')\n        >>> table4\n        +-----+-----+------+\n        | foo | bar | baz  |\n        +=====+=====+======+\n        | 'a' |   4 |  9.3 |\n        +-----+-----+------+\n        | 'a' |   2 | 88.2 |\n        +-----+-----+------+\n\n    The complement of the selection can be returned (i.e., the query can be\n    inverted) by providing `complement=True` as a keyword argument.\n\n    \"\"\"\n\n    missing = kwargs.get('missing', None)\n    complement = kwargs.get('complement', False)\n\n    if len(args) == 0:\n        raise ArgumentError('missing positional argument')\n    elif len(args) == 1:\n        where = args[0]\n        if isinstance(where, string_types):\n            where = expr(where)\n        else:\n            assert callable(where), 'second argument must be string or callable'\n        return RowSelectView(table, where, missing=missing,\n                             complement=complement)\n    else:\n        field = args[0]\n        where = args[1]\n        assert callable(where), 'third argument must be callable'\n        return FieldSelectView(table, field, where, complement=complement,\n                               missing=missing)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rowlenselect(table, n, complement=False):\n\n    where = lambda row: len(row) == n\n    return select(table, where, complement=complement)", "response": "Select rows of length n."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef selectop(table, field, value, op, complement=False):\n\n    return select(table, field, lambda v: op(v, value),\n                  complement=complement)", "response": "Select rows where the function op applied to the given field and\n    the given value returns True."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef selecteq(table, field, value, complement=False):\n\n    return selectop(table, field, value, operator.eq, complement=complement)", "response": "Select rows where the given field equals the given value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nselects rows where the given field does not equal the given value.", "response": "def selectne(table, field, value, complement=False):\n    \"\"\"Select rows where the given field does not equal the given value.\"\"\"\n\n    return selectop(table, field, value, operator.ne, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting rows where the given field is less than the given value.", "response": "def selectlt(table, field, value, complement=False):\n    \"\"\"Select rows where the given field is less than the given value.\"\"\"\n\n    value = Comparable(value)\n    return selectop(table, field, value, operator.lt, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting rows where the given field is less than or equal to the given value.", "response": "def selectle(table, field, value, complement=False):\n    \"\"\"Select rows where the given field is less than or equal to the given\n    value.\"\"\"\n\n    value = Comparable(value)\n    return selectop(table, field, value, operator.le, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect rows where the given field is greater than the given value.", "response": "def selectgt(table, field, value, complement=False):\n    \"\"\"Select rows where the given field is greater than the given value.\"\"\"\n\n    value = Comparable(value)\n    return selectop(table, field, value, operator.gt, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef selectge(table, field, value, complement=False):\n\n    value = Comparable(value)\n    return selectop(table, field, value, operator.ge, complement=complement)", "response": "Select rows where the given field is greater than or equal to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects rows where the given field contains the given value.", "response": "def selectcontains(table, field, value, complement=False):\n    \"\"\"Select rows where the given field contains the given value.\"\"\"\n\n    return selectop(table, field, value, operator.contains,\n                    complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef selectin(table, field, value, complement=False):\n\n    return select(table, field, lambda v: v in value,\n                  complement=complement)", "response": "Select rows where the given field is a member of the given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef selectnotin(table, field, value, complement=False):\n\n    return select(table, field, lambda v: v not in value,\n                  complement=complement)", "response": "Select rows where the given field is not a member of the given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect rows where the given field is the given value.", "response": "def selectis(table, field, value, complement=False):\n    \"\"\"Select rows where the given field `is` the given value.\"\"\"\n\n    return selectop(table, field, value, operator.is_, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nselects rows where the given field is not the given value.", "response": "def selectisnot(table, field, value, complement=False):\n    \"\"\"Select rows where the given field `is not` the given value.\"\"\"\n\n    return selectop(table, field, value, operator.is_not, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nselect rows where the given field is an instance of the given type.", "response": "def selectisinstance(table, field, value, complement=False):\n    \"\"\"Select rows where the given field is an instance of the given type.\"\"\"\n\n    return selectop(table, field, value, isinstance, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef selectrangeopenleft(table, field, minv, maxv, complement=False):\n\n    minv = Comparable(minv)\n    maxv = Comparable(maxv)\n    return select(table, field, lambda v: minv <= v < maxv,\n                  complement=complement)", "response": "Select rows where the given field is greater than or equal to minv and less than maxv."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef selecttrue(table, field, complement=False):\n\n    return select(table, field, lambda v: bool(v), complement=complement)", "response": "Select rows where the given field evaluates True."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nselect rows where the given field evaluates False.", "response": "def selectfalse(table, field, complement=False):\n    \"\"\"Select rows where the given field evaluates `False`.\"\"\"\n\n    return select(table, field, lambda v: not bool(v),\n                  complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting rows where the given field is None.", "response": "def selectnone(table, field, complement=False):\n    \"\"\"Select rows where the given field is `None`.\"\"\"\n\n    return select(table, field, lambda v: v is None, complement=complement)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef selectnotnone(table, field, complement=False):\n\n    return select(table, field, lambda v: v is not None,\n                  complement=complement)", "response": "Select rows where the given field is not None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef facet(table, key):\n\n    fct = dict()\n    for v in set(values(table, key)):\n        fct[v] = selecteq(table, key, v)\n    return fct", "response": "Return a dictionary mapping field values to tables. Eg."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef biselect(table, *args, **kwargs):\n\n    # override complement kwarg\n    kwargs['complement'] = False\n    t1 = select(table, *args, **kwargs)\n    kwargs['complement'] = True\n    t2 = select(table, *args, **kwargs)\n    return t1, t2", "response": "Return two tables the first containing selected rows the second containing remaining rows"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nredirects stdout or stderr to a StringIO object. As of python 3.4, there is a standard library contextmanager for this, but backwards compatibility!", "response": "def stdchannel_redirected(stdchannel):\n    \"\"\"\n    Redirects stdout or stderr to a StringIO object. As of python 3.4, there is a\n    standard library contextmanager for this, but backwards compatibility!\n    \"\"\"\n    try:\n        s = io.StringIO()\n        old = getattr(sys, stdchannel)\n        setattr(sys, stdchannel, s)\n        yield s\n    finally:\n        setattr(sys, stdchannel, old)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(param):\n    return (\n        Pattern(param) if isinstance(param, str)\n        else param if param is not None\n        else Null()\n    )", "response": "Load a single object from a string or None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving results for the :meth:'path.using_module' classmethod. When Python 3.2 is available, use functools.lru_cache instead.", "response": "def simple_cache(func):\n    \"\"\"\n    Save results for the :meth:'path.using_module' classmethod.\n    When Python 3.2 is available, use functools.lru_cache instead.\n    \"\"\"\n    saved_results = {}\n\n    def wrapper(cls, module):\n        if module in saved_results:\n            return saved_results[module]\n        saved_results[module] = func(cls, module)\n        return saved_results[module]\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping a copy function (like shutil.copy2) to return the dst if it's newer than the source.", "response": "def only_newer(copy_func):\n    \"\"\"\n    Wrap a copy function (like shutil.copy2) to return\n    the dst if it's newer than the source.\n    \"\"\"\n    @functools.wraps(copy_func)\n    def wrapper(src, dst, *args, **kwargs):\n        is_newer_dst = (\n            dst.exists()\n            and dst.getmtime() >= src.getmtime()\n        )\n        if is_newer_dst:\n            return dst\n        return copy_func(src, dst, *args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsupports multiple Unix chmod symbolic modes.", "response": "def _multi_permission_mask(mode):\n    \"\"\"\n    Support multiple, comma-separated Unix chmod symbolic modes.\n\n    >>> _multi_permission_mask('a=r,u+w')(0) == 0o644\n    True\n    \"\"\"\n    def compose(f, g):\n        return lambda *args, **kwargs: g(f(*args, **kwargs))\n    return functools.reduce(compose, map(_permission_mask, mode.split(',')))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a Unix chmod symbolic mode like ugo + rwx to a function suitable for applying to a mask to affect that change.", "response": "def _permission_mask(mode):\n    \"\"\"\n    Convert a Unix chmod symbolic mode like ``'ugo+rwx'`` to a function\n    suitable for applying to a mask to affect that change.\n\n    >>> mask = _permission_mask('ugo+rwx')\n    >>> mask(0o554) == 0o777\n    True\n\n    >>> _permission_mask('go-x')(0o777) == 0o766\n    True\n\n    >>> _permission_mask('o-x')(0o445) == 0o444\n    True\n\n    >>> _permission_mask('a+x')(0) == 0o111\n    True\n\n    >>> _permission_mask('a=rw')(0o057) == 0o666\n    True\n\n    >>> _permission_mask('u=x')(0o666) == 0o166\n    True\n\n    >>> _permission_mask('g=')(0o157) == 0o107\n    True\n    \"\"\"\n    # parse the symbolic mode\n    parsed = re.match('(?P<who>[ugoa]+)(?P<op>[-+=])(?P<what>[rwx]*)$', mode)\n    if not parsed:\n        raise ValueError(\"Unrecognized symbolic mode\", mode)\n\n    # generate a mask representing the specified permission\n    spec_map = dict(r=4, w=2, x=1)\n    specs = (spec_map[perm] for perm in parsed.group('what'))\n    spec = functools.reduce(operator.or_, specs, 0)\n\n    # now apply spec to each subject in who\n    shift_map = dict(u=6, g=3, o=0)\n    who = parsed.group('who').replace('a', 'ugo')\n    masks = (spec << shift_map[subj] for subj in who)\n    mask = functools.reduce(operator.or_, masks)\n\n    op = parsed.group('op')\n\n    # if op is -, invert the mask\n    if op == '-':\n        mask ^= 0o777\n\n    # if op is =, retain extant values for unreferenced subjects\n    if op == '=':\n        masks = (0o7 << shift_map[subj] for subj in who)\n        retain = functools.reduce(operator.or_, masks) ^ 0o777\n\n    op_map = {\n        '+': operator.or_,\n        '-': operator.and_,\n        '=': lambda mask, target: target & retain ^ mask,\n    }\n    return functools.partial(op_map[op], mask)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stem(self):\n        base, ext = self.module.splitext(self.name)\n        return base", "response": "The same as name but with one file extension stripped off."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef with_suffix(self, suffix):\n        if not suffix.startswith('.'):\n            raise ValueError(\"Invalid suffix {suffix!r}\".format(**locals()))\n\n        return self.stripext() + suffix", "response": "Return a new path with the file suffix changed or added."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef splitpath(self):\n        parent, child = self.module.split(self)\n        return self._next_class(parent), child", "response": "Split the path into parent and child names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef splitdrive(self):\n        drive, rel = self.module.splitdrive(self)\n        return self._next_class(drive), rel", "response": "p. splitdrive - > Return the drive and the rest of p."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef splitext(self):\n        filename, ext = self.module.splitext(self)\n        return self._next_class(filename), ext", "response": "splitext - Split the filename extension from this path and return the first part of the filename and the extension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit the unc into a base class and the rest.", "response": "def splitunc(self):\n        \"\"\" .. seealso:: :func:`os.path.splitunc` \"\"\"\n        unc, rest = self.module.splitunc(self)\n        return self._next_class(unc), rest"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef uncshare(self):\n        unc, r = self.module.splitunc(self)\n        return self._next_class(unc)", "response": "Returns the UNC mount point for this path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\njoin first to zero or more Path components and return a new instance of cls.", "response": "def joinpath(cls, first, *others):\n        \"\"\"\n        Join first to zero or more :class:`Path` components,\n        adding a separator character (:samp:`{first}.module.sep`)\n        if needed.  Returns a new instance of\n        :samp:`{first}._next_class`.\n\n        .. seealso:: :func:`os.path.join`\n        \"\"\"\n        if not isinstance(first, cls):\n            first = cls(first)\n        return first._next_class(first.module.join(first, *others))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef splitall(self):\n        parts = []\n        loc = self\n        while loc != os.curdir and loc != os.pardir:\n            prev = loc\n            loc, child = prev.splitpath()\n            if loc == prev:\n                break\n            parts.append(child)\n        parts.append(loc)\n        parts.reverse()\n        return parts", "response": "r Returns a list of the path components in this path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relpath(self, start='.'):\n        cwd = self._next_class(start)\n        return cwd.relpathto(self)", "response": "Return this path as a relative path based from start."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listdir(self, match=None):\n        match = matchers.load(match)\n        return list(filter(match, (\n            self / child for child in os.listdir(self)\n        )))", "response": "D. listdir - Returns a list of items in this directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dirs(self, *args, **kwargs):\n        return [p for p in self.listdir(*args, **kwargs) if p.isdir()]", "response": "Returns a list of all the subdirectories of this directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef files(self, *args, **kwargs):\n\n        return [p for p in self.listdir(*args, **kwargs) if p.isfile()]", "response": "D. files - > List of the files in this directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef walk(self, match=None, errors='strict'):\n        class Handlers:\n            def strict(msg):\n                raise\n\n            def warn(msg):\n                warnings.warn(msg, TreeWalkWarning)\n\n            def ignore(msg):\n                pass\n\n        if not callable(errors) and errors not in vars(Handlers):\n            raise ValueError(\"invalid errors parameter\")\n        errors = vars(Handlers).get(errors, errors)\n\n        match = matchers.load(match)\n\n        try:\n            childList = self.listdir()\n        except Exception:\n            exc = sys.exc_info()[1]\n            tmpl = \"Unable to list directory '%(self)s': %(exc)s\"\n            msg = tmpl % locals()\n            errors(msg)\n            return\n\n        for child in childList:\n            if match(child):\n                yield child\n            try:\n                isdir = child.isdir()\n            except Exception:\n                exc = sys.exc_info()[1]\n                tmpl = \"Unable to access '%(child)s': %(exc)s\"\n                msg = tmpl % locals()\n                errors(msg)\n                isdir = False\n\n            if isdir:\n                for item in child.walk(errors=errors, match=match):\n                    yield item", "response": "A recursive generator that returns the names of each child item of this directory and its descendants."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef walkfiles(self, *args, **kwargs):\n        return (\n            item\n            for item in self.walk(*args, **kwargs)\n            if item.isfile()\n        )", "response": "A generator function that returns an iterator over the files in the D."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fnmatch(self, pattern, normcase=None):\n        default_normcase = getattr(pattern, 'normcase', self.module.normcase)\n        normcase = normcase or default_normcase\n        name = normcase(self.name)\n        pattern = normcase(pattern)\n        return fnmatch.fnmatchcase(name, pattern)", "response": "Return True if self. name matches the given pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef glob(self, pattern):\n        cls = self._next_class\n        return [cls(s) for s in glob.glob(self / pattern)]", "response": "Return a list of Path objects that match the pattern."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator yielding chunks of the file.", "response": "def chunks(self, size, *args, **kwargs):\n        \"\"\" Returns a generator yielding chunks of the file, so it can\n            be read piece by piece with a simple for loop.\n\n           Any argument you pass after `size` will be passed to :meth:`open`.\n\n           :example:\n\n               >>> hash = hashlib.md5()\n               >>> for chunk in Path(\"CHANGES.rst\").chunks(8192, mode='rb'):\n               ...     hash.update(chunk)\n\n            This will read the file by chunks of 8192 bytes.\n        \"\"\"\n        with self.open(*args, **kwargs) as f:\n            for chunk in iter(lambda: f.read(size) or None, None):\n                yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_bytes(self, bytes, append=False):\n        if append:\n            mode = 'ab'\n        else:\n            mode = 'wb'\n        with self.open(mode) as f:\n            f.write(bytes)", "response": "Open this file and write the given bytes to it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening this file read all lines return them in a list.", "response": "def lines(self, encoding=None, errors='strict', retain=True):\n        r\"\"\" Open this file, read all lines, return them in a list.\n\n        Optional arguments:\n            `encoding` - The Unicode encoding (or character set) of\n                the file.  The default is ``None``, meaning the content\n                of the file is read as 8-bit characters and returned\n                as a list of (non-Unicode) str objects.\n            `errors` - How to handle Unicode errors; see help(str.decode)\n                for the options.  Default is ``'strict'``.\n            `retain` - If ``True``, retain newline characters; but all newline\n                character combinations (``'\\r'``, ``'\\n'``, ``'\\r\\n'``) are\n                translated to ``'\\n'``.  If ``False``, newline characters are\n                stripped off.  Default is ``True``.\n\n        .. seealso:: :meth:`text`\n        \"\"\"\n        return self.text(encoding, errors).splitlines(retain)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_lines(self, lines, encoding=None, errors='strict',\n                    linesep=os.linesep, append=False):\n        r\"\"\" Write the given lines of text to this file.\n\n        By default this overwrites any existing file at this path.\n\n        This puts a platform-specific newline sequence on every line.\n        See `linesep` below.\n\n            `lines` - A list of strings.\n\n            `encoding` - A Unicode encoding to use.  This applies only if\n                `lines` contains any Unicode strings.\n\n            `errors` - How to handle errors in Unicode encoding.  This\n                also applies only to Unicode strings.\n\n            linesep - The desired line-ending.  This line-ending is\n                applied to every line.  If a line already has any\n                standard line ending (``'\\r'``, ``'\\n'``, ``'\\r\\n'``,\n                ``u'\\x85'``, ``u'\\r\\x85'``, ``u'\\u2028'``), that will\n                be stripped off and this will be used instead.  The\n                default is os.linesep, which is platform-dependent\n                (``'\\r\\n'`` on Windows, ``'\\n'`` on Unix, etc.).\n                Specify ``None`` to write the lines as-is, like\n                :meth:`file.writelines`.\n\n        Use the keyword argument ``append=True`` to append lines to the\n        file.  The default is to overwrite the file.\n\n        .. warning ::\n\n            When you use this with Unicode data, if the encoding of the\n            existing data in the file is different from the encoding\n            you specify with the `encoding=` parameter, the result is\n            mixed-encoding data, which can really confuse someone trying\n            to read the file later.\n        \"\"\"\n        with self.open('ab' if append else 'wb') as f:\n            for line in lines:\n                isUnicode = isinstance(line, str)\n                if linesep is not None:\n                    pattern = U_NL_END if isUnicode else NL_END\n                    line = pattern.sub('', line) + linesep\n                if isUnicode:\n                    line = line.encode(\n                        encoding or sys.getdefaultencoding(), errors)\n                f.write(line)", "response": "r Write the given lines of text to the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a hash object for the current path.", "response": "def _hash(self, hash_name):\n        \"\"\" Returns a hash object for the file at the current path.\n\n        `hash_name` should be a hash algo name (such as ``'md5'``\n        or ``'sha1'``) that's available in the :mod:`hashlib` module.\n        \"\"\"\n        m = hashlib.new(hash_name)\n        for chunk in self.chunks(8192, mode=\"rb\"):\n            m.update(chunk)\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef samefile(self, other):\n        if not hasattr(self.module, 'samefile'):\n            other = Path(other).realpath().normpath().normcase()\n            return self.realpath().normpath().normcase() == other\n        return self.module.samefile(self, other)", "response": "Check if two files are the same."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __get_owner_windows(self):\n        desc = win32security.GetFileSecurity(\n            self, win32security.OWNER_SECURITY_INFORMATION)\n        sid = desc.GetSecurityDescriptorOwner()\n        account, domain, typecode = win32security.LookupAccountSid(None, sid)\n        return domain + '\\\\' + account", "response": "Return the name of the owner of this file or directory. Follow\n        symbolic links."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __get_owner_unix(self):\n        st = self.stat()\n        return pwd.getpwuid(st.st_uid).pw_name", "response": "Return the name of the owner of this file or directory. Follow\n        symbolic links."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the mode of the object.", "response": "def chmod(self, mode):\n        \"\"\"\n        Set the mode. May be the new mode (os.chmod behavior) or a `symbolic\n        mode <http://en.wikipedia.org/wiki/Chmod#Symbolic_modes>`_.\n\n        .. seealso:: :func:`os.chmod`\n        \"\"\"\n        if isinstance(mode, str):\n            mask = _multi_permission_mask(mode)\n            mode = mask(self.stat().st_mode)\n        os.chmod(self, mode)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging the owner and group by names rather than the uid or gid numbers.", "response": "def chown(self, uid=-1, gid=-1):\n        \"\"\"\n        Change the owner and group by names rather than the uid or gid numbers.\n\n        .. seealso:: :func:`os.chown`\n        \"\"\"\n        if hasattr(os, 'chown'):\n            if 'pwd' in globals() and isinstance(uid, str):\n                uid = pwd.getpwnam(uid).pw_uid\n            if 'grp' in globals() and isinstance(gid, str):\n                gid = grp.getgrnam(gid).gr_gid\n            os.chown(self, uid, gid)\n        else:\n            msg = \"Ownership not available on this platform.\"\n            raise NotImplementedError(msg)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrename the current object to the given name.", "response": "def rename(self, new):\n        \"\"\" .. seealso:: :func:`os.rename` \"\"\"\n        os.rename(self, new)\n        return self._next_class(new)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef renames(self, new):\n        os.renames(self, new)\n        return self._next_class(new)", "response": "Returns a new version of the current one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mkdir_p(self, mode=0o777):\n        with contextlib.suppress(FileExistsError):\n            self.mkdir(mode)\n        return self", "response": "Like mkdir but does not raise an exception."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makedirs_p(self, mode=0o777):\n        with contextlib.suppress(FileExistsError):\n            self.makedirs(mode)\n        return self", "response": "Like makedirs but does not raise an exception."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike rmdir but does not raise an exception if the directory is not empty.", "response": "def rmdir_p(self):\n        \"\"\" Like :meth:`rmdir`, but does not raise an exception if the\n        directory is not empty or does not exist. \"\"\"\n        suppressed = FileNotFoundError, FileExistsError, DirectoryNotEmpty\n        with contextlib.suppress(suppressed):\n            with DirectoryNotEmpty.translate():\n                self.rmdir()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking removedirs but does not raise an exception if the the directory is not empty.", "response": "def removedirs_p(self):\n        \"\"\" Like :meth:`removedirs`, but does not raise an exception if the\n        directory is not empty or does not exist. \"\"\"\n        with contextlib.suppress(FileExistsError, DirectoryNotEmpty):\n            with DirectoryNotEmpty.translate():\n                self.removedirs()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the access and modified times of this file to the current time.", "response": "def touch(self):\n        \"\"\" Set the access/modified times of this file to the current time.\n        Create the file if it does not exist.\n        \"\"\"\n        fd = os.open(self, os.O_WRONLY | os.O_CREAT, 0o666)\n        os.close(fd)\n        os.utime(self, None)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_tree(\n            self, dst, symlinks=False,\n            *,\n            update=False,\n            copy_function=shutil.copy2,\n            ignore=lambda dir, contents: []):\n        \"\"\"\n        Copy entire contents of self to dst, overwriting existing\n        contents in dst with those in self.\n\n        Pass ``symlinks=True`` to copy symbolic links as links.\n\n        Accepts a ``copy_function``, similar to copytree.\n\n        To avoid overwriting newer files, supply a copy function\n        wrapped in ``only_newer``. For example::\n\n            src.merge_tree(dst, copy_function=only_newer(shutil.copy2))\n        \"\"\"\n        dst = self._next_class(dst)\n        dst.makedirs_p()\n\n        if update:\n            warnings.warn(\n                \"Update is deprecated; \"\n                \"use copy_function=only_newer(shutil.copy2)\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            copy_function = only_newer(copy_function)\n\n        sources = self.listdir()\n        _ignored = ignore(self, [item.name for item in sources])\n\n        def ignored(item):\n            return item.name in _ignored\n\n        for source in itertools.filterfalse(ignored, sources):\n            dest = dst / source.name\n            if symlinks and source.islink():\n                target = source.readlink()\n                target.symlink(dest)\n            elif source.isdir():\n                source.merge_tree(\n                    dest,\n                    symlinks=symlinks,\n                    update=update,\n                    copy_function=copy_function,\n                    ignore=ignore,\n                )\n            else:\n                copy_function(source, dest)\n\n        self.copystat(dst)", "response": "Recursively copies contents of self to dst."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in_place(\n            self, mode='r', buffering=-1, encoding=None, errors=None,\n            newline=None, backup_extension=None,\n    ):\n        \"\"\"\n        A context in which a file may be re-written in-place with\n        new content.\n\n        Yields a tuple of :samp:`({readable}, {writable})` file\n        objects, where `writable` replaces `readable`.\n\n        If an exception occurs, the old file is restored, removing the\n        written data.\n\n        Mode *must not* use ``'w'``, ``'a'``, or ``'+'``; only\n        read-only-modes are allowed. A :exc:`ValueError` is raised\n        on invalid modes.\n\n        For example, to add line numbers to a file::\n\n            p = Path(filename)\n            assert p.isfile()\n            with p.in_place() as (reader, writer):\n                for number, line in enumerate(reader, 1):\n                    writer.write('{0:3}: '.format(number)))\n                    writer.write(line)\n\n        Thereafter, the file at `filename` will have line numbers in it.\n        \"\"\"\n        import io\n\n        if set(mode).intersection('wa+'):\n            raise ValueError('Only read-only file modes can be used')\n\n        # move existing file to backup, create new file with same permissions\n        # borrowed extensively from the fileinput module\n        backup_fn = self + (backup_extension or os.extsep + 'bak')\n        try:\n            os.unlink(backup_fn)\n        except os.error:\n            pass\n        os.rename(self, backup_fn)\n        readable = io.open(\n            backup_fn, mode, buffering=buffering,\n            encoding=encoding, errors=errors, newline=newline,\n        )\n        try:\n            perm = os.fstat(readable.fileno()).st_mode\n        except OSError:\n            writable = open(\n                self, 'w' + mode.replace('r', ''),\n                buffering=buffering, encoding=encoding, errors=errors,\n                newline=newline,\n            )\n        else:\n            os_mode = os.O_CREAT | os.O_WRONLY | os.O_TRUNC\n            if hasattr(os, 'O_BINARY'):\n                os_mode |= os.O_BINARY\n            fd = os.open(self, os_mode, perm)\n            writable = io.open(\n                fd, \"w\" + mode.replace('r', ''),\n                buffering=buffering, encoding=encoding, errors=errors,\n                newline=newline,\n            )\n            try:\n                if hasattr(os, 'chmod'):\n                    os.chmod(self, perm)\n            except OSError:\n                pass\n        try:\n            yield readable, writable\n        except Exception:\n            # move backup back\n            readable.close()\n            writable.close()\n            try:\n                os.unlink(self)\n            except os.error:\n                pass\n            os.rename(backup_fn, self)\n            raise\n        else:\n            readable.close()\n            writable.close()\n        finally:\n            try:\n                os.unlink(backup_fn)\n            except os.error:\n                pass", "response": "A function that creates a new file in - place with the contents of the current file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dir(self, scope, class_):\n        prop_name = '{scope}_{class_}_dir'.format(**locals())\n        value = getattr(self.wrapper, prop_name)\n        MultiPath = Multi.for_class(self.path_class)\n        return MultiPath.detect(value)", "response": "Return the callable function from appdirs but with the\n        result wrapped in self. path_class\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _next_class(cls):\n        return next(\n            class_\n            for class_ in cls.__mro__\n            if not issubclass(class_, Multi)\n        )", "response": "Returns the next Multi class in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef escape(s, quote=True):\n    s = s.replace(\"&\", \"&amp;\") # Must be done first!\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    if quote:\n        s = s.replace('\"', \"&quot;\")\n        s = s.replace('\\'', \"&#x27;\")\n    return s", "response": "Escape special characters &lt and > to HTML - safe sequences."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render_math(self, token):\n        if token.content.startswith('$$'):\n            return self.render_raw_text(token)\n        return '${}$'.format(self.render_raw_text(token))", "response": "Render a math token."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders a list of HTML documents.", "response": "def markdown(iterable, renderer=HTMLRenderer):\n    \"\"\"\n    Output HTML with default settings.\n    Enables inline and block-level HTML tags.\n    \"\"\"\n    with renderer() as renderer:\n        return renderer.render(Document(iterable))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tokenize_block(iterable, token_types):\n    lines = FileWrapper(iterable)\n    parse_buffer = ParseBuffer()\n    line = lines.peek()\n    while line is not None:\n        for token_type in token_types:\n            if token_type.start(line):\n                result = token_type.read(lines)\n                if result is not None:\n                    parse_buffer.append((token_type, result))\n                    break\n        else:  # unmatched newlines\n            next(lines)\n            parse_buffer.loose = True\n        line = lines.peek()\n    return parse_buffer", "response": "Tokenize a block of text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_tokens(parse_buffer):\n    tokens = []\n    for token_type, result in parse_buffer:\n        token = token_type(result)\n        if token is not None:\n            tokens.append(token)\n    return tokens", "response": "Takes a list of pairs token_type and read_result and returns a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a Markdown file and dump the output to stdout.", "response": "def convert_file(filename, renderer):\n    \"\"\"\n    Parse a Markdown file and dump the output to stdout.\n    \"\"\"\n    try:\n        with open(filename, 'r') as fin:\n            rendered = mistletoe.markdown(fin, renderer)\n            print(rendered, end='')\n    except OSError:\n        sys.exit('Cannot open file \"{}\".'.format(filename))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef interactive(renderer):\n    _import_readline()\n    _print_heading(renderer)\n    contents = []\n    more = False\n    while True:\n        try:\n            prompt, more = ('... ', True) if more else ('>>> ', True)\n            contents.append(input(prompt) + '\\n')\n        except EOFError:\n            print('\\n' + mistletoe.markdown(contents, renderer), end='')\n            more = False\n            contents = []\n        except KeyboardInterrupt:\n            print('\\nExiting.')\n            break", "response": "Parse user input dump to stdout rinse and repeat."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toc(self):\n        from mistletoe.block_token import List\n        def get_indent(level):\n            if self.omit_title:\n                level -= 1\n            return ' ' * 4 * (level - 1)\n\n        def build_list_item(heading):\n            level, content = heading\n            template = '{indent}- {content}\\n'\n            return template.format(indent=get_indent(level), content=content)\n\n        return List([build_list_item(heading) for heading in self._headings])", "response": "Returns a table of contents as a block_token. List instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides render_heading to store rendered heading first and return it.", "response": "def render_heading(self, token):\n        \"\"\"\n        Overrides super().render_heading; stores rendered heading first,\n        then returns it.\n        \"\"\"\n        rendered = super().render_heading(token)\n        content = self.parse_rendered_heading(rendered)\n        if not (self.omit_title and token.level == 1\n                or token.level > self.depth\n                or any(cond(content) for cond in self.filter_conds)):\n            self._headings.append((token.level, content))\n        return rendered"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a marker line and return a pair of prepend and leader.", "response": "def parse_marker(cls, line):\n        \"\"\"\n        Returns a pair (prepend, leader) iff the line has a valid leader.\n        \"\"\"\n        match_obj = cls.pattern.match(line)\n        if match_obj is None:\n            return None        # no valid leader\n        leader = match_obj.group(1)\n        content = match_obj.group(0).replace(leader+'\\t', leader+'   ', 1)\n        # reassign prepend and leader\n        prepend = len(content)\n        if prepend == len(line.rstrip('\\n')):\n            prepend = match_obj.end(1) + 1\n        else:\n            spaces = match_obj.group(2)\n            if spaces.startswith('\\t'):\n                spaces = spaces.replace('\\t', '   ', 1)\n            spaces = spaces.replace('\\t', '    ')\n            n_spaces = len(spaces)\n            if n_spaces > 4:\n                prepend = match_obj.end(1) + 1\n        return prepend, leader"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_inner(self, token):\n        rendered = [self.render(child) for child in token.children]\n        return ''.join(rendered)", "response": "Recursively renders child tokens. Joins the rendered string with no space in between."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the complexity of a signal.", "response": "def complexity(signal, sampling_rate=1000, shannon=True, sampen=True, multiscale=True, spectral=True, svd=True, correlation=True, higushi=True, petrosian=True, fisher=True, hurst=True, dfa=True, lyap_r=False, lyap_e=False, emb_dim=2, tolerance=\"default\", k_max=8, bands=None, tau=1):\n    \"\"\"\n    Computes several chaos/complexity indices of a signal (including entropy, fractal dimensions, Hurst and Lyapunov exponent etc.).\n\n    Parameters\n    ----------\n    signal : list or array\n        List or array of values.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    shannon : bool\n        Computes Shannon entropy.\n    sampen : bool\n        Computes approximate sample entropy (sampen) using Chebychev and Euclidean distances.\n    multiscale : bool\n        Computes multiscale entropy (MSE). Note that it uses the 'euclidean' distance.\n    spectral : bool\n        Computes Spectral Entropy.\n    svd : bool\n        Computes the Singular Value Decomposition (SVD) entropy.\n    correlation : bool\n        Computes the fractal (correlation) dimension.\n    higushi : bool\n        Computes the Higushi fractal dimension.\n    petrosian : bool\n        Computes the Petrosian fractal dimension.\n    fisher : bool\n        Computes the Fisher Information.\n    hurst : bool\n        Computes the Hurst exponent.\n    dfa : bool\n        Computes DFA.\n    lyap_r : bool\n        Computes Positive Lyapunov exponents (Rosenstein et al. (1993) method).\n    lyap_e : bool\n        Computes Positive Lyapunov exponents (Eckmann et al. (1986) method).\n    emb_dim : int\n        The embedding dimension (*m*, the length of vectors to compare). Used in sampen, fisher, svd and fractal_dim.\n    tolerance : float\n        Distance *r* threshold for two template vectors to be considered equal. Default is 0.2*std(signal). Used in sampen and fractal_dim.\n    k_max : int\n        The maximal value of k used for Higushi fractal dimension. The point at which the FD plateaus is considered a saturation point and that kmax value should be selected (G\u00f3mez, 2009). Some studies use a value of 8 or 16 for ECG signal and other 48 for MEG.\n    bands : int\n        Used for spectral density. A list of numbers delimiting the bins of the frequency bands. If None the entropy is computed over the whole range of the DFT (from 0 to `f_s/2`).\n    tau : int\n        The delay. Used for fisher, svd, lyap_e and lyap_r.\n\n    Returns\n    ----------\n    complexity : dict\n        Dict containing values for each indices.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> import numpy as np\n    >>>\n    >>> signal = np.sin(np.log(np.random.sample(666)))\n    >>> complexity = nk.complexity(signal)\n\n    Notes\n    ----------\n    *Details*\n\n    - **Entropy**: Entropy is a measure of unpredictability of the state, or equivalently, of its average information content.\n\n      - *Shannon entropy*: Shannon entropy was introduced by Claude E. Shannon in his 1948 paper \"A Mathematical Theory of Communication\". Shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source.\n      - *Sample entropy (sampen)*: Measures the complexity of a time-series, based on approximate entropy. The sample entropy of a time series is defined as the negative natural logarithm of the conditional probability that two sequences similar for emb_dim points remain similar at the next point, excluding self-matches. A lower value for the sample entropy therefore corresponds to a higher probability indicating more self-similarity.\n      - *Multiscale entropy*: Multiscale entropy (MSE) analysis is a new method of measuring the complexity of finite length time series.\n      - *SVD Entropy*: Indicator of how many vectors are needed for an adequate explanation of the data set. Measures feature-richness in the sense that the higher the entropy of the set of SVD weights, the more orthogonal vectors are required to adequately explain it.\n\n    - **fractal dimension**: The term *fractal* was first introduced by Mandelbrot in 1983. A fractal is a set of points that when looked at smaller scales, resembles the whole set. The concept of fractak dimension (FD) originates from fractal geometry. In traditional geometry, the topological or Euclidean dimension of an object is known as the number of directions each differential of the object occupies in space. This definition of dimension works well for geometrical objects whose level of detail, complexity or *space-filling* is the same. However, when considering two fractals of the same topological dimension, their level of *space-filling* is different, and that information is not given by the topological dimension. The FD emerges to provide a measure of how much space an object occupies between Euclidean dimensions. The FD of a waveform represents a powerful tool for transient detection. This feature has been used in the analysis of ECG and EEG to identify and distinguish specific states of physiologic function. Many algorithms are available to determine the FD of the waveform (Acharya, 2005).\n\n      - *Correlation*: A measure of the fractal (or correlation) dimension of a time series which is also related to complexity. The correlation dimension is a characteristic measure that can be used to describe the geometry of chaotic attractors. It is defined using the correlation sum C(r) which is the fraction of pairs of points X_i in the phase space whose distance is smaller than r.\n      - *Higushi*: Higuchi proposed in 1988 an efficient algorithm for measuring the FD of discrete time sequences. As the reconstruction of the attractor phase space is not necessary, this algorithm is simpler and faster than D2 and other classical measures derived from chaos theory. FD can be used to quantify the complexity and self-similarity of a signal. HFD has already been used to analyse the complexity of brain recordings and other biological signals.\n      - *Petrosian Fractal Dimension*: Provide a fast computation of the FD of a signal by translating the series into a binary sequence.\n\n    - **Other**:\n\n      - *Fisher Information*:  A way of measuring the amount of information that an observable random variable X carries about an unknown parameter \u03b8 of a distribution that models X. Formally, it is the variance of the score, or the expected value of the observed information.\n      - *Hurst*: The Hurst exponent is a measure of the \"long-term memory\" of a time series. It can be used to determine whether the time series is more, less, or equally likely to increase if it has increased in previous steps. This property makes the Hurst exponent especially interesting for the analysis of stock data.\n      - *DFA*: DFA measures the Hurst parameter H, which is very similar to the Hurst exponent. The main difference is that DFA can be used for non-stationary processes (whose mean and/or variance change over time).\n      - *Lyap*: Positive Lyapunov exponents indicate chaos and unpredictability. Provides the algorithm of Rosenstein et al. (1993) to estimate the largest Lyapunov exponent and the algorithm of Eckmann et al. (1986) to estimate the whole spectrum of Lyapunov exponents.\n\n    *Authors*\n\n    - Dominique Makowski (https://github.com/DominiqueMakowski)\n    - Christopher Sch\u00f6lzel (https://github.com/CSchoel)\n    - tjugo (https://github.com/nikdon)\n    - Quentin Geissmann (https://github.com/qgeissmann)\n\n    *Dependencies*\n\n    - nolds\n    - numpy\n\n    *See Also*\n\n    - nolds package: https://github.com/CSchoel/nolds\n    - pyEntropy package: https://github.com/nikdon/pyEntropy\n    - pyrem package: https://github.com/gilestrolab/pyrem\n\n    References\n    -----------\n    - Accardo, A., Affinito, M., Carrozzi, M., & Bouquet, F. (1997). Use of the fractal dimension for the analysis of electroencephalographic time series. Biological cybernetics, 77(5), 339-350.\n    - Pierzchalski, M. Application of Higuchi Fractal Dimension in Analysis of Heart Rate Variability with Artificial and Natural Noise. Recent Advances in Systems Science.\n    - Acharya, R., Bhat, P. S., Kannathal, N., Rao, A., & Lim, C. M. (2005). Analysis of cardiac health using fractal dimension and wavelet transformation. ITBM-RBM, 26(2), 133-139.\n    - Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n    - Costa, M., Goldberger, A. L., & Peng, C. K. (2005). Multiscale entropy analysis of biological signals. Physical review E, 71(2), 021906.\n    \"\"\"\n\n    if tolerance == \"default\":\n        tolerance = 0.2*np.std(signal)\n\n    # Initialize results storing\n    complexity = {}\n\n\n# ------------------------------------------------------------------------------\n\n    # Shannon\n    if shannon is True:\n        try:\n            complexity[\"Entropy_Shannon\"] = complexity_entropy_shannon(signal)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute Shannon entropy.\")\n            complexity[\"Entropy_Shannon\"] = np.nan\n\n    # Sampen\n    if sampen is True:\n        try:\n            complexity[\"Entropy_Sample\"] = nolds.sampen(signal, emb_dim, tolerance, dist=\"chebychev\", debug_plot=False, plot_file=None)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute sample entropy (sampen).\")\n            complexity[\"Entropy_Sample\"] = np.nan\n\n\n    # multiscale\n    if multiscale is True:\n        try:\n            complexity[\"Entropy_Multiscale\"] = complexity_entropy_multiscale(signal, emb_dim, tolerance)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute Multiscale Entropy (MSE).\")\n            complexity[\"Entropy_Multiscale\"] = np.nan\n\n    # spectral\n    if spectral is True:\n        try:\n            complexity[\"Entropy_Spectral\"] = complexity_entropy_spectral(signal, sampling_rate=sampling_rate, bands=bands)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute Spectral Entropy.\")\n            complexity[\"Entropy_Spectral\"] = np.nan\n\n    # SVD\n    if svd is True:\n        try:\n            complexity[\"Entropy_SVD\"] = complexity_entropy_svd(signal, tau=tau, emb_dim=emb_dim)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute SVD Entropy.\")\n            complexity[\"Entropy_SVD\"] = np.nan\n\n# ------------------------------------------------------------------------------\n    # fractal_dim\n    if correlation is True:\n        try:\n            complexity[\"Fractal_Dimension_Correlation\"] = nolds.corr_dim(signal, emb_dim, rvals=None, fit=\"RANSAC\", debug_plot=False, plot_file=None)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute fractal_dim.\")\n            complexity[\"Fractal_Dimension_Correlation\"] = np.nan\n\n    # higushi\n    if higushi is True:\n        try:\n            complexity[\"Fractal_Dimension_Higushi\"] = complexity_fd_higushi(signal, k_max)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute higushi.\")\n            complexity[\"Fractal_Dimension_Higushi\"] = np.nan\n\n    # petrosian\n    if petrosian is True:\n        try:\n            complexity[\"Fractal_Dimension_Petrosian\"] = complexity_fd_petrosian(signal)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute petrosian.\")\n            complexity[\"Fractal_Dimension_Petrosian\"] = np.nan\n\n# ------------------------------------------------------------------------------\n\n    # Fisher\n    if fisher is True:\n        try:\n            complexity[\"Fisher_Information\"] = complexity_fisher_info(signal, tau=tau, emb_dim=emb_dim)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute Fisher Information.\")\n            complexity[\"Fisher_Information\"] = np.nan\n\n\n    # Hurst\n    if hurst is True:\n        try:\n            complexity[\"Hurst\"] = nolds.hurst_rs(signal, nvals=None, fit=\"RANSAC\", debug_plot=False, plot_file=None)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute hurst.\")\n            complexity[\"Hurst\"] = np.nan\n\n    # DFA\n    if dfa is True:\n        try:\n            complexity[\"DFA\"] = nolds.dfa(signal, nvals=None, overlap=True, order=1, fit_trend=\"poly\", fit_exp=\"RANSAC\", debug_plot=False, plot_file=None)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute dfa.\")\n            complexity[\"DFA\"] = np.nan\n\n    # Lyap_r\n    if lyap_r is True:\n        try:\n            complexity[\"Lyapunov_R\"] = nolds.lyap_r(signal, emb_dim=10, lag=None, min_tsep=None, tau=tau, min_vectors=20, trajectory_len=20, fit=\"RANSAC\", debug_plot=False, plot_file=None)\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute lyap_r.\")\n            complexity[\"Lyapunov_R\"] = np.nan\n\n    # Lyap_e\n    if lyap_e is True:\n        try:\n            result = nolds.lyap_e(signal, emb_dim=10, matrix_dim=4, min_nb=None, min_tsep=0, tau=tau, debug_plot=False, plot_file=None)\n            for i, value in enumerate(result):\n                complexity[\"Lyapunov_E_\" + str(i)] = value\n        except:\n            print(\"NeuroKit warning: complexity(): Failed to compute lyap_e.\")\n            complexity[\"Lyapunov_E\"] = np.nan\n\n    return(complexity)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef complexity_entropy_shannon(signal):\n     # Check if string\n    if not isinstance(signal, str):\n        signal = list(signal)\n\n    signal = np.array(signal)\n\n\n    # Create a frequency data\n    data_set = list(set(signal))\n    freq_list = []\n    for entry in data_set:\n        counter = 0.\n        for i in signal:\n            if i == entry:\n                counter += 1\n        freq_list.append(float(counter) / len(signal))\n\n    # Shannon entropy\n    shannon_entropy = 0.0\n    for freq in freq_list:\n        shannon_entropy += freq * np.log2(freq)\n    shannon_entropy = -shannon_entropy\n\n    return(shannon_entropy)", "response": "Computes the Shannon Entropy of a set of items in the signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the Multiscale Entropy for a set of signal.", "response": "def complexity_entropy_multiscale(signal, max_scale_factor=20, m=2, r=\"default\"):\n    \"\"\"\n    Computes the Multiscale Entropy. Uses sample entropy with 'chebychev' distance.\n\n    Parameters\n    ----------\n    signal : list or array\n        List or array of values.\n    max_scale_factor: int\n        Max scale factor (*tau*). The max length of coarse-grained time series analyzed. Will analyze scales for all integers from 1:max_scale_factor.\n        See Costa (2005).\n    m : int\n        The embedding dimension (*m*, the length of vectors to compare).\n    r : float\n        Similarity factor *r*. Distance threshold for two template vectors to be considered equal. Default is 0.15*std(signal).\n\n    Returns\n    ----------\n    mse: dict\n        A dict containing \"MSE_Parameters\" (a dict with the actual max_scale_factor, m and r), \"MSE_Values\" (an array with the sample entropy for each scale_factor up to the max_scale_factor), \"MSE_AUC\" (A float: The area under the MSE_Values curve. A point-estimate of mse) and \"MSE_Sum\" (A float: The sum of MSE_Values curve. Another point-estimate of mse; Norris, 2008).\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> signal = np.sin(np.log(np.random.sample(666)))\n    >>> mse = nk.complexity_entropy_multiscale(signal)\n    >>> mse_values = mse[\"MSE_Values\"]\n\n    Notes\n    ----------\n    *Details*\n\n    - **multiscale entropy**: Entropy is a measure of unpredictability of the state, or equivalently,\n    of its average information content. Multiscale entropy (MSE) analysis is a new method of measuring\n    the complexity of coarse grained versions of the original data, where coarse graining is at all\n    scale factors from 1:max_scale_factor.\n\n\n    *Authors*\n\n    - tjugo (https://github.com/nikdon)\n    - Dominique Makowski (https://github.com/DominiqueMakowski)\n    - Anthony Gatti (https://github.com/gattia)\n\n    *Dependencies*\n\n    - numpy\n    - nolds\n\n    *See Also*\n\n    - pyEntropy package: https://github.com/nikdon/pyEntropy\n\n    References\n    -----------\n    - Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy\n        and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n    - Costa, M., Goldberger, A. L., & Peng, C. K. (2005). Multiscale entropy analysis of biological signals.\n        Physical review E, 71(2), 021906.\n    - Gow, B. J., Peng, C. K., Wayne, P. M., & Ahn, A. C. (2015). Multiscale entropy analysis of center-of-pressure\n        dynamics in human postural control: methodological considerations. Entropy, 17(12), 7926-7947.\n    - Norris, P. R., Anderson, S. M., Jenkins, J. M., Williams, A. E., & Morris Jr, J. A. (2008).\n        Heart rate multiscale entropy at three hours predicts hospital mortality in 3,154 trauma patients. Shock, 30(1), 17-22.\n    \"\"\"\n    if r == \"default\":\n        r = 0.15*np.std(signal)\n\n    n = len(signal)\n    per_scale_entropy_values = np.zeros(max_scale_factor)\n\n    # Compute SampEn for all scale factors\n    for i in range(max_scale_factor):\n\n        b = int(np.fix(n / (i + 1)))\n        temp_ts = [0] * int(b)\n\n        for j in range(b):\n            num = sum(signal[j * (i + 1): (j + 1) * (i + 1)])\n            den = i + 1\n            temp_ts[j] = float(num) / float(den)\n\n        se = nolds.sampen(temp_ts, m, r, nolds.measures.rowwise_chebyshev, debug_plot=False, plot_file=None)\n\n        if np.isinf(se):\n            print(\"NeuroKit warning: complexity_entropy_multiscale(): Signal might be to short to compute SampEn for scale factors > \" + str(i) + \". Setting max_scale_factor to \" + str(i) + \".\")\n            max_scale_factor = i\n            break\n        else:\n            per_scale_entropy_values[i] = se\n\n    all_entropy_values = per_scale_entropy_values[0:max_scale_factor]\n\n    # Compute final indices\n    parameters = {\"max_scale_factor\": max_scale_factor,\n                  \"r\": r,\n                  \"m\": m}\n\n    mse = {\"MSE_Parameters\": parameters,\n           \"MSE_Values\" : all_entropy_values,\n           \"MSE_AUC\": np.trapz(all_entropy_values),\n           \"MSE_Sum\": np.sum(all_entropy_values)}\n\n\n    return (mse)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef complexity_fd_higushi(signal, k_max):\n    signal = np.array(signal)\n    L = []\n    x = []\n    N = signal.size\n\n    km_idxs = np.triu_indices(k_max - 1)\n    km_idxs = k_max - np.flipud(np.column_stack(km_idxs)) -1\n    km_idxs[:,1] -= 1\n\n\n    for k in range(1, k_max):\n        Lk = 0\n        for m in range(0, k):\n            #we pregenerate all idxs\n            idxs = np.arange(1,int(np.floor((N-m)/k)))\n\n            Lmk = np.sum(np.abs(signal[m+idxs*k] - signal[m+k*(idxs-1)]))\n            Lmk = (Lmk*(N - 1)/(((N - m)/ k)* k)) / k\n            Lk += Lmk\n\n        if Lk != 0:\n            L.append(np.log(Lk/(m+1)))\n            x.append([np.log(1.0/ k), 1])\n\n    (p, r1, r2, s)=np.linalg.lstsq(x, L)\n    fd_higushi = p[0]\n    return (fd_higushi)", "response": "Computes the Higushi Fractal Dimension of a signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the Spectral Entropy of a signal.", "response": "def complexity_entropy_spectral(signal, sampling_rate, bands=None):\n    \"\"\"\n    Computes Spectral Entropy of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann. The power spectrum is computed through fft. Then, it is normalised and assimilated to a probability density function.\n\n    Parameters\n    ----------\n    signal : list or array\n        List or array of values.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    bands : list or array\n        A list of numbers delimiting the bins of the frequency bands. If None the entropy is computed over the whole range of the DFT (from 0 to `f_s/2`).\n\n    Returns\n    ----------\n    spectral_entropy : float\n        The spectral entropy as float value.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> signal = np.sin(np.log(np.random.sample(666)))\n    >>> spectral_entropy = nk.complexity_entropy_spectral(signal, 1000)\n\n    Notes\n    ----------\n    *Details*\n\n    - **Spectral Entropy**: Entropy for different frequency bands.\n\n\n    *Authors*\n\n    - Quentin Geissmann (https://github.com/qgeissmann)\n\n    *Dependencies*\n\n    - numpy\n\n    *See Also*\n\n    - pyrem package: https://github.com/gilestrolab/pyrem\n    \"\"\"\n\n    psd = np.abs(np.fft.rfft(signal))**2\n    psd /= np.sum(psd) # psd as a pdf (normalised to one)\n\n    if bands is None:\n        power_per_band= psd[psd>0]\n    else:\n        freqs = np.fft.rfftfreq(signal.size, 1/float(sampling_rate))\n        bands = np.asarray(bands)\n\n        freq_limits_low = np.concatenate([[0.0],bands])\n        freq_limits_up = np.concatenate([bands, [np.Inf]])\n\n        power_per_band = [np.sum(psd[np.bitwise_and(freqs >= low, freqs<up)])\n                for low,up in zip(freq_limits_low, freq_limits_up)]\n\n        power_per_band= np.array(power_per_band)[np.array(power_per_band) > 0]\n\n    spectral = - np.sum(power_per_band * np.log2(power_per_band))\n    return(spectral)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef complexity_entropy_svd(signal, tau=1, emb_dim=2):\n    mat =  _embed_seq(signal, tau, emb_dim)\n    W = np.linalg.svd(mat, compute_uv = False)\n    W /= sum(W) # normalize singular values\n    entropy_svd = -1*sum(W * np.log2(W))\n    return(entropy_svd)", "response": "Computes the Singular Value Decomposition Entropy of a signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef complexity_fd_petrosian(signal):\n    diff = np.diff(signal)\n    # x[i] * x[i-1] for i in t0 -> tmax\n    prod = diff[1:-1] * diff[0:-2]\n\n    # Number of sign changes in derivative of the signal\n    N_delta = np.sum(prod < 0)\n    n = len(signal)\n    fd_petrosian = np.log(n)/(np.log(n)+np.log(n/(n+0.4*N_delta)))\n    return(fd_petrosian)", "response": "Calculates the Petrosian FD of a signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the Fisher information of a signal.", "response": "def complexity_fisher_info(signal, tau=1, emb_dim=2):\n    \"\"\"\n    Computes the Fisher information of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann.\n\n    Parameters\n    ----------\n    signal : list or array\n        List or array of values.\n    tau : int\n        The delay\n    emb_dim : int\n        The embedding dimension (*m*, the length of vectors to compare).\n\n    Returns\n    ----------\n    fisher_info : float\n        The Fisher information as float value.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> signal = np.sin(np.log(np.random.sample(666)))\n    >>> fisher_info = nk.complexity_fisher_info(signal, 1, 2)\n\n    Notes\n    ----------\n    *Details*\n\n    - **Fisher Information**:  A way of measuring the amount of information that an observable random variable X carries about an unknown parameter \u03b8 of a distribution that models X. Formally, it is the variance of the score, or the expected value of the observed information.\n\n    *Authors*\n\n    - Quentin Geissmann (https://github.com/qgeissmann)\n\n    *Dependencies*\n\n    - numpy\n\n    *See Also*\n\n    - pyrem package: https://github.com/gilestrolab/pyrem\n    \"\"\"\n\n    mat =  _embed_seq(signal, tau, emb_dim)\n    W = np.linalg.svd(mat, compute_uv = False)\n    W /= sum(W) # normalize singular values\n    FI_v = (W[1:] - W[:-1]) **2 / W[:-1]\n\n    fisher_info = np.sum(FI_v)\n    return(fisher_info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing GFP peaks for all electrodes in a given time point.", "response": "def eeg_gfp_peaks(data, gflp_method='GFPL1', smoothing=False, smoothing_window=100, peak_method=\"wavelet\", normalize=False):\n    \"\"\"\n    The Global Field Power (GFP) is a scalar measure of the strength of the scalp potential field and is calculated as the standard deviation of all electrodes at a given time point (Lehmann and Skrandies, 1980; Michel et al., 1993; Murray et al., 2008; Brunet et al., 2011). Between two GFP troughs, the strength of the potential field varies but the topography remains generally stable. The local maxima of the GFP are thus the best representative of a given microstate in terms of signal-to-noise ratio (Pascual-Marqui et al., 1995), corresponding to moments of high global neuronal synchronization (Skrandies, 2007).\n\n    Parameters\n    ----------\n    X (ndarray):\n        Array containing values for all time frames and channels.\n        Dimension: number of time frames x number of channels\n    method ({'GFPL1', 'GFPL2'}):\n        `GFPL1` : use L1-Norm to compute GFP peaks\n        `GFPL2` : use L2-Norm to compute GFP peaks\n    smoothing ({'hamming', 'hanning'}):\n            `hamming` : use hamming window to smooth\n            `hanning` : use hanning window to smooth\n    smoothing_window = int\n        about 100\n    peak_method = str\n        \"relative\" or \"wavelet\"\n\n    Returns\n    ----------\n        ret : ndarray\n            GFP curve\n    \"\"\"\n    ntf = data.shape[0]\n    gfp_curve = np.zeros((ntf, ))\n\n    if gflp_method == 'GFPL2':\n        for i in range(ntf):\n            x = data[i,:]\n            gfp_curve[i] = np.sqrt(np.sum((x - x.mean())**2 / len(x) ))\n    elif gflp_method == 'GFPL1':\n        for i in range(ntf):\n            x = data[i,:]\n            gfp_curve[i] = np.sum(np.abs(x - x.mean())) / len(x)\n\n    if peak_method == \"wavelet\":\n        gfp_peaks = np.asarray(scipy.signal.find_peaks_cwt(gfp_curve, np.arange(1, 10)))  #we would expect a peak at about each 50 ms\n    else:\n        gfp_peaks = scipy.signal.argrelmax(gfp_curve)[0]\n\n\n    if smoothing == 'hamming':\n        gfp_curve = scipy.signal.convolve(gfp_curve, scipy.signal.hamming(smoothing_window) )\n    elif smoothing == 'hanning':\n        gfp_curve = scipy.signal.convolve(gfp_curve, scipy.signal.hanning(smoothing_window) )\n    else:\n        pass\n\n    # Normalize\n    if normalize is True:\n        for i in range(len(data)):\n            data[i,:] = data[i,:]/gfp_curve[i]\n\n\n    return (data, gfp_curve, gfp_peaks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the GFP analysis on the raw object.", "response": "def eeg_gfp(raws, gflp_method=\"GFPL1\", scale=True, normalize=True, smoothing=None):\n    \"\"\"\n    Run the GFP analysis.\n    \"\"\"\n\n    # Load data if necessary\n#    if isinstance(raws, str):\n#        raws = load_object(filename=raws)\n\n    # Initialize empty dict\n    gfp = {}\n    for participant in raws:\n\n        gfp[participant] = {}\n        for run in raws[participant]:\n\n            # Generate empty dic\n            gfp[participant][run] = {}\n\n            # Assign raw object to raw\n            raw = raws[participant][run].copy()\n\n            # Check if MEG or EEG data\n            if True in set([\"MEG\" in ch for ch in raw.info[\"ch_names\"]]):\n                meg = True\n                eeg = False\n            else:\n                meg = False\n                eeg = True\n\n            # Save ECG channel\n            try:\n                gfp[participant][run][\"ecg\"] = np.array(raw.copy().pick_types(meg=False, eeg=False, ecg=True).to_data_frame())\n            except ValueError:\n                gfp[participant][run][\"ecg\"] = np.nan\n\n            # Select appropriate channels\n            data = raw.copy().pick_types(meg=meg, eeg=eeg)\n            gfp[participant][run][\"data_info\"] = data.info\n            gfp[participant][run][\"data_freq\"] = data.info[\"sfreq\"]\n            gfp[participant][run][\"run_duration\"] = len(data) / data.info[\"sfreq\"]\n\n            # Convert to numpy array\n            data = np.array(data.to_data_frame())\n\n            # find GFP peaks\n            data, gfp_curve, gfp_peaks = eeg_gfp_peaks(data,\n                                                 gflp_method=gflp_method,\n                                                 smoothing=smoothing,\n                                                 smoothing_window=100,\n                                                 peak_method=\"wavelet\",\n                                                 normalize=normalize)\n            # Store them\n            gfp[participant][run][\"microstates_times\"] = gfp_peaks\n\n\n            # Select brain state at peaks\n            data_peaks = data[gfp_peaks]\n\n            # Store the data and scale parameters\n            if scale is True:\n                gfp[participant][run][\"data\"] = z_score(data_peaks)\n            else:\n                gfp[participant][run][\"data\"] = data_peaks\n\n            gfp[participant][run][\"data_scale\"] = scale\n            gfp[participant][run][\"data_normalize\"] = normalize\n            gfp[participant][run][\"data_smoothing\"] = smoothing\n\n\n\n    return(gfp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the classifier and predict the more likely cluster for each observation.", "response": "def eeg_microstates_clustering(data, n_microstates=4, clustering_method=\"kmeans\", n_jobs=1, n_init=25, occurence_rejection_treshold=0.05, max_refitting=5, verbose=True):\n    \"\"\"\n    Fit the clustering algorithm.\n    \"\"\"\n    # Create training set\n    training_set = data.copy()\n\n    if verbose is True:\n        print(\"- Initializing the clustering algorithm...\")\n    if clustering_method == \"kmeans\":\n        algorithm = sklearn.cluster.KMeans(init='k-means++', n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n    elif clustering_method == \"spectral\":\n        algorithm = sklearn.cluster.SpectralClustering(n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n    elif clustering_method == \"agglom\":\n        algorithm = sklearn.cluster.AgglomerativeClustering(n_clusters=n_microstates, linkage=\"complete\")\n    elif clustering_method == \"dbscan\":\n        algorithm = sklearn.cluster.DBSCAN(min_samples=100)\n    elif clustering_method == \"affinity\":\n        algorithm = sklearn.cluster.AffinityPropagation(damping=0.5)\n    else:\n        print(\"NeuroKit Error: eeg_microstates(): clustering_method must be 'kmeans', 'spectral', 'dbscan', 'affinity' or 'agglom'\")\n\n\n    refitting = 0  # Initialize the number of refittings\n    good_fit_achieved = False\n    while good_fit_achieved is False:\n        good_fit_achieved = True\n        if verbose is True:\n            print(\"- Fitting the classifier...\")\n        # Fit the algorithm\n        algorithm.fit(training_set)\n\n        if verbose is True:\n            print(\"- Clustering back the initial data...\")\n        # Predict the more likely cluster for each observation\n        predicted = algorithm.fit_predict(training_set)\n\n        if verbose is True:\n            print(\"- Check for abnormalities...\")\n        # Check for abnormalities and prune the training set until none found\n        occurences = dict(collections.Counter(predicted))\n        masks = [np.array([True]*len(training_set))]\n        for microstate in occurences:\n            # is the frequency of one microstate inferior to a treshold\n            if occurences[microstate] < len(data)*occurence_rejection_treshold:\n                good_fit_achieved = False\n                refitting += 1  # Increment the refitting\n                print(\"NeuroKit Warning: eeg_microstates(): detected some outliers: refitting the classifier (n=\" + str(refitting) + \").\")\n                masks.append(predicted!=microstate)\n        mask = np.all(masks, axis=0)\n        training_set = training_set[mask]\n\n    return(algorithm)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eeg_microstates_features(results, method, ecg=True, nonlinearity=True, verbose=True):\n\n    for participant in results:\n        for run in results[participant]:\n            if verbose is True:\n                print(\"- \" + participant)\n\n            # Frequencies of each microstate\n            occurences = dict(collections.Counter(results[participant][run][\"microstates\"]))\n\n            # Compute complexity measures of the microstate sequence\n            if nonlinearity is True:\n                results[participant][run][\"nonlinearity\"] = complexity(results[participant][run][\"microstates\"])\n\n            # ECG coherence\n#            results[participant][run][\"ecg\"]\n#            statsmodels.tsa.stattools.grangercausalitytests([])\n\n            results[participant][run][\"parameters\"] = {}\n            # Compute parameters for each microstates:\n            for microstate in set(method[\"microstates\"]):\n\n                results[participant][run][\"parameters\"][microstate] = {}\n\n                try:\n                    # Coverage\n                    results[participant][run][\"parameters\"][microstate][\"coverage\"] = occurences[microstate]/len(results[participant][run][\"microstates\"])\n\n                    # Duration\n                    uniques = find_following_duplicates(results[participant][run][\"microstates\"])\n                    uniques_times = results[participant][run][\"microstates_times\"][np.where(uniques)]\n                    uniques_ms = results[participant][run][\"microstates\"][np.where(uniques)]\n                    times = uniques_times[np.array(np.where(uniques_ms==microstate))]\n                    times_1 = np.take(uniques_times, np.array(np.where(uniques_ms==microstate)) + 1, mode='clip')\n                    results[participant][run][\"parameters\"][microstate][\"duration\"] = np.mean(times_1 - times)/results[participant][run][\"data_sfreq\"]*1000\n\n                    # Occurence\n                    results[participant][run][\"parameters\"][microstate][\"occurence\"] = occurences[microstate] / results[participant][run][\"run_duration\"]\n\n                except KeyError:\n                    results[participant][run][\"parameters\"][microstate][\"coverage\"] = 0\n                    results[participant][run][\"parameters\"][microstate][\"duration\"] = np.nan\n                    results[participant][run][\"parameters\"][microstate][\"occurence\"] = 0\n    return(results)", "response": "Compute statistics and features for each microstate sequence in the specified method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the microstates analysis of a single game.", "response": "def eeg_microstates(gfp, n_microstates=4, clustering_method=\"kmeans\", n_jobs=1, n_init=25, occurence_rejection_treshold=0.05, max_refitting=5, clustering_metrics=True, good_fit_treshold=0, feature_reduction_method=\"PCA\", n_features=32, nonlinearity=True, verbose=True):\n    \"\"\"\n    Run the full microstates analysis.\n\n    Parameters\n    ----------\n    raws = dict\n        Two levels dictionary containing the participants, within which the run(s), associated with an mne.io.Raw class object.\n    method ({'GFPL1', 'GFPL2'}):\n        `GFPL1` : use L1-Norm to compute GFP peaks\n        `GFPL2` : use L2-Norm to compute GFP peaks\n    smoothing ({'hamming', 'hanning'}):\n            `hamming` : use hamming window to smooth\n            `hanning` : use hanning window to smooth\n    smoothing_window = int\n        about 100\n    peak_method = str\n        \"relative\" or \"wavelet\"\n\n    Returns\n    ----------\n        ret : ndarray\n            GFP curve\n\n    Example\n    ----------\n    NA\n\n    Authors\n    ----------\n    Dominique Makowski\n\n    Dependencies\n    ----------\n    - pygame 1.9.2\n    \"\"\"\n#    if isinstance(gfp, str):\n#        results = load_object(filename=gfp)\n#    else:\n#        results = gfp\n\n    if verbose is True:\n        print(\"\"\"\n    STARTING MICROSTATES ANALYSIS...\n    # ===================================\n    Infering microstates pattern from all data points...\n    # -----------------------------------------\n        \"\"\")\n\n    # Create empty dict to store all info about the method used\n    method = {}\n\n    # Merge all data at peaks\n    # Also, extract some info about the GFP method\n    data_all = []\n    for participant in results:\n        for run in results[participant]:\n            data_all.append(results[participant][run][\"data\"])\n\n            # GFP method\n            method[\"data_scale\"] = results[participant][run][\"data_scale\"]\n            method[\"data_normalize\"] = results[participant][run][\"data_normalize\"]\n            method[\"data_smoothing\"] = results[participant][run][\"data_smoothing\"]\n    data_all = np.concatenate(data_all, axis=0)\n\n\n    # Feature reduction\n    if verbose is True:\n        print(\"- Applying Feature Reduction...\")\n    data_processed = feature_reduction(data_all,\n                                       method=feature_reduction_method,\n                                       n_features=n_features)\n\n    try:\n        # Fit clustering aglorithm\n        algorithm = eeg_microstates_clustering(data=data_processed,\n                                   n_microstates=n_microstates,\n                                   clustering_method=clustering_method,\n                                   n_jobs=n_jobs,\n                                   n_init=n_init,\n                                   occurence_rejection_treshold=occurence_rejection_treshold,\n                                   max_refitting=max_refitting,\n                                   verbose=verbose)\n    except:\n        print(\"NeuroKit Error: eeg_microstates(): error in clustering.\")\n        return(data_processed, method)\n\n\n    if verbose is True:\n        print(\"- Storing the algorithm...\")\n    # Store results on a global level\n\n    method[\"algorithm\"] = algorithm\n    method[\"raw.info_example\"] = results[participant][run][\"data_info\"]  # Take the info of the last participant nad last run\n    method[\"feature_reduction_method\"] = feature_reduction_method\n    method[\"n_features\"] = n_features\n    method[\"data\"] = data_all\n    method[\"clustering_method\"] = clustering_method\n    method[\"n_microstates\"] = len(data_all)\n\n    if verbose is True:\n        print(\"\"\"\n    Computing microstates features on a global level...\n    # ----------------------------------------------------\n        \"\"\")\n    # Predict the more likely cluster for each observation on the initial set\n    predicted = algorithm.fit_predict(data_processed)\n    method[\"microstates\"] = predicted\n\n\n\n    # -------------------------------------------------------------------------\n    if clustering_metrics is True:\n        if verbose is True:\n            print(\"\"\"\n    Computing microstates metrics...\n    # ----------------------------------------------------\n            \"\"\")\n        # Get metrics about the clustering\n        method[\"silhouette_coefs\"] = sklearn.metrics.silhouette_samples(data_processed, predicted)  #  compute the silhouette coefficient for each data point\n        method[\"calinski_harabaz\"] = sklearn.metrics.calinski_harabaz_score(data_processed, predicted)\n\n        # Mark as Bad the data points with bad fit index\n        method[\"microstates_good_fit\"] = np.where(method[\"silhouette_coefs\"]>good_fit_treshold, predicted, \"Bad\")\n        method[\"percentage_bad_fit\"] = dict(collections.Counter(method[\"microstates_good_fit\"]))[\"Bad\"]/len(predicted)\n\n\n    # -------------------------------------------------------------------------\n    if verbose is True:\n        print(\"\"\"\n    Computing microstates features on a local level...\n    # ---------------------------------------------------\n        \"\"\")\n    # Store results for each run\n    # Generate index attributing each point of the predicted data to its subject number\n    index_participant = []\n    index_run = []\n    for participant in results:\n        for run in results[participant]:\n            index_participant += [participant] * len(results[participant][run][\"data\"])\n            index_run += [run] * len(results[participant][run][\"data\"])\n\n    # For each subject, select the appropriate part of the predicted sequence and store it\n    for participant in results:\n        for run in results[participant]:\n            # Create appropriate bool masks, combine them then select appropriate chunk of data\n            mask1 = np.array(index_participant)==participant\n            mask2 = np.array(index_run)==run\n            mask = np.all([mask1, mask2], axis=0)\n            results[participant][run][\"microstates\"] = method[\"microstates\"][mask]\n\n    results = eeg_microstates_features(results, method, nonlinearity=nonlinearity, verbose=verbose)\n\n    if verbose is True:\n        print(\"\"\"\n    Done.\n    # ------\n        \"\"\")\n    return(results, method)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the microstates of the current state.", "response": "def eeg_microstates_plot(method, path=\"\", extension=\".png\", show_sensors_position=False, show_sensors_name=False, plot=True, save=True, dpi=150, contours=0, colorbar=False, separate=False):\n    \"\"\"\n    Plot the microstates.\n    \"\"\"\n    # Generate and store figures\n    figures = []\n    names = []\n\n    # Check if microstates metrics available\n    try:\n        microstates = method[\"microstates_good_fit\"]\n    except KeyError:\n        microstates = method[\"microstates\"]\n\n    # Create individual plot for each microstate\n    for microstate in set(microstates):\n        if microstate != \"Bad\":\n            values = np.mean(method[\"data\"][np.where(microstates == microstate)], axis=0)\n            values = np.array(values, ndmin=2).T\n            evoked = mne.EvokedArray(values, method[\"raw.info_example\"], 0)\n\n            fig = evoked.plot_topomap(times=0, title=microstate, size=6, contours=contours, time_format=\"\", show=plot, colorbar=colorbar, show_names=show_sensors_name, sensors=show_sensors_position)\n\n            figures.append(fig)\n\n            # Save separate figures\n            name = path + \"microstate_%s_%s%s%s_%s%i_%s%s\" %(microstate, method[\"data_scale\"],  method[\"data_normalize\"], method[\"data_smoothing\"], method[\"feature_reduction_method\"], method[\"n_features\"], method[\"clustering_method\"], extension)\n\n            fig.savefig(name, dpi=dpi)\n            names.append(name)\n\n    # Save Combined plot\n    if save is True:\n        # Combine all plots\n        image_template = PIL.Image.open(names[0])\n        X, Y = image_template.size\n        image_template.close()\n\n        combined = PIL.Image.new('RGB', (int(X*len(set(microstates))/2), int( Y*len(set(microstates))/2)))\n        fig = 0\n        for x in np.arange(0, len(set(microstates))/2*int(X), int(X)):\n            for y in np.arange(0, len(set(microstates))/2*int(Y), int(Y)):\n                try:\n                    newfig = PIL.Image.open(names[fig])\n                    combined.paste(newfig, (int(x), int(y)))\n                    newfig.close()\n                except:\n                    pass\n                fig += 1\n#combined.show()\n        combined_name = path + \"microstates_%s%s%s_%s%i_%s%s\" %(method[\"data_scale\"],  method[\"data_normalize\"], method[\"data_smoothing\"], method[\"feature_reduction_method\"], method[\"n_features\"], method[\"clustering_method\"], extension)\n        combined.save(combined_name)\n\n    # Detete separate plots in needed\n    if separate is False or save is False:\n        for name in names:\n            os.remove(name)\n\n    return(figures)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eeg_microstates_relabel(method, results, microstates_labels, reverse_microstates=None):\n\n    microstates = list(method['microstates'])\n\n    for index, microstate in enumerate(method['microstates']):\n\n        if microstate in list(reverse_microstates.keys()):\n            microstates[index] = reverse_microstates[microstate]\n            method[\"data\"][index] = -1*method[\"data\"][index]\n\n        if microstate in list(microstates_labels.keys()):\n            microstates[index] = microstates_labels[microstate]\n\n    method['microstates'] = np.array(microstates)\n\n    return(results, method)", "response": "Relabel the microstates in the Eeg method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getWaveletData(eda):\n    '''\n    This function computes the wavelet coefficients\n    INPUT:\n        data:           DataFrame, index is a list of timestamps at 8Hz, columns include EDA, filtered_eda\n    OUTPUT:\n        wave1Second:    DateFrame, index is a list of timestamps at 1Hz, columns include OneSecond_feature1, OneSecond_feature2, OneSecond_feature3\n        waveHalfSecond: DateFrame, index is a list of timestamps at 2Hz, columns include HalfSecond_feature1, HalfSecond_feature2\n    '''\n\n    # Create wavelet dataframes\n    oneSecond =\n    halfSecond =\n\n    # Compute wavelets\n    cA_n, cD_3, cD_2, cD_1 = pywt.wavedec(eda, 'Haar', level=3) #3 = 1Hz, 2 = 2Hz, 1=4Hz\n\n    # Wavelet 1 second window\n    N = int(len(eda)/sampling_rate)\n    coeff1 = np.max(abs(np.reshape(cD_1[0:4*N],(N,4))), axis=1)\n    coeff2 = np.max(abs(np.reshape(cD_2[0:2*N],(N,2))), axis=1)\n    coeff3 = abs(cD_3[0:N])\n    wave1Second = pd.DataFrame({'OneSecond_feature1':coeff1,'OneSecond_feature2':coeff2,'OneSecond_feature3':coeff3})\n    wave1Second.index = oneSecond[:len(wave1Second)]\n\n    # Wavelet Half second window\n    N = int(np.floor((len(data)/8.0)*2))\n    coeff1 = np.max(abs(np.reshape(cD_1[0:2*N],(N,2))),axis=1)\n    coeff2 = abs(cD_2[0:N])\n    waveHalfSecond = pd.DataFrame({'HalfSecond_feature1':coeff1,'HalfSecond_feature2':coeff2})\n    waveHalfSecond.index = halfSecond[:len(waveHalfSecond)]\n\n    return wave1Second,waveHalfSecond", "response": "This function computes the wavelet coefficients of the current language of the current language of the current language of the current language of the current language of the current language of the current language of the current language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nautomating processing of bio signals. Wrapper for other bio processing functions. Parameters ---------- ecg : list or array ECG signal array. rsp : list or array Respiratory signal array. eda : list or array EDA signal array. emg : list, array or DataFrame EMG signal array. Can include multiple channels. add : pandas.DataFrame Dataframe or channels to add by concatenation to the processed dataframe. sampling_rate : int Sampling rate (samples/second). age : float Subject's age. sex : str Subject's gender (\"m\" or \"f\"). position : str Recording position. To compare with data from Voss et al. (2015), use \"supine\". ecg_filter_type : str Can be Finite Impulse Response filter (\"FIR\"), Butterworth filter (\"butter\"), Chebyshev filters (\"cheby1\" and \"cheby2\"), Elliptic filter (\"ellip\") or Bessel filter (\"bessel\"). ecg_filter_band : str Band type, can be Low-pass filter (\"lowpass\"), High-pass filter (\"highpass\"), Band-pass filter (\"bandpass\"), Band-stop filter (\"bandstop\"). ecg_filter_frequency : int or list Cutoff frequencies, format depends on type of band: \"lowpass\" or \"bandpass\": single frequency (int), \"bandpass\" or \"bandstop\": pair of frequencies (list). ecg_quality_model : str Path to model used to check signal quality. \"default\" uses the builtin model. None to skip this function. ecg_hrv_features : list What HRV indices to compute. Any or all of 'time', 'frequency' or 'nonlinear'. None to skip this function. ecg_segmenter : str The cardiac phase segmenter. Can be \"hamilton\", \"gamboa\", \"engzee\", \"christov\" or \"ssf\". See :func:`neurokit.ecg_preprocess()` for details. eda_alpha : float cvxEDA penalization for the sparse SMNA driver. eda_gamma : float cvxEDA penalization for the tonic spline coefficients. scr_method : str SCR extraction algorithm. \"makowski\" (default), \"kim\" (biosPPy's default; See Kim et al., 2004) or \"gamboa\" (Gamboa, 2004). scr_treshold : float SCR minimum treshold (in terms of signal standart deviation). emg_names : list List of EMG channel names. Returns ---------- processed_bio : dict Dict containing processed bio features. Contains the ECG raw signal, the filtered signal, the R peaks indexes, HRV characteristics, all the heartbeats, the Heart Rate, and the RSP filtered signal (if respiration provided), respiratory sinus arrhythmia (RSA) features, the EDA raw signal, the filtered signal, the phasic component (if cvxEDA is True), the SCR onsets, peak indexes and amplitudes, the EMG raw signal, the filtered signal and pulse onsets. Example ---------- >>> import neurokit as nk >>> >>> bio_features = nk.bio_process(ecg=ecg_signal, rsp=ecg_signal, eda=eda_signal) Notes ---------- *Details* - **ECG Features**: See :func:`neurokit.ecg_process()`. - **EDA Features**: See :func:`neurokit.eda_process()`. - **RSP Features**: See :func:`neurokit.rsp_process()`. - **EMG Features**: See :func:`neurokit.emg_process()`. *Authors* - `Dominique Makowski <https://dominiquemakowski.github.io/>`_ *Dependencies* - pandas *See Also* - BioSPPY: https://github.com/PIA-Group/BioSPPy - hrv: https://github.com/rhenanbartels/hrv - cvxEDA: https://github.com/lciti/cvxEDA References ----------- - Heart rate variability. (1996). Standards of measurement, physiological interpretation, and clinical use. Task Force of the European Society of Cardiology and the North American Society of Pacing and Electrophysiology. Eur Heart J, 17, 354-381. - Voss, A., Schroeder, R., Heitmann, A., Peters, A., & Perz, S. (2015). Short-term heart rate variability\u2014influence of gender and age in healthy subjects. PloS one, 10(3), e0118308. - Greco, A., Valenza, G., & Scilingo, E. P. (2016). Evaluation of CDA and CvxEDA Models. In Advances in Electrodermal Activity Processing with Applications for Mental Health (pp. 35-43). Springer International Publishing. - Greco, A., Valenza, G., Lanata, A., Scilingo, E. P., & Citi, L. (2016). cvxEDA: A convex optimization approach to electrodermal activity processing. IEEE Transactions on Biomedical Engineering, 63(4), 797-804. - Zohar, A. H., Cloninger, C. R., & McCraty, R. (2013). Personality and heart rate variability: exploring pathways from personality to cardiac coherence and health. Open Journal of Social Sciences, 1(06), 32. - Smith, A. L., Owen, H., & Reynolds, K. J. (2013). Heart rate variability indices for very short-term (30 beat) analysis. Part 2: validation. Journal of clinical monitoring and computing, 27(5), 577-585. - Azevedo, R. T., Garfinkel, S. N., Critchley, H. D., & Tsakiris, M. (2017). Cardiac afferent activity modulates the expression of racial stereotypes. Nature communications, 8. - Edwards, L., Ring, C., McIntyre, D., & Carroll, D. (2001). Modulation of the human nociceptive flexion reflex across the cardiac cycle. Psychophysiology, 38(4), 712-718. - Gray, M. A., Rylander, K., Harrison, N. A., Wallin, B. G., & Critchley, H. D. (2009). Following one's heart: cardiac rhythms gate central initiation of sympathetic reflexes. Journal of Neuroscience, 29(6), 1817-1825. - Kim, K. H., Bang, S. W., & Kim, S. R. (2004). Emotion recognition system using short-term monitoring of physiological signals. Medical and biological engineering and computing, 42(3), 419-427. - Gamboa, H. (2008). Multi-Modal Behavioral Biometrics Based on HCI and Electrophysiology (Doctoral dissertation, PhD thesis, Universidade T\u00e9cnica de Lisboa, Instituto Superior T\u00e9cnico).", "response": "def bio_process(ecg=None, rsp=None, eda=None, emg=None, add=None, sampling_rate=1000, age=None, sex=None, position=None, ecg_filter_type=\"FIR\", ecg_filter_band=\"bandpass\", ecg_filter_frequency=[3, 45], ecg_segmenter=\"hamilton\", ecg_quality_model=\"default\", ecg_hrv_features=[\"time\", \"frequency\"], eda_alpha=8e-4, eda_gamma=1e-2, scr_method=\"makowski\", scr_treshold=0.1, emg_names=None, emg_envelope_freqs=[10, 400], emg_envelope_lfreq=4, emg_activation_treshold=\"default\", emg_activation_n_above=0.25, emg_activation_n_below=1):\n    \"\"\"\n    Automated processing of bio signals. Wrapper for other bio processing functions.\n\n    Parameters\n    ----------\n    ecg : list or array\n        ECG signal array.\n    rsp : list or array\n        Respiratory signal array.\n    eda :  list or array\n        EDA signal array.\n    emg :  list, array or DataFrame\n        EMG signal array. Can include multiple channels.\n    add : pandas.DataFrame\n        Dataframe or channels to add by concatenation to the processed dataframe.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    age : float\n        Subject's age.\n    sex : str\n        Subject's gender (\"m\" or \"f\").\n    position : str\n        Recording position. To compare with data from Voss et al. (2015), use \"supine\".\n    ecg_filter_type : str\n        Can be Finite Impulse Response filter (\"FIR\"), Butterworth filter (\"butter\"), Chebyshev filters (\"cheby1\" and \"cheby2\"), Elliptic filter (\"ellip\") or Bessel filter (\"bessel\").\n    ecg_filter_band : str\n        Band type, can be Low-pass filter (\"lowpass\"), High-pass filter (\"highpass\"), Band-pass filter (\"bandpass\"), Band-stop filter (\"bandstop\").\n    ecg_filter_frequency : int or list\n        Cutoff frequencies, format depends on type of band: \"lowpass\" or \"bandpass\": single frequency (int), \"bandpass\" or \"bandstop\": pair of frequencies (list).\n    ecg_quality_model : str\n        Path to model used to check signal quality. \"default\" uses the builtin model. None to skip this function.\n    ecg_hrv_features : list\n        What HRV indices to compute. Any or all of 'time', 'frequency' or 'nonlinear'. None to skip this function.\n    ecg_segmenter : str\n        The cardiac phase segmenter. Can be \"hamilton\", \"gamboa\", \"engzee\", \"christov\" or \"ssf\". See :func:`neurokit.ecg_preprocess()` for details.\n    eda_alpha : float\n        cvxEDA penalization for the sparse SMNA driver.\n    eda_gamma : float\n        cvxEDA penalization for the tonic spline coefficients.\n    scr_method : str\n        SCR extraction algorithm. \"makowski\" (default), \"kim\" (biosPPy's default; See Kim et al., 2004) or \"gamboa\" (Gamboa, 2004).\n    scr_treshold : float\n        SCR minimum treshold (in terms of signal standart deviation).\n    emg_names : list\n        List of EMG channel names.\n\n\n    Returns\n    ----------\n    processed_bio : dict\n        Dict containing processed bio features.\n\n        Contains the ECG raw signal, the filtered signal, the R peaks indexes, HRV characteristics, all the heartbeats, the Heart Rate, and the RSP filtered signal (if respiration provided), respiratory sinus arrhythmia (RSA) features, the EDA raw signal, the filtered signal, the phasic component (if cvxEDA is True), the SCR onsets, peak indexes and amplitudes, the EMG raw signal, the filtered signal and pulse onsets.\n\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> bio_features = nk.bio_process(ecg=ecg_signal, rsp=ecg_signal, eda=eda_signal)\n\n    Notes\n    ----------\n    *Details*\n\n    - **ECG Features**: See :func:`neurokit.ecg_process()`.\n    - **EDA Features**: See :func:`neurokit.eda_process()`.\n    - **RSP Features**: See :func:`neurokit.rsp_process()`.\n    - **EMG Features**: See :func:`neurokit.emg_process()`.\n\n\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - pandas\n\n    *See Also*\n\n    - BioSPPY: https://github.com/PIA-Group/BioSPPy\n    - hrv: https://github.com/rhenanbartels/hrv\n    - cvxEDA: https://github.com/lciti/cvxEDA\n\n    References\n    -----------\n    - Heart rate variability. (1996). Standards of measurement, physiological interpretation, and clinical use. Task Force of the European Society of Cardiology and the North American Society of Pacing and Electrophysiology. Eur Heart J, 17, 354-381.\n    - Voss, A., Schroeder, R., Heitmann, A., Peters, A., & Perz, S. (2015). Short-term heart rate variability\u2014influence of gender and age in healthy subjects. PloS one, 10(3), e0118308.\n    - Greco, A., Valenza, G., & Scilingo, E. P. (2016). Evaluation of CDA and CvxEDA Models. In Advances in Electrodermal Activity Processing with Applications for Mental Health (pp. 35-43). Springer International Publishing.\n    - Greco, A., Valenza, G., Lanata, A., Scilingo, E. P., & Citi, L. (2016). cvxEDA: A convex optimization approach to electrodermal activity processing. IEEE Transactions on Biomedical Engineering, 63(4), 797-804.\n    - Zohar, A. H., Cloninger, C. R., & McCraty, R. (2013). Personality and heart rate variability: exploring pathways from personality to cardiac coherence and health. Open Journal of Social Sciences, 1(06), 32.\n    - Smith, A. L., Owen, H., & Reynolds, K. J. (2013). Heart rate variability indices for very short-term (30 beat) analysis. Part 2: validation. Journal of clinical monitoring and computing, 27(5), 577-585.\n    - Azevedo, R. T., Garfinkel, S. N., Critchley, H. D., & Tsakiris, M. (2017). Cardiac afferent activity modulates the expression of racial stereotypes. Nature communications, 8.\n    - Edwards, L., Ring, C., McIntyre, D., & Carroll, D. (2001). Modulation of the human nociceptive flexion reflex across the cardiac cycle. Psychophysiology, 38(4), 712-718.\n    - Gray, M. A., Rylander, K., Harrison, N. A., Wallin, B. G., & Critchley, H. D. (2009). Following one's heart: cardiac rhythms gate central initiation of sympathetic reflexes. Journal of Neuroscience, 29(6), 1817-1825.\n    - Kim, K. H., Bang, S. W., & Kim, S. R. (2004). Emotion recognition system using short-term monitoring of physiological signals. Medical and biological engineering and computing, 42(3), 419-427.\n    - Gamboa, H. (2008). Multi-Modal Behavioral Biometrics Based on HCI and Electrophysiology (Doctoral dissertation, PhD thesis, Universidade T\u00e9cnica de Lisboa, Instituto Superior T\u00e9cnico).\n    \"\"\"\n    processed_bio = {}\n    bio_df = pd.DataFrame({})\n\n    # ECG & RSP\n    if ecg is not None:\n        ecg = ecg_process(ecg=ecg, rsp=rsp, sampling_rate=sampling_rate, filter_type=ecg_filter_type, filter_band=ecg_filter_band, filter_frequency=ecg_filter_frequency, segmenter=ecg_segmenter, quality_model=ecg_quality_model, hrv_features=ecg_hrv_features, age=age, sex=sex, position=position)\n        processed_bio[\"ECG\"] = ecg[\"ECG\"]\n        if rsp is not None:\n            processed_bio[\"RSP\"] = ecg[\"RSP\"]\n        bio_df = pd.concat([bio_df, ecg[\"df\"]], axis=1)\n\n    if rsp is not None and ecg is None:\n        rsp = rsp_process(rsp=rsp, sampling_rate=sampling_rate)\n        processed_bio[\"RSP\"] = rsp[\"RSP\"]\n        bio_df = pd.concat([bio_df, rsp[\"df\"]], axis=1)\n\n\n    # EDA\n    if eda is not None:\n        eda = eda_process(eda=eda, sampling_rate=sampling_rate, alpha=eda_alpha, gamma=eda_gamma, scr_method=scr_method, scr_treshold=scr_treshold)\n        processed_bio[\"EDA\"] = eda[\"EDA\"]\n        bio_df = pd.concat([bio_df, eda[\"df\"]], axis=1)\n\n    # EMG\n    if emg is not None:\n        emg = emg_process(emg=emg, sampling_rate=sampling_rate, emg_names=emg_names, envelope_freqs=emg_envelope_freqs, envelope_lfreq=emg_envelope_lfreq, activation_treshold=emg_activation_treshold, activation_n_above=emg_activation_n_above, activation_n_below=emg_activation_n_below)\n        bio_df = pd.concat([bio_df, emg.pop(\"df\")], axis=1)\n        for i in emg:\n            processed_bio[i] = emg[i]\n\n\n    if add is not None:\n        add = add.reset_index(drop=True)\n        bio_df = pd.concat([bio_df, add], axis=1)\n    processed_bio[\"df\"] = bio_df\n\n    return(processed_bio)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bio_EventRelated(epoch, event_length, window_post_ecg=0, window_post_rsp=4, window_post_eda=4, ecg_features=[\"Heart_Rate\", \"Cardiac_Phase\", \"RR_Interval\", \"RSA\", \"HRV\"]):\n    bio_response = {}\n\n    ECG_Response = ecg_EventRelated(epoch, event_length, window_post=window_post_ecg, features=ecg_features)\n    bio_response.update(ECG_Response)\n\n    RSP_Response = rsp_EventRelated(epoch, event_length, window_post=window_post_rsp)\n    bio_response.update(RSP_Response)\n\n    EDA_Response = eda_EventRelated(epoch, event_length, window_post=window_post_eda)\n    bio_response.update(EDA_Response)\n\n    return(bio_response)", "response": "This function extracts event - related BIOs from a dataframe of EDA and RSP and returns a new BIOs with the same event - related events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nautomating processing of ECG and RSP signals. Parameters ---------- ecg : list or ndarray ECG signal array. rsp : list or ndarray Respiratory (RSP) signal array. sampling_rate : int Sampling rate (samples/second). filter_type : str Can be Finite Impulse Response filter (\"FIR\"), Butterworth filter (\"butter\"), Chebyshev filters (\"cheby1\" and \"cheby2\"), Elliptic filter (\"ellip\") or Bessel filter (\"bessel\"). filter_band : str Band type, can be Low-pass filter (\"lowpass\"), High-pass filter (\"highpass\"), Band-pass filter (\"bandpass\"), Band-stop filter (\"bandstop\"). filter_frequency : int or list Cutoff frequencies, format depends on type of band: \"lowpass\" or \"bandpass\": single frequency (int), \"bandpass\" or \"bandstop\": pair of frequencies (list). segmenter : str The cardiac phase segmenter. Can be \"hamilton\", \"gamboa\", \"engzee\", \"christov\" or \"ssf\". See :func:`neurokit.ecg_preprocess()` for details. quality_model : str Path to model used to check signal quality. \"default\" uses the builtin model. None to skip this function. hrv_features : list What HRV indices to compute. Any or all of 'time', 'frequency' or 'nonlinear'. None to skip this function. age : float Subject's age for adjusted HRV. sex : str Subject's gender (\"m\" or \"f\") for adjusted HRV. position : str Recording position. To compare with data from Voss et al. (2015), use \"supine\". Returns ---------- processed_ecg : dict Dict containing processed ECG features. Contains the ECG raw signal, the filtered signal, the R peaks indexes, HRV features, all the heartbeats, the Heart Rate, the RSP filtered signal (if respiration provided) and the respiratory sinus arrhythmia (RSA). Example ---------- >>> import neurokit as nk >>> processed_ecg = nk.ecg_process(ecg_signal, resp_signal) Notes ---------- *Details* - **Cardiac Cycle**: A typical ECG showing a heartbeat consists of a P wave, a QRS complex and a T wave.The P wave represents the wave of depolarization that spreads from the SA-node throughout the atria. The QRS complex reflects the rapid depolarization of the right and left ventricles. Since the ventricles are the largest part of the heart, in terms of mass, the QRS complex usually has a much larger amplitude than the P-wave. The T wave represents the ventricular repolarization of the ventricles. On rare occasions, a U wave can be seen following the T wave. The U wave is believed to be related to the last remnants of ventricular repolarization. - **RSA**: Respiratory sinus arrhythmia (RSA) is a naturally occurring variation in heart rate that occurs during the breathing cycle, serving as a measure of parasympathetic nervous system activity. See :func:`neurokit.ecg_rsa()` for details. - **HRV**: Heart-Rate Variability (HRV) is a finely tuned measure of heart-brain communication, as well as a strong predictor of morbidity and death (Zohar et al., 2013). It describes the complex variation of beat-to-beat intervals mainly controlled by the autonomic nervous system (ANS) through the interplay of sympathetic and parasympathetic neural activity at the sinus node. In healthy subjects, the dynamic cardiovascular control system is characterized by its ability to adapt to physiologic perturbations and changing conditions maintaining the cardiovascular homeostasis (Voss, 2015). In general, the HRV is influenced by many several factors like chemical, hormonal and neural modulations, circadian changes, exercise, emotions, posture and preload. There are several procedures to perform HRV analysis, usually classified into three categories: time domain methods, frequency domain methods and non-linear methods. See :func:`neurokit.ecg_hrv()` for a description of indices. - **Adjusted HRV**: The raw HRV features are normalized :math:`(raw - Mcluster) / sd` according to the participant's age and gender. In data from Voss et al. (2015), HRV analysis was performed on 5-min ECG recordings (lead II and lead V2 simultaneously, 500 Hz sample rate) obtained in supine position after a 5\u201310 minutes resting phase. The cohort of healthy subjects consisted of 782 women and 1124 men between the ages of 25 and 74 years, clustered into 4 groups: YF (Female, Age = [25-49], n=571), YM (Male, Age = [25-49], n=744), EF (Female, Age = [50-74], n=211) and EM (Male, Age = [50-74], n=571). - **Systole/Diastole**: One prominent channel of body and brain communication is that conveyed by baroreceptors, pressure and stretch-sensitive receptors within the heart and surrounding arteries. Within each cardiac cycle, bursts of baroreceptor afferent activity encoding the strength and timing of each heartbeat are carried via the vagus and glossopharyngeal nerve afferents to the nucleus of the solitary tract. This is the principal route that communicates to the brain the dynamic state of the heart, enabling the representation of cardiovascular arousal within viscerosensory brain regions, and influence ascending neuromodulator systems implicated in emotional and motivational behaviour. Because arterial baroreceptors are activated by the arterial pulse pressure wave, their phasic discharge is maximal during and immediately after the cardiac systole, that is, when the blood is ejected from the heart, and minimal during cardiac diastole, that is, between heartbeats (Azevedo, 2017). - **ECG Signal Quality**: Using the PTB-Diagnostic dataset available from PhysioNet, we extracted all the ECG signals from the healthy participants, that contained 15 recording leads/subject. We extracted all cardiac cycles, for each lead, and downsampled them from 600 to 200 datapoints. Note that we dropped the 8 first values that were NaNs. Then, we fitted a neural network model on 2/3 of the dataset (that contains 134392 cardiac cycles) to predict the lead. Model evaluation was done on the remaining 1/3. The model show good performances in predicting the correct recording lead (accuracy=0.91, precision=0.91). In this function, this model is fitted on each cardiac cycle of the provided ECG signal. It returns the probable recording lead (the most common predicted lead), the signal quality of each cardiac cycle (the probability of belonging to the probable recording lead) and the overall signal quality (the mean of signal quality). See creation `scripts <https://github.com/neuropsychology/NeuroKit.py/tree/master/utils/ecg_signal_quality_model_creation>`_. *Authors* - `Dominique Makowski <https://dominiquemakowski.github.io/>`_ - Rhenan Bartels (https://github.com/rhenanbartels) *Dependencies* - biosppy - numpy - pandas *See Also* - BioSPPY: https://github.com/PIA-Group/BioSPPy - hrv: https://github.com/rhenanbartels/hrv - RHRV: http://rhrv.r-forge.r-project.org/ References ------------ - Heart rate variability. (1996). Standards of measurement, physiological interpretation, and clinical use. Task Force of the European Society of Cardiology and the North American Society of Pacing and Electrophysiology. Eur Heart J, 17, 354-381. - Voss, A., Schroeder, R., Heitmann, A., Peters, A., & Perz, S. (2015). Short-term heart rate variability\u2014influence of gender and age in healthy subjects. PloS one, 10(3), e0118308. - Zohar, A. H., Cloninger, C. R., & McCraty, R. (2013). Personality and heart rate variability: exploring pathways from personality to cardiac coherence and health. Open Journal of Social Sciences, 1(06), 32. - Smith, A. L., Owen, H., & Reynolds, K. J. (2013). Heart rate variability indices for very short-term (30 beat) analysis. Part 2: validation. Journal of clinical monitoring and computing, 27(5), 577-585. - Azevedo, R. T., Garfinkel, S. N., Critchley, H. D., & Tsakiris, M. (2017). Cardiac afferent activity modulates the expression of racial stereotypes. Nature communications, 8. - Edwards, L., Ring, C., McIntyre, D., & Carroll, D. (2001). Modulation of the human nociceptive flexion reflex across the cardiac cycle. Psychophysiology, 38(4), 712-718. - Gray, M. A., Rylander, K., Harrison, N. A., Wallin, B. G., & Critchley, H. D. (2009). Following one's heart: cardiac rhythms gate central initiation of sympathetic reflexes. Journal of Neuroscience, 29(6), 1817-1825.", "response": "def ecg_process(ecg, rsp=None, sampling_rate=1000, filter_type=\"FIR\", filter_band=\"bandpass\", filter_frequency=[3, 45], segmenter=\"hamilton\", quality_model=\"default\", hrv_features=[\"time\", \"frequency\"], age=None, sex=None, position=None):\n    \"\"\"\n    Automated processing of ECG and RSP signals.\n\n    Parameters\n    ----------\n    ecg : list or ndarray\n        ECG signal array.\n    rsp : list or ndarray\n        Respiratory (RSP) signal array.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    filter_type : str\n        Can be Finite Impulse Response filter (\"FIR\"), Butterworth filter (\"butter\"), Chebyshev filters (\"cheby1\" and \"cheby2\"), Elliptic filter (\"ellip\") or Bessel filter (\"bessel\").\n    filter_band : str\n        Band type, can be Low-pass filter (\"lowpass\"), High-pass filter (\"highpass\"), Band-pass filter (\"bandpass\"), Band-stop filter (\"bandstop\").\n    filter_frequency : int or list\n        Cutoff frequencies, format depends on type of band: \"lowpass\" or \"bandpass\": single frequency (int), \"bandpass\" or \"bandstop\": pair of frequencies (list).\n    segmenter : str\n        The cardiac phase segmenter. Can be \"hamilton\", \"gamboa\", \"engzee\", \"christov\" or \"ssf\". See :func:`neurokit.ecg_preprocess()` for details.\n    quality_model : str\n        Path to model used to check signal quality. \"default\" uses the builtin model. None to skip this function.\n    hrv_features : list\n        What HRV indices to compute. Any or all of 'time', 'frequency' or 'nonlinear'. None to skip this function.\n    age : float\n        Subject's age for adjusted HRV.\n    sex : str\n        Subject's gender (\"m\" or \"f\") for adjusted HRV.\n    position : str\n        Recording position. To compare with data from Voss et al. (2015), use \"supine\".\n\n    Returns\n    ----------\n    processed_ecg : dict\n        Dict containing processed ECG features.\n\n        Contains the ECG raw signal, the filtered signal, the R peaks indexes, HRV features, all the heartbeats, the Heart Rate, the RSP filtered signal (if respiration provided) and the respiratory sinus arrhythmia (RSA).\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> processed_ecg = nk.ecg_process(ecg_signal, resp_signal)\n\n    Notes\n    ----------\n    *Details*\n\n    - **Cardiac Cycle**: A typical ECG showing a heartbeat consists of a P wave, a QRS complex and a T wave.The P wave represents the wave of depolarization that spreads from the SA-node throughout the atria. The QRS complex reflects the rapid depolarization of the right and left ventricles. Since the ventricles are the largest part of the heart, in terms of mass, the QRS complex usually has a much larger amplitude than the P-wave. The T wave represents the ventricular repolarization of the ventricles. On rare occasions, a U wave can be seen following the T wave. The U wave is believed to be related to the last remnants of ventricular repolarization.\n    - **RSA**: Respiratory sinus arrhythmia (RSA) is a naturally occurring variation in heart rate that occurs during the breathing cycle, serving as a measure of parasympathetic nervous system activity. See :func:`neurokit.ecg_rsa()` for details.\n    - **HRV**: Heart-Rate Variability (HRV) is a finely tuned measure of heart-brain communication, as well as a strong predictor of morbidity and death (Zohar et al., 2013). It describes the complex variation of beat-to-beat intervals mainly controlled by the autonomic nervous system (ANS) through the interplay of sympathetic and parasympathetic neural activity at the sinus node. In healthy subjects, the dynamic cardiovascular control system is characterized by its ability to adapt to physiologic perturbations and changing conditions maintaining the cardiovascular homeostasis (Voss, 2015). In general, the HRV is influenced by many several factors like chemical, hormonal and neural modulations, circadian changes, exercise, emotions, posture and preload. There are several procedures to perform HRV analysis, usually classified into three categories: time domain methods, frequency domain methods and non-linear methods. See :func:`neurokit.ecg_hrv()` for a description of indices.\n    - **Adjusted HRV**: The raw HRV features are normalized :math:`(raw - Mcluster) / sd` according to the participant's age and gender. In data from Voss et al. (2015), HRV analysis was performed on 5-min ECG recordings (lead II and lead V2 simultaneously, 500 Hz sample rate) obtained in supine position after a 5\u201310 minutes resting phase. The cohort of healthy subjects consisted of 782 women and 1124 men between the ages of 25 and 74 years, clustered into 4 groups: YF (Female, Age = [25-49], n=571), YM (Male, Age = [25-49], n=744), EF (Female, Age = [50-74], n=211) and EM (Male, Age = [50-74], n=571).\n    - **Systole/Diastole**: One prominent channel of body and brain communication is that conveyed by baroreceptors, pressure and stretch-sensitive receptors within the heart and surrounding arteries. Within each cardiac cycle, bursts of baroreceptor afferent activity encoding the strength and timing of each heartbeat are carried via the vagus and glossopharyngeal nerve afferents to the nucleus of the solitary tract. This is the principal route that communicates to the brain the dynamic state of the heart, enabling the representation of cardiovascular arousal within viscerosensory brain regions, and influence ascending neuromodulator systems implicated in emotional and motivational behaviour. Because arterial baroreceptors are activated by the arterial pulse pressure wave, their phasic discharge is maximal during and immediately after the cardiac systole, that is, when the blood is ejected from the heart, and minimal during cardiac diastole, that is, between heartbeats (Azevedo, 2017).\n    - **ECG Signal Quality**: Using the PTB-Diagnostic dataset available from PhysioNet, we extracted all the ECG signals from the healthy participants, that contained 15 recording leads/subject. We extracted all cardiac cycles, for each lead, and downsampled them from 600 to 200 datapoints. Note that we dropped the 8 first values that were NaNs. Then, we fitted a neural network model on 2/3 of the dataset (that contains 134392 cardiac cycles) to predict the lead. Model evaluation was done on the remaining 1/3. The model show good performances in predicting the correct recording lead (accuracy=0.91, precision=0.91). In this function, this model is fitted on each cardiac cycle of the provided ECG signal. It returns the probable recording lead (the most common predicted lead), the signal quality of each cardiac cycle (the probability of belonging to the probable recording lead) and the overall signal quality (the mean of signal quality). See creation `scripts <https://github.com/neuropsychology/NeuroKit.py/tree/master/utils/ecg_signal_quality_model_creation>`_.\n\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n    - Rhenan Bartels (https://github.com/rhenanbartels)\n\n    *Dependencies*\n\n    - biosppy\n    - numpy\n    - pandas\n\n    *See Also*\n\n    - BioSPPY: https://github.com/PIA-Group/BioSPPy\n    - hrv: https://github.com/rhenanbartels/hrv\n    - RHRV: http://rhrv.r-forge.r-project.org/\n\n    References\n    ------------\n    - Heart rate variability. (1996). Standards of measurement, physiological interpretation, and clinical use. Task Force of the European Society of Cardiology and the North American Society of Pacing and Electrophysiology. Eur Heart J, 17, 354-381.\n    - Voss, A., Schroeder, R., Heitmann, A., Peters, A., & Perz, S. (2015). Short-term heart rate variability\u2014influence of gender and age in healthy subjects. PloS one, 10(3), e0118308.\n    - Zohar, A. H., Cloninger, C. R., & McCraty, R. (2013). Personality and heart rate variability: exploring pathways from personality to cardiac coherence and health. Open Journal of Social Sciences, 1(06), 32.\n    - Smith, A. L., Owen, H., & Reynolds, K. J. (2013). Heart rate variability indices for very short-term (30 beat) analysis. Part 2: validation. Journal of clinical monitoring and computing, 27(5), 577-585.\n    - Azevedo, R. T., Garfinkel, S. N., Critchley, H. D., & Tsakiris, M. (2017). Cardiac afferent activity modulates the expression of racial stereotypes. Nature communications, 8.\n    - Edwards, L., Ring, C., McIntyre, D., & Carroll, D. (2001). Modulation of the human nociceptive flexion reflex across the cardiac cycle. Psychophysiology, 38(4), 712-718.\n    - Gray, M. A., Rylander, K., Harrison, N. A., Wallin, B. G., & Critchley, H. D. (2009). Following one's heart: cardiac rhythms gate central initiation of sympathetic reflexes. Journal of Neuroscience, 29(6), 1817-1825.\n    \"\"\"\n    # Preprocessing\n    # =============\n    processed_ecg = ecg_preprocess(ecg,\n                                   sampling_rate=sampling_rate,\n                                   filter_type=filter_type,\n                                   filter_band=filter_band,\n                                   filter_frequency=filter_frequency,\n                                   segmenter=segmenter)\n\n    # Signal quality\n    # ===============\n    if quality_model is not None:\n        quality = ecg_signal_quality(cardiac_cycles=processed_ecg[\"ECG\"][\"Cardiac_Cycles\"], sampling_rate=sampling_rate, rpeaks=processed_ecg[\"ECG\"][\"R_Peaks\"], quality_model=quality_model)\n        processed_ecg[\"ECG\"].update(quality)\n        processed_ecg[\"df\"] = pd.concat([processed_ecg[\"df\"], quality[\"ECG_Signal_Quality\"]], axis=1)\n\n    # HRV\n    # =============\n    if hrv_features is not None:\n        hrv = ecg_hrv(rpeaks=processed_ecg[\"ECG\"][\"R_Peaks\"], sampling_rate=sampling_rate, hrv_features=hrv_features)\n        try:\n            processed_ecg[\"df\"] = pd.concat([processed_ecg[\"df\"], hrv.pop(\"df\")], axis=1)\n        except KeyError:\n            pass\n        processed_ecg[\"ECG\"][\"HRV\"] = hrv\n        if age is not None and sex is not None and position is not None:\n            processed_ecg[\"ECG\"][\"HRV_Adjusted\"] = ecg_hrv_assessment(hrv, age, sex, position)\n\n    # RSP\n    # =============\n    if rsp is not None:\n        rsp = rsp_process(rsp=rsp, sampling_rate=sampling_rate)\n        processed_ecg[\"RSP\"] = rsp[\"RSP\"]\n        processed_ecg[\"df\"] = pd.concat([processed_ecg[\"df\"], rsp[\"df\"]], axis=1)\n\n        # RSA\n        # =============\n        rsa = ecg_rsa(processed_ecg[\"ECG\"][\"R_Peaks\"], rsp[\"df\"][\"RSP_Filtered\"], sampling_rate=sampling_rate)\n        processed_ecg[\"ECG\"][\"RSA\"] = rsa\n        processed_ecg[\"df\"] = pd.concat([processed_ecg[\"df\"], rsa.pop(\"df\")], axis=1)\n\n    return(processed_ecg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ecg_rsa(rpeaks, rsp, sampling_rate=1000):\n    # Preprocessing\n    # =================\n    rsp_cycles = rsp_find_cycles(rsp)\n    rsp_onsets = rsp_cycles[\"RSP_Cycles_Onsets\"]\n    rsp_cycle_center = rsp_cycles[\"RSP_Expiration_Onsets\"]\n    rsp_cycle_center = np.array(rsp_cycle_center)[rsp_cycle_center > rsp_onsets[0]]\n    if len(rsp_cycle_center) - len(rsp_onsets) == 0:\n        rsp_cycle_center = rsp_cycle_center[:-1]\n    if len(rsp_cycle_center) - len(rsp_onsets) != -1:\n        print(\"NeuroKit Error: ecg_rsp(): Couldn't find clean rsp cycles onsets and centers. Check your RSP signal.\")\n        return()\n    rsa = {}\n\n\n    # Peak-to-trough algorithm (P2T)\n    # ===============================\n    # Find all RSP cycles and the Rpeaks within\n    cycles_rri = []\n    for idx in range(len(rsp_onsets) - 1):\n        cycle_init = rsp_onsets[idx]\n        cycle_end = rsp_onsets[idx + 1]\n        cycles_rri.append(rpeaks[np.logical_and(rpeaks >= cycle_init,\n                                                rpeaks < cycle_end)])\n\n    # Iterate over all cycles\n    rsa[\"RSA_P2T_Values\"] = []\n    for cycle in cycles_rri:\n        RRis = np.diff(cycle)/sampling_rate\n        if len(RRis) > 1:\n            rsa[\"RSA_P2T_Values\"].append(np.max(RRis) - np.min(RRis))\n        else:\n            rsa[\"RSA_P2T_Values\"].append(np.nan)\n    rsa[\"RSA_P2T_Mean\"] = pd.Series(rsa[\"RSA_P2T_Values\"]).mean()\n    rsa[\"RSA_P2T_Mean_log\"] = np.log(rsa[\"RSA_P2T_Mean\"])\n    rsa[\"RSA_P2T_Variability\"] = pd.Series(rsa[\"RSA_P2T_Values\"]).std()\n\n    # Continuous RSA - Interpolation using a 3rd order spline\n    if len(rsp_cycle_center) - len(rsa[\"RSA_P2T_Values\"]) != 0:\n        print(\"NeuroKit Error: ecg_rsp(): Couldn't find clean rsp cycles onsets and centers. Check your RSP signal.\")\n        return()\n    values=pd.Series(rsa[\"RSA_P2T_Values\"])\n    NaNs_indices = values.index[values.isnull()]  # get eventual artifacts indices\n    values = values.drop(NaNs_indices)  # remove the artifacts\n    value_times=(np.array(rsp_cycle_center))\n    value_times = np.delete(value_times, NaNs_indices)  # delete also the artifacts from times indices\n\n    rsa_interpolated = interpolate(values=values, value_times=value_times, sampling_rate=sampling_rate)\n\n\n    # Continuous RSA - Steps\n    current_rsa = np.nan\n\n    continuous_rsa = []\n    phase_counter = 0\n    for i in range(len(rsp)):\n        if i == rsp_onsets[phase_counter]:\n            current_rsa = rsa[\"RSA_P2T_Values\"][phase_counter]\n            if phase_counter < len(rsp_onsets)-2:\n                phase_counter += 1\n        continuous_rsa.append(current_rsa)\n\n    # Find last phase\n    continuous_rsa = np.array(continuous_rsa)\n    continuous_rsa[max(rsp_onsets):] = np.nan\n\n    df = pd.DataFrame({\"RSP\":rsp})\n    df[\"RSA_Values\"] = continuous_rsa\n    df[\"RSA\"] = rsa_interpolated\n    rsa[\"df\"] = df\n\n    # Porges\u2013Bohrer method (RSAP\u2013B)\n    # ==============================\n    # Need help to implement this method (See Lewis, 2012)\n\n    return(rsa)", "response": "Returns a dict containing the Respiratory Sinus Arrhythmia features."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ecg_signal_quality(cardiac_cycles, sampling_rate, rpeaks=None, quality_model=\"default\"):\n    if len(cardiac_cycles) > 200:\n        cardiac_cycles = cardiac_cycles.rolling(20).mean().resample(\"3L\").pad()\n    if len(cardiac_cycles) < 200:\n        cardiac_cycles = cardiac_cycles.resample(\"1L\").pad()\n        cardiac_cycles = cardiac_cycles.rolling(20).mean().resample(\"3L\").pad()\n\n    if len(cardiac_cycles) < 200:\n        fill_dict = {}\n        for i in cardiac_cycles.columns:\n            fill_dict[i] = [np.nan] * (200-len(cardiac_cycles))\n        cardiac_cycles = pd.concat([pd.DataFrame(fill_dict), cardiac_cycles], ignore_index=True)\n\n    cardiac_cycles = cardiac_cycles.fillna(method=\"bfill\")\n    cardiac_cycles = cardiac_cycles.reset_index(drop=True)[8:200]\n    cardiac_cycles = z_score(cardiac_cycles).T\n    cardiac_cycles = np.array(cardiac_cycles)\n\n    if quality_model == \"default\":\n        model = sklearn.externals.joblib.load(Path.materials() + 'heartbeat_classification.model')\n    else:\n        model = sklearn.externals.joblib.load(quality_model)\n\n    # Initialize empty dict\n    quality = {}\n\n    # Find dominant class\n    lead = model.predict(cardiac_cycles)\n    lead = pd.Series(lead).value_counts().index[0]\n    quality[\"Probable_Lead\"] = lead\n\n    predict = pd.DataFrame(model.predict_proba(cardiac_cycles))\n    predict.columns = model.classes_\n    quality[\"Cardiac_Cycles_Signal_Quality\"] = predict[lead].values\n    quality[\"Average_Signal_Quality\"] = predict[lead].mean()\n\n    # Interpolate to get a continuous signal\n    if rpeaks is not None:\n        signal = quality[\"Cardiac_Cycles_Signal_Quality\"]\n        signal = interpolate(signal, rpeaks, sampling_rate)  # Interpolation using 3rd order spline\n        signal.name = \"ECG_Signal_Quality\"\n        quality[\"ECG_Signal_Quality\"] = signal\n\n    return(quality)", "response": "Returns a new node with the most common predicted lead and the signal quality of the provided ECG signal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ecg_hrv(rpeaks=None, rri=None, sampling_rate=1000, hrv_features=[\"time\", \"frequency\", \"nonlinear\"]):\n    # Check arguments: exactly one of rpeaks or rri has to be given as input\n    if rpeaks is None and rri is None:\n        raise ValueError(\"Either rpeaks or RRIs needs to be given.\")\n\n    if rpeaks is not None and rri is not None:\n        raise ValueError(\"Either rpeaks or RRIs should be given but not both.\")\n\n    # Initialize empty dict\n    hrv = {}\n\n    # Preprocessing\n    # ==================\n    # Extract RR intervals (RRis)\n    if rpeaks is not None:\n        # Rpeaks is given, RRis need to be computed\n        RRis = np.diff(rpeaks)\n    else:\n        # Case where RRis are already given:\n        RRis = rri\n\n\n    # Basic resampling to 1Hz to standardize the scale\n    RRis = RRis/sampling_rate\n    RRis = RRis.astype(float)\n\n\n    # Artifact detection - Statistical\n    for index, rr in enumerate(RRis):\n        # Remove RR intervals that differ more than 25% from the previous one\n        if RRis[index] < RRis[index-1]*0.75:\n            RRis[index] = np.nan\n        if RRis[index] > RRis[index-1]*1.25:\n            RRis[index] = np.nan\n\n    # Artifact detection - Physiological (http://emedicine.medscape.com/article/2172196-overview)\n    RRis = pd.Series(RRis)\n    RRis[RRis < 0.6] = np.nan\n    RRis[RRis > 1.3] = np.nan\n\n     # Sanity check\n    if len(RRis) <= 1:\n        print(\"NeuroKit Warning: ecg_hrv(): Not enough R peaks to compute HRV :/\")\n        return(hrv)\n\n    # Artifacts treatment\n    hrv[\"n_Artifacts\"] = pd.isnull(RRis).sum()/len(RRis)\n    artifacts_indices = RRis.index[RRis.isnull()]  # get the artifacts indices\n    RRis = RRis.drop(artifacts_indices)  # remove the artifacts\n\n\n    # Rescale to 1000Hz\n    RRis = RRis*1000\n    hrv[\"RR_Intervals\"] = RRis  # Values of RRis\n\n    # Sanity check after artifact removal\n    if len(RRis) <= 1:\n        print(\"NeuroKit Warning: ecg_hrv(): Not enough normal R peaks to compute HRV :/\")\n        return(hrv)\n\n    # Time Domain\n    # ==================\n    if \"time\" in hrv_features:\n        hrv[\"RMSSD\"] = np.sqrt(np.mean(np.diff(RRis) ** 2))\n        hrv[\"meanNN\"] = np.mean(RRis)\n        hrv[\"sdNN\"] = np.std(RRis, ddof=1)  # make it calculate N-1\n        hrv[\"cvNN\"] = hrv[\"sdNN\"] / hrv[\"meanNN\"]\n        hrv[\"CVSD\"] = hrv[\"RMSSD\"] / hrv[\"meanNN\"]\n        hrv[\"medianNN\"] = np.median(abs(RRis))\n        hrv[\"madNN\"] = mad(RRis, constant=1)\n        hrv[\"mcvNN\"] = hrv[\"madNN\"] / hrv[\"medianNN\"]\n        nn50 = sum(abs(np.diff(RRis)) > 50)\n        nn20 = sum(abs(np.diff(RRis)) > 20)\n        hrv[\"pNN50\"] = nn50 / len(RRis) * 100\n        hrv[\"pNN20\"] = nn20 / len(RRis) * 100\n\n\n\n\n\n\n    # Frequency Domain Preparation\n    # ==============================\n    if \"frequency\" in hrv_features:\n\n        # Interpolation\n        # =================\n        # Convert to continuous RR interval (RRi)\n        beats_times = rpeaks[1:].copy()  # the time at which each beat occured starting from the 2nd beat\n        beats_times -= list(beats_times)[0]  # So it starts at 0\n        beats_times = np.delete(list(beats_times), artifacts_indices)  # delete also the artifact beat moments\n\n        try:\n            RRi = interpolate(RRis, beats_times, sampling_rate)  # Interpolation using 3rd order spline\n        except TypeError:\n            print(\"NeuroKit Warning: ecg_hrv(): Sequence too short to compute interpolation. Will skip many features.\")\n            return(hrv)\n\n\n        hrv[\"df\"] = RRi.to_frame(\"ECG_RR_Interval\")  # Continuous (interpolated) signal of RRi\n\n\n\n        # Geometrical Method (actually part of time domain)\n        # =========================================\n        # TODO: This part needs to be checked by an expert. Also, it would be better to have Renyi entropy (a generalization of shannon's), but I don't know how to compute it.\n        try:\n            bin_number = 32  # Initialize bin_width value\n            # find the appropriate number of bins so the class width is approximately 8 ms (Voss, 2015)\n            for bin_number_current in range(2, 50):\n                bin_width = np.diff(np.histogram(RRi, bins=bin_number_current, density=True)[1])[0]\n                if abs(8 - bin_width) < abs(8 - np.diff(np.histogram(RRi, bins=bin_number, density=True)[1])[0]):\n                    bin_number = bin_number_current\n            hrv[\"Triang\"] = len(RRis)/np.max(np.histogram(RRi, bins=bin_number, density=True)[0])\n            hrv[\"Shannon_h\"] = complexity_entropy_shannon(np.histogram(RRi, bins=bin_number, density=True)[0])\n        except ValueError:\n            hrv[\"Triang\"] = np.nan\n            hrv[\"Shannon_h\"] = np.nan\n\n\n\n        # Frequency Domain Features\n        # ==========================\n        freq_bands = {\n          \"ULF\": [0.0001, 0.0033],\n          \"VLF\": [0.0033, 0.04],\n          \"LF\": [0.04, 0.15],\n          \"HF\": [0.15, 0.40],\n          \"VHF\": [0.4, 0.5]}\n\n\n        # Frequency-Domain Power over time\n        freq_powers = {}\n        for band in freq_bands:\n            freqs = freq_bands[band]\n            # Filter to keep only the band of interest\n            filtered, sampling_rate, params = biosppy.signals.tools.filter_signal(signal=RRi, ftype='butter', band='bandpass', order=1, frequency=freqs, sampling_rate=sampling_rate)\n            # Apply Hilbert transform\n            amplitude, phase = biosppy.signals.tools.analytic_signal(filtered)\n            # Extract Amplitude of Envelope (power)\n            freq_powers[\"ECG_HRV_\" + band] = amplitude\n\n        freq_powers = pd.DataFrame.from_dict(freq_powers)\n        freq_powers.index = hrv[\"df\"].index\n        hrv[\"df\"] = pd.concat([hrv[\"df\"], freq_powers], axis=1)\n\n\n        # Compute Power Spectral Density (PSD) using multitaper method\n        power, freq = mne.time_frequency.psd_array_multitaper(RRi, sfreq=sampling_rate, fmin=0, fmax=0.5,  adaptive=False, normalization='length')\n\n        def power_in_band(power, freq, band):\n            power =  np.trapz(y=power[(freq >= band[0]) & (freq < band[1])], x=freq[(freq >= band[0]) & (freq < band[1])])\n            return(power)\n\n        # Extract Power according to frequency bands\n        hrv[\"ULF\"] = power_in_band(power, freq, freq_bands[\"ULF\"])\n        hrv[\"VLF\"] = power_in_band(power, freq, freq_bands[\"VLF\"])\n        hrv[\"LF\"] = power_in_band(power, freq, freq_bands[\"LF\"])\n        hrv[\"HF\"] = power_in_band(power, freq, freq_bands[\"HF\"])\n        hrv[\"VHF\"] = power_in_band(power, freq, freq_bands[\"VHF\"])\n        hrv[\"Total_Power\"] = power_in_band(power, freq, [0, 0.5])\n\n        hrv[\"LFn\"] = hrv[\"LF\"]/(hrv[\"LF\"]+hrv[\"HF\"])\n        hrv[\"HFn\"] = hrv[\"HF\"]/(hrv[\"LF\"]+hrv[\"HF\"])\n        hrv[\"LF/HF\"] = hrv[\"LF\"]/hrv[\"HF\"]\n        hrv[\"LF/P\"] = hrv[\"LF\"]/hrv[\"Total_Power\"]\n        hrv[\"HF/P\"] = hrv[\"HF\"]/hrv[\"Total_Power\"]\n\n\n    # TODO: THIS HAS TO BE CHECKED BY AN EXPERT - Should it be applied on the interpolated on raw RRis?\n    # Non-Linear Dynamics\n    # ======================\n    if \"nonlinear\" in hrv_features:\n        if len(RRis) > 17:\n            hrv[\"DFA_1\"] = nolds.dfa(RRis, range(4, 17))\n        if len(RRis) > 66:\n            hrv[\"DFA_2\"] = nolds.dfa(RRis, range(16, 66))\n        hrv[\"Shannon\"] = complexity_entropy_shannon(RRis)\n        hrv[\"Sample_Entropy\"] = nolds.sampen(RRis, emb_dim=2)\n        try:\n            hrv[\"Correlation_Dimension\"] = nolds.corr_dim(RRis, emb_dim=2)\n        except AssertionError as error:\n            print(\"NeuroKit Warning: ecg_hrv(): Correlation Dimension. Error: \" + str(error))\n            hrv[\"Correlation_Dimension\"] = np.nan\n        mse = complexity_entropy_multiscale(RRis, max_scale_factor=20, m=2)\n        hrv[\"Entropy_Multiscale_AUC\"] = mse[\"MSE_AUC\"]\n        hrv[\"Entropy_SVD\"] = complexity_entropy_svd(RRis, emb_dim=2)\n        hrv[\"Entropy_Spectral_VLF\"] = complexity_entropy_spectral(RRis, sampling_rate, bands=np.arange(0.0033, 0.04, 0.001))\n        hrv[\"Entropy_Spectral_LF\"] = complexity_entropy_spectral(RRis, sampling_rate, bands=np.arange(0.04, 0.15, 0.001))\n        hrv[\"Entropy_Spectral_HF\"] = complexity_entropy_spectral(RRis, sampling_rate, bands=np.arange(0.15, 0.40, 0.001))\n        hrv[\"Fisher_Info\"] = complexity_fisher_info(RRis, tau=1, emb_dim=2)\n#        lyap exp doesn't work for some reasons\n#        hrv[\"Lyapunov\"] = np.max(nolds.lyap_e(RRis, emb_dim=58, matrix_dim=4))\n\n        hrv[\"FD_Petrosian\"] = complexity_fd_petrosian(RRis)\n        hrv[\"FD_Higushi\"] = complexity_fd_higushi(RRis, k_max=16)\n\n    # TO DO:\n    # Include many others (see Voss 2015)\n\n\n    return(hrv)", "response": "Returns a new base Heart - Rate Variability object for the given set of R - peaks and R - RRI intervals."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncorrects HRV features based on normative data from Voss et al. (2015). Parameters ---------- hrv : dict HRV features obtained by :function:`neurokit.ecg_hrv`. age : float Subject's age. sex : str Subject's gender (\"m\" or \"f\"). position : str Recording position. To compare with data from Voss et al. (2015), use \"supine\". Returns ---------- hrv_adjusted : dict Adjusted HRV features. Example ---------- >>> import neurokit as nk >>> hrv = nk.bio_ecg.ecg_hrv(rpeaks=rpeaks) >>> ecg_hrv_assessment = nk.bio_ecg.ecg_hrv_assessment(hrv) Notes ---------- *Authors* - `Dominique Makowski <https://dominiquemakowski.github.io/>`_ *Details* - **Adjusted HRV**: The raw HRV features are normalized :math:`(raw - Mcluster) / sd` according to the participant's age and gender. In data from Voss et al. (2015), HRV analysis was performed on 5-min ECG recordings (lead II and lead V2 simultaneously, 500 Hz sampling rate) obtained in supine position after a 5\u201310 minutes resting phase. The cohort of healthy subjects consisted of 782 women and 1124 men between the ages of 25 and 74 years, clustered into 4 groups: YF (Female, Age = [25-49], n=571), YM (Male, Age = [25-49], n=744), EF (Female, Age = [50-74], n=211) and EM (Male, Age = [50-74], n=571). References ----------- - Voss, A., Schroeder, R., Heitmann, A., Peters, A., & Perz, S. (2015). Short-term heart rate variability\u2014influence of gender and age in healthy subjects. PloS one, 10(3), e0118308.", "response": "def ecg_hrv_assessment(hrv, age=None, sex=None, position=None):\n    \"\"\"\n    Correct HRV features based on normative data from Voss et al. (2015).\n\n    Parameters\n    ----------\n    hrv : dict\n        HRV features obtained by :function:`neurokit.ecg_hrv`.\n    age : float\n        Subject's age.\n    sex : str\n        Subject's gender (\"m\" or \"f\").\n    position : str\n        Recording position. To compare with data from Voss et al. (2015), use \"supine\".\n\n\n    Returns\n    ----------\n    hrv_adjusted : dict\n        Adjusted HRV features.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> hrv = nk.bio_ecg.ecg_hrv(rpeaks=rpeaks)\n    >>> ecg_hrv_assessment = nk.bio_ecg.ecg_hrv_assessment(hrv)\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Details*\n\n    - **Adjusted HRV**: The raw HRV features are normalized :math:`(raw - Mcluster) / sd` according to the participant's age and gender. In data from Voss et al. (2015), HRV analysis was performed on 5-min ECG recordings (lead II and lead V2 simultaneously, 500 Hz sampling rate) obtained in supine position after a 5\u201310 minutes resting phase. The cohort of healthy subjects consisted of 782 women and 1124 men between the ages of 25 and 74 years, clustered into 4 groups: YF (Female, Age = [25-49], n=571), YM (Male, Age = [25-49], n=744), EF (Female, Age = [50-74], n=211) and EM (Male, Age = [50-74], n=571).\n\n\n    References\n    -----------\n    - Voss, A., Schroeder, R., Heitmann, A., Peters, A., & Perz, S. (2015). Short-term heart rate variability\u2014influence of gender and age in healthy subjects. PloS one, 10(3), e0118308.\n    \"\"\"\n    hrv_adjusted = {}\n\n    if position == \"supine\":\n        if sex == \"m\":\n            if age <= 49:\n                hrv_adjusted[\"meanNN_Adjusted\"] = (hrv[\"meanNN\"]-930)/133\n                hrv_adjusted[\"sdNN_Adjusted\"] = (hrv[\"sdNN\"]-45.8)/18.8\n                hrv_adjusted[\"RMSSD_Adjusted\"] = (hrv[\"RMSSD\"]-34.0)/18.3\n\n                hrv_adjusted[\"LF_Adjusted\"] = (hrv[\"LF\"]-203)/262\n                hrv_adjusted[\"HF_Adjusted\"] = (hrv[\"HF\"]-101)/143\n                hrv_adjusted[\"LF/HF_Adjusted\"] = (hrv[\"LF/HF\"]-3.33)/3.47\n            else:\n                hrv_adjusted[\"meanNN_Adjusted\"] = (hrv[\"meanNN\"]-911)/128\n                hrv_adjusted[\"sdNN_Adjusted\"] = (hrv[\"sdNN\"]-33.0)/14.8\n                hrv_adjusted[\"RMSSD_Adjusted\"] = (hrv[\"RMSSD\"]-20.5)/11.0\n\n                hrv_adjusted[\"LF_Adjusted\"] = (hrv[\"LF\"]-84)/115\n                hrv_adjusted[\"HF_Adjusted\"] = (hrv[\"HF\"]-29.5)/36.6\n                hrv_adjusted[\"LF/HF_Adjusted\"] = (hrv[\"LF/HF\"]-4.29)/4.06\n        if sex == \"f\":\n            if age <= 49:\n                hrv_adjusted[\"meanNN_Adjusted\"] = (hrv[\"meanNN\"]-901)/117\n                hrv_adjusted[\"sdNN_Adjusted\"] = (hrv[\"sdNN\"]-44.9)/19.2\n                hrv_adjusted[\"RMSSD_Adjusted\"] = (hrv[\"RMSSD\"]-36.5)/20.1\n\n                hrv_adjusted[\"LF_Adjusted\"] = (hrv[\"LF\"]-159)/181\n                hrv_adjusted[\"HF_Adjusted\"] = (hrv[\"HF\"]-125)/147\n                hrv_adjusted[\"LF/HF_Adjusted\"] = (hrv[\"LF/HF\"]-2.75)/2.93\n            else:\n                hrv_adjusted[\"meanNN_Adjusted\"] = (hrv[\"meanNN\"]-880)/115\n                hrv_adjusted[\"sdNN_Adjusted\"] = (hrv[\"sdNN\"]-31.6)/13.6\n                hrv_adjusted[\"RMSSD_Adjusted\"] = (hrv[\"RMSSD\"]-22.0)/13.2\n\n                hrv_adjusted[\"LF_Adjusted\"] = (hrv[\"LF\"]-66)/83\n                hrv_adjusted[\"HF_Adjusted\"] = (hrv[\"HF\"]-41.4)/72.1\n                hrv_adjusted[\"LF/HF_Adjusted\"] = (hrv[\"LF/HF\"]-2.09)/2.05\n\n    return(hrv_adjusted)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract event-related ECG changes. Parameters ---------- epoch : pandas.DataFrame An epoch contained in the epochs dict returned by :function:`neurokit.create_epochs()` on dataframe returned by :function:`neurokit.bio_process()`. Index should range from -4s to +4s (relatively to event onset and end). event_length : int Event length in seconds. window_post : float Post-stimulus window size (in seconds) to include late responses (usually 3 or 4). features : list List of ECG features to compute, can contain \"Heart_Rate\", \"Cardiac_Phase\", \"RR_Interval\", \"RSA\", \"HRV\". Returns ---------- ECG_Response : dict Event-related ECG response features. Example ---------- >>> import neurokit as nk >>> bio = nk.bio_process(ecg=data[\"ECG\"], rsp=data[\"RSP\"], eda=data[\"EDA\"], sampling_rate=1000, add=data[\"Photosensor\"]) >>> df = bio[\"df\"] >>> events = nk.find_events(df[\"Photosensor\"], cut=\"lower\") >>> epochs = nk.create_epochs(df, events[\"onsets\"], duration=7, onset=-0.5) >>> for epoch in epochs: >>> bio_response = nk.bio_EventRelated(epoch, event_length=4, window_post=3) Notes ---------- *Details* - ***_Baseline**: Signal at onset. - ***_Min**: Mininmum of signal after stimulus onset. - ***_MinDiff**: Signal mininum - baseline. - ***_MinTime**: Time of signal minimum. - ***_Max**: Maximum of signal after stimulus onset. - ***_MaxDiff**: Signal maximum - baseline. - ***_MaxTime**: Time of signal maximum. - ***_Mean**: Mean signal after stimulus onset. - ***_MeanDiff**: Mean signal - baseline. - **ECG_Phase_Systole**: Cardiac phase on stimulus onset (1 = systole, 0 = diastole). - **ECG_Phase_Systole_Completion**: Percentage of cardiac phase completion on simulus onset. - **ECG_HRV_***: Time-domain HRV features. See :func:`neurokit.ecg_hrv()`. *Authors* - `Dominique Makowski <https://dominiquemakowski.github.io/>`_ *Dependencies* - numpy - pandas *See Also* References -----------", "response": "def ecg_EventRelated(epoch, event_length=1, window_post=0, features=[\"Heart_Rate\", \"Cardiac_Phase\", \"RR_Interval\", \"RSA\", \"HRV\"]):\n    \"\"\"\n    Extract event-related ECG changes.\n\n    Parameters\n    ----------\n    epoch : pandas.DataFrame\n        An epoch contained in the epochs dict returned by :function:`neurokit.create_epochs()` on dataframe returned by :function:`neurokit.bio_process()`. Index should range from -4s to +4s (relatively to event onset and end).\n    event_length : int\n        Event length in seconds.\n    window_post : float\n        Post-stimulus window size (in seconds) to include late responses (usually 3 or 4).\n    features : list\n        List of ECG features to compute, can contain \"Heart_Rate\", \"Cardiac_Phase\", \"RR_Interval\", \"RSA\", \"HRV\".\n\n    Returns\n    ----------\n    ECG_Response : dict\n        Event-related ECG response features.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> bio = nk.bio_process(ecg=data[\"ECG\"], rsp=data[\"RSP\"], eda=data[\"EDA\"], sampling_rate=1000, add=data[\"Photosensor\"])\n    >>> df = bio[\"df\"]\n    >>> events = nk.find_events(df[\"Photosensor\"], cut=\"lower\")\n    >>> epochs = nk.create_epochs(df, events[\"onsets\"], duration=7, onset=-0.5)\n    >>> for epoch in epochs:\n    >>>     bio_response = nk.bio_EventRelated(epoch, event_length=4, window_post=3)\n\n    Notes\n    ----------\n    *Details*\n\n    - ***_Baseline**: Signal at onset.\n    - ***_Min**: Mininmum of signal after stimulus onset.\n    - ***_MinDiff**: Signal mininum - baseline.\n    - ***_MinTime**: Time of signal minimum.\n    - ***_Max**: Maximum of signal after stimulus onset.\n    - ***_MaxDiff**: Signal maximum - baseline.\n    - ***_MaxTime**: Time of signal maximum.\n    - ***_Mean**: Mean signal after stimulus onset.\n    - ***_MeanDiff**: Mean signal - baseline.\n    - **ECG_Phase_Systole**: Cardiac phase on stimulus onset (1 = systole, 0 = diastole).\n    - **ECG_Phase_Systole_Completion**: Percentage of cardiac phase completion on simulus onset.\n    - **ECG_HRV_***: Time-domain HRV features. See :func:`neurokit.ecg_hrv()`.\n\n\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - numpy\n    - pandas\n\n    *See Also*\n\n    References\n    -----------\n    \"\"\"\n    def compute_features(variable, prefix, response):\n        \"\"\"\n        Internal function to compute features and avoid spaguetti code.\n        \"\"\"\n        response[prefix + \"_Baseline\"] = epoch[variable][0]\n        response[prefix + \"_Min\"] = epoch[variable][0:window_end].min()\n        response[prefix + \"_MinDiff\"] = response[prefix + \"_Min\"] - response[prefix + \"_Baseline\"]\n        response[prefix + \"_MinTime\"] = epoch[variable][0:window_end].idxmin()\n        response[prefix + \"_Max\"] = epoch[variable][0:window_end].max()\n        response[prefix + \"_MaxDiff\"] = response[prefix + \"_Max\"] - response[prefix + \"_Baseline\"]\n        response[prefix + \"_MaxTime\"] = epoch[variable][0:window_end].idxmax()\n        response[prefix + \"_Mean\"] = epoch[variable][0:window_end].mean()\n        response[prefix + \"_MeanDiff\"] = response[prefix + \"_Mean\"] - response[prefix + \"_Baseline\"]\n\n        return(response)\n\n    # Initialization\n    ECG_Response = {}\n    window_end = event_length + window_post\n\n    # Heart Rate\n    # =============\n    if \"Heart_Rate\" in features:\n        if \"Heart_Rate\" in epoch.columns:\n            ECG_Response = compute_features(\"Heart_Rate\", \"ECG_Heart_Rate\", ECG_Response)\n    #\n    # Cardiac Phase\n    # =============\n    if \"Cardiac_Phase\" in features:\n        if \"ECG_Systole\" in epoch.columns:\n            ECG_Response[\"ECG_Phase_Systole\"] = epoch[\"ECG_Systole\"][0]\n\n            # Identify beginning and end\n            systole_beg = np.nan\n            systole_end = np.nan\n            for i in epoch[0:window_end].index:\n                if epoch[\"ECG_Systole\"][i] != ECG_Response[\"ECG_Phase_Systole\"]:\n                    systole_end = i\n                    break\n            for i in epoch[:0].index[::-1]:\n                if epoch[\"ECG_Systole\"][i] != ECG_Response[\"ECG_Phase_Systole\"]:\n                    systole_beg = i\n                    break\n\n            # Compute percentage\n            ECG_Response[\"ECG_Phase_Systole_Completion\"] = -1*systole_beg/(systole_end - systole_beg)*100\n\n\n    # RR Interval\n    # ==================\n    if \"RR_Interval\" in features:\n        if \"ECG_RR_Interval\" in epoch.columns:\n            ECG_Response = compute_features(\"ECG_RR_Interval\", \"ECG_RRi\", ECG_Response)\n\n\n    # RSA\n    # ==========\n    if \"RSA\" in features:\n        if \"RSA\" in epoch.columns:\n            ECG_Response = compute_features(\"RSA\", \"ECG_RSA\", ECG_Response)\n\n    # HRV\n    # ====\n    if \"HRV\" in features:\n        if \"ECG_R_Peaks\" in epoch.columns:\n            rpeaks = epoch[epoch[\"ECG_R_Peaks\"]==1][0:event_length].index*1000\n            hrv = ecg_hrv(rpeaks=rpeaks, sampling_rate=1000, hrv_features=[\"time\"])\n\n            # HRV time domain feature computation\n            for key in hrv:\n                if isinstance(hrv[key], float):  # Avoid storing series or dataframes\n                    ECG_Response[\"ECG_HRV_\" + key] = hrv[key]\n\n            # Computation for baseline\n            if epoch.index[0] > -4:  # Sanity check\n                print(\"NeuroKit Warning: ecg_EventRelated(): your epoch starts less than 4 seconds before stimulus onset. That's too short to compute HRV baseline features.\")\n            else:\n                rpeaks = epoch[epoch[\"ECG_R_Peaks\"]==1][:0].index*1000\n                hrv = ecg_hrv(rpeaks=rpeaks, sampling_rate=1000, hrv_features=[\"time\"])\n\n                for key in hrv:\n                    if isinstance(hrv[key], float):  # Avoid storing series or dataframes\n                        ECG_Response[\"ECG_HRV_\" + key + \"_Baseline\"] = hrv[key]\n\n                # Compute differences between features and baseline\n                keys = [key for key in ECG_Response.keys() if '_Baseline' in key]  # Find keys\n                keys = [key for key in keys if 'ECG_HRV_' in key]\n                keys = [s.replace('_Baseline', '') for s in keys]  # Remove baseline part\n                for key in keys:\n                    try:\n                        ECG_Response[key + \"_Diff\"] = ECG_Response[key] - ECG_Response[key + \"_Baseline\"]\n                    except KeyError:\n                        ECG_Response[key + \"_Diff\"] = np.nan\n\n\n\n        if \"ECG_HRV_VHF\" in epoch.columns:\n            ECG_Response = compute_features(\"ECG_HRV_VHF\", \"ECG_HRV_VHF\", ECG_Response)\n\n        if \"ECG_HRV_HF\" in epoch.columns:\n            ECG_Response = compute_features(\"ECG_HRV_HF\", \"ECG_HRV_HF\", ECG_Response)\n\n        if \"ECG_HRV_LF\" in epoch.columns:\n            ECG_Response = compute_features(\"ECG_HRV_LF\", \"ECG_HRV_LF\", ECG_Response)\n\n        if \"ECG_HRV_VLF\" in epoch.columns:\n            ECG_Response = compute_features(\"ECG_HRV_VLF\", \"ECG_HRV_VLF\", ECG_Response)\n\n\n    return(ECG_Response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsimulating an ECG signal and returns a dict containing the ECG response features.", "response": "def ecg_simulate(duration=10, sampling_rate=1000, bpm=60, noise=0.01):\n    \"\"\"\n    Simulates an ECG signal.\n\n    Parameters\n    ----------\n    duration : int\n        Desired recording length.\n    sampling_rate : int\n        Desired sampling rate.\n    bpm : int\n        Desired simulated heart rate.\n    noise : float\n        Desired noise level.\n\n\n    Returns\n    ----------\n    ECG_Response : dict\n        Event-related ECG response features.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> import pandas as pd\n    >>>\n    >>> ecg = nk.ecg_simulate(duration=10, bpm=60, sampling_rate=1000, noise=0.01)\n    >>> pd.Series(ecg).plot()\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Diarmaid O Cualain <https://github.com/diarmaidocualain>`_\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - numpy\n    - scipy.signal\n\n    References\n    -----------\n    \"\"\"\n    # The \"Daubechies\" wavelet is a rough approximation to a real, single, cardiac cycle\n    cardiac = scipy.signal.wavelets.daub(10)\n    # Add the gap after the pqrst when the heart is resting.\n    cardiac = np.concatenate([cardiac, np.zeros(10)])\n\n    # Caculate the number of beats in capture time period\n    num_heart_beats = int(duration * bpm / 60)\n\n    # Concatenate together the number of heart beats needed\n    ecg = np.tile(cardiac , num_heart_beats)\n\n    # Add random (gaussian distributed) noise\n    noise = np.random.normal(0, noise, len(ecg))\n    ecg = noise + ecg\n\n    # Resample\n    ecg = scipy.signal.resample(ecg, sampling_rate*duration)\n\n    return(ecg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rsp_process(rsp, sampling_rate=1000):\n    processed_rsp = {\"df\": pd.DataFrame({\"RSP_Raw\": np.array(rsp)})}\n\n    biosppy_rsp = dict(biosppy.signals.resp.resp(rsp, sampling_rate=sampling_rate, show=False))\n    processed_rsp[\"df\"][\"RSP_Filtered\"] = biosppy_rsp[\"filtered\"]\n\n\n#   RSP Rate\n#   ============\n    rsp_rate = biosppy_rsp[\"resp_rate\"]*60  # Get RSP rate value (in cycles per minute)\n    rsp_times = biosppy_rsp[\"resp_rate_ts\"]   # the time (in sec) of each rsp rate value\n    rsp_times = np.round(rsp_times*sampling_rate).astype(int)  # Convert to timepoints\n    try:\n        rsp_rate = interpolate(rsp_rate, rsp_times, sampling_rate)  # Interpolation using 3rd order spline\n        processed_rsp[\"df\"][\"RSP_Rate\"] = rsp_rate\n    except TypeError:\n        print(\"NeuroKit Warning: rsp_process(): Sequence too short to compute respiratory rate.\")\n        processed_rsp[\"df\"][\"RSP_Rate\"] = np.nan\n\n\n#   RSP Cycles\n#   ===========================\n    rsp_cycles = rsp_find_cycles(biosppy_rsp[\"filtered\"])\n    processed_rsp[\"df\"][\"RSP_Inspiration\"] = rsp_cycles[\"RSP_Inspiration\"]\n\n    processed_rsp[\"RSP\"] = {}\n    processed_rsp[\"RSP\"][\"Cycles_Onsets\"] = rsp_cycles[\"RSP_Cycles_Onsets\"]\n    processed_rsp[\"RSP\"][\"Expiration_Onsets\"] = rsp_cycles[\"RSP_Expiration_Onsets\"]\n    processed_rsp[\"RSP\"][\"Cycles_Length\"] = rsp_cycles[\"RSP_Cycles_Length\"]/sampling_rate\n\n#   RSP Variability\n#   ===========================\n    rsp_diff = processed_rsp[\"RSP\"][\"Cycles_Length\"]\n\n    processed_rsp[\"RSP\"][\"Respiratory_Variability\"] = {}\n    processed_rsp[\"RSP\"][\"Respiratory_Variability\"][\"RSPV_SD\"] = np.std(rsp_diff)\n    processed_rsp[\"RSP\"][\"Respiratory_Variability\"][\"RSPV_RMSSD\"] = np.sqrt(np.mean(rsp_diff ** 2))\n    processed_rsp[\"RSP\"][\"Respiratory_Variability\"][\"RSPV_RMSSD_Log\"] = np.log(processed_rsp[\"RSP\"][\"Respiratory_Variability\"][\"RSPV_RMSSD\"])\n\n\n    return(processed_rsp)", "response": "A function that processes the RSP signal and returns a dict containing the processed RSP features."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict containing the RSP cycles onsets durations and phases.", "response": "def rsp_find_cycles(signal):\n    \"\"\"\n    Find Respiratory cycles onsets, durations and phases.\n\n    Parameters\n    ----------\n    signal : list or array\n        Respiratory (RSP) signal (preferably filtered).\n\n\n    Returns\n    ----------\n    rsp_cycles : dict\n        RSP cycles features.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> rsp_cycles = nk.rsp_find_cycles(signal)\n\n    Notes\n    ----------\n    *Authors*\n\n    - Dominique Makowski (https://github.com/DominiqueMakowski)\n\n    *Dependencies*\n\n    - biosppy\n\n    *See Also*\n\n    - BioSPPY: https://github.com/PIA-Group/BioSPPy\n\n    \"\"\"\n    # Compute gradient (sort of derivative)\n    gradient = np.gradient(signal)\n    # Find zero-crossings\n    zeros, = biosppy.tools.zero_cross(signal=gradient, detrend=True)\n\n    # Find respiratory phases\n    phases_indices = []\n    for i in zeros:\n        if gradient[i+1] > gradient[i-1]:\n            phases_indices.append(\"Inspiration\")\n        else:\n            phases_indices.append(\"Expiration\")\n\n    # Select cycles (inspiration) and expiration onsets\n    inspiration_onsets = []\n    expiration_onsets = []\n    for index, onset in enumerate(zeros):\n        if phases_indices[index] == \"Inspiration\":\n            inspiration_onsets.append(onset)\n        if phases_indices[index] == \"Expiration\":\n            expiration_onsets.append(onset)\n\n\n    # Create a continuous inspiration signal\n    # ---------------------------------------\n    # Find initial phase\n    if phases_indices[0] == \"Inspiration\":\n        phase = \"Expiration\"\n    else:\n        phase = \"Inspiration\"\n\n    inspiration = []\n    phase_counter = 0\n    for i, value in enumerate(signal):\n        if i == zeros[phase_counter]:\n            phase = phases_indices[phase_counter]\n            if phase_counter < len(zeros)-1:\n                phase_counter += 1\n        inspiration.append(phase)\n\n    # Find last phase\n    if phases_indices[len(phases_indices)-1] == \"Inspiration\":\n        last_phase = \"Expiration\"\n    else:\n        last_phase = \"Inspiration\"\n    inspiration = np.array(inspiration)\n    inspiration[max(zeros):] = last_phase\n\n    # Convert to binary\n    inspiration[inspiration == \"Inspiration\"] = 1\n    inspiration[inspiration == \"Expiration\"] = 0\n    inspiration = pd.to_numeric(inspiration)\n\n    cycles_length = np.diff(inspiration_onsets)\n\n    rsp_cycles = {\"RSP_Inspiration\": inspiration,\n                  \"RSP_Expiration_Onsets\": expiration_onsets,\n                  \"RSP_Cycles_Onsets\": inspiration_onsets,\n                  \"RSP_Cycles_Length\": cycles_length}\n\n    return(rsp_cycles)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rsp_EventRelated(epoch, event_length, window_post=4):\n    # Initialization\n    RSP_Response = {}\n    window_end = event_length + window_post\n\n    # RSP Rate\n    # =============\n    if \"RSP_Rate\" in epoch.columns:\n        RSP_Response[\"RSP_Rate_Baseline\"] = epoch[\"RSP_Rate\"].ix[0]\n        RSP_Response[\"RSP_Rate_Min\"] = epoch[\"RSP_Rate\"].ix[0:window_end].min()\n        RSP_Response[\"RSP_Rate_MinDiff\"] = RSP_Response[\"RSP_Rate_Min\"] - RSP_Response[\"RSP_Rate_Baseline\"]\n        RSP_Response[\"RSP_Rate_MinTime\"] = epoch[\"RSP_Rate\"].ix[0:window_end].idxmin()\n        RSP_Response[\"RSP_Rate_Max\"] = epoch[\"RSP_Rate\"].ix[0:window_end].max()\n        RSP_Response[\"RSP_Rate_MaxDiff\"] = RSP_Response[\"RSP_Rate_Max\"] - RSP_Response[\"RSP_Rate_Baseline\"]\n        RSP_Response[\"RSP_Rate_MaxTime\"] = epoch[\"RSP_Rate\"].ix[0:window_end].idxmax()\n        RSP_Response[\"RSP_Rate_Mean\"] = epoch[\"RSP_Rate\"].ix[0:window_end].mean()\n        RSP_Response[\"RSP_Rate_MeanDiff\"] = RSP_Response[\"RSP_Rate_Mean\"] - RSP_Response[\"RSP_Rate_Baseline\"]\n\n\n    # RSP Phase\n    # =============\n    if \"RSP_Inspiration\" in epoch.columns:\n        RSP_Response[\"RSP_Inspiration\"] = epoch[\"RSP_Inspiration\"].ix[0]\n\n        # Identify beginning and end\n        phase_beg = np.nan\n        phase_end = np.nan\n        for i in epoch[0:window_end].index:\n            if epoch[\"RSP_Inspiration\"].ix[i] != RSP_Response[\"RSP_Inspiration\"]:\n                phase_end = i\n                break\n        for i in epoch[:0].index[::-1]:\n            if epoch[\"RSP_Inspiration\"].ix[i] != RSP_Response[\"RSP_Inspiration\"]:\n                phase_beg = i\n                break\n\n        RSP_Response[\"RSP_Inspiration_Completion\"] = -1*phase_beg/(phase_end - phase_beg)*100\n\n\n    return(RSP_Response)", "response": "Extract event - related RSP changes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eeg_add_channel(raw, channel, sync_index_eeg=0, sync_index_channel=0, channel_type=None, channel_name=None):\n    if channel_name is None:\n        if isinstance(channel, pd.core.series.Series):\n            if channel.name is not None:\n                channel_name = channel.name\n            else:\n                channel_name = \"Added_Channel\"\n        else:\n            channel_name = \"Added_Channel\"\n\n    # Compute the distance between the two signals\n    diff = sync_index_channel - sync_index_eeg\n    if diff > 0:\n        channel = list(channel)[diff:len(channel)]\n        channel = channel + [np.nan]*diff\n    if diff < 0:\n        channel = [np.nan]*diff + list(channel)\n        channel = list(channel)[0:len(channel)]\n\n    # Adjust to raw size\n    if len(channel) < len(raw):\n        channel = list(channel) + [np.nan]*(len(raw)-len(channel))\n    else:\n        channel = list(channel)[0:len(raw)]  # Crop to fit the raw data\n\n    info = mne.create_info([channel_name], raw.info[\"sfreq\"], ch_types=channel_type)\n    channel = mne.io.RawArray([channel], info)\n\n    raw.add_channels([channel], force_update_info=True)\n\n    return(raw)", "response": "This function adds a channel to a mne s Raw EEG file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nselecting one or several channels by name and returns them in a dataframe.", "response": "def eeg_select_channels(raw, channel_names):\n    \"\"\"\n    Select one or several channels by name and returns them in a dataframe.\n\n    Parameters\n    ----------\n    raw : mne.io.Raw\n        Raw EEG data.\n    channel_names : str or list\n        Channel's name(s).\n\n    Returns\n    ----------\n    channels : pd.DataFrame\n        Channel.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> raw = nk.eeg_select_channel(raw, \"TP7\")\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - mne\n\n    *See Also*\n\n    - mne package: http://martinos.org/mne/dev/index.html\n    \"\"\"\n    if isinstance(channel_names, list) is False:\n        channel_names = [channel_names]\n\n    channels, time_index = raw.copy().pick_channels(channel_names)[:]\n    if len(channel_names) > 1:\n        channels = pd.DataFrame(channels.T, columns=channel_names)\n    else:\n        channels = pd.Series(channels[0])\n        channels.name = channel_names[0]\n    return(channels)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nselect electrodes from a set of electrodes.", "response": "def eeg_select_electrodes(eeg, include=\"all\", exclude=None, hemisphere=\"both\", central=True):\n    \"\"\"\n    Returns electrodes/sensors names of selected region (according to a 10-20 EEG montage).\n\n    Parameters\n    ----------\n    eeg : mne.Raw or mne.Epochs\n        EEG data.\n    include : str ot list\n        Sensor area to include.\n    exclude : str or list or None\n        Sensor area to exclude.\n    hemisphere : str\n        Select both hemispheres? \"both\", \"left\" or \"right\".\n    central : bool\n        Select the central line.\n\n    Returns\n    ----------\n    electrodes : list\n        List of electrodes/sensors corresponding to the selected area.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> nk.eeg_select_electrodes(include=\"F\", exclude=\"C\")\n\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    \"\"\"\n    # Get all channel names\n    eeg = eeg.copy().pick_types(meg=False, eeg=True)\n    channel_list = eeg.ch_names\n\n    # Include\n    if include == \"all\":\n        electrodes = channel_list\n    elif isinstance(include, str):\n        electrodes = [s for s in channel_list if include in s]\n    elif isinstance(include, list):\n        electrodes = []\n        for i in include:\n            electrodes += [s for s in channel_list if i in s]\n    else:\n        print(\"NeuroKit Warning: eeg_select_electrodes(): 'include' parameter must be 'all', str or list.\")\n\n    # Exclude\n    if exclude is not None:\n        if isinstance(exclude, str):\n            to_remove = [s for s in channel_list if exclude in s]\n            electrodes = [s for s in electrodes if s not in to_remove]\n        elif isinstance(exclude, list):\n            to_remove = []\n            for i in exclude:\n                to_remove += [s for s in channel_list if i in s]\n            electrodes = [s for s in electrodes if s not in to_remove]\n        else:\n            print(\"NeuroKit Warning: eeg_select_electrodes(): 'exclude' parameter must be None, str or list.\")\n\n    # Laterality\n    if hemisphere != \"both\":\n        if hemisphere.lower() == \"left\" or hemisphere.lower() == \"l\":\n            hemi = [s for s in electrodes if len(re.findall(r'\\d+', s)) > 0 and int(re.findall(r'\\d+', s)[0])%2 > 0]\n        elif hemisphere.lower() == \"right\" or hemisphere.lower() == \"r\":\n            hemi = [s for s in electrodes if len(re.findall(r'\\d+', s)) > 0 and int(re.findall(r'\\d+', s)[0])%2 == 0]\n        else:\n            print(\"NeuroKit Warning: eeg_select_electrodes(): 'hemisphere' parameter must be 'both', 'left' or 'right'. Returning both.\")\n\n        if central is True:\n            hemi += [s for s in electrodes if 'z' in s]\n\n        electrodes = hemi\n\n    return(electrodes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates MNE compatible events.", "response": "def eeg_create_mne_events(onsets, conditions=None):\n    \"\"\"\n    Create MNE compatible events.\n\n    Parameters\n    ----------\n    onsets : list or array\n        Events onsets.\n    conditions : list\n        A list of equal length containing the stimuli types/conditions.\n\n\n    Returns\n    ----------\n    (events, event_id) : tuple\n        MNE-formated events and a dictionary with event's names.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> events, event_id = nk.eeg_create_mne_events(events_onset, conditions)\n\n    Authors\n    ----------\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    \"\"\"\n    event_id = {}\n\n    if conditions is None:\n        conditions = [\"Event\"] * len(onsets)\n\n    # Sanity check\n    if len(conditions) != len(onsets):\n        print(\"NeuroKit Warning: eeg_create_events(): conditions parameter of different length than onsets. Aborting.\")\n        return()\n\n\n\n    event_names = list(set(conditions))\n#    event_index = [1, 2, 3, 4, 5, 32, 64, 128]\n    event_index = list(range(len(event_names)))\n    for i in enumerate(event_names):\n        conditions = [event_index[i[0]] if x==i[1] else x for x in conditions]\n        event_id[i[1]] = event_index[i[0]]\n\n    events = np.array([onsets, [0]*len(onsets), conditions]).T\n    return(events, event_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the raw data into an MNE compatible format and add them to the internal data structures.", "response": "def eeg_add_events(raw, events_channel, conditions=None, treshold=\"auto\", cut=\"higher\", time_index=None, number=\"all\", after=0, before=None, min_duration=1):\n    \"\"\"\n    Find events on a channel, convert them into an MNE compatible format, and add them to the raw data.\n\n    Parameters\n    ----------\n    raw : mne.io.Raw\n        Raw EEG data.\n    events_channel : str or array\n        Name of the trigger channel if in the raw, or array of equal length if externally supplied.\n    conditions : list\n        List containing the stimuli types/conditions.\n    treshold : float\n        The treshold value by which to select the events. If \"auto\", takes the value between the max and the min.\n    cut : str\n        \"higher\" or \"lower\", define the events as above or under the treshold. For photosensors, a white screen corresponds usually to higher values. Therefore, if your events were signalled by a black colour, events values would be the lower ones, and you should set the cut to \"lower\".\n        Add a corresponding datetime index, will return an addional array with the onsets as datetimes.\n    number : str or int\n        How many events should it select.\n    after : int\n        If number different than \"all\", then at what time should it start selecting the events.\n    before : int\n        If number different than \"all\", before what time should it select the events.\n    min_duration : int\n        The minimum duration of an event (in timepoints).\n\n    Returns\n    ----------\n    (raw, events, event_id) : tuple\n        The raw file with events, the mne-formatted events and event_id.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> raw, events, event_id = nk.eeg_add_events(raw, events_channel, conditions)\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - pandas\n\n    *See Also*\n\n    - mne: http://martinos.org/mne/dev/index.html\n\n\n    References\n    -----------\n    - None\n    \"\"\"\n    # Extract the events_channel from raw if needed\n    if isinstance(events_channel, str):\n        try:\n            events_channel = eeg_select_channels(raw, events_channel)\n        except:\n            print(\"NeuroKit error: eeg_add_events(): Wrong events_channel name provided.\")\n\n    # Find event onsets\n    events = find_events(events_channel, treshold=treshold, cut=cut, time_index=time_index, number=number, after=after, before=before, min_duration=min_duration)\n\n    # Create mne compatible events\n    events, event_id = eeg_create_mne_events(events[\"onsets\"], conditions)\n\n    # Add them\n    raw.add_events(events)\n\n    return(raw, events, event_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eeg_to_all_evokeds(all_epochs, conditions=None):\n    if conditions is None:\n        # Get event_id\n        conditions = {}\n        for participant, epochs in all_epochs.items():\n            conditions.update(epochs.event_id)\n\n    all_evokeds = {}\n    for participant, epochs in all_epochs.items():\n        evokeds = {}\n        for cond in conditions:\n            try:\n                evokeds[cond] = epochs[cond].average()\n            except KeyError:\n                pass\n        all_evokeds[participant] = evokeds\n\n    return(all_evokeds)", "response": "Convert all_epochs to all_evokeds."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts mne. Epochs object to dataframe or dict of dataframes.", "response": "def eeg_to_df(eeg, index=None, include=\"all\", exclude=None, hemisphere=\"both\", central=True):\n    \"\"\"\n    Convert mne Raw or Epochs object to dataframe or dict of dataframes.\n\n    DOCS INCOMPLETE :(\n    \"\"\"\n    if isinstance(eeg, mne.Epochs):\n        data = {}\n\n        if index is None:\n            index = range(len(eeg))\n\n        for epoch_index, epoch in zip(index, eeg.get_data()):\n\n            epoch = pd.DataFrame(epoch.T)\n            epoch.columns = eeg.ch_names\n            epoch.index = eeg.times\n\n            selection = eeg_select_electrodes(eeg, include=include, exclude=exclude, hemisphere=hemisphere, central=central)\n\n            data[epoch_index] = epoch[selection]\n\n    else:  # it might be a Raw object\n        data = eeg.get_data().T\n        data = pd.DataFrame(data)\n        data.columns = eeg.ch_names\n        data.index = eeg.times\n\n    return(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a Polar bar chart for a set of scores.", "response": "def plot_polarbar(scores, labels=None, labels_size=15, colors=\"default\", distribution_means=None, distribution_sds=None, treshold=1.28, fig_size=(15, 15)):\n    \"\"\"\n    Polar bar chart.\n\n    Parameters\n    ----------\n    scores : list or dict\n        Scores to plot.\n    labels : list\n        List of labels to be used for ticks.\n    labels_size : int\n        Label's size.\n    colors : list or str\n        List of colors or \"default\".\n    distribution_means : int or list\n        List of means to add a range ribbon.\n    distribution_sds : int or list\n        List of SDs to add a range ribbon.\n    treshold : float\n        Limits of the range ribbon (in terms of standart deviation from mean).\n    fig_size : tuple\n        Figure size.\n\n\n    Returns\n    ----------\n    plot : matplotlig figure\n        The figure.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> fig = nk.plot_polarbar(scores=[1, 2, 3, 4, 5], labels=[\"A\", \"B\", \"C\", \"D\", \"E\"], distribution_means=3, distribution_sds=1)\n    >>> fig.show()\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - matplotlib\n    - numpy\n    \"\"\"\n\n\n    # Sanity check\n    if isinstance(scores, dict):\n        if labels is None:\n            labels = list(scores.keys())\n        try:\n            scores = [scores[key] for key in labels]\n        except KeyError:\n            print(\"NeuroKit Error: plot_polarbar(): labels and scores keys not matching. Recheck them.\")\n\n\n\n    # Parameters\n    if colors == \"default\":\n        if len(scores) < 9:\n            colors = [\"#f44336\", \"#9C27B0\", \"#3F51B5\",\"#03A9F4\", \"#009688\", \"#8BC34A\", \"#FFEB3B\", \"#FF9800\", \"#795548\"]\n        else:\n            colors = None\n    if labels is None:\n        labels = range(len(scores))\n\n    N = len(scores)\n    theta = np.linspace(0.0, -2 * np.pi, N, endpoint=False)\n    width = 2 * np.pi / N\n\n\n\n    # Main\n    plot = plt.figure(figsize=fig_size)\n\n    layer1 = plot.add_subplot(111, projection=\"polar\")\n    bars1 = layer1.bar(theta+np.pi/len(scores), scores, width=width, bottom=0.0)\n\n    layer1.yaxis.set_ticks(range(11))\n    layer1.yaxis.set_ticklabels([])\n\n    layer1.xaxis.set_ticks(theta+np.pi/len(scores))\n    layer1.xaxis.set_ticklabels(labels, fontsize=labels_size)\n\n    for index, bar in enumerate(bars1):\n        if colors is not None:\n            bar.set_facecolor(colors[index])\n        bar.set_alpha(1)\n\n    # Layer 2\n    if distribution_means is not None and distribution_sds is not None:\n\n        # Sanity check\n        if isinstance(distribution_means, int):\n            distribution_means = [distribution_means]*N\n        if isinstance(distribution_sds, int):\n            distribution_sds = [distribution_sds]*N\n\n        # TODO: add convertion if those parameter are dict\n\n\n        bottoms, tops = normal_range(np.array(distribution_means), np.array(distribution_sds), treshold=treshold)\n        tops = tops - bottoms\n\n        layer2 = plot.add_subplot(111, polar=True)\n        bars2 = layer2.bar(theta, tops, width=width, bottom=bottoms, linewidth=0)\n        layer2.xaxis.set_ticks(theta+np.pi/len(scores))\n        layer2.xaxis.set_ticklabels(labels, fontsize=labels_size)\n\n        for index, bar in enumerate(bars2):\n            bar.set_facecolor(\"#607D8B\")\n            bar.set_alpha(0.3)\n\n    return(plot)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef feature_reduction(data, method, n_features):\n    if method == \"PCA\":\n        feature_red_method = sklearn.decomposition.PCA(n_components=n_features)\n        data_processed = feature_red_method.fit_transform(data)\n\n    elif method == \"agglom\":\n        feature_red_method = sklearn.cluster.FeatureAgglomeration(n_clusters=n_features)\n        data_processed = feature_red_method.fit_transform(data)\n\n    elif method == \"ica\":\n        feature_red_method = sklearn.decomposition.FastICA(n_components=n_features)\n        data_processed = feature_red_method.fit_transform(data)\n\n    elif method == \"kernelPCA\":\n        feature_red_method = sklearn.decomposition.KernelPCA(n_components=n_features, kernel='linear')\n        data_processed = feature_red_method.fit_transform(data)\n\n    elif method == \"kernelPCA\":\n        feature_red_method = sklearn.decomposition.KernelPCA(n_components=n_features, kernel='linear')\n        data_processed = feature_red_method.fit_transform(data)\n\n    elif method == \"sparsePCA\":\n        feature_red_method = sklearn.decomposition.SparsePCA(n_components=n_features)\n        data_processed = feature_red_method.fit_transform(data)\n\n    elif method == \"incrementalPCA\":\n        feature_red_method = sklearn.decomposition.IncrementalPCA(n_components=n_features)\n        data_processed = feature_red_method.fit_transform(data)\n\n    elif method == \"nmf\":\n        if np.min(data) < 0:\n            data -= np.min(data)\n        feature_red_method = sklearn.decomposition.NMF(n_components=n_features)\n        data_processed = feature_red_method.fit_transform(data)\n\n    else:\n        feature_red_method = None\n        data_processed = data.copy()\n\n    return(data_processed)", "response": "Feature reduction.\n\n    Parameters\n    ----------\n    NA\n\n    Returns\n    ----------\n    NA\n\n    Example\n    ----------\n    NA\n\n    Authors\n    ----------\n    Dominique Makowski\n\n    Dependencies\n    ----------\n    - sklearn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef binarize_signal(signal, treshold=\"auto\", cut=\"higher\"):\n    if treshold == \"auto\":\n        treshold = (np.max(np.array(signal)) - np.min(np.array(signal)))/2\n    signal = list(signal)\n    binary_signal = []\n    for i in range(len(signal)):\n        if cut == \"higher\":\n            if signal[i] > treshold:\n                binary_signal.append(1)\n            else:\n                binary_signal.append(0)\n        else:\n            if signal[i] < treshold:\n                binary_signal.append(1)\n            else:\n                binary_signal.append(0)\n    return(binary_signal)", "response": "Binarize a signal channel into a list of binary events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new n - grams of events that are binarized by a continuous signal.", "response": "def localize_events(events_channel, treshold=\"auto\", cut=\"higher\", time_index=None):\n    \"\"\"\n    Find the onsets of all events based on a continuous signal.\n\n    Parameters\n    ----------\n    events_channel = array or list\n        The trigger channel.\n    treshold = float\n        The treshold value by which to select the events. If \"auto\", takes the value between the max and the min.\n    cut = str\n        \"higher\" or \"lower\", define the events as above or under the treshold. For photosensors, a white screen corresponds usually to higher values. Therefore, if your events were signalled by a black colour, events values would be the lower ones, and you should set the cut to \"lower\".\n    time_index = array or list\n        Add a corresponding datetime index, will return an addional array with the onsets as datetimes.\n\n    Returns\n    ----------\n    dict\n        dict containing the onsets, the duration and the time index if provided.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> events_onset = nk.events_onset(events_channel)\n\n    Authors\n    ----------\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    Dependencies\n    ----------\n    None\n    \"\"\"\n    events_channel = binarize_signal(events_channel, treshold=treshold, cut=cut)\n\n    events = {\"onsets\":[], \"durations\":[]}\n    if time_index is not None:\n        events[\"onsets_time\"] = []\n\n    index = 0\n    for key, g in (groupby(events_channel)):\n        duration = len(list(g))\n        if key == 1:\n            events[\"onsets\"].append(index)\n            events[\"durations\"].append(duration)\n            if time_index is not None:\n                events[\"onsets_time\"].append(time_index[index])\n        index += duration\n    return(events)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_events(events_channel, treshold=\"auto\", cut=\"higher\", time_index=None, number=\"all\", after=0, before=None, min_duration=1):\n    events = localize_events(events_channel, treshold=treshold, cut=cut, time_index=time_index)\n\n    # Warning when no events detected\n    if len(events[\"onsets\"]) == 0:\n        print(\"NeuroKit warning: find_events(): No events found. Check your events_channel or adjust trehsold.\")\n        return()\n\n    # Remove less than duration\n    toremove = []\n    for event in range(len(events[\"onsets\"])):\n        if events[\"durations\"][event] < min_duration:\n            toremove.append(False)\n        else:\n            toremove.append(True)\n    events[\"onsets\"] = np.array(events[\"onsets\"])[np.array(toremove)]\n    events[\"durations\"] = np.array(events[\"durations\"])[np.array(toremove)]\n    if time_index is not None:\n        events[\"onsets_time\"] = np.array(events[\"onsets_time\"])[np.array(toremove)]\n\n    # Before and after\n    if isinstance(number, int):\n        after_times = []\n        after_onsets = []\n        after_length = []\n        before_times = []\n        before_onsets = []\n        before_length = []\n        if after != None:\n            if events[\"onsets_time\"] == []:\n                events[\"onsets_time\"] = np.array(events[\"onsets\"])\n            else:\n                events[\"onsets_time\"] = np.array(events[\"onsets_time\"])\n            after_onsets = list(np.array(events[\"onsets\"])[events[\"onsets_time\"]>after])[:number]\n            after_times = list(np.array(events[\"onsets_time\"])[events[\"onsets_time\"]>after])[:number]\n            after_length = list(np.array(events[\"durations\"])[events[\"onsets_time\"]>after])[:number]\n        if before != None:\n            if events[\"onsets_time\"] == []:\n                events[\"onsets_time\"] = np.array(events[\"onsets\"])\n            else:\n                events[\"onsets_time\"] = np.array(events[\"onsets_time\"])\n            before_onsets = list(np.array(events[\"onsets\"])[events[\"onsets_time\"]<before])[:number]\n            before_times = list(np.array(events[\"onsets_time\"])[events[\"onsets_time\"]<before])[:number]\n            before_length = list(np.array(events[\"durations\"])[events[\"onsets_time\"]<before])[:number]\n        events[\"onsets\"] = before_onsets + after_onsets\n        events[\"onsets_time\"] = before_times + after_times\n        events[\"durations\"] = before_length + after_length\n\n    return(events)", "response": "Find and select events based on a continuous signal."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot events in signal.", "response": "def plot_events_in_signal(signal, events_onsets, color=\"red\", marker=None):\n    \"\"\"\n    Plot events in signal.\n\n    Parameters\n    ----------\n    signal : array or DataFrame\n        Signal array (can be a dataframe with many signals).\n    events_onsets : list or ndarray\n        Events location.\n    color : int or list\n        Marker color.\n    marker : marker or list of markers (for possible marker values, see: https://matplotlib.org/api/markers_api.html)\n        Marker type.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> bio = nk.bio_process(ecg=signal, sampling_rate=1000)\n    >>> events_onsets = bio[\"ECG\"][\"R_Peaks\"]\n    >>> plot_events_in_signal(bio[\"df\"][\"ECG_Filtered\"], events_onsets)\n    >>> plot_events_in_signal(bio[\"df\"][\"ECG_Filtered\"], events_onsets, color=\"red\", marker=\"o\")\n    >>> plot_events_in_signal(bio[\"df\"][\"ECG_Filtered\"], [bio[\"ECG\"][\"P_Waves\"], bio[\"ECG\"][\"R_Peaks\"]], color=[\"blue\", \"red\"], marker=[\"d\",\"o\"])\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n    - `Renatosc <https://github.com/renatosc/>`_\n\n    *Dependencies*\n\n    - matplotlib\n    - pandas\n    \"\"\"\n\n    df = pd.DataFrame(signal)\n    ax = df.plot()\n\n    def plotOnSignal(x, color, marker=None):\n        if (marker is None):\n            plt.axvline(x=event, color=color)\n        else:\n            plt.plot(x, signal[x], marker, color=color)\n\n\n    events_onsets = np.array(events_onsets)\n    try:\n        len(events_onsets[0])\n        for index, dim in enumerate(events_onsets):\n            for event in dim:\n                plotOnSignal(x=event,\n                             color=color[index] if isinstance(color, list) else color,\n                             marker=marker[index] if isinstance(marker, list) else marker)\n    except TypeError:\n        for event in events_onsets:\n            plotOnSignal(x=event,\n                         color=color[0] if isinstance(color, list) else color,\n                         marker=marker[0] if isinstance(marker, list) else marker)\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread and Format a BIOPAC s AcqKnowledge file into a pandas dataframe.", "response": "def read_acqknowledge(filename, path=\"\", index=\"datetime\", sampling_rate=\"max\", resampling_method=\"pad\", fill_interruptions=True, return_sampling_rate=True):\n    \"\"\"\n    Read and Format a BIOPAC's AcqKnowledge file into a pandas' dataframe.\n\n    Parameters\n    ----------\n    filename :  str\n        Filename (with or without the extension) of a BIOPAC's AcqKnowledge file.\n    path : str\n        Data directory.\n    index : str\n        How to index the dataframe. \"datetime\" for aproximate datetime (based on the file creation/change) and \"range\" for a simple range index.\n    sampling_rate : int\n        Final sampling rate (samples/second).\n    resampling_method : str\n        The resampling method: \"mean\", \"pad\" or \"bfill\",\n    fill_interruptions : bool\n        Automatically fill the eventual signal interruptions using a backfill method.\n    return_sampling_rate : bool\n        Should it return the sampling rate in a tuple with the dataframe? Default will be changed to True in the future.\n\n    Returns\n    ----------\n    df, sampling_rate : pandas.DataFrame(), int\n        The AcqKnowledge file converted to a dataframe and its sampling_rate.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> df, sampling_rate = nk.read_acqknowledge('file.acq', return_sampling_rate=True)\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - pandas\n    - bioread\n    - datetime\n\n    *See Also*\n\n    - bioread package: https://github.com/njvack/bioread\n\n    \"\"\"\n\n\n\n    # Check path\n    file = path + filename\n    if \".acq\" not in file:\n        file += \".acq\"\n    if os.path.exists(file) is False:\n        print(\"NeuroKit Error: read_acqknowledge(): couldn't find the following file: \" + filename)\n        return()\n\n    # Convert creation date\n    creation_date = find_creation_date(file)\n    creation_date = datetime.datetime.fromtimestamp(creation_date)\n\n\n    # Read file\n    file = bioread.read(file)\n\n\n    # Get the channel frequencies\n    freq_list = []\n    for channel in file.named_channels:\n        freq_list.append(file.named_channels[channel].samples_per_second)\n\n    # Get data with max frequency and the others\n    data = {}\n    data_else = {}\n    for channel in file.named_channels:\n        if file.named_channels[channel].samples_per_second == max(freq_list):\n            data[channel] = file.named_channels[channel].data\n        else:\n            data_else[channel] = file.named_channels[channel].data\n\n    # Create index\n    time = []\n    beginning_date = creation_date - datetime.timedelta(0, max(file.time_index))\n    for timestamps in file.time_index:\n        time.append(beginning_date + datetime.timedelta(0, timestamps))\n    df = pd.DataFrame(data, index=time)\n\n\n\n\n\n    # max frequency must be 1000\n    if len(data_else.keys()) > 0:  # if not empty\n        for channel in data_else:\n            channel_frequency = file.named_channels[channel].samples_per_second\n            serie = data_else[channel]\n            index = list(np.arange(0, max(file.time_index), 1/channel_frequency))\n            index = index[:len(serie)]\n\n            # Create index\n            time = []\n            for timestamps in index:\n                time.append(beginning_date + datetime.timedelta(0, timestamps))\n            data_else[channel] = pd.Series(serie, index=time)\n        df2 = pd.DataFrame(data_else)\n\n    # Create resampling factor\n    if sampling_rate == \"max\":\n        sampling_rate = max(freq_list)\n\n    try:\n        resampling_factor = str(int(1000/sampling_rate)) + \"L\"\n    except TypeError:\n        print(\"NeuroKit Warning: read_acqknowledge(): sampling_rate must be either num or 'max'. Setting to 'max'.\")\n        sampling_rate = max(freq_list)\n        resampling_factor = str(int(1000/sampling_rate)) + \"L\"\n\n\n    # Resample\n    if resampling_method not in [\"mean\", \"bfill\", \"pad\"]:\n        print(\"NeuroKit Warning: read_acqknowledge(): resampling_factor must be 'mean', 'bfill' or 'pad'. Setting to 'pad'.\")\n        resampling_method = 'pad'\n\n    if resampling_method == \"mean\":\n        if len(data_else.keys()) > 0:\n            df2 = df2.resample(resampling_factor).mean()\n        if int(sampling_rate) != int(max(freq_list)):\n            df = df.resample(resampling_factor).mean()\n    if resampling_method == \"bfill\":\n        if len(data_else.keys()) > 0:\n            df2 = df2.resample(resampling_factor).bfill()\n        if int(sampling_rate) != int(max(freq_list)):\n            df = df.resample(resampling_factor).bfill()\n    if resampling_method == \"pad\":\n        if len(data_else.keys()) > 0:\n            df2 = df2.resample(resampling_factor).pad()\n        if int(sampling_rate) != int(max(freq_list)):\n            df = df.resample(resampling_factor).pad()\n\n\n\n    # Join dataframes\n    if len(data_else.keys()) > 0:\n        df = pd.concat([df, df2], 1)\n\n    if index == \"range\":\n        df = df.reset_index()\n\n    # Fill signal interruptions\n    if fill_interruptions is True:\n        df = df.fillna(method=\"backfill\")\n\n    if return_sampling_rate is False:\n        return(df)\n    else:\n        return(df, sampling_rate)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nautomate processing of EDA signal using convex optimization (CVXEDA; Greco et al., 2015). Parameters ---------- eda : list or array EDA signal array. sampling_rate : int Sampling rate (samples/second). alpha : float cvxEDA penalization for the sparse SMNA driver. gamma : float cvxEDA penalization for the tonic spline coefficients. filter_type : str or None Can be Butterworth filter (\"butter\"), Finite Impulse Response filter (\"FIR\"), Chebyshev filters (\"cheby1\" and \"cheby2\"), Elliptic filter (\"ellip\") or Bessel filter (\"bessel\"). Set to None to skip filtering. scr_method : str SCR extraction algorithm. \"makowski\" (default), \"kim\" (biosPPy's default; See Kim et al., 2004) or \"gamboa\" (Gamboa, 2004). scr_treshold : float SCR minimum treshold (in terms of signal standart deviation). Returns ---------- processed_eda : dict Dict containing processed EDA features. Contains the EDA raw signal, the filtered signal, the phasic compnent (if cvxEDA is True), the SCR onsets, peak indexes and amplitudes. This function is mainly a wrapper for the biosppy.eda.eda() and cvxEDA() functions. Credits go to their authors. Example ---------- >>> import neurokit as nk >>> >>> processed_eda = nk.eda_process(eda_signal) Notes ---------- *Details* - **cvxEDA**: Based on a model which describes EDA as the sum of three terms: the phasic component, the tonic component, and an additive white Gaussian noise term incorporating model prediction errors as well as measurement errors and artifacts. This model is physiologically inspired and fully explains EDA through a rigorous methodology based on Bayesian statistics, mathematical convex optimization and sparsity. *Authors* - `Dominique Makowski <https://dominiquemakowski.github.io/>`_ *Dependencies* - biosppy - numpy - pandas - cvxopt *See Also* - BioSPPy: https://github.com/PIA-Group/BioSPPy - cvxEDA: https://github.com/lciti/cvxEDA References ----------- - Greco, A., Valenza, G., & Scilingo, E. P. (2016). Evaluation of CDA and CvxEDA Models. In Advances in Electrodermal Activity Processing with Applications for Mental Health (pp. 35-43). Springer International Publishing. - Greco, A., Valenza, G., Lanata, A., Scilingo, E. P., & Citi, L. (2016). cvxEDA: A convex optimization approach to electrodermal activity processing. IEEE Transactions on Biomedical Engineering, 63(4), 797-804. - Kim, K. H., Bang, S. W., & Kim, S. R. (2004). Emotion recognition system using short-term monitoring of physiological signals. Medical and biological engineering and computing, 42(3), 419-427. - Gamboa, H. (2008). Multi-Modal Behavioral Biometrics Based on HCI and Electrophysiology (Doctoral dissertation, PhD thesis, Universidade T\u00e9cnica de Lisboa, Instituto Superior T\u00e9cnico).", "response": "def eda_process(eda, sampling_rate=1000, alpha=8e-4, gamma=1e-2, filter_type = \"butter\", scr_method=\"makowski\", scr_treshold=0.1):\n    \"\"\"\n    Automated processing of EDA signal using convex optimization (CVXEDA; Greco et al., 2015).\n\n    Parameters\n    ----------\n    eda :  list or array\n        EDA signal array.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    alpha : float\n        cvxEDA penalization for the sparse SMNA driver.\n    gamma : float\n        cvxEDA penalization for the tonic spline coefficients.\n    filter_type : str or None\n        Can be Butterworth filter (\"butter\"), Finite Impulse Response filter (\"FIR\"), Chebyshev filters (\"cheby1\" and \"cheby2\"), Elliptic filter (\"ellip\") or Bessel filter (\"bessel\"). Set to None to skip filtering.\n    scr_method : str\n        SCR extraction algorithm. \"makowski\" (default), \"kim\" (biosPPy's default; See Kim et al., 2004) or \"gamboa\" (Gamboa, 2004).\n    scr_treshold : float\n        SCR minimum treshold (in terms of signal standart deviation).\n\n    Returns\n    ----------\n    processed_eda : dict\n        Dict containing processed EDA features.\n\n        Contains the EDA raw signal, the filtered signal, the phasic compnent (if cvxEDA is True), the SCR onsets, peak indexes and amplitudes.\n\n        This function is mainly a wrapper for the biosppy.eda.eda() and cvxEDA() functions. Credits go to their authors.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> processed_eda = nk.eda_process(eda_signal)\n\n\n    Notes\n    ----------\n    *Details*\n\n    - **cvxEDA**: Based on a model which describes EDA as the sum of three terms: the phasic component, the tonic component, and an additive white Gaussian noise term incorporating model prediction errors as well as measurement errors and artifacts. This model is physiologically inspired and fully explains EDA through a rigorous methodology based on Bayesian statistics, mathematical convex optimization and sparsity.\n\n\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - biosppy\n    - numpy\n    - pandas\n    - cvxopt\n\n    *See Also*\n\n    - BioSPPy: https://github.com/PIA-Group/BioSPPy\n    - cvxEDA: https://github.com/lciti/cvxEDA\n\n    References\n    -----------\n    - Greco, A., Valenza, G., & Scilingo, E. P. (2016). Evaluation of CDA and CvxEDA Models. In Advances in Electrodermal Activity Processing with Applications for Mental Health (pp. 35-43). Springer International Publishing.\n    - Greco, A., Valenza, G., Lanata, A., Scilingo, E. P., & Citi, L. (2016). cvxEDA: A convex optimization approach to electrodermal activity processing. IEEE Transactions on Biomedical Engineering, 63(4), 797-804.\n    - Kim, K. H., Bang, S. W., & Kim, S. R. (2004). Emotion recognition system using short-term monitoring of physiological signals. Medical and biological engineering and computing, 42(3), 419-427.\n    - Gamboa, H. (2008). Multi-Modal Behavioral Biometrics Based on HCI and Electrophysiology (Doctoral dissertation, PhD thesis, Universidade T\u00e9cnica de Lisboa, Instituto Superior T\u00e9cnico).\n    \"\"\"\n    # Initialization\n    eda = np.array(eda)\n    eda_df = pd.DataFrame({\"EDA_Raw\": np.array(eda)})\n\n    # Preprocessing\n    # ===================\n    # Filtering\n    if filter_type is not None:\n        filtered, _, _ = biosppy.tools.filter_signal(signal=eda,\n                                     ftype=filter_type,\n                                     band='lowpass',\n                                     order=4,\n                                     frequency=5,\n                                     sampling_rate=sampling_rate)\n\n        # Smoothing\n        filtered, _ = biosppy.tools.smoother(signal=filtered,\n                                  kernel='boxzen',\n                                  size=int(0.75 * sampling_rate),\n                                  mirror=True)\n        eda_df[\"EDA_Filtered\"] = filtered\n\n    # Derive Phasic and Tonic\n    try:\n        tonic, phasic = cvxEDA(eda, sampling_rate=sampling_rate, alpha=alpha, gamma=gamma)\n        eda_df[\"EDA_Phasic\"] = phasic\n        eda_df[\"EDA_Tonic\"] = tonic\n        signal = phasic\n    except:\n        print(\"NeuroKit Warning: eda_process(): Error in cvxEDA algorithm, couldn't extract phasic and tonic components. Using raw signal.\")\n        signal = eda\n\n    # Skin-Conductance Responses\n    # ===========================\n    if scr_method == \"kim\":\n        onsets, peaks, amplitudes = biosppy.eda.kbk_scr(signal=signal, sampling_rate=sampling_rate, min_amplitude=scr_treshold)\n        recoveries = [np.nan]*len(onsets)\n\n    elif scr_method == \"gamboa\":\n        onsets, peaks, amplitudes = biosppy.eda.basic_scr(signal=signal, sampling_rate=sampling_rate)\n        recoveries = [np.nan]*len(onsets)\n\n    else:  # makowski's algorithm\n        onsets, peaks, amplitudes, recoveries = eda_scr(signal, sampling_rate=sampling_rate, treshold=scr_treshold, method=\"fast\")\n\n\n    # Store SCR onsets and recoveries positions\n    scr_onsets = np.array([np.nan]*len(signal))\n    if len(onsets) > 0:\n        scr_onsets[onsets] = 1\n    eda_df[\"SCR_Onsets\"] = scr_onsets\n\n    scr_recoveries = np.array([np.nan]*len(signal))\n    if len(recoveries) > 0:\n        scr_recoveries[recoveries[pd.notnull(recoveries)].astype(int)] = 1\n    eda_df[\"SCR_Recoveries\"] = scr_recoveries\n\n    # Store SCR peaks and amplitudes\n    scr_peaks = np.array([np.nan]*len(eda))\n    peak_index = 0\n    for index in range(len(scr_peaks)):\n        try:\n            if index == peaks[peak_index]:\n                scr_peaks[index] = amplitudes[peak_index]\n                peak_index += 1\n        except:\n            pass\n    eda_df[\"SCR_Peaks\"] = scr_peaks\n\n    processed_eda = {\"df\": eda_df,\n                     \"EDA\": {\n                            \"SCR_Onsets\": onsets,\n                            \"SCR_Peaks_Indexes\": peaks,\n                            \"SCR_Recovery_Indexes\": recoveries,\n                            \"SCR_Peaks_Amplitudes\": amplitudes}}\n    return(processed_eda)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cvxEDA(eda, sampling_rate=1000, tau0=2., tau1=0.7, delta_knot=10., alpha=8e-4, gamma=1e-2, solver=None, verbose=False, options={'reltol':1e-9}):\n    frequency = 1/sampling_rate\n\n    # Normalizing signal\n    eda = z_score(eda)\n    eda = np.array(eda)[:,0]\n\n    n = len(eda)\n    eda = eda.astype('double')\n    eda = cv.matrix(eda)\n\n    # bateman ARMA model\n    a1 = 1./min(tau1, tau0) # a1 > a0\n    a0 = 1./max(tau1, tau0)\n    ar = np.array([(a1*frequency + 2.) * (a0*frequency + 2.), 2.*a1*a0*frequency**2 - 8.,\n        (a1*frequency - 2.) * (a0*frequency - 2.)]) / ((a1 - a0) * frequency**2)\n    ma = np.array([1., 2., 1.])\n\n    # matrices for ARMA model\n    i = np.arange(2, n)\n    A = cv.spmatrix(np.tile(ar, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n    M = cv.spmatrix(np.tile(ma, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n\n    # spline\n    delta_knot_s = int(round(delta_knot / frequency))\n    spl = np.r_[np.arange(1.,delta_knot_s), np.arange(delta_knot_s, 0., -1.)] # order 1\n    spl = np.convolve(spl, spl, 'full')\n    spl /= max(spl)\n    # matrix of spline regressors\n    i = np.c_[np.arange(-(len(spl)//2), (len(spl)+1)//2)] + np.r_[np.arange(0, n, delta_knot_s)]\n    nB = i.shape[1]\n    j = np.tile(np.arange(nB), (len(spl),1))\n    p = np.tile(spl, (nB,1)).T\n    valid = (i >= 0) & (i < n)\n    B = cv.spmatrix(p[valid], i[valid], j[valid])\n\n    # trend\n    C = cv.matrix(np.c_[np.ones(n), np.arange(1., n+1.)/n])\n    nC = C.size[1]\n\n    # Solve the problem:\n    # .5*(M*q + B*l + C*d - eda)^2 + alpha*sum(A,1)*p + .5*gamma*l'*l\n    # s.t. A*q >= 0\n\n    if verbose is False:\n        options[\"show_progress\"] = False\n    old_options = cv.solvers.options.copy()\n    cv.solvers.options.clear()\n    cv.solvers.options.update(options)\n    if solver == 'conelp':\n        # Use conelp\n        z = lambda m,n: cv.spmatrix([],[],[],(m,n))\n        G = cv.sparse([[-A,z(2,n),M,z(nB+2,n)],[z(n+2,nC),C,z(nB+2,nC)],\n                    [z(n,1),-1,1,z(n+nB+2,1)],[z(2*n+2,1),-1,1,z(nB,1)],\n                    [z(n+2,nB),B,z(2,nB),cv.spmatrix(1.0, range(nB), range(nB))]])\n        h = cv.matrix([z(n,1),.5,.5,eda,.5,.5,z(nB,1)])\n        c = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T,z(nC,1),1,gamma,z(nB,1)])\n        res = cv.solvers.conelp(c, G, h, dims={'l':n,'q':[n+2,nB+2],'s':[]})\n        obj = res['primal objective']\n    else:\n        # Use qp\n        Mt, Ct, Bt = M.T, C.T, B.T\n        H = cv.sparse([[Mt*M, Ct*M, Bt*M], [Mt*C, Ct*C, Bt*C],\n                    [Mt*B, Ct*B, Bt*B+gamma*cv.spmatrix(1.0, range(nB), range(nB))]])\n        f = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T - Mt*eda,  -(Ct*eda), -(Bt*eda)])\n        res = cv.solvers.qp(H, f, cv.spmatrix(-A.V, A.I, A.J, (n,len(f))),\n                            cv.matrix(0., (n,1)), solver=solver)\n        obj = res['primal objective'] + .5 * (eda.T * eda)\n    cv.solvers.options.clear()\n    cv.solvers.options.update(old_options)\n\n    l = res['x'][-nB:]\n    d = res['x'][n:n+nC]\n    tonic = B*l + C*d\n    q = res['x'][:n]\n    p = A * q\n    phasic = M * q\n    e = eda - phasic - tonic\n\n    phasic = np.array(phasic)[:,0]\n#    results = (np.array(a).ravel() for a in (r, t, p, l, d, e, obj))\n\n    return(tonic, phasic)", "response": "This function implements the convex optimization approach to electrodermal activity processing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new base - 2 neurokit - base SCR for the given signal.", "response": "def eda_scr(signal, sampling_rate=1000, treshold=0.1, method=\"fast\"):\n    \"\"\"\n    Skin-Conductance Responses extraction algorithm.\n\n    Parameters\n    ----------\n    signal :  list or array\n        EDA signal array.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    treshold : float\n        SCR minimum treshold (in terms of signal standart deviation).\n    method : str\n        \"fast\" or \"slow\". Either use a gradient-based approach or a local extrema one.\n\n    Returns\n    ----------\n    onsets, peaks, amplitudes, recoveries : lists\n        SCRs features.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> onsets, peaks, amplitudes, recoveries = nk.eda_scr(eda_signal)\n\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - biosppy\n    - numpy\n    - pandas\n\n    *See Also*\n\n    - BioSPPy: https://github.com/PIA-Group/BioSPPy\n\n\n    References\n    -----------\n    - Kim, K. H., Bang, S. W., & Kim, S. R. (2004). Emotion recognition system using short-term monitoring of physiological signals. Medical and biological engineering and computing, 42(3), 419-427.\n    - Gamboa, H. (2008). Multi-Modal Behavioral Biometrics Based on HCI and Electrophysiology (Doctoral dissertation, PhD thesis, Universidade T\u00e9cnica de Lisboa, Instituto Superior T\u00e9cnico).\n    \"\"\"\n    # Processing\n    # ===========\n    if method == \"slow\":\n        # Compute gradient (sort of derivative)\n        gradient = np.gradient(signal)\n        # Smoothing\n        size = int(0.1 * sampling_rate)\n        smooth, _ = biosppy.tools.smoother(signal=gradient, kernel='bartlett', size=size, mirror=True)\n        # Find zero-crossings\n        zeros, = biosppy.tools.zero_cross(signal=smooth, detrend=True)\n\n        # Separate onsets and peaks\n        onsets = []\n        peaks = []\n        for i in zeros:\n            if smooth[i+1] > smooth[i-1]:\n                onsets.append(i)\n            else:\n                peaks.append(i)\n        peaks = np.array(peaks)\n        onsets = np.array(onsets)\n    else:\n        # find extrema\n        peaks, _ = biosppy.tools.find_extrema(signal=signal, mode='max')\n        onsets, _ = biosppy.tools.find_extrema(signal=signal, mode='min')\n\n    # Keep only pairs\n    peaks = peaks[peaks > onsets[0]]\n    onsets = onsets[onsets < peaks[-1]]\n\n    # Artifact Treatment\n    # ====================\n    # Compute rising times\n    risingtimes = peaks-onsets\n    risingtimes = risingtimes/sampling_rate*1000\n\n    peaks = peaks[risingtimes > 100]\n    onsets = onsets[risingtimes > 100]\n\n    # Compute amplitudes\n    amplitudes = signal[peaks]-signal[onsets]\n\n    # Remove low amplitude variations\n    mask = amplitudes > np.std(signal)*treshold\n    peaks = peaks[mask]\n    onsets = onsets[mask]\n    amplitudes = amplitudes[mask]\n\n    # Recovery moments\n    recoveries = []\n    for x, peak in enumerate(peaks):\n        try:\n            window = signal[peak:onsets[x+1]]\n        except IndexError:\n            window = signal[peak:]\n        recovery_amp = signal[peak]-amplitudes[x]/2\n        try:\n            smaller = find_closest_in_list(recovery_amp, window, \"smaller\")\n            recovery_pos = peak + list(window).index(smaller)\n            recoveries.append(recovery_pos)\n        except ValueError:\n            recoveries.append(np.nan)\n    recoveries = np.array(recoveries)\n\n    return(onsets, peaks, amplitudes, recoveries)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract event - related EDA and Skin Conductance Response features.", "response": "def eda_EventRelated(epoch, event_length, window_post=4):\n    \"\"\"\n    Extract event-related EDA and Skin Conductance Response (SCR).\n\n    Parameters\n    ----------\n    epoch : pandas.DataFrame\n        An epoch contains in the epochs dict returned by :function:`neurokit.create_epochs()` on dataframe returned by :function:`neurokit.bio_process()`. Index must range from -4s to +4s (relatively to event onset and end).\n    event_length : int\n        Event's length in seconds.\n    window_post : float\n        Post-stimulus window size (in seconds) to include eventual responses (usually 3 or 4).\n\n    Returns\n    ----------\n    EDA_Response : dict\n        Event-related EDA response features.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> bio = nk.bio_process(ecg=data[\"ECG\"], rsp=data[\"RSP\"], eda=data[\"EDA\"], sampling_rate=1000, add=data[\"Photosensor\"])\n    >>> df = bio[\"df\"]\n    >>> events = nk.find_events(df[\"Photosensor\"], cut=\"lower\")\n    >>> epochs = nk.create_epochs(df, events[\"onsets\"], duration=7, onset=-0.5)\n    >>> for epoch in epochs:\n    >>>     bio_response = nk.bio_EventRelated(epoch, event_length=4, window_post=3)\n\n    Notes\n    ----------\n    **Looking for help**: *Experimental*: respiration artifacts correction needs to be implemented.\n\n    *Details*\n\n    - **EDA_Peak**: Max of EDA (in a window starting 1s after the stim onset) minus baseline.\n    - **SCR_Amplitude**: Peak of SCR. If no SCR, returns NA.\n    - **SCR_Magnitude**: Peak of SCR. If no SCR, returns 0.\n    - **SCR_Amplitude_Log**: log of 1+amplitude.\n    - **SCR_Magnitude_Log**: log of 1+magnitude.\n    - **SCR_PeakTime**: Time of peak.\n    - **SCR_Latency**: Time between stim onset and SCR onset.\n    - **SCR_RiseTime**: Time between SCR onset and peak.\n    - **SCR_Strength**: *Experimental*: peak divided by latency. Angle of the line between peak and onset.\n    - **SCR_RecoveryTime**: Time between peak and recovery point (half of the amplitude).\n\n\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - numpy\n    - pandas\n\n    *See Also*\n\n    - https://www.biopac.com/wp-content/uploads/EDA-SCR-Analysis.pdf\n\n    References\n    -----------\n    - Schneider, R., Schmidt, S., Binder, M., Sch\u00e4fer, F., & Walach, H. (2003). Respiration-related artifacts in EDA recordings: introducing a standardized method to overcome multiple interpretations. Psychological reports, 93(3), 907-920.\n    - Leiner, D., Fahr, A., & Fr\u00fch, H. (2012). EDA positive change: A simple algorithm for electrodermal activity to measure general audience arousal during media exposure. Communication Methods and Measures, 6(4), 237-250.\n    \"\"\"\n    # Initialization\n    EDA_Response = {}\n    window_end = event_length + window_post\n\n    # Sanity check\n    if epoch.index[-1]-event_length < 1:\n        print(\"NeuroKit Warning: eda_EventRelated(): your epoch only lasts for about %.2f s post stimulus. You might lose some SCRs.\" %(epoch.index[-1]-event_length))\n\n\n    # EDA Based\n    # =================\n    # This is a basic and bad model\n    if \"EDA_Filtered\" in epoch.columns:\n        baseline = epoch[\"EDA_Filtered\"][0:1].min()\n        eda_peak = epoch[\"EDA_Filtered\"][1:window_end].max()\n        EDA_Response[\"EDA_Peak\"] = eda_peak - baseline\n\n\n    # SCR Based\n    # =================\n    if \"SCR_Onsets\" in epoch.columns:\n        # Computation\n        peak_onset = epoch[\"SCR_Onsets\"][1:window_end].idxmax()\n        if pd.notnull(peak_onset):\n            amplitude = epoch[\"SCR_Peaks\"][peak_onset:window_end].max()\n            peak_time = epoch[\"SCR_Peaks\"][peak_onset:window_end].idxmax()\n\n            if pd.isnull(amplitude):\n                magnitude = 0\n            else:\n                magnitude = amplitude\n\n            risetime = peak_time - peak_onset\n            if risetime > 0:\n                strength = magnitude/risetime\n            else:\n                strength = np.nan\n\n            if pd.isnull(peak_time) is False:\n                recovery = epoch[\"SCR_Recoveries\"][peak_time:window_end].idxmax() - peak_time\n            else:\n                recovery = np.nan\n\n        else:\n            amplitude = np.nan\n            magnitude = 0\n            risetime = np.nan\n            strength = np.nan\n            peak_time = np.nan\n            recovery = np.nan\n\n        # Storage\n        EDA_Response[\"SCR_Amplitude\"] = amplitude\n        EDA_Response[\"SCR_Magnitude\"] = magnitude\n        EDA_Response[\"SCR_Amplitude_Log\"] = np.log(1+amplitude)\n        EDA_Response[\"SCR_Magnitude_Log\"] = np.log(1+magnitude)\n        EDA_Response[\"SCR_Latency\"] = peak_onset\n        EDA_Response[\"SCR_PeakTime\"] = peak_time\n        EDA_Response[\"SCR_RiseTime\"] = risetime\n        EDA_Response[\"SCR_Strength\"] = strength  # Experimental\n        EDA_Response[\"SCR_RecoveryTime\"] = recovery\n\n\n\n\n    # Artifact Correction\n    # ====================\n    # TODO !!\n    # Respiration artifacts\n#    if \"RSP_Filtered\" in epoch.columns:\n#        pass  # I Dunno, maybe with granger causality or something?\n\n\n    return(EDA_Response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the complexity of a single EEG object.", "response": "def eeg_complexity(eeg, sampling_rate, times=None, index=None, include=\"all\", exclude=None, hemisphere=\"both\", central=True, verbose=True, shannon=True, sampen=True, multiscale=True, spectral=True, svd=True, correlation=True, higushi=True, petrosian=True, fisher=True, hurst=True, dfa=True, lyap_r=False, lyap_e=False, names=\"Complexity\"):\n    \"\"\"\n    Compute complexity indices of epochs or raw object.\n\n    DOCS INCOMPLETE :(\n    \"\"\"\n\n\n    data = eeg_to_df(eeg, index=index, include=include, exclude=exclude, hemisphere=hemisphere, central=central)\n\n    # if data was Raw, make as if it was an Epoch so the following routine is only written once\n    if isinstance(data, dict) is False:\n        data = {0: data}\n\n    # Create time windows\n    if isinstance(times, tuple):\n        times = list(times)\n    if isinstance(times, list):\n        if isinstance(times[0], list) is False:\n            times = [times]\n    else:\n        times = [[0, None]]\n\n\n    # Deal with names\n    if isinstance(names, str):\n        prefix = [names] * len(times)\n        if len(times) > 1:\n            for time_index, time_window in enumerate(times):\n                prefix[time_index] = prefix[time_index] + \"_%.2f_%.2f\" %(time_window[0], time_window[1])\n    else:\n        prefix = names\n\n\n    # Iterate\n    complexity_all = pd.DataFrame()\n    for time_index, time_window in enumerate(times):\n        if len(times) > 1 and verbose is True:\n            print(\"Computing complexity features... window \" + str(time_window) + \"/\" + str(len(times)))\n\n        complexity_features = {}\n        # Compute complexity for each channel for each epoch\n        index = 0\n        for epoch_index, epoch in data.items():\n            if len(times) == 1 and verbose is True:\n                print(\"Computing complexity features... \" + str(round(index/len(data.items())*100, 2)) + \"%\")\n            index +=1\n\n            df = epoch[time_window[0]:time_window[1]].copy()\n\n            complexity_features[epoch_index] = {}\n            for channel in df:\n                signal = df[channel].values\n\n                features = complexity(signal, sampling_rate=sampling_rate, shannon=shannon, sampen=sampen, multiscale=multiscale, spectral=spectral, svd=svd, correlation=correlation, higushi=higushi, petrosian=petrosian, fisher=fisher, hurst=hurst, dfa=dfa, lyap_r=lyap_r, lyap_e=lyap_e)\n\n                for key, feature in features.items():\n                    if key in complexity_features[epoch_index].keys():\n                        complexity_features[epoch_index][key].append(feature)\n                    else:\n                        complexity_features[epoch_index][key] = [feature]\n\n        for epoch_index, epoch in complexity_features.items():\n            for feature in epoch:\n                complexity_features[epoch_index][feature] = pd.Series(complexity_features[epoch_index][feature]).mean()\n\n        # Convert to dataframe\n        complexity_features = pd.DataFrame.from_dict(complexity_features, orient=\"index\")\n        complexity_features.columns = [prefix[time_index] + \"_\" + s for s in complexity_features.columns]\n\n\n        complexity_all = pd.concat([complexity_all, complexity_features], axis=1)\n    return(complexity_all)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_dprime(n_Hit=None, n_Miss=None, n_FA=None, n_CR=None):\n\n    # Ratios\n    hit_rate = n_Hit/(n_Hit + n_Miss)\n    fa_rate = n_FA/(n_FA + n_CR)\n\n\n    # Adjusted ratios\n    hit_rate_adjusted = (n_Hit+ 0.5)/((n_Hit+ 0.5) + n_Miss + 1)\n    fa_rate_adjusted = (n_FA+ 0.5)/((n_FA+ 0.5) + n_CR + 1)\n\n\n    # dprime\n    dprime = scipy.stats.norm.ppf(hit_rate_adjusted) - scipy.stats.norm.ppf(fa_rate_adjusted)\n\n    # beta\n    zhr = scipy.stats.norm.ppf(hit_rate_adjusted)\n    zfar = scipy.stats.norm.ppf(fa_rate_adjusted)\n    beta = np.exp(-zhr*zhr/2 + zfar*zfar/2)\n\n    # aprime\n    a = 1/2+((hit_rate-fa_rate)*(1+hit_rate-fa_rate) / (4*hit_rate*(1-fa_rate)))\n    b = 1/2-((fa_rate-hit_rate)*(1+fa_rate-hit_rate) / (4*fa_rate*(1-hit_rate)))\n\n    if fa_rate > hit_rate:\n        aprime = b\n    elif fa_rate < hit_rate:\n        aprime = a\n    else:\n        aprime = 0.5\n\n    # bppd\n    bppd = ((1-hit_rate)*(1-fa_rate)-hit_rate*fa_rate) / ((1-hit_rate)*(1-fa_rate)+hit_rate*fa_rate)\n\n    # c\n    c = -(scipy.stats.norm.ppf(hit_rate_adjusted) + scipy.stats.norm.ppf(fa_rate_adjusted))/2\n\n    parameters = dict(dprime=dprime, beta=beta, aprime=aprime, bppd=bppd, c=c)\n    return(parameters)", "response": "Computes the d beta aprime b''d and c parameters based on the signal detection theory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the traditional BMI for a given set of body mass index values.", "response": "def compute_BMI(height, weight, age, sex):\n    \"\"\"\n    Returns the traditional BMI, the 'new' Body Mass Index and estimates the Body Fat Percentage (BFP; Deurenberg et al., 1991).\n\n    Parameters\n    ----------\n    height : float\n        Height in cm.\n    weight : float\n        Weight in kg.\n    age : float\n        Age in years.\n    sex : str\n        \"m\" or \"f\".\n\n    Returns\n    ----------\n    bmi : dict\n        dict containing values and their interpretations.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> nk.compute_BMI(height=166, weight=54, age=22, sex=\"f\")\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *See Also*\n\n    - https://people.maths.ox.ac.uk/trefethen/bmi.html\n\n    References\n    -----------\n    - Deurenberg, P., Andreoli, A., Borg, P., & Kukkonen-Harjula, K. (2001). The validity of predicted body fat percentage from body mass index and from impedance in samples of five European populations. European Journal of Clinical Nutrition, 55(11), 973.\n    - Deurenberg, P., Weststrate, J. A., & Seidell, J. C. (1991). Body mass index as a measure of body fatness: age-and sex-specific prediction formulas. British journal of nutrition, 65(02), 105-114.\n    \"\"\"\n    # BMI\n    height = height/100\n    bmi = {}\n    bmi[\"BMI_old\"] = weight/(height**2)\n    bmi[\"BMI_new\"] = 1.3*weight/height**2.5\n    if bmi[\"BMI_new\"] < 15:\n        bmi[\"BMI_category\"] = \"Very severely underweight\"\n    if 15 < bmi[\"BMI_new\"] < 16:\n         bmi[\"BMI_category\"] = \"Severely underweight\"\n    if 16 < bmi[\"BMI_new\"] < 18.5:\n         bmi[\"BMI_category\"] = \"Underweight\"\n    if 18.5 < bmi[\"BMI_new\"] < 25:\n         bmi[\"BMI_category\"] = \"Healthy weight\"\n    if 25 < bmi[\"BMI_new\"] < 30:\n         bmi[\"BMI_category\"] = \"Overweight\"\n    if 30 < bmi[\"BMI_new\"] < 35:\n         bmi[\"BMI_category\"] = \"Moderately obese\"\n    if 35 < bmi[\"BMI_new\"] < 40:\n         bmi[\"BMI_category\"] = \"Severely obese\"\n    if bmi[\"BMI_new\"] > 40:\n         bmi[\"BMI_category\"] = \"Very severely obese\"\n\n    # BFP\n    if sex.lower() == \"m\":\n        sex = 1\n    else:\n        sex = 0\n\n    if age <= 15:\n        bmi[\"BFP\"] = 1.51*bmi[\"BMI_old\"]-0.70*age-3.6*sex+1.4\n    else:\n        bmi[\"BFP\"] = 1.20*bmi[\"BMI_old\"] + 0.23*age-10.8*sex-5.4\n\n    if sex == 1:\n        if bmi[\"BFP\"] < 2:\n            bmi[\"BFP_category\"] = \"Critical\"\n        if 2 <= bmi[\"BFP\"] < 6:\n            bmi[\"BFP_category\"] = \"Essential\"\n        if 6 <= bmi[\"BFP\"] < 13:\n            bmi[\"BFP_category\"] = \"Athletic\"\n        if 13 <= bmi[\"BFP\"] < 17:\n            bmi[\"BFP_category\"] = \"Fitness\"\n        if 17 <= bmi[\"BFP\"] < 22:\n            bmi[\"BFP_category\"] = \"Average\"\n        if 22 <= bmi[\"BFP\"] < 30:\n            bmi[\"BFP_category\"] = \"Overweight\"\n        if bmi[\"BFP\"] >= 30:\n            bmi[\"BFP_category\"] = \"Obese\"\n    else:\n        if bmi[\"BFP\"] < 10:\n            bmi[\"BFP_category\"] = \"Critical\"\n        if 10 <= bmi[\"BFP\"] < 14:\n            bmi[\"BFP_category\"] = \"Essential\"\n        if 14 <= bmi[\"BFP\"] < 21:\n            bmi[\"BFP_category\"] = \"Athletic\"\n        if 21 <= bmi[\"BFP\"] < 25:\n            bmi[\"BFP_category\"] = \"Fitness\"\n        if 25 <= bmi[\"BFP\"] < 31:\n            bmi[\"BFP_category\"] = \"Average\"\n        if 31 <= bmi[\"BFP\"] < 40:\n            bmi[\"BFP_category\"] = \"Overweight\"\n        if bmi[\"BFP\"] >= 40:\n            bmi[\"BFP_category\"] = \"Obese\"\n\n\n\n    return(bmi)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute interoceptive accuracy according to Garfinkel et al. 2015.", "response": "def compute_interoceptive_accuracy(nbeats_real, nbeats_reported):\n    \"\"\"\n    Computes interoceptive accuracy according to Garfinkel et al., (2015).\n\n    Parameters\n    ----------\n    nbeats_real : int or list\n        Real number of heartbeats.\n    nbeats_reported : int or list\n        Reported number of heartbeats.\n\n    Returns\n    ----------\n    accuracy : float or list\n        Objective accuracy in detecting internal bodily sensations. It is the central construct underpinning other interoceptive measures (Garfinkel et al., 2015).\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> nk.compute_interoceptive_accuracy(5, 3)\n\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - numpy\n\n    References\n    -----------\n    - Garfinkel, S. N., Seth, A. K., Barrett, A. B., Suzuki, K., & Critchley, H. D. (2015). Knowing your own heart: distinguishing interoceptive accuracy from interoceptive awareness. Biological psychology, 104, 65-74.\n    \"\"\"\n    # Convert to array if list\n    if isinstance(nbeats_real, list):\n        nbeats_real = np.array(nbeats_real)\n        nbeats_reported = np.array(nbeats_reported)\n    # Compute accuracy\n    accuracy = 1 - (abs(nbeats_real-nbeats_reported))/((nbeats_real+nbeats_reported)/2)\n\n    return(accuracy)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd response to the data.", "response": "def add_response(self, response, value):\n        \"\"\"\n        Add response to staircase.\n\n        Parameters\n        ----------\n        response : int or bool\n            0 or 1.\n        value : int or float\n            Signal corresponding to response.\n        \"\"\"\n        if value != \"stop\":\n            self.X = pd.concat([self.X, pd.DataFrame({\"Signal\":[value]})])\n            self.y = np.array(list(self.y) + [response])\n            if len(set(list(self.y))) > 1:\n                self.model = self.fit_model(self.X , self.y)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, reset=True):\n        t = (builtin_time.clock()-self.clock)*1000\n\n        if reset is True:\n            self.reset()\n        return(t)", "response": "Get time since last initialisation or reset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_epochs(data, events_onsets, sampling_rate=1000, duration=1, onset=0, index=None):\n    # Convert ints to arrays if needed\n    if isinstance(duration, list) or isinstance(duration, np.ndarray):\n        duration = np.array(duration)\n    else:\n        duration = np.array([duration]*len(events_onsets))\n\n    if isinstance(onset, list) or isinstance(onset, np.ndarray):\n        onset = np.array(onset)\n    else:\n        onset = np.array([onset]*len(events_onsets))\n\n    if isinstance(data, list) or isinstance(data, np.ndarray) or isinstance(data, pd.Series):\n        data = pd.DataFrame({\"Signal\": list(data)})\n\n    # Store durations\n    duration_in_s = duration.copy()\n    onset_in_s = onset.copy()\n\n    # Convert to timepoints\n    duration = duration*sampling_rate\n    onset = onset*sampling_rate\n\n\n\n    # Create the index\n    if index is None:\n        index = list(range(len(events_onsets)))\n    else:\n        if len(list(set(index))) != len(index):\n            print(\"NeuroKit Warning: create_epochs(): events_names does not contain uniques names, replacing them by numbers.\")\n            index = list(range(len(events_onsets)))\n        else:\n            index = list(index)\n\n\n    # Create epochs\n    epochs = {}\n    for event, event_onset in enumerate(events_onsets):\n\n        epoch_onset = int(event_onset + onset[event])\n        epoch_end = int(event_onset+duration[event]+1)\n\n        epoch = data[epoch_onset:epoch_end].copy()\n        epoch.index  = np.linspace(start=onset_in_s[event], stop=duration_in_s[event], num=len(epoch), endpoint=True)\n\n\n        relative_time = np.linspace(start=onset[event], stop=duration[event], num=len(epoch), endpoint=True).astype(int).tolist()\n        absolute_time = np.linspace(start=epoch_onset, stop=epoch_end, num=len(epoch), endpoint=True).astype(int).tolist()\n\n        epoch[\"Epoch_Relative_Time\"] = relative_time\n        epoch[\"Epoch_Absolute_Time\"] = absolute_time\n\n        epochs[index[event]] = epoch\n\n\n\n    return(epochs)", "response": "Create a new epochs dictionary for a given data frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interpolate(values, value_times, sampling_rate=1000):\n#    values=RRis.copy()\n#    value_times=beats_times.copy()\n    # Preprocessing\n    initial_index = value_times[0]\n    value_times = np.array(value_times) - initial_index\n\n    # fit a 3rd degree spline on the data.\n    spline = scipy.interpolate.splrep(x=value_times, y=values, k=3, s=0)  # s=0 guarantees that it will pass through ALL the given points\n    x = np.arange(0, value_times[-1], 1)\n    # Get the values indexed per time\n    signal = scipy.interpolate.splev(x=x, tck=spline, der=0)\n    # Transform to series\n    signal = pd.Series(signal)\n    signal.index = np.array(np.arange(initial_index, initial_index+len(signal), 1))\n\n    return(signal)", "response": "Interpolate a dataframe of values at a given time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_peaks(signal):\n    derivative = np.gradient(signal, 2)\n    peaks = np.where(np.diff(np.sign(derivative)))[0]\n    return(peaks)", "response": "Locate peaks based on derivative."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of names corresponding to the given list of frequencies.", "response": "def eeg_name_frequencies(freqs):\n    \"\"\"\n    Name frequencies according to standart classifications.\n\n    Parameters\n    ----------\n    freqs : list or numpy.array\n        list of floats containing frequencies to classify.\n\n    Returns\n    ----------\n    freqs_names : list\n        Named frequencies\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> nk.eeg_name_frequencies([0.5, 1.5, 3, 5, 7, 15])\n\n    Notes\n    ----------\n    *Details*\n\n    - Delta: 1-3Hz\n    - Theta: 4-7Hz\n    - Alpha1: 8-9Hz\n    - Alpha2: 10-12Hz\n    - Beta1: 13-17Hz\n    - Beta2: 18-30Hz\n    - Gamma1: 31-40Hz\n    - Gamma2: 41-50Hz\n    - Mu: 8-13Hz\n\n    *Authors*\n\n    - Dominique Makowski (https://github.com/DominiqueMakowski)\n\n    References\n    ------------\n    - None\n    \"\"\"\n    freqs = list(freqs)\n    freqs_names = []\n    for freq in freqs:\n        if freq < 1:\n            freqs_names.append(\"UltraLow\")\n        elif freq <= 3:\n            freqs_names.append(\"Delta\")\n        elif freq <= 7:\n            freqs_names.append(\"Theta\")\n        elif freq <= 9:\n            freqs_names.append(\"Alpha1/Mu\")\n        elif freq <= 12:\n            freqs_names.append(\"Alpha2/Mu\")\n        elif freq <= 13:\n            freqs_names.append(\"Beta1/Mu\")\n        elif freq <= 17:\n            freqs_names.append(\"Beta1\")\n        elif freq <= 30:\n            freqs_names.append(\"Beta2\")\n        elif freq <= 40:\n            freqs_names.append(\"Gamma1\")\n        elif freq <= 50:\n            freqs_names.append(\"Gamma2\")\n        else:\n            freqs_names.append(\"UltraHigh\")\n    return(freqs_names)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the power - spectral density of a single - time site.", "response": "def eeg_psd(raw, sensors_include=\"all\", sensors_exclude=None, fmin=0.016, fmax=60, method=\"multitaper\", proj=False):\n    \"\"\"\n    Compute Power-Spectral Density (PSD).\n\n    Parameters\n    ----------\n    raw : mne.io.Raw\n        Raw EEG data.\n    sensors_include : str\n        Sensor area to include. See :func:`neurokit.eeg_select_sensors()`.\n    sensors_exclude : str\n        Sensor area to exclude. See :func:`neurokit.eeg_select_sensors()`.\n    fmin : float\n        Min frequency of interest.\n    fmax: float\n        Max frequency of interest.\n    method : str\n        \"multitaper\" or \"welch\".\n    proj : bool\n        add projectors.\n\n    Returns\n    ----------\n    mean_psd : pandas.DataFrame\n        Averaged PSDs.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n\n\n    Notes\n    ----------\n    *Details*\n\n    - Delta: 1-3Hz\n    - Theta: 4-7Hz\n    - Alpha1: 8-9Hz\n    - Alpha2: 10-12Hz\n    - Beta1: 13-17Hz\n    - Beta2: 18-30Hz\n    - Gamma1: 31-40Hz\n    - Gamma2: 41-50Hz\n    - Mu: 8-13Hz\n\n    *Authors*\n\n    - Dominique Makowski (https://github.com/DominiqueMakowski)\n\n    References\n    ------------\n    - None\n    \"\"\"\n    picks = mne.pick_types(raw.info, include=eeg_select_electrodes(include=sensors_include, exclude=sensors_exclude), exclude=\"bads\")\n\n    if method == \"multitaper\":\n        psds, freqs = mne.time_frequency.psd_multitaper(raw,\n                                                        fmin=fmin,\n                                                        fmax=fmax,\n                                                        low_bias=True,\n                                                        proj=proj,\n                                                        picks=picks)\n    else:\n        psds, freqs = mne.time_frequency.psd_welch(raw,\n                                                        fmin=fmin,\n                                                        fmax=fmax,\n                                                        proj=proj,\n                                                        picks=picks)\n    tf = pd.DataFrame(psds)\n    tf.columns = eeg_name_frequencies(freqs)\n    tf = tf.mean(axis=0)\n\n    mean_psd = {}\n    for freq in [\"UltraLow\", \"Delta\", \"Theta\", \"Alpha\", \"Alpha1\", \"Alpha2\", \"Mu\", \"Beta\", \"Beta1\", \"Beta2\", \"Gamma\", \"Gamma1\", \"Gamma2\", \"UltraHigh\"]:\n        mean_psd[freq] = tf[[freq in s for s in tf.index]].mean()\n    mean_psd = pd.DataFrame.from_dict(mean_psd, orient=\"index\").T\n\n    return(mean_psd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eeg_create_frequency_bands(bands=\"all\", step=1):\n    if bands == \"all\" or bands == \"All\":\n        bands = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"Mu\"]\n    if \"Alpha\" in bands:\n        bands.remove(\"Alpha\")\n        bands += [\"Alpha1\", \"Alpha2\"]\n    if \"Beta\" in bands:\n        bands.remove(\"Beta\")\n        bands += [\"Beta1\", \"Beta2\"]\n    if \"Gamma\" in bands:\n        bands.remove(\"Gamma\")\n        bands += [\"Gamma1\", \"Gamma2\"]\n\n    frequencies = {}\n    for band in bands:\n        if band == \"Delta\":\n            frequencies[band] = np.arange(1, 3+0.1, step)\n        if band == \"Theta\":\n            frequencies[band] = np.arange(4, 7+0.1, step)\n        if band == \"Alpha1\":\n            frequencies[band] = np.arange(8, 9+0.1, step)\n        if band == \"Alpha2\":\n            frequencies[band] = np.arange(10, 12+0.1, step)\n        if band == \"Beta1\":\n            frequencies[band] = np.arange(13, 17+0.1, step)\n        if band == \"Beta2\":\n            frequencies[band] = np.arange(18, 30+0.1, step)\n        if band == \"Gamma1\":\n            frequencies[band] = np.arange(31, 40+0.1, step)\n        if band == \"Gamma2\":\n            frequencies[band] = np.arange(41, 50+0.1, step)\n        if band == \"Mu\":\n            frequencies[band] = np.arange(8, 13+0.1, step)\n    return(frequencies)", "response": "Create a frequency array for the given bands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mad(var, constant=1):\n    median = np.median(var)\n    mad = np.median(np.abs(var - median))\n    mad = mad*constant\n    return(mad)", "response": "Returns a MAD of the given variance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming an array serie or list into Z scores.", "response": "def z_score(raw_scores, center=True, scale=True):\n    \"\"\"\n    Transform an array, serie or list into Z scores (scaled and centered scores).\n\n    Parameters\n    ----------\n    raw_scores : list, ndarray or pandas.Series\n        ECG signal array.\n    centered : bool\n        Center the array (mean = 0).\n    scale :  bool\n        scale the array (sd = 1).\n\n    Returns\n    ----------\n    z_scores : pandas.DataFrame\n        The normalized scores.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> nk.z_score([3, 1, 2, 4, 6])\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - pandas\n    \"\"\"\n    df = pd.DataFrame(raw_scores)\n\n    mean = df.mean(axis=0)\n    sd = df.std(axis=0)\n    z_scores = (df - mean)/sd\n\n    return(z_scores)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding outliers in a set of data.", "response": "def find_outliers(data, treshold=2.58):\n    \"\"\"\n    Identify outliers (abnormal values) using the standart deviation.\n\n    Parameters\n    ----------\n    data : list or ndarray\n        Data array\n    treshold : float\n        Maximum deviation (in terms of standart deviation). Rule of thumb of a gaussian distribution: 2.58 = rejecting 1%, 2.33 = rejecting 2%, 1.96 = 5% and 1.28 = rejecting 10%.\n\n    Returns\n    ----------\n    outliers : ndarray\n        A list of True/False with True being the outliers.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> outliers = nk.find_outliers([1, 2, 1, 5, 666, 4, 1 ,3, 5])\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Dependencies*\n\n    - numpy\n    \"\"\"\n    outliers = []\n    mean = np.mean(data)\n    std = np.std(data)\n    for i in data:\n        if abs(i - mean)/std < treshold:\n            outliers.append(False)\n        else:\n            outliers.append(True)\n    outliers = np.array(outliers)\n    return (outliers)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normal_range(mean, sd, treshold=1.28):\n    bottom = mean - sd*treshold\n    top = mean + sd*treshold\n    return(bottom, top)", "response": "Returns a bottom and top limit on a normal distribution portion based on a treshold."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_following_duplicates(array):\n    array = array[:]\n\n\n    uniques = []\n    for i in range(len(array)):\n        if i == 0:\n            uniques.append(True)\n        else:\n            if array[i] == array[i-1]:\n                uniques.append(False)\n            else:\n                uniques.append(True)\n\n    return(uniques)", "response": "Find the duplicates that are following themselves."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the closest number in the array from x.", "response": "def find_closest_in_list(number, array, direction=\"both\", strictly=False):\n    \"\"\"\n    Find the closest number in the array from x.\n\n    Parameters\n    ----------\n    number : float\n        The number.\n    array : list\n        The list to look in.\n    direction : str\n        \"both\" for smaller or greater, \"greater\" for only greater numbers and \"smaller\" for the closest smaller.\n    strictly : bool\n        False for stricly superior or inferior or True for including equal.\n\n    Returns\n    ----------\n    closest : int\n        The closest number in the array.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> nk.find_closest_in_list(1.8, [3, 5, 6, 1, 2])\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    \"\"\"\n    if direction == \"both\":\n        closest = min(array, key=lambda x:abs(x-number))\n    if direction == \"smaller\":\n        if strictly is True:\n            closest = max(x for x in array if x < number)\n        else:\n            closest = max(x for x in array if x <= number)\n    if direction == \"greater\":\n        if strictly is True:\n            closest = min(filter(lambda x: x > number, array))\n        else:\n            closest = min(filter(lambda x: x >= number, array))\n\n    return(closest)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nautomates processing of EMG signal. Parameters ---------- emg : list, array or DataFrame EMG signal array. Can include multiple channels. sampling_rate : int Sampling rate (samples/second). emg_names : list List of EMG channel names. envelope_freqs : list [fc_h, fc_l], optional cutoff frequencies for the band-pass filter (in Hz). envelope_lfreq : number, optional cutoff frequency for the low-pass filter (in Hz). activation_treshold : float minimum amplitude of `x` to detect. activation_n_above : float minimum continuous time (in s) greater than or equal to `threshold` to detect (but see the parameter `n_below`). activation_n_below : float minimum time (in s) below `threshold` that will be ignored in the detection of `x` >= `threshold`. Returns ---------- processed_emg : dict Dict containing processed EMG features. Contains the EMG raw signal, the filtered signal and pulse onsets. This function is mainly a wrapper for the biosppy.emg.emg() function. Credits go to its authors. Example ---------- >>> import neurokit as nk >>> >>> processed_emg = nk.emg_process(emg_signal) Notes ---------- *Authors* - Dominique Makowski (https://github.com/DominiqueMakowski) *Dependencies* - biosppy - numpy - pandas *See Also* - BioSPPy: https://github.com/PIA-Group/BioSPPy References ----------- - None", "response": "def emg_process(emg, sampling_rate=1000, emg_names=None, envelope_freqs=[10, 400], envelope_lfreq=4, activation_treshold=\"default\", activation_n_above=0.25, activation_n_below=1):\n    \"\"\"\n    Automated processing of EMG signal.\n\n    Parameters\n    ----------\n    emg :  list, array or DataFrame\n        EMG signal array. Can include multiple channels.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    emg_names : list\n        List of EMG channel names.\n    envelope_freqs : list [fc_h, fc_l], optional\n            cutoff frequencies for the band-pass filter (in Hz).\n    envelope_lfreq : number, optional\n            cutoff frequency for the low-pass filter (in Hz).\n    activation_treshold : float\n        minimum amplitude of `x` to detect.\n    activation_n_above : float\n        minimum continuous time (in s) greater than or equal to `threshold` to detect (but see the parameter `n_below`).\n    activation_n_below : float\n        minimum time (in s) below `threshold` that will be ignored in the detection of `x` >= `threshold`.\n\n\n    Returns\n    ----------\n    processed_emg : dict\n        Dict containing processed EMG features.\n\n        Contains the EMG raw signal, the filtered signal and pulse onsets.\n\n        This function is mainly a wrapper for the biosppy.emg.emg() function. Credits go to its authors.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>>\n    >>> processed_emg = nk.emg_process(emg_signal)\n\n\n    Notes\n    ----------\n    *Authors*\n\n    - Dominique Makowski (https://github.com/DominiqueMakowski)\n\n    *Dependencies*\n\n    - biosppy\n    - numpy\n    - pandas\n\n    *See Also*\n\n    - BioSPPy: https://github.com/PIA-Group/BioSPPy\n\n    References\n    -----------\n    - None\n    \"\"\"\n    if emg_names is None:\n        if isinstance(emg, pd.DataFrame):\n            emg_names = emg.columns.values\n\n    emg = np.array(emg)\n    if len(np.shape(emg)) == 1:\n        emg = np.array(pd.DataFrame(emg))\n\n    if emg_names is None:\n        if np.shape(emg)[1]>1:\n            emg_names = []\n            for index in range(np.shape(emg)[1]):\n                emg_names.append(\"EMG_\" + str(index))\n        else:\n            emg_names = [\"EMG\"]\n\n\n    processed_emg = {\"df\": pd.DataFrame()}\n    for index, emg_chan in enumerate(emg.T):\n        # Store Raw signal\n        processed_emg[\"df\"][emg_names[index] + \"_Raw\"] = emg_chan\n\n        # Compute several features using biosppy\n        biosppy_emg = dict(biosppy.emg.emg(emg_chan, sampling_rate=sampling_rate, show=False))\n\n        # Store EMG pulse onsets\n        pulse_onsets = np.array([np.nan]*len(emg))\n        if len(biosppy_emg['onsets']) > 0:\n            pulse_onsets[biosppy_emg['onsets']] = 1\n        processed_emg[\"df\"][emg_names[index] + \"_Pulse_Onsets\"] = pulse_onsets\n\n        processed_emg[\"df\"][emg_names[index] + \"_Filtered\"] = biosppy_emg[\"filtered\"]\n        processed_emg[emg_names[index]] = {}\n        processed_emg[emg_names[index]][\"EMG_Pulse_Onsets\"] = biosppy_emg['onsets']\n\n        # Envelope\n        envelope = emg_linear_envelope(biosppy_emg[\"filtered\"], sampling_rate=sampling_rate, freqs=envelope_freqs, lfreq=envelope_lfreq)\n        processed_emg[\"df\"][emg_names[index] + \"_Envelope\"] = envelope\n\n        # Activation\n        if activation_treshold == \"default\":\n            activation_treshold = 1*np.std(envelope)\n        processed_emg[\"df\"][emg_names[index] + \"_Activation\"] = emg_find_activation(envelope, sampling_rate=sampling_rate, threshold=1*np.std(envelope), n_above=activation_n_above, n_below=activation_n_below)\n\n    return(processed_emg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emg_tkeo(emg):\n    emg = np.asarray(emg)\n    tkeo = np.copy(emg)\n    # Teager\u2013Kaiser Energy operator\n    tkeo[1:-1] = emg[1:-1]*emg[1:-1] - emg[:-2]*emg[2:]\n    # correct the data in the extremities\n    tkeo[0], tkeo[-1] = tkeo[1], tkeo[-2]\n\n    return(tkeo)", "response": "Calculates the Teager\u2013Kaiser Energy operator for a given EMG signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emg_linear_envelope(emg, sampling_rate=1000, freqs=[10, 400], lfreq=4):\n    emg = emg_tkeo(emg)\n\n    if np.size(freqs) == 2:\n        # band-pass filter\n        b, a = scipy.signal.butter(2, np.array(freqs)/(sampling_rate/2.), btype = 'bandpass')\n        emg = scipy.signal.filtfilt(b, a, emg)\n    if np.size(lfreq) == 1:\n        # full-wave rectification\n        envelope = abs(emg)\n        # low-pass Butterworth filter\n        b, a = scipy.signal.butter(2, np.array(lfreq)/(sampling_rate/2.), btype = 'low')\n        envelope = scipy.signal.filtfilt(b, a, envelope)\n\n    return (envelope)", "response": "r Calculates the linear envelope of a raw EMG signal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetect onset in data based on amplitude threshold.", "response": "def emg_find_activation(envelope, sampling_rate=1000, threshold=0, n_above=0.25, n_below=1):\n    \"\"\"Detects onset in data based on amplitude threshold.\n\n    Parameters\n    ----------\n    envelope : array\n        Linear envelope of EMG signal.\n    sampling_rate : int\n        Sampling rate (samples/second).\n    threshold : float\n        minimum amplitude of `x` to detect.\n    n_above : float\n        minimum continuous time (in s) greater than or equal to `threshold` to detect (but see the parameter `n_below`).\n    n_below : float\n        minimum time (in s) below `threshold` that will be ignored in the detection of `x` >= `threshold`.\n\n    Returns\n    -------\n    activation : array\n        With 1 when muscle activated and 0 when not.\n\n    Notes\n    -----\n    You might have to tune the parameters according to the signal-to-noise\n    characteristic of the data.\n\n    See this IPython Notebook [1]_.\n\n    References\n    ----------\n    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectOnset.ipynb\n    \"\"\"\n    n_above = n_above*sampling_rate\n    n_below = n_below*sampling_rate\n\n\n    envelope = np.atleast_1d(envelope).astype('float64')\n    # deal with NaN's (by definition, NaN's are not greater than threshold)\n    envelope[np.isnan(envelope)] = -np.inf\n    # indices of data greater than or equal to threshold\n    inds = np.nonzero(envelope >= threshold)[0]\n    if inds.size:\n        # initial and final indexes of continuous data\n        inds = np.vstack((inds[np.diff(np.hstack((-np.inf, inds))) > n_below+1], \\\n                          inds[np.diff(np.hstack((inds, np.inf))) > n_below+1])).T\n        # indexes of continuous data longer than or equal to n_above\n        inds = inds[inds[:, 1]-inds[:, 0] >= n_above-1, :]\n    if not inds.size:\n        inds = np.array([])  # standardize inds shape\n\n    inds = np.array(inds)\n\n    activation = np.array([0]*len(envelope))\n    for i in inds:\n        activation[i[0]:i[1]] = 1\n    return (activation)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ecg_preprocess(ecg, sampling_rate=1000, filter_type=\"FIR\", filter_band=\"bandpass\", filter_frequency=[3, 45], filter_order=0.3, segmenter=\"hamilton\"):\n    # Signal Processing\n    # =======================\n    # Transform to array\n    ecg = np.array(ecg)\n\n    # Filter signal\n    if filter_type in [\"FIR\", \"butter\", \"cheby1\", \"cheby2\", \"ellip\", \"bessel\"]:\n        order = int(filter_order * sampling_rate)\n        filtered, _, _ = biosppy.tools.filter_signal(signal=ecg,\n                                          ftype=filter_type,\n                                          band=filter_band,\n                                          order=order,\n                                          frequency=filter_frequency,\n                                          sampling_rate=sampling_rate)\n    else:\n        filtered = ecg  # filtered is not-filtered\n\n    # Segment\n    if segmenter == \"hamilton\":\n        rpeaks, = biosppy.ecg.hamilton_segmenter(signal=filtered, sampling_rate=sampling_rate)\n    elif segmenter == \"gamboa\":\n        rpeaks, = biosppy.ecg.gamboa_segmenter(signal=filtered, sampling_rate=sampling_rate, tol=0.002)\n    elif segmenter == \"engzee\":\n        rpeaks, = biosppy.ecg.engzee_segmenter(signal=filtered, sampling_rate=sampling_rate, threshold=0.48)\n    elif segmenter == \"christov\":\n        rpeaks, = biosppy.ecg.christov_segmenter(signal=filtered, sampling_rate=sampling_rate)\n    elif segmenter == \"ssf\":\n        rpeaks, = biosppy.ecg.ssf_segmenter(signal=filtered, sampling_rate=sampling_rate, threshold=20, before=0.03, after=0.01)\n    elif segmenter == \"pekkanen\":\n        rpeaks = segmenter_pekkanen(ecg=filtered, sampling_rate=sampling_rate, window_size=5.0, lfreq=5.0, hfreq=15.0)\n    else:\n        raise ValueError(\"Unknown segmenter: %s.\" % segmenter)\n\n\n    # Correct R-peak locations\n    rpeaks, = biosppy.ecg.correct_rpeaks(signal=filtered,\n                             rpeaks=rpeaks,\n                             sampling_rate=sampling_rate,\n                             tol=0.05)\n\n    # Extract cardiac cycles and rpeaks\n    cardiac_cycles, rpeaks = biosppy.ecg.extract_heartbeats(signal=filtered,\n                                           rpeaks=rpeaks,\n                                           sampling_rate=sampling_rate,\n                                           before=0.2,\n                                           after=0.4)\n\n    # Compute heart rate\n    heart_rate_idx, heart_rate = biosppy.tools.get_heart_rate(beats=rpeaks,\n                                   sampling_rate=sampling_rate,\n                                   smooth=True,\n                                   size=3)\n\n    # Get time indices\n    length = len(ecg)\n    T = (length - 1) / float(sampling_rate)\n    ts = np.linspace(0, T, length, endpoint=False)\n    heart_rate_times = ts[heart_rate_idx]\n    heart_rate_times = np.round(heart_rate_times*sampling_rate).astype(int)  # Convert heart rate times to timepoints\n\n    # what for is this line in biosppy??\n#    cardiac_cycles_tmpl = np.linspace(-0.2, 0.4, cardiac_cycles.shape[1], endpoint=False)\n\n    # Prepare Output Dataframe\n    # ==========================\n    ecg_df = pd.DataFrame({\"ECG_Raw\": np.array(ecg)})  # Create a dataframe\n    ecg_df[\"ECG_Filtered\"] = filtered  # Add filtered signal\n\n    # Add R peaks\n    rpeaks_signal = np.array([np.nan]*len(ecg))\n    rpeaks_signal[rpeaks] = 1\n    ecg_df[\"ECG_R_Peaks\"] = rpeaks_signal\n\n\n    # Heart Rate\n    try:\n        heart_rate = interpolate(heart_rate, heart_rate_times, sampling_rate)  # Interpolation using 3rd order spline\n        ecg_df[\"Heart_Rate\"] = heart_rate\n    except TypeError:\n        print(\"NeuroKit Warning: ecg_process(): Sequence too short to compute heart rate.\")\n        ecg_df[\"Heart_Rate\"] = np.nan\n\n    # Store Additional Feature\n    # ========================\n    processed_ecg = {\"df\": ecg_df,\n                     \"ECG\": {\n                            \"R_Peaks\": rpeaks\n                            }\n                     }\n\n    # Heartbeats\n    heartbeats = pd.DataFrame(cardiac_cycles).T\n    heartbeats.index = pd.date_range(pd.datetime.today(), periods=len(heartbeats), freq=str(int(1000000/sampling_rate)) + \"us\")\n    processed_ecg[\"ECG\"][\"Cardiac_Cycles\"] = heartbeats\n\n    # Waves\n    waves = ecg_wave_detector(ecg_df[\"ECG_Filtered\"], rpeaks)\n    processed_ecg[\"ECG\"].update(waves)\n\n    # Systole\n    processed_ecg[\"df\"][\"ECG_Systole\"] = ecg_systole(ecg_df[\"ECG_Filtered\"], rpeaks, waves[\"T_Waves_Ends\"])\n\n\n    return(processed_ecg)", "response": "Preprocess a set of ECG signal arrays and return a new dict of all the fields in the new ECG."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds R peaks on the ECG channel.", "response": "def ecg_find_peaks(signal, sampling_rate=1000):\n    \"\"\"\n    Find R peaks indices on the ECG channel.\n\n    Parameters\n    ----------\n    signal : list or ndarray\n        ECG signal (preferably filtered).\n    sampling_rate : int\n        Sampling rate (samples/second).\n\n\n    Returns\n    ----------\n    rpeaks : list\n        List of R-peaks location indices.\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> Rpeaks = nk.ecg_find_peaks(signal)\n\n    Notes\n    ----------\n    *Authors*\n\n    - the bioSSPy dev team (https://github.com/PIA-Group/BioSPPy)\n\n    *Dependencies*\n\n    - biosppy\n\n    *See Also*\n\n    - BioSPPY: https://github.com/PIA-Group/BioSPPy\n\n    \"\"\"\n    rpeaks, = biosppy.ecg.hamilton_segmenter(np.array(signal), sampling_rate=sampling_rate)\n    rpeaks, = biosppy.ecg.correct_rpeaks(signal=np.array(signal), rpeaks=rpeaks, sampling_rate=sampling_rate, tol=0.05)\n    return(rpeaks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ecg_wave_detector(ecg, rpeaks):\n    q_waves = []\n    p_waves = []\n    q_waves_starts = []\n    s_waves = []\n    t_waves = []\n    t_waves_starts = []\n    t_waves_ends = []\n    for index, rpeak in enumerate(rpeaks[:-3]):\n\n        try:\n            epoch_before = np.array(ecg)[int(rpeaks[index-1]):int(rpeak)]\n            epoch_before = epoch_before[int(len(epoch_before)/2):len(epoch_before)]\n            epoch_before = list(reversed(epoch_before))\n\n            q_wave_index = np.min(find_peaks(epoch_before))\n            q_wave = rpeak - q_wave_index\n            p_wave_index = q_wave_index + np.argmax(epoch_before[q_wave_index:])\n            p_wave = rpeak - p_wave_index\n\n            inter_pq = epoch_before[q_wave_index:p_wave_index]\n            inter_pq_derivative = np.gradient(inter_pq, 2)\n            q_start_index = find_closest_in_list(len(inter_pq_derivative)/2, find_peaks(inter_pq_derivative))\n            q_start = q_wave - q_start_index\n\n            q_waves.append(q_wave)\n            p_waves.append(p_wave)\n            q_waves_starts.append(q_start)\n        except ValueError:\n            pass\n        except IndexError:\n            pass\n\n        try:\n            epoch_after = np.array(ecg)[int(rpeak):int(rpeaks[index+1])]\n            epoch_after = epoch_after[0:int(len(epoch_after)/2)]\n\n            s_wave_index = np.min(find_peaks(epoch_after))\n            s_wave = rpeak + s_wave_index\n            t_wave_index = s_wave_index + np.argmax(epoch_after[s_wave_index:])\n            t_wave = rpeak + t_wave_index\n\n            inter_st = epoch_after[s_wave_index:t_wave_index]\n            inter_st_derivative = np.gradient(inter_st, 2)\n            t_start_index = find_closest_in_list(len(inter_st_derivative)/2, find_peaks(inter_st_derivative))\n            t_start = s_wave + t_start_index\n            t_end = np.min(find_peaks(epoch_after[t_wave_index:]))\n            t_end = t_wave + t_end\n\n            s_waves.append(s_wave)\n            t_waves.append(t_wave)\n            t_waves_starts.append(t_start)\n            t_waves_ends.append(t_end)\n        except ValueError:\n            pass\n        except IndexError:\n            pass\n\n# pd.Series(epoch_before).plot()\n#    t_waves = []\n#    for index, rpeak in enumerate(rpeaks[0:-1]):\n#\n#        epoch = np.array(ecg)[int(rpeak):int(rpeaks[index+1])]\n#        pd.Series(epoch).plot()\n#\n#        # T wave\n#        middle = (rpeaks[index+1] - rpeak) / 2\n#        quarter = middle/2\n#\n#        epoch = np.array(ecg)[int(rpeak+quarter):int(rpeak+middle)]\n#\n#        try:\n#            t_wave = int(rpeak+quarter) + np.argmax(epoch)\n#            t_waves.append(t_wave)\n#        except ValueError:\n#            pass\n#\n#    p_waves = []\n#    for index, rpeak in enumerate(rpeaks[1:]):\n#        index += 1\n#        # Q wave\n#        middle = (rpeak - rpeaks[index-1]) / 2\n#        quarter = middle/2\n#\n#        epoch = np.array(ecg)[int(rpeak-middle):int(rpeak-quarter)]\n#\n#        try:\n#            p_wave = int(rpeak-quarter) + np.argmax(epoch)\n#            p_waves.append(p_wave)\n#        except ValueError:\n#            pass\n#\n#    q_waves = []\n#    for index, p_wave in enumerate(p_waves):\n#        epoch = np.array(ecg)[int(p_wave):int(rpeaks[rpeaks>p_wave][0])]\n#\n#        try:\n#            q_wave = p_wave + np.argmin(epoch)\n#            q_waves.append(q_wave)\n#        except ValueError:\n#            pass\n#\n#    # TODO: manage to find the begininng of the Q and the end of the T wave so we can extract the QT interval\n\n\n    ecg_waves = {\"T_Waves\": t_waves,\n                 \"P_Waves\": p_waves,\n                 \"Q_Waves\": q_waves,\n                 \"S_Waves\": s_waves,\n                 \"Q_Waves_Onsets\": q_waves_starts,\n                 \"T_Waves_Onsets\": t_waves_starts,\n                 \"T_Waves_Ends\": t_waves_ends}\n    return(ecg_waves)", "response": "Returns the localization of the P Q T waves in the given ECG signal."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the localization of systoles and diastoles for a given ECG signal.", "response": "def ecg_systole(ecg, rpeaks, t_waves_ends):\n    \"\"\"\n    Returns the localization of systoles and diastoles.\n\n    Parameters\n    ----------\n    ecg : list or ndarray\n        ECG signal (preferably filtered).\n    rpeaks : list or ndarray\n        R peaks localization.\n    t_waves_ends : list or ndarray\n        T waves localization.\n\n    Returns\n    ----------\n    systole : ndarray\n        Array indicating where systole (1) and diastole (0).\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> systole = nk.ecg_systole(ecg, rpeaks, t_waves_ends)\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n\n    *Details*\n\n    - **Systole/Diastole**: One prominent channel of body and brain communication is that conveyed by baroreceptors, pressure and stretch-sensitive receptors within the heart and surrounding arteries. Within each cardiac cycle, bursts of baroreceptor afferent activity encoding the strength and timing of each heartbeat are carried via the vagus and glossopharyngeal nerve afferents to the nucleus of the solitary tract. This is the principal route that communicates to the brain the dynamic state of the heart, enabling the representation of cardiovascular arousal within viscerosensory brain regions, and influence ascending neuromodulator systems implicated in emotional and motivational behaviour. Because arterial baroreceptors are activated by the arterial pulse pressure wave, their phasic discharge is maximal during and immediately after the cardiac systole, that is, when the blood is ejected from the heart, and minimal during cardiac diastole, that is, between heartbeats (Azevedo, 2017).\n\n    References\n    -----------\n    - Azevedo, R. T., Garfinkel, S. N., Critchley, H. D., & Tsakiris, M. (2017). Cardiac afferent activity modulates the expression of racial stereotypes. Nature communications, 8.\n    - Edwards, L., Ring, C., McIntyre, D., & Carroll, D. (2001). Modulation of the human nociceptive flexion reflex across the cardiac cycle. Psychophysiology, 38(4), 712-718.\n    - Gray, M. A., Rylander, K., Harrison, N. A., Wallin, B. G., & Critchley, H. D. (2009). Following one's heart: cardiac rhythms gate central initiation of sympathetic reflexes. Journal of Neuroscience, 29(6), 1817-1825.\n    \"\"\"\n    waves = np.array([\"\"]*len(ecg))\n    waves[rpeaks] = \"R\"\n    waves[t_waves_ends] = \"T\"\n\n    systole = [0]\n    current = 0\n    for index, value in enumerate(waves[1:]):\n        if waves[index-1] == \"R\":\n            current = 1\n        if waves[index-1] == \"T\":\n            current = 0\n        systole.append(current)\n\n    return(systole)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef segmenter_pekkanen(ecg, sampling_rate, window_size=5.0, lfreq=5.0, hfreq=15.0):\n\n    window_size = int(window_size*sampling_rate)\n\n    lowpass = scipy.signal.butter(1, hfreq/(sampling_rate/2.0), 'low')\n    highpass = scipy.signal.butter(1, lfreq/(sampling_rate/2.0), 'high')\n\n    # TODO: Could use an actual bandpass filter\n    ecg_low = scipy.signal.filtfilt(*lowpass, x=ecg)\n    ecg_band = scipy.signal.filtfilt(*highpass, x=ecg_low)\n\n    # Square (=signal power) of the first difference of the signal\n    decg = np.diff(ecg_band)\n    decg_power = decg**2\n\n    # Robust threshold and normalizator estimation\n    thresholds = []\n    max_powers = []\n    for i in range(int(len(decg_power)/window_size)):\n        sample = slice(i*window_size, (i+1)*window_size)\n        d = decg_power[sample]\n        thresholds.append(0.5*np.std(d))\n        max_powers.append(np.max(d))\n\n    threshold = 0.5*np.std(decg_power)\n    threshold = np.median(thresholds)\n    max_power = np.median(max_powers)\n    decg_power[decg_power < threshold] = 0\n\n    decg_power = decg_power/max_power\n    decg_power[decg_power > 1.0] = 1.0\n    square_decg_power = decg_power**2\n\n#    shannon_energy = -square_decg_power*np.log(square_decg_power)  # This errors\n#    shannon_energy[np.where(np.isfinite(shannon_energy) == False)] = 0.0\n    shannon_energy = -square_decg_power*np.log(square_decg_power.clip(min=1e-6))\n    shannon_energy[np.where(shannon_energy <= 0)] = 0.0\n\n\n    mean_window_len = int(sampling_rate*0.125+1)\n    lp_energy = np.convolve(shannon_energy, [1.0/mean_window_len]*mean_window_len, mode='same')\n    #lp_energy = scipy.signal.filtfilt(*lowpass2, x=shannon_energy)\n\n    lp_energy = scipy.ndimage.gaussian_filter1d(lp_energy, sampling_rate/8.0)\n    lp_energy_diff = np.diff(lp_energy)\n\n    rpeaks = (lp_energy_diff[:-1] > 0) & (lp_energy_diff[1:] < 0)\n    rpeaks = np.flatnonzero(rpeaks)\n    rpeaks -= 1\n\n    return(rpeaks)", "response": "Segmenter for the Kathirvel et al. 2001."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eeg_erp(eeg, times=None, index=None, include=\"all\", exclude=None, hemisphere=\"both\", central=True, verbose=True, names=\"ERP\", method=\"mean\"):\n    erp = {}\n\n    data = eeg_to_df(eeg, index=index, include=include, exclude=exclude, hemisphere=hemisphere, central=central)\n\n    for epoch_index, epoch in data.items():\n        # Segment according to window\n        if isinstance(times, list):\n            if isinstance(times[0], list):\n                values = {}\n                for window_index, window in enumerate(times):\n                    df = epoch[window[0]:window[1]]\n                    value = df.mean().mean()\n                    values[window_index] = value\n                erp[epoch_index] = values\n            else:\n                df = epoch[times[0]:times[1]]\n                value = df.mean().mean()\n                erp[epoch_index] = [value]\n        elif isinstance(times, tuple):\n            values = {}\n            for window_index, window in enumerate(times):\n                df = epoch[window[0]:window[1]]\n                if method == \"max\":\n                    value = df.mean().max()\n                elif method == \"min\":\n                    value = df.mean().min()\n                else:\n                    value = df.mean().mean()\n                values[window_index] = value\n            erp[epoch_index] = values\n        else:\n            df = epoch[0:]\n            value = df.mean().mean()\n            erp[epoch_index] = [value]\n\n    # Convert to dataframe\n    erp = pd.DataFrame.from_dict(erp, orient=\"index\")\n    if isinstance(names, str):\n        names = [names]\n    if len(names) == len(erp.columns):\n        erp.columns = names\n\n    return(erp)", "response": "Returns a dataframe of the EEG data for a single EEG."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_eeg_erp(all_epochs, conditions=None, times=None, include=\"all\", exclude=None, hemisphere=\"both\", central=True, name=None, colors=None, gfp=False, ci=0.95, ci_alpha=0.333, invert_y=False, linewidth=1, linestyle=\"-\", filter_hfreq=None):\n    # Preserve original\n    all_epochs_current = all_epochs.copy()\n\n    # Filter using Savitzky-Golay polynomial method\n    if (filter_hfreq is not None) and (isinstance(filter_hfreq, int)):\n        for participant, epochs in all_epochs_current.items():\n            all_epochs_current[participant] = epochs.savgol_filter(filter_hfreq, copy=True)\n\n    # Crop\n    if isinstance(times, list) and len(times) == 2:\n        for participant, epochs in all_epochs_current.items():\n            all_epochs_current[participant] = epochs.copy().crop(times[0], times[1])\n\n\n    # Transform to evokeds\n    all_evokeds = eeg_to_all_evokeds(all_epochs_current, conditions=conditions)\n\n    data = {}\n    for participant, epochs in all_evokeds.items():\n        for condition, epoch in epochs.items():\n            data[condition] = []\n    for participant, epochs in all_evokeds.items():\n        for condition, epoch in epochs.items():\n            data[condition].append(epoch)\n\n\n    conditions = list(data.keys())\n\n    # Line styles\n    if isinstance(linestyle, str):\n        linestyle = [linestyle] * len(conditions)\n    elif isinstance(linestyle, list) and len(linestyle) >= len(conditions):\n        pass\n    elif isinstance(linestyle, dict) and len(linestyle.keys()) >= len(conditions):\n        linestyle = [linestyle[cond] for cond in conditions]\n    else:\n        print(\"NeuroKit Warning: plot_eeg_erp(): linestyle must be either a str, a list or a dict.\")\n\n\n    # Colors\n    if isinstance(colors, str):\n        colors = {condition: colors for condition in conditions}\n    elif isinstance(colors, list) and len(colors) >= len(conditions):\n        colors=  {condition: colors[index] for index, condition in enumerate(conditions)}\n    elif isinstance(colors, dict) and len(colors.keys()) >= len(conditions):\n        pass\n    elif colors is None:\n        pass\n    else:\n        print(\"NeuroKit Warning: plot_eeg_erp(): colors must be either a str, a list, a dict or None.\")\n\n\n    # Modify styles\n    styles = {}\n    for index, condition in enumerate(conditions):\n        styles[condition] = {\"linewidth\": linewidth, \"linestyle\": linestyle[index]}\n\n\n    # Select electrodes\n    picks = mne.pick_types(epoch.info, eeg=True, selection=eeg_select_electrodes(epoch, include=include, exclude=exclude, hemisphere=hemisphere, central=central))\n\n    # Plot\n    try:\n        plot = mne.viz.plot_compare_evokeds(data, picks=picks, colors=colors, styles=styles, title=name, gfp=gfp, ci=ci, invert_y=invert_y, ci_alpha=ci_alpha)\n    except TypeError:\n        print(\"NeuroKit Warning: plot_eeg_erp(): You're using a version of mne that does not support ci_alpha or ci_method parameters. Leaving defaults.\")\n        plot = mne.viz.plot_compare_evokeds(data, picks=picks, colors=colors, styles=styles, title=name, gfp=gfp, ci=ci, invert_y=invert_y)\n\n    return(plot)", "response": "Plot Eeg and Energy - Energy curves."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the evoked and grand average of all evoked butterfly.", "response": "def plot_eeg_erp_topo(all_epochs, colors=None):\n    \"\"\"\n    Plot butterfly plot.\n\n    DOCS INCOMPLETE :(\n    \"\"\"\n    all_evokeds = eeg_to_all_evokeds(all_epochs)\n\n    data = {}\n    for participant, epochs in all_evokeds.items():\n        for cond, epoch in epochs.items():\n            data[cond] = []\n    for participant, epochs in all_evokeds.items():\n        for cond, epoch in epochs.items():\n            data[cond].append(epoch)\n\n    if colors is not None:\n        color_list = []\n    else:\n        color_list = None\n\n    evokeds = []\n    for condition, evoked in data.items():\n        grand_average = mne.grand_average(evoked)\n        grand_average.comment = condition\n        evokeds += [grand_average]\n        if colors is not None:\n            color_list.append(colors[condition])\n\n    plot = mne.viz.plot_evoked_topo(evokeds, background_color=\"w\", color=color_list)\n    return(plot)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_nk_object(obj, filename=\"file\", path=\"\", extension=\"nk\", compress=False, compatibility=-1):\n    if compress is True:\n        with gzip.open(path + filename + \".\" + extension, 'wb') as name:\n            pickle.dump(obj, name, protocol=compatibility)\n    else:\n        with open(path + filename + \".\" + extension, 'wb') as name:\n            pickle.dump(obj, name, protocol=compatibility)", "response": "Save a python object to a pickled file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_nk_object(filename, path=\"\"):\n    filename = path + filename\n    \n    try:\n        with open(filename, 'rb') as name:\n            file = pickle.load(name)\n    except pickle.UnpicklingError:\n        with gzip.open(filename, 'rb') as name:\n            file = pickle.load(name)\n    except ModuleNotFoundError:  # In case you're trying to unpickle a dataframe made with pandas < 0.17\n        try:\n            file = pd.read_pickle(filename)\n        except:\n            pass\n    return(file)", "response": "Reads a pickled file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to get the date that a file was created falling back to when it was last modified if that s not possible.", "response": "def find_creation_date(path):\n    \"\"\"\n    Try to get the date that a file was created, falling back to when it was last modified if that's not possible.\n\n    Parameters\n    ----------\n    path : str\n       File's path.\n\n    Returns\n    ----------\n    creation_date : str\n        Time of file creation.\n\n\n    Example\n    ----------\n    >>> import neurokit as nk\n    >>> import datetime\n    >>>\n    >>> creation_date = nk.find_creation_date(file)\n    >>> creation_date = datetime.datetime.fromtimestamp(creation_date)\n\n    Notes\n    ----------\n    *Authors*\n\n    - `Dominique Makowski <https://dominiquemakowski.github.io/>`_\n    - Mark Amery\n\n    *Dependencies*\n\n    - platform\n    - os\n\n    *See Also*\n\n    - http://stackoverflow.com/a/39501288/1709587\n\n    \"\"\"\n    if platform.system() == 'Windows':\n        return(os.path.getctime(path))\n    else:\n        stat = os.stat(path)\n        try:\n            return(stat.st_birthtime)\n        except AttributeError:\n            print(\"Neuropsydia error: get_creation_date(): We're probably on Linux. No easy way to get creation dates here, so we'll settle for when its content was last modified.\")\n            return(stat.st_mtime)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_debug_info():\n        from . import __version__\n        d = OrderedDict()\n        d['Version'] = '%s' % __version__\n\n        for key, val in PyVisaLibrary.get_session_classes().items():\n            key_name = '%s %s' % (key[0].name.upper(), key[1])\n            try:\n                d[key_name] = getattr(val, 'session_issue').split('\\n')\n            except AttributeError:\n                d[key_name] = 'Available ' + val.get_low_level_info()\n\n        return d", "response": "Return a list of lines with backend info."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a random but unique session handle for a session object and returns the value of the session object.", "response": "def _register(self, obj):\n        \"\"\"Creates a random but unique session handle for a session object,\n        register it in the sessions dictionary and return the value\n\n        :param obj: a session object.\n        :return: session handle\n        :rtype: int\n        \"\"\"\n        session = None\n\n        while session is None or session in self.sessions:\n            session = random.randint(1000000, 9999999)\n\n        self.sessions[session] = obj\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking return values for errors and warnings.", "response": "def _return_handler(self, ret_value, func, arguments):\n        \"\"\"Check return values for errors and warnings.\n\n        TODO: THIS IS JUST COPIED PASTED FROM NIVisaLibrary.\n        Needs to be adapted.\n        \"\"\"\n\n        logger.debug('%s%s -> %r',\n                     func.__name__, _args_to_str(arguments), ret_value,\n                     extra=self._logging_extra)\n\n        try:\n            ret_value = StatusCode(ret_value)\n        except ValueError:\n            pass\n\n        self._last_status = ret_value\n\n        # The first argument of almost all registered visa functions is a session.\n        # We store the error code per session\n        session = None\n        if func.__name__ not in ('viFindNext', ):\n            try:\n                session = arguments[0]\n            except KeyError:\n                raise Exception('Function %r does not seem to be a valid '\n                                'visa function (len args %d)' % (func, len(arguments)))\n\n            # Functions that use the first parameter to get a session value.\n            if func.__name__ in ('viOpenDefaultRM', ):\n                # noinspection PyProtectedMember\n                session = session._obj.value\n\n            if isinstance(session, integer_types):\n                self._last_status_in_session[session] = ret_value\n            else:\n                # Functions that might or might have a session in the first argument.\n                if func.__name__ not in ('viClose', 'viGetAttribute', 'viSetAttribute', 'viStatusDesc'):\n                    raise Exception('Function %r does not seem to be a valid '\n                                    'visa function (type args[0] %r)' % (func, type(session)))\n\n        if ret_value < 0:\n            raise errors.VisaIOError(ret_value)\n\n        if ret_value in self.issue_warning_on:\n            if session and ret_value not in self._ignore_warning_in_session[session]:\n                warnings.warn(errors.VisaIOWarning(ret_value), stacklevel=2)\n\n        return ret_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclear a device. Corresponds to viClear function of the VISA library. :param session: Unique logical identifier to a session. :return: return value of the library call. :rtype: :class:`pyvisa.constants.StatusCode`", "response": "def clear(self, session):\n        \"\"\"Clears a device.\n\n        Corresponds to viClear function of the VISA library.\n\n        :param session: Unique logical identifier to a session.\n        :return: return value of the library call.\n        :rtype: :class:`pyvisa.constants.StatusCode`\n        \"\"\"\n        try:\n            sess = self.sessions[session]\n        except KeyError:\n            return constants.StatusCode.error_invalid_object\n        return sess.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gpib_command(self, session, command_byte):\n        try:\n            return self.sessions[session].gpib_command(command_byte)\n\n        except KeyError:\n            return constants.StatusCode.error_invalid_object", "response": "Write GPIB command byte on the VISA library."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assert_trigger(self, session, protocol):\n        try:\n            return self.sessions[session].assert_trigger(protocol)\n\n        except KeyError:\n            return constants.StatusCode.error_invalid_object", "response": "Asserts software or hardware trigger."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self, session):\n        try:\n            sess = self.sessions[session]\n            if sess is not self:\n                sess.close()\n        except KeyError:\n            return StatusCode.error_invalid_object", "response": "Closes the specified session event or find list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a tuple of all connected devices matching query.", "response": "def list_resources(self, session, query='?*::INSTR'):\n        \"\"\"Returns a tuple of all connected devices matching query.\n\n        :param query: regular expression used to match devices.\n        \"\"\"\n\n        # For each session type, ask for the list of connected resources and\n        # merge them into a single list.\n\n        resources = sum([st.list_resources()\n                         for key, st in sessions.Session.iter_valid_session_classes()], [])\n\n        resources = rname.filter(resources, query)\n\n        return resources"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, session, count):\n\n        # from the session handle, dispatch to the read method of the session object.\n        try:\n            ret = self.sessions[session].read(count)\n        except KeyError:\n            return 0, StatusCode.error_invalid_object\n\n        if ret[1] < 0:\n            raise errors.VisaIOError(ret[1])\n\n        return ret", "response": "Reads data from device or interface synchronously."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites data to the device or interface synchronously.", "response": "def write(self, session, data):\n        \"\"\"Writes data to device or interface synchronously.\n\n        Corresponds to viWrite function of the VISA library.\n\n        :param session: Unique logical identifier to a session.\n        :param data: data to be written.\n        :type data: str\n        :return: Number of bytes actually transferred, return value of the library call.\n        :rtype: int, VISAStatus\n        \"\"\"\n\n        # from the session handle, dispatch to the write method of the session object.\n        try:\n            ret = self.sessions[session].write(data)\n        except KeyError:\n            return 0, StatusCode.error_invalid_object\n\n        if ret[1] < 0:\n            raise errors.VisaIOError(ret[1])\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_attribute(self, session, attribute):\n        try:\n            sess = self.sessions[session]\n        except KeyError:\n            return None, StatusCode.error_invalid_object\n\n        return sess.get_attribute(attribute)", "response": "Retrieves the state of an attribute for a specified resource in a specified session."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the state of an attribute for a session.", "response": "def set_attribute(self, session, attribute, attribute_state):\n        \"\"\"Sets the state of an attribute.\n\n        Corresponds to viSetAttribute function of the VISA library.\n\n        :param session: Unique logical identifier to a session.\n        :param attribute: Attribute for which the state is to be modified. (Attributes.*)\n        :param attribute_state: The state of the attribute to be set for the specified object.\n        :return: return value of the library call.\n        :rtype: VISAStatus\n        \"\"\"\n\n        try:\n            sess = self.sessions[session]\n        except KeyError:\n            return StatusCode.error_invalid_object\n\n        return sess.set_attribute(attribute, attribute_state)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nestablish an access mode to the specified resources.", "response": "def lock(self, session, lock_type, timeout, requested_key=None):\n        \"\"\"Establishes an access mode to the specified resources.\n\n        Corresponds to viLock function of the VISA library.\n\n        :param session: Unique logical identifier to a session.\n        :param lock_type: Specifies the type of lock requested, either Constants.EXCLUSIVE_LOCK or Constants.SHARED_LOCK.\n        :param timeout: Absolute time period (in milliseconds) that a resource waits to get unlocked by the\n                        locking session before returning an error.\n        :param requested_key: This parameter is not used and should be set to VI_NULL when lockType is VI_EXCLUSIVE_LOCK.\n        :return: access_key that can then be passed to other sessions to share the lock, return value of the library call.\n        :rtype: str, :class:`pyvisa.constants.StatusCode`\n        \"\"\"\n        try:\n            sess = self.sessions[session]\n        except KeyError:\n            return StatusCode.error_invalid_object\n\n        return sess.lock(lock_type, timeout, requested_key)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unlock(self, session):\n        try:\n            sess = self.sessions[session]\n        except KeyError:\n            return StatusCode.error_invalid_object\n\n        return sess.unlock()", "response": "Relinquishes a lock for the specified resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_raw_devices(vendor=None, product=None, serial_number=None,\n                     custom_match=None, **kwargs):\n    \"\"\"Find connected USB RAW devices. See usbutil.find_devices for more info.\n    \"\"\"\n    def is_usbraw(dev):\n        if custom_match and not custom_match(dev):\n            return False\n        return bool(find_interfaces(dev, bInterfaceClass=0xFF,\n                                    bInterfaceSubClass=0xFF))\n\n    return find_devices(vendor, product, serial_number, is_usbraw, **kwargs)", "response": "Find connected USB RAW devices."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, data):\n\n        begin, end, size = 0, 0, len(data)\n        bytes_sent = 0\n\n        raw_write = super(USBRawDevice, self).write\n\n        while not end > size:\n            begin = end\n            end = begin + self.RECV_CHUNK\n            bytes_sent += raw_write(data[begin:end])\n\n        return bytes_sent", "response": "Send raw bytes to the instrument."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, size):\n\n        raw_read = super(USBRawDevice, self).read\n\n        received = bytearray()\n\n        while not len(received) >= size:\n            resp = raw_read(self.RECV_CHUNK)\n\n            received.extend(resp)\n\n        return bytes(received)", "response": "Read raw bytes from the instrument."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_devices(vendor=None, product=None, serial_number=None,\n                 custom_match=None, **kwargs):\n    \"\"\"Find connected USB devices matching certain keywords.\n\n    Wildcards can be used for vendor, product and serial_number.\n\n    :param vendor: name or id of the vendor (manufacturer)\n    :param product: name or id of the product\n    :param serial_number: serial number.\n    :param custom_match: callable returning True or False that takes a device as only input.\n    :param kwargs: other properties to match. See usb.core.find\n    :return:\n    \"\"\"\n    kwargs = kwargs or {}\n    attrs = {}\n    if isinstance(vendor, str):\n        attrs['manufacturer'] = vendor\n    elif vendor is not None:\n        kwargs['idVendor'] = vendor\n\n    if isinstance(product, str):\n        attrs['product'] = product\n    elif product is not None:\n        kwargs['idProduct'] = product\n\n    if serial_number:\n        attrs['serial_number'] = str(serial_number)\n\n    if attrs:\n        def cm(dev):\n            if custom_match is not None and not custom_match(dev):\n                return False\n            for attr, pattern in attrs.items():\n                if not fnmatch(getattr(dev, attr).lower(), pattern.lower()):\n                    return False\n            return True\n    else:\n        cm = custom_match\n\n    return usb.core.find(find_all=True, custom_match=cm, **kwargs)", "response": "Find connected USB devices matching certain keywords."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_interfaces(device, **kwargs):\n    interfaces = []\n    try:\n        for cfg in device:\n            try:\n                interfaces.extend(usb_find_desc(cfg, find_all=True, **kwargs))\n            except:\n                pass\n    except:\n        pass\n    return interfaces", "response": "find_interfaces - Finds all the interfaces on the device"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, count):\n\n        def _usb_reader():\n            \"\"\"Data reader identifying usb timeout exception.\"\"\"\n            try:\n                return self.interface.read(count)\n            except usb.USBError as exc:\n                if exc.errno in (errno.ETIMEDOUT, -errno.ETIMEDOUT):\n                    raise USBTimeoutException()\n                raise\n\n        supress_end_en, _ = self.get_attribute(constants.VI_ATTR_SUPPRESS_END_EN)\n\n        if supress_end_en:\n            raise ValueError('VI_ATTR_SUPPRESS_END_EN == True is currently unsupported by pyvisa-py')\n\n        term_char, _ = self.get_attribute(constants.VI_ATTR_TERMCHAR)\n        term_char_en, _ = self.get_attribute(constants.VI_ATTR_TERMCHAR_EN)\n\n        return self._read(_usb_reader,\n                          count,\n                          lambda current: True, # USB always returns a complete message\n                          supress_end_en,\n                          term_char,\n                          term_char_en,\n                          USBTimeoutException)", "response": "Reads data from the device or interface synchronously."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite data to the device or interface synchronously.", "response": "def write(self, data):\n        \"\"\"Writes data to device or interface synchronously.\n\n        Corresponds to viWrite function of the VISA library.\n\n        :param data: data to be written.\n        :type data: bytes\n        :return: Number of bytes actually transferred, return value of the library call.\n        :rtype: (int, VISAStatus)\n        \"\"\"\n\n        send_end, _ = self.get_attribute(constants.VI_ATTR_SEND_END_EN)\n\n        count = self.interface.write(data)\n\n        return count, StatusCode.success"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef needs_to_be_resolved(parent_obj, attr_name):\n    if hasattr(get_model(parent_obj), \"_tx_reference_resolver\"):\n        return get_model(parent_obj)._tx_reference_resolver. \\\n            has_unresolved_crossrefs(parent_obj, attr_name)\n    else:\n        return False", "response": "This function determines if the attribute needs to be resolved or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_list_of_concatenated_objects(obj, dot_separated_name,\n                                     lst=None):\n    \"\"\"\n    get a list of the objects consisting of\n    - obj\n    - obj+\".\"+dot_separated_name\n    - (obj+\".\"+dot_separated_name)+\".\"+dot_separated_name (called recursively)\n    Note: lists are expanded\n\n    Args:\n        obj: the starting point\n        dot_separated_name: \"the search path\" (applied recursively)\n        lst: the initial list (e.g. [])\n\n    Returns:\n        the filled list (if one single object is requested, a list with one\n        entry is returned).\n    \"\"\"\n    from textx.scoping import Postponed\n    if lst is None:\n        lst = []\n    if not obj:\n        return lst\n    if obj in lst:\n        return lst\n    lst.append(obj)\n    if type(obj) is Postponed:\n        return lst\n    ret = get_referenced_object(None, obj, dot_separated_name)\n    if type(ret) is list:\n        for r in ret:\n            lst = get_list_of_concatenated_objects(r, dot_separated_name, lst)\n    else:\n        lst = get_list_of_concatenated_objects(ret, dot_separated_name, lst)\n    return lst", "response": "get a list of the objects consisting of obj. dot_separated_name. obj. dot_separated_name. obj. dot_separated_name returns obj. dot_separated_name. obj. dot_separated_name returns obj. lst. obj. lst. obj. lst. obj. lst. lst."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_location(model_obj):\n    the_model = get_model(model_obj)\n    line, col = the_model._tx_parser.pos_to_linecol(\n        model_obj._tx_position)\n    return {\"line\": line, \"col\": col, \"filename\": the_model._tx_filename}", "response": "Returns the line col and filename of the model element."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_referenced_object(prev_obj, obj, dot_separated_name,\n                          desired_type=None):\n    \"\"\"\n    get objects based on a path\n\n    Args:\n        prev_obj: the object containing obj (req. if obj is a list)\n        obj: the current object\n        dot_separated_name: the attribute name \"a.b.c.d\" starting from obj\n           Note: the attribute \"parent(TYPE)\" is a shortcut to jump to the\n           parent of type \"TYPE\" (exact match of type name).\n        desired_type: (optional)\n\n    Returns:\n        the object if found, None if not found or Postponed() if some postponed\n        refs are found on the path\n    \"\"\"\n    from textx.scoping import Postponed\n    assert prev_obj or not type(obj) is list\n    names = dot_separated_name.split(\".\")\n    match = re.match(r'parent\\((\\w+)\\)', names[0])\n    if match:\n        next_obj = obj\n        desired_parent_typename = match.group(1)\n        next_obj = get_recursive_parent_with_typename(next_obj,\n                                                      desired_parent_typename)\n        if next_obj:\n            return get_referenced_object(None, next_obj, \".\".join(names[1:]),\n                                         desired_type)\n        else:\n            return None\n    elif type(obj) is list:\n        next_obj = None\n        for res in obj:\n            if hasattr(res, \"name\") and res.name == names[0]:\n                if desired_type is None or textx_isinstance(res, desired_type):\n                    next_obj = res\n                else:\n                    raise TypeError(\n                        \"{} has type {} instead of {}.\".format(\n                            names[0], type(res).__name__,\n                            desired_type.__name__))\n        if not next_obj:\n            # if prev_obj needs to be resolved: return Postponed.\n            if needs_to_be_resolved(prev_obj, names[0]):\n                return Postponed()\n            else:\n                return None\n    elif type(obj) is Postponed:\n        return Postponed()\n    else:\n        next_obj = getattr(obj, names[0])\n    if not next_obj:\n        # if obj in in crossref return Postponed, else None\n        if needs_to_be_resolved(obj, names[0]):\n            return Postponed()\n        else:\n            return None\n    if len(names) > 1:\n        return get_referenced_object(obj, next_obj, \".\".join(\n            names[1:]), desired_type)\n    if type(next_obj) is list and needs_to_be_resolved(obj, names[0]):\n        return Postponed()\n    return next_obj", "response": "Get objects based on a path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_referenced_object_as_list(\n        prev_obj, obj, dot_separated_name, desired_type=None):\n    \"\"\"\n    Same as get_referenced_object, but always returns a list.\n\n    Args:\n        prev_obj: see get_referenced_object\n        obj: see get_referenced_object\n        dot_separated_name: see get_referenced_object\n        desired_type: see get_referenced_object\n\n    Returns:\n        same as get_referenced_object, but always returns a list\n    \"\"\"\n    res = get_referenced_object(prev_obj, obj, dot_separated_name,\n                                desired_type)\n    if res is None:\n        return []\n    elif type(res) is list:\n        return res\n    else:\n        return [res]", "response": "Same as get_referenced_object but always returns a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a unique named object in all known models.", "response": "def get_unique_named_object_in_all_models(root, name):\n    \"\"\"\n    retrieves a unique named object (no fully qualified name)\n\n    Args:\n        root: start of search (if root is a model all known models are searched\n              as well)\n        name: name of object\n\n    Returns:\n        the object (if not unique, raises an error)\n    \"\"\"\n    if hasattr(root, '_tx_model_repository'):\n        src = list(\n            root._tx_model_repository.local_models.filename_to_model.values())\n        if root not in src:\n            src.append(root)\n    else:\n        src = [root]\n\n    a = []\n    for m in src:\n        print(\"analyzing {}\".format(m._tx_filename))\n        a = a + get_children(\n            lambda x: hasattr(x, 'name') and x.name == name, m)\n\n    assert len(a) == 1\n    return a[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a unique named object in the tree root.", "response": "def get_unique_named_object(root, name):\n    \"\"\"\n    retrieves a unique named object (no fully qualified name)\n\n    Args:\n        root: start of search\n        name: name of object\n\n    Returns:\n        the object (if not unique, raises an error)\n    \"\"\"\n    a = get_children(lambda x: hasattr(x, 'name') and x.name == name, root)\n    assert len(a) == 1\n    return a[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_unique_named_object_has_class(root, name, class_name):\n    assert class_name == get_unique_named_object(root, name).__class__.__name__", "response": "Checks if a unique named object has the given class name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_models_including_attached_models(model):\n    if (hasattr(model, \"_tx_model_repository\")):\n        models = list(\n            model._tx_model_repository.all_models.filename_to_model.values())\n        if model not in models:\n            models.append(model)\n    else:\n        models = [model]\n    return models", "response": "get a list of all models stored within a model"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_models_using_filepattern(\n            self, filename_pattern, model, glob_args, is_main_model=False,\n            encoding='utf-8', add_to_local_models=True):\n        \"\"\"\n        add a new model to all relevant objects\n\n        Args:\n            filename_pattern: models to be loaded\n            model: model holding the loaded models in its _tx_model_repository\n                   field (may be None).\n            glob_args: arguments passed to the glob.glob function.\n\n        Returns:\n            the list of loaded models\n        \"\"\"\n        if (model):\n            self.update_model_in_repo_based_on_filename(model)\n        filenames = glob.glob(filename_pattern, **glob_args)\n        if len(filenames) == 0:\n            raise IOError(\n                errno.ENOENT, os.strerror(errno.ENOENT), filename_pattern)\n        loaded_models = []\n        for filename in filenames:\n            the_metamodel = MetaModelProvider.get_metamodel(model, filename)\n            loaded_models.append(\n                self.load_model(the_metamodel, filename, is_main_model,\n                                encoding=encoding,\n                                add_to_local_models=add_to_local_models))\n        return loaded_models", "response": "Loads all models in the specified file pattern into the specified model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a model from a file using a list of search directories.", "response": "def load_model_using_search_path(\n            self, filename, model, search_path, is_main_model=False,\n            encoding='utf8', add_to_local_models=True):\n        \"\"\"\n        add a new model to all relevant objects\n\n        Args:\n            filename: models to be loaded\n            model: model holding the loaded models in its _tx_model_repository\n                   field (may be None).\n            search_path: list of search directories.\n\n        Returns:\n            the loaded model\n        \"\"\"\n        if (model):\n            self.update_model_in_repo_based_on_filename(model)\n        for the_path in search_path:\n            full_filename = join(the_path, filename)\n            # print(full_filename)\n            if exists(full_filename):\n                the_metamodel = \\\n                    MetaModelProvider.get_metamodel(model, full_filename)\n                return self.load_model(the_metamodel,\n                                       full_filename,\n                                       is_main_model,\n                                       encoding=encoding,\n                                       add_to_local_models=add_to_local_models)\n\n        raise IOError(\n            errno.ENOENT, os.strerror(errno.ENOENT), filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a single model from a file.", "response": "def load_model(\n            self, the_metamodel, filename, is_main_model, encoding='utf-8',\n            add_to_local_models=True):\n        \"\"\"\n        load a single model\n\n        Args:\n            the_metamodel: the metamodel used to load the model\n            filename: the model to be loaded (if not cached)\n\n        Returns:\n            the loaded/cached model\n        \"\"\"\n\n        if not self.local_models.has_model(filename):\n            if self.all_models.has_model(filename):\n                new_model = self.all_models.filename_to_model[filename]\n            else:\n                # print(\"LOADING {}\".format(filename))\n                # all models loaded here get their references resolved from the\n                # root model\n                new_model = the_metamodel.internal_model_from_file(\n                    filename, pre_ref_resolution_callback=lambda\n                    other_model: self.pre_ref_resolution_callback(other_model),\n                    is_main_model=is_main_model, encoding=encoding)\n                self.all_models.filename_to_model[filename] = new_model\n            # print(\"ADDING {}\".format(filename))\n            if add_to_local_models:\n                self.local_models.filename_to_model[filename] = new_model\n        assert self.all_models.has_model(filename)  # to be sure...\n        return self.all_models.filename_to_model[filename]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_model_in_repo_based_on_filename(self, model):\n        if model._tx_filename is None:\n            for fn in self.all_models.filename_to_model:\n                if self.all_models.filename_to_model[fn] == model:\n                    return fn\n            i = 0\n            while self.all_models.has_model(\"anonymous{}\".format(i)):\n                i += 1\n            myfilename = \"anonymous{}\".format(i)\n            self.all_models.filename_to_model[myfilename] = model\n        else:\n            myfilename = model._tx_filename\n            if (not self.all_models.has_model(myfilename)):\n                self.all_models.filename_to_model[myfilename] = model\n        return myfilename", "response": "Updates the filename_to_model dict with the model s filename."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check(ctx, meta_model_file, model_file, ignore_case):\n    debug = ctx.obj['debug']\n    check_model(meta_model_file, model_file, debug, ignore_case)", "response": "Check validity of meta - model and optionally model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visualize(ctx, meta_model_file, model_file, ignore_case, output_format):\n    debug = ctx.obj['debug']\n    meta_model, model = check_model(meta_model_file,\n                                    model_file, debug, ignore_case)\n    if output_format == 'plantuml':\n        pu_file = \"{}.pu\".format(meta_model_file)\n        click.echo(\"Generating '{}' file for meta-model.\".format(pu_file))\n        click.echo(\"To convert to png run 'plantuml {}'\".format(pu_file))\n        click.echo(\"To convert to svg run 'plantuml -tsvg {}'\".format(pu_file))\n        metamodel_export(meta_model, pu_file, PlantUmlRenderer())\n    else:\n        dot_file = \"{}.dot\".format(meta_model_file)\n        click.echo(\"Generating '{}' file for meta-model.\".format(dot_file))\n        click.echo(\"To convert to png run 'dot -Tpng -O {}'\".format(dot_file))\n        metamodel_export(meta_model, dot_file)\n\n    if model_file:\n        if output_format == 'plantuml':\n            raise Exception(\"plantuml is not supported for model files, yet.\")\n        dot_file = \"{}.dot\".format(model_file)\n        click.echo(\"Generating '{}' file for model.\".format(model_file))\n        click.echo(\n            \"To convert to png run 'dot -Tpng -O {}'\".format(model_file))\n        model_export(model, dot_file)", "response": "Generate. dot file for the current language."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_textx_subcommands():\n    # Register direct sub-commands\n    global textx\n    for subcommand in pkg_resources.iter_entry_points(group='textx_commands'):\n        textx.command()(subcommand.load())\n\n    # Register sub-command groups\n    for subgroup in pkg_resources.iter_entry_points(\n            group='textx_command_groups'):\n        subgroup.load()(textx)", "response": "Find and use all textx sub - commands registered through extension points."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds and returns a meta - model for Entity language.", "response": "def get_entity_mm():\n    \"\"\"\n    Builds and returns a meta-model for Entity language.\n    \"\"\"\n    type_builtins = {\n            'integer': SimpleType(None, 'integer'),\n            'string': SimpleType(None, 'string')\n    }\n    entity_mm = metamodel_from_file(join(this_folder, 'entity.tx'),\n                                    classes=[SimpleType],\n                                    builtins=type_builtins)\n\n    return entity_mm"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming given state machine model to dot str.", "response": "def sm_to_dot(model):\n    \"\"\"\n    Transforms given state machine model to dot str.\n    \"\"\"\n\n    dot_str = HEADER\n\n    # Render states\n    first = True\n    for state in model.states:\n        dot_str += '{}[label=\"{{{}{}|{}}}\"]\\n'.format(\n            id(state), r\"-\\> \" if first else \"\", state.name,\n            \"\\\\n\".join(action.name for action in state.actions))\n        first = False\n\n        # Render transitions\n        for transition in state.transitions:\n            dot_str += '{} -> {} [label=\"{}\"]\\n'\\\n                .format(id(state), id(transition.to_state),\n                        transition.event.name)\n\n    # If there are reset events declared render them.\n    if model.resetEvents:\n        dot_str += 'reset_events [label=\"{{Reset Events|{}}}\", style=\"\"]\\n'\\\n            .format(\"\\\\n\".join(event.name for event in model.resetEvents))\n\n    dot_str += '\\n}\\n'\n    return dot_str"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_language(language_name):\n\n    langs = list(pkg_resources.iter_entry_points(group=LANG_EP,\n                                                 name=language_name))\n\n    if not langs:\n        raise TextXError('Language \"{}\" is not registered.'\n                         .format(language_name))\n\n    if len(langs) > 1:\n        # Multiple languages defined with the same name\n        raise TextXError('Language \"{}\" registered multiple times:\\n{}'\n                         .format(language_name,\n                                 \"\\n\".join([l.dist for l in langs])))\n\n    return langs[0].load()()", "response": "Returns a callable that instantiates meta - model for the given language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the model root element for the given object.", "response": "def get_model(obj):\n    \"\"\"\n    Finds model root element for the given object.\n    \"\"\"\n    p = obj\n    while hasattr(p, 'parent'):\n        p = p.parent\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parent_of_type(typ, obj):\n    if type(typ) is not text:\n        typ = typ.__name__\n\n    while hasattr(obj, 'parent'):\n        obj = obj.parent\n        if obj.__class__.__name__ == typ:\n            return obj", "response": "Finds the first object up the parent chain of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_children(decider, root):\n    collected = []\n\n    def follow(elem):\n\n        if elem in collected:\n            return\n\n        # Use meta-model to search for all contained child elements.\n        cls = elem.__class__\n\n        if hasattr(cls, '_tx_attrs') and decider(elem):\n            collected.append(elem)\n\n        if hasattr(cls, '_tx_attrs'):\n            for attr_name, attr in cls._tx_attrs.items():\n                # Follow only attributes with containment semantics\n                if attr.cont:\n                    if attr.mult in (MULT_ONE, MULT_OPTIONAL):\n                        new_elem = getattr(elem, attr_name)\n                        if new_elem:\n                            follow(new_elem)\n                    else:\n                        new_elem_list = getattr(elem, attr_name)\n                        if new_elem_list:\n                            for new_elem in new_elem_list:\n                                follow(new_elem)\n\n    follow(root)\n    return collected", "response": "Returns a list of all model elements of type typ starting from model\n    element root."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all model elements of type typ starting from model element root.", "response": "def get_children_of_type(typ, root):\n    \"\"\"\n    Returns a list of all model elements of type 'typ' starting from model\n    element 'root'. The search process will follow containment links only.\n    Non-containing references shall not be followed.\n\n    Args:\n        typ(str or python class): The type of the model object we are\n            looking for.\n        root (model object): Python model object which is the start of the\n            search process.\n    \"\"\"\n\n    if type(typ) is not text:\n        typ = typ.__name__\n\n    return get_children(lambda x: x.__class__.__name__ == typ, root)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a parser for the given language.", "response": "def get_model_parser(top_rule, comments_model, **kwargs):\n    \"\"\"\n    Creates model parser for the given language.\n    \"\"\"\n\n    class TextXModelParser(Parser):\n        \"\"\"\n        Parser created from textual textX language description.\n        Semantic actions for this parser will construct object\n        graph representing model on the given language.\n        \"\"\"\n\n        def __init__(self, *args, **kwargs):\n            super(TextXModelParser, self).__init__(*args, **kwargs)\n\n            # By default first rule is starting rule\n            # and must be followed by the EOF\n            self.parser_model = Sequence(\n                nodes=[top_rule, EOF()], rule_name='Model', root=True)\n            self.comments_model = comments_model\n\n            # Stack for metaclass instances\n            self._inst_stack = []\n\n            # Dict for cross-ref resolving\n            # { id(class): { obj.name: obj}}\n            self._instances = {}\n\n            # List to keep track of all cross-ref that need to be resolved\n            # Contained elements are tuples: (instance, metaattr, cross-ref)\n            self._crossrefs = []\n\n        def clone(self):\n            \"\"\"\n            Responsibility: create a clone in order to parse a separate file.\n            It must be possible that more than one clone exist in parallel,\n            without being influenced by other parser clones.\n\n            Returns:\n                A clone of this parser\n            \"\"\"\n            import copy\n            the_clone = copy.copy(self)  # shallow copy\n\n            # create new objects for parse-dependent data\n            the_clone._inst_stack = []\n            the_clone._instances = {}\n            the_clone._crossrefs = []\n\n            # TODO self.memoization = memoization\n            the_clone.comments = []\n            the_clone.comment_positions = {}\n            the_clone.sem_actions = {}\n\n            return the_clone\n\n        def _parse(self):\n            try:\n                return self.parser_model.parse(self)\n            except NoMatch as e:\n                line, col = e.parser.pos_to_linecol(e.position)\n                raise TextXSyntaxError(message=text(e),\n                                       line=line,\n                                       col=col,\n                                       expected_rules=e.rules)\n\n        def get_model_from_file(self, file_name, encoding, debug,\n                                pre_ref_resolution_callback=None,\n                                is_main_model=True):\n            \"\"\"\n            Creates model from the parse tree from the previous parse call.\n            If file_name is given file will be parsed before model\n            construction.\n            \"\"\"\n            with codecs.open(file_name, 'r', encoding) as f:\n                model_str = f.read()\n\n            model = self.get_model_from_str(\n                model_str, file_name=file_name, debug=debug,\n                pre_ref_resolution_callback=pre_ref_resolution_callback,\n                is_main_model=is_main_model, encoding=encoding)\n\n            return model\n\n        def get_model_from_str(self, model_str, file_name=None, debug=None,\n                               pre_ref_resolution_callback=None,\n                               is_main_model=True, encoding='utf-8'):\n            \"\"\"\n            Parses given string and creates model object graph.\n            \"\"\"\n            old_debug_state = self.debug\n\n            try:\n                if debug is not None:\n                    self.debug = debug\n\n                if self.debug:\n                    self.dprint(\"*** PARSING MODEL ***\")\n\n                self.parse(model_str, file_name=file_name)\n                # Transform parse tree to model. Skip root node which\n                # represents the whole file ending in EOF.\n                model = parse_tree_to_objgraph(\n                    self, self.parse_tree[0], file_name=file_name,\n                    pre_ref_resolution_callback=pre_ref_resolution_callback,\n                    is_main_model=is_main_model, encoding=encoding)\n            finally:\n                if debug is not None:\n                    self.debug = old_debug_state\n\n            try:\n                model._tx_metamodel = self.metamodel\n            except AttributeError:\n                # model is some primitive python type (e.g. str)\n                pass\n            return model\n\n    return TextXModelParser(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming parse tree to object graph.", "response": "def parse_tree_to_objgraph(parser, parse_tree, file_name=None,\n                           pre_ref_resolution_callback=None,\n                           is_main_model=True, encoding='utf-8'):\n    \"\"\"\n    Transforms parse_tree to object graph representing model in a\n    new language.\n    \"\"\"\n\n    metamodel = parser.metamodel\n\n    if metamodel.textx_tools_support:\n        pos_rule_dict = {}\n    pos_crossref_list = []\n\n    def process_match(nt):\n        \"\"\"\n        Process subtree for match rules.\n        \"\"\"\n        if isinstance(nt, Terminal):\n            return metamodel.convert(nt.value, nt.rule_name)\n        else:\n            # If RHS of assignment is NonTerminal it is a product of\n            # complex match rule. Convert nodes to text and do the join.\n            if len(nt) > 1:\n                result = \"\".join([text(process_match(n)) for n in nt])\n            else:\n                result = process_match(nt[0])\n            obj_processor = metamodel.obj_processors.get(nt.rule_name, None)\n            if obj_processor:\n                return obj_processor(result)\n            else:\n                return result\n\n    def process_node(node):\n        if isinstance(node, Terminal):\n            from arpeggio import RegExMatch\n            if metamodel.use_regexp_group and \\\n                    isinstance(node.rule, RegExMatch):\n                if node.rule.regex.groups == 1:\n                    value = node.extra_info.group(1)\n                    return metamodel.convert(value, node.rule_name)\n                else:\n                    return metamodel.convert(node.value, node.rule_name)\n            else:\n                return metamodel.convert(node.value, node.rule_name)\n\n        assert node.rule.root, \\\n            \"Not a root node: {}\".format(node.rule.rule_name)\n        # If this node is created by some root rule\n        # create metaclass instance.\n        inst = None\n        if not node.rule_name.startswith('__asgn'):\n            # If not assignment\n            # Get class\n            mclass = node.rule._tx_class\n\n            if mclass._tx_type == RULE_ABSTRACT:\n                # If this meta-class is product of abstract rule replace it\n                # with matched concrete meta-class down the inheritance tree.\n                # Abstract meta-class should never be instantiated.\n                if len(node) > 1:\n                    try:\n                        return process_node(\n                            next(n for n in node\n                                 if type(n) is not Terminal\n                                 and n.rule._tx_class is not RULE_MATCH))\n                    except StopIteration:\n                        # All nodes are match rules, do concatenation\n                        return ''.join(text(n) for n in node)\n                else:\n                    return process_node(node[0])\n            elif mclass._tx_type == RULE_MATCH:\n                # If this is a product of match rule handle it as a RHS\n                # of assignment and return converted python type.\n                return process_match(node)\n\n            if parser.debug:\n                parser.dprint(\"CREATING INSTANCE {}\".format(node.rule_name))\n\n            # If user class is given\n            # use it instead of generic one\n            if node.rule_name in metamodel.user_classes:\n                user_class = metamodel.user_classes[node.rule_name]\n\n                # Object initialization will be done afterwards\n                # At this point we need object to be allocated\n                # So that nested object get correct reference\n                inst = user_class.__new__(user_class)\n\n                # Initialize object attributes for user class\n                parser.metamodel._init_obj_attrs(inst, user=True)\n            else:\n                # Generic class will call attributes init\n                # from the constructor\n                inst = mclass.__new__(mclass)\n\n                # Initialize object attributes\n                parser.metamodel._init_obj_attrs(inst)\n\n            # Collect attributes directly on meta-class instance\n            obj_attrs = inst\n\n            inst._tx_position = node.position\n            inst._tx_position_end = node.position_end\n\n            # Push real obj. and dummy attr obj on the instance stack\n            parser._inst_stack.append((inst, obj_attrs))\n\n            for n in node:\n                if parser.debug:\n                    parser.dprint(\"Recursing into {} = '{}'\"\n                                  .format(type(n).__name__, text(n)))\n                process_node(n)\n\n            parser._inst_stack.pop()\n\n            # If this object is nested add 'parent' reference\n            if parser._inst_stack:\n                if node.rule_name in metamodel.user_classes:\n                    obj_attrs._txa_parent = parser._inst_stack[-1][0]\n                else:\n                    obj_attrs.parent = parser._inst_stack[-1][0]\n\n            # If the class is user supplied we need to do\n            # a proper initialization at this point.\n            if node.rule_name in metamodel.user_classes:\n                try:\n                    # Get only attributes defined by the grammar as well\n                    # as `parent` if exists\n                    attrs = {}\n                    if hasattr(obj_attrs, '_txa_parent'):\n                        attrs['parent'] = obj_attrs._txa_parent\n                        del obj_attrs._txa_parent\n                    for a in obj_attrs.__class__._tx_attrs:\n                        attrs[a] = getattr(obj_attrs, \"_txa_%s\" % a)\n                        delattr(obj_attrs, \"_txa_%s\" % a)\n                    inst.__init__(**attrs)\n                except TypeError as e:\n                    # Add class name information in case of\n                    # wrong constructor parameters\n                    e.args += (\"for class %s\" %\n                               inst.__class__.__name__,)\n                    parser.dprint(traceback.print_exc())\n                    raise e\n\n            # Special case for 'name' attrib. It is used for cross-referencing\n            if hasattr(inst, 'name') and inst.name:\n                # Objects of each class are in its own namespace\n                if not id(inst.__class__) in parser._instances:\n                    parser._instances[id(inst.__class__)] = {}\n                parser._instances[id(inst.__class__)][inst.name] = inst\n\n            if parser.debug:\n                parser.dprint(\"LEAVING INSTANCE {}\".format(node.rule_name))\n\n        else:\n            # Handle assignments\n            attr_name = node.rule._attr_name\n            op = node.rule_name.split('_')[-1]\n            model_obj, obj_attr = parser._inst_stack[-1]\n            cls = type(model_obj)\n            metaattr = cls._tx_attrs[attr_name]\n\n            # Mangle attribute name to prevent name clashing with property\n            # setters on user classes\n            if cls.__name__ in metamodel.user_classes:\n                txa_attr_name = \"_txa_%s\" % attr_name\n            else:\n                txa_attr_name = attr_name\n\n            if parser.debug:\n                parser.dprint('Handling assignment: {} {}...'\n                              .format(op, txa_attr_name))\n\n            if op == 'optional':\n                setattr(obj_attr, txa_attr_name, True)\n\n            elif op == 'plain':\n                attr_value = getattr(obj_attr, txa_attr_name)\n                if attr_value and type(attr_value) is not list:\n                    fmt = \"Multiple assignments to attribute {} at {}\"\n                    raise TextXSemanticError(\n                        message=fmt.format(\n                            attr_name, parser.pos_to_linecol(node.position)),\n                        err_type=MULT_ASSIGN_ERROR)\n\n                # Convert tree bellow assignment to proper value\n                value = process_node(node[0])\n\n                if metaattr.ref and not metaattr.cont:\n                    # If this is non-containing reference create ObjCrossRef\n                    value = ObjCrossRef(obj_name=value, cls=metaattr.cls,\n                                        position=node[0].position)\n                    parser._crossrefs.append((model_obj, metaattr, value))\n                    return model_obj\n\n                if type(attr_value) is list:\n                    attr_value.append(value)\n                else:\n                    setattr(obj_attr, txa_attr_name, value)\n\n            elif op in ['list', 'oneormore', 'zeroormore']:\n                for n in node:\n                    # If the node is separator skip\n                    if n.rule_name != 'sep':\n                        # Convert node to proper type\n                        # Rule links will be resolved later\n                        value = process_node(n)\n\n                        if metaattr.ref and not metaattr.cont:\n                            # If this is non-containing reference\n                            # create ObjCrossRef\n\n                            value = ObjCrossRef(obj_name=value,\n                                                cls=metaattr.cls,\n                                                position=n.position)\n\n                            parser._crossrefs.append((obj_attr, metaattr,\n                                                      value))\n                            continue\n\n                        if not hasattr(obj_attr, txa_attr_name) or \\\n                                getattr(obj_attr, txa_attr_name) is None:\n                            setattr(obj_attr, txa_attr_name, [])\n                        getattr(obj_attr, txa_attr_name).append(value)\n            else:\n                # This shouldn't happen\n                assert False\n\n        # Collect rules for textx-tools\n        if inst and metamodel.textx_tools_support:\n            pos = (inst._tx_position, inst._tx_position_end)\n            pos_rule_dict[pos] = inst\n\n        return inst\n\n    def call_obj_processors(metamodel, model_obj,\n                            metaclass_of_grammar_rule=None):\n        \"\"\"\n        Depth-first model object processing.\n        \"\"\"\n        try:\n            if metaclass_of_grammar_rule is None:\n                metaclass_of_grammar_rule = \\\n                    metamodel[model_obj.__class__.__name__]\n        except KeyError:\n            raise TextXSemanticError(\n                'Unknown meta-class \"{}\".'\n                .format(model.obj.__class__.__name__))\n\n        if metaclass_of_grammar_rule._tx_type is RULE_MATCH:\n            # Object processors for match rules are already called\n            # in `process_match`\n            return\n\n        many = [MULT_ONEORMORE, MULT_ZEROORMORE]\n\n        # return value of obj_processor\n        return_value_grammar = None\n        return_value_current = None\n\n        # enter recursive visit of attributes only, if the class of the\n        # object being processed is a meta class of the current meta model\n        if model_obj.__class__.__name__ in metamodel:\n            current_metaclass_of_obj = metamodel[model_obj.__class__.__name__]\n\n            for metaattr in current_metaclass_of_obj._tx_attrs.values():\n                # If attribute is base type or containment reference go down\n                if metaattr.cont:\n                    attr = getattr(model_obj, metaattr.name)\n                    if attr:\n                        if metaattr.mult in many:\n                            for idx, obj in enumerate(attr):\n                                if obj:\n                                    result = call_obj_processors(metamodel,\n                                                                 obj,\n                                                                 metaattr.cls)\n                                    if result is not None:\n                                        attr[idx] = result\n                        else:\n                            result = call_obj_processors(metamodel,\n                                                         attr, metaattr.cls)\n                            if result is not None:\n                                setattr(model_obj, metaattr.name, result)\n\n            # call obj_proc of the current meta_class if type == RULE_ABSTRACT\n            if current_metaclass_of_obj._tx_fqn !=\\\n                    metaclass_of_grammar_rule._tx_fqn:\n                assert RULE_ABSTRACT == metaclass_of_grammar_rule._tx_type\n                obj_processor_current = metamodel.obj_processors.get(\n                    current_metaclass_of_obj.__name__, None)\n                if obj_processor_current:\n                    return_value_current = obj_processor_current(model_obj)\n\n        # call obj_proc of rule found in grammar\n        obj_processor_grammar = metamodel.obj_processors.get(\n            metaclass_of_grammar_rule.__name__, None)\n        if obj_processor_grammar:\n            return_value_grammar = obj_processor_grammar(model_obj)\n\n        # both obj_processors are called, if two different processors\n        # are defined for the object metaclass and the grammar metaclass\n        # (can happen with type==RULE_ABSTRACT):\n        # e.g.\n        #   Base: Special1|Special2;\n        #   RuleCurrentlyChecked: att_to_be_checked=[Base]\n        # with object processors defined for Base, Special1, and Special2.\n        #\n        # Both processors are called, but for the return value the\n        # obj_processor corresponding to the object (e.g. of type Special1)\n        # dominates over the obj_processor of the grammar rule (Base).\n        #\n        # The order they are called is: first object (e.g., Special1), then\n        # the grammar based metaclass object processor (e.g., Base).\n        if return_value_current is not None:\n            return return_value_current\n        else:\n            return return_value_grammar  # may be None\n\n    model = process_node(parse_tree)\n    # Register filename of the model for later use (e.g. imports/scoping).\n    is_primitive_type = False\n    try:\n        model._tx_filename = file_name\n    except AttributeError:\n        # model is some primitive python type (e.g. str)\n        is_primitive_type = True\n        pass\n\n    if pre_ref_resolution_callback:\n        pre_ref_resolution_callback(model)\n\n    for scope_provider in metamodel.scope_providers.values():\n        from textx.scoping import ModelLoader\n        if isinstance(scope_provider, ModelLoader):\n            scope_provider.load_models(model, encoding=encoding)\n\n    if not is_primitive_type:\n        model._tx_reference_resolver = ReferenceResolver(\n            parser, model, pos_crossref_list)\n        model._tx_parser = parser\n\n    if is_main_model:\n        from textx.scoping import get_all_models_including_attached_models\n        models = get_all_models_including_attached_models(model)\n        # filter out all models w/o resolver:\n        models = list(filter(\n            lambda x: hasattr(x, \"_tx_reference_resolver\"), models))\n\n        resolved_count = 1\n        unresolved_count = 1\n        while unresolved_count > 0 and resolved_count > 0:\n            resolved_count = 0\n            unresolved_count = 0\n            # print(\"***RESOLVING {} models\".format(len(models)))\n            for m in models:\n                resolved_count_for_this_model, delayed_crossrefs = \\\n                    m._tx_reference_resolver.resolve_one_step()\n                resolved_count += resolved_count_for_this_model\n                unresolved_count += len(delayed_crossrefs)\n            # print(\"DEBUG: delayed #:{} unresolved #:{}\".\n            #      format(unresolved_count,unresolved_count))\n        if (unresolved_count > 0):\n            error_text = \"Unresolvable cross references:\"\n\n            for m in models:\n                for _, _, delayed \\\n                        in m._tx_reference_resolver.delayed_crossrefs:\n                    line, col = parser.pos_to_linecol(delayed.position)\n                    error_text += ' \"{}\" of class \"{}\" at {}'.format(\n                        delayed.obj_name, delayed.cls.__name__, (line, col))\n            raise TextXSemanticError(error_text, line=line, col=col)\n\n        for m in models:\n            # TODO: what does this check?\n            assert not m._tx_reference_resolver.parser._inst_stack\n\n        # cleanup\n        for m in models:\n            del m._tx_reference_resolver\n\n        # final check that everything went ok\n        for m in models:\n            assert 0 == len(get_children_of_type(Postponed.__class__, m))\n\n            # We have model loaded and all link resolved\n            # So we shall do a depth-first call of object\n            # processors if any processor is defined.\n            if m._tx_metamodel.obj_processors:\n                if parser.debug:\n                    parser.dprint(\"CALLING OBJECT PROCESSORS\")\n                call_obj_processors(m._tx_metamodel, m)\n\n    if metamodel.textx_tools_support \\\n            and type(model) not in PRIMITIVE_PYTHON_TYPES:\n        # Cross-references for go-to definition language server support\n        # Already sorted based on ref_pos_start attr\n        # (required for binary search)\n        model._pos_crossref_list = pos_crossref_list\n\n        # Dict for storing rules where key is position of rule instance in text\n        # Sorted based on nested rules\n        model._pos_rule_dict = OrderedDict(sorted(pos_rule_dict.items(),\n                                                  key=lambda x: x[0],\n                                                  reverse=True))\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_unresolved_crossrefs(self, obj, attr_name=None):\n        if get_model(obj) != self.model:\n            return get_model(obj). \\\n                _tx_reference_resolver.has_unresolved_crossrefs(obj)\n        else:\n            for crossref_obj, attr, crossref in self.parser._crossrefs:\n                if crossref_obj is obj:\n                    if (not attr_name) or attr_name == attr.name:\n                        return True\n            return False", "response": "Returns True if the object has unresolved crossrefs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload all registered models in the given model repository.", "response": "def load_models_in_model_repo(self, global_model_repo=None,\n                                  encoding='utf-8'):\n        \"\"\"\n        load all registered models (called explicitly from\n        the user and not as an automatic activity).\n        Normally this is done automatically while\n        reference resolution of one loaded model.\n\n        However, if you wish to load all models\n        you can call this and get a model repository.\n\n        The metamodels must be identifiable via the MetaModelProvider.\n\n        Returns:\n            a GlobalModelRepository with the loaded models\n        \"\"\"\n        import textx.scoping\n        if not global_model_repo:\n            global_model_repo = textx.scoping.GlobalModelRepository()\n        for filename_pattern in self.filename_pattern_list:\n            global_model_repo.load_models_using_filepattern(\n                filename_pattern, model=None, glob_args=self.glob_args,\n                is_main_model=True, encoding=encoding\n            )\n        return global_model_repo"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Python type from the name of base textx type.", "response": "def python_type(textx_type_name):\n    \"\"\"Return Python type from the name of base textx type.\"\"\"\n    return {\n        'ID': text,\n        'BOOL': bool,\n        'INT': int,\n        'FLOAT': float,\n        'STRICTFLOAT': float,\n        'STRING': text,\n        'NUMBER': float,\n        'BASETYPE': text,\n    }.get(textx_type_name, textx_type_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef language_from_str(language_def, metamodel):\n\n    if type(language_def) is not text:\n        raise TextXError(\"textX accepts only unicode strings.\")\n\n    if metamodel.debug:\n        metamodel.dprint(\"*** PARSING LANGUAGE DEFINITION ***\")\n\n    # Check the cache for already conctructed textX parser\n    if metamodel.debug in textX_parsers:\n        parser = textX_parsers[metamodel.debug]\n    else:\n        # Create parser for TextX grammars using\n        # the arpeggio grammar specified in this module\n        parser = ParserPython(textx_model, comment_def=comment,\n                              ignore_case=False,\n                              reduce_tree=False,\n                              memoization=metamodel.memoization,\n                              debug=metamodel.debug,\n                              file=metamodel.file)\n\n        # Cache it for subsequent calls\n        textX_parsers[metamodel.debug] = parser\n\n    # Parse language description with textX parser\n    try:\n        parse_tree = parser.parse(language_def)\n    except NoMatch as e:\n        line, col = parser.pos_to_linecol(e.position)\n        raise TextXSyntaxError(text(e), line, col)\n\n    # Construct new parser and meta-model based on the given language\n    # description.\n    lang_parser = visit_parse_tree(parse_tree,\n                                   TextXVisitor(parser, metamodel))\n\n    # Meta-model is constructed. Validate its semantics.\n    metamodel.validate()\n\n    # Here we connect meta-model and language parser for convenience.\n    lang_parser.metamodel = metamodel\n    metamodel._parser_blueprint = lang_parser\n\n    if metamodel.debug:\n        # Create dot file for debuging purposes\n        PMDOTExporter().exportFile(\n            lang_parser.parser_model,\n            \"{}_parser_model.dot\".format(metamodel.rootcls.__name__))\n\n    return lang_parser", "response": "Constructs a new parser and initializes metamodel from a given language description."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef second_textx_model(self, model_parser):\n\n        if self.grammar_parser.debug:\n            self.grammar_parser.dprint(\"RESOLVING MODEL PARSER: second_pass\")\n\n        self._resolve_rule_refs(self.grammar_parser, model_parser)\n        self._determine_rule_types(model_parser.metamodel)\n        self._resolve_cls_refs(self.grammar_parser, model_parser)\n\n        return model_parser", "response": "Cross reference resolving for parser model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _resolve_rule_refs(self, grammar_parser, model_parser):\n\n        def _resolve_rule(rule):\n            \"\"\"\n            Recursively resolve peg rule references.\n\n            Args:\n                rule(ParsingExpression or RuleCrossRef)\n            \"\"\"\n            if not isinstance(rule, RuleCrossRef) and rule in resolved_rules:\n                return rule\n            resolved_rules.add(rule)\n\n            if grammar_parser.debug:\n                grammar_parser.dprint(\"Resolving rule: {}\".format(rule))\n\n            if type(rule) is RuleCrossRef:\n                rule_name = rule.rule_name\n                suppress = rule.suppress\n                if rule_name in model_parser.metamodel:\n                    rule = model_parser.metamodel[rule_name]._tx_peg_rule\n                    if type(rule) is RuleCrossRef:\n                        rule = _resolve_rule(rule)\n                        model_parser.metamodel[rule_name]._tx_peg_rule = rule\n                    if suppress:\n                        # Special case. Suppression on rule reference.\n                        _tx_class = rule._tx_class\n                        rule = Sequence(nodes=[rule],\n                                        rule_name=rule_name,\n                                        suppress=suppress)\n                        rule._tx_class = _tx_class\n                else:\n                    line, col = grammar_parser.pos_to_linecol(rule.position)\n                    raise TextXSemanticError(\n                        'Unexisting rule \"{}\" at position {}.'\n                        .format(rule.rule_name,\n                                (line, col)), line, col)\n\n            assert isinstance(rule, ParsingExpression),\\\n                \"{}:{}\".format(type(rule), text(rule))\n\n            # Recurse into subrules, and resolve rules.\n            for idx, child in enumerate(rule.nodes):\n                if child not in resolved_rules:\n                    child = _resolve_rule(child)\n                    rule.nodes[idx] = child\n\n            return rule\n\n        # Two pass resolving\n        for i in range(2):\n            if grammar_parser.debug:\n                grammar_parser.dprint(\"RESOLVING RULE CROSS-REFS - PASS {}\"\n                                      .format(i + 1))\n\n            resolved_rules = set()\n            _resolve_rule(model_parser.parser_model)\n\n            # Resolve rules of all meta-classes to handle unreferenced\n            # rules also.\n            for cls in model_parser.metamodel:\n                cls._tx_peg_rule = _resolve_rule(cls._tx_peg_rule)", "response": "Resolves parser ParsingExpression crossrefs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _determine_rule_types(self, metamodel):\n\n        def _determine_rule_type(cls):\n            \"\"\"\n            Determine rule type (abstract, match, common) and inherited\n            classes.\n            \"\"\"\n\n            if cls in resolved_classes:\n                return\n            resolved_classes.add(cls)\n\n            # If there are attributes collected than this is a common\n            # rule\n            if len(cls._tx_attrs) > 0:\n                if cls._tx_type != RULE_COMMON:\n                    cls._tx_type = RULE_COMMON\n                    has_change[0] = True\n                return\n\n            rule = cls._tx_peg_rule\n\n            # Check if this rule is abstract. Abstract are root rules which\n            # haven't got any attributes and reference at least one non-match\n            # rule.\n            abstract = False\n            if rule.rule_name and cls.__name__ != rule.rule_name:\n                # Special case. Body of the rule is a single rule reference and\n                # the referenced rule is not match rule.\n                target_cls = metamodel[rule.rule_name]\n                _determine_rule_type(target_cls)\n                abstract = target_cls._tx_type != RULE_MATCH\n            else:\n                # Find at least one referenced rule that is not match rule by\n                # going down the parser model and finding root rules.\n                def _has_nonmatch_ref(rule):\n                    for r in rule.nodes:\n                        if r.root:\n                            _determine_rule_type(r._tx_class)\n                            result = r._tx_class._tx_type != RULE_MATCH\n                        else:\n                            result = _has_nonmatch_ref(r)\n                        if result:\n                            return True\n                abstract = _has_nonmatch_ref(rule)\n\n            if abstract and cls._tx_type != RULE_ABSTRACT:\n                cls._tx_type = RULE_ABSTRACT\n                has_change[0] = True\n                # Add inherited classes to this rule's meta-class\n                if rule.rule_name and cls.__name__ != rule.rule_name:\n                    if rule._tx_class not in cls._tx_inh_by:\n                        cls._tx_inh_by.append(rule._tx_class)\n                else:\n                    # Recursively append all referenced classes.\n                    def _add_reffered_classes(rule, inh_by, start=False):\n                        if rule.root and not start:\n                            if hasattr(rule, '_tx_class'):\n                                _determine_rule_type(rule._tx_class)\n                                if rule._tx_class._tx_type != RULE_MATCH and\\\n                                        rule._tx_class not in inh_by:\n                                    inh_by.append(rule._tx_class)\n                        else:\n                            for r in rule.nodes:\n                                _add_reffered_classes(r, inh_by)\n\n                    if type(rule) is OrderedChoice:\n                        for r in rule.nodes:\n                            _add_reffered_classes(r, cls._tx_inh_by)\n                    else:\n                        _add_reffered_classes(rule, cls._tx_inh_by, start=True)\n\n        # Multi-pass rule type resolving to support circular rule references.\n        # `has_change` is a list to support outer scope variable change in\n        # Python 2.x\n        has_change = [True]\n        while has_change[0]:\n            has_change[0] = False\n            resolved_classes = set()\n            for cls in metamodel:\n                _determine_rule_type(cls)", "response": "Determine the types of the rule and metaclass."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate parser rule for assignments and register attribute types and register containments on metaclass.", "response": "def visit_assignment(self, node, children):\n        \"\"\"\n        Create parser rule for assignments and register attribute types\n        on metaclass.\n        \"\"\"\n        attr_name = children[0]\n        op = children[1]\n        rhs_rule, modifiers = children[2]\n        cls = self._current_cls\n        target_cls = None\n\n        if self.debug:\n            self.dprint(\"Processing assignment {}{}...\"\n                        .format(attr_name, op))\n\n        if self.debug:\n            self.dprint(\"Creating attribute {}:{}\".format(cls.__name__,\n                                                          attr_name))\n            self.dprint(\"Assignment operation = {}\".format(op))\n\n        if attr_name in cls._tx_attrs:\n            # If attribute already exists in the metamodel it is\n            # multiple assignment to the same attribute.\n\n            # Cannot use operator ?= on multiple assignments\n            if op == '?=':\n                line, col = self.grammar_parser.pos_to_linecol(node.position)\n                raise TextXSemanticError(\n                    'Cannot use \"?=\" operator on multiple'\n                    ' assignments for attribute \"{}\" at {}'\n                    .format(attr_name, (line, col)), line, col)\n\n            cls_attr = cls._tx_attrs[attr_name]\n        else:\n            cls_attr = self.metamodel._new_cls_attr(cls, name=attr_name,\n                                                    position=node.position)\n\n        # Keep track of metaclass references and containments\n        if type(rhs_rule) is tuple and rhs_rule[0] == \"obj_ref\":\n            cls_attr.cont = False\n            cls_attr.ref = True\n            # Override rhs by its PEG rule for further processing\n            rhs_rule = rhs_rule[1]\n            # Target class is not the same as target rule\n            target_cls = rhs_rule.cls\n\n        base_rule_name = rhs_rule.rule_name\n        if op == '+=':\n            assignment_rule = OneOrMore(\n                nodes=[rhs_rule],\n                rule_name='__asgn_oneormore', root=True)\n            cls_attr.mult = MULT_ONEORMORE\n        elif op == '*=':\n            assignment_rule = ZeroOrMore(\n                nodes=[rhs_rule],\n                rule_name='__asgn_zeroormore', root=True)\n            if cls_attr.mult is not MULT_ONEORMORE:\n                cls_attr.mult = MULT_ZEROORMORE\n        elif op == '?=':\n            assignment_rule = Optional(\n                nodes=[rhs_rule],\n                rule_name='__asgn_optional', root=True)\n            cls_attr.mult = MULT_OPTIONAL\n            base_rule_name = 'BOOL'\n\n            # ?= assigment should have default value of False.\n            # so we shall mark it as such.\n            cls_attr.bool_assignment = True\n\n        else:\n            assignment_rule = Sequence(\n                nodes=[rhs_rule],\n                rule_name='__asgn_plain', root=True)\n\n        # Modifiers\n        if modifiers:\n            modifiers, position = modifiers\n            # Sanity check. Modifiers do not make\n            # sense for ?= and = operator at the moment.\n            if op == '?=' or op == '=':\n                line, col = self.grammar_parser.pos_to_linecol(position)\n                raise TextXSyntaxError(\n                    'Modifiers are not allowed for \"{}\" operator at {}'\n                    .format(op, text((line, col))), line, col)\n\n            # Separator modifier\n            assignment_rule.sep = modifiers.get('sep', None)\n\n            # End of line termination modifier\n            if 'eolterm' in modifiers:\n                assignment_rule.eolterm = True\n\n        if target_cls:\n            attr_type = target_cls\n        else:\n            # Use STRING as default attr class\n            attr_type = base_rule_name if base_rule_name else 'STRING'\n        if not cls_attr.cls:\n            cls_attr.cls = ClassCrossRef(cls_name=attr_type,\n                                         position=node.position)\n        else:\n            # cls cross ref might already be set in case of multiple assignment\n            # to the same attribute. If types are not the same we shall use\n            # OBJECT as generic type.\n            if cls_attr.cls.cls_name != attr_type:\n                cls_attr.cls.cls_name = 'OBJECT'\n\n        if self.debug:\n            self.dprint(\"Created attribute {}:{}[cls={}, cont={}, \"\n                        \"ref={}, mult={}, pos={}]\"\n                        .format(cls.__name__, attr_name, cls_attr.cls.cls_name,\n                                cls_attr.cont, cls_attr.ref, cls_attr.mult,\n                                cls_attr.position))\n\n        assignment_rule._attr_name = attr_name\n        assignment_rule._exp_str = attr_name    # For nice error reporting\n        return assignment_rule"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match_abstract_str(cls):\n    def r(s):\n        if s.root:\n            if s in visited or s.rule_name in ALL_TYPE_NAMES or \\\n                    (hasattr(s, '_tx_class') and\n                     s._tx_class._tx_type is not RULE_MATCH):\n                return s.rule_name\n\n        visited.add(s)\n        if isinstance(s, Match):\n            result = text(s)\n        elif isinstance(s, OrderedChoice):\n            result = \"|\".join([r(x) for x in s.nodes])\n        elif isinstance(s, Sequence):\n            result = \" \".join([r(x) for x in s.nodes])\n        elif isinstance(s, ZeroOrMore):\n            result = \"({})*\".format(r(s.nodes[0]))\n        elif isinstance(s, OneOrMore):\n            result = \"({})+\".format(r(s.nodes[0]))\n        elif isinstance(s, Optional):\n            result = \"{}?\".format(r(s.nodes[0]))\n        elif isinstance(s, SyntaxPredicate):\n            result = \"\"\n        return \"{}{}\".format(result, \"-\" if s.suppress else \"\")\n\n    mstr = \"\"\n    if cls.__name__ not in ALL_TYPE_NAMES and \\\n            not (cls._tx_type is RULE_ABSTRACT and\n                 cls.__name__ != cls._tx_peg_rule.rule_name):\n        e = cls._tx_peg_rule\n        visited = set()\n        if not isinstance(e, Match):\n            visited.add(e)\n        if isinstance(e, OrderedChoice):\n            mstr = \"|\".join([r(x) for x in e.nodes\n                             if x.rule_name in BASE_TYPE_NAMES or not x.root])\n        elif isinstance(e, Sequence):\n            mstr = \" \".join([r(x) for x in e.nodes])\n        else:\n            mstr = r(e)\n\n        mstr = dot_escape(mstr)\n\n    return mstr", "response": "Returns a nice string representation of the abstract or match rule meta - class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting the given model to the given file.", "response": "def model_export(model, file_name, repo=None):\n    \"\"\"\n    Args:\n        model: the model to be exported (may be None if repo is not None)\n        file_name: the output file name\n        repo: the model repo (alternative to model input) to be exported\n\n    Returns:\n        Nothing\n    \"\"\"\n    with codecs.open(file_name, 'w', encoding=\"utf-8\") as f:\n        model_export_to_file(f, model, repo)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport the current state of the object to a file.", "response": "def model_export_to_file(f, model=None, repo=None):\n    \"\"\"\n    Args:\n        f: the file object to be used as output.\n        model: the model to be exported (alternative to repo)\n        repo: the repo to be exported (alternative to model)\n\n    Returns:\n        Nothing\n    \"\"\"\n    if not model and not repo:\n        raise Exception(\"specity either a model or a repo\")\n    if model and repo:\n        raise Exception(\"specity either a model or a repo\")\n\n    processed_set = set()\n    f.write(HEADER)\n\n    def _export(obj):\n\n        if obj is None or obj in processed_set or type(obj) \\\n                in PRIMITIVE_PYTHON_TYPES:\n            return\n\n        processed_set.add(obj)\n\n        attrs = \"\"\n        obj_cls = obj.__class__\n        name = \"\"\n        for attr_name, attr in obj_cls._tx_attrs.items():\n\n            attr_value = getattr(obj, attr_name)\n\n            endmark = 'arrowtail=diamond dir=both' if attr.cont else \"\"\n            required = \"+\" if attr.mult in \\\n                [MULT_ONE, MULT_ONEORMORE] else \"\"\n\n            if attr.mult in [MULT_ONEORMORE, MULT_ZEROORMORE]:\n                if all([type(x) in PRIMITIVE_PYTHON_TYPES\n                        for x in attr_value]):\n                    attrs += \"{}{}:list=[\".format(required, attr_name)\n                    attrs += \",\".join([dot_repr(x) for x in attr_value])\n                    attrs += \"]\\\\l\"\n                else:\n                    for idx, list_obj in enumerate(attr_value):\n                        if list_obj is not None:\n                            if type(list_obj) in PRIMITIVE_PYTHON_TYPES:\n                                f.write(\n                                    '{} -> \"{}:{}\" [label=\"{}:{}\" {}]\\n'\n                                    .format(id(obj), list_obj,\n                                            type(list_obj).__name__,\n                                            attr_name, idx, endmark))\n                            else:\n                                f.write('{} -> {} [label=\"{}:{}\" {}]\\n'\n                                        .format(id(obj), id(list_obj),\n                                                attr_name, idx, endmark))\n                                _export(list_obj)\n            else:\n\n                # Plain attributes\n                if type(attr_value) is text and attr_name != 'name':\n                    attr_value = dot_repr(attr_value)\n\n                if type(attr_value) in PRIMITIVE_PYTHON_TYPES:\n                    if attr_name == 'name':\n                        name = attr_value\n                    else:\n                        attrs += \"{}{}:{}={}\\\\l\".format(\n                            required, attr_name, type(attr_value)\n                            .__name__, attr_value)\n                else:\n                    # Object references\n                    if attr_value is not None:\n                        f.write('{} -> {} [label=\"{}\" {}]\\n'.format(\n                            id(obj), id(attr_value),\n                            attr_name, endmark))\n                        _export(attr_value)\n\n        name = \"{}:{}\".format(name, obj_cls.__name__)\n\n        f.write('{}[label=\"{{{}|{}}}\"]\\n'.format(id(obj), name, attrs))\n\n    def _export_subgraph(m):\n        from textx import get_children\n        f.write('subgraph \"cluster_{}\" {{\\n'.format(m._tx_filename))\n        f.write('''\n        penwidth=2.0\n        color=darkorange4;\n        label = \"{}\";\n                    '''.format(m._tx_filename))\n        for obj in get_children(lambda _: True, m):\n            f.write('{};\\n'.format(id(obj)))\n        f.write('\\n}\\n')\n\n    if repo or hasattr(model, \"_tx_model_repository\"):\n        if not repo:\n            repo = model._tx_model_repository.all_models\n        for m in repo.filename_to_model.values():\n            _export_subgraph(m)\n            _export(m)\n    else:\n        _export(model)\n\n    f.write('\\n}\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef metamodel_from_str(lang_desc, metamodel=None, **kwargs):\n\n    if not metamodel:\n        metamodel = TextXMetaModel(**kwargs)\n\n    language_from_str(lang_desc, metamodel)\n\n    return metamodel", "response": "Creates a metamodel from the given textX language description."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef metamodel_from_file(file_name, **kwargs):\n    with codecs.open(file_name, 'r', 'utf-8') as f:\n        lang_desc = f.read()\n\n    metamodel = metamodel_from_str(lang_desc=lang_desc,\n                                   file_name=file_name,\n                                   **kwargs)\n\n    return metamodel", "response": "Creates a metamodel from a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _enter_namespace(self, namespace_name):\n        if namespace_name not in self.namespaces:\n            self.namespaces[namespace_name] = {}\n\n            # BASETYPE namespace is imported in each namespace\n            # as the first namespace to be searched.\n            self._imported_namespaces[namespace_name] = \\\n                [self.namespaces['__base__']]\n\n        self._namespace_stack.append(namespace_name)", "response": "A namespace is usually an absolute file name of the grammar.\n        A special namespace '__base__' is used for BASETYPE namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts a new import.", "response": "def _new_import(self, import_name):\n        \"\"\"\n        Starts a new import.\n        Args:\n            import_name(str): A relative import in the dot syntax\n                (e.g. \"first.second.expressions\")\n        \"\"\"\n\n        # Import can't be used if meta-model is loaded from string\n        assert self.root_path is not None, \\\n            '\"import\" statement can not be used if meta-model is ' \\\n            'loaded from string.'\n\n        # Find the absolute file name of the import based on the relative\n        # import_name and current namespace\n        current_namespace = self._namespace_stack[-1]\n        if '.' in current_namespace:\n            root_namespace = current_namespace.rsplit('.', 1)[0]\n            import_name = \"%s.%s\" % (root_namespace, import_name)\n\n        import_file_name = \"%s.tx\" % os.path.join(self.root_path,\n                                                  *import_name.split(\".\"))\n\n        if import_name not in self.namespaces:\n            self._enter_namespace(import_name)\n            if self.debug:\n                self.dprint(\"*** IMPORTING FILE: %s\" % import_file_name)\n            metamodel_from_file(import_file_name, metamodel=self)\n            self._leave_namespace()\n\n        # Add the import to the imported_namespaces for current namespace\n        # so that resolving of current grammar searches imported grammars\n        # in the order of import\n        self._imported_namespaces[current_namespace].append(\n            self.namespaces[import_name])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new class with the given name.", "response": "def _new_class(self, name, peg_rule, position, position_end=None,\n                   inherits=None, root=False, rule_type=RULE_MATCH):\n        \"\"\"\n        Creates a new class with the given name in the current namespace.\n        Args:\n            name(str): The name of the class.\n            peg_rule(ParserExpression): An arpeggio peg rule used to match\n                this class.\n            position(int): A position in the input where class is defined.\n            root(bool): Is this class a root class of the metamodel.\n            rule_type: The type of the rule this meta-class is for. One of\n                RULE_COMMON, RULE_ABSTRACT or RULE_MATCH.\n        \"\"\"\n\n        class TextXMetaClass(type):\n\n            def __repr__(cls):\n                return '<textx:{} class at {}>'.format(cls._tx_fqn,\n                                                       id(cls))\n\n        @add_metaclass(TextXMetaClass)\n        class TextXClass(object):\n            \"\"\"\n            Dynamically created class. Each textX rule will result in\n            creating one Python class with the type name of the rule.\n            textX model is a graph of instances of these Python classes.\n\n            Attributes:\n                _tx_attrs(dict): A dict of meta-attributes keyed by name.\n                    Used by common rules.\n                _tx_inh_by(list): Classes that inherits this one. Used by\n                    abstract rules.\n                _tx_position(int): A position in the input string where\n                    this class is defined.\n                _tx_position_end(int): A position in the input string where\n                    this class ends.\n                _tx_type(int): The type of the textX rule this class is\n                    created for. See textx.const\n                _tx_metamodel(TextXMetaModel): A metamodel this class\n                    belongs to.\n                _tx_peg_rule(ParsingExpression): An Arpeggio PEG rule that\n                    matches this class.\n\n            \"\"\"\n\n            def __repr__(self):\n                \"\"\"\n                Used for TextXClass bellow.\n                \"\"\"\n                if hasattr(self, 'name'):\n                    return \"<{}:{}>\".format(name, self.name)\n                else:\n                    return \"<textx:{} instance at {}>\"\\\n                        .format(self._tx_fqn, hex(id(self)))\n\n        cls = TextXClass\n        cls.__name__ = name\n\n        self._init_class(cls, peg_rule, position, position_end, inherits, root,\n                         rule_type)\n\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the meta - class instance for the given class.", "response": "def _init_class(self, cls, peg_rule, position, position_end=None,\n                    inherits=None, root=False, rule_type=RULE_MATCH):\n        \"\"\"\n        Setup meta-class special attributes, namespaces etc. This is called\n        both for textX created classes as well as user classes.\n        \"\"\"\n        cls._tx_metamodel = self\n\n        # Attribute information (MetaAttr instances) keyed by name.\n        cls._tx_attrs = OrderedDict()\n\n        # A list of inheriting classes\n        cls._tx_inh_by = inherits if inherits else []\n\n        cls._tx_position = position\n\n        cls._tx_position_end = \\\n            position if position_end is None else position_end\n\n        # The type of the rule this meta-class results from.\n        # There are three rule types: common, abstract and match\n        # Base types are match rules.\n        cls._tx_type = rule_type\n\n        cls._tx_peg_rule = peg_rule\n        if peg_rule:\n            peg_rule._tx_class = cls\n\n        # Push this class and PEG rule in the current namespace\n        current_namespace = self.namespaces[self._namespace_stack[-1]]\n        cls._tx_fqn = self._cls_fqn(cls)\n        current_namespace[cls.__name__] = cls\n\n        if root:\n            self.rootcls = cls"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn fully qualified name for the class based on current namespace and the class name.", "response": "def _cls_fqn(self, cls):\n        \"\"\"\n        Returns fully qualified name for the class based on current namespace\n        and the class name.\n        \"\"\"\n        ns = self._namespace_stack[-1]\n        if ns in ['__base__', None]:\n            return cls.__name__\n        else:\n            return ns + '.' + cls.__name__"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_obj_attrs(self, obj, user=False):\n        for attr in obj.__class__._tx_attrs.values():\n\n            if user:\n                # Mangle name to prvent name clashing\n                attr_name = \"_txa_%s\" % attr.name\n            else:\n                attr_name = attr.name\n\n            if attr.mult in [MULT_ZEROORMORE, MULT_ONEORMORE]:\n                # list\n                setattr(obj, attr_name, [])\n            elif attr.cls.__name__ in BASE_TYPE_NAMES:\n                # Instantiate base python type\n                if self.auto_init_attributes:\n                    setattr(obj, attr_name,\n                            python_type(attr.cls.__name__)())\n                else:\n                    # See https://github.com/textX/textX/issues/11\n                    if attr.bool_assignment:\n                        # Only ?= assignments shall have default\n                        # value of False.\n                        setattr(obj, attr_name, False)\n                    else:\n                        # Set base type attribute to None initially\n                        # in order to be able to detect if an optional\n                        # values are given in the model. Default values\n                        # can be specified using object processors.\n                        setattr(obj, attr_name, None)\n            else:\n                # Reference to other obj\n                setattr(obj, attr_name, None)", "response": "Initialize the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new meta attribute of this class.", "response": "def _new_cls_attr(self, clazz, name, cls=None, mult=MULT_ONE, cont=True,\n                      ref=False, bool_assignment=False, position=0):\n        \"\"\"Creates new meta attribute of this class.\"\"\"\n        attr = MetaAttr(name, cls, mult, cont, ref, bool_assignment,\n                        position)\n        clazz._tx_attrs[name] = attr\n        return attr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert(self, value, _type):\n        return self.type_convertors.get(_type, lambda x: x)(value)", "response": "Convert instances of textx types and match rules to python types."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef model_from_str(self, model_str, debug=None,\n                       pre_ref_resolution_callback=None):\n        \"\"\"\n        Instantiates model from the given string.\n        :param pre_ref_resolution_callback: called before references are\n               resolved. This can be useful to manage models distributed\n               across files (scoping)\n        \"\"\"\n\n        if type(model_str) is not text:\n            raise TextXError(\"textX accepts only unicode strings.\")\n\n        model = self._parser_blueprint.clone().get_model_from_str(\n            model_str, debug=debug,\n            pre_ref_resolution_callback=pre_ref_resolution_callback)\n        for p in self._model_processors:\n            p(model, self)\n        return model", "response": "Instantiates a new model from a string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef internal_model_from_file(\n            self, file_name, encoding='utf-8', debug=None,\n            pre_ref_resolution_callback=None, is_main_model=True):\n        \"\"\"\n        Instantiates model from the given file.\n        :param pre_ref_resolution_callback: called before references are\n               resolved. This can be useful to manage models distributed\n               across files (scoping)\n        \"\"\"\n        model = None\n        callback = pre_ref_resolution_callback\n\n        if hasattr(self, \"_tx_model_repository\"):\n            # metamodel has a global repo\n            if not callback:\n                def _pre_ref_resolution_callback(other_model):\n                    from textx.scoping import GlobalModelRepository\n                    filename = other_model._tx_filename\n                    assert filename\n                    # print(\"METAMODEL PRE-CALLBACK{}\".format(filename))\n                    other_model._tx_model_repository = GlobalModelRepository(\n                        self._tx_model_repository.all_models)\n                    self._tx_model_repository.all_models\\\n                        .filename_to_model[filename] = other_model\n\n                callback = _pre_ref_resolution_callback\n            if self._tx_model_repository.all_models.has_model(file_name):\n                model = self._tx_model_repository.all_models\\\n                    .filename_to_model[file_name]\n\n        if not model:\n            # model not present (from global repo) -> load it\n            model = self._parser_blueprint.clone().get_model_from_file(\n                file_name, encoding, debug=debug,\n                pre_ref_resolution_callback=callback,\n                is_main_model=is_main_model)\n\n        for p in self._model_processors:\n            p(model, self)\n        return model", "response": "Instantiates a model from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_obj_processors(self, obj_processors):\n        self.obj_processors = obj_processors\n        self.type_convertors.update(obj_processors)", "response": "Register object processors for this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninterpreting the input loop for the current calendar and prints the menu.", "response": "def interpret(self):\n        \"\"\"\n        Main interpreter loop.\n        \"\"\"\n        self.print_menu()\n        while True:\n            try:\n                event = input()\n                if event == 'q':\n                    return\n                event = int(event)\n                event = self.model.events[event-1]\n            except Exception:\n                print('Invalid input')\n\n            self.event(event)\n            self.print_menu()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the font and a Boolean indicating if the match is exact.", "response": "def find_font(font_name):\n    \"\"\"Return the font and a Boolean indicating if the match is exact.\"\"\"\n    if font_name in STANDARD_FONT_NAMES:\n        return font_name, True\n    elif font_name in _registered_fonts:\n        return font_name, _registered_fonts[font_name]\n\n    NOT_FOUND = (None, False)\n    try:\n        # Try first to register the font if it exists as ttf,\n        # based on ReportLab font search.\n        registerFont(TTFont(font_name, '%s.ttf' % font_name))\n        _registered_fonts[font_name] = True\n        return font_name, True\n    except TTFError:\n        # Try searching with Fontconfig\n        try:\n            pipe = subprocess.Popen(\n                ['fc-match', '-s', '--format=%{file}\\\\n', font_name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n            output = pipe.communicate()[0].decode(sys.getfilesystemencoding())\n            font_path = output.split('\\n')[0]\n        except OSError:\n            return NOT_FOUND\n        try:\n            registerFont(TTFont(font_name, font_path))\n        except TTFError:\n            return NOT_FOUND\n        # Fontconfig may return a default font totally unrelated with font_name\n        exact = font_name.lower() in os.path.basename(font_path).lower()\n        _registered_fonts[font_name] = exact\n        return font_name, exact"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an SVG file to an RLG Drawing object.", "response": "def svg2rlg(path, **kwargs):\n    \"Convert an SVG file to an RLG Drawing object.\"\n\n    # unzip .svgz file into .svg\n    unzipped = False\n    if isinstance(path, str) and os.path.splitext(path)[1].lower() == \".svgz\":\n        with gzip.open(path, 'rb') as f_in, open(path[:-1], 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n        path = path[:-1]\n        unzipped = True\n\n    svg_root = load_svg_file(path)\n    if svg_root is None:\n        return\n\n    # convert to a RLG drawing\n    svgRenderer = SvgRenderer(path, **kwargs)\n    drawing = svgRenderer.render(svg_root)\n\n    # remove unzipped .svgz file (.svg)\n    if unzipped:\n        os.remove(path)\n\n    return drawing"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef monkeypatch_reportlab():\n    from reportlab.pdfgen.canvas import Canvas\n    from reportlab.graphics import shapes\n\n    original_renderPath = shapes._renderPath\n    def patchedRenderPath(path, drawFuncs, **kwargs):\n        # Patched method to transfer fillRule from Path to PDFPathObject\n        # Get back from bound method to instance\n        try:\n            drawFuncs[0].__self__.fillMode = path._fillRule\n        except AttributeError:\n            pass\n        return original_renderPath(path, drawFuncs, **kwargs)\n    shapes._renderPath = patchedRenderPath\n\n    original_drawPath = Canvas.drawPath\n    def patchedDrawPath(self, path, **kwargs):\n        current = self._fillMode\n        if hasattr(path, 'fillMode'):\n            self._fillMode = path.fillMode\n        else:\n            self._fillMode = FILL_NON_ZERO\n        original_drawPath(self, path, **kwargs)\n        self._fillMode = current\n    Canvas.drawPath = patchedDrawPath", "response": "Monkey patches ReportLab to use even - Odd filling mode for paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying parsing compound attribute string. Return a dictionary with single attributes in line.", "response": "def parseMultiAttributes(self, line):\n        \"\"\"Try parsing compound attribute string.\n\n        Return a dictionary with single attributes in 'line'.\n        \"\"\"\n\n        attrs = line.split(';')\n        attrs = [a.strip() for a in attrs]\n        attrs = filter(lambda a:len(a)>0, attrs)\n\n        new_attrs = {}\n        for a in attrs:\n            k, v = a.split(':')\n            k, v = [s.strip() for s in (k, v)]\n            new_attrs[k] = v\n\n        return new_attrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches an attribute with some name in some node or above.", "response": "def findAttr(self, svgNode, name):\n        \"\"\"Search an attribute with some name in some node or above.\n\n        First the node is searched, then its style attribute, then\n        the search continues in the node's parent node. If no such\n        attribute is found, '' is returned.\n        \"\"\"\n\n        # This needs also to lookup values like \"url(#SomeName)\"...\n\n        if self.css_rules is not None and not svgNode.attrib.get('__rules_applied', False):\n            if isinstance(svgNode, NodeTracker):\n                svgNode.apply_rules(self.css_rules)\n            else:\n                ElementWrapper(svgNode).apply_rules(self.css_rules)\n        attr_value = svgNode.attrib.get(name, '').strip()\n\n        if attr_value and attr_value != \"inherit\":\n            return attr_value\n        elif svgNode.attrib.get(\"style\"):\n            dict = self.parseMultiAttributes(svgNode.attrib.get(\"style\"))\n            if name in dict:\n                return dict[name]\n        if svgNode.getparent() is not None:\n            return self.findAttr(svgNode.getparent(), name)\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of all attributes of svgNode or those inherited by it.", "response": "def getAllAttributes(self, svgNode):\n        \"Return a dictionary of all attributes of svgNode or those inherited by it.\"\n\n        dict = {}\n\n        if node_name(svgNode.getparent()) == 'g':\n            dict.update(self.getAllAttributes(svgNode.getparent()))\n\n        style = svgNode.attrib.get(\"style\")\n        if style:\n            d = self.parseMultiAttributes(style)\n            dict.update(d)\n\n        for key, value in svgNode.attrib.items():\n            if key != \"style\":\n                dict[key] = value\n\n        return dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convertTransform(self, svgAttr):\n\n        line = svgAttr.strip()\n\n        ops = line[:]\n        brackets = []\n        indices = []\n        for i, lin in enumerate(line):\n            if lin in \"()\":\n                brackets.append(i)\n        for i in range(0, len(brackets), 2):\n            bi, bj = brackets[i], brackets[i+1]\n            subline = line[bi+1:bj]\n            subline = subline.strip()\n            subline = subline.replace(',', ' ')\n            subline = re.sub(\"[ ]+\", ',', subline)\n            try:\n                if ',' in subline:\n                    indices.append(tuple(float(num) for num in subline.split(',')))\n                else:\n                    indices.append(float(subline))\n            except ValueError:\n                continue\n            ops = ops[:bi] + ' '*(bj-bi+1) + ops[bj+1:]\n        ops = ops.replace(',', ' ').split()\n\n        if len(ops) != len(indices):\n            logger.warning(\"Unable to parse transform expression '%s'\" % svgAttr)\n            return []\n\n        result = []\n        for i, op in enumerate(ops):\n            result.append((op, indices[i]))\n\n        return result", "response": "Parse transform attribute string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convertLength(self, svgAttr, percentOf=100, em_base=12):\n        \"Convert length to points.\"\n\n        text = svgAttr\n        if not text:\n            return 0.0\n        if ' ' in text.replace(',', ' ').strip():\n            logger.debug(\"Only getting first value of %s\" % text)\n            text = text.replace(',', ' ').split()[0]\n\n        if text.endswith('%'):\n            logger.debug(\"Fiddling length unit: %\")\n            return float(text[:-1]) / 100 * percentOf\n        elif text.endswith(\"pc\"):\n            return float(text[:-2]) * pica\n        elif text.endswith(\"pt\"):\n            return float(text[:-2]) * 1.25\n        elif text.endswith(\"em\"):\n            return float(text[:-2]) * em_base\n        elif text.endswith(\"px\"):\n            return float(text[:-2])\n\n        if \"ex\" in text:\n            logger.warning(\"Ignoring unit ex\")\n            text = text.replace(\"ex\", '')\n\n        text = text.strip()\n        length = toLength(text)  # this does the default measurements such as mm and cm\n\n        return length", "response": "Convert length to points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a list of lengths.", "response": "def convertLengthList(self, svgAttr):\n        \"\"\"Convert a list of lengths.\"\"\"\n        return [self.convertLength(a) for a in self.split_attr_list(svgAttr)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts string to a RL color object.", "response": "def convertColor(self, svgAttr):\n        \"Convert string to a RL color object.\"\n\n        # fix it: most likely all \"web colors\" are allowed\n        predefined = \"aqua black blue fuchsia gray green lime maroon navy \"\n        predefined = predefined + \"olive orange purple red silver teal white yellow \"\n        predefined = predefined + \"lawngreen indianred aquamarine lightgreen brown\"\n\n        # This needs also to lookup values like \"url(#SomeName)\"...\n\n        text = svgAttr\n        if not text or text == \"none\":\n            return None\n\n        if text in predefined.split():\n            return self.color_converter(getattr(colors, text))\n        elif text == \"currentColor\":\n            return \"currentColor\"\n        elif len(text) == 7 and text[0] == '#':\n            return self.color_converter(colors.HexColor(text))\n        elif len(text) == 4 and text[0] == '#':\n            return self.color_converter(colors.HexColor('#' + 2*text[1] + 2*text[2] + 2*text[3]))\n        elif text.startswith('rgb') and '%' not in text:\n            t = text[3:].strip('()')\n            tup = [h[2:] for h in [hex(int(num)) for num in t.split(',')]]\n            tup = [(2 - len(h)) * '0' + h for h in tup]\n            col = \"#%s%s%s\" % tuple(tup)\n            return self.color_converter(colors.HexColor(col))\n        elif text.startswith('rgb') and '%' in text:\n            t = text[3:].replace('%', '').strip('()')\n            tup = (float(val)/100.0 for val in t.split(','))\n            return self.color_converter(colors.Color(*tup))\n\n        logger.warning(\"Can't handle color: %s\" % text)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the clipping Path object referenced by the node clip - path attribute if any.", "response": "def get_clippath(self, node):\n        \"\"\"\n        Return the clipping Path object referenced by the node 'clip-path'\n        attribute, if any.\n        \"\"\"\n        def get_path_from_node(node):\n            for child in node.getchildren():\n                if node_name(child) == 'path':\n                    group = self.shape_converter.convertShape('path', NodeTracker(child))\n                    return group.contents[-1]\n                else:\n                    return get_path_from_node(child)\n\n        clip_path = node.getAttribute('clip-path')\n        if clip_path:\n            m = re.match(r'url\\(#([^\\)]*)\\)', clip_path)\n            if m:\n                ref = m.groups()[0]\n                if ref in self.definitions:\n                    path = get_path_from_node(self.definitions[ref])\n                    if path:\n                        path = ClippingPath(copy_from=path)\n                        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_text(self, text, preserve_space):\n        if text is None:\n            return\n        if preserve_space:\n            text = text.replace('\\r\\n', ' ').replace('\\n', ' ').replace('\\t', ' ')\n        else:\n            text = text.replace('\\r\\n', '').replace('\\n', '').replace('\\t', ' ')\n            text = text.strip()\n            while ('  ' in text):\n                text = text.replace('  ', ' ')\n        return text", "response": "Text cleaning as per SVG text. html#WhiteSpace\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef applyTransformOnGroup(self, transform, group):\n\n        tr = self.attrConverter.convertTransform(transform)\n        for op, values in tr:\n            if op == \"scale\":\n                if not isinstance(values, tuple):\n                    values = (values, values)\n                group.scale(*values)\n            elif op == \"translate\":\n                if isinstance(values, (int, float)):\n                    # From the SVG spec: If <ty> is not provided, it is assumed to be zero.\n                    values = values, 0\n                group.translate(*values)\n            elif op == \"rotate\":\n                if not isinstance(values, tuple) or len(values) == 1:\n                    group.rotate(values)\n                elif len(values) == 3:\n                    angle, cx, cy = values\n                    group.translate(cx, cy)\n                    group.rotate(angle)\n                    group.translate(-cx, -cy)\n            elif op == \"skewX\":\n                group.skew(values, 0)\n            elif op == \"skewY\":\n                group.skew(0, values)\n            elif op == \"matrix\":\n                group.transform = values\n            else:\n                logger.debug(\"Ignoring transform: %s %s\" % (op, values))", "response": "Applies an SVG transformation to a RL Group shape."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying styles from an SVG element to an RL shape.", "response": "def applyStyleOnShape(self, shape, node, only_explicit=False):\n        \"\"\"\n        Apply styles from an SVG element to an RLG shape.\n        If only_explicit is True, only attributes really present are applied.\n        \"\"\"\n\n        # RLG-specific: all RLG shapes\n        \"Apply style attributes of a sequence of nodes to an RL shape.\"\n\n        # tuple format: (svgAttr, rlgAttr, converter, default)\n        mappingN = (\n            (\"fill\", \"fillColor\", \"convertColor\", \"black\"),\n            (\"fill-opacity\", \"fillOpacity\", \"convertOpacity\", 1),\n            (\"fill-rule\", \"_fillRule\", \"convertFillRule\", \"nonzero\"),\n            (\"stroke\", \"strokeColor\", \"convertColor\", \"none\"),\n            (\"stroke-width\", \"strokeWidth\", \"convertLength\", \"1\"),\n            (\"stroke-opacity\", \"strokeOpacity\", \"convertOpacity\", 1),\n            (\"stroke-linejoin\", \"strokeLineJoin\", \"convertLineJoin\", \"0\"),\n            (\"stroke-linecap\", \"strokeLineCap\", \"convertLineCap\", \"0\"),\n            (\"stroke-dasharray\", \"strokeDashArray\", \"convertDashArray\", \"none\"),\n        )\n        mappingF = (\n            (\"font-family\", \"fontName\", \"convertFontFamily\", DEFAULT_FONT_NAME),\n            (\"font-size\", \"fontSize\", \"convertLength\", \"12\"),\n            (\"text-anchor\", \"textAnchor\", \"id\", \"start\"),\n        )\n\n        if shape.__class__ == Group:\n            # Recursively apply style on Group subelements\n            for subshape in shape.contents:\n                self.applyStyleOnShape(subshape, node, only_explicit=only_explicit)\n            return\n\n        ac = self.attrConverter\n        for mapping in (mappingN, mappingF):\n            if shape.__class__ != String and mapping == mappingF:\n                continue\n            for (svgAttrName, rlgAttr, func, default) in mapping:\n                svgAttrValue = ac.findAttr(node, svgAttrName)\n                if svgAttrValue == '':\n                    if only_explicit:\n                        continue\n                    else:\n                        svgAttrValue = default\n                if svgAttrValue == \"currentColor\":\n                    svgAttrValue = ac.findAttr(node.getparent(), \"color\") or default\n                try:\n                    meth = getattr(ac, func)\n                    setattr(shape, rlgAttr, meth(svgAttrValue))\n                except (AttributeError, KeyError, ValueError):\n                    pass\n        if getattr(shape, 'fillOpacity', None) is not None and shape.fillColor:\n            shape.fillColor.alpha = shape.fillOpacity"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting value into a list of floats.", "response": "def split_floats(op, min_num, value):\n    \"\"\"Split `value`, a list of numbers as a string, to a list of float numbers.\n\n    Also optionally insert a `l` or `L` operation depending on the operation\n    and the length of values.\n    Example: with op='m' and value='10,20 30,40,' the returned value will be\n             ['m', [10.0, 20.0], 'l', [30.0, 40.0]]\n    \"\"\"\n    floats = [float(seq) for seq in re.findall(r'(-?\\d*\\.?\\d*(?:e[+-]\\d+)?)', value) if seq]\n    res = []\n    for i in range(0, len(floats), min_num):\n        if i > 0 and op in {'m', 'M'}:\n            op = 'l' if op == 'm' else 'L'\n        res.extend([op, floats[i:i + min_num]])\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalise_svg_path(attr):\n\n    # operator codes mapped to the minimum number of expected arguments\n    ops = {\n        'A': 7, 'a': 7,\n        'Q': 4, 'q': 4, 'T': 2, 't': 2, 'S': 4, 's': 4,\n        'M': 2, 'L': 2, 'm': 2, 'l': 2, 'H': 1, 'V': 1,\n        'h': 1, 'v': 1, 'C': 6, 'c': 6, 'Z': 0, 'z': 0,\n    }\n    op_keys = ops.keys()\n\n    # do some preprocessing\n    result = []\n    groups = re.split('([achlmqstvz])', attr.strip(), flags=re.I)\n    op = None\n    for item in groups:\n        if item.strip() == '':\n            continue\n        if item in op_keys:\n            # fix sequences of M to one M plus a sequence of L operators,\n            # same for m and l.\n            if item == 'M' and item == op:\n                op = 'L'\n            elif item == 'm' and item == op:\n                op = 'l'\n            else:\n                op = item\n            if ops[op] == 0:  # Z, z\n                result.extend([op, []])\n        else:\n            result.extend(split_floats(op, ops[op], item))\n            op = result[-2]  # Remember last op\n\n    return result", "response": "This function normalises SVG path. It also fixes operator codes for multi - argument M or MLLL... and mlll... and mlll... and mlll... and mlll... and mlll... and mlll... and mlll... and mlll... and mlll...."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a quadratic Bezier curve through q0 q1 q2 to a cubic one.", "response": "def convert_quadratic_to_cubic_path(q0, q1, q2):\n    \"\"\"\n    Convert a quadratic Bezier curve through q0, q1, q2 to a cubic one.\n    \"\"\"\n    c0 = q0\n    c1 = (q0[0] + 2. / 3 * (q1[0] - q0[0]), q0[1] + 2. / 3 * (q1[1] - q0[1]))\n    c2 = (c1[0] + 1. / 3 * (q2[0] - q0[0]), c1[1] + 1. / 3 * (q2[1] - q0[1]))\n    c3 = q2\n    return c0, c1, c2, c3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsee http://www.w3.org/TR/SVG/implnote.html#ArcImplementationNotes F.6.5 note that we reduce phi to zero outside this routine", "response": "def end_point_to_center_parameters(x1, y1, x2, y2, fA, fS, rx, ry, phi=0):\n    '''\n    See http://www.w3.org/TR/SVG/implnote.html#ArcImplementationNotes F.6.5\n    note that we reduce phi to zero outside this routine\n    '''\n    rx = fabs(rx)\n    ry = fabs(ry)\n\n    # step 1\n    if phi:\n        phi_rad = radians(phi)\n        sin_phi = sin(phi_rad)\n        cos_phi = cos(phi_rad)\n        tx = 0.5 * (x1 - x2)\n        ty = 0.5 * (y1 - y2)\n        x1d = cos_phi * tx - sin_phi * ty\n        y1d = sin_phi * tx + cos_phi * ty\n    else:\n        x1d = 0.5 * (x1 - x2)\n        y1d = 0.5 * (y1 - y2)\n\n    # step 2\n    # we need to calculate\n    # (rx*rx*ry*ry-rx*rx*y1d*y1d-ry*ry*x1d*x1d)\n    # -----------------------------------------\n    #     (rx*rx*y1d*y1d+ry*ry*x1d*x1d)\n    #\n    # that is equivalent to\n    #\n    #          rx*rx*ry*ry\n    # = -----------------------------  -    1\n    #   (rx*rx*y1d*y1d+ry*ry*x1d*x1d)\n    #\n    #              1\n    # = -------------------------------- - 1\n    #   x1d*x1d/(rx*rx) + y1d*y1d/(ry*ry)\n    #\n    # = 1/r - 1\n    #\n    # it turns out r is what they recommend checking\n    # for the negative radicand case\n    r = x1d * x1d / (rx * rx) + y1d * y1d / (ry * ry)\n    if r > 1:\n        rr = sqrt(r)\n        rx *= rr\n        ry *= rr\n        r = x1d * x1d / (rx * rx) + y1d * y1d / (ry * ry)\n    elif r != 0:\n        r = 1 / r - 1\n    if -1e-10 < r < 0:\n        r = 0\n    r = sqrt(r)\n    if fA == fS:\n        r = -r\n    cxd = (r * rx * y1d) / ry\n    cyd = -(r * ry * x1d) / rx\n\n    # step 3\n    if phi:\n        cx = cos_phi * cxd - sin_phi * cyd + 0.5 * (x1 + x2)\n        cy = sin_phi * cxd + cos_phi * cyd + 0.5 * (y1 + y2)\n    else:\n        cx = cxd + 0.5 * (x1 + x2)\n        cy = cyd + 0.5 * (y1 + y2)\n\n    # step 4\n    theta1 = vector_angle((1, 0), ((x1d - cxd) / rx, (y1d - cyd) / ry))\n    dtheta = vector_angle(\n        ((x1d - cxd) / rx, (y1d - cyd) / ry),\n        ((-x1d - cxd) / rx, (-y1d - cyd) / ry)\n    ) % 360\n    if fS == 0 and dtheta > 0:\n        dtheta -= 360\n    elif fS == 1 and dtheta < 0:\n        dtheta += 360\n    return cx, cy, rx, ry, -theta1, -dtheta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef client_start(request, socket, context):\n    CLIENTS[socket.session.session_id] = (request, socket, context)", "response": "Add the client triple to the CLIENTS list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef client_end(request, socket, context):\n    # Send the unsubscribe event prior to actually unsubscribing, so\n    # that the finish event can still match channels if applicable.\n    for channel in socket.channels:\n        events.on_unsubscribe.send(request, socket, context, channel)\n    events.on_finish.send(request, socket, context)\n    # Actually unsubscribe to cleanup channel data.\n    for channel in socket.channels[:]:\n        socket.unsubscribe(channel)\n    # Remove the client.\n    del CLIENTS[socket.session.session_id]", "response": "Handles cleanup when a client triple is finished."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nends the client cleanup on all clients.", "response": "def client_end_all():\n    \"\"\"\n    Performs cleanup on all clients - called by runserver_socketio\n    when the server is shut down or reloaded.\n    \"\"\"\n    for request, socket, context in CLIENTS.values()[:]:\n        client_end(request, socket, context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsubscribe to the given channel.", "response": "def subscribe(self, channel):\n        \"\"\"\n        Add the channel to this socket's channels, and to the list of\n        subscribed session IDs for the channel. Return False if\n        already subscribed, otherwise True.\n        \"\"\"\n        if channel in self.channels:\n            return False\n        CHANNELS[channel].append(self.socket.session.session_id)\n        self.channels.append(channel)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the channel from this socket's channels, and from the list of subscribed session IDs for the channel. Return False if not subscribed, otherwise True.", "response": "def unsubscribe(self, channel):\n        \"\"\"\n        Remove the channel from this socket's channels, and from the\n        list of subscribed session IDs for the channel. Return False\n        if not subscribed, otherwise True.\n        \"\"\"\n        try:\n            CHANNELS[channel].remove(self.socket.session.session_id)\n            self.channels.remove(channel)\n        except ValueError:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef broadcast_channel(self, message, channel=None):\n        if channel is None:\n            channels = self.channels\n        else:\n            channels = [channel]\n        for channel in channels:\n            for subscriber in CHANNELS[channel]:\n                if subscriber != self.socket.session.session_id:\n                    session = self.socket.handler.server.sessions[subscriber]\n                    self._write(message, session)", "response": "Send the given message to all subscribers for the given channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a message to all sockets subscribed to a channel.", "response": "def send_and_broadcast_channel(self, message, channel=None):\n        \"\"\"\n        Shortcut for a socket to broadcast to all sockets subscribed\n        to a channel, and itself.\n        \"\"\"\n        self.send(message)\n        self.broadcast_channel(message, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef message(request, socket, context, message):\n    room = get_object_or_404(ChatRoom, id=message[\"room\"])\n    if message[\"action\"] == \"start\":\n        name = strip_tags(message[\"name\"])\n        user, created = room.users.get_or_create(name=name)\n        if not created:\n            socket.send({\"action\": \"in-use\"})\n        else:\n            context[\"user\"] = user\n            users = [u.name for u in room.users.exclude(id=user.id)]\n            socket.send({\"action\": \"started\", \"users\": users})\n            user.session = socket.session.session_id\n            user.save()\n            joined = {\"action\": \"join\", \"name\": user.name, \"id\": user.id}\n            socket.send_and_broadcast_channel(joined)\n    else:\n        try:\n            user = context[\"user\"]\n        except KeyError:\n            return\n        if message[\"action\"] == \"message\":\n            message[\"message\"] = strip_tags(message[\"message\"])\n            message[\"name\"] = user.name\n            socket.send_and_broadcast_channel(message)", "response": "This function handles the message event."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a message to the given session ID.", "response": "def send(session_id, message):\n    \"\"\"\n    Send a message to the socket for the given session ID.\n    \"\"\"\n    try:\n        socket = CLIENTS[session_id][1]\n    except KeyError:\n        raise NoSocket(\"There is no socket with the session ID: \" + session_id)\n    socket.send(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbroadcasts a message to all sockets in the current node.", "response": "def broadcast(message):\n    \"\"\"\n    Find the first socket and use it to broadcast to all sockets\n    including the socket itself.\n    \"\"\"\n    try:\n        socket = CLIENTS.values()[0][1]\n    except IndexError:\n        raise NoSocket(\"There are no clients.\")\n    socket.send_and_broadcast(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef broadcast_channel(message, channel):\n    try:\n        socket = CLIENTS[CHANNELS.get(channel, [])[0]][1]\n    except (IndexError, KeyError):\n        raise NoSocket(\"There are no clients on the channel: \" + channel)\n    socket.send_and_broadcast_channel(message, channel)", "response": "Broadcasts a message to the given channel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a log message similar to gevent s pywsgi request logging.", "response": "def format_log(request, message_type, message):\n    \"\"\"\n    Formats a log message similar to gevent's pywsgi request logging.\n    \"\"\"\n    from django_socketio.settings import MESSAGE_LOG_FORMAT\n    if MESSAGE_LOG_FORMAT is None:\n        return None\n    now = datetime.now().replace(microsecond=0)\n    args = dict(request.META, TYPE=message_type, MESSAGE=message, TIME=now)\n    return (MESSAGE_LOG_FORMAT % args) + \"\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_handler(self, *args, **options):\n        handler = WSGIHandler()\n        try:\n            from django.contrib.staticfiles.handlers import StaticFilesHandler\n        except ImportError:\n            return handler\n        use_static_handler = options.get('use_static_handler', True)\n        insecure_serving = options.get('insecure_serving', False)\n        if (settings.DEBUG and use_static_handler or\n                (use_static_handler and insecure_serving)):\n            handler = StaticFilesHandler(handler)\n        return handler", "response": "Returns the WSGIHandler object that will handle the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(self, request, socket, context, *args):\n        for handler, pattern in self.handlers:\n            no_channel = not pattern and not socket.channels\n            if self.name.endswith(\"subscribe\") and pattern:\n                matches = [pattern.match(args[0])]\n            else:\n                matches = [pattern.match(c) for c in socket.channels if pattern]\n            if no_channel or filter(None, matches):\n                handler(request, socket, context, *args)", "response": "This method is called by the event dispatch method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(request):\n    name = request.POST.get(\"name\")\n    if name:\n        room, created = ChatRoom.objects.get_or_create(name=name)\n        return redirect(room)\n    return redirect(rooms)", "response": "Handles the Add room form on the homepage and the new room."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_outer_solar_system(sim):\n    Gfac = 1./0.01720209895       # Gaussian constant \n    if sim.G is not None:\n        Gfac *= math.sqrt(sim.G)\n\n    sim.add( m=1.00000597682, x=-4.06428567034226e-3, y=-6.08813756435987e-3, z=-1.66162304225834e-6, vx=+6.69048890636161e-6*Gfac, vy=-6.33922479583593e-6*Gfac, vz=-3.13202145590767e-9*Gfac )  # Sun\n    sim.add( m=1./1047.355,   x=+3.40546614227466e+0, y=+3.62978190075864e+0, z=+3.42386261766577e-2, vx=-5.59797969310664e-3*Gfac, vy=+5.51815399480116e-3*Gfac, vz=-2.66711392865591e-6*Gfac )  # Jupiter\n    sim.add( m=1./3501.6,     x=+6.60801554403466e+0, y=+6.38084674585064e+0, z=-1.36145963724542e-1, vx=-4.17354020307064e-3*Gfac, vy=+3.99723751748116e-3*Gfac, vz=+1.67206320571441e-5*Gfac )  # Saturn\n    sim.add( m=1./22869.,     x=+1.11636331405597e+1, y=+1.60373479057256e+1, z=+3.61783279369958e-1, vx=-3.25884806151064e-3*Gfac, vy=+2.06438412905916e-3*Gfac, vz=-2.17699042180559e-5*Gfac )  # Uranus\n    sim.add( m=1./19314.,     x=-3.01777243405203e+1, y=+1.91155314998064e+0, z=-1.53887595621042e-1, vx=-2.17471785045538e-4*Gfac, vy=-3.11361111025884e-3*Gfac, vz=+3.58344705491441e-5*Gfac )  # Neptune\n    sim.add( m=7.4074074e-09, x=-2.13858977531573e+1, y=+3.20719104739886e+1, z=+2.49245689556096e+0, vx=-1.76936577252484e-3*Gfac, vy=-2.06720938381724e-3*Gfac, vz=+6.58091931493844e-4*Gfac )", "response": "Add the planet of the outer Solar System as a test problem."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef OrbitPlot(sim, figsize=None, lim=None, limz=None, Narc=100, unitlabel=None, color=False, periastron=False, trails=True, show_orbit=True, lw=1., glow=False, slices=False, plotparticles=[], primary=None, fancy=False):\n    try:\n        import matplotlib.pyplot as plt\n        from matplotlib import gridspec\n        import numpy as np\n    except:\n        raise ImportError(\"Error importing matplotlib and/or numpy. Plotting functions not available. If running from within a jupyter notebook, try calling '%matplotlib inline' beforehand.\")\n    if unitlabel is not None:\n        unitlabel = \" \" + unitlabel\n    else:\n        unitlabel = \"\"\n    if slices:\n        if figsize is None:\n            figsize = (8,8)\n        fig, ax = plt.subplots(2, 2, figsize=figsize)\n        gs = gridspec.GridSpec(2, 2, width_ratios=[3., 2.], height_ratios=[2.,3.],wspace=0., hspace=0.) \n        ax0=plt.subplot(gs[0])\n        ax2=plt.subplot(gs[2])\n        ax3=plt.subplot(gs[3])\n        OrbitPlotOneSlice(sim, ax2, lim=lim, Narc=Narc, color=color, periastron=periastron, trails=trails, show_orbit=show_orbit, lw=lw, axes=\"xy\",fancy=fancy, plotparticles=plotparticles, primary=primary, glow=glow)\n        OrbitPlotOneSlice(sim, ax3, lim=lim, limz=limz, Narc=Narc, color=color, periastron=periastron, trails=trails, show_orbit=show_orbit, lw=lw,fancy=fancy, axes=\"zy\", plotparticles=plotparticles, primary=primary, glow=glow)\n        OrbitPlotOneSlice(sim, ax0, lim=lim, limz=limz, Narc=Narc, color=color, periastron=periastron, trails=trails, show_orbit=show_orbit, lw=lw,fancy=fancy, axes=\"xz\", plotparticles=plotparticles, primary=primary, glow=glow)\n        ax2.set_xlabel(\"x\"+unitlabel)\n        ax2.set_ylabel(\"y\"+unitlabel)\n      \n        plt.setp(ax0.get_xticklabels(), visible=False)\n        ax0.set_ylabel(\"z\"+unitlabel)\n        \n        ax3.set_xlabel(\"z\"+unitlabel)\n        plt.setp(ax3.get_yticklabels(), visible=False)\n    else:\n        if figsize is None:\n            figsize = (5,5)\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n        ax.set_xlabel(\"x\"+unitlabel)\n        ax.set_ylabel(\"y\"+unitlabel)\n        OrbitPlotOneSlice(sim, ax, lim=lim, Narc=Narc, color=color, periastron=periastron, trails=trails, show_orbit=show_orbit, lw=lw,fancy=fancy, plotparticles=plotparticles, primary=primary, glow=glow)\n    return fig", "response": "Plots the orbit of the current system."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_color(color):\n\n    if isinstance(color, tuple) and len(color) == 3: # already a tuple of RGB values\n        return color\n\n    try:\n        import matplotlib.colors as mplcolors\n    except:\n        raise ImportError(\"Error importing matplotlib. If running from within a jupyter notebook, try calling '%matplotlib inline' beforehand.\")\n   \n    try:\n        hexcolor = mplcolors.cnames[color]\n    except KeyError:\n        raise AttributeError(\"Color not recognized in matplotlib.\")\n\n    hexcolor = hexcolor.lstrip('#')\n    lv = len(hexcolor)\n    return tuple(int(hexcolor[i:i + lv // 3], 16)/255. for i in range(0, lv, lv // 3))", "response": "Returns a tuple of RGB values for the given color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fading_line(x, y, color='black', alpha_initial=1., alpha_final=0., glow=False, **kwargs):\n    try:\n        from matplotlib.collections import LineCollection\n        from matplotlib.colors import LinearSegmentedColormap\n        import numpy as np\n    except:\n        raise ImportError(\"Error importing matplotlib and/or numpy. Plotting functions not available. If running from within a jupyter notebook, try calling '%matplotlib inline' beforehand.\")\n\n\n    if glow:\n        glow = False\n        kwargs[\"lw\"] = 1\n        fl1 = fading_line(x, y, color, alpha_initial, alpha_final, glow=False, **kwargs)\n        kwargs[\"lw\"] = 2\n        alpha_initial *= 0.5\n        alpha_final *= 0.5\n        fl2 = fading_line(x, y, color, alpha_initial, alpha_final, glow=False, **kwargs)\n        kwargs[\"lw\"] = 6\n        alpha_initial *= 0.5\n        alpha_final *= 0.5\n        fl3 = fading_line(x, y, color, alpha_initial, alpha_final, glow=False, **kwargs)\n        return [fl3,fl2,fl1]\n\n    color = get_color(color)\n    cdict = {'red': ((0.,color[0],color[0]),(1.,color[0],color[0])),\n             'green': ((0.,color[1],color[1]),(1.,color[1],color[1])),\n             'blue': ((0.,color[2],color[2]),(1.,color[2],color[2])),\n             'alpha': ((0.,alpha_initial, alpha_initial), (1., alpha_final, alpha_final))}\n    \n    Npts = len(x)\n    if len(y) != Npts:\n        raise AttributeError(\"x and y must have same dimension.\")\n   \n    segments = np.zeros((Npts-1,2,2))\n    segments[0][0] = [x[0], y[0]]\n    for i in range(1,Npts-1):\n        pt = [x[i], y[i]]\n        segments[i-1][1] = pt\n        segments[i][0] = pt \n    segments[-1][1] = [x[-1], y[-1]]\n\n    individual_cm = LinearSegmentedColormap('indv1', cdict)\n    lc = LineCollection(segments, cmap=individual_cm, **kwargs)\n    lc.set_array(np.linspace(0.,1.,len(segments)))\n    return lc", "response": "Returns a matplotlib LineCollection connecting the points x and y lists with a single color and alpha varying from alpha_initial to alpha_final along the line."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _getSnapshotIndex(self, t):\n        if t>self.tmax or t<self.tmin:\n            raise ValueError(\"Requested time outside of baseline stored in binary file.\")\n        # Bisection method\n        l = 0\n        r = len(self)\n        while True:\n            bi = l+(r-l)//2\n            if self.t[bi]>t:\n                r = bi\n            else:\n                l = bi\n            if r-1<=l:\n                bi = l\n                break\n        return bi, self.t[bi]", "response": "Return the index for the snapshot just before t"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getSimulation(self, t, mode='snapshot', keep_unsynchronized=1):\n        if mode not in ['snapshot', 'close', 'exact']:\n            raise AttributeError(\"Unknown mode.\")\n\n        bi, bt = self._getSnapshotIndex(t)\n        sim = Simulation()\n        w = c_int(0)\n        clibrebound.reb_create_simulation_from_simulationarchive_with_messages(byref(sim),byref(self),bi,byref(w))\n\n        # Restore function pointers and any additional setup required by the user/reboundx provided functions\n        if self.setup:\n            self.setup(sim, *self.setup_args)\n        if self.rebxfilename:\n            import reboundx\n            rebx = reboundx.Extras.from_file(sim, self.rebxfilename)\n\n        if mode=='snapshot':\n            if sim.integrator==\"whfast\" and sim.ri_whfast.safe_mode == 1:\n                keep_unsynchronized = 0\n            sim.ri_whfast.keep_unsynchronized = keep_unsynchronized\n            sim.integrator_synchronize()\n            return sim\n        else:\n            if mode=='exact':\n                keep_unsynchronized==0\n            if sim.integrator==\"whfast\" and sim.ri_whfast.safe_mode == 1:\n                keep_unsynchronized = 0\n\n            sim.ri_whfast.keep_unsynchronized = keep_unsynchronized\n            exact_finish_time = 1 if mode=='exact' else 0\n            sim.integrate(t,exact_finish_time=exact_finish_time)\n                \n            return sim", "response": "This function returns a simulation object at the requested time t."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getSimulations(self, times, **kwargs):\n        for t in times:\n            yield self.getSimulation(t, **kwargs)", "response": "A generator to quickly access many simulations. \n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a deep copy of the particle.", "response": "def copy(self):\n        \"\"\"\n        Returns a deep copy of the particle. The particle is not added to any simulation by default.\n        \"\"\"\n        np = Particle()\n        memmove(byref(np), byref(self), sizeof(self))\n        return np"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_orbit(self, primary=None, G=None):\n        if not self._sim:\n            # Particle not in a simulation\n            if primary is None:\n                raise ValueError(\"Particle does not belong to any simulation and no primary given. Cannot calculate orbit.\")\n            if G is None:\n                raise ValueError(\"Particle does not belong to any simulation and G not given. Cannot calculate orbit.\")\n            else:\n                G = c_double(G)\n        else:\n            # First check whether this is particles[0]\n            clibrebound.reb_get_particle_index.restype = c_int\n            index = clibrebound.reb_get_particle_index(byref(self)) # first check this isn't particles[0]\n            if index == 0 and primary is None:\n                raise ValueError(\"Orbital elements for particle[0] not implemented unless primary is provided\")\n\n            if primary is None:    # Use default, i.e., Jacobi coordinates\n                clibrebound.reb_get_jacobi_com.restype = Particle   # now return jacobi center of mass\n                primary = clibrebound.reb_get_jacobi_com(byref(self))\n            G = c_double(self._sim.contents.G)\n        \n        err = c_int()\n        clibrebound.reb_tools_particle_to_orbit_err.restype = rebound.Orbit\n        o = clibrebound.reb_tools_particle_to_orbit_err(G, self, primary, byref(err))\n\n        if err.value == 1:\n            raise ValueError(\"Primary has no mass.\")\n        if err.value == 2:\n            raise ValueError(\"Particle and primary positions are the same.\")\n\n        return o", "response": "Calculates the keplerian orbital elements for the particle with the specified primary and G."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a nested list of xyz positions along the osculating orbit of the particle.", "response": "def sample_orbit(self, Npts=100, primary=None, trailing=True, timespan=None, useTrueAnomaly=True):\n        \"\"\"\n        Returns a nested list of xyz positions along the osculating orbit of the particle. \n        If primary is not passed, returns xyz positions along the Jacobi osculating orbit\n        (with mu = G*Minc, where Minc is the total mass from index 0 to the particle's index, inclusive). \n\n        Parameters\n        ----------\n        Npts    : int, optional  \n            Number of points along the orbit to return  (default: 100)\n        primary : rebound.Particle, optional\n            Primary to use for the osculating orbit (default: Jacobi center of mass)\n        trailing: bool, optional\n            Whether to return points stepping backwards in time (True) or forwards (False). (default: True)\n        timespan: float, optional    \n            Return points (for the osculating orbit) from the current position to timespan (forwards or backwards in time depending on trailing keyword). \n            Defaults to the orbital period for bound orbits, and to the rough time it takes the orbit to move by the current distance from the primary for a hyperbolic orbit. Implementation currently only supports this option if useTrueAnomaly=False.\n        useTrueAnomaly: bool, optional\n            Will sample equally spaced points in true anomaly if True, otherwise in mean anomaly.\n            Latter might be better for hyperbolic orbits, where true anomaly can stay near the limiting value for a long time, and then switch abruptly at pericenter. (Default: True)\n        \"\"\"\n        pts = []\n        if primary is None:\n            primary = self.jacobi_com\n        o = self.calculate_orbit(primary=primary)\n\n        if timespan is None:\n            if o.a < 0.: # hyperbolic orbit\n                timespan = 2*math.pi*o.d/o.v # rough time to cross display box\n            else:\n                timespan = o.P\n        \n        lim_phase = abs(o.n)*timespan # n is negative for hyperbolic orbits\n\n        if trailing is True:\n            lim_phase *= -1 # sample phase backwards from current value\n        phase = [lim_phase*i/(Npts-1) for i in range(Npts)]\n\n        for i,ph in enumerate(phase):\n            if useTrueAnomaly is True:\n                newp = Particle(a=o.a, f=o.f+ph, inc=o.inc, omega=o.omega, Omega=o.Omega, e=o.e, m=self.m, primary=primary, simulation=self._sim.contents)\n            else: \n                newp = Particle(a=o.a, M=o.M+ph, inc=o.inc, omega=o.omega, Omega=o.Omega, e=o.e, m=self.m, primary=primary, simulation=self._sim.contents)\n\n            pts.append(newp.xyz)\n        \n        return pts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef refresh(self, simp=None, isauto=0):\n\n        if simp==None:\n            simp = self.simp\n        if self.autorefresh==0 and isauto==1:\n            return\n        sim = simp.contents\n        size_changed = clibrebound.reb_display_copy_data(simp)\n        clibrebound.reb_display_prepare_data(simp,c_int(self.orbits))\n        if sim.N>0:\n            self.particle_data = (c_char * (4*7*sim.N)).from_address(sim.display_data.contents.particle_data).raw\n            if self.orbits:\n                self.orbit_data = (c_char * (4*9*(sim.N-1))).from_address(sim.display_data.contents.orbit_data).raw\n        if size_changed:\n            #TODO: Implement better GPU size change\n            pass\n        if self.useroverlay==True:\n            self.overlay = \"REBOUND (%s), N=%d, t=%g\"%(sim.integrator,sim.N,sim.t)\n        elif self.useroverlay==None or self.useroverlay==False:\n            self.overlay = \"\"\n        else:\n            self.overlay = self.useroverlay + \", N=%d, t=%g\"%(sim.N,sim.t)\n        self.N = sim.N\n        self.t = sim.t\n        self.count += 1", "response": "Refreshes a single entry in the hierarchy of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake one or more screenshots of the widget and save the images to a file.", "response": "def takeScreenshot(self, times=None, prefix=\"./screenshot\", resetCounter=False, archive=None,mode=\"snapshot\"):\n        \"\"\"\n        Take one or more screenshots of the widget and save the images to a file. \n        The images can be used to create a video.\n\n        This function cannot be called multiple times within one cell.\n\n        Note: this is a new feature and might not work on all systems.\n        It was tested on python 2.7.10 and 3.5.2 on MacOSX.\n        \n        Parameters\n        ----------\n        times : (float, list), optional\n            If this argument is not given a screenshot of the widget will be made \n            as it is (without integrating the simulation). If a float is given, then the\n            simulation will be integrated to that time and then a screenshot will \n            be taken. If a list of floats is given, the simulation will be integrated\n            to each time specified in the array. A separate screenshot for \n            each time will be saved.\n        prefix : (str), optional\n            This string will be part of the output filename for each image.\n            Follow by a five digit integer and the suffix .png. By default the\n            prefix is './screenshot' which outputs images in the current\n            directory with the filnames screenshot00000.png, screenshot00001.png...\n            Note that the prefix can include a directory.\n        resetCounter : (bool), optional\n            Resets the output counter to 0. \n        archive : (rebound.SimulationArchive), optional\n            Use a REBOUND SimulationArchive. Thus, instead of integratating the \n            Simulation from the current time, it will use the SimulationArchive\n            to load a snapshot. See examples for usage.\n        mode : (string), optional\n            Mode to use when querying the SimulationArchive. See SimulationArchive\n            documentation for details. By default the value is \"snapshot\".\n\n        Examples\n        --------\n\n        First, create a simulation and widget. All of the following can go in \n        one cell.\n\n        >>> sim = rebound.Simulation()\n        >>> sim.add(m=1.)\n        >>> sim.add(m=1.e-3,x=1.,vy=1.)\n        >>> w = sim.getWidget()\n        >>> w\n\n        The widget should show up. To take a screenshot, simply call \n\n        >>> w.takeScreenshot()\n\n        A new file with the name screenshot00000.png will appear in the \n        current directory. \n\n        Note that the takeScreenshot command needs to be in a separate cell, \n        i.e. after you see the widget. \n\n        You can pass an array of times to the function. This allows you to \n        take multiple screenshots, for example to create a movie, \n\n        >>> times = [0,10,100]\n        >>> w.takeScreenshot(times)\n\n        \"\"\"\n        self.archive = archive\n        if resetCounter:\n            self.screenshotcountall = 0\n        self.screenshotprefix = prefix\n        self.screenshotcount = 0\n        self.overlay = \"REBOUND\"\n        self.screenshot = \"\" \n        if archive is None:\n            if times is None:\n                times = self.simp.contents.t\n            try:\n                # List\n                len(times)\n            except:\n                # Float:\n                times = [times]\n            self.times = times\n            self.observe(savescreenshot,names=\"screenshot\")\n            self.simp.contents.integrate(times[0])\n            self.screenshotcount += 1 # triggers first screenshot\n        else:\n            if times is None:\n                raise ValueError(\"Need times argument for archive mode.\")\n            try:\n                len(times)\n            except:\n                raise ValueError(\"Need a list of times for archive mode.\")\n            self.times = times\n            self.mode = mode\n            self.observe(savescreenshot,names=\"screenshot\")\n            sim = archive.getSimulation(times[0],mode=mode)\n            self.refresh(pointer(sim))\n            self.screenshotcount += 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coordinates(self):\n        i = self._coordinates\n        for name, _i in COORDINATES.items():\n            if i==_i:\n                return name\n        return i", "response": "Get or set the internal coordinate system name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_archive(cls, filename,snapshot=-1):\n        warnings.warn( \"rebound.Simulation.from_archive(filename,snapshot) is deprecated and will be removed in the future. Use rebound.Simulation(filename,snapshot) instead\", FutureWarning)\n        return cls(filename=filename,snapshot=snapshot)", "response": "Create a new instance of the class from a archive file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self):\n        w = c_int(0)\n        sim = Simulation()\n        clibrebound._reb_copy_simulation_with_messages(byref(sim),byref(self),byref(w))\n        for majorerror, value, message in BINARY_WARNINGS:\n            if w.value & value:\n                if majorerror:\n                    raise RuntimeError(message)\n                else:  \n                    # Just a warning\n                    warnings.warn(message, RuntimeWarning)\n        return sim", "response": "Returns a deep copy of this simulation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps function that returns a new widget attached to this simulation.", "response": "def getWidget(self,**kwargs):\n        \"\"\"\n        Wrapper function that returns a new widget attached to this simulation.\n\n        Widgets provide real-time 3D visualizations from within an Jupyter notebook.\n        See the Widget class for more details on the possible arguments.\n        \n        \n        Arguments\n        ---------\n        All arguments passed to this wrapper function will be passed to /Widget class.\n        \n        Returns\n        ------- \n        A rebound.Widget object.\n        \n        Examples\n        --------\n\n        >>> sim = rebound.Simulation()\n        >>> sim.add(m=1.)\n        >>> sim.add(m=1.e-3,x=1.,vy=1.)\n        >>> sim.getWidget()\n\n        \"\"\"\n        from .widget import Widget # ondemand\n        from ipywidgets import DOMWidget\n        from IPython.display import display, HTML\n        if not hasattr(self, '_widgets'):\n            self._widgets = []\n            def display_heartbeat(simp):\n                for w in self._widgets:\n                    w.refresh(simp,isauto=1)\n            self.visualization = VISUALIZATIONS[\"webgl\"] \n            clibrebound.reb_display_init_data(byref(self));\n            self._dhbf = AFF(display_heartbeat)\n            self._display_heartbeat = self._dhbf\n            display(HTML(Widget.getClientCode())) # HACK! Javascript should go into custom.js\n        newWidget = Widget(self,**kwargs)\n        self._widgets.append(newWidget)\n        newWidget.refresh(isauto=0)\n        return newWidget"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refreshWidgets(self):\n        if hasattr(self, '_widgets'):\n            for w in self._widgets:\n                w.refresh(isauto=0)\n        else:\n            raise RuntimeError(\"No widgets found\")", "response": "This function will refresh all the widgets attached to this simulation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simulationarchive_snapshot(self, filename):\n        clibrebound.reb_simulationarchive_snapshot(byref(self), c_char_p(filename.encode(\"ascii\")))", "response": "Take a snapshot and save it to a SimulationArchive file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting a summary of the current status of the simulation.", "response": "def status(self):\n        \"\"\" \n        Prints a summary of the current status \n        of the simulation.\n        \"\"\"\n        from rebound import __version__, __build__\n        s= \"\"\n        s += \"---------------------------------\\n\"\n        s += \"REBOUND version:     \\t%s\\n\" %__version__\n        s += \"REBOUND built on:    \\t%s\\n\" %__build__\n        s += \"Number of particles: \\t%d\\n\" %self.N       \n        s += \"Selected integrator: \\t\" + self.integrator + \"\\n\"       \n        s += \"Simulation time:     \\t%.16e\\n\" %self.t\n        s += \"Current timestep:    \\t%f\\n\" %self.dt\n        if self.N>0:\n            s += \"---------------------------------\\n\"\n            for p in self.particles:\n                s += str(p) + \"\\n\"\n        s += \"---------------------------------\"\n        print(s)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef integrator(self):\n        i = self._integrator\n        for name, _i in INTEGRATORS.items():\n            if i==_i:\n                return name\n        return i", "response": "Get or set the intergrator module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef boundary(self):\n        i = self._boundary\n        for name, _i in BOUNDARIES.items():\n            if i==_i:\n                return name\n        return i", "response": "Get or set the boundary module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget or set the gravity module.", "response": "def gravity(self):\n        \"\"\"\n        Get or set the gravity module.\n\n        Available gravity modules are:\n\n        - ``'none'`` \n        - ``'basic'`` (default)\n        - ``'compensated'``\n        - ``'tree'``\n        \n        Check the online documentation for a full description of each of the modules. \n        \"\"\"\n        i = self._gravity\n        for name, _i in GRAVITIES.items():\n            if i==_i:\n                return name\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collision(self):\n        i = self._collision\n        for name, _i in COLLISIONS.items():\n            if i==_i:\n                return name\n        return i", "response": "Get or set the collision module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of the units for length time and mass.", "response": "def units(self):\n        \"\"\"\n        Tuple of the units for length, time and mass.  Can be set in any order, and strings are not case-sensitive.  See ipython_examples/Units.ipynb for more information.  You can check the units' exact values and add Additional units in rebound/rebound/units.py.  Units should be set before adding particles to the simulation (will give error otherwise).\n\n        Currently supported Units \n        -------------------------\n\n        Times:\n        Hr          : Hours\n        Yr          : Julian years\n        Jyr         : Julian years\n        Sidereal_yr : Sidereal year\n        Yr2pi       : Year divided by 2pi, with year defined as orbital period of planet at 1AU around 1Msun star\n        Kyr         : Kiloyears (Julian)\n        Myr         : Megayears (Julian)\n        Gyr         : Gigayears (Julian)\n\n        Lengths:\n        M           : Meters\n        Cm          : Centimeters\n        Km          : Kilometers\n        AU          : Astronomical Units\n\n        Masses:\n        Kg          : Kilograms\n        Msun        : Solar masses\n        Mmercury    : Mercury masses\n        Mvenus      : Venus masses\n        Mearth      : Earth masses\n        Mmars       : Mars masses\n        Mjupiter    : Jupiter masses\n        Msaturn     : Saturn masses\n        Muranus     : Neptune masses\n        Mpluto      : Pluto masses\n        \n        Examples\n        --------\n        \n        >>> sim = rebound.Simulation()\n        >>> sim.units = ('yr', 'AU', 'Msun')\n\n        \"\"\"\n        \n        return {'length':hash_to_unit(self.python_unit_l), 'mass':hash_to_unit(self.python_unit_m), 'time':hash_to_unit(self.python_unit_t)}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the units of the particles in the simulation to the corresponding units of time length and mass.", "response": "def convert_particle_units(self, *args): \n        \"\"\"\n        Will convert the units for the simulation (i.e. convert G) as well as the particles' cartesian elements.\n        Must have set sim.units ahead of calling this function so REBOUND knows what units to convert from.\n\n        Parameters\n        ----------\n        3 strings corresponding to units of time, length and mass.  Can be in any order and aren't case sensitive.        You can add new units to rebound/rebound/units.py\n        \"\"\"\n        if self.python_unit_l == 0 or self.python_unit_m == 0 or self.python_unit_t == 0:\n            raise AttributeError(\"Must set sim.units before calling convert_particle_units in order to know what units to convert from.\")\n        new_l, new_t, new_m = check_units(args)\n        for p in self.particles:\n            units_convert_particle(p, hash_to_unit(self.python_unit_l), hash_to_unit(self.python_unit_t), hash_to_unit(self.python_unit_m), new_l, new_t, new_m)\n        self.update_units((new_l, new_t, new_m))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_variation(self,order=1,first_order=None, first_order_2=None, testparticle=-1):\n        cur_var_config_N = self.var_config_N\n        if order==1:\n            index = clibrebound.reb_add_var_1st_order(byref(self),c_int(testparticle))\n        elif order==2:\n            if first_order is None:\n                raise AttributeError(\"Please specify corresponding first order variational equations when initializing second order variational equations.\")\n            if first_order_2 is None:\n                first_order_2 = first_order\n            index = clibrebound.reb_add_var_2nd_order(byref(self),c_int(testparticle),c_int(first_order.index),c_int(first_order_2.index))\n        else:\n            raise AttributeError(\"Only variational equations of first and second order are supported.\")\n\n        # Need a copy because location of original might shift if more variations added\n        s = Variation.from_buffer_copy(self.var_config[cur_var_config_N])\n\n        return s", "response": "This function adds a set of real and variational particles to the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_megno(self):\n        if self._calculate_megno==0:\n            raise RuntimeError(\"MEGNO cannot be calculated. Make sure to call init_megno() after adding all particles but before integrating the simulation.\")\n\n        clibrebound.reb_tools_calculate_megno.restype = c_double\n        return clibrebound.reb_tools_calculate_megno(byref(self))", "response": "Calculates the MEGNO value of the current MEGNO."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_lyapunov(self):\n        if self._calculate_megno==0:\n            raise RuntimeError(\"Lyapunov Characteristic Number cannot be calculated. Make sure to call init_megno() after adding all particles but before integrating the simulation.\")\n\n        clibrebound.reb_tools_calculate_lyapunov.restype = c_double\n        return clibrebound.reb_tools_calculate_lyapunov(byref(self))", "response": "Calculate the current Lyapunov Characteristic Number."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a particle to the REBOUND simulation box.", "response": "def add(self, particle=None, **kwargs):   \n        \"\"\"\n        Adds a particle to REBOUND. Accepts one of the following:\n\n        1) A single Particle structure.\n        2) The particle's mass and a set of cartesian coordinates: m,x,y,z,vx,vy,vz.\n        3) The primary as a Particle structure, the particle's mass and a set of orbital elements: primary,m,a,anom,e,omega,inv,Omega,MEAN (see :class:`.Orbit` for the definition of orbital elements).\n        4) A name of an object (uses NASA Horizons to look up coordinates)\n        5) A list of particles or names.\n        \"\"\"\n        if particle is not None:\n            if isinstance(particle, Particle):\n                if (self.gravity == \"tree\" or self.collision == \"tree\") and self.root_size <=0.:\n                    raise ValueError(\"The tree code for gravity and/or collision detection has been selected. However, the simulation box has not been configured yet. You cannot add particles until the the simulation box has a finite size.\")\n\n                clibrebound.reb_add(byref(self), particle)\n            elif isinstance(particle, list):\n                for p in particle:\n                    self.add(p, **kwargs)\n            elif isinstance(particle,str):\n                if self.python_unit_l == 0 or self.python_unit_m == 0 or self.python_unit_t == 0:\n                    self.units = ('AU', 'yr2pi', 'Msun')\n                self.add(horizons.getParticle(particle, **kwargs), hash=particle)\n                units_convert_particle(self.particles[-1], 'km', 's', 'kg', hash_to_unit(self.python_unit_l), hash_to_unit(self.python_unit_t), hash_to_unit(self.python_unit_m))\n            else: \n                raise ValueError(\"Argument passed to add() not supported.\")\n        else: \n            self.add(Particle(simulation=self, **kwargs))\n        if hasattr(self, '_widgets'):\n            self._display_heartbeat(pointer(self))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, index=None, hash=None, keepSorted=True):\n        if index is not None:\n            clibrebound.reb_remove(byref(self), index, keepSorted)\n        if hash is not None:\n            hash_types = c_uint32, c_uint, c_ulong\n            PY3 = sys.version_info[0] == 3\n            if PY3:\n                string_types = str,\n                int_types = int,\n            else:\n                string_types = basestring,\n                int_types = int, long\n            if isinstance(hash, string_types):\n                clibrebound.reb_remove_by_hash(byref(self), rebhash(hash), keepSorted)\n            elif isinstance(hash, int_types):\n                clibrebound.reb_remove_by_hash(byref(self), c_uint32(hash), keepSorted)\n            elif isinstance(hash, hash_types):\n                clibrebound.reb_remove_by_hash(byref(self), hash, keepSorted)\n        if hasattr(self, '_widgets'):\n            self._display_heartbeat(pointer(self))\n\n        self.process_messages()", "response": "This is a convenience method that removes a particle from the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef particles_ascii(self, prec=8):\n        s = \"\"\n        for p in self.particles:\n            s += ((\"%%.%de \"%prec) * 8)%(p.m, p.r, p.x, p.y, p.z, p.vx, p.vy, p.vz) + \"\\n\"\n        if len(s):\n            s = s[:-1]\n        return s", "response": "Returns an ASCII string with all particles masses radii positions and velocities."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding particles from an ASCII string.", "response": "def add_particles_ascii(self, s):\n        \"\"\"\n        Adds particles from an ASCII string. \n\n        Parameters\n        ----------\n        s : string\n            One particle per line. Each line should include particle's mass, radius, position and velocity.\n        \"\"\"\n        for l in s.split(\"\\n\"):\n            r = l.split()\n            if len(r):\n                try:\n                    r = [float(x) for x in r]\n                    p = Particle(simulation=self, m=r[0], r=r[1], x=r[2], y=r[3], z=r[4], vx=r[5], vy=r[6], vz=r[7])\n                    self.add(p)\n                except:\n                    raise AttributeError(\"Each line requires 8 floats corresponding to mass, radius, position (x,y,z) and velocity (x,y,z).\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_orbits(self, primary=None, jacobi_masses=False, heliocentric=None, barycentric=None):\n        orbits = []\n       \n        if heliocentric is not None or barycentric is not None:\n            raise AttributeError('heliocentric and barycentric keywords in calculate_orbits are deprecated. Pass primary keyword instead (sim.particles[0] for heliocentric and sim.calculate_com() for barycentric)')\n\n        if primary is None:\n            jacobi = True\n            primary = self.particles[0]\n            clibrebound.reb_get_com_of_pair.restype = Particle\n        else:\n            jacobi = False\n\n        for p in self.particles[1:self.N_real]:\n            if jacobi_masses is True:\n                interior_mass = primary.m\n                # orbit conversion uses mu=G*(p.m+primary.m) so set prim.m=Mjac-m so mu=G*Mjac\n                primary.m = self.particles[0].m*(p.m + interior_mass)/interior_mass - p.m\n                orbits.append(p.calculate_orbit(primary=primary))\n                primary.m = interior_mass # back to total mass of interior bodies to update com\n            else:\n                orbits.append(p.calculate_orbit(primary=primary))\n            if jacobi is True: # update com to include current particle for next iteration\n                primary = clibrebound.reb_get_com_of_pair(primary, p)\n\n        return orbits", "response": "Calculates the orbits of all variational particles in the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_com(self, first=0, last=None):\n        if last is None:\n            last = self.N_real\n        clibrebound.reb_get_com_range.restype = Particle\n        return clibrebound.reb_get_com_range(byref(self), c_int(first), c_int(last))", "response": "Calculates the center of momentum for all particles in the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serialize_particle_data(self,**kwargs):\n        N = self.N\n        possible_keys = [\"hash\",\"m\",\"r\",\"xyz\",\"vxvyvz\",\"xyzvxvyvz\"]\n        d = {x:None for x in possible_keys}\n        for k,v in kwargs.items():\n            if k in d:\n                if k == \"hash\":\n                    if v.dtype!= \"uint32\":\n                        raise AttributeError(\"Expected 'uint32' data type for '%s' array.\"%k)\n                    if v.size<N:\n                        raise AttributeError(\"Array '%s' is not large enough.\"%k)\n                    d[k] = v.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32))\n                else:\n                    if v.dtype!= \"float64\":\n                        raise AttributeError(\"Expected 'float64' data type for %s array.\"%k)\n                    if k in [\"xyz\", \"vxvyvz\"]:\n                        minsize = 3*N\n                    elif k in [\"xyzvxvyvz\"]:\n                        minsize = 6*N\n                    else:\n                        minsize = N\n                    if v.size<minsize:\n                        raise AttributeError(\"Array '%s' is not large enough.\"%k)\n                    d[k] = v.ctypes.data_as(ctypes.POINTER(ctypes.c_double))\n            else:\n                raise AttributeError(\"Only '%s' are currently supported attributes for serialization.\" % \"', '\".join(d.keys()))\n\n        clibrebound.reb_serialize_particle_data(byref(self), d[\"hash\"], d[\"m\"], d[\"r\"], d[\"xyz\"], d[\"vxvyvz\"], d[\"xyzvxvyvz\"])", "response": "This is a fast way to access serialized particle data via numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_energy(self):\n        clibrebound.reb_tools_energy.restype = c_double\n        return clibrebound.reb_tools_energy(byref(self))", "response": "Calculates the sum of potential and kinetic energy of all particles in the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_angular_momentum(self):\n        clibrebound.reb_tools_angular_momentum.restype = reb_vec3d\n        L = clibrebound.reb_tools_angular_momentum(byref(self))\n        return [L.x, L.y, L.z]", "response": "Calculate the total angular momentum of all particles in the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure_box(self, boxsize, root_nx=1, root_ny=1, root_nz=1):\n        clibrebound.reb_configure_box(byref(self), c_double(boxsize), c_int(root_nx), c_int(root_ny), c_int(root_nz))\n        return", "response": "This function initializes the simulation box."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure_ghostboxes(self, nghostx=0, nghosty=0, nghostz=0):\n        clibrebound.nghostx = c_int(nghostx)\n        clibrebound.nghosty = c_int(nghosty)\n        clibrebound.nghostz = c_int(nghostz)\n        return", "response": "This function initializes the ghost boxes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, filename):\n        clibrebound.reb_output_binary(byref(self), c_char_p(filename.encode(\"ascii\")))", "response": "Save the entire REBOUND simulation to a binary file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef integrate(self, tmax, exact_finish_time=1):\n        if debug.integrator_package ==\"REBOUND\":\n            self.exact_finish_time = c_int(exact_finish_time)\n            ret_value = clibrebound.reb_integrate(byref(self), c_double(tmax))\n            if ret_value == 1:\n                self.process_messages()\n                raise SimulationError(\"An error occured during the integration.\")\n            if ret_value == 2:\n                raise NoParticles(\"No more particles left in simulation.\")\n            if ret_value == 3:\n                raise Encounter(\"Two particles had a close encounter (d<exit_min_distance).\")\n            if ret_value == 4:\n                raise Escape(\"A particle escaped (r>exit_max_distance).\")\n            if ret_value == 5:\n                raise Escape(\"User caused exit. Simulation did not finish.\") # should not occur in python\n            if ret_value == 6:\n                raise KeyboardInterrupt\n            if ret_value == 7:\n                raise Collision(\"Two particles collided (d < r1+r2)\")\n        else:\n            debug.integrate_other_package(tmax,exact_finish_time)\n        self.process_messages()", "response": "Integrates the simulation to tmax."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef particles(self):\n        sim = self._sim.contents\n        ps = []\n        if self.testparticle>=0:\n            N = 1\n        else:\n            N = sim.N-sim.N_var \n        \n        ParticleList = Particle*N\n        ps = ParticleList.from_address(ctypes.addressof(sim._particles.contents)+self.index*ctypes.sizeof(Particle))\n        return ps", "response": "Returns a list of the variational particles in the set of variational equations in the current set of variational equations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a links attribute to the data that maps field names to URLs.", "response": "def merge_link_object(serializer, data, instance):\n    \"\"\"Add a 'links' attribute to the data that maps field names to URLs.\n\n    NOTE: This is the format that Ember Data supports, but alternative\n          implementations are possible to support other formats.\n    \"\"\"\n\n    link_object = {}\n\n    if not getattr(instance, 'pk', None):\n        # If instance doesn't have a `pk` field, we'll assume it doesn't\n        # have a canonical resource URL to hang a link off of.\n        # This generally only affectes Ephemeral Objects.\n        return data\n\n    link_fields = serializer.get_link_fields()\n    for name, field in six.iteritems(link_fields):\n        # For included fields, omit link if there's no data.\n        if name in data and not data[name]:\n            continue\n\n        link = getattr(field, 'link', None)\n        if link is None:\n            base_url = ''\n            if settings.ENABLE_HOST_RELATIVE_LINKS:\n                # if the resource isn't registered, this will default back to\n                # using resource-relative urls for links.\n                base_url = DynamicRouter.get_canonical_path(\n                    serializer.get_resource_key(),\n                    instance.pk\n                ) or ''\n            link = '%s%s/' % (base_url, name)\n        # Default to DREST-generated relation endpoints.\n        elif callable(link):\n            link = link(name, field, data, instance)\n\n        link_object[name] = link\n\n    if link_object:\n        data['links'] = link_object\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a post processor function to be run as the final step in serialization.", "response": "def register_post_processor(func):\n    \"\"\"\n    Register a post processor function to be run as the final step in\n    serialization. The data passed in will already have gone through the\n    sideloading processor.\n\n    Usage:\n        @register_post_processor\n        def my_post_processor(data):\n            # do stuff with `data`\n            return data\n    \"\"\"\n\n    global POST_PROCESSORS\n\n    key = func.__name__\n    POST_PROCESSORS[key] = func\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting request fields from the parent serializer.", "response": "def _get_request_fields_from_parent(self):\n        \"\"\"Get request fields from the parent serializer.\"\"\"\n        if not self.parent:\n            return None\n\n        if not getattr(self.parent, 'request_fields'):\n            return None\n\n        if not isinstance(self.parent.request_fields, dict):\n            return None\n\n        return self.parent.request_fields.get(self.field_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef determine_metadata(self, request, view):\n        metadata = super(\n            DynamicMetadata,\n            self).determine_metadata(\n            request,\n            view)\n        metadata['features'] = getattr(view, 'features', [])\n        if hasattr(view, 'get_serializer'):\n            serializer = view.get_serializer(dynamic=False)\n            if hasattr(serializer, 'get_name'):\n                metadata['resource_name'] = serializer.get_name()\n            if hasattr(serializer, 'get_plural_name'):\n                metadata['resource_name_plural'] = serializer.get_plural_name()\n        metadata['properties'] = self.get_serializer_info(serializer)\n        return metadata", "response": "Adds properties and features to the metadata response."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_field_info(self, field):\n        field_info = OrderedDict()\n        for attr in ('required', 'read_only', 'default', 'label'):\n            field_info[attr] = getattr(field, attr)\n        if field_info['default'] is empty:\n            field_info['default'] = None\n        if hasattr(field, 'immutable'):\n            field_info['immutable'] = field.immutable\n        field_info['nullable'] = field.allow_null\n        if hasattr(field, 'choices'):\n            field_info['choices'] = [\n                {\n                    'value': choice_value,\n                    'display_name': force_text(choice_name, strings_only=True)\n                }\n                for choice_value, choice_name in field.choices.items()\n            ]\n        many = False\n        if isinstance(field, DynamicRelationField):\n            field = field.serializer\n        if isinstance(field, ListSerializer):\n            field = field.child\n            many = True\n        if isinstance(field, ModelSerializer):\n            type = 'many' if many else 'one'\n            field_info['related_to'] = field.get_plural_name()\n        else:\n            type = self.label_lookup[field]\n\n        field_info['type'] = type\n        return field_info", "response": "Adds related_to and nullable to the metadata response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Django field given a model and field name.", "response": "def get_model_field(model, field_name):\n    \"\"\"Return a field given a model and field name.\n\n    Arguments:\n        model: a Django model\n        field_name: the name of a field\n\n    Returns:\n        A Django field if `field_name` is a valid field for `model`,\n            None otherwise.\n    \"\"\"\n    meta = model._meta\n    try:\n        if DJANGO19:\n            field = meta.get_field(field_name)\n        else:\n            field = meta.get_field_by_name(field_name)[0]\n        return field\n    except:\n        if DJANGO19:\n            related_objs = (\n                f for f in meta.get_fields()\n                if (f.one_to_many or f.one_to_one)\n                and f.auto_created and not f.concrete\n            )\n            related_m2m_objs = (\n                f for f in meta.get_fields(include_hidden=True)\n                if f.many_to_many and f.auto_created\n            )\n        else:\n            related_objs = meta.get_all_related_objects()\n            related_m2m_objs = meta.get_all_related_many_to_many_objects()\n\n        related_objects = {\n            o.get_accessor_name(): o\n            for o in chain(related_objs, related_m2m_objs)\n        }\n        if field_name in related_objects:\n            return related_objects[field_name]\n        else:\n            # check virtual fields (1.7)\n            if hasattr(meta, 'virtual_fields'):\n                for field in meta.virtual_fields:\n                    if field.name == field_name:\n                        return field\n\n            raise AttributeError(\n                '%s is not a valid field for %s' % (field_name, model)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_field_remote(model, field_name):\n    if not hasattr(model, '_meta'):\n        # ephemeral model with no metaclass\n        return False\n\n    model_field = get_model_field(model, field_name)\n    return isinstance(model_field, (ManyToManyField, RelatedObject))", "response": "Check whether a given model field is a remote field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resettable_cached_property(func):\n\n    def wrapper(self):\n        if not hasattr(self, '_resettable_cached_properties'):\n            self._resettable_cached_properties = {}\n        if func.__name__ not in self._resettable_cached_properties:\n            self._resettable_cached_properties[func.__name__] = func(self)\n        return self._resettable_cached_properties[func.__name__]\n\n    # Returns a property whose getter is the 'wrapper' function\n    return property(wrapper)", "response": "Decorator to add cached computed properties to an object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cacheable_object(cls):\n\n    def reset(self):\n        if hasattr(self, '_resettable_cached_properties'):\n            self._resettable_cached_properties = {}\n\n    cls.reset = reset\n    return cls", "response": "Decorator to add a reset method that clears data cached by\n    the @resettable_cached_property decorator. Technically this is a mixin..."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _settings_changed(self, *args, **kwargs):\n        setting, value = kwargs['setting'], kwargs['value']\n        if setting == self.name:\n            self._reload(value)", "response": "Handle changes to core settings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bind(self, *args, **kwargs):\n        if self.bound:  # Prevent double-binding\n            return\n        super(DynamicRelationField, self).bind(*args, **kwargs)\n        self.bound = True\n        parent_model = getattr(self.parent.Meta, 'model', None)\n\n        remote = is_field_remote(parent_model, self.source)\n\n        try:\n            model_field = get_model_field(parent_model, self.source)\n        except:\n            # model field may not be available for m2o fields with no\n            # related_name\n            model_field = None\n\n        # Infer `required` and `allow_null`\n        if 'required' not in self.kwargs and (\n                remote or (\n                    model_field and (\n                        model_field.has_default() or model_field.null\n                    )\n                )\n        ):\n            self.required = False\n        if 'allow_null' not in self.kwargs and getattr(\n                model_field, 'null', False\n        ):\n            self.allow_null = True\n\n        self.model_field = model_field", "response": "Bind to the parent serializer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef root_serializer(self):\n        if not self.parent:\n            # Don't cache, so that we'd recompute if parent is set.\n            return None\n\n        node = self\n        seen = set()\n        while True:\n            seen.add(node)\n            if getattr(node, 'parent', None):\n                node = node.parent\n                if node in seen:\n                    return None\n            else:\n                return node", "response": "Return the root serializer for the primary resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts any necessary attributes from parent serializer to propagate down to child serializer.", "response": "def _inherit_parent_kwargs(self, kwargs):\n        \"\"\"Extract any necessary attributes from parent serializer to\n        propagate down to child serializer.\n        \"\"\"\n\n        if not self.parent or not self._is_dynamic:\n            return kwargs\n\n        if 'request_fields' not in kwargs:\n            # If 'request_fields' isn't explicitly set, pull it from the\n            # parent serializer.\n            request_fields = self._get_request_fields_from_parent()\n            if request_fields is None:\n                # Default to 'id_only' for nested serializers.\n                request_fields = True\n            kwargs['request_fields'] = request_fields\n\n        if self.embed and kwargs.get('request_fields') is True:\n            # If 'embed' then make sure we fetch the full object.\n            kwargs['request_fields'] = {}\n\n        if hasattr(self.parent, 'sideloading'):\n            kwargs['sideloading'] = self.parent.sideloading\n\n        if hasattr(self.parent, 'debug'):\n            kwargs['debug'] = self.parent.debug\n\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_serializer(self, *args, **kwargs):\n        init_args = {\n            k: v for k, v in six.iteritems(self.kwargs)\n            if k in self.SERIALIZER_KWARGS\n        }\n\n        kwargs = self._inherit_parent_kwargs(kwargs)\n        init_args.update(kwargs)\n\n        if self.embed and self._is_dynamic:\n            init_args['embed'] = True\n\n        return self._get_cached_serializer(args, init_args)", "response": "Get an instance of the child serializer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_representation(self, instance):\n        serializer = self.serializer\n        model = serializer.get_model()\n        source = self.source\n\n        if not self.kwargs['many'] and serializer.id_only():\n            # attempt to optimize by reading the related ID directly\n            # from the current instance rather than from the related object\n            source_id = '%s_id' % source\n            # try the faster way first:\n            if hasattr(instance, source_id):\n                return getattr(instance, source_id)\n            elif model is not None:\n                # this is probably a one-to-one field, or a reverse related\n                # lookup, so let's look it up the slow way and let the\n                # serializer handle the id dereferencing\n                try:\n                    instance = getattr(instance, source)\n                except model.DoesNotExist:\n                    instance = None\n\n        # dereference ephemeral objects\n        if model is None:\n            instance = getattr(instance, source)\n\n        if instance is None:\n            return None\n\n        return serializer.to_representation(instance)", "response": "Represent the relationship either as an ID or object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_internal_value_single(self, data, serializer):\n        related_model = serializer.Meta.model\n        if isinstance(data, related_model):\n            return data\n        try:\n            instance = related_model.objects.get(pk=data)\n        except related_model.DoesNotExist:\n            raise ValidationError(\n                \"Invalid value for '%s': %s object with ID=%s not found\" %\n                (self.field_name, related_model.__name__, data)\n            )\n        return instance", "response": "Return the underlying object given the serialized form."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_internal_value(self, data):\n        if self.kwargs['many']:\n            serializer = self.serializer.child\n            if not isinstance(data, list):\n                raise ParseError(\"'%s' value must be a list\" % self.field_name)\n            return [\n                self.to_internal_value_single(\n                    instance,\n                    serializer\n                ) for instance in data\n            ]\n        return self.to_internal_value_single(data, self.serializer)", "response": "Return the underlying object ( s ) given the serialized form."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the class of the child serializer. Resolves string imports.", "response": "def serializer_class(self):\n        \"\"\"Get the class of the child serializer.\n\n        Resolves string imports.\n        \"\"\"\n        serializer_class = self._serializer_class\n        if not isinstance(serializer_class, six.string_types):\n            return serializer_class\n\n        parts = serializer_class.split('.')\n        module_path = '.'.join(parts[:-1])\n        if not module_path:\n            if getattr(self, 'parent', None) is None:\n                raise Exception(\n                    \"Can not load serializer '%s'\" % serializer_class +\n                    ' before binding or without specifying full path')\n\n            # try the module of the parent class\n            module_path = self.parent.__module__\n\n        module = importlib.import_module(module_path)\n        serializer_class = getattr(module, parts[-1])\n\n        self._serializer_class = serializer_class\n        return serializer_class"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_django_queryset(self):\n\n        prefetches = []\n        for field, fprefetch in self.prefetches.items():\n            has_query = hasattr(fprefetch, 'query')\n            qs = fprefetch.query.queryset if has_query else None\n            prefetches.append(\n                Prefetch(field, queryset=qs)\n            )\n\n        queryset = self.queryset\n        if prefetches:\n            queryset = queryset.prefetch_related(*prefetches)\n\n        return queryset", "response": "Return Django QuerySet with prefetches properly configured."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a value to the internal list.", "response": "def add(self, key, value):\n        \"\"\"\n        Method to accept a list of values and append to flat list.\n        QueryDict.appendlist(), if given a list, will append the list,\n        which creates nested lists. In most cases, we want to be able\n        to pass in a list (for convenience) but have it appended into\n        a flattened list.\n        TODO: Possibly throw an error if add() is used on a non-list param.\n        \"\"\"\n        if isinstance(value, list):\n            for val in value:\n                self.appendlist(key, val)\n        else:\n            self.appendlist(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding initialize_request method to convert request. GET to QueryParams and convert request. data to dict.", "response": "def initialize_request(self, request, *args, **kargs):\n        \"\"\"\n        Override DRF initialize_request() method to swap request.GET\n        (which is aliased by request.query_params) with a mutable instance\n        of QueryParams, and to convert request MergeDict to a subclass of dict\n        for consistency (MergeDict is not a subclass of dict)\n        \"\"\"\n\n        def handle_encodings(request):\n            \"\"\"\n            WSGIRequest does not support Unicode values in the query string.\n            WSGIRequest handling has a history of drifting behavior between\n            combinations of Python versions, Django versions and DRF versions.\n            Django changed its QUERY_STRING handling here:\n            https://goo.gl/WThXo6. DRF 3.4.7 changed its behavior here:\n            https://goo.gl/0ojIIO.\n            \"\"\"\n            try:\n                return QueryParams(request.GET)\n            except UnicodeEncodeError:\n                pass\n\n            s = request.environ.get('QUERY_STRING', '')\n            try:\n                s = s.encode('utf-8')\n            except UnicodeDecodeError:\n                pass\n            return QueryParams(s)\n\n        request.GET = handle_encodings(request)\n        request = super(WithDynamicViewSetMixin, self).initialize_request(\n            request, *args, **kargs\n        )\n\n        try:\n            # Django<1.9, DRF<3.2\n\n            # MergeDict doesn't have the same API as dict.\n            # Django has deprecated MergeDict and DRF is moving away from\n            # using it - thus, were comfortable replacing it with a QueryDict\n            # This will allow the data property to have normal dict methods.\n            from django.utils.datastructures import MergeDict\n            if isinstance(request._full_data, MergeDict):\n                data_as_dict = request.data.dicts[0]\n                for d in request.data.dicts[1:]:\n                    data_as_dict.update(d)\n                request._full_data = data_as_dict\n        except:\n            pass\n\n        return request"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_renderers(self):\n        renderers = super(WithDynamicViewSetMixin, self).get_renderers()\n        if settings.ENABLE_BROWSABLE_API is False:\n            return [\n                r for r in renderers if not isinstance(r, BrowsableAPIRenderer)\n            ]\n        else:\n            return renderers", "response": "Optionally block Browsable API rendering."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_request_feature(self, name):\n        if '[]' in name:\n            # array-type\n            return self.request.query_params.getlist(\n                name) if name in self.features else None\n        elif '{}' in name:\n            # object-type (keys are not consistent)\n            return self._extract_object_params(\n                name) if name in self.features else {}\n        else:\n            # single-type\n            return self.request.query_params.get(\n                name) if name in self.features else None", "response": "Parses the request for a particular feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _extract_object_params(self, name):\n\n        params = self.request.query_params.lists()\n        params_map = {}\n        prefix = name[:-1]\n        offset = len(prefix)\n        for name, value in params:\n            if name.startswith(prefix):\n                if name.endswith('}'):\n                    name = name[offset:-1]\n                elif name.endswith('}[]'):\n                    # strip off trailing []\n                    # this fixes an Ember queryparams issue\n                    name = name[offset:-3]\n                else:\n                    # malformed argument like:\n                    # filter{foo=bar\n                    raise exceptions.ParseError(\n                        '\"%s\" is not a well-formed filter key.' % name\n                    )\n            else:\n                continue\n            params_map[name] = value\n\n        return params_map", "response": "Extract object params from the request query params."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_queryset(self, queryset=None):\n        serializer = self.get_serializer()\n        return getattr(self, 'queryset', serializer.Meta.model.objects.all())", "response": "Returns a root - level queryset for this request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_request_fields(self):\n        if hasattr(self, '_request_fields'):\n            return self._request_fields\n\n        include_fields = self.get_request_feature(self.INCLUDE)\n        exclude_fields = self.get_request_feature(self.EXCLUDE)\n        request_fields = {}\n        for fields, include in(\n                (include_fields, True),\n                (exclude_fields, False)):\n            if fields is None:\n                continue\n            for field in fields:\n                field_segments = field.split('.')\n                num_segments = len(field_segments)\n                current_fields = request_fields\n                for i, segment in enumerate(field_segments):\n                    last = i == num_segments - 1\n                    if segment:\n                        if last:\n                            current_fields[segment] = include\n                        else:\n                            if segment not in current_fields:\n                                current_fields[segment] = {}\n                            current_fields = current_fields[segment]\n                    elif not last:\n                        # empty segment must be the last segment\n                        raise exceptions.ParseError(\n                            '\"%s\" is not a valid field.' %\n                            field\n                        )\n\n        self._request_fields = request_fields\n        return request_fields", "response": "Parses the INCLUDE and EXCLUDE features and returns a nested dict mapping serializer keys to True or False."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_related(self, request, pk=None, field_name=None):\n\n        # Explicitly disable support filtering. Applying filters to this\n        # endpoint would require us to pass through sideload filters, which\n        # can have unintended consequences when applied asynchronously.\n        if self.get_request_feature(self.FILTER):\n            raise ValidationError(\n                'Filtering is not enabled on relation endpoints.'\n            )\n\n        # Prefix include/exclude filters with field_name so it's scoped to\n        # the parent object.\n        field_prefix = field_name + '.'\n        self._prefix_inex_params(request, self.INCLUDE, field_prefix)\n        self._prefix_inex_params(request, self.EXCLUDE, field_prefix)\n\n        # Filter for parent object, include related field.\n        self.request.query_params.add('filter{pk}', pk)\n        self.request.query_params.add(self.INCLUDE, field_prefix)\n\n        # Get serializer and field.\n        serializer = self.get_serializer()\n        field = serializer.fields.get(field_name)\n        if field is None:\n            raise ValidationError('Unknown field: \"%s\".' % field_name)\n\n        # Query for root object, with related field prefetched\n        queryset = self.get_queryset()\n        queryset = self.filter_queryset(queryset)\n        obj = queryset.first()\n\n        if not obj:\n            return Response(\"Not found\", status=404)\n\n        # Serialize the related data. Use the field's serializer to ensure\n        # it's configured identically to the sideload case.\n        serializer = field.get_serializer(envelope=True)\n        try:\n            # TODO(ryo): Probably should use field.get_attribute() but that\n            #            seems to break a bunch of things. Investigate later.\n            serializer.instance = getattr(obj, field.source)\n        except ObjectDoesNotExist:\n            # See:\n            # http://jsonapi.org/format/#fetching-relationships-responses-404\n            # This is a case where the \"link URL exists but the relationship\n            # is empty\" and therefore must return a 200.\n            return Response({}, status=200)\n\n        return Response(serializer.data)", "response": "Fetch related object for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating one or more model instances.", "response": "def update(self, request, *args, **kwargs):\n        \"\"\"Update one or more model instances.\n\n        If ENABLE_BULK_UPDATE is set, multiple previously-fetched records\n        may be updated in a single call, provided their IDs.\n\n        If ENABLE_PATCH_ALL is set, multiple records\n        may be updated in a single PATCH call, even without knowing their IDs.\n\n        *WARNING*: ENABLE_PATCH_ALL should be considered an advanced feature\n        and used with caution. This feature must be enabled at the viewset level\n        and must also be requested explicitly by the client\n        via the \"patch-all\" query parameter.\n\n        This parameter can have one of the following values:\n\n            true (or 1): records will be fetched and then updated in a transaction loop\n              - The `Model.save` method will be called and model signals will run\n              - This can be slow if there are too many signals or many records in the query\n              - This is considered the more safe and default behavior\n            query: records will be updated in a single query\n              - The `QuerySet.update` method will be called and model signals will not run\n              - This will be fast, but may break data constraints that are controlled by signals\n              - This is considered unsafe but useful in certain situations\n\n        The server's successful response to a patch-all request\n        will NOT include any individual records. Instead, the response content will contain\n        a \"meta\" object with an \"updated\" count of updated records.\n\n        Examples:\n\n        Update one dog:\n\n            PATCH /dogs/1/\n            {\n                'fur': 'white'\n            }\n\n        Update many dogs by ID:\n\n            PATCH /dogs/\n            [\n                {'id': 1, 'fur': 'white'},\n                {'id': 2, 'fur': 'black'},\n                {'id': 3, 'fur': 'yellow'}\n            ]\n\n        Update all dogs in a query:\n\n            PATCH /dogs/?filter{fur.contains}=brown&patch-all=true\n            {\n                'fur': 'gold'\n            }\n        \"\"\"  # noqa\n        if self.ENABLE_BULK_UPDATE:\n            patch_all = self.get_request_patch_all()\n            if self.ENABLE_PATCH_ALL and patch_all:\n                # patch-all update\n                data = request.data\n                return self._patch_all(\n                    data,\n                    query=(patch_all == 'query')\n                )\n            else:\n                # bulk payload update\n                partial = 'partial' in kwargs\n                bulk_payload = self._get_bulk_payload(request)\n                if bulk_payload:\n                    return self._bulk_update(bulk_payload, partial)\n\n        # singular update\n        try:\n            return super(DynamicModelViewSet, self).update(request, *args,\n                                                           **kwargs)\n        except AssertionError as e:\n            err = str(e)\n            if 'Fix your URL conf' in err:\n                # this error is returned by DRF if a client\n                # makes an update request (PUT or PATCH) without an ID\n                # since DREST supports bulk updates with IDs contained in data,\n                # we return a 400 instead of a 500 for this case,\n                # as this is not considered a misconfiguration\n                raise exceptions.ValidationError(err)\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a single or many model instances in bulk.", "response": "def create(self, request, *args, **kwargs):\n        \"\"\"\n        Either create a single or many model instances in bulk\n        using the Serializer's many=True ability from Django REST >= 2.2.5.\n\n        The data can be represented by the serializer name (single or plural\n        forms), dict or list.\n\n        Examples:\n\n        POST /dogs/\n        {\n          \"name\": \"Fido\",\n          \"age\": 2\n        }\n\n        POST /dogs/\n        {\n          \"dog\": {\n            \"name\": \"Lucky\",\n            \"age\": 3\n          }\n        }\n\n        POST /dogs/\n        {\n          \"dogs\": [\n            {\"name\": \"Fido\", \"age\": 2},\n            {\"name\": \"Lucky\", \"age\": 3}\n          ]\n        }\n\n        POST /dogs/\n        [\n            {\"name\": \"Fido\", \"age\": 2},\n            {\"name\": \"Lucky\", \"age\": 3}\n        ]\n        \"\"\"\n        bulk_payload = self._get_bulk_payload(request)\n        if bulk_payload:\n            return self._create_many(bulk_payload)\n        return super(DynamicModelViewSet, self).create(\n            request, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a single or many model instances in bulk.", "response": "def destroy(self, request, *args, **kwargs):\n        \"\"\"\n        Either delete a single or many model instances in bulk\n\n        DELETE /dogs/\n        {\n            \"dogs\": [\n                {\"id\": 1},\n                {\"id\": 2}\n            ]\n        }\n\n        DELETE /dogs/\n        [\n            {\"id\": 1},\n            {\"id\": 2}\n        ]\n        \"\"\"\n        bulk_payload = self._get_bulk_payload(request)\n        if bulk_payload:\n            return self._destroy_many(bulk_payload)\n        lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field\n        if lookup_url_kwarg not in kwargs:\n            # assume that it is a poorly formatted bulk request\n            return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n        return super(DynamicModelViewSet, self).destroy(\n            request, *args, **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn canonical resource key usually the DB table name.", "response": "def get_resource_key(self):\n        \"\"\"Return canonical resource key, usually the DB table name.\"\"\"\n        model = self.get_model()\n        if model:\n            return get_model_table(model)\n        else:\n            return self.get_name()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef data(self):\n        data = super(DynamicListSerializer, self).data\n        processed_data = ReturnDict(\n            SideloadingProcessor(self, data).data,\n            serializer=self\n        ) if self.child.envelope else ReturnList(\n            data,\n            serializer=self\n        )\n        processed_data = post_process(processed_data)\n        return processed_data", "response": "Get the data after performing post - processing if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _dynamic_init(self, only_fields, include_fields, exclude_fields):\n\n        if not self.dynamic:\n            return\n\n        if (isinstance(self.request_fields, dict) and\n                self.request_fields.pop('*', None) is False):\n            exclude_fields = '*'\n\n        only_fields = set(only_fields or [])\n        include_fields = include_fields or []\n        exclude_fields = exclude_fields or []\n\n        if only_fields:\n            exclude_fields = '*'\n            include_fields = only_fields\n\n        if exclude_fields == '*':\n            # First exclude all, then add back in explicitly included fields.\n            include_fields = set(\n                list(include_fields) + [\n                    field for field, val in six.iteritems(self.request_fields)\n                    if val or val == {}\n                ]\n            )\n            all_fields = set(self.get_all_fields().keys())  # this is slow\n            exclude_fields = all_fields - include_fields\n        elif include_fields == '*':\n            all_fields = set(self.get_all_fields().keys())  # this is slow\n            include_fields = all_fields\n\n        for name in exclude_fields:\n            self.request_fields[name] = False\n\n        for name in include_fields:\n            if not isinstance(self.request_fields.get(name), dict):\n                # not sideloading this field\n                self.request_fields[name] = True", "response": "Initializes the internal fields of the current object based on the dynamic fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the serializer name.", "response": "def get_name(cls):\n        \"\"\"Get the serializer name.\n\n        The name can be defined on the Meta class or will be generated\n        automatically from the model name.\n        \"\"\"\n        if not hasattr(cls.Meta, 'name'):\n            class_name = getattr(cls.get_model(), '__name__', None)\n            setattr(\n                cls.Meta,\n                'name',\n                inflection.underscore(class_name) if class_name else None\n            )\n\n        return cls.Meta.name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the serializer s plural name.", "response": "def get_plural_name(cls):\n        \"\"\"Get the serializer's plural name.\n\n        The plural name may be defined on the Meta class.\n        If the plural name is not defined,\n        the pluralized form of the name will be returned.\n        \"\"\"\n        if not hasattr(cls.Meta, 'plural_name'):\n            setattr(\n                cls.Meta,\n                'plural_name',\n                inflection.pluralize(cls.get_name())\n            )\n        return cls.Meta.plural_name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the entire serializer field set. Does not respect dynamic field inclusions or exclusions.", "response": "def _all_fields(self):\n        \"\"\"Returns the entire serializer field set.\n\n        Does not respect dynamic field inclusions/exclusions.\n        \"\"\"\n        if (\n            not settings.ENABLE_FIELDS_CACHE or\n            not self.ENABLE_FIELDS_CACHE or\n            self.__class__ not in FIELDS_CACHE\n        ):\n            all_fields = super(\n                WithDynamicSerializerMixin,\n                self\n            ).get_fields()\n\n            if (\n                settings.ENABLE_FIELDS_CACHE and\n                self.ENABLE_FIELDS_CACHE\n            ):\n                FIELDS_CACHE[self.__class__] = all_fields\n        else:\n            all_fields = copy.copy(FIELDS_CACHE[self.__class__])\n            for k, field in six.iteritems(all_fields):\n                if hasattr(field, 'reset'):\n                    field.reset()\n\n        for k, field in six.iteritems(all_fields):\n            field.field_name = k\n            field.parent = self\n\n        return all_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fields(self):\n        all_fields = self.get_all_fields()\n        if self.dynamic is False:\n            return all_fields\n\n        if self.id_only():\n            return {}\n\n        serializer_fields = copy.deepcopy(all_fields)\n        request_fields = self.request_fields\n        deferred = self._get_deferred_field_names(serializer_fields)\n\n        # apply request overrides\n        if request_fields:\n            for name, include in six.iteritems(request_fields):\n                if name not in serializer_fields:\n                    raise exceptions.ParseError(\n                        '\"%s\" is not a valid field name for \"%s\".' %\n                        (name, self.get_name())\n                    )\n                if include is not False and name in deferred:\n                    deferred.remove(name)\n                elif include is False:\n                    deferred.add(name)\n\n        for name in deferred:\n            serializer_fields.pop(name)\n\n        # Set read_only flags based on read_only_fields meta list.\n        # Here to cover DynamicFields not covered by DRF.\n        ro_fields = getattr(self.Meta, 'read_only_fields', [])\n        self.flag_fields(serializer_fields, ro_fields, 'read_only', True)\n\n        pw_fields = getattr(self.Meta, 'untrimmed_fields', [])\n        self.flag_fields(\n            serializer_fields,\n            pw_fields,\n            'trim_whitespace',\n            False,\n        )\n\n        # Toggle read_only flags for immutable fields.\n        # Note: This overrides `read_only` if both are set, to allow\n        #       inferred DRF fields to be made immutable.\n        immutable_field_names = self._get_flagged_field_names(\n            serializer_fields,\n            'immutable'\n        )\n        self.flag_fields(\n            serializer_fields,\n            immutable_field_names,\n            'read_only',\n            value=False if self.get_request_method() == 'POST' else True\n        )\n\n        return serializer_fields", "response": "Returns the serializer s field set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct dict of name : field for linkable fields.", "response": "def _link_fields(self):\n        \"\"\"Construct dict of name:field for linkable fields.\"\"\"\n        query_params = self.get_request_attribute('query_params', {})\n        if 'exclude_links' in query_params:\n            return {}\n        else:\n            all_fields = self.get_all_fields()\n            return {\n                name: field for name, field in six.iteritems(all_fields)\n                if isinstance(field, DynamicRelationField) and\n                getattr(field, 'link', True) and\n                not (\n                    # Skip sideloaded fields\n                    name in self.fields and\n                    self.is_field_sideloaded(name)\n                ) and not (\n                    # Skip included single relations\n                    # TODO: Use links, when we can generate canonical URLs\n                    name in self.fields and\n                    not getattr(field, 'many', False)\n                )\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _faster_to_representation(self, instance):\n\n        ret = {}\n        fields = self._readable_fields\n\n        is_fast = isinstance(instance, prefetch.FastObject)\n        id_fields = self._readable_id_fields\n\n        for field in fields:\n            attribute = None\n\n            # we exclude dynamic fields here because the proper fastquery\n            # dereferencing happens in the `get_attribute` method now\n            if (\n                is_fast and\n                not isinstance(\n                    field,\n                    (DynamicGenericRelationField, DynamicRelationField)\n                )\n            ):\n                if field in id_fields and field.source not in instance:\n                    # TODO - make better.\n                    attribute = instance.get(field.source + '_id')\n                    ret[field.field_name] = attribute\n                    continue\n                else:\n                    try:\n                        attribute = instance[field.source]\n                    except KeyError:\n                        # slower, but does more stuff\n                        # Also, some temp debugging\n                        if hasattr(instance, field.source):\n                            attribute = getattr(instance, field.source)\n                        else:\n                            # Fall back on DRF behavior\n                            attribute = field.get_attribute(instance)\n                            print(\n                                'Missing %s from %s' % (\n                                    field.field_name,\n                                    self.__class__.__name__\n                                )\n                            )\n            else:\n                try:\n                    attribute = field.get_attribute(instance)\n                except SkipField:\n                    continue\n\n            if attribute is None:\n                # We skip `to_representation` for `None` values so that\n                # fields do not have to explicitly deal with that case.\n                ret[field.field_name] = None\n            else:\n                ret[field.field_name] = field.to_representation(attribute)\n\n        return ret", "response": "Modified to_representation with optimizations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmodify to_representation method. Optionally may cache objects.", "response": "def to_representation(self, instance):\n        \"\"\"Modified to_representation method. Optionally may cache objects.\n\n        Arguments:\n            instance: A model instance or data object.\n        Returns:\n            Instance ID if the serializer is meant to represent its ID.\n            Otherwise, a tagged data dict representation.\n        \"\"\"\n        if self.id_only():\n            return instance.pk\n\n        pk = getattr(instance, 'pk', None)\n        if not settings.ENABLE_SERIALIZER_OBJECT_CACHE or pk is None:\n            return self._to_representation(instance)\n        else:\n            if pk not in self.obj_cache:\n                self.obj_cache[pk] = self._to_representation(instance)\n            return self.obj_cache[pk]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, *args, **kwargs):\n        update = getattr(self, 'instance', None) is not None\n        instance = super(\n            WithDynamicSerializerMixin,\n            self\n        ).save(\n            *args,\n            **kwargs\n        )\n        view = self._context.get('view')\n        if view and update:\n            if int(DRF_VERSION[0]) <= 3 and int(DRF_VERSION[1]) < 5:\n                # Reload the object on update\n                # to get around prefetch cache issues\n                # Fixed in DRF in 3.5.0\n                instance = self.instance = view.get_object()\n        return instance", "response": "Serializer save that address prefetch issues."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of fields consisting of at minimum the PK field name.", "response": "def get_id_fields(self):\n        \"\"\"\n        Called to return a list of fields consisting of, at minimum,\n        the PK field name. The output of this method is used to\n        construct a Prefetch object with a .only() queryset\n        when this field is not being sideloaded but we need to\n        return a list of IDs.\n        \"\"\"\n        model = self.get_model()\n\n        out = [model._meta.pk.name]  # get PK field name\n\n        # If this is being called, it means it\n        # is a many-relation  to its parent.\n        # Django wants the FK to the parent,\n        # but since accurately inferring the FK\n        # pointing back to the parent is less than trivial,\n        # we will just pull all ID fields.\n        # TODO: We also might need to return all non-nullable fields,\n        #    or else it is possible Django will issue another request.\n        for field in model._meta.fields:\n            if isinstance(field, models.ForeignKey):\n                out.append(field.name + '_id')\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_representation(self, instance):\n\n        if not isinstance(instance, dict):\n            data = super(\n                DynamicEphemeralSerializer,\n                self\n            ).to_representation(instance)\n        else:\n            data = instance\n            instance = EphemeralObject(data)\n\n        if self.id_only():\n            return data\n        else:\n            return tag_dict(data, serializer=self, instance=instance)", "response": "Provides post processing for the object that is passed to the object s to_representation method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_directory(request):\n\n    def get_url(url):\n        return reverse(url, request=request) if url else url\n\n    def is_active_url(path, url):\n        return path.startswith(url) if url and path else False\n\n    path = request.path\n    directory_list = []\n\n    def sort_key(r):\n        return r[0]\n\n    # TODO(ant): support arbitrarily nested\n    # structure, for now it is capped at a single level\n    # for UX reasons\n    for group_name, endpoints in sorted(\n        six.iteritems(directory),\n        key=sort_key\n    ):\n        endpoints_list = []\n        for endpoint_name, endpoint in sorted(\n            six.iteritems(endpoints),\n            key=sort_key\n        ):\n            if endpoint_name[:1] == '_':\n                continue\n            endpoint_url = get_url(endpoint.get('_url', None))\n            active = is_active_url(path, endpoint_url)\n            endpoints_list.append(\n                (endpoint_name, endpoint_url, [], active)\n            )\n\n        url = get_url(endpoints.get('_url', None))\n        active = is_active_url(path, url)\n        directory_list.append(\n            (group_name, url, endpoints_list, active)\n        )\n    return directory_list", "response": "Get API directory as a nested list of lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_api_root_view(self, **kwargs):\n        class API(views.APIView):\n            _ignore_model_permissions = True\n\n            def get(self, request, *args, **kwargs):\n                directory_list = get_directory(request)\n                result = OrderedDict()\n                for group_name, url, endpoints, _ in directory_list:\n                    if url:\n                        result[group_name] = url\n                    else:\n                        group = OrderedDict()\n                        for endpoint_name, url, _, _ in endpoints:\n                            group[endpoint_name] = url\n                        result[group_name] = group\n                return Response(result)\n\n        return API.as_view()", "response": "Return the API root view using the global directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd any registered route into a global API directory.", "response": "def register(self, prefix, viewset, base_name=None):\n        \"\"\"Add any registered route into a global API directory.\n\n        If the prefix includes a path separator,\n        store the URL in the directory under the first path segment.\n        Otherwise, store it as-is.\n\n        For example, if there are two registered prefixes,\n        'v1/users' and 'groups', `directory` will look liks:\n\n        {\n            'v1': {\n                'users': {\n                    '_url': 'users-list'\n                    '_viewset': <class 'UserViewSet'>\n                },\n            }\n            'groups': {\n               '_url': 'groups-list'\n               '_viewset': <class 'GroupViewSet'>\n            }\n        }\n        \"\"\"\n        if base_name is None:\n            base_name = prefix\n\n        super(DynamicRouter, self).register(prefix, viewset, base_name)\n\n        prefix_parts = prefix.split('/')\n        if len(prefix_parts) > 1:\n            prefix = prefix_parts[0]\n            endpoint = '/'.join(prefix_parts[1:])\n        else:\n            endpoint = prefix\n            prefix = None\n\n        if prefix and prefix not in directory:\n            current = directory[prefix] = {}\n        else:\n            current = directory.get(prefix, directory)\n\n        list_name = self.routes[0].name\n        url_name = list_name.format(basename=base_name)\n        if endpoint not in current:\n            current[endpoint] = {}\n        current[endpoint]['_url'] = url_name\n        current[endpoint]['_viewset'] = viewset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_resource(self, viewset, namespace=None):\n\n        # Try to extract resource name from viewset.\n        try:\n            serializer = viewset.serializer_class()\n            resource_key = serializer.get_resource_key()\n            resource_name = serializer.get_name()\n            path_name = serializer.get_plural_name()\n        except:\n            import traceback\n            traceback.print_exc()\n            raise Exception(\n                \"Failed to extract resource name from viewset: '%s'.\"\n                \" It, or its serializer, may not be DREST-compatible.\" % (\n                    viewset\n                )\n            )\n\n        # Construct canonical path and register it.\n        if namespace:\n            namespace = namespace.rstrip('/') + '/'\n        base_path = namespace or ''\n        base_path = r'%s' % base_path + path_name\n        self.register(base_path, viewset)\n\n        # Make sure resource isn't already registered.\n        if resource_key in resource_map:\n            raise Exception(\n                \"The resource '%s' has already been mapped to '%s'.\"\n                \" Each resource can only be mapped to one canonical\"\n                \" path. \" % (\n                    resource_key,\n                    resource_map[resource_key]['path']\n                )\n            )\n\n        # Register resource in reverse map.\n        resource_map[resource_key] = {\n            'path': base_path,\n            'viewset': viewset\n        }\n\n        # Make sure the resource name isn't registered, either\n        # TODO: Think of a better way to clean this up, there's a lot of\n        # duplicated effort here, between `resource_name` and `resource_key`\n        # This resource name -> key mapping is currently only used by\n        # the DynamicGenericRelationField\n        if resource_name in resource_name_map:\n            resource_key = resource_name_map[resource_name]\n            raise Exception(\n                \"The resource name '%s' has already been mapped to '%s'.\"\n                \" A resource name can only be used once.\" % (\n                    resource_name,\n                    resource_map[resource_key]['path']\n                )\n            )\n\n        # map the resource name to the resource key for easier lookup\n        resource_name_map[resource_name] = resource_key", "response": "This function registers a viewset that should be considered the canonical URL for a particular resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_canonical_path(resource_key, pk=None):\n\n        if resource_key not in resource_map:\n            # Note: Maybe raise?\n            return None\n\n        base_path = get_script_prefix() + resource_map[resource_key]['path']\n        if pk:\n            return '%s/%s/' % (base_path, pk)\n        else:\n            return base_path", "response": "Returns the canonical resource path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the canonical serializer class for a given resource name.", "response": "def get_canonical_serializer(\n        resource_key,\n        model=None,\n        instance=None,\n        resource_name=None\n    ):\n        \"\"\"\n        Return canonical serializer for a given resource name.\n\n        Arguments:\n            resource_key - Resource key, usually DB table for model-based\n                           resources, otherwise the plural name.\n            model - (Optional) Model class to look up by.\n            instance - (Optional) Model object instance.\n        Returns: serializer class\n        \"\"\"\n\n        if model:\n            resource_key = get_model_table(model)\n        elif instance:\n            resource_key = instance._meta.db_table\n        elif resource_name:\n            resource_key = resource_name_map[resource_name]\n\n        if resource_key not in resource_map:\n            return None\n\n        return resource_map[resource_key]['viewset'].serializer_class"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_routes(self, viewset):\n        routes = super(DynamicRouter, self).get_routes(viewset)\n        routes += self.get_relation_routes(viewset)\n        return routes", "response": "A method that gets called when the viewset is added to the routes list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates routes to serve relational objects.", "response": "def get_relation_routes(self, viewset):\n        \"\"\"\n        Generate routes to serve relational objects. This method will add\n        a sub-URL for each relational field.\n\n        e.g.\n        A viewset for the following serializer:\n\n          class UserSerializer(..):\n              events = DynamicRelationField(EventSerializer, many=True)\n              groups = DynamicRelationField(GroupSerializer, many=True)\n              location = DynamicRelationField(LocationSerializer)\n\n        will have the following URLs added:\n\n          /users/<pk>/events/\n          /users/<pk>/groups/\n          /users/<pk>/location/\n        \"\"\"\n\n        routes = []\n\n        if not hasattr(viewset, 'serializer_class'):\n            return routes\n        if not hasattr(viewset, 'list_related'):\n            return routes\n\n        serializer = viewset.serializer_class()\n        fields = getattr(serializer, 'get_link_fields', lambda: [])()\n\n        route_name = '{basename}-{methodnamehyphen}'\n\n        for field_name, field in six.iteritems(fields):\n            methodname = 'list_related'\n            url = (\n                r'^{prefix}/{lookup}/(?P<field_name>%s)'\n                '{trailing_slash}$' % field_name\n            )\n            routes.append(Route(\n                url=url,\n                mapping={'get': methodname},\n                name=replace_methodname(route_name, field_name),\n                initkwargs={}\n            ))\n        return routes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all paths from the root to the leaves.", "response": "def get_paths(self):\n        \"\"\"Get all paths from the root to the leaves.\n\n        For example, given a chain like `{'a':{'b':{'c':None}}}`,\n        this method would return `[['a', 'b', 'c']]`.\n\n        Returns:\n            A list of lists of paths.\n        \"\"\"\n        paths = []\n        for key, child in six.iteritems(self):\n            if isinstance(child, TreeMap) and child:\n                # current child is an intermediate node\n                for path in child.get_paths():\n                    path.insert(0, key)\n                    paths.append(path)\n            else:\n                # current child is an endpoint\n                paths.append([key])\n        return paths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninsert a list of nodes into the tree.", "response": "def insert(self, parts, leaf_value, update=False):\n        \"\"\"Add a list of nodes into the tree.\n\n        The list will be converted into a TreeMap (chain) and then\n        merged with the current TreeMap.\n\n        For example, this method would insert `['a','b','c']` as\n        `{'a':{'b':{'c':{}}}}`.\n\n        Arguments:\n            parts: List of nodes representing a chain.\n            leaf_value: Value to insert into the leaf of the chain.\n            update: Whether or not to update the leaf with the given value or\n                to replace the value.\n\n        Returns:\n            self\n        \"\"\"\n        tree = self\n        if not parts:\n            return tree\n\n        cur = tree\n        last = len(parts) - 1\n        for i, part in enumerate(parts):\n            if part not in cur:\n                cur[part] = TreeMap() if i != last else leaf_value\n            elif i == last:  # found leaf\n                if update:\n                    cur[part].update(leaf_value)\n                else:\n                    cur[part] = leaf_value\n\n            cur = cur[part]\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a TaggedDict instance based on the type of obj. Will either be an OrderedDict or TaggedPlainDict depending on the type of obj.", "response": "def tag_dict(obj, *args, **kwargs):\n    \"\"\"Create a TaggedDict instance. Will either be a TaggedOrderedDict\n    or TaggedPlainDict depending on the type of `obj`.\"\"\"\n\n    if isinstance(obj, OrderedDict):\n        return _TaggedOrderedDict(obj, *args, **kwargs)\n    else:\n        return _TaggedPlainDict(obj, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True iff. a queryset includes joins.", "response": "def has_joins(queryset):\n    \"\"\"Return True iff. a queryset includes joins.\n\n    If this is the case, it is possible for the queryset\n    to return duplicate results.\n    \"\"\"\n    for join in six.itervalues(queryset.query.alias_map):\n        if join.join_type:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a key that can be passed to Django s filter method.", "response": "def generate_query_key(self, serializer):\n        \"\"\"Get the key that can be passed to Django's filter method.\n\n        To account for serialier field name rewrites, this method\n        translates serializer field names to model field names\n        by inspecting `serializer`.\n\n        For example, a query like `filter{users.events}` would be\n        returned as `users__events`.\n\n        Arguments:\n            serializer: A DRF serializer\n\n        Returns:\n            A filter key.\n        \"\"\"\n        rewritten = []\n        last = len(self.field) - 1\n        s = serializer\n        field = None\n        for i, field_name in enumerate(self.field):\n            # Note: .fields can be empty for related serializers that aren't\n            # sideloaded. Fields that are deferred also won't be present.\n            # If field name isn't in serializer.fields, get full list from\n            # get_all_fields() method. This is somewhat expensive, so only do\n            # this if we have to.\n            fields = s.fields\n            if field_name not in fields:\n                fields = getattr(s, 'get_all_fields', lambda: {})()\n\n            if field_name == 'pk':\n                rewritten.append('pk')\n                continue\n\n            if field_name not in fields:\n                raise ValidationError(\n                    \"Invalid filter field: %s\" % field_name\n                )\n\n            field = fields[field_name]\n\n            # For remote fields, strip off '_set' for filtering. This is a\n            # weird Django inconsistency.\n            model_field_name = field.source or field_name\n            model_field = get_model_field(s.get_model(), model_field_name)\n            if isinstance(model_field, RelatedObject):\n                model_field_name = model_field.field.related_query_name()\n\n            # If get_all_fields() was used above, field could be unbound,\n            # and field.source would be None\n            rewritten.append(model_field_name)\n\n            if i == last:\n                break\n\n            # Recurse into nested field\n            s = getattr(field, 'serializer', None)\n            if isinstance(s, serializers.ListSerializer):\n                s = s.child\n            if not s:\n                raise ValidationError(\n                    \"Invalid nested filter field: %s\" % field_name\n                )\n\n        if self.operator:\n            rewritten.append(self.operator)\n\n        return ('__'.join(rewritten), field)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfilters the queryset. This is the main entry-point to this class, and is called by DRF's list handler.", "response": "def filter_queryset(self, request, queryset, view):\n        \"\"\"Filter the queryset.\n\n        This is the main entry-point to this class, and\n        is called by DRF's list handler.\n        \"\"\"\n        self.request = request\n        self.view = view\n\n        # enable addition of extra filters (i.e., a Q())\n        # so custom filters can be added to the queryset without\n        # running into https://code.djangoproject.com/ticket/18437\n        # which, without this, would mean that filters added to the queryset\n        # after this is called may not behave as expected\n        extra_filters = self.view.get_extra_filters(request)\n\n        disable_prefetches = self.view.is_update()\n\n        self.DEBUG = settings.DEBUG\n\n        return self._build_queryset(\n            queryset=queryset,\n            extra_filters=extra_filters,\n            disable_prefetches=disable_prefetches,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_requested_filters(self, **kwargs):\n\n        filters_map = (\n            kwargs.get('filters_map') or\n            self.view.get_request_feature(self.view.FILTER)\n        )\n\n        out = TreeMap()\n\n        for spec, value in six.iteritems(filters_map):\n\n            # Inclusion or exclusion?\n            if spec[0] == '-':\n                spec = spec[1:]\n                inex = '_exclude'\n            else:\n                inex = '_include'\n\n            # for relational filters, separate out relation path part\n            if '|' in spec:\n                rel, spec = spec.split('|')\n                rel = rel.split('.')\n            else:\n                rel = None\n\n            parts = spec.split('.')\n\n            # Last part could be operator, e.g. \"events.capacity.gte\"\n            if len(parts) > 1 and parts[-1] in self.VALID_FILTER_OPERATORS:\n                operator = parts.pop()\n            else:\n                operator = None\n\n            # All operators except 'range' and 'in' should have one value\n            if operator == 'range':\n                value = value[:2]\n            elif operator == 'in':\n                # no-op: i.e. accept `value` as an arbitrarily long list\n                pass\n            elif operator in self.VALID_FILTER_OPERATORS:\n                value = value[0]\n                if (\n                    operator == 'isnull' and\n                    isinstance(value, six.string_types)\n                ):\n                    value = is_truthy(value)\n                elif operator == 'eq':\n                    operator = None\n\n            node = FilterNode(parts, operator, value)\n\n            # insert into output tree\n            path = rel if rel else []\n            path += [inex, node.key]\n            out.insert(path, node)\n\n        return out", "response": "Convert the filters query params into a dict that can be passed to Q. Returns a dict that can be passed to Q."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _filters_to_query(self, includes, excludes, serializer, q=None):\n\n        def rewrite_filters(filters, serializer):\n            out = {}\n            for k, node in six.iteritems(filters):\n                filter_key, field = node.generate_query_key(serializer)\n                if isinstance(field, (BooleanField, NullBooleanField)):\n                    node.value = is_truthy(node.value)\n                out[filter_key] = node.value\n\n            return out\n\n        q = q or Q()\n\n        if not includes and not excludes:\n            return None\n\n        if includes:\n            includes = rewrite_filters(includes, serializer)\n            q &= Q(**includes)\n        if excludes:\n            excludes = rewrite_filters(excludes, serializer)\n            for k, v in six.iteritems(excludes):\n                q &= ~Q(**{k: v})\n        return q", "response": "Returns a Django Query object from the request s includes and excludes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_implicit_prefetches(\n        self,\n        model,\n        prefetches,\n        requirements\n    ):\n        \"\"\"Build a prefetch dictionary based on internal requirements.\"\"\"\n\n        for source, remainder in six.iteritems(requirements):\n            if not remainder or isinstance(remainder, six.string_types):\n                # no further requirements to prefetch\n                continue\n\n            related_field = get_model_field(model, source)\n            related_model = get_related_model(related_field)\n\n            queryset = self._build_implicit_queryset(\n                related_model,\n                remainder\n            ) if related_model else None\n\n            prefetches[source] = self._create_prefetch(\n                source,\n                queryset\n            )\n\n        return prefetches", "response": "Build a prefetch dictionary based on internal requirements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a queryset based on implicit requirements.", "response": "def _build_implicit_queryset(self, model, requirements):\n        \"\"\"Build a queryset based on implicit requirements.\"\"\"\n\n        queryset = self._make_model_queryset(model)\n        prefetches = {}\n        self._build_implicit_prefetches(\n            model,\n            prefetches,\n            requirements\n        )\n        prefetch = prefetches.values()\n        queryset = queryset.prefetch_related(*prefetch).distinct()\n        if self.DEBUG:\n            queryset._using_prefetches = prefetches\n        return queryset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a prefetch dictionary based on request requirements.", "response": "def _build_requested_prefetches(\n        self,\n        prefetches,\n        requirements,\n        model,\n        fields,\n        filters\n    ):\n        \"\"\"Build a prefetch dictionary based on request requirements.\"\"\"\n\n        for name, field in six.iteritems(fields):\n            original_field = field\n            if isinstance(field, DynamicRelationField):\n                field = field.serializer\n            if isinstance(field, serializers.ListSerializer):\n                field = field.child\n            if not isinstance(field, serializers.ModelSerializer):\n                continue\n\n            source = field.source or name\n            if '.' in source:\n                raise ValidationError(\n                    'nested relationship values '\n                    'are not supported'\n                )\n\n            if source in prefetches:\n                # ignore duplicated sources\n                continue\n\n            is_remote = is_field_remote(model, source)\n            is_id_only = getattr(field, 'id_only', lambda: False)()\n            if is_id_only and not is_remote:\n                continue\n\n            related_queryset = getattr(original_field, 'queryset', None)\n\n            if callable(related_queryset):\n                related_queryset = related_queryset(field)\n\n            source = field.source or name\n            # Popping the source here (during explicit prefetch construction)\n            # guarantees that implicitly required prefetches that follow will\n            # not conflict.\n            required = requirements.pop(source, None)\n\n            prefetch_queryset = self._build_queryset(\n                serializer=field,\n                filters=filters.get(name, {}),\n                queryset=related_queryset,\n                requirements=required\n            )\n\n            # Note: There can only be one prefetch per source, even\n            #       though there can be multiple fields pointing to\n            #       the same source. This could break in some cases,\n            #       but is mostly an issue on writes when we use all\n            #       fields by default.\n            prefetches[source] = self._create_prefetch(\n                source,\n                prefetch_queryset\n            )\n\n        return prefetches"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_implicit_requirements(\n        self,\n        fields,\n        requirements\n    ):\n        \"\"\"Extract internal prefetch requirements from serializer fields.\"\"\"\n        for name, field in six.iteritems(fields):\n            source = field.source\n            # Requires may be manually set on the field -- if not,\n            # assume the field requires only its source.\n            requires = getattr(field, 'requires', None) or [source]\n            for require in requires:\n                if not require:\n                    # ignore fields with empty source\n                    continue\n\n                requirement = require.split('.')\n                if requirement[-1] == '':\n                    # Change 'a.b.' -> 'a.b.*',\n                    # supporting 'a.b.' for backwards compatibility.\n                    requirement[-1] = '*'\n                requirements.insert(requirement, TreeMap(), update=True)", "response": "Extract internal prefetch requirements from serializer fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a nested queryset that pulls in all data required by this request.", "response": "def _build_queryset(\n        self,\n        serializer=None,\n        filters=None,\n        queryset=None,\n        requirements=None,\n        extra_filters=None,\n        disable_prefetches=False,\n    ):\n        \"\"\"Build a queryset that pulls in all data required by this request.\n\n        Handles nested prefetching of related data and deferring fields\n        at the queryset level.\n\n        Arguments:\n            serializer: An optional serializer to use a base for the queryset.\n                If no serializer is passed, the `get_serializer` method will\n                be used to initialize the base serializer for the viewset.\n            filters: An optional TreeMap of nested filters.\n            queryset: An optional base queryset.\n            requirements: An optional TreeMap of nested requirements.\n        \"\"\"\n\n        is_root_level = False\n        if not serializer:\n            serializer = self.view.get_serializer()\n            is_root_level = True\n\n        queryset = self._get_queryset(queryset=queryset, serializer=serializer)\n\n        model = getattr(serializer.Meta, 'model', None)\n\n        if not model:\n            return queryset\n\n        prefetches = {}\n\n        # build a nested Prefetch queryset\n        # based on request parameters and serializer fields\n        fields = serializer.fields\n\n        if requirements is None:\n            requirements = TreeMap()\n\n        self._get_implicit_requirements(\n            fields,\n            requirements\n        )\n\n        if filters is None:\n            filters = self._get_requested_filters()\n\n        # build nested Prefetch queryset\n        self._build_requested_prefetches(\n            prefetches,\n            requirements,\n            model,\n            fields,\n            filters\n        )\n\n        # build remaining prefetches out of internal requirements\n        # that are not already covered by request requirements\n        self._build_implicit_prefetches(\n            model,\n            prefetches,\n            requirements\n        )\n\n        # use requirements at this level to limit fields selected\n        # only do this for GET requests where we are not requesting the\n        # entire fieldset\n        if (\n            '*' not in requirements and\n            not self.view.is_update() and\n            not self.view.is_delete()\n        ):\n            id_fields = getattr(serializer, 'get_id_fields', lambda: [])()\n            # only include local model fields\n            only = [\n                field for field in set(\n                    id_fields + list(requirements.keys())\n                ) if is_model_field(model, field) and\n                not is_field_remote(model, field)\n            ]\n            queryset = queryset.only(*only)\n\n        # add request filters\n        query = self._filters_to_query(\n            includes=filters.get('_include'),\n            excludes=filters.get('_exclude'),\n            serializer=serializer\n        )\n\n        # add additional filters specified by calling view\n        if extra_filters:\n            query = extra_filters if not query else extra_filters & query\n\n        if query:\n            # Convert internal django ValidationError to\n            # APIException-based one in order to resolve validation error\n            # from 500 status code to 400.\n            try:\n                queryset = queryset.filter(query)\n            except InternalValidationError as e:\n                raise ValidationError(\n                    dict(e) if hasattr(e, 'error_dict') else list(e)\n                )\n            except Exception as e:\n                # Some other Django error in parsing the filter.\n                # Very likely a bad query, so throw a ValidationError.\n                err_msg = getattr(e, 'message', '')\n                raise ValidationError(err_msg)\n\n        # A serializer can have this optional function\n        # to dynamically apply additional filters on\n        # any queries that will use that serializer\n        # You could use this to have (for example) different\n        # serializers for different subsets of a model or to\n        # implement permissions which work even in sideloads\n        if hasattr(serializer, 'filter_queryset'):\n            queryset = self._serializer_filter(\n                serializer=serializer,\n                queryset=queryset\n            )\n\n        # add prefetches and remove duplicates if necessary\n        prefetch = prefetches.values()\n        if prefetch and not disable_prefetches:\n            queryset = queryset.prefetch_related(*prefetch)\n        elif isinstance(queryset, Manager):\n            queryset = queryset.all()\n        if has_joins(queryset) or not is_root_level:\n            queryset = queryset.distinct()\n\n        if self.DEBUG:\n            queryset._using_prefetches = prefetches\n        return queryset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter the queryset applying the ordering.", "response": "def filter_queryset(self, request, queryset, view):\n        \"\"\"\"Filter the queryset, applying the ordering.\n\n        The `ordering_param` can be overwritten here.\n        In DRF, the ordering_param is 'ordering', but we support changing it\n        to allow the viewset to control the parameter.\n        \"\"\"\n        self.ordering_param = view.SORT\n\n        ordering = self.get_ordering(request, queryset, view)\n        if ordering:\n            return queryset.order_by(*ordering)\n\n        return queryset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an ordering for a given request.", "response": "def get_ordering(self, request, queryset, view):\n        \"\"\"Return an ordering for a given request.\n\n        DRF expects a comma separated list, while DREST expects an array.\n        This method overwrites the DRF default so it can parse the array.\n        \"\"\"\n        params = view.get_request_feature(view.SORT)\n        if params:\n            fields = [param.strip() for param in params]\n            valid_ordering, invalid_ordering = self.remove_invalid_fields(\n                queryset, fields, view\n            )\n\n            # if any of the sort fields are invalid, throw an error.\n            # else return the ordering\n            if invalid_ordering:\n                raise ValidationError(\n                    \"Invalid filter field: %s\" % invalid_ordering\n                )\n            else:\n                return valid_ordering\n\n        # No sorting was included\n        return self.get_default_ordering(view)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_invalid_fields(self, queryset, fields, view):\n\n        valid_orderings = []\n        invalid_orderings = []\n\n        # for each field sent down from the query param,\n        # determine if its valid or invalid\n        for term in fields:\n            stripped_term = term.lstrip('-')\n            # add back the '-' add the end if necessary\n            reverse_sort_term = '' if len(stripped_term) is len(term) else '-'\n            ordering = self.ordering_for(stripped_term, view)\n\n            if ordering:\n                valid_orderings.append(reverse_sort_term + ordering)\n            else:\n                invalid_orderings.append(term)\n\n        return valid_orderings, invalid_orderings", "response": "Remove invalid fields from an ordering."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ordering_for(self, term, view):\n        if not self._is_allowed_term(term, view):\n            return None\n\n        serializer = self._get_serializer_class(view)()\n        serializer_chain = term.split('.')\n\n        model_chain = []\n\n        for segment in serializer_chain[:-1]:\n            field = serializer.get_all_fields().get(segment)\n\n            if not (field and field.source != '*' and\n                    isinstance(field, DynamicRelationField)):\n                return None\n\n            model_chain.append(field.source or segment)\n\n            serializer = field.serializer_class()\n\n        last_segment = serializer_chain[-1]\n        last_field = serializer.get_all_fields().get(last_segment)\n\n        if not last_field or last_field.source == '*':\n            return None\n\n        model_chain.append(last_field.source or last_segment)\n\n        return '__'.join(model_chain)", "response": "Returns the model field chain for a given term and view."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef combine(line, left, intersect, right):\n    # Yield left border.\n    if left:\n        yield left\n\n    # Yield items with intersect characters.\n    if intersect:\n        try:\n            for j, i in enumerate(line, start=-len(line) + 1):\n                yield i\n                if j:\n                    yield intersect\n        except TypeError:  # Generator.\n            try:\n                item = next(line)\n            except StopIteration:  # Was empty all along.\n                pass\n            else:\n                while True:\n                    yield item\n                    try:\n                        peek = next(line)\n                    except StopIteration:\n                        break\n                    yield intersect\n                    item = peek\n    else:\n        for i in line:\n            yield i\n\n    # Yield right border.\n    if right:\n        yield right", "response": "Yields items in line with left and right borders."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_border(outer_widths, horizontal, left, intersect, right, title=None):\n    length = 0\n\n    # Hide title if it doesn't fit.\n    if title is not None and outer_widths:\n        try:\n            length = visible_width(title)\n        except TypeError:\n            title = str(title)\n            length = visible_width(title)\n        if length > sum(outer_widths) + len(intersect) * (len(outer_widths) - 1):\n            title = None\n\n    # Handle no title.\n    if title is None or not outer_widths or not horizontal:\n        return combine((horizontal * c for c in outer_widths), left, intersect, right)\n\n    # Handle title fitting in the first column.\n    if length == outer_widths[0]:\n        return combine([title] + [horizontal * c for c in outer_widths[1:]], left, intersect, right)\n    if length < outer_widths[0]:\n        columns = [title + horizontal * (outer_widths[0] - length)] + [horizontal * c for c in outer_widths[1:]]\n        return combine(columns, left, intersect, right)\n\n    # Handle wide titles/narrow columns.\n    columns_and_intersects = [title]\n    for width in combine(outer_widths, None, bool(intersect), None):\n        # If title is taken care of.\n        if length < 1:\n            columns_and_intersects.append(intersect if width is True else horizontal * width)\n        # If title's last character overrides an intersect character.\n        elif width is True and length == 1:\n            length = 0\n        # If this is an intersect character that is overridden by the title.\n        elif width is True:\n            length -= 1\n        # If title's last character is within a column.\n        elif width >= length:\n            columns_and_intersects[0] += horizontal * (width - length)  # Append horizontal chars to title.\n            length = 0\n        # If remainder of title won't fit in a column.\n        else:\n            length -= width\n\n    return combine(columns_and_intersects, left, None, right)", "response": "Builds the top bottom middle row."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_row(row, left, center, right):\n    if not row or not row[0]:\n        yield combine((), left, center, right)\n        return\n    for row_index in range(len(row[0])):\n        yield combine((c[row_index] for c in row), left, center, right)", "response": "Combine single or multi - line cells into a single row of list of lists including borders."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks variables and check tox. ini.", "response": "def run(cls):\n        \"\"\"Check variables.\"\"\"\n        project = __import__(IMPORT, fromlist=[''])\n        for expected, var in [('@Robpol86', '__author__'), (LICENSE, '__license__'), (VERSION, '__version__')]:\n            if getattr(project, var) != expected:\n                raise SystemExit('Mismatch: {0}'.format(var))\n        # Check changelog.\n        if not re.compile(r'^%s - \\d{4}-\\d{2}-\\d{2}[\\r\\n]' % VERSION, re.MULTILINE).search(readme()):\n            raise SystemExit('Version not found in readme/changelog file.')\n        # Check tox.\n        if INSTALL_REQUIRES:\n            contents = readme('tox.ini')\n            section = re.compile(r'[\\r\\n]+install_requires =[\\r\\n]+(.+?)[\\r\\n]+\\w', re.DOTALL).findall(contents)\n            if not section:\n                raise SystemExit('Missing install_requires section in tox.ini.')\n            in_tox = re.findall(r'    ([^=]+)==[\\w\\d.-]+', section[0])\n            if INSTALL_REQUIRES != in_tox:\n                raise SystemExit('Missing/unordered pinned dependencies in tox.ini.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets information about this current console window.", "response": "def get_console_info(kernel32, handle):\n    \"\"\"Get information about this current console window (Windows only).\n\n    https://github.com/Robpol86/colorclass/blob/ab42da59/colorclass/windows.py#L111\n\n    :raise OSError: When handle is invalid or GetConsoleScreenBufferInfo API call fails.\n\n    :param ctypes.windll.kernel32 kernel32: Loaded kernel32 instance.\n    :param int handle: stderr or stdout handle.\n\n    :return: Width (number of characters) and height (number of lines) of the terminal.\n    :rtype: tuple\n    \"\"\"\n    if handle == INVALID_HANDLE_VALUE:\n        raise OSError('Invalid handle.')\n\n    # Query Win32 API.\n    lpcsbi = ctypes.create_string_buffer(22)  # Populated by GetConsoleScreenBufferInfo.\n    if not kernel32.GetConsoleScreenBufferInfo(handle, lpcsbi):\n        raise ctypes.WinError()  # Subclass of OSError.\n\n    # Parse data.\n    left, top, right, bottom = struct.unpack('hhhhHhhhhhh', lpcsbi.raw)[5:-2]\n    width, height = right - left, bottom - top\n    return width, height"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the width and height of the terminal.", "response": "def terminal_size(kernel32=None):\n    \"\"\"Get the width and height of the terminal.\n\n    http://code.activestate.com/recipes/440694-determine-size-of-console-window-on-windows/\n    http://stackoverflow.com/questions/17993814/why-the-irrelevant-code-made-a-difference\n\n    :param kernel32: Optional mock kernel32 object. For testing.\n\n    :return: Width (number of characters) and height (number of lines) of the terminal.\n    :rtype: tuple\n    \"\"\"\n    if IS_WINDOWS:\n        kernel32 = kernel32 or ctypes.windll.kernel32\n        try:\n            return get_console_info(kernel32, kernel32.GetStdHandle(STD_ERROR_HANDLE))\n        except OSError:\n            try:\n                return get_console_info(kernel32, kernel32.GetStdHandle(STD_OUTPUT_HANDLE))\n            except OSError:\n                return DEFAULT_WIDTH, DEFAULT_HEIGHT\n\n    try:\n        device = __import__('fcntl').ioctl(0, __import__('termios').TIOCGWINSZ, '\\0\\0\\0\\0\\0\\0\\0\\0')\n    except IOError:\n        return DEFAULT_WIDTH, DEFAULT_HEIGHT\n    height, width = struct.unpack('hhhh', device)[:2]\n    return width, height"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the terminal title.", "response": "def set_terminal_title(title, kernel32=None):\n    \"\"\"Set the terminal title.\n\n    :param title: The title to set (string, unicode, bytes accepted).\n    :param kernel32: Optional mock kernel32 object. For testing.\n\n    :return: If title changed successfully (Windows only, always True on Linux/OSX).\n    :rtype: bool\n    \"\"\"\n    try:\n        title_bytes = title.encode('utf-8')\n    except AttributeError:\n        title_bytes = title\n\n    if IS_WINDOWS:\n        kernel32 = kernel32 or ctypes.windll.kernel32\n        try:\n            is_ascii = all(ord(c) < 128 for c in title)  # str/unicode.\n        except TypeError:\n            is_ascii = all(c < 128 for c in title)  # bytes.\n        if is_ascii:\n            return kernel32.SetConsoleTitleA(title_bytes) != 0\n        else:\n            return kernel32.SetConsoleTitleW(title) != 0\n\n    # Linux/OSX.\n    sys.stdout.write(b'\\033]0;' + title_bytes + b'\\007')\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns table string to be printed.", "response": "def table_server_timings():\n    \"\"\"Return table string to be printed.\"\"\"\n    table_data = [\n        [Color('{autogreen}<10ms{/autogreen}'), '192.168.0.100, 192.168.0.101'],\n        [Color('{autoyellow}10ms <= 100ms{/autoyellow}'), '192.168.0.102, 192.168.0.103'],\n        [Color('{autored}>100ms{/autored}'), '192.168.0.105'],\n    ]\n    table_instance = SingleTable(table_data)\n    table_instance.inner_heading_row_border = False\n    return table_instance.table"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning table string to be printed.", "response": "def table_server_status():\n    \"\"\"Return table string to be printed.\"\"\"\n    table_data = [\n        [Color('Low Space'), Color('{autocyan}Nominal Space{/autocyan}'), Color('Excessive Space')],\n        [Color('Low Load'), Color('Nominal Load'), Color('{autored}High Load{/autored}')],\n        [Color('{autocyan}Low Free RAM{/autocyan}'), Color('Nominal Free RAM'), Color('High Free RAM')],\n    ]\n    table_instance = SingleTable(table_data, '192.168.0.105')\n    table_instance.inner_heading_row_border = False\n    table_instance.inner_row_border = True\n    table_instance.justify_columns = {0: 'center', 1: 'center', 2: 'center'}\n    return table_instance.table"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning table string to be printed. Two tables on one line.", "response": "def table_abcd():\n    \"\"\"Return table string to be printed. Two tables on one line.\"\"\"\n    table_instance = SingleTable([['A', 'B'], ['C', 'D']])\n\n    # Get first table lines.\n    table_instance.outer_border = False\n    table_inner_borders = table_instance.table.splitlines()\n\n    # Get second table lines.\n    table_instance.outer_border = True\n    table_instance.inner_heading_row_border = False\n    table_instance.inner_column_border = False\n    table_outer_borders = table_instance.table.splitlines()\n\n    # Combine.\n    smallest, largest = sorted([table_inner_borders, table_outer_borders], key=len)\n    smallest += [''] * (len(largest) - len(smallest))  # Make both same size.\n    combined = list()\n    for i, row in enumerate(largest):\n        combined.append(row.ljust(10) + '          ' + smallest[i])\n    return '\\n'.join(combined)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the maximum width of a column based on the current terminal width.", "response": "def column_max_width(self, column_number):\n        \"\"\"Return the maximum width of a column based on the current terminal width.\n\n        :param int column_number: The column number to query.\n\n        :return: The max width of the column.\n        :rtype: int\n        \"\"\"\n        inner_widths = max_dimensions(self.table_data)[0]\n        outer_border = 2 if self.outer_border else 0\n        inner_border = 1 if self.inner_column_border else 0\n        padding = self.padding_left + self.padding_right\n        return column_max_width(inner_widths, column_number, outer_border, inner_border, padding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef table_width(self):\n        outer_widths = max_dimensions(self.table_data, self.padding_left, self.padding_right)[2]\n        outer_border = 2 if self.outer_border else 0\n        inner_border = 1 if self.inner_column_border else 0\n        return table_width(outer_widths, outer_border, inner_border)", "response": "Return the width of the table including padding and borders."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle the GitHub heading border.", "response": "def horizontal_border(self, _, outer_widths):\n        \"\"\"Handle the GitHub heading border.\n\n        E.g.:\n        |:---|:---:|---:|----|\n\n        :param _: Unused.\n        :param iter outer_widths: List of widths (with padding) for each column.\n\n        :return: Prepared border strings in a generator.\n        :rtype: iter\n        \"\"\"\n        horizontal = str(self.CHAR_INNER_HORIZONTAL)\n        left = self.CHAR_OUTER_LEFT_VERTICAL\n        intersect = self.CHAR_INNER_VERTICAL\n        right = self.CHAR_OUTER_RIGHT_VERTICAL\n\n        columns = list()\n        for i, width in enumerate(outer_widths):\n            justify = self.justify_columns.get(i)\n            width = max(3, width)  # Width should be at least 3 so justification can be applied.\n            if justify == 'left':\n                columns.append(':' + horizontal * (width - 1))\n            elif justify == 'right':\n                columns.append(horizontal * (width - 1) + ':')\n            elif justify == 'center':\n                columns.append(':' + horizontal * (width - 2) + ':')\n            else:\n                columns.append(horizontal * width)\n\n        return combine(columns, left, intersect, right)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine everything and yield every line of the entire table with borders.", "response": "def gen_table(self, inner_widths, inner_heights, outer_widths):\n        \"\"\"Combine everything and yield every line of the entire table with borders.\n\n        :param iter inner_widths: List of widths (no padding) for each column.\n        :param iter inner_heights: List of heights (no padding) for each row.\n        :param iter outer_widths: List of widths (with padding) for each column.\n        :return:\n        \"\"\"\n        for i, row in enumerate(self.table_data):\n            # Yield the row line by line (e.g. multi-line rows).\n            for line in self.gen_row_lines(row, 'row', inner_widths, inner_heights[i]):\n                yield line\n            # Yield heading separator.\n            if i == 0:\n                yield self.horizontal_border(None, outer_widths)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the visible width of a unicode string.", "response": "def visible_width(string):\n    \"\"\"Get the visible width of a unicode string.\n\n    Some CJK unicode characters are more than one byte unlike ASCII and latin unicode characters.\n\n    From: https://github.com/Robpol86/terminaltables/pull/9\n\n    :param str string: String to measure.\n\n    :return: String's width.\n    :rtype: int\n    \"\"\"\n    if '\\033' in string:\n        string = RE_COLOR_ANSI.sub('', string)\n\n    # Convert to unicode.\n    try:\n        string = string.decode('u8')\n    except (AttributeError, UnicodeEncodeError):\n        pass\n\n    width = 0\n    for char in string:\n        if unicodedata.east_asian_width(char) in ('F', 'W'):\n            width += 2\n        else:\n            width += 1\n\n    return width"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef align_and_pad_cell(string, align, inner_dimensions, padding, space=' '):\n    if not hasattr(string, 'splitlines'):\n        string = str(string)\n\n    # Handle trailing newlines or empty strings, str.splitlines() does not satisfy.\n    lines = string.splitlines() or ['']\n    if string.endswith('\\n'):\n        lines.append('')\n\n    # Vertically align and pad.\n    if 'bottom' in align:\n        lines = ([''] * (inner_dimensions[1] - len(lines) + padding[2])) + lines + ([''] * padding[3])\n    elif 'middle' in align:\n        delta = inner_dimensions[1] - len(lines)\n        lines = ([''] * (delta // 2 + delta % 2 + padding[2])) + lines + ([''] * (delta // 2 + padding[3]))\n    else:\n        lines = ([''] * padding[2]) + lines + ([''] * (inner_dimensions[1] - len(lines) + padding[3]))\n\n    # Horizontally align and pad.\n    for i, line in enumerate(lines):\n        new_width = inner_dimensions[0] + len(line) - visible_width(line)\n        if 'right' in align:\n            lines[i] = line.rjust(padding[0] + new_width, space) + (space * padding[1])\n        elif 'center' in align:\n            lines[i] = (space * padding[0]) + line.center(new_width, space) + (space * padding[1])\n        else:\n            lines[i] = (space * padding[0]) + line.ljust(new_width + padding[1], space)\n\n    return lines", "response": "Align a string horizontally and vertically. Also add additional padding in both dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef max_dimensions(table_data, padding_left=0, padding_right=0, padding_top=0, padding_bottom=0):\n    inner_widths = [0] * (max(len(r) for r in table_data) if table_data else 0)\n    inner_heights = [0] * len(table_data)\n\n    # Find max width and heights.\n    for j, row in enumerate(table_data):\n        for i, cell in enumerate(row):\n            if not hasattr(cell, 'count') or not hasattr(cell, 'splitlines'):\n                cell = str(cell)\n            if not cell:\n                continue\n            inner_heights[j] = max(inner_heights[j], cell.count('\\n') + 1)\n            inner_widths[i] = max(inner_widths[i], *[visible_width(l) for l in cell.splitlines()])\n\n    # Calculate with padding.\n    outer_widths = [padding_left + i + padding_right for i in inner_widths]\n    outer_heights = [padding_top + i + padding_bottom for i in inner_heights]\n\n    return inner_widths, inner_heights, outer_widths, outer_heights", "response": "Get maximum width and height of each column and row."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef column_max_width(inner_widths, column_number, outer_border, inner_border, padding):\n    column_count = len(inner_widths)\n    terminal_width = terminal_size()[0]\n\n    # Count how much space padding, outer, and inner borders take up.\n    non_data_space = outer_border\n    non_data_space += inner_border * (column_count - 1)\n    non_data_space += column_count * padding\n\n    # Exclude selected column's width.\n    data_space = sum(inner_widths) - inner_widths[column_number]\n\n    return terminal_width - data_space - non_data_space", "response": "Determine the maximum width of a column based on the current terminal width."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef table_width(outer_widths, outer_border, inner_border):\n    column_count = len(outer_widths)\n\n    # Count how much space outer and inner borders take up.\n    non_data_space = outer_border\n    if column_count:\n        non_data_space += inner_border * (column_count - 1)\n\n    # Space of all columns and their padding.\n    data_space = sum(outer_widths)\n    return data_space + non_data_space", "response": "Determine the width of the entire table including borders and padding."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding any kind of horizontal border for the table.", "response": "def horizontal_border(self, style, outer_widths):\n        \"\"\"Build any kind of horizontal border for the table.\n\n        :param str style: Type of border to return.\n        :param iter outer_widths: List of widths (with padding) for each column.\n\n        :return: Prepared border as a tuple of strings.\n        :rtype: tuple\n        \"\"\"\n        if style == 'top':\n            horizontal = self.CHAR_OUTER_TOP_HORIZONTAL\n            left = self.CHAR_OUTER_TOP_LEFT\n            intersect = self.CHAR_OUTER_TOP_INTERSECT if self.inner_column_border else ''\n            right = self.CHAR_OUTER_TOP_RIGHT\n            title = self.title\n        elif style == 'bottom':\n            horizontal = self.CHAR_OUTER_BOTTOM_HORIZONTAL\n            left = self.CHAR_OUTER_BOTTOM_LEFT\n            intersect = self.CHAR_OUTER_BOTTOM_INTERSECT if self.inner_column_border else ''\n            right = self.CHAR_OUTER_BOTTOM_RIGHT\n            title = None\n        elif style == 'heading':\n            horizontal = self.CHAR_H_INNER_HORIZONTAL\n            left = self.CHAR_H_OUTER_LEFT_INTERSECT if self.outer_border else ''\n            intersect = self.CHAR_H_INNER_INTERSECT if self.inner_column_border else ''\n            right = self.CHAR_H_OUTER_RIGHT_INTERSECT if self.outer_border else ''\n            title = None\n        elif style == 'footing':\n            horizontal = self.CHAR_F_INNER_HORIZONTAL\n            left = self.CHAR_F_OUTER_LEFT_INTERSECT if self.outer_border else ''\n            intersect = self.CHAR_F_INNER_INTERSECT if self.inner_column_border else ''\n            right = self.CHAR_F_OUTER_RIGHT_INTERSECT if self.outer_border else ''\n            title = None\n        else:\n            horizontal = self.CHAR_INNER_HORIZONTAL\n            left = self.CHAR_OUTER_LEFT_INTERSECT if self.outer_border else ''\n            intersect = self.CHAR_INNER_INTERSECT if self.inner_column_border else ''\n            right = self.CHAR_OUTER_RIGHT_INTERSECT if self.outer_border else ''\n            title = None\n        return build_border(outer_widths, horizontal, left, intersect, right, title)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_table(self, inner_widths, inner_heights, outer_widths):\n        # Yield top border.\n        if self.outer_border:\n            yield self.horizontal_border('top', outer_widths)\n\n        # Yield table body.\n        row_count = len(self.table_data)\n        last_row_index, before_last_row_index = row_count - 1, row_count - 2\n        for i, row in enumerate(self.table_data):\n            # Yield the row line by line (e.g. multi-line rows).\n            if self.inner_heading_row_border and i == 0:\n                style = 'heading'\n            elif self.inner_footing_row_border and i == last_row_index:\n                style = 'footing'\n            else:\n                style = 'row'\n            for line in self.gen_row_lines(row, style, inner_widths, inner_heights[i]):\n                yield line\n            # If this is the last row then break. No separator needed.\n            if i == last_row_index:\n                break\n            # Yield heading separator.\n            if self.inner_heading_row_border and i == 0:\n                yield self.horizontal_border('heading', outer_widths)\n            # Yield footing separator.\n            elif self.inner_footing_row_border and i == before_last_row_index:\n                yield self.horizontal_border('footing', outer_widths)\n            # Yield row separator.\n            elif self.inner_row_border:\n                yield self.horizontal_border('row', outer_widths)\n\n        # Yield bottom border.\n        if self.outer_border:\n            yield self.horizontal_border('bottom', outer_widths)", "response": "Combine everything and yield every line of the entire table with borders."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef table(self):\n        dimensions = max_dimensions(self.table_data, self.padding_left, self.padding_right)[:3]\n        return flatten(self.gen_table(*dimensions))", "response": "Return a large string of the entire table ready to be printed to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef table(self):\n        ascii_table = super(UnixTable, self).table\n        optimized = ascii_table.replace('\\033(B\\033(0', '')\n        return optimized", "response": "Return a large string of the entire table ready to be printed to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sigmoidal(arr, contrast, bias):\n    if (arr.max() > 1.0 + epsilon) or (arr.min() < 0 - epsilon):\n        raise ValueError(\"Input array must have float values between 0 and 1\")\n\n    if (bias > 1.0 + epsilon) or (bias < 0 - epsilon):\n        raise ValueError(\"bias must be a scalar float between 0 and 1\")\n\n    alpha, beta = bias, contrast\n    # We use the names a and b to match documentation.\n\n    if alpha == 0:\n        alpha = epsilon\n\n    if beta == 0:\n        return arr\n\n    np.seterr(divide=\"ignore\", invalid=\"ignore\")\n\n    if beta > 0:\n        numerator = 1 / (1 + np.exp(beta * (alpha - arr))) - 1 / (\n            1 + np.exp(beta * alpha)\n        )\n        denominator = 1 / (1 + np.exp(beta * (alpha - 1))) - 1 / (\n            1 + np.exp(beta * alpha)\n        )\n        output = numerator / denominator\n\n    else:\n        # Inverse sigmoidal function:\n        # todo: account for 0s\n        # todo: formatting ;)\n        output = (\n            (beta * alpha)\n            - np.log(\n                (\n                    1\n                    / (\n                        (arr / (1 + np.exp(beta * alpha - beta)))\n                        - (arr / (1 + np.exp(beta * alpha)))\n                        + (1 / (1 + np.exp(beta * alpha)))\n                    )\n                )\n                - 1\n            )\n        ) / beta\n\n    return output", "response": "r This function applies a sigmoidal transfer function to the internal contrast of the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gamma(arr, g):\n    if (arr.max() > 1.0 + epsilon) or (arr.min() < 0 - epsilon):\n        raise ValueError(\"Input array must have float values between 0 and 1\")\n    if g <= 0 or np.isnan(g):\n        raise ValueError(\"gamma must be greater than 0\")\n\n    return arr ** (1.0 / g)", "response": "r Gamma correction function for nonlinear image"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a simple atmospheric correction formula.", "response": "def simple_atmo_opstring(haze, contrast, bias):\n    \"\"\"Make a simple atmospheric correction formula.\"\"\"\n    gamma_b = 1 - haze\n    gamma_g = 1 - (haze / 3.0)\n    ops = (\n        \"gamma g {gamma_g}, \" \"gamma b {gamma_b}, \" \"sigmoidal rgb {contrast} {bias}\"\n    ).format(gamma_g=gamma_g, gamma_b=gamma_b, contrast=contrast, bias=bias)\n    return ops"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an operation function closure don t call directly use parse_operations returns a function which takes and returns ndarrays", "response": "def _op_factory(func, kwargs, opname, bands, rgb_op=False):\n    \"\"\"create an operation function closure\n    don't call directly, use parse_operations\n    returns a function which itself takes and returns ndarrays\n    \"\"\"\n\n    def f(arr):\n        # Avoid mutation by copying\n        newarr = arr.copy()\n        if rgb_op:\n            # apply func to array's first 3 bands, assumed r,g,b\n            # additional band(s) are untouched\n            newarr[0:3] = func(newarr[0:3], **kwargs)\n        else:\n            # apply func to array band at a time\n            for b in bands:\n                newarr[b - 1] = func(arr[b - 1], **kwargs)\n        return newarr\n\n    f.__name__ = str(opname)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_operations(ops_string):\n    band_lookup = {\"r\": 1, \"g\": 2, \"b\": 3}\n    count = len(band_lookup)\n\n    opfuncs = {\"saturation\": saturation, \"sigmoidal\": sigmoidal, \"gamma\": gamma}\n\n    opkwargs = {\n        \"saturation\": (\"proportion\",),\n        \"sigmoidal\": (\"contrast\", \"bias\"),\n        \"gamma\": (\"g\",),\n    }\n\n    # Operations that assume RGB colorspace\n    rgb_ops = (\"saturation\",)\n\n    # split into tokens, commas are optional whitespace\n    tokens = [x.strip() for x in ops_string.replace(\",\", \"\").split(\" \")]\n    operations = []\n    current = []\n    for token in tokens:\n        if token.lower() in opfuncs.keys():\n            if len(current) > 0:\n                operations.append(current)\n                current = []\n        current.append(token.lower())\n    if len(current) > 0:\n        operations.append(current)\n\n    result = []\n    for parts in operations:\n        opname = parts[0]\n        bandstr = parts[1]\n        args = parts[2:]\n\n        try:\n            func = opfuncs[opname]\n        except KeyError:\n            raise ValueError(\"{} is not a valid operation\".format(opname))\n\n        if opname in rgb_ops:\n            # ignore bands, assumed to be in rgb\n            # push 2nd arg into args\n            args = [bandstr] + args\n            bands = (1, 2, 3)\n        else:\n            # 2nd arg is bands\n            # parse r,g,b ~= 1,2,3\n            bands = set()\n            for bs in bandstr:\n                try:\n                    band = int(bs)\n                except ValueError:\n                    band = band_lookup[bs.lower()]\n                if band < 1 or band > count:\n                    raise ValueError(\n                        \"{} BAND must be between 1 and {}\".format(opname, count)\n                    )\n                bands.add(band)\n\n        # assume all args are float\n        args = [float(arg) for arg in args]\n        kwargs = dict(zip(opkwargs[opname], args))\n\n        # Create opperation function\n        f = _op_factory(\n            func=func,\n            kwargs=kwargs,\n            opname=opname,\n            bands=bands,\n            rgb_op=(opname in rgb_ops),\n        )\n        result.append(f)\n\n    return result", "response": "Takes a string of operations written with a handy DSL\n    OPERATION - NAME BANDS ARG1 ARG2 ARG3"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_jobs(jobs):\n    if jobs == 0:\n        raise click.UsageError(\"Jobs must be >= 1 or == -1\")\n    elif jobs < 0:\n        import multiprocessing\n\n        jobs = multiprocessing.cpu_count()\n    return jobs", "response": "Validate number of jobs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef color(ctx, jobs, out_dtype, src_path, dst_path, operations, creation_options):\n    with rasterio.open(src_path) as src:\n        opts = src.profile.copy()\n        windows = [(window, ij) for ij, window in src.block_windows()]\n\n    opts.update(**creation_options)\n    opts[\"transform\"] = guard_transform(opts[\"transform\"])\n\n    out_dtype = out_dtype if out_dtype else opts[\"dtype\"]\n    opts[\"dtype\"] = out_dtype\n\n    args = {\"ops_string\": \" \".join(operations), \"out_dtype\": out_dtype}\n    # Just run this for validation this time\n    # parsing will be run again within the worker\n    # where its returned value will be used\n    try:\n        parse_operations(args[\"ops_string\"])\n    except ValueError as e:\n        raise click.UsageError(str(e))\n\n    jobs = check_jobs(jobs)\n\n    if jobs > 1:\n        with riomucho.RioMucho(\n            [src_path],\n            dst_path,\n            color_worker,\n            windows=windows,\n            options=opts,\n            global_args=args,\n            mode=\"manual_read\",\n        ) as mucho:\n            mucho.run(jobs)\n    else:\n        with rasterio.open(dst_path, \"w\", **opts) as dest:\n            with rasterio.open(src_path) as src:\n                rasters = [src]\n                for window, ij in windows:\n                    arr = color_worker(rasters, window, ij, args)\n                    dest.write(arr, window=window)\n\n                dest.colorinterp = src.colorinterp", "response": "Color corrections the image using the RioMucho color system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_math_type(arr):\n    max_int = np.iinfo(arr.dtype).max\n    return arr.astype(math_type) / max_int", "response": "Convert an array from native integer dtype range to 0.. 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert an array from 0.. 1 to dtype scaling up linearly", "response": "def scale_dtype(arr, dtype):\n    \"\"\"Convert an array from 0..1 to dtype, scaling up linearly\n    \"\"\"\n    max_int = np.iinfo(dtype).max\n    return (arr * max_int).astype(dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate a limited subset of imagemagick convert commands to rio color operations", "response": "def magick_to_rio(convert_opts):\n    \"\"\"Translate a limited subset of imagemagick convert commands\n    to rio color operations\n\n    Parameters\n    ----------\n    convert_opts: String, imagemagick convert options\n\n    Returns\n    -------\n    operations string, ordered rio color operations\n    \"\"\"\n    ops = []\n    bands = None\n\n    def set_band(x):\n        global bands\n        if x.upper() == \"RGB\":\n            x = \"RGB\"\n        bands = x.upper()\n\n    set_band(\"RGB\")\n\n    def append_sig(arg):\n        global bands\n        args = list(filter(None, re.split(\"[,x]+\", arg)))\n        if len(args) == 1:\n            args.append(0.5)\n        elif len(args) == 2:\n            args[1] = float(args[1].replace(\"%\", \"\")) / 100.0\n        ops.append(\"sigmoidal {} {} {}\".format(bands, *args))\n\n    def append_gamma(arg):\n        global bands\n        ops.append(\"gamma {} {}\".format(bands, arg))\n\n    def append_sat(arg):\n        args = list(filter(None, re.split(\"[,x]+\", arg)))\n        # ignore args[0]\n        # convert to proportion\n        prop = float(args[1]) / 100\n        ops.append(\"saturation {}\".format(prop))\n\n    nextf = None\n    for part in convert_opts.strip().split(\" \"):\n        if part == \"-channel\":\n            nextf = set_band\n        elif part == \"+channel\":\n            set_band(\"RGB\")\n            nextf = None\n        elif part == \"-sigmoidal-contrast\":\n            nextf = append_sig\n        elif part == \"-gamma\":\n            nextf = append_gamma\n        elif part == \"-modulate\":\n            nextf = append_sat\n        else:\n            if nextf:\n                nextf(part)\n            nextf = None\n\n    return \" \".join(ops)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef time_string(seconds):\n    s = int(round(seconds))  # round to nearest second\n    h, s = divmod(s, 3600)  # get hours and remainder\n    m, s = divmod(s, 60)  # split remainder into minutes and seconds\n    return \"%2i:%02i:%02i\" % (h, m, s)", "response": "Returns time in seconds as a string formatted HHHHMMSS."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef progress_report(\n    curr, best, curr_score, best_score, step, totalsteps, accept, improv, elaps, remain\n):\n    \"\"\"Report progress\"\"\"\n    text = \"\"\"\nCurrent Formula    {curr}   (hist distance {curr_score})\nBest Formula       {best}   (hist distance {best_score})\nStep {step} of {totalsteps}\nAcceptance Rate : {accept} %\nImprovement Rate: {improv} %\nTime  {elaps} ( {remain} Remaing)\"\"\".format(\n        **locals()\n    )\n    return text", "response": "Report the progress of a single resource in a single resource group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef histogram_distance(arr1, arr2, bins=None):\n    eps = 1e-6\n    assert arr1.min() > 0 - eps\n    assert arr1.max() < 1 + eps\n    assert arr2.min() > 0 - eps\n    assert arr2.max() < 1 + eps\n\n    if not bins:\n        bins = [x / 10 for x in range(11)]\n    hist1 = np.histogram(arr1, bins=bins)[0] / arr1.size\n    hist2 = np.histogram(arr2, bins=bins)[0] / arr2.size\n\n    assert abs(hist1.sum() - 1.0) < eps\n    assert abs(hist2.sum() - 1.0) < eps\n\n    sqerr = (hist1 - hist2) ** 2\n    return sqerr.sum()", "response": "This function returns the sum of the squared error between two histograms"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a source image and a reference image, Find the rio color formula which results in an output with similar histogram to the reference image. Uses simulated annealing to determine optimal settings. Increase the --downsample option to speed things up. Increase the --steps to get better results (longer runtime).", "response": "def main(source, reference, downsample, steps, plot):\n    \"\"\"Given a source image and a reference image,\n    Find the rio color formula which results in an\n    output with similar histogram to the reference image.\n\n    Uses simulated annealing to determine optimal settings.\n\n    Increase the --downsample option to speed things up.\n    Increase the --steps to get better results (longer runtime).\n    \"\"\"\n    global fig, txt, imgs\n\n    click.echo(\"Reading source data...\", err=True)\n    with rasterio.open(source) as src:\n        if downsample is None:\n            ratio = calc_downsample(src.width, src.height)\n        else:\n            ratio = downsample\n        w = int(src.width // ratio)\n        h = int(src.height // ratio)\n        rgb = src.read((1, 2, 3), out_shape=(3, h, w))\n        orig_rgb = to_math_type(rgb)\n\n    click.echo(\"Reading reference data...\", err=True)\n    with rasterio.open(reference) as ref:\n        if downsample is None:\n            ratio = calc_downsample(ref.width, ref.height)\n        else:\n            ratio = downsample\n        w = int(ref.width / ratio)\n        h = int(ref.height / ratio)\n        rgb = ref.read((1, 2, 3), out_shape=(3, h, w))\n        ref_rgb = to_math_type(rgb)\n\n    click.echo(\"Annealing...\", err=True)\n    est = ColorEstimator(orig_rgb, ref_rgb)\n\n    if plot:\n        import matplotlib.pyplot as plt\n\n        fig = plt.figure(figsize=(20, 10))\n        fig.suptitle(\"Color Formula Optimization\", fontsize=18, fontweight=\"bold\")\n        txt = fig.text(0.02, 0.05, \"foo\", family=\"monospace\", fontsize=16)\n        type(txt)\n        axs = (\n            fig.add_subplot(1, 4, 1),\n            fig.add_subplot(1, 4, 2),\n            fig.add_subplot(1, 4, 3),\n            fig.add_subplot(1, 4, 4),\n        )\n        fig.tight_layout()\n        axs[0].set_title(\"Source\")\n        axs[1].set_title(\"Current Formula\")\n        axs[2].set_title(\"Best Formula\")\n        axs[3].set_title(\"Reference\")\n        imgs.append(axs[0].imshow(reshape_as_image(est.src)))\n        imgs.append(axs[1].imshow(reshape_as_image(est.src)))\n        imgs.append(axs[2].imshow(reshape_as_image(est.src)))\n        imgs.append(axs[3].imshow(reshape_as_image(est.ref)))\n        fig.show()\n\n    schedule = dict(\n        tmax=25.0,  # Max (starting) temperature\n        tmin=1e-4,  # Min (ending) temperature\n        steps=steps,  # Number of iterations\n        updates=steps / 20,  # Number of updates\n    )\n\n    est.set_schedule(schedule)\n    est.save_state_on_exit = False\n    optimal, score = est.anneal()\n    optimal[\"energy\"] = score\n    ops = est.cmd(optimal)\n    click.echo(\"rio color -j4 {} {} {}\".format(source, \"/tmp/output.tif\", ops))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a state change.", "response": "def move(self):\n        \"\"\"Create a state change.\"\"\"\n        k = random.choice(self.keys)\n        multiplier = random.choice((0.95, 1.05))\n\n        invalid_key = True\n        while invalid_key:\n            # make sure bias doesn't exceed 1.0\n            if k == \"bias\":\n                if self.state[k] > 0.909:\n                    k = random.choice(self.keys)\n                    continue\n\n            invalid_key = False\n\n        newval = self.state[k] * multiplier\n        self.state[k] = newval"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_color(self, arr, state):\n        ops = self.cmd(state)\n        for func in parse_operations(ops):\n            arr = func(arr)\n        return arr", "response": "Apply color formula to an array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate state s energy.", "response": "def energy(self):\n        \"\"\"Calculate state's energy.\"\"\"\n        arr = self.src.copy()\n        arr = self.apply_color(arr, self.state)\n\n        scores = [histogram_distance(self.ref[i], arr[i]) for i in range(3)]\n\n        # Important: scale by 100 for readability\n        return sum(scores) * 100"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef atmos_worker(srcs, window, ij, args):\n    src = srcs[0]\n    rgb = src.read(window=window)\n    rgb = to_math_type(rgb)\n\n    atmos = simple_atmo(rgb, args[\"atmo\"], args[\"contrast\"], args[\"bias\"])\n\n    # should be scaled 0 to 1, scale to outtype\n    return scale_dtype(atmos, args[\"out_dtype\"])", "response": "A simple atmospheric correction user function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_circle(\n        self,\n        x0,\n        radius,\n        lcar=None,\n        R=None,\n        compound=False,\n        num_sections=3,\n        holes=None,\n        make_surface=True,\n    ):\n        \"\"\"Add circle in the :math:`x`-:math:`y`-plane.\n        \"\"\"\n        if holes is None:\n            holes = []\n        else:\n            assert make_surface\n\n        # Define points that make the circle (midpoint and the four cardinal\n        # directions).\n        X = numpy.zeros((num_sections + 1, len(x0)))\n        if num_sections == 4:\n            # For accuracy, the points are provided explicitly.\n            X[1:, [0, 1]] = numpy.array(\n                [[radius, 0.0], [0.0, radius], [-radius, 0.0], [0.0, -radius]]\n            )\n        else:\n            X[1:, [0, 1]] = numpy.array(\n                [\n                    [\n                        radius * numpy.cos(2 * numpy.pi * k / num_sections),\n                        radius * numpy.sin(2 * numpy.pi * k / num_sections),\n                    ]\n                    for k in range(num_sections)\n                ]\n            )\n\n        if R is not None:\n            assert numpy.allclose(\n                abs(numpy.linalg.eigvals(R)), numpy.ones(X.shape[1])\n            ), \"The transformation matrix doesn't preserve circles; at least one eigenvalue lies off the unit circle.\"\n            X = numpy.dot(X, R.T)\n\n        X += x0\n\n        # Add Gmsh Points.\n        p = [self.add_point(x, lcar=lcar) for x in X]\n\n        # Define the circle arcs.\n        arcs = [self.add_circle_arc(p[k], p[0], p[k + 1]) for k in range(1, len(p) - 1)]\n        arcs.append(self.add_circle_arc(p[-1], p[0], p[1]))\n\n        if compound:\n            if self._gmsh_major() == 3:\n                arcs = [self.add_compound_line(arcs)]\n            elif self._gmsh_major() == 4:\n                self.add_raw_code(\n                    \"Compound Curve{{{}}};\".format(\",\".join([arc.id for arc in arcs]))\n                )\n\n        line_loop = self.add_line_loop(arcs)\n\n        if make_surface:\n            plane_surface = self.add_plane_surface(line_loop, holes)\n            if compound and self._gmsh_major() == 4:\n                self.add_raw_code(\"Compound Surface{{{}}};\".format(plane_surface.id))\n        else:\n            plane_surface = None\n\n        class Circle(object):\n            def __init__(\n                self,\n                x0,\n                radius,\n                R,\n                compound,\n                num_sections,\n                holes,\n                line_loop,\n                plane_surface,\n                lcar=None,\n            ):\n                self.x0 = x0\n                self.radius = radius\n                self.lcar = lcar\n                self.R = R\n                self.compound = compound\n                self.num_sections = num_sections\n                self.holes = holes\n                self.line_loop = line_loop\n                self.plane_surface = plane_surface\n                return\n\n        return Circle(\n            x0,\n            radius,\n            R,\n            compound,\n            num_sections,\n            holes,\n            line_loop,\n            plane_surface,\n            lcar=lcar,\n        )", "response": "Add a circle to the current entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the string representation of the extrude operation for the given base - level entity.", "response": "def extrude(\n        self,\n        input_entity,\n        translation_axis=None,\n        rotation_axis=None,\n        point_on_axis=None,\n        angle=None,\n        num_layers=None,\n        recombine=False,\n    ):\n        \"\"\"Extrusion (translation + rotation) of any entity along a given\n        translation_axis, around a given rotation_axis, about a given angle. If\n        one of the entities is not provided, this method will produce only\n        translation or rotation.\n        \"\"\"\n        self._EXTRUDE_ID += 1\n\n        if _is_string(input_entity):\n            entity = Dummy(input_entity)\n        elif isinstance(input_entity, PointBase):\n            entity = Dummy(\"Point{{{}}}\".format(input_entity.id))\n        elif isinstance(input_entity, SurfaceBase):\n            entity = Dummy(\"Surface{{{}}}\".format(input_entity.id))\n        elif hasattr(input_entity, \"surface\"):\n            entity = Dummy(\"Surface{{{}}}\".format(input_entity.surface.id))\n        else:\n            assert isinstance(input_entity, LineBase), \"Illegal extrude entity.\"\n            entity = Dummy(\"Line{{{}}}\".format(input_entity.id))\n\n        extrusion_string = \"\"\n\n        # out[] = Extrude{0,1,0}{ Line{1}; };\n        name = \"ex{}\".format(self._EXTRUDE_ID)\n        if translation_axis is not None:\n            if rotation_axis is not None:\n                extrusion_string += \"{}[] = Extrude{{{{{}}}, {{{}}}, {{{}}}, {}}}{{{};\".format(\n                    name,\n                    \",\".join(repr(x) for x in translation_axis),\n                    \",\".join(repr(x) for x in rotation_axis),\n                    \",\".join(repr(x) for x in point_on_axis),\n                    angle,\n                    entity.id,\n                )\n            else:\n                # Only translation\n                extrusion_string += \"{}[] = Extrude {{{}}} {{{};\".format(\n                    name, \",\".join(repr(x) for x in translation_axis), entity.id\n                )\n        else:\n            assert (\n                rotation_axis is not None\n            ), \"Specify at least translation or rotation.\"\n            # Only rotation\n            extrusion_string += \"{}[] = Extrude{{{{{}}}, {{{}}}, {}}}{{{};\".format(\n                name,\n                \",\".join(repr(x) for x in rotation_axis),\n                \",\".join(repr(x) for x in point_on_axis),\n                angle,\n                entity.id,\n            )\n\n        if num_layers is not None:\n            extrusion_string += \" Layers{{{}}}; {}\".format(\n                num_layers, \"Recombine;\" if recombine else \"\"\n            )\n\n        # close command\n        extrusion_string += \"};\"\n        self._GMSH_CODE.append(extrusion_string)\n\n        # From <https://www.manpagez.com/info/gmsh/gmsh-2.4.0/gmsh_66.php>:\n        #\n        # > In this last extrusion command we retrieved the volume number\n        # > programatically by saving the output of the command into a\n        # > list. This list will contain the \"top\" of the extruded surface (in\n        # > out[0]) as well as the newly created volume (in out[1]).\n        #\n        top = \"{}[0]\".format(name)\n        extruded = \"{}[1]\".format(name)\n\n        if isinstance(input_entity, LineBase):\n            top = LineBase(top)\n            # A surface extruded from a single line has always 4 edges\n            extruded = SurfaceBase(extruded, 4)\n        elif isinstance(input_entity, SurfaceBase):\n            top = SurfaceBase(top, input_entity.num_edges)\n            extruded = VolumeBase(extruded)\n        elif isinstance(input_entity, PointBase):\n            top = PointBase(top)\n            extruded = LineBase(extruded)\n        else:\n            top = Dummy(top)\n            extruded = Dummy(extruded)\n\n        lat = []\n        # lateral surfaces can be deduced only if we start from a SurfaceBase\n        if isinstance(input_entity, SurfaceBase):\n            # out[0]` is the surface, out[1] the top, and everything after that\n            # the sides, cf.\n            # <https://gmsh.info/doc/texinfo/gmsh.html#Extrusions>. Each\n            # lateral surface has 4 edges: the one from input_entity, the one\n            # from top, and the two lines (or splines) connecting their extreme\n            # points.\n            lat = [\n                SurfaceBase(\"{}[{}]\".format(name, i + 2), 4)\n                for i in range(input_entity.num_edges)\n            ]\n\n        return top, extruded, lat"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_raw_code(self, string_or_list):\n        if _is_string(string_or_list):\n            self._GMSH_CODE.append(string_or_list)\n        else:\n            assert isinstance(string_or_list, list)\n            for string in string_or_list:\n                self._GMSH_CODE.append(string)\n        return", "response": "Add raw Gmsh code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an ellipsoid around a given midpoint.", "response": "def add_ellipsoid(self, x0, radii, lcar=None, with_volume=True, holes=None):\n        \"\"\"Creates an ellipsoid with radii around a given midpoint\n        :math:`x_0`.\n        \"\"\"\n        if holes is None:\n            holes = []\n\n        if holes:\n            assert with_volume\n\n        # Add points.\n        p = [\n            self.add_point(x0, lcar=lcar),\n            self.add_point([x0[0] + radii[0], x0[1], x0[2]], lcar=lcar),\n            self.add_point([x0[0], x0[1] + radii[1], x0[2]], lcar=lcar),\n            self.add_point([x0[0], x0[1], x0[2] + radii[2]], lcar=lcar),\n            self.add_point([x0[0] - radii[0], x0[1], x0[2]], lcar=lcar),\n            self.add_point([x0[0], x0[1] - radii[1], x0[2]], lcar=lcar),\n            self.add_point([x0[0], x0[1], x0[2] - radii[2]], lcar=lcar),\n        ]\n        # Add skeleton.\n        # Alternative for circles:\n        # `self.add_circle_arc(a, b, c)`\n        c = [\n            self.add_ellipse_arc(p[1], p[0], p[6], p[6]),\n            self.add_ellipse_arc(p[6], p[0], p[4], p[4]),\n            self.add_ellipse_arc(p[4], p[0], p[3], p[3]),\n            self.add_ellipse_arc(p[3], p[0], p[1], p[1]),\n            self.add_ellipse_arc(p[1], p[0], p[2], p[2]),\n            self.add_ellipse_arc(p[2], p[0], p[4], p[4]),\n            self.add_ellipse_arc(p[4], p[0], p[5], p[5]),\n            self.add_ellipse_arc(p[5], p[0], p[1], p[1]),\n            self.add_ellipse_arc(p[6], p[0], p[2], p[2]),\n            self.add_ellipse_arc(p[2], p[0], p[3], p[3]),\n            self.add_ellipse_arc(p[3], p[0], p[5], p[5]),\n            self.add_ellipse_arc(p[5], p[0], p[6], p[6]),\n        ]\n        # Add surfaces (1/8th of the ball surface).\n        ll = [\n            # one half\n            self.add_line_loop([c[4], c[9], c[3]]),\n            self.add_line_loop([c[8], -c[4], c[0]]),\n            self.add_line_loop([-c[9], c[5], c[2]]),\n            self.add_line_loop([-c[5], -c[8], c[1]]),\n            # the other half\n            self.add_line_loop([c[7], -c[3], c[10]]),\n            self.add_line_loop([c[11], -c[7], -c[0]]),\n            self.add_line_loop([-c[10], -c[2], c[6]]),\n            self.add_line_loop([-c[1], -c[6], -c[11]]),\n        ]\n        # Create a surface for each line loop.\n        s = [self.add_surface(l) for l in ll]\n\n        # Combine the surfaces to avoid seams\n        if self._gmsh_major() == 3:\n            s = [self.add_compound_surface(s[:4]), self.add_compound_surface(s[4:])]\n        else:\n            assert self._gmsh_major() == 4\n            # <https://gitlab.onelab.info/gmsh/gmsh/issues/507>\n            self.add_raw_code(\n                \"Compound Surface{{{}}};\".format(\",\".join([surf.id for surf in s[:4]]))\n            )\n            self.add_raw_code(\n                \"Compound Surface{{{}}};\".format(\",\".join([surf.id for surf in s[4:]]))\n            )\n\n        # Create the surface loop.\n        surface_loop = self.add_surface_loop(s)\n        # if holes:\n        #     # Create an array of surface loops; the first entry is the outer\n        #     # surface loop, the following ones are holes.\n        #     surface_loop = self.add_array([surface_loop] + holes)\n        # Create volume.\n        volume = self.add_volume(surface_loop, holes) if with_volume else None\n\n        class Ellipsoid(object):\n            dimension = 3\n\n            def __init__(self, x0, radii, surface_loop, volume, lcar=None):\n                self.x0 = x0\n                self.lcar = lcar\n                self.radii = radii\n                self.surface_loop = surface_loop\n                self.volume = volume\n                return\n\n        return Ellipsoid(x0, radii, surface_loop, volume, lcar=lcar)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_torus_extrude_lines(\n        self, irad, orad, lcar=None, R=numpy.eye(3), x0=numpy.array([0.0, 0.0, 0.0])\n    ):\n        \"\"\"Create Gmsh code for the torus in the x-y plane under the coordinate\n        transformation\n\n        .. math::\n            \\\\hat{x} = R x + x_0.\n\n        :param irad: inner radius of the torus\n        :param orad: outer radius of the torus\n        \"\"\"\n        self.add_comment(\"Torus\")\n\n        # Add circle\n        x0t = numpy.dot(R, numpy.array([0.0, orad, 0.0]))\n        # Get circles in y-z plane\n        Rc = numpy.array([[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]])\n        c = self.add_circle(x0 + x0t, irad, lcar=lcar, R=numpy.dot(R, Rc))\n\n        rot_axis = [0.0, 0.0, 1.0]\n        rot_axis = numpy.dot(R, rot_axis)\n        point_on_rot_axis = [0.0, 0.0, 0.0]\n        point_on_rot_axis = numpy.dot(R, point_on_rot_axis) + x0\n\n        # Form the torus by extruding the circle three times by 2/3*pi. This\n        # works around the inability of Gmsh to extrude by pi or more. The\n        # Extrude() macro returns an array; the first [0] entry in the array is\n        # the entity that has been extruded at the far end. This can be used\n        # for the following Extrude() step.  The second [1] entry of the array\n        # is the surface that was created by the extrusion.\n        previous = c.line_loop.lines\n        angle = \"2*Pi/3\"\n        all_surfaces = []\n        for i in range(3):\n            self.add_comment(\"Round no. {}\".format(i + 1))\n            for k, p in enumerate(previous):\n                # ts1[] = Extrude {{0,0,1}, {0,0,0}, 2*Pi/3}{Line{tc1};};\n                # ...\n                top, surf, _ = self.extrude(\n                    p,\n                    rotation_axis=rot_axis,\n                    point_on_axis=point_on_rot_axis,\n                    angle=angle,\n                )\n                all_surfaces.append(surf)\n                previous[k] = top\n\n        # compound_surface = CompoundSurface(all_surfaces)\n\n        surface_loop = self.add_surface_loop(all_surfaces)\n        vol = self.add_volume(surface_loop)\n\n        # The newline at the end is essential:\n        # If a GEO file doesn't end in a newline, Gmsh will report a syntax\n        # error.\n        self.add_comment(\"\\n\")\n        return vol", "response": "Create Gmsh code for the torus extrusion of the array of all surfaces."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_torus_extrude_circle(\n        self, irad, orad, lcar=None, R=numpy.eye(3), x0=numpy.array([0.0, 0.0, 0.0])\n    ):\n        \"\"\"Create Gmsh code for the torus under the coordinate transformation\n\n        .. math::\n            \\\\hat{x} = R x + x_0.\n\n        :param irad: inner radius of the torus\n        :param orad: outer radius of the torus\n        \"\"\"\n        self.add_comment(76 * \"-\")\n        self.add_comment(\"Torus\")\n\n        # Add circle\n        x0t = numpy.dot(R, numpy.array([0.0, orad, 0.0]))\n        Rc = numpy.array([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n        c = self.add_circle(x0 + x0t, irad, lcar=lcar, R=numpy.dot(R, Rc))\n\n        rot_axis = [0.0, 0.0, 1.0]\n        rot_axis = numpy.dot(R, rot_axis)\n        point_on_rot_axis = [0.0, 0.0, 0.0]\n        point_on_rot_axis = numpy.dot(R, point_on_rot_axis) + x0\n\n        # Form the torus by extruding the circle three times by 2/3*pi. This\n        # works around the inability of Gmsh to extrude by pi or more. The\n        # Extrude() macro returns an array; the first [0] entry in the array is\n        # the entity that has been extruded at the far end. This can be used\n        # for the following Extrude() step.  The second [1] entry of the array\n        # is the surface that was created by the extrusion. The third [2-end]\n        # is a list of all the planes of the lateral surface.\n        previous = c.plane_surface\n        all_volumes = []\n        num_steps = 3\n        for _ in range(num_steps):\n            top, vol, _ = self.extrude(\n                previous,\n                rotation_axis=rot_axis,\n                point_on_axis=point_on_rot_axis,\n                angle=\"2*Pi/{}\".format(num_steps),\n            )\n            previous = top\n            all_volumes.append(vol)\n\n        if self._gmsh_major() == 3:\n            # This actually returns the volume, but the gmsh 4 version doesn't have that\n            # feature. Hence, for compatibility, also ditch it here.\n            self.add_compound_volume(all_volumes)\n        else:\n            assert self._gmsh_major() == 4\n            self.add_raw_code(\n                \"Compound Volume{{{}}};\".format(\",\".join(v.id for v in all_volumes))\n            )\n\n        self.add_comment(76 * \"-\" + \"\\n\")\n        return", "response": "Create Gmsh code for the torus extrusion circle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_pipe_by_rectangle_rotation(\n        self,\n        outer_radius,\n        inner_radius,\n        length,\n        R=numpy.eye(3),\n        x0=numpy.array([0.0, 0.0, 0.0]),\n        lcar=None,\n    ):\n        \"\"\"Hollow cylinder.\n        Define a rectangle, extrude it by rotation.\n        \"\"\"\n        self.add_comment(\"Define rectangle.\")\n        X = numpy.array(\n            [\n                [0.0, outer_radius, -0.5 * length],\n                [0.0, outer_radius, +0.5 * length],\n                [0.0, inner_radius, +0.5 * length],\n                [0.0, inner_radius, -0.5 * length],\n            ]\n        )\n        # Apply transformation.\n        X = [numpy.dot(R, x) + x0 for x in X]\n        # Create points set.\n        p = [self.add_point(x, lcar=lcar) for x in X]\n\n        # Define edges.\n        e = [\n            self.add_line(p[0], p[1]),\n            self.add_line(p[1], p[2]),\n            self.add_line(p[2], p[3]),\n            self.add_line(p[3], p[0]),\n        ]\n\n        rot_axis = [0.0, 0.0, 1.0]\n        rot_axis = numpy.dot(R, rot_axis)\n        point_on_rot_axis = [0.0, 0.0, 0.0]\n        point_on_rot_axis = numpy.dot(R, point_on_rot_axis) + x0\n\n        # Extrude all edges three times by 2*Pi/3.\n        previous = e\n        angle = \"2*Pi/3\"\n        all_surfaces = []\n        # com = []\n        self.add_comment(\"Extrude in 3 steps.\")\n        for i in range(3):\n            self.add_comment(\"Step {}\".format(i + 1))\n            for k, p in enumerate(previous):\n                # ts1[] = Extrude {{0,0,1}, {0,0,0}, 2*Pi/3}{Line{tc1};};\n                top, surf, _ = self.extrude(\n                    p,\n                    rotation_axis=rot_axis,\n                    point_on_axis=point_on_rot_axis,\n                    angle=angle,\n                )\n                # if k==0:\n                #     com.append(surf)\n                # else:\n                #     all_names.appends(surf)\n                all_surfaces.append(surf)\n                previous[k] = top\n        #\n        # cs = CompoundSurface(com)\n        # Now just add surface loop and volume.\n        # all_surfaces = all_names + [cs]\n        surface_loop = self.add_surface_loop(all_surfaces)\n        vol = self.add_volume(surface_loop)\n        return vol", "response": "Add a pipe to the current contour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_pipe_by_circle_extrusion(\n        self,\n        outer_radius,\n        inner_radius,\n        length,\n        R=numpy.eye(3),\n        x0=numpy.array([0.0, 0.0, 0.0]),\n        lcar=None,\n    ):\n        \"\"\"Hollow cylinder.\n        Define a ring, extrude it by translation.\n        \"\"\"\n        # Define ring which to Extrude by translation.\n        Rc = numpy.array([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n        c_inner = self.add_circle(\n            x0, inner_radius, lcar=lcar, R=numpy.dot(R, Rc), make_surface=False\n        )\n        circ = self.add_circle(\n            x0, outer_radius, lcar=lcar, R=numpy.dot(R, Rc), holes=[c_inner.line_loop]\n        )\n\n        # Now Extrude the ring surface.\n        _, vol, _ = self.extrude(\n            circ.plane_surface, translation_axis=numpy.dot(R, [length, 0, 0])\n        )\n        return vol", "response": "Add a pipe to the current cylinder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef translate(self, input_entity, vector):\n        d = {1: \"Line\", 2: \"Surface\", 3: \"Volume\"}\n        self._GMSH_CODE.append(\n            \"Translate {{{}}} {{ {}{{{}}}; }}\".format(\n                \", \".join([str(co) for co in vector]),\n                d[input_entity.dimension],\n                input_entity.id,\n            )\n        )\n        return", "response": "Translates input_entity itself by vector."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef symmetry(self, input_entity, coefficients, duplicate=True):\n        d = {1: \"Line\", 2: \"Surface\", 3: \"Volume\"}\n        entity = \"{}{{{}}};\".format(d[input_entity.dimension], input_entity.id)\n\n        if duplicate:\n            entity = \"Duplicata{{{}}}\".format(entity)\n\n        self._GMSH_CODE.append(\n            \"Symmetry {{{}}} {{{}}}\".format(\n                \", \".join([str(co) for co in coefficients]), entity\n            )\n        )\n        return", "response": "Transforms all elementary entities symmetrically to a plane."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _boolean_operation(\n        self,\n        operation,\n        input_entities,\n        tool_entities,\n        delete_first=True,\n        delete_other=True,\n    ):\n        \"\"\"Boolean operations, see\n        https://gmsh.info/doc/texinfo/gmsh.html#Boolean-operations input_entity\n        and tool_entity are called object and tool in gmsh documentation.\n        \"\"\"\n        self._BOOLEAN_ID += 1\n\n        # assert that all entities are of the same dimensionality\n        dim = None\n        legal_dim_types = {1: \"Line\", 2: \"Surface\", 3: \"Volume\"}\n        for ldt in legal_dim_types:\n            if input_entities[0].dimension == ldt:\n                dim = ldt\n                break\n        assert dim is not None, \"Illegal input type '{}' for Boolean operation.\".format(\n            type(input_entities[0])\n        )\n        for e in input_entities[1:] + tool_entities:\n            assert (\n                e.dimension == dim\n            ), \"Incompatible input type '{}' for Boolean operation.\".format(type(e))\n\n        name = \"bo{}\".format(self._BOOLEAN_ID)\n\n        input_delete = \"Delete;\" if delete_first else \"\"\n\n        tool_delete = \"Delete;\" if delete_other else \"\"\n\n        legal_dim_type = legal_dim_types[dim]\n\n        if input_entities:\n            formatted_input_entities = (\n                \";\".join([\"%s{%s}\" % (legal_dim_type, e.id) for e in input_entities])\n                + \";\"\n            )\n        else:\n            formatted_input_entities = \"\"\n\n        if tool_entities:\n            formatted_tool_entities = (\n                \";\".join([\"%s{%s}\" % (legal_dim_type, e.id) for e in tool_entities])\n                + \";\"\n            )\n        else:\n            formatted_tool_entities = \"\"\n\n        self._GMSH_CODE.append(\n            # I wonder what this line does in Lisp. ;)\n            # '{}[] = {}{{{} {{{}}}; {}}} {{{} {{{}}}; {}}};'\n            # .format(\n            #    name,\n            #    operation,\n            #    legal_dim_types[dim],\n            #    ';'.join(e.id for e in input_entities),\n            #    'Delete;' if delete_first else '',\n            #    legal_dim_types[dim],\n            #    ';'.join(e.id for e in tool_entities),\n            #    'Delete;' if delete_other else ''\n            #    ))\n            \"%(name)s[] = %(op)s{ %(ientities)s %(idelete)s } { %(tentities)s %(tdelete)s};\"\n            % {\n                \"name\": name,\n                \"op\": operation,\n                \"ientities\": formatted_input_entities,\n                \"idelete\": input_delete,\n                \"tentities\": formatted_tool_entities,\n                \"tdelete\": tool_delete,\n            }\n        )\n        mapping = {\"Line\": None, \"Surface\": SurfaceBase, \"Volume\": VolumeBase}\n        return mapping[legal_dim_types[dim]](id0=name, is_list=True)", "response": "Internal method that performs the Boolean operation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef boolean_intersection(self, entities, delete_first=True, delete_other=True):\n        assert len(entities) > 1\n        return self._boolean_operation(\n            \"BooleanIntersection\",\n            [entities[0]],\n            entities[1:],\n            delete_first=delete_first,\n            delete_other=delete_other,\n        )", "response": "Boolean intersection, see\n        https://gmsh.info/doc/texinfo/gmsh.html#Boolean-operations input_entity\n        and tool_entity are called object and tool in gmsh documentation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef boolean_union(self, entities, delete_first=True, delete_other=True):\n        out = self._boolean_operation(\n            \"BooleanUnion\",\n            [entities[0]],\n            entities[1:],\n            delete_first=delete_first,\n            delete_other=delete_other,\n        )\n        # Cannot add Compound Surface yet; see\n        # <https://gitlab.onelab.info/gmsh/gmsh/issues/525>.\n        # if compound:\n        #     self._GMSH_CODE.append(\"Compound Surface {{{}}};\".format(out.id))\n        return out", "response": "Boolean union of the specified set of entities."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rotation_matrix(u, theta):\n    assert numpy.isclose(numpy.inner(u, u), 1.0), \"the rotation axis must be unitary\"\n\n    # Cross-product matrix.\n    cpm = numpy.array([[0.0, -u[2], u[1]], [u[2], 0.0, -u[0]], [-u[1], u[0], 0.0]])\n    c = numpy.cos(theta)\n    s = numpy.sin(theta)\n    R = numpy.eye(3) * c + s * cpm + (1.0 - c) * numpy.outer(u, u)\n    return R", "response": "Return matrix that implements the rotation around the vector u by the angle theta."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a canonical mesh from a GeoJSON object.", "response": "def generate_mesh(\n    geo_object,\n    verbose=True,\n    dim=3,\n    prune_vertices=True,\n    prune_z_0=False,\n    remove_faces=False,\n    gmsh_path=None,\n    extra_gmsh_arguments=None,\n    # for debugging purposes:\n    geo_filename=None,\n    mesh_file_type=\"msh\",\n):\n    \"\"\"Return a meshio.Mesh, storing the mesh points, cells, and data,\n    generated by Gmsh from the `geo_object`, written to a temporary file,\n    and reread by `meshio`.\n\n    Gmsh's native \"msh\" format is ill-suited to fast I/O.  This can\n    greatly reduce the performance of pygmsh.  As alternatives, try\n    `mesh_file_type=`:\n\n    - \"vtk\"`, though Gmsh doesn't write the physical tags\n    to VTK <https://gitlab.onelab.info/gmsh/gmsh/issues/389> or\n\n    - `\"mesh\"`, though this only supports a few basic elements - \"line\",\n    \"triangle\", \"quad\", \"tetra\", \"hexahedron\" - and doesn't preserve\n    the `$PhysicalNames`, just the `int` tags.\n\n    \"\"\"\n    if extra_gmsh_arguments is None:\n        extra_gmsh_arguments = []\n\n    # For format \"mesh\", ask Gmsh to save the physical tags\n    # http://gmsh.info/doc/texinfo/gmsh.html#index-Mesh_002eSaveElementTagType\n    if mesh_file_type == \"mesh\":\n        extra_gmsh_arguments += [\"-string\", \"Mesh.SaveElementTagType=2;\"]\n\n    preserve_geo = geo_filename is not None\n    if geo_filename is None:\n        with tempfile.NamedTemporaryFile(suffix=\".geo\") as f:\n            geo_filename = f.name\n\n    with open(geo_filename, \"w\") as f:\n        f.write(geo_object.get_code())\n\n    # As of Gmsh 4.1.3, the mesh format options are\n    # ```\n    # auto, msh1, msh2, msh3, msh4, msh, unv, vtk, wrl, mail, stl, p3d, mesh, bdf, cgns,\n    # med, diff, ir3, inp, ply2, celum, su2, x3d, dat, neu, m, key\n    # ```\n    # Pick the correct filename suffix.\n    filename_suffix = \"msh\" if mesh_file_type[:3] == \"msh\" else mesh_file_type\n\n    with tempfile.NamedTemporaryFile(suffix=\".\" + filename_suffix) as handle:\n        msh_filename = handle.name\n\n    gmsh_executable = gmsh_path if gmsh_path is not None else _get_gmsh_exe()\n\n    args = [\n        \"-{}\".format(dim),\n        geo_filename,\n        \"-format\",\n        mesh_file_type,\n        \"-bin\",\n        \"-o\",\n        msh_filename,\n    ] + extra_gmsh_arguments\n\n    # https://stackoverflow.com/a/803421/353337\n    p = subprocess.Popen(\n        [gmsh_executable] + args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n    )\n    if verbose:\n        while True:\n            line = p.stdout.readline()\n            if not line:\n                break\n            print(line.decode(\"utf-8\"), end=\"\")\n\n    p.communicate()\n    assert p.returncode == 0, \"Gmsh exited with error (return code {}).\".format(\n        p.returncode\n    )\n\n    mesh = meshio.read(msh_filename)\n\n    if remove_faces:\n        # Only keep the cells of highest topological dimension; discard faces\n        # and such.\n        two_d_cells = set([\"triangle\", \"quad\"])\n        three_d_cells = set(\n            [\"tetra\", \"hexahedron\", \"wedge\", \"pyramid\", \"penta_prism\", \"hexa_prism\"]\n        )\n        if any(k in mesh.cells for k in three_d_cells):\n            keep_keys = three_d_cells.intersection(mesh.cells.keys())\n        elif any(k in mesh.cells for k in two_d_cells):\n            keep_keys = two_d_cells.intersection(mesh.cells.keys())\n        else:\n            keep_keys = mesh.cells.keys()\n\n        mesh.cells = {key: mesh.cells[key] for key in keep_keys}\n        mesh.cell_data = {key: mesh.cell_data[key] for key in keep_keys}\n\n    if prune_vertices:\n        # Make sure to include only those vertices which belong to a cell.\n        ncells = numpy.concatenate([numpy.concatenate(c) for c in mesh.cells.values()])\n        uvertices, uidx = numpy.unique(ncells, return_inverse=True)\n\n        k = 0\n        for key in mesh.cells.keys():\n            n = numpy.prod(mesh.cells[key].shape)\n            mesh.cells[key] = uidx[k : k + n].reshape(mesh.cells[key].shape)\n            k += n\n\n        mesh.points = mesh.points[uvertices]\n        for key in mesh.point_data:\n            mesh.point_data[key] = mesh.point_data[key][uvertices]\n\n    # clean up\n    os.remove(msh_filename)\n    if preserve_geo:\n        print(\"\\ngeo file: {}\".format(geo_filename))\n    else:\n        os.remove(geo_filename)\n\n    if (\n        prune_z_0\n        and mesh.points.shape[1] == 3\n        and numpy.all(numpy.abs(mesh.points[:, 2]) < 1.0e-13)\n    ):\n        mesh.points = mesh.points[:, :2]\n\n    return mesh"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getexistingdirectory(parent=None, caption='', basedir='',\r\n                         options=QFileDialog.ShowDirsOnly):\r\n    \"\"\"Wrapper around QtGui.QFileDialog.getExistingDirectory static method\r\n    Compatible with PyQt >=v4.4 (API #1 and #2) and PySide >=v1.0\"\"\"\r\n    # Calling QFileDialog static method\r\n    if sys.platform == \"win32\":\r\n        # On Windows platforms: redirect standard outputs\r\n        _temp1, _temp2 = sys.stdout, sys.stderr\r\n        sys.stdout, sys.stderr = None, None\r\n    try:\r\n        result = QFileDialog.getExistingDirectory(parent, caption, basedir,\r\n                                                  options)\r\n    finally:\r\n        if sys.platform == \"win32\":\r\n            # On Windows platforms: restore standard outputs\r\n            sys.stdout, sys.stderr = _temp1, _temp2\r\n    if not is_text_string(result):\r\n        # PyQt API #1\r\n        result = to_text_string(result)\r\n    return result", "response": "Wrapper around QtGui. QFileDialog. getExistingDirectory method\r\n            is deprecated in PyQt > = v1. 0 and PySide > v1. 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getopenfilename(parent=None, caption='', basedir='', filters='',\r\n                    selectedfilter='', options=None):\r\n    \"\"\"Wrapper around QtGui.QFileDialog.getOpenFileName static method\r\n    Returns a tuple (filename, selectedfilter) -- when dialog box is canceled,\r\n    returns a tuple of empty strings\r\n    Compatible with PyQt >=v4.4 (API #1 and #2) and PySide >=v1.0\"\"\"\r\n    return _qfiledialog_wrapper('getOpenFileName', parent=parent,\r\n                                caption=caption, basedir=basedir,\r\n                                filters=filters, selectedfilter=selectedfilter,\r\n                                options=options)", "response": "Wrapper around QtGui. QFileDialog. getOpenFileName static method\r\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap around QtGui. QFileDialog. getOpenFileNames static method", "response": "def getopenfilenames(parent=None, caption='', basedir='', filters='',\r\n                     selectedfilter='', options=None):\r\n    \"\"\"Wrapper around QtGui.QFileDialog.getOpenFileNames static method\r\n    Returns a tuple (filenames, selectedfilter) -- when dialog box is canceled,\r\n    returns a tuple (empty list, empty string)\r\n    Compatible with PyQt >=v4.4 (API #1 and #2) and PySide >=v1.0\"\"\"\r\n    return _qfiledialog_wrapper('getOpenFileNames', parent=parent,\r\n                                caption=caption, basedir=basedir,\r\n                                filters=filters, selectedfilter=selectedfilter,\r\n                                options=options)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap around QtGui. QFileDialog. getSaveFileName static method", "response": "def getsavefilename(parent=None, caption='', basedir='', filters='',\r\n                    selectedfilter='', options=None):\r\n    \"\"\"Wrapper around QtGui.QFileDialog.getSaveFileName static method\r\n    Returns a tuple (filename, selectedfilter) -- when dialog box is canceled,\r\n    returns a tuple of empty strings\r\n    Compatible with PyQt >=v4.4 (API #1 and #2) and PySide >=v1.0\"\"\"\r\n    return _qfiledialog_wrapper('getSaveFileName', parent=parent,\r\n                                caption=caption, basedir=basedir,\r\n                                filters=filters, selectedfilter=selectedfilter,\r\n                                options=options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_qcombobox(QComboBox):\n\n    from ..QtGui import QIcon\n    from ..QtCore import Qt, QObject\n\n    class userDataWrapper():\n        \"\"\"\n        This class is used to wrap any userData object. If we don't do this,\n        then certain types of objects can cause segmentation faults or issues\n        depending on whether/how __getitem__ is defined.\n        \"\"\"\n        def __init__(self, data):\n            self.data = data\n\n    _addItem = QComboBox.addItem\n\n    def addItem(self, *args, **kwargs):\n        if len(args) == 3 or (not isinstance(args[0], QIcon)\n                              and len(args) == 2):\n            args, kwargs['userData'] = args[:-1], args[-1]\n        if 'userData' in kwargs:\n            kwargs['userData'] = userDataWrapper(kwargs['userData'])\n        _addItem(self, *args, **kwargs)\n\n    _insertItem = QComboBox.insertItem\n\n    def insertItem(self, *args, **kwargs):\n        if len(args) == 4 or (not isinstance(args[1], QIcon)\n                              and len(args) == 3):\n            args, kwargs['userData'] = args[:-1], args[-1]\n        if 'userData' in kwargs:\n            kwargs['userData'] = userDataWrapper(kwargs['userData'])\n        _insertItem(self, *args, **kwargs)\n\n    _setItemData = QComboBox.setItemData\n\n    def setItemData(self, index, value, role=Qt.UserRole):\n        value = userDataWrapper(value)\n        _setItemData(self, index, value, role=role)\n\n    _itemData = QComboBox.itemData\n\n    def itemData(self, index, role=Qt.UserRole):\n        userData = _itemData(self, index, role=role)\n        if isinstance(userData, userDataWrapper):\n            userData = userData.data\n        return userData\n\n    def findData(self, value):\n        for i in range(self.count()):\n            if self.itemData(i) == value:\n                return i\n        return -1\n\n    QComboBox.addItem = addItem\n    QComboBox.insertItem = insertItem\n    QComboBox.setItemData = setItemData\n    QComboBox.itemData = itemData\n    QComboBox.findData = findData", "response": "Patch the QComboBox when we find a new version of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a CesiumJS scenario file.", "response": "def to_czml(\n    traffic: Union[Traffic, SO6],\n    filename: Union[str, Path],\n    minimum_time: Optional[timelike] = None,\n) -> None:\n    \"\"\"Generates a CesiumJS scenario file.\"\"\"\n\n    if isinstance(traffic, Traffic):\n        if \"baro_altitude\" in traffic.data.columns:\n            traffic = traffic.query(\"baro_altitude == baro_altitude\")\n        elif \"altitude\" in traffic.data.columns:\n            traffic = traffic.query(\"altitude == altitude\")\n\n    if minimum_time is not None:\n        minimum_time = to_datetime(minimum_time)\n        traffic = cast(Traffic, traffic.query(f\"timestamp >= '{minimum_time}'\"))\n\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    if not filename.parent.exists():\n        filename.parent.mkdir(parents=True)\n\n    start = format_ts(traffic.start_time)\n    availability = f\"{start}/{format_ts(traffic.end_time)}\"\n    export = [\n        {\n            \"id\": \"document\",\n            \"name\": f\"Traffic_{start}\",\n            \"version\": \"1.0\",\n            \"author\": getpass.getuser(),\n            \"clock\": {\n                \"interval\": availability,\n                \"currentTime\": start,\n                \"multiplier\": _CZML_Params.default_time_multiplier,\n            },\n        }\n    ]\n    for flight in traffic:\n        for elt in export_flight(flight):\n            export.append(elt)\n\n    with filename.open(\"w\") as fh:\n        json.dump(export, fh, indent=2)\n\n    logging.info(f\"Scenario file {filename} written\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef closest_point_of_approach(\n    traffic: Traffic,\n    lateral_separation: float,\n    vertical_separation: float,\n    projection: Union[pyproj.Proj, crs.Projection, None] = None,\n    round_t: str = \"d\",\n    max_workers: int = 4,\n) -> CPA:\n    \"\"\"\n    Computes a CPA dataframe for all pairs of trajectories candidates for\n    being separated by less than lateral_separation in vertical_separation.\n\n    In order to be computed efficiently, the method needs the following\n    parameters:\n\n    - projection: a first filtering is applied on the bounding boxes of\n    trajectories, expressed in meters. You need to provide a decent\n    projection able to approximate distances by Euclide formula.\n    By default, EuroPP() projection is considered, but a non explicit\n    argument will raise a warning.\n\n    - round_t: an additional column will be added in the DataFrame to group\n    trajectories by relevant time frames. Distance computations will be\n    considered only between trajectories flown in the same time frame.\n    By default, the 'd' pandas freq parameter is considered, to group\n    trajectories by day, but other ways of splitting ('h') may be more\n    relevant and impact performance.\n\n    - max_workers: distance computations are spread over a given number of\n    processors.\n\n    \"\"\"\n\n    if projection is None:\n        logging.warn(\"Defaulting to projection EuroPP()\")\n        projection = crs.EuroPP()\n\n    if isinstance(projection, crs.Projection):\n        projection = pyproj.Proj(projection.proj4_init)\n\n    def yield_pairs(t_chunk: Traffic):\n        \"\"\"\n        This function yields all pairs of possible candidates for a CPA\n        calculation.\n        \"\"\"\n\n        # combinations types Iterator[Tuple[T, ...]]\n        for first, second in cast(\n            Iterator[Tuple[Flight, Flight]], combinations(t_chunk, 2)\n        ):\n            # cast are necessary because of the lru_cache \u00d7 property bug\n            if (\n                cast(pd.Timestamp, first.start)\n                > cast(pd.Timestamp, second.stop)\n            ) or (\n                cast(pd.Timestamp, second.start)\n                > cast(pd.Timestamp, first.stop)\n            ):\n                # Flights must fly at the same time\n                continue\n            if (\n                first.min(\"altitude\")\n                > second.max(\"altitude\") + vertical_separation\n            ):\n                # Bounding boxes in altitude must cross\n                continue\n            if (\n                second.min(\"altitude\")\n                > first.max(\"altitude\") + vertical_separation\n            ):\n                # Bounding boxes in altitude must cross\n                continue\n            if first.min(\"x\") > second.max(\"x\") + lateral_separation:\n                # Bounding boxes in x must cross\n                continue\n            if second.min(\"x\") > first.max(\"x\") + lateral_separation:\n                # Bounding boxes in x must cross\n                continue\n            if first.min(\"y\") > second.max(\"y\") + lateral_separation:\n                # Bounding boxes in y must cross\n                continue\n            if second.min(\"y\") > first.max(\"y\") + lateral_separation:\n                # Bounding boxes in y must cross\n                continue\n\n            # Next step is to check the 2D footprint of the trajectories\n            # intersect. Before computing the intersection we bufferize the\n            # trajectories by half the requested separation.\n\n            first_shape = first.project_shape(projection)\n            second_shape = second.project_shape(projection)\n            if first_shape is None or second_shape is None:\n                continue\n\n            first_shape = first_shape.simplify(1e3).buffer(\n                lateral_separation / 2\n            )\n            second_shape = first_shape.simplify(1e3).buffer(\n                lateral_separation / 2\n            )\n\n            if first_shape.intersects(second_shape):\n                yield first, second\n\n    t_xyt = (\n        traffic.airborne()\n        .compute_xy(projection)\n        .assign(round_t=lambda df: df.timestamp.dt.round(round_t))\n    )\n\n    cumul = list()\n\n    # Multiprocessing is implemented on each timerange slot only.\n    # TODO: it would probably be more efficient to multiprocess over each\n    # t_chunk rather than multiprocess the distance computation.\n\n    for _, t_chunk in tqdm(\n        t_xyt.groupby(\"round_t\"), total=len(set(t_xyt.data.round_t))\n    ):\n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            tasks = {\n                # TODO submit(Flight.distance, first, second)\n                executor.submit(first.distance, second): (\n                    first.flight_id,\n                    second.flight_id,\n                )\n                for (first, second) in yield_pairs(Traffic(t_chunk))\n            }\n\n            for future in as_completed(tasks):\n                cumul.append(future.result())\n\n    return CPA(pd.concat(cumul, sort=False))", "response": "Computes a CPA dataframe for all possible point of approach."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the object in a scatter plot.", "response": "def plot(\n        self, ax: GeoAxesSubplot, cmap: str = \"inferno\", s: int = 5, **kwargs\n    ) -> Artist:\n        \"\"\"Plotting function. All arguments are passed to ax.scatter\"\"\"\n        return ax.scatter(\n            self.df.longitude,\n            self.df.latitude,\n            s=s,\n            transform=PlateCarree(),\n            c=-self.df.altitude,\n            cmap=cmap,\n            **kwargs,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots function. All arguments are passed to the geometry", "response": "def plot(self, ax: GeoAxesSubplot, **kwargs) -> Artist:\n        \"\"\"Plotting function. All arguments are passed to the geometry\"\"\"\n\n        if \"facecolor\" not in kwargs:\n            kwargs[\"facecolor\"] = \"None\"\n        if \"edgecolor\" not in kwargs:\n            kwargs[\"edgecolor\"] = ax._get_lines.get_next_color()\n\n        if \"projection\" in ax.__dict__:\n            return ax.add_geometries([self.shape], crs=PlateCarree(), **kwargs)\n        else:\n            return ax.add_patch(\n                MplPolygon(list(self.shape.exterior.coords), **kwargs)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef api_states(\n        self,\n        own: bool = False,\n        bounds: Union[\n            BaseGeometry, Tuple[float, float, float, float], None\n        ] = None,\n    ) -> StateVectors:\n        \"\"\"Returns the current state vectors from OpenSky REST API.\n\n        If own parameter is set to True, returns only the state vectors\n        associated to own sensors (requires authentication)\n\n        bounds parameter can be a shape or a tuple of float.\n\n        Official documentation\n        ----------------------\n\n        Limitiations for anonymous (unauthenticated) users\n\n        Anonymous are those users who access the API without using credentials.\n        The limitations for anonymous users are:\n\n        Anonymous users can only get the most recent state vectors, i.e. the\n        time parameter will be ignored.  Anonymous users can only retrieve data\n        with a time resultion of 10 seconds. That means, the API will return\n        state vectors for time now \u2212 (now mod 10)\n\n        Limitations for OpenSky users\n\n        An OpenSky user is anybody who uses a valid OpenSky account (see below)\n        to access the API. The rate limitations for OpenSky users are:\n\n        - OpenSky users can retrieve data of up to 1 hour in the past. If the\n        time parameter has a value t < now\u22123600 the API will return\n        400\u00a0Bad\u00a0Request.\n\n        - OpenSky users can retrieve data with a time resultion of 5 seconds.\n        That means, if the time parameter was set to t , the API will return\n        state vectors for time t\u2212(t mod 5).\n\n        \"\"\"\n\n        what = \"own\" if (own and self.auth is not None) else \"all\"\n\n        if bounds is not None:\n            try:\n                # thinking of shapely bounds attribute (in this order)\n                # I just don't want to add the shapely dependency here\n                west, south, east, north = bounds.bounds  # type: ignore\n            except AttributeError:\n                west, south, east, north = bounds\n\n            what += f\"?lamin={south}&lamax={north}&lomin={west}&lomax={east}\"\n\n        c = requests.get(\n            f\"https://opensky-network.org/api/states/{what}\", auth=self.auth\n        )\n\n        if c.status_code != 200:\n            raise ValueError(c.content.decode())\n        r = pd.DataFrame.from_records(\n            c.json()[\"states\"], columns=self._json_columns\n        )\n        r = r.drop([\"origin_country\", \"spi\", \"sensors\"], axis=1)\n        r = r.dropna()\n\n        return StateVectors(\n            self._format_dataframe(r, nautical_units=True), self\n        )", "response": "Returns the current state vectors from OpenSky REST API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef api_tracks(self, icao24: str) -> Flight:\n        c = requests.get(\n            f\"https://opensky-network.org/api/tracks/?icao24={icao24}\"\n        )\n        if c.status_code != 200:\n            raise ValueError(c.content.decode())\n        json = c.json()\n\n        df = pd.DataFrame.from_records(\n            json[\"path\"],\n            columns=[\n                \"timestamp\",\n                \"latitude\",\n                \"longitude\",\n                \"altitude\",\n                \"track\",\n                \"onground\",\n            ],\n        ).assign(icao24=json[\"icao24\"], callsign=json[\"callsign\"])\n        return Flight(self._format_dataframe(df, nautical_units=True))", "response": "Retrieves the trajectories for a given aircraft at a given time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the route associated to a callsign.", "response": "def api_routes(self, callsign: str) -> Tuple[Airport, ...]:\n        \"\"\"Returns the route associated to a callsign.\"\"\"\n        from .. import airports\n\n        c = requests.get(\n            f\"https://opensky-network.org/api/routes?callsign={callsign}\"\n        )\n        if c.status_code == 404:\n            raise ValueError(\"Unknown callsign\")\n        if c.status_code != 200:\n            raise ValueError(c.content.decode())\n        json = c.json()\n\n        return tuple(airports[a] for a in json[\"route\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef api_aircraft(\n        self,\n        icao24: str,\n        begin: Optional[timelike] = None,\n        end: Optional[timelike] = None,\n    ) -> pd.DataFrame:\n        \"\"\"Returns a flight table associated to an aircraft.\n\n        Official documentation\n        ----------------------\n\n        This API call retrieves flights for a particular aircraft within a\n        certain time interval. Resulting flights departed and arrived within\n        [begin, end]. If no flights are found for the given period, HTTP stats\n        404 - Not found is returned with an empty response body.\n\n        \"\"\"\n\n        if begin is None:\n            begin = round_time(datetime.now(timezone.utc), by=timedelta(days=1))\n        begin = to_datetime(begin)\n        if end is None:\n            end = begin + timedelta(days=1)\n        else:\n            end = to_datetime(end)\n\n        begin = int(begin.timestamp())\n        end = int(end.timestamp())\n\n        c = requests.get(\n            f\"https://opensky-network.org/api/flights/aircraft\"\n            f\"?icao24={icao24}&begin={begin}&end={end}\"\n        )\n        if c.status_code != 200:\n            raise ValueError(c.content.decode())\n        return (\n            pd.DataFrame.from_records(c.json())[\n                [\n                    \"firstSeen\",\n                    \"lastSeen\",\n                    \"icao24\",\n                    \"callsign\",\n                    \"estDepartureAirport\",\n                    \"estArrivalAirport\",\n                ]\n            ]\n            .assign(\n                firstSeen=lambda df: pd.to_datetime(\n                    df.firstSeen * 1e9\n                ).dt.tz_localize(\"utc\"),\n                lastSeen=lambda df: pd.to_datetime(\n                    df.lastSeen * 1e9\n                ).dt.tz_localize(\"utc\"),\n            )\n            .sort_values(\"lastSeen\")\n        )", "response": "This API endpoint returns a flight table associated to an aircraft."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef api_sensors(self) -> Set[str]:\n        today = round_time(datetime.now(timezone.utc), by=timedelta(days=1))\n        c = requests.get(\n            f\"https://opensky-network.org/api/sensor/myStats\"\n            f\"?days={int(today.timestamp())}\",\n            auth=self.auth,\n        )\n        if c.status_code != 200:\n            raise ValueError(c.content.decode())\n        return set(c.json()[0][\"stats\"].keys())", "response": "The set of sensors you own."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef api_range(\n        self, serial: str, date: Optional[timelike] = None\n    ) -> SensorRange:\n        \"\"\"Wraps a polygon representing a sensor's range.\n\n        By default, returns the current range. Otherwise, you may enter a\n        specific day (as a string, as an epoch or as a datetime)\n        \"\"\"\n\n        if date is None:\n            date = round_time(datetime.now(timezone.utc), by=timedelta(days=1))\n        else:\n            date = to_datetime(date)\n        date = int(date.timestamp())\n\n        c = requests.get(\n            f\"https://opensky-network.org/api/range/days\"\n            f\"?days={date}&serials={serial}\"\n        )\n        if c.status_code != 200:\n            raise ValueError(c.content.decode())\n        return SensorRange(c.json())", "response": "Wraps a polygon representing a sensor s range."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a DataFrame containing the flights for a certain airport.", "response": "def api_arrival(\n        self,\n        airport: Union[str, Airport],\n        begin: Optional[timelike] = None,\n        end: Optional[timelike] = None,\n    ) -> pd.DataFrame:\n        \"\"\"Returns a flight table associated to an airport.\n\n        By default, returns the current table. Otherwise, you may enter a\n        specific date (as a string, as an epoch or as a datetime)\n\n        Official documentation\n        ----------------------\n\n        Retrieve flights for a certain airport which arrived within a given time\n        interval [begin, end]. If no flights are found for the given period,\n        HTTP stats 404 - Not found is returned with an empty response body.\n\n        \"\"\"\n\n        if isinstance(airport, str):\n            from .. import airports\n\n            airport_code = airports[airport].icao\n        else:\n            airport_code = airport.icao\n\n        if begin is None:\n            begin = round_time(datetime.now(timezone.utc), by=timedelta(days=1))\n        begin = to_datetime(begin)\n        if end is None:\n            end = begin + timedelta(days=1)\n        else:\n            end = to_datetime(end)\n\n        begin = int(begin.timestamp())\n        end = int(end.timestamp())\n\n        c = requests.get(\n            f\"https://opensky-network.org/api/flights/arrival\"\n            f\"?begin={begin}&airport={airport_code}&end={end}\"\n        )\n\n        if c.status_code != 200:\n            raise ValueError(c.content.decode())\n\n        return (\n            pd.DataFrame.from_records(c.json())[\n                [\n                    \"firstSeen\",\n                    \"lastSeen\",\n                    \"icao24\",\n                    \"callsign\",\n                    \"estDepartureAirport\",\n                    \"estArrivalAirport\",\n                ]\n            ]\n            .assign(\n                firstSeen=lambda df: pd.to_datetime(\n                    df.firstSeen * 1e9\n                ).dt.tz_localize(\"utc\"),\n                lastSeen=lambda df: pd.to_datetime(\n                    df.lastSeen * 1e9\n                ).dt.tz_localize(\"utc\"),\n            )\n            .sort_values(\"lastSeen\")\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self) -> pd.Timestamp:\n        start = self.data.timestamp.min()\n        self.data = self.data.assign(start=start)\n        return start", "response": "Returns the minimum timestamp value of the DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the unique number value of the DataFrame.", "response": "def number(self) -> Optional[Union[str, Set[str]]]:\n        \"\"\"Returns the unique number value(s) of the DataFrame.\"\"\"\n        if \"number\" not in self.data.columns:\n            return None\n        if all(self.data.number.isna()):\n            return None\n        tmp = set(self.data.number)\n        if len(tmp) == 1:\n            return tmp.pop()\n        logging.warn(\"Several numbers for one flight, consider splitting\")\n        return tmp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flight_id(self) -> Optional[str]:\n        if \"flight_id\" not in self.data.columns:\n            return None\n        tmp = set(self.data.flight_id)\n        if len(tmp) != 1:\n            logging.warn(\"Several ids for one flight, consider splitting\")\n        return tmp.pop()", "response": "Returns the unique flight_id value of the DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef squawk(self) -> Set[str]:\n        return set(self.data.squawk.ffill().bfill())", "response": "Returns all the unique squawk values in the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the unique destination value of the DataFrame.", "response": "def destination(self) -> Optional[Union[str, Set[str]]]:\n        \"\"\"Returns the unique destination value(s) of the DataFrame.\n\n        The destination airport is mostly represented as a ICAO or a IATA code.\n        \"\"\"\n        if \"destination\" not in self.data.columns:\n            return None\n        tmp = set(self.data.destination)\n        if len(tmp) == 1:\n            return tmp.pop()\n        logging.warn(\"Several destinations for one flight, consider splitting\")\n        return tmp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the equivalent Flight from OpenSky History.", "response": "def query_opensky(self) -> Optional[\"Flight\"]:\n        \"\"\"Return the equivalent Flight from OpenSky History.\"\"\"\n        from ..data import opensky\n\n        query_params = {\n            \"start\": self.start,\n            \"stop\": self.stop,\n            \"callsign\": self.callsign,\n            \"icao24\": self.icao24,\n        }\n        return opensky.history(**query_params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the EHS of the current flight.", "response": "def query_ehs(\n        self,\n        data: Optional[pd.DataFrame] = None,\n        failure_mode: str = \"warning\",\n        progressbar: Optional[Callable[[Iterable], Iterable]] = None,\n    ) -> \"Flight\":\n        \"\"\"Extend data with extra columns from EHS messages.\n\n        By default, raw messages are requested from the OpenSky Impala server.\n\n        Making a lot of small requests can be very inefficient and may look\n        like a denial of service. If you get the raw messages using a different\n        channel, you can provide the resulting dataframe as a parameter.\n\n        The data parameter expect three colmuns: icao24, rawmsg and mintime, in\n        conformance with the OpenSky API.\n        \"\"\"\n        from ..data import opensky, ModeS_Decoder\n\n        if not isinstance(self.icao24, str):\n            raise RuntimeError(\"Several icao24 for this flight\")\n\n        def fail_warning():\n            \"\"\"Called when nothing can be added to data.\"\"\"\n            id_ = self.flight_id\n            if id_ is None:\n                id_ = self.callsign\n            logging.warn(f\"No data on Impala for flight {id_}.\")\n            return self\n\n        def fail_silent():\n            return self\n\n        failure_dict = dict(warning=fail_warning, silent=fail_silent)\n        failure = failure_dict[failure_mode]\n\n        if data is None:\n            df = opensky.extended(self.start, self.stop, icao24=self.icao24)\n        else:\n            df = data.query(\"icao24 == @self.icao24\").sort_values(\"mintime\")\n\n        if df is None:\n            return failure()\n\n        timestamped_df = df.sort_values(\"mintime\").assign(\n            timestamp=lambda df: df.mintime.dt.round(\"s\")\n        )\n\n        referenced_df = (\n            timestamped_df.merge(self.data, on=\"timestamp\", how=\"outer\")\n            .sort_values(\"timestamp\")\n            .rename(\n                columns=dict(\n                    altitude=\"alt\",\n                    altitude_y=\"alt\",\n                    groundspeed=\"spd\",\n                    track=\"trk\",\n                )\n            )[[\"timestamp\", \"alt\", \"spd\", \"trk\"]]\n            .ffill()\n            .drop_duplicates()  # bugfix! NEVER ERASE THAT LINE!\n            .merge(\n                timestamped_df[[\"timestamp\", \"icao24\", \"rawmsg\"]],\n                on=\"timestamp\",\n                how=\"right\",\n            )\n        )\n\n        identifier = (\n            self.flight_id if self.flight_id is not None else self.callsign\n        )\n        # who cares about default lat0, lon0 with EHS\n        decoder = ModeS_Decoder((0, 0))\n\n        if progressbar is None:\n            progressbar = lambda x: tqdm(  # noqa: E731\n                x,\n                total=referenced_df.shape[0],\n                desc=f\"{identifier}:\",\n                leave=False,\n            )\n\n        for _, line in progressbar(referenced_df.iterrows()):\n\n            decoder.process(\n                line.timestamp,\n                line.rawmsg,\n                spd=line.spd,\n                trk=line.trk,\n                alt=line.alt,\n            )\n\n        if decoder.traffic is None:\n            return failure()\n\n        extended = decoder.traffic[self.icao24]\n        if extended is None:\n            return failure()\n\n        # fix for https://stackoverflow.com/q/53657210/1595335\n        if \"last_position\" in self.data.columns:\n            extended = extended.assign(last_position=pd.NaT)\n        if \"start\" in self.data.columns:\n            extended = extended.assign(start=pd.NaT)\n        if \"stop\" in self.data.columns:\n            extended = extended.assign(stop=pd.NaT)\n\n        t = extended + self\n        if \"flight_id\" in self.data.columns:\n            t.data.flight_id = self.flight_id\n\n        # sometimes weird callsigns are decoded and should be discarded\n        # so it seems better to filter on callsign rather than on icao24\n        flight = t[self.callsign]\n        if flight is None:\n            return failure()\n\n        return flight.sort_values(\"timestamp\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef coords(self) -> Iterator[Tuple[float, float, float]]:\n        data = self.data[self.data.longitude.notnull()]\n        yield from zip(data[\"longitude\"], data[\"latitude\"], data[\"altitude\"])", "response": "Iterates on longitudes latitudes and altitudes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate on longitudes latitudes and timestamps.", "response": "def xy_time(self) -> Iterator[Tuple[float, float, float]]:\n        \"\"\"Iterates on longitudes, latitudes and timestamps.\"\"\"\n        iterator = iter(zip(self.coords, self.timestamp))\n        while True:\n            next_ = next(iterator, None)\n            if next_ is None:\n                return\n            coords, time = next_\n            yield (coords[0], coords[1], time.to_pydatetime().timestamp())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split(self, value=10, unit=None):  # noqa: F811\n        if type(value) == int and unit is None:\n            # default value is 10 m\n            unit = \"m\"\n\n        for data in _split(self.data, value, unit):\n            yield self.__class__(data)", "response": "Splits Flights in several legs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resample(self, rule: Union[str, int] = \"1s\") -> \"Flight\":\n\n        if isinstance(rule, str):\n            data = (\n                self._handle_last_position()\n                .data.assign(start=self.start, stop=self.stop)\n                .set_index(\"timestamp\")\n                .resample(rule)\n                .first()  # better performance than min() for duplicate index\n                .interpolate()\n                .reset_index()\n                .fillna(method=\"pad\")\n            )\n        elif isinstance(rule, int):\n            data = (\n                self._handle_last_position()\n                .data.set_index(\"timestamp\")\n                .asfreq(\n                    (self.stop - self.start) / (rule - 1),  # type: ignore\n                    method=\"nearest\",\n                )\n                .reset_index()\n            )\n        else:\n            raise TypeError(\"rule must be a str or an int\")\n\n        return self.__class__(data)", "response": "Resamples a Flight at a one point per second rate."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsimplify a trajectory with Douglas - Peucker algorithm.", "response": "def simplify(\n        self,\n        tolerance: float,\n        altitude: Optional[str] = None,\n        z_factor: float = 3.048,\n        return_type: Type[Mask] = Type[\"Flight\"],\n    ) -> Mask:\n        \"\"\"Simplifies a trajectory with Douglas-Peucker algorithm.\n\n        The method uses latitude and longitude, projects the trajectory to a\n        conformal projection and applies the algorithm. By default, a 2D version\n        is called, unless you pass a column name for altitude (z parameter). You\n        may scale the z-axis for more relevance (z_factor); the default value\n        works well in most situations.\n\n        The method returns a Flight unless you specify a np.ndarray[bool] as\n        return_type for getting a mask.\n        \"\"\"\n        # returns a mask\n        mask = douglas_peucker(\n            df=self.data,\n            tolerance=tolerance,\n            lat=\"latitude\",\n            lon=\"longitude\",\n            z=altitude,\n            z_factor=z_factor,\n        )\n        if return_type == Type[\"Flight\"]:\n            return self.__class__(self.data.loc[mask])\n        else:\n            return mask"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dont_crash(fn):\n\n    def safe_exec(self, *args, **kwargs):\n        try:\n            return fn(self, *args, **kwargs)\n        except Exception as e:\n            logging.exception(e)\n            QMessageBox.information(\n                self, type(e).__name__, \" \".join(str(x) for x in e.args)\n            )\n\n    return safe_exec", "response": "Decorator that does not crash the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the bounds of the shape.", "response": "def bounds(self) -> Tuple[float, float, float, float]:\n        \"\"\"Returns the bounds of the shape.\n        Bounds are given in the following order in the origin crs:\n        west, south, east, north\n        \"\"\"\n        return self.shape.bounds"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the extent of the shape.", "response": "def extent(self) -> Tuple[float, float, float, float]:\n        \"\"\"Returns the extent of the shape.\n        Extent is given in the following order in the origin crs:\n        west, east, south, north\n\n        This method is convenient for the ax.set_extent method\n        \"\"\"\n        west, south, east, north = self.bounds\n        return west, east, south, north"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprojecting the structure to the specified projection.", "response": "def project_shape(\n        self, projection: Union[pyproj.Proj, crs.Projection, None] = None\n    ) -> base.BaseGeometry:\n        \"\"\"Projection for a decent representation of the structure.\n\n        By default, an equivalent projection is applied. Equivalent projections\n        locally respect areas, which is convenient for the area attribute.\n        \"\"\"\n\n        if self.shape is None:\n            return None\n\n        if isinstance(projection, crs.Projection):\n            projection = pyproj.Proj(projection.proj4_init)\n\n        if projection is None:\n            bounds = self.bounds\n            projection = pyproj.Proj(\n                proj=\"aea\",  # equivalent projection\n                lat_1=bounds[1],\n                lat_2=bounds[3],\n                lat_0=(bounds[1] + bounds[3]) / 2,\n                lon_0=(bounds[0] + bounds[2]) / 2,\n            )\n        projected_shape = transform(\n            partial(\n                pyproj.transform, pyproj.Proj(init=\"EPSG:4326\"), projection\n            ),\n            self.shape,\n        )\n\n        if not projected_shape.is_valid:\n            warnings.warn(\"The chosen projection is invalid for current shape\")\n        return projected_shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_xy(\n        self, projection: Union[pyproj.Proj, crs.Projection, None] = None\n    ):\n        \"\"\"Computes x and y columns from latitudes and longitudes.\n\n        The source projection is WGS84 (EPSG 4326).\n        The default destination projection is a Lambert Conformal Conical\n        projection centered on the data inside the dataframe.\n\n        For consistency reasons with pandas DataFrame, a new Traffic structure\n        is returned.\n\n        \"\"\"\n        if isinstance(projection, crs.Projection):\n            projection = pyproj.Proj(projection.proj4_init)\n\n        if projection is None:\n            projection = pyproj.Proj(\n                proj=\"lcc\",\n                lat_1=self.data.latitude.min(),\n                lat_2=self.data.latitude.max(),\n                lat_0=self.data.latitude.mean(),\n                lon_0=self.data.longitude.mean(),\n            )\n\n        x, y = pyproj.transform(\n            pyproj.Proj(init=\"EPSG:4326\"),\n            projection,\n            self.data.longitude.values,\n            self.data.latitude.values,\n        )\n\n        return self.__class__(self.data.assign(x=x, y=y))", "response": "Computes x and y columns from latitudes and longitudes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn only the most relevant callsigns", "response": "def callsigns(self) -> Set[str]:\n        \"\"\"Return only the most relevant callsigns\"\"\"\n        sub = self.data.query(\"callsign == callsign\")\n        return set(cs for cs in sub.callsign if len(cs) > 3 and \" \" not in cs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resample(\n        self,\n        rule: Union[str, int] = \"1s\",\n        max_workers: int = 4,\n    ) -> \"Traffic\":\n        \"\"\"Resamples all trajectories, flight by flight.\n\n        `rule` defines the desired sample rate (default: 1s)\n        \"\"\"\n\n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            cumul = []\n            tasks = {\n                executor.submit(flight.resample, rule): flight\n                for flight in self\n            }\n            for future in tqdm(as_completed(tasks), total=len(tasks)):\n                cumul.append(future.result())\n\n        return self.__class__.from_flights(cumul)", "response": "Resamples all trajectories flight by flight.\n        rule defines the desired sample rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef closest_point_of_approach(\n        self,\n        lateral_separation: float,\n        vertical_separation: float,\n        projection: Union[pyproj.Proj, crs.Projection, None] = None,\n        round_t: str = \"d\",\n        max_workers: int = 4,\n    ) -> \"CPA\":\n        \"\"\"\n        Computes a CPA dataframe for all pairs of trajectories candidates for\n        being separated by less than lateral_separation in vertical_separation.\n\n        In order to be computed efficiently, the method needs the following\n        parameters:\n\n        - projection: a first filtering is applied on the bounding boxes of\n        trajectories, expressed in meters. You need to provide a decent\n        projection able to approximate distances by Euclide formula.\n        By default, EuroPP() projection is considered, but a non explicit\n        argument will raise a warning.\n\n        - round_t: an additional column will be added in the DataFrame to group\n        trajectories by relevant time frames. Distance computations will be\n        considered only between trajectories flown in the same time frame.\n        By default, the 'd' pandas freq parameter is considered, to group\n        trajectories by day, but other ways of splitting ('h') may be more\n        relevant and impact performance.\n\n        - max_workers: distance computations are spread over a given number of\n        processors.\n\n        \"\"\"\n\n        from ..algorithms.cpa import closest_point_of_approach\n\n        return closest_point_of_approach(\n            self,\n            lateral_separation,\n            vertical_separation,\n            projection,\n            round_t,\n            max_workers,\n        )", "response": "Computes a CPA dataframe for all pairs of trajectories for the given lateral separation and vertical separation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vtas2mach(tas, h):\n    a = vvsound(h)\n    M = tas / a\n    return M", "response": "True airspeed ( tas ) to mach number conversion"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntrues airspeed ( tas ) to mach number conversion", "response": "def vmach2tas(M, h):\n    \"\"\" True airspeed (tas) to mach number conversion \"\"\"\n    a = vvsound(h)\n    tas = M * a\n    return tas"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef veas2tas(eas, h):\n    rho = vdensity(h)\n    tas = eas * np.sqrt(rho0 / rho)\n    return tas", "response": "Equivalent airspeed to true airspeed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert tas to cas", "response": "def vtas2cas(tas, h):\n    \"\"\" tas2cas conversion both m/s \"\"\"\n    p, rho, T = vatmos(h)\n    qdyn = p*((1.+rho*tas*tas/(7.*p))**3.5-1.)\n    cas = np.sqrt(7.*p0/rho0*((qdyn/p0+1.)**(2./7.)-1.))\n\n    # cope with negative speed\n    cas = np.where(tas<0, -1*cas, cas)\n    return cas"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vmach2cas(M, h):\n    tas = vmach2tas(M, h)\n    cas = vtas2cas(tas, h)\n    return cas", "response": "Mach to CAS conversion"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vcas2mach(cas, h):\n    tas = vcas2tas(cas, h)\n    M   = vtas2mach(tas, h)\n    return M", "response": "CAS to Mach conversion"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef atmos(h):\n\n    # Constants\n\n    # Base values and gradient in table from hand-out\n    # (but corrected to avoid small discontinuities at borders of layers)\n    h0 = [0.0, 11000., 20000., 32000., 47000., 51000., 71000., 86852.]\n\n    p0 = [101325.,                  # Sea level\n           22631.7009099,           # 11 km\n            5474.71768857,          # 20 km\n             867.974468302,         # 32 km\n             110.898214043,         # 47 km\n              66.939,               # 51 km\n               3.9564 ]             # 71 km\n\n    T0 = [288.15,  # Sea level\n          216.65,  # 11 km\n          216.65,  # 20 km\n          228.65,  # 32 km\n          270.65,  # 47 km\n          270.65,  # 51 km\n          214.65]  # 71 km\n\n    # a = lapse rate (temp gradient)\n    # integer 0 indicates isothermic layer!\n    a  = [-0.0065, # 0-11 km\n            0,     # 11-20 km\n          0.001,   # 20-32 km\n          0.0028,  # 32-47 km\n            0,     # 47-51 km\n          -0.0028, # 51-71 km\n          -0.002]  # 71-   km\n\n    # Clip altitude to maximum!\n    h = max(0.0, min(float(h), h0[-1]))\n\n    # Find correct layer\n    i = 0\n    while h > h0[i+1] and i < len(h0) - 2:\n        i = i+1\n\n    # Calculate if sothermic layer\n    if a[i] == 0:\n        T   = T0[i]\n        p   = p0[i]*np.exp(-g0/(R*T)*(h-h0[i]))\n        rho = p/(R*T)\n\n    # Calculate for temperature gradient\n    else:\n        T   = T0[i] + a[i]*(h-h0[i])\n        p   = p0[i]*((T/T0[i])**(-g0/(a[i]*R)))\n        rho = p/(R*T)\n\n    return p, rho, T", "response": "Returns a base value and gradient of the isothermic layer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the temperature of the isothermic atmosphere at the specified altitude.", "response": "def temp(h):\n    \"\"\" temp (altitude): Temperature only version of ISA atmos\n\n        Input:\n              h =  altitude in meters 0.0 < h < 84852.\n        (will be clipped when outside range, integer input allowed)\n        Output:\n              T    (in SI-unit: K \"\"\"\n\n    # Base values and gradient in table from hand-out\n    # (but corrected to avoid small discontinuities at borders of layers)\n    h0 = [0.0, 11000., 20000., 32000., 47000., 51000., 71000., 86852.]\n\n    T0 = [288.15,  # Sea level\n          216.65,  # 11 km\n          216.65,  # 20 km\n          228.65,  # 32 km\n          270.65,  # 47 km\n          270.65,  # 51 km\n          214.65]  # 71 km\n\n    # a = lapse rate (temp gradient)\n    # integer 0 indicates isothermic layer!\n    a  = [-0.0065, # 0-11 km\n            0 ,    # 11-20 km\n          0.001,   # 20-32 km\n          0.0028,  # 32-47 km\n            0 ,    # 47-51 km\n          -0.0028, # 51-71 km\n          -0.002]  # 71-   km\n\n    # Clip altitude to maximum!\n    h = max(0.0,min(float(h),h0[-1]))\n\n\n    # Find correct layer\n    i = 0\n    while h>h0[i+1] and i<len(h0)-2:\n        i = i+1\n\n    # Calculate if sothermic layer\n    if a[i]==0:\n        T   = T0[i]\n\n    # Calculate for temperature gradient\n    else:\n        T   = T0[i] + a[i]*(h-h0[i])\n\n    return T"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntruing airspeed tas to mach number conversion", "response": "def tas2mach(tas, h):\n    \"\"\" True airspeed (tas) to mach number conversion \"\"\"\n    a = vsound(h)\n    M = tas / a\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntrues airspeed ( tas ) to mach number conversion", "response": "def mach2tas(M, h):\n    \"\"\" True airspeed (tas) to mach number conversion \"\"\"\n    a = vsound(h)\n    tas = M * a\n    return tas"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cas2tas(cas, h):\n    p, rho, T = atmos(h)\n    qdyn = p0*((1.+rho0*cas*cas/(7.*p0))**3.5-1.)\n    tas = np.sqrt(7.*p/rho*((1.+qdyn/p)**(2./7.)-1.))\n    tas = -1 * tas if cas < 0 else tas\n    return tas", "response": "cas2tas conversion both m and s h in m"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mach2cas(M, h):\n    tas = mach2tas(M, h)\n    cas = tas2cas(tas, h)\n    return cas", "response": "Mach to CAS conversion"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_bluesky(\n    traffic: Traffic,\n    filename: Union[str, Path],\n    minimum_time: Optional[timelike] = None,\n) -> None:\n    \"\"\"Generates a Bluesky scenario file.\"\"\"\n\n    if minimum_time is not None:\n        minimum_time = to_datetime(minimum_time)\n        traffic = traffic.query(f\"timestamp >= '{minimum_time}'\")\n\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    if not filename.parent.exists():\n        filename.parent.mkdir(parents=True)\n\n    altitude = (\n        \"baro_altitude\"\n        if \"baro_altitude\" in traffic.data.columns\n        else \"altitude\"\n    )\n\n    if \"mdl\" not in traffic.data.columns:\n        traffic = aircraft.merge(traffic)\n\n    if \"cas\" not in traffic.data.columns:\n        traffic = Traffic(\n            traffic.data.assign(\n                cas=vtas2cas(traffic.data.ground_speed, traffic.data[altitude])\n            )\n        )\n\n    with filename.open(\"w\") as fh:\n        t_delta = traffic.data.timestamp - traffic.start_time\n        data = (\n            traffic.assign_id()\n            .data.groupby(\"flight_id\")\n            .filter(lambda x: x.shape[0] > 3)\n            .assign(timedelta=t_delta.apply(fmt_timedelta))\n            .sort_values(by=\"timestamp\")\n        )\n\n        for column in data.columns:\n            data[column] = data[column].astype(np.str)\n\n        is_created: List[str] = []\n        is_deleted: List[str] = []\n\n        start_time = cast(pd.Timestamp, traffic.start_time).time()\n        fh.write(f\"00:00:00> TIME {start_time}\\n\")\n\n        # Add some bluesky command for the visualisation\n        # fh.write(\"00:00:00>trail on\\n\")\n        # fh.write(\"00:00:00>ssd conflicts\\n\")\n\n        # We remove an object when it's its last data point\n        buff = data.groupby(\"flight_id\").timestamp.max()\n        dd = pd.DataFrame(\n            columns=[\"timestamp\"], data=buff.values, index=buff.index.values\n        )\n        map_icao24_last_point = {}\n        for i, v in dd.iterrows():\n            map_icao24_last_point[i] = v[0]\n\n        # Main loop to write lines in the scenario file\n        for _, v in data.iterrows():\n            if v.flight_id not in is_created:\n                # If the object is not created then create it\n                is_created.append(v.flight_id)\n                fh.write(\n                    f\"{v.timedelta}> CRE {v.callsign} {v.mdl} \"\n                    f\"{v.latitude} {v.longitude} {v.track} \"\n                    f\"{v[altitude]} {v.cas}\\n\"\n                )\n\n            elif v.timestamp == map_icao24_last_point[v.flight_id]:\n                # Remove an aircraft when no data are available\n                if v.flight_id not in is_deleted:\n                    is_deleted.append(v.flight_id)\n                    fh.write(f\"{v.timedelta}> DEL {v.callsign}\\n\")\n\n            elif v.flight_id not in is_deleted:\n                # Otherwise update the object position\n                fh.write(\n                    f\"{v.timedelta}> MOVE {v.callsign} \"\n                    f\"{v.latitude} {v.longitude} {v[altitude]} \"\n                    f\"{v.track} {v.cas} {v.vertical_rate}\\n\"\n                )\n\n        logging.info(f\"Scenario file {filename} written\")", "response": "Generates a Bluesky scenario file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports all submodules of a module recursively including subpackages", "response": "def import_submodules(package, recursive=True):\n    \"\"\" Import all submodules of a module, recursively, including subpackages\n\n    :param package: package (name or actual module)\n    :type package: str | module\n    :rtype: dict[str, types.ModuleType]\n    \"\"\"\n    if isinstance(package, str):\n        package = importlib.import_module(package)\n    results = {}\n    for loader, name, is_pkg in pkgutil.walk_packages(package.__path__):\n        full_name = package.__name__ + \".\" + name\n        results[name] = importlib.import_module(full_name)\n        if recursive and is_pkg:\n            results.update(import_submodules(full_name))\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot(self, ax: GeoAxesSubplot, s: int = 10, **kwargs) -> Artist:\n        return ax.scatter(\n            self.data.longitude,\n            self.data.latitude,\n            s=s,\n            transform=PlateCarree(),\n            **kwargs,\n        )", "response": "Plotting function. All arguments are passed to ax. scatter"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef interpolate(self, times, proj=PlateCarree()) -> np.ndarray:\n        if proj not in self.interpolator:\n            self.interpolator[proj] = interp1d(\n                np.stack(t.to_pydatetime().timestamp() for t in self.timestamp),\n                proj.transform_points(\n                    PlateCarree(), *np.stack(self.coords).T\n                ).T,\n            )\n        return PlateCarree().transform_points(\n            proj, *self.interpolator[proj](times)\n        )", "response": "Interpolates a trajectory in time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a DataFrame containing the flights contained in the structure.", "response": "def stats(self) -> pd.DataFrame:\n        \"\"\"Statistics about flights contained in the structure.\n        Useful for a meaningful representation.\n        \"\"\"\n        cumul = []\n        for f in self:\n            info = {\n                \"flight_id\": f.flight_id,\n                \"callsign\": f.callsign,\n                \"origin\": f.origin,\n                \"destination\": f.destination,\n                \"duration\": f.stop - f.start,  # type: ignore\n            }\n            cumul.append(info)\n\n        return (\n            pd.DataFrame.from_records(cumul)\n            .set_index(\"flight_id\")\n            .sort_values(\"duration\", ascending=False)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef douglas_peucker(*args, df: pd.DataFrame=None, tolerance: float,\n                    x='x', y='y', z=None, z_factor: float = 3.048,\n                    lat=None, lon=None) -> np.ndarray:\n    \"\"\"Ramer-Douglas-Peucker algorithm for 2D/3D trajectories.\n\n    Simplify a trajectory by keeping the points further away from the straight\n    line.\n\n    Parameters:\n        df        Optional                a Pandas dataframe\n        tolerance float                   the threshold for cutting the\n                                          trajectory\n        z_factor  float                   for ft/m conversion (default 3.048)\n                                            1km lateral, 100m vertical seems\n                                            like a good ratio\n        x, y, z   str or ndarray[float]   the column names if a dataframe is\n                                          given, otherwise a series of float\n        lat, lon  str or ndarray[float]   the column names if a dataframe is\n                                          given, otherwise a series of float.\n                                          x, y are built with a Lambert\n                                          Conformal projection\n\n        Note that lat, lon has precedence over x, y\n\n    Returns:\n        a np.array of booleans serving as a mask on the dataframe or\n        on the numpy array\n\n    See also: https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm\n\n    \"\"\"\n\n    if df is None and (isinstance(x, str) or isinstance(y, str)):\n        raise ValueError(\"Provide a dataframe if x and y are column names\")\n    if df is None and (isinstance(lon, str) or isinstance(lat, str)):\n        raise ValueError(\"Provide a dataframe if lat and lon are column names\")\n    if tolerance < 0:\n        raise ValueError(\"tolerance must be a positive float\")\n\n    if df is not None and isinstance(lat, str) and isinstance(lon, str):\n        lat, lon = df[lat].values, df[lon].values\n    if df is not None and lat is not None and lon is not None:\n        lat, lon = np.array(lat), np.array(lon)\n        x, y = pyproj.transform(\n            pyproj.Proj(init='EPSG:4326'),\n            pyproj.Proj(proj='lcc',\n                        lat0=lat.mean(), lon0=lon.mean(),\n                        lat1=lat.min(), lat2=lat.max(),\n                        ), lon, lat)\n    else:\n        if df is not None:\n            x, y = df[x].values, df[y].values\n        x, y = np.array(x), np.array(y)\n\n    if z is not None:\n        if df is not None:\n            z = df[z].values\n        z = z_factor * np.array(z)\n\n    mask = np.ones(len(x), dtype=bool)\n    if z is None:\n        _douglas_peucker_rec(x, y, mask, tolerance)\n    else:\n        _douglas_peucker_rec_3d(x, y, z, mask, tolerance)\n\n    return mask", "response": "This function is used to perform a 2D or 3D algorithm for 2D or 3D trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _format_dataframe(\n        df: pd.DataFrame, nautical_units=True\n    ) -> pd.DataFrame:\n        \"\"\"\n        This function converts types, strips spaces after callsigns and sorts\n        the DataFrame by timestamp.\n\n        For some reason, all data arriving from OpenSky are converted to\n        units in metric system. Optionally, you may convert the units back to\n        nautical miles, feet and feet/min.\n\n        \"\"\"\n\n        if \"callsign\" in df.columns and df.callsign.dtype == object:\n            df.callsign = df.callsign.str.strip()\n\n        if nautical_units:\n            df.altitude = df.altitude / 0.3048\n            if \"geoaltitude\" in df.columns:\n                df.geoaltitude = df.geoaltitude / 0.3048\n            if \"groundspeed\" in df.columns:\n                df.groundspeed = df.groundspeed / 1852 * 3600\n            if \"vertical_rate\" in df.columns:\n                df.vertical_rate = df.vertical_rate / 0.3048 * 60\n\n        df.timestamp = pd.to_datetime(df.timestamp * 1e9).dt.tz_localize(\"utc\")\n\n        if \"last_position\" in df.columns:\n            df = df.query(\"last_position == last_position\").assign(\n                last_position=pd.to_datetime(\n                    df.last_position * 1e9\n                ).dt.tz_localize(\"utc\")\n            )\n\n        return df.sort_values(\"timestamp\")", "response": "This function formats the DataFrame to be used by OpenSky to get the appropriate format for the new version of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef history(\n        self,\n        start: timelike,\n        stop: Optional[timelike] = None,\n        *args,  # more reasonable to be explicit about arguments\n        date_delta: timedelta = timedelta(hours=1),\n        callsign: Union[None, str, Iterable[str]] = None,\n        icao24: Union[None, str, Iterable[str]] = None,\n        serials: Union[None, str, Iterable[str]] = None,\n        bounds: Union[\n            BaseGeometry, Tuple[float, float, float, float], None\n        ] = None,\n        cached: bool = True,\n        count: bool = False,\n        other_tables: str = \"\",\n        other_params: str = \"\",\n        progressbar: Callable[[Iterable], Iterable] = iter,\n    ) -> Optional[Union[Traffic, Flight]]:\n\n        \"\"\"Get Traffic from the OpenSky Impala shell.\n\n        The method builds appropriate SQL requests, caches results and formats\n        data into a proper pandas DataFrame. Requests are split by hour (by\n        default) in case the connection fails.\n\n        Args:\n            start: a string, epoch or datetime\n            stop (optional): a string, epoch or datetime, by default, one day\n            after start\n            date_delta (optional): how to split the requests (default: one day)\n            callsign (optional): a string or a list of strings (default: empty)\n            icao24 (optional): a string or a list of strings identifying the\n            transponder code of the aircraft (default: empty)\n            serials (optional): a string or a list of strings identifying the\n            sensors receiving the data. (default: empty)\n            bounds (optional): a shape (requires the bounds attribute) or a\n            tuple of floats (west, south, east, north) to put a geographical\n            limit on the request. (default: empty)\n            cached (boolean): whether to look first whether the request has been\n            cached (default: True)\n            count (boolean): add a column stating how many sensors received each\n            line (default: False)\n\n        Returns:\n            a Traffic structure wrapping the dataframe\n\n        \"\"\"\n\n        return_flight = False\n        start = to_datetime(start)\n        if stop is not None:\n            stop = to_datetime(stop)\n        else:\n            stop = start + timedelta(days=1)\n\n        if progressbar == iter and stop - start > timedelta(hours=1):\n            progressbar = tqdm\n\n        if isinstance(serials, Iterable):\n            other_tables += \", state_vectors_data4.serials s \"\n            other_params += \"and s.ITEM in {} \".format(tuple(serials))\n\n        if isinstance(icao24, str):\n            other_params += \"and icao24='{}' \".format(icao24)\n\n        elif isinstance(icao24, Iterable):\n            icao24 = \",\".join(\"'{}'\".format(c) for c in icao24)\n            other_params += \"and icao24 in ({}) \".format(icao24)\n\n        if isinstance(callsign, str):\n            other_params += \"and callsign='{:<8s}' \".format(callsign)\n            return_flight = True\n\n        elif isinstance(callsign, Iterable):\n            callsign = \",\".join(\"'{:<8s}'\".format(c) for c in callsign)\n            other_params += \"and callsign in ({}) \".format(callsign)\n\n        if bounds is not None:\n            try:\n                # thinking of shapely bounds attribute (in this order)\n                # I just don't want to add the shapely dependency here\n                west, south, east, north = bounds.bounds  # type: ignore\n            except AttributeError:\n                west, south, east, north = bounds\n\n            other_params += \"and lon>={} and lon<={} \".format(west, east)\n            other_params += \"and lat>={} and lat<={} \".format(south, north)\n\n        cumul = []\n        sequence = list(split_times(start, stop, date_delta))\n        columns = \", \".join(self._impala_columns)\n        parse_columns = \", \".join(self._impala_columns)\n\n        if count is True:\n            other_params += \"group by \" + columns\n            columns = \"count(*) as count, \" + columns\n            parse_columns = \"count, \" + parse_columns\n            other_tables += \", state_vectors_data4.serials s\"\n\n        for bt, at, bh, ah in progressbar(sequence):\n\n            logging.info(\n                f\"Sending request between time {bt} and {at} \"\n                f\"and hour {bh} and {ah}\"\n            )\n\n            request = self.basic_request.format(\n                columns=columns,\n                before_time=bt.timestamp(),\n                after_time=at.timestamp(),\n                before_hour=bh.timestamp(),\n                after_hour=ah.timestamp(),\n                other_tables=other_tables,\n                other_params=other_params,\n            )\n\n            df = self._impala(request, columns=parse_columns, cached=cached)\n\n            if df is None:\n                continue\n\n            df = self._format_history(df)\n            df = self._format_dataframe(df)\n            cumul.append(df)\n\n        if len(cumul) == 0:\n            return None\n\n        df = pd.concat(cumul, sort=True).sort_values(\"timestamp\")\n\n        if count is True:\n            df = df.assign(count=lambda df: df[\"count\"].astype(int))\n\n        if return_flight:\n            return Flight(df)\n\n        return Traffic(df)", "response": "This method returns a DataFrame that contains the Traffic data for the specified time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extended(\n        self,\n        start: timelike,\n        stop: Optional[timelike] = None,\n        *args,  # more reasonable to be explicit about arguments\n        date_delta: timedelta = timedelta(hours=1),\n        icao24: Union[None, str, Iterable[str]] = None,\n        serials: Union[None, int, Iterable[int]] = None,\n        other_tables: str = \"\",\n        other_params: str = \"\",\n        progressbar: Callable[[Iterable], Iterable] = iter,\n        cached: bool = True,\n    ) -> pd.DataFrame:\n        \"\"\"Get EHS message from the OpenSky Impala shell.\n\n        The method builds appropriate SQL requests, caches results and formats\n        data into a proper pandas DataFrame. Requests are split by hour (by\n        default) in case the connection fails.\n\n        Args:\n            start: a string, epoch or datetime\n            stop (optional): a string, epoch or datetime, by default, one day\n            after start\n            date_delta (optional): how to split the requests (default: one day)\n            icao24 (optional): a string or a list of strings identifying the\n            transponder code of the aircraft (default: empty)\n            serials (optional): an int or a list of int identifying the\n            sensors receiving the data. (default: empty)\n            cached (boolean): whether to look first whether the request has been\n            cached (default: True)\n\n        Returns:\n            a Traffic structure wrapping the dataframe\n\n        \"\"\"\n\n        _request = (\n            \"select {columns} from rollcall_replies_data4 r {other_tables} \"\n            \"where hour>={before_hour} and hour<{after_hour} \"\n            \"and r.mintime>={before_time} and r.mintime<{after_time} \"\n            \"{other_params}\"\n        )\n\n        columns = (\n            \"r.mintime, r.maxtime, \"\n            \"rawmsg, msgcount, icao24, message, altitude, identity, hour\"\n        )\n\n        parse_columns = (\n            \"mintime, maxtime, \"\n            \"rawmsg, msgcount, icao24, message, altitude, identity, hour\"\n        )\n\n        start = to_datetime(start)\n        if stop is not None:\n            stop = to_datetime(stop)\n        else:\n            stop = start + timedelta(days=1)\n\n        if isinstance(icao24, str):\n            other_params += \"and icao24='{}' \".format(icao24)\n        elif isinstance(icao24, Iterable):\n            icao24 = \",\".join(\"'{}'\".format(c) for c in icao24)\n            other_params += \"and icao24 in ({}) \".format(icao24)\n\n        if isinstance(serials, Iterable):\n            other_tables += \", rollcall_replies_data4.sensors s \"\n            other_params += \"and s.serial in {} \".format(tuple(serials))\n            columns = \"s.serial, s.mintime as time, \" + columns\n            parse_columns = \"serial, time, \" + parse_columns\n        elif isinstance(serials, int):\n            other_tables += \", rollcall_replies_data4.sensors s \"\n            other_params += \"and s.serial = {} \".format((serials))\n            columns = \"s.serial, s.mintime as time, \" + columns\n            parse_columns = \"serial, time, \" + parse_columns\n\n        other_params += \"and message is not null \"\n        sequence = list(split_times(start, stop, date_delta))\n        cumul = []\n\n        for bt, at, bh, ah in progressbar(sequence):\n\n            logging.info(\n                f\"Sending request between time {bt} and {at} \"\n                f\"and hour {bh} and {ah}\"\n            )\n\n            request = _request.format(\n                columns=columns,\n                before_time=int(bt.timestamp()),\n                after_time=int(at.timestamp()),\n                before_hour=bh.timestamp(),\n                after_hour=ah.timestamp(),\n                other_tables=other_tables,\n                other_params=other_params,\n            )\n\n            df = self._impala(request, columns=parse_columns, cached=cached)\n\n            if df is None:\n                continue\n\n            if df.hour.dtype == object:\n                df = df[df.hour != \"hour\"]\n\n            for column_name in [\"mintime\", \"maxtime\"]:\n                df[column_name] = pd.to_datetime(\n                    df[column_name].astype(float) * 1e9\n                ).dt.tz_localize(\"utc\")\n\n            df.icao24 = (\n                df.icao24.apply(int, base=16)\n                .apply(hex)\n                .str.slice(2)\n                .str.pad(6, fillchar=\"0\")\n            )\n            df.altitude = df.altitude.astype(float) * 0.3048\n\n            cumul.append(df)\n\n        if len(cumul) == 0:\n            return None\n\n        return pd.concat(cumul).sort_values(\"mintime\")", "response": "This method returns a pandas DataFrame containing the data from the OpenSky Impala shell."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef within_bounds(\n        self,\n        start: timelike,\n        stop: timelike,\n        bounds: Union[BaseGeometry, Tuple[float, float, float, float]],\n    ) -> Optional[pd.DataFrame]:\n        \"\"\"EXPERIMENTAL.\"\"\"\n\n        start = to_datetime(start)\n        stop = to_datetime(stop)\n\n        before_hour = round_time(start, \"before\")\n        after_hour = round_time(stop, \"after\")\n\n        try:\n            # thinking of shapely bounds attribute (in this order)\n            # I just don't want to add the shapely dependency here\n            west, south, east, north = bounds.bounds  # type: ignore\n        except AttributeError:\n            west, south, east, north = bounds\n\n        other_params = \"and lon>={} and lon<={} \".format(west, east)\n        other_params += \"and lat>={} and lat<={} \".format(south, north)\n\n        query = self.basic_request.format(\n            columns=\"icao24, callsign, s.ITEM as serial, count(*) as count\",\n            other_tables=\", state_vectors_data4.serials s\",\n            before_time=start.timestamp(),\n            after_time=stop.timestamp(),\n            before_hour=before_hour.timestamp(),\n            after_hour=after_hour.timestamp(),\n            other_params=other_params + \"group by icao24, callsign, s.ITEM\",\n        )\n\n        logging.info(f\"Sending request: {query}\")\n        df = self._impala(query, columns=\"icao24, callsign, serial, count\")\n        if df is None:\n            return None\n\n        df = df[df[\"count\"] != \"count\"]\n        df[\"count\"] = df[\"count\"].astype(int)\n\n        return df", "response": "Returns a DataFrame containing the state vectors for the specified time range."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of packages that should be filtered based on the configuration.", "response": "def _determine_unfiltered_package_names(self):\n        \"\"\"\n        Return a list of package names to be filtered base on the configuration\n        file.\n        \"\"\"\n        # This plugin only processes packages, if the line in the packages\n        # configuration contains a PEP440 specifier it will be processed by the\n        # blacklist release filter.  So we need to remove any packages that\n        # are not applicable for this plugin.\n        unfiltered_packages = set()\n        try:\n            lines = self.configuration[\"whitelist\"][\"packages\"]\n            package_lines = lines.split(\"\\n\")\n        except KeyError:\n            package_lines = []\n        for package_line in package_lines:\n            package_line = package_line.strip()\n            if not package_line or package_line.startswith(\"#\"):\n                continue\n            unfiltered_packages.add(package_line)\n        return list(unfiltered_packages)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_match(self, **kwargs):\n        if not self.whitelist_package_names:\n            return False\n\n        name = kwargs.get(\"name\", None)\n        if not name:\n            return False\n\n        if name in self.whitelist_package_names:\n            logger.info(f\"Package {name!r} is whitelisted\")\n            return False\n        return True", "response": "Check if the package name matches against a project that is blacklisted by the blacklist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_plugin(self):\n        if self._patterns or self._packagetypes:\n            logger.debug(\n                \"Skipping initalization of Exclude Platform plugin. \"\n                + \"Already initialized\"\n            )\n            return\n\n        try:\n            tags = self.configuration[\"blacklist\"][\"platforms\"].split()\n        except KeyError:\n            logger.error(f\"Plugin {self.name}: missing platforms= setting\")\n            return\n\n        for platform in tags:\n            lplatform = platform.lower()\n\n            if lplatform in (\"windows\", \"win\"):\n                # PEP 425\n                # see also setuptools/package_index.py\n                self._patterns.extend([\".win32\", \"-win32\", \"win_amd64\", \"win-amd64\"])\n                # PEP 527\n                self._packagetypes.extend([\"bdist_msi\", \"bdist_wininst\"])\n\n            elif lplatform in (\"macos\", \"macosx\"):\n                self._patterns.extend([\"macosx_\", \"macosx-\"])\n                self._packagetypes.extend([\"bdist_dmg\"])\n\n            elif lplatform in (\"freebsd\"):\n                # concerns only very few files\n                self._patterns.extend([\".freebsd\", \"-freebsd\"])\n\n            elif lplatform in (\"linux\"):\n                self._patterns.extend(\n                    [\n                        \"linux-i686\",  # PEP 425\n                        \"linux-x86_64\",  # PEP 425\n                        \"linux_armv7l\",  # https://github.com/pypa/warehouse/pull/2010\n                        \"linux_armv6l\",  # https://github.com/pypa/warehouse/pull/2012\n                        \"manylinux1_\",  # PEP 513\n                        \"manylinux2010_\",  # PEP 571\n                    ]\n                )\n                self._packagetypes.extend([\"bdist_rpm\"])\n\n        logger.info(f\"Initialized {self.name} plugin with {self._patterns!r}\")", "response": "Initialize the plugin reading patterns from the config."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves files from releases that match any pattern.", "response": "def filter(self, info, releases):\n        \"\"\"\n        Remove files from `releases` that match any pattern.\n        \"\"\"\n\n        # Make a copy of releases keys\n        # as we may delete packages during iteration\n        removed = 0\n        versions = list(releases.keys())\n        for version in versions:\n            new_files = []\n            for file_desc in releases[version]:\n                if self._check_match(file_desc):\n                    removed += 1\n                else:\n                    new_files.append(file_desc)\n            if len(new_files) == 0:\n                del releases[version]\n            else:\n                releases[version] = new_files\n        logger.debug(f\"{self.name}: filenames removed: {removed}\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a release version matches any of the specificed patterns.", "response": "def _check_match(self, file_desc) -> bool:\n        \"\"\"\n        Check if a release version matches any of the specificed patterns.\n\n        Parameters\n        ==========\n        name: file_desc\n            file description entry\n\n        Returns\n        =======\n        bool:\n            True if it matches, False otherwise.\n        \"\"\"\n\n        # source dist: never filter out\n        pt = file_desc.get(\"packagetype\")\n        if pt == \"sdist\":\n            return False\n\n        # Windows installer\n        if pt in self._packagetypes:\n            return True\n\n        fn = file_desc[\"filename\"]\n        for i in self._patterns:\n            if i in fn:\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload all blacklist plugins that are registered with pkg_resources.", "response": "def load_filter_plugins(entrypoint_group: str) -> Iterable[Filter]:\n    \"\"\"\n    Load all blacklist plugins that are registered with pkg_resources\n\n    Parameters\n    ==========\n    entrypoint_group: str\n        The entrypoint group name to load plugins from\n\n    Returns\n    =======\n    List of Blacklist:\n        A list of objects derived from the Blacklist class\n    \"\"\"\n    global loaded_filter_plugins\n    enabled_plugins: List[str] = []\n    config = BandersnatchConfig().config\n    try:\n        config_blacklist_plugins = config[\"blacklist\"][\"plugins\"]\n        split_plugins = config_blacklist_plugins.split(\"\\n\")\n        if \"all\" in split_plugins:\n            enabled_plugins = [\"all\"]\n        else:\n            for plugin in split_plugins:\n                if not plugin:\n                    continue\n                enabled_plugins.append(plugin)\n    except KeyError:\n        pass\n\n    # If the plugins for the entrypoint_group have been loaded return them\n    cached_plugins = loaded_filter_plugins.get(entrypoint_group)\n    if cached_plugins:\n        return cached_plugins\n\n    plugins = set()\n    for entry_point in pkg_resources.iter_entry_points(group=entrypoint_group):\n        plugin_class = entry_point.load()\n        plugin_instance = plugin_class()\n        if \"all\" in enabled_plugins or plugin_instance.name in enabled_plugins:\n            plugins.add(plugin_instance)\n\n    loaded_filter_plugins[entrypoint_group] = list(plugins)\n\n    return plugins"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the plugin reading patterns from the config.", "response": "def initialize_plugin(self):\n        \"\"\"\n        Initialize the plugin reading patterns from the config.\n        \"\"\"\n        if not self.patterns:\n            self.patterns = [\n                re.compile(pattern_string)\n                for pattern_string in self.PRERELEASE_PATTERNS\n            ]\n            logger.info(f\"Initialized prerelease plugin with {self.patterns}\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all release versions that match any of the specificed patterns.", "response": "def filter(self, info, releases):\n        \"\"\"\n        Remove all release versions that match any of the specificed patterns.\n        \"\"\"\n        for version in list(releases.keys()):\n            if any(pattern.match(version) for pattern in self.patterns):\n                del releases[version]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initialize_plugin(self):\n        # TODO: should retrieving the plugin's config be part of the base class?\n        try:\n            config = self.configuration[\"filter_regex\"][\"releases\"]\n        except KeyError:\n            return\n        else:\n            if not self.patterns:\n                pattern_strings = [pattern for pattern in config.split(\"\\n\") if pattern]\n                self.patterns = [\n                    re.compile(pattern_string) for pattern_string in pattern_strings\n                ]\n\n                logger.info(f\"Initialized regex release plugin with {self.patterns}\")", "response": "Initialize the plugin reading patterns from the config."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_match(self, name):\n        return any(pattern.match(name) for pattern in self.patterns)", "response": "Check if a release version matches any of the specificed patterns."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the packages that should be filtered by the blacklist release filter.", "response": "def _determine_filtered_package_names(self):\n        \"\"\"\n        Return a list of package names to be filtered base on the configuration\n        file.\n        \"\"\"\n        # This plugin only processes packages, if the line in the packages\n        # configuration contains a PEP440 specifier it will be processed by the\n        # blacklist release filter.  So we need to remove any packages that\n        # are not applicable for this plugin.\n        filtered_packages = set()\n        try:\n            lines = self.configuration[\"blacklist\"][\"packages\"]\n            package_lines = lines.split(\"\\n\")\n        except KeyError:\n            package_lines = []\n        for package_line in package_lines:\n            package_line = package_line.strip()\n            if not package_line or package_line.startswith(\"#\"):\n                continue\n            package_requirement = Requirement(package_line)\n            if package_requirement.specifier:\n                continue\n            if package_requirement.name != package_line:\n                logger.debug(\n                    \"Package line %r does not requirement name %r\",\n                    package_line,\n                    package_requirement.name,\n                )\n                continue\n            filtered_packages.add(package_line)\n        logger.debug(\"Project blacklist is %r\", list(filtered_packages))\n        return list(filtered_packages)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the package name matches against a project that is blacklisted by the blacklist.", "response": "def check_match(self, **kwargs):\n        \"\"\"\n        Check if the package name matches against a project that is blacklisted\n        in the configuration.\n\n        Parameters\n        ==========\n        name: str\n            The normalized package name of the package/project to check against\n            the blacklist.\n\n        Returns\n        =======\n        bool:\n            True if it matches, False otherwise.\n        \"\"\"\n        name = kwargs.get(\"name\", None)\n        if not name:\n            return False\n\n        if name in self.blacklist_package_names:\n            logger.info(f\"Package {name!r} is blacklisted\")\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the release plugin.", "response": "def initialize_plugin(self):\n        \"\"\"\n        Initialize the plugin\n        \"\"\"\n        # Generate a list of blacklisted packages from the configuration and\n        # store it into self.blacklist_package_names attribute so this\n        # operation doesn't end up in the fastpath.\n        if not self.blacklist_package_names:\n            self.blacklist_release_requirements = (\n                self._determine_filtered_package_requirements()\n            )\n            logger.info(\n                f\"Initialized release plugin {self.name}, filtering \"\n                + f\"{self.blacklist_release_requirements}\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the configuration file for [ blacklist ] packages returning a list of packaging. requirements. Requirement objects", "response": "def _determine_filtered_package_requirements(self):\n        \"\"\"\n        Parse the configuration file for [blacklist]packages\n\n        Returns\n        -------\n        list of packaging.requirements.Requirement\n            For all PEP440 package specifiers\n        \"\"\"\n        filtered_requirements = set()\n        try:\n            lines = self.configuration[\"blacklist\"][\"packages\"]\n            package_lines = lines.split(\"\\n\")\n        except KeyError:\n            package_lines = []\n        for package_line in package_lines:\n            package_line = package_line.strip()\n            if not package_line or package_line.startswith(\"#\"):\n                continue\n            filtered_requirements.add(Requirement(package_line))\n        return list(filtered_requirements)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_match(self, name, version_string) -> bool:\n        if not name or not version_string:\n            return False\n\n        try:\n            version = Version(version_string)\n        except InvalidVersion:\n            logger.debug(f\"Package {name}=={version_string} has an invalid version\")\n            return False\n        for requirement in self.blacklist_release_requirements:\n            if name != requirement.name:\n                continue\n            if version in requirement.specifier:\n                logger.debug(\n                    f\"MATCH: Release {name}=={version} matches specifier \"\n                    f\"{requirement.specifier}\"\n                )\n                return True\n        return False", "response": "Check if the package name and version matches against a blacklisted release."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find(root: Union[Path, str], dirs: bool = True) -> str:\n    if isinstance(root, str):\n        root = Path(root)\n\n    results: List[Path] = []\n    for dirpath, dirnames, filenames in os.walk(root):\n        names = filenames\n        if dirs:\n            names += dirnames\n        for name in names:\n            results.append(Path(dirpath) / name)\n    results.sort()\n    return \"\\n\".join(str(result.relative_to(root)) for result in results)", "response": "A test helper simulating find."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrewriting an existing file atomically to avoid programs running in parallel to have race conditions while reading.", "response": "def rewrite(\n    filepath: Union[str, Path], mode: str = \"w\", **kw: Any\n) -> Generator[IO, None, None]:\n    \"\"\"Rewrite an existing file atomically to avoid programs running in\n    parallel to have race conditions while reading.\"\"\"\n    if isinstance(filepath, str):\n        base_dir = os.path.dirname(filepath)\n        filename = os.path.basename(filepath)\n    else:\n        base_dir = str(filepath.parent)\n        filename = filepath.name\n\n    # Change naming format to be more friendly with distributed POSIX\n    # filesystems like GlusterFS that hash based on filename\n    # GlusterFS ignore '.' at the start of filenames and this avoid rehashing\n    with tempfile.NamedTemporaryFile(\n        mode=mode, prefix=f\".{filename}.\", delete=False, dir=base_dir, **kw\n    ) as f:\n        filepath_tmp = f.name\n        yield f\n\n    if not os.path.exists(filepath_tmp):\n        # Allow our clients to remove the file in case it doesn't want it to be\n        # put in place actually but also doesn't want to error out.\n        return\n    os.chmod(filepath_tmp, 0o100644)\n    os.rename(filepath_tmp, filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unlink_parent_dir(path: Path) -> None:\n    logger.info(f\"unlink {str(path)}\")\n    path.unlink()\n\n    parent_path = path.parent\n    try:\n        parent_path.rmdir()\n        logger.info(f\"rmdir {str(parent_path)}\")\n    except OSError as oe:\n        logger.debug(f\"Did not remove {str(parent_path)}: {str(oe)}\")", "response": "Remove a file and remove it from the parent dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrewrites a file atomically.", "response": "def update_safe(filename: str, **kw: Any) -> Generator[IO, None, None]:\n    \"\"\"Rewrite a file atomically.\n\n    Clients are allowed to delete the tmpfile to signal that they don't\n    want to have it updated.\n\n    \"\"\"\n    with tempfile.NamedTemporaryFile(\n        dir=os.path.dirname(filename),\n        delete=False,\n        prefix=f\"{os.path.basename(filename)}.\",\n        **kw,\n    ) as tf:\n        if os.path.exists(filename):\n            os.chmod(tf.name, os.stat(filename).st_mode & 0o7777)\n        tf.has_changed = False  # type: ignore\n        yield tf\n        if not os.path.exists(tf.name):\n            return\n        filename_tmp = tf.name\n    if os.path.exists(filename) and filecmp.cmp(filename, filename_tmp, shallow=False):\n        os.unlink(filename_tmp)\n    else:\n        os.rename(filename_tmp, filename)\n        tf.has_changed = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_json_metadata(self, package_info: Dict) -> bool:\n\n        try:\n            with utils.rewrite(self.json_file) as jf:\n                dump(package_info, jf, indent=4, sort_keys=True)\n        except Exception as e:\n            logger.error(\n                \"Unable to write json to {}: {}\".format(self.json_file, str(e))\n            )\n            return False\n\n        symlink_dir = self.json_pypi_symlink.parent\n        if not symlink_dir.exists():\n            symlink_dir.mkdir()\n        try:\n            # If symlink already exists throw a FileExistsError\n            self.json_pypi_symlink.symlink_to(self.json_file)\n        except FileExistsError:\n            pass\n\n        return True", "response": "Save the JSON metadata to disk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the release filtering plugins", "response": "def _filter_releases(self):\n        \"\"\"\n        Run the release filtering plugins\n        \"\"\"\n        global display_filter_log\n        filter_plugins = filter_release_plugins()\n        if not filter_plugins:\n            if display_filter_log:\n                logger.info(\"No release filters are enabled. Skipping filtering\")\n                display_filter_log = False\n        else:\n            for plugin in filter_plugins:\n                plugin.filter(self.info, self.releases)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npurges + download files returning files removed + added", "response": "def sync_release_files(self):\n        \"\"\" Purge + download files returning files removed + added \"\"\"\n        release_files = []\n\n        for release in self.releases.values():\n            release_files.extend(release)\n\n        downloaded_files = set()\n        deferred_exception = None\n        for release_file in release_files:\n            try:\n                downloaded_file = self.download_file(\n                    release_file[\"url\"], release_file[\"digests\"][\"sha256\"]\n                )\n                if downloaded_file:\n                    downloaded_files.add(\n                        str(downloaded_file.relative_to(self.mirror.homedir))\n                    )\n            except Exception as e:\n                logger.exception(\n                    f\"Continuing to next file after error downloading: \"\n                    f\"{release_file['url']}\"\n                )\n                if not deferred_exception:  # keep first exception\n                    deferred_exception = e\n        if deferred_exception:\n            raise deferred_exception  # raise the exception after trying all files\n\n        self.mirror.altered_packages[self.name] = downloaded_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndoing a couple of cleanup tasks to ensure consistent data for later processing.", "response": "def _cleanup(self):\n        \"\"\"Does a couple of cleanup tasks to ensure consistent data for later\n        processing.\"\"\"\n        if self.todolist.exists():\n            try:\n                saved_todo = iter(open(self.todolist, encoding=\"utf-8\"))\n                int(next(saved_todo).strip())\n                for line in saved_todo:\n                    _, serial = line.strip().split()\n                    int(serial)\n            except (StopIteration, ValueError):\n                # The todo list was inconsistent. This may happen if we get\n                # killed e.g. by the timeout wrapper. Just remove it - we'll\n                # just have to do whatever happened since the last successful\n                # sync.\n                logger.info(\"Removing inconsistent todo list.\")\n                self.todolist.unlink()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter the packages in the packages_to_sync list and remove any packages that match any filters.", "response": "def _filter_packages(self):\n        \"\"\"\n        Run the package filtering plugins and remove any packages from the\n        packages_to_sync that match any filters.\n        - Logging of action will be done within the check_match methods\n        \"\"\"\n        global LOG_PLUGINS\n\n        filter_plugins = filter_project_plugins()\n        if not filter_plugins:\n            if LOG_PLUGINS:\n                logger.info(\"No project filters are enabled. Skipping filtering\")\n                LOG_PLUGINS = False\n            return\n\n        # Make a copy of self.packages_to_sync keys\n        # as we may delete packages during iteration\n        packages = list(self.packages_to_sync.keys())\n        for package_name in packages:\n            for plugin in filter_plugins:\n                if plugin.check_match(name=package_name):\n                    if package_name not in self.packages_to_sync:\n                        logger.debug(\n                            f\"{package_name} not found in packages to sync - \"\n                            + f\"{plugin.name} has no effect here ...\"\n                        )\n                    else:\n                        del self.packages_to_sync[package_name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef determine_packages_to_sync(self):\n        # In case we don't find any changes we will stay on the currently\n        # synced serial.\n        self.target_serial = self.synced_serial\n        self.packages_to_sync = {}\n        logger.info(f\"Current mirror serial: {self.synced_serial}\")\n\n        if self.todolist.exists():\n            # We started a sync previously and left a todo list as well as the\n            # targetted serial. We'll try to keep going through the todo list\n            # and then mark the targetted serial as done.\n            logger.info(\"Resuming interrupted sync from local todo list.\")\n            saved_todo = iter(open(self.todolist, encoding=\"utf-8\"))\n            self.target_serial = int(next(saved_todo).strip())\n            for line in saved_todo:\n                package, serial = line.strip().split()\n                self.packages_to_sync[package] = int(serial)\n        elif not self.synced_serial:\n            logger.info(\"Syncing all packages.\")\n            # First get the current serial, then start to sync. This makes us\n            # more defensive in case something changes on the server between\n            # those two calls.\n            self.packages_to_sync.update(self.master.all_packages())\n            self.target_serial = max(\n                [self.synced_serial] + list(self.packages_to_sync.values())\n            )\n        else:\n            logger.info(\"Syncing based on changelog.\")\n            self.packages_to_sync.update(\n                self.master.changed_packages(self.synced_serial)\n            )\n            self.target_serial = max(\n                [self.synced_serial] + list(self.packages_to_sync.values())\n            )\n            # We can avoid downloading the main index page if we don't have\n            # anything todo at all during a changelog-based sync.\n            self.need_index_sync = bool(self.packages_to_sync)\n\n        self._filter_packages()\n        logger.info(f\"Trying to reach serial: {self.target_serial}\")\n        pkg_count = len(self.packages_to_sync)\n        logger.info(f\"{pkg_count} packages to sync.\")", "response": "Determine the packages that need to be synced."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_simple_dirs(self, simple_dir: Path) -> List[Path]:\n        if self.hash_index:\n            # We are using index page directory hashing, so the directory\n            # format is /simple/f/foo/.  We want to return a list of dirs\n            # like \"simple/f\".\n            subdirs = [simple_dir / x for x in simple_dir.iterdir() if x.is_dir()]\n        else:\n            # This is the traditional layout of /simple/foo/.  We should\n            # return a single directory, \"simple\".\n            subdirs = [simple_dir]\n        return subdirs", "response": "Return a list of simple index directories that should be searched\n            for package indexes when compiling the main index page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_package_indexes_in_dir(self, simple_dir):\n        packages = sorted(\n            {\n                # Filter out all of the \"non\" normalized names here\n                canonicalize_name(x)\n                for x in os.listdir(simple_dir)\n            }\n        )\n        # Package indexes must be in directories, so ignore anything else.\n        packages = [x for x in packages if os.path.isdir(os.path.join(simple_dir, x))]\n        return packages", "response": "Given a directory that contains simple packages indexes return\n        a sorted list of normalized package names."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_configuration(self) -> None:\n        config_file = self.default_config_file\n        if self.config_file:\n            config_file = self.config_file\n        self.config = ConfigParser()\n        self.config.read(config_file)", "response": "Read the configuration from a configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify that all saved JSON metadata files exist and delete if delete - generate a diff of unowned files", "response": "async def metadata_verify(config, args) -> int:\n    \"\"\" Crawl all saved JSON metadata or online to check we have all packages\n        if delete - generate a diff of unowned files  \"\"\"\n    all_package_files = []  # type: List[Path]\n    loop = asyncio.get_event_loop()\n    mirror_base = config.get(\"mirror\", \"directory\")\n    json_base = Path(mirror_base) / \"web\" / \"json\"\n    workers = args.workers or config.getint(\"mirror\", \"workers\")\n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=workers)\n\n    logger.info(f\"Starting verify for {mirror_base} with {workers} workers\")\n    try:\n        json_files = await loop.run_in_executor(executor, os.listdir, json_base)\n    except FileExistsError as fee:\n        logger.error(f\"Metadata base dir {json_base} does not exist: {fee}\")\n        return 2\n    if not json_files:\n        logger.error(\"No JSON metadata files found. Can not verify\")\n        return 3\n\n    logger.debug(f\"Found {len(json_files)} objects in {json_base}\")\n    logger.debug(f\"Using a {workers} thread ThreadPoolExecutor\")\n    await async_verify(\n        config, all_package_files, mirror_base, json_files, args, executor\n    )\n\n    if not args.delete:\n        return 0\n\n    return await delete_files(mirror_base, executor, all_package_files, args.dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _der_to_pem(der_key, marker):\n    pem_key_chunks = [('-----BEGIN %s-----' % marker).encode('utf-8')]\n\n    # Limit base64 output lines to 64 characters by limiting input lines to 48 characters.\n    for chunk_start in range(0, len(der_key), 48):\n        pem_key_chunks.append(b64encode(der_key[chunk_start:chunk_start + 48]))\n\n    pem_key_chunks.append(('-----END %s-----' % marker).encode('utf-8'))\n\n    return b'\\n'.join(pem_key_chunks)", "response": "Perform a simple DER to PEM conversion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a PKCS8 - encoded RSA private key to PKCS1.", "response": "def rsa_private_key_pkcs8_to_pkcs1(pkcs8_key):\n    \"\"\"Convert a PKCS8-encoded RSA private key to PKCS1.\"\"\"\n    decoded_values = decoder.decode(pkcs8_key, asn1Spec=PKCS8PrivateKey())\n\n    try:\n        decoded_key = decoded_values[0]\n    except IndexError:\n        raise ValueError(\"Invalid private key encoding\")\n\n    return decoded_key[\"privateKey\"]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a PKCS1 - encoded RSA private key to PKCS8.", "response": "def rsa_private_key_pkcs1_to_pkcs8(pkcs1_key):\n    \"\"\"Convert a PKCS1-encoded RSA private key to PKCS8.\"\"\"\n    algorithm = RsaAlgorithmIdentifier()\n    algorithm[\"rsaEncryption\"] = RSA_ENCRYPTION_ASN1_OID\n\n    pkcs8_key = PKCS8PrivateKey()\n    pkcs8_key[\"version\"] = 0\n    pkcs8_key[\"privateKeyAlgorithm\"] = algorithm\n    pkcs8_key[\"privateKey\"] = pkcs1_key\n\n    return encoder.encode(pkcs8_key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rsa_public_key_pkcs1_to_pkcs8(pkcs1_key):\n    algorithm = RsaAlgorithmIdentifier()\n    algorithm[\"rsaEncryption\"] = RSA_ENCRYPTION_ASN1_OID\n\n    pkcs8_key = PublicKeyInfo()\n    pkcs8_key[\"algorithm\"] = algorithm\n    pkcs8_key[\"publicKey\"] = univ.BitString.fromOctetString(pkcs1_key)\n\n    return encoder.encode(pkcs8_key)", "response": "Convert a PKCS1 - encoded RSA private key to PKCS8."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a PKCS8 - encoded RSA private key to PKCS1.", "response": "def rsa_public_key_pkcs8_to_pkcs1(pkcs8_key):\n    \"\"\"Convert a PKCS8-encoded RSA private key to PKCS1.\"\"\"\n    decoded_values = decoder.decode(pkcs8_key, asn1Spec=PublicKeyInfo())\n\n    try:\n        decoded_key = decoded_values[0]\n    except IndexError:\n        raise ValueError(\"Invalid public key encoding.\")\n\n    return decoded_key[\"publicKey\"].asOctets()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a signature from DER encoding to RAW encoding.", "response": "def _der_to_raw(self, der_signature):\n        \"\"\"Convert signature from DER encoding to RAW encoding.\"\"\"\n        r, s = decode_dss_signature(der_signature)\n        component_length = self._sig_component_length()\n        return int_to_bytes(r, component_length) + int_to_bytes(s, component_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a raw signature from RAW encoding to DER encoding.", "response": "def _raw_to_der(self, raw_signature):\n        \"\"\"Convert signature from RAW encoding to DER encoding.\"\"\"\n        component_length = self._sig_component_length()\n        if len(raw_signature) != int(2 * component_length):\n            raise ValueError(\"Invalid signature\")\n\n        r_bytes = raw_signature[:component_length]\n        s_bytes = raw_signature[component_length:]\n        r = int_from_bytes(r_bytes, \"big\")\n        s = int_from_bytes(s_bytes, \"big\")\n        return encode_dss_signature(r, s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencode a claims set and returns a JWT string.", "response": "def encode(claims, key, algorithm=ALGORITHMS.HS256, headers=None, access_token=None):\n    \"\"\"Encodes a claims set and returns a JWT string.\n\n    JWTs are JWS signed objects with a few reserved claims.\n\n    Args:\n        claims (dict): A claims set to sign\n        key (str or dict): The key to use for signing the claim set. Can be\n            individual JWK or JWK set.\n        algorithm (str, optional): The algorithm to use for signing the\n            the claims.  Defaults to HS256.\n        headers (dict, optional): A set of headers that will be added to\n            the default headers.  Any headers that are added as additional\n            headers will override the default headers.\n        access_token (str, optional): If present, the 'at_hash' claim will\n            be calculated and added to the claims present in the 'claims'\n            parameter.\n\n    Returns:\n        str: The string representation of the header, claims, and signature.\n\n    Raises:\n        JWTError: If there is an error encoding the claims.\n\n    Examples:\n\n        >>> jwt.encode({'a': 'b'}, 'secret', algorithm='HS256')\n        'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhIjoiYiJ9.jiMyrsmD8AoHWeQgmxZ5yq8z0lXS67_QGs52AzC8Ru8'\n\n    \"\"\"\n\n    for time_claim in ['exp', 'iat', 'nbf']:\n\n        # Convert datetime to a intDate value in known time-format claims\n        if isinstance(claims.get(time_claim), datetime):\n            claims[time_claim] = timegm(claims[time_claim].utctimetuple())\n\n    if access_token:\n        claims['at_hash'] = calculate_at_hash(access_token,\n                                              ALGORITHMS.HASHES[algorithm])\n\n    return jws.sign(claims, key, headers=headers, algorithm=algorithm)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode(token, key, algorithms=None, options=None, audience=None,\n           issuer=None, subject=None, access_token=None):\n    \"\"\"Verifies a JWT string's signature and validates reserved claims.\n\n    Args:\n        token (str): A signed JWS to be verified.\n        key (str or dict): A key to attempt to verify the payload with. Can be\n            individual JWK or JWK set.\n        algorithms (str or list): Valid algorithms that should be used to verify the JWS.\n        audience (str): The intended audience of the token.  If the \"aud\" claim is\n            included in the claim set, then the audience must be included and must equal\n            the provided claim.\n        issuer (str or iterable): Acceptable value(s) for the issuer of the token.\n            If the \"iss\" claim is included in the claim set, then the issuer must be\n            given and the claim in the token must be among the acceptable values.\n        subject (str): The subject of the token.  If the \"sub\" claim is\n            included in the claim set, then the subject must be included and must equal\n            the provided claim.\n        access_token (str): An access token string. If the \"at_hash\" claim is included in the\n            claim set, then the access_token must be included, and it must match\n            the \"at_hash\" claim.\n        options (dict): A dictionary of options for skipping validation steps.\n\n            defaults = {\n                'verify_signature': True,\n                'verify_aud': True,\n                'verify_iat': True,\n                'verify_exp': True,\n                'verify_nbf': True,\n                'verify_iss': True,\n                'verify_sub': True,\n                'verify_jti': True,\n                'verify_at_hash': True,\n                'require_aud': False,\n                'require_iat': False,\n                'require_exp': False,\n                'require_nbf': False,\n                'require_iss': False,\n                'require_sub': False,\n                'require_jti': False,\n                'require_at_hash': False,\n                'leeway': 0,\n            }\n\n    Returns:\n        dict: The dict representation of the claims set, assuming the signature is valid\n            and all requested data validation passes.\n\n    Raises:\n        JWTError: If the signature is invalid in any way.\n        ExpiredSignatureError: If the signature has expired.\n        JWTClaimsError: If any claim is invalid in any way.\n\n    Examples:\n\n        >>> payload = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhIjoiYiJ9.jiMyrsmD8AoHWeQgmxZ5yq8z0lXS67_QGs52AzC8Ru8'\n        >>> jwt.decode(payload, 'secret', algorithms='HS256')\n\n    \"\"\"\n\n    defaults = {\n        'verify_signature': True,\n        'verify_aud': True,\n        'verify_iat': True,\n        'verify_exp': True,\n        'verify_nbf': True,\n        'verify_iss': True,\n        'verify_sub': True,\n        'verify_jti': True,\n        'verify_at_hash': True,\n        'require_aud': False,\n        'require_iat': False,\n        'require_exp': False,\n        'require_nbf': False,\n        'require_iss': False,\n        'require_sub': False,\n        'require_jti': False,\n        'require_at_hash': False,\n        'leeway': 0,\n    }\n\n    if options:\n        defaults.update(options)\n\n    verify_signature = defaults.get('verify_signature', True)\n\n    try:\n        payload = jws.verify(token, key, algorithms, verify=verify_signature)\n    except JWSError as e:\n        raise JWTError(e)\n\n    # Needed for at_hash verification\n    algorithm = jws.get_unverified_header(token)['alg']\n\n    try:\n        claims = json.loads(payload.decode('utf-8'))\n    except ValueError as e:\n        raise JWTError('Invalid payload string: %s' % e)\n\n    if not isinstance(claims, Mapping):\n        raise JWTError('Invalid payload string: must be a json object')\n\n    _validate_claims(claims, audience=audience, issuer=issuer,\n                     subject=subject, algorithm=algorithm,\n                     access_token=access_token,\n                     options=defaults)\n\n    return claims", "response": "Verifies a signed JWS and validates reserved claims."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the nbf claim is valid.", "response": "def _validate_nbf(claims, leeway=0):\n    \"\"\"Validates that the 'nbf' claim is valid.\n\n    The \"nbf\" (not before) claim identifies the time before which the JWT\n    MUST NOT be accepted for processing.  The processing of the \"nbf\"\n    claim requires that the current date/time MUST be after or equal to\n    the not-before date/time listed in the \"nbf\" claim.  Implementers MAY\n    provide for some small leeway, usually no more than a few minutes, to\n    account for clock skew.  Its value MUST be a number containing a\n    NumericDate value.  Use of this claim is OPTIONAL.\n\n    Args:\n        claims (dict): The claims dictionary to validate.\n        leeway (int): The number of seconds of skew that is allowed.\n    \"\"\"\n\n    if 'nbf' not in claims:\n        return\n\n    try:\n        nbf = int(claims['nbf'])\n    except ValueError:\n        raise JWTClaimsError('Not Before claim (nbf) must be an integer.')\n\n    now = timegm(datetime.utcnow().utctimetuple())\n\n    if nbf > (now + leeway):\n        raise JWTClaimsError('The token is not yet valid (nbf)')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_exp(claims, leeway=0):\n\n    if 'exp' not in claims:\n        return\n\n    try:\n        exp = int(claims['exp'])\n    except ValueError:\n        raise JWTClaimsError('Expiration Time claim (exp) must be an integer.')\n\n    now = timegm(datetime.utcnow().utctimetuple())\n\n    if exp < (now - leeway):\n        raise ExpiredSignatureError('Signature has expired.')", "response": "Validates that the exp claim is valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_aud(claims, audience=None):\n\n    if 'aud' not in claims:\n        # if audience:\n        #     raise JWTError('Audience claim expected, but not in claims')\n        return\n\n    audience_claims = claims['aud']\n    if isinstance(audience_claims, string_types):\n        audience_claims = [audience_claims]\n    if not isinstance(audience_claims, list):\n        raise JWTClaimsError('Invalid claim format in token')\n    if any(not isinstance(c, string_types) for c in audience_claims):\n        raise JWTClaimsError('Invalid claim format in token')\n    if audience not in audience_claims:\n        raise JWTClaimsError('Invalid audience')", "response": "Validates that the aud claim is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that the iss claim is valid.", "response": "def _validate_iss(claims, issuer=None):\n    \"\"\"Validates that the 'iss' claim is valid.\n\n    The \"iss\" (issuer) claim identifies the principal that issued the\n    JWT.  The processing of this claim is generally application specific.\n    The \"iss\" value is a case-sensitive string containing a StringOrURI\n    value.  Use of this claim is OPTIONAL.\n\n    Args:\n        claims (dict): The claims dictionary to validate.\n        issuer (str or iterable): Acceptable value(s) for the issuer that\n                                  signed the token.\n    \"\"\"\n\n    if issuer is not None:\n        if isinstance(issuer, string_types):\n            issuer = (issuer,)\n        if claims.get('iss') not in issuer:\n            raise JWTClaimsError('Invalid issuer')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the sub claim is valid.", "response": "def _validate_sub(claims, subject=None):\n    \"\"\"Validates that the 'sub' claim is valid.\n\n    The \"sub\" (subject) claim identifies the principal that is the\n    subject of the JWT.  The claims in a JWT are normally statements\n    about the subject.  The subject value MUST either be scoped to be\n    locally unique in the context of the issuer or be globally unique.\n    The processing of this claim is generally application specific.  The\n    \"sub\" value is a case-sensitive string containing a StringOrURI\n    value.  Use of this claim is OPTIONAL.\n\n    Args:\n        claims (dict): The claims dictionary to validate.\n        subject (str): The subject of the token.\n    \"\"\"\n\n    if 'sub' not in claims:\n        return\n\n    if not isinstance(claims['sub'], string_types):\n        raise JWTClaimsError('Subject must be a string.')\n\n    if subject is not None:\n        if claims.get('sub') != subject:\n            raise JWTClaimsError('Invalid subject')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gcd(a, b):\n    while b:\n        a, b = b, (a % b)\n    return a", "response": "Calculate the Greatest Common Divisor of a and b."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing factors p and q from the private exponent d. We assume that n has no more than two factors. This function is adapted from code in PyCrypto.", "response": "def _rsa_recover_prime_factors(n, e, d):\n    \"\"\"\n    Compute factors p and q from the private exponent d. We assume that n has\n    no more than two factors. This function is adapted from code in PyCrypto.\n    \"\"\"\n    # See 8.2.2(i) in Handbook of Applied Cryptography.\n    ktot = d * e - 1\n    # The quantity d*e-1 is a multiple of phi(n), even,\n    # and can be represented as t*2^s.\n    t = ktot\n    while t % 2 == 0:\n        t = t // 2\n    # Cycle through all multiplicative inverses in Zn.\n    # The algorithm is non-deterministic, but there is a 50% chance\n    # any candidate a leads to successful factoring.\n    # See \"Digitalized Signatures and Public Key Functions as Intractable\n    # as Factorization\", M. Rabin, 1979\n    spotted = False\n    a = 2\n    while not spotted and a < _MAX_RECOVERY_ATTEMPTS:\n        k = t\n        # Cycle through all values a^{t*2^i}=a^k\n        while k < ktot:\n            cand = pow(a, k, n)\n            # Check if a^k is a non-trivial root of unity (mod n)\n            if cand != 1 and cand != (n - 1) and pow(cand, 2, n) == 1:\n                # We have found a number such that (cand-1)(cand+1)=0 (mod n).\n                # Either of the terms divides n.\n                p = _gcd(cand + 1, n)\n                spotted = True\n                break\n            k *= 2\n        # This value was not any good... let's try another!\n        a += 2\n    if not spotted:\n        raise ValueError(\"Unable to compute factors p and q from exponent d.\")\n    # Found !\n    q, r = divmod(n, p)\n    assert r == 0\n    p, q = sorted((p, q), reverse=True)\n    return (p, q)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a legacy RSA private key to PKCS1.", "response": "def _legacy_private_key_pkcs8_to_pkcs1(pkcs8_key):\n    \"\"\"Legacy RSA private key PKCS8-to-PKCS1 conversion.\n\n    .. warning::\n\n        This is incorrect parsing and only works because the legacy PKCS1-to-PKCS8\n        encoding was also incorrect.\n    \"\"\"\n    # Only allow this processing if the prefix matches\n    # AND the following byte indicates an ASN1 sequence,\n    # as we would expect with the legacy encoding.\n    if not pkcs8_key.startswith(LEGACY_INVALID_PKCS8_RSA_HEADER + ASN1_SEQUENCE_ID):\n        raise ValueError(\"Invalid private key encoding\")\n\n    return pkcs8_key[len(LEGACY_INVALID_PKCS8_RSA_HEADER):]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the at_hash value of the given access token.", "response": "def calculate_at_hash(access_token, hash_alg):\n    \"\"\"Helper method for calculating an access token\n    hash, as described in http://openid.net/specs/openid-connect-core-1_0.html#CodeIDToken\n\n    Its value is the base64url encoding of the left-most half of the hash of the octets\n    of the ASCII representation of the access_token value, where the hash algorithm\n    used is the hash algorithm used in the alg Header Parameter of the ID Token's JOSE\n    Header. For instance, if the alg is RS256, hash the access_token value with SHA-256,\n    then take the left-most 128 bits and base64url encode them. The at_hash value is a\n    case sensitive string.\n\n    Args:\n        access_token (str): An access token string.\n        hash_alg (callable): A callable returning a hash object, e.g. hashlib.sha256\n\n    \"\"\"\n    hash_digest = hash_alg(access_token.encode('utf-8')).digest()\n    cut_at = int(len(hash_digest) / 2)\n    truncated = hash_digest[:cut_at]\n    at_hash = base64url_encode(truncated)\n    return at_hash.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef constant_time_string_compare(a, b):\n\n    try:\n        return hmac.compare_digest(a, b)\n    except AttributeError:\n\n        if len(a) != len(b):\n            return False\n\n        result = 0\n\n        for x, y in zip(a, b):\n            result |= ord(x) ^ ord(y)\n\n        return result == 0", "response": "Helper function for comparing string in constant time independent\n    of the python version being used."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sign(payload, key, headers=None, algorithm=ALGORITHMS.HS256):\n\n    if algorithm not in ALGORITHMS.SUPPORTED:\n        raise JWSError('Algorithm %s not supported.' % algorithm)\n\n    encoded_header = _encode_header(algorithm, additional_headers=headers)\n    encoded_payload = _encode_payload(payload)\n    signed_output = _sign_header_and_claims(encoded_header, encoded_payload, algorithm, key)\n\n    return signed_output", "response": "Signs a claims set and returns a JWS string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies a JWS string s signature.", "response": "def verify(token, key, algorithms, verify=True):\n    \"\"\"Verifies a JWS string's signature.\n\n    Args:\n        token (str): A signed JWS to be verified.\n        key (str or dict): A key to attempt to verify the payload with. Can be\n            individual JWK or JWK set.\n        algorithms (str or list): Valid algorithms that should be used to verify the JWS.\n\n    Returns:\n        str: The str representation of the payload, assuming the signature is valid.\n\n    Raises:\n        JWSError: If there is an exception verifying a token.\n\n    Examples:\n\n        >>> token = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhIjoiYiJ9.jiMyrsmD8AoHWeQgmxZ5yq8z0lXS67_QGs52AzC8Ru8'\n        >>> jws.verify(token, 'secret', algorithms='HS256')\n\n    \"\"\"\n\n    header, payload, signing_input, signature = _load(token)\n\n    if verify:\n        _verify_signature(signing_input, header, signature, key, algorithms)\n\n    return payload"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef construct(key_data, algorithm=None):\n\n    # Allow for pulling the algorithm off of the passed in jwk.\n    if not algorithm and isinstance(key_data, dict):\n        algorithm = key_data.get('alg', None)\n\n    if not algorithm:\n        raise JWKError('Unable to find a algorithm for key: %s' % key_data)\n\n    key_class = get_key(algorithm)\n    if not key_class:\n        raise JWKError('Unable to find a algorithm for key: %s' % key_data)\n    return key_class(key_data, algorithm)", "response": "Construct a Key object for the given algorithm with the given key_data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef report_entry_label(self, entry_id):\n\n        pruning = self._find_root_pruning(entry_id)\n        return self.classes[self._best_label(pruning)]", "response": "Returns the best label of the asked entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report_all_label(self):\n\n        labels = np.empty(len(self.dataset), dtype=int)\n        for pruning in self.prunings:\n            best_label = self._best_label(pruning)\n            leaves = self._find_leaves(pruning)\n            labels[leaves] = best_label\n        return labels", "response": "Returns the best label of the asked entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef train(self, dataset):\n        X, Y = dataset.format_sklearn()\n        X = np.array(X)\n        Y = np.array(Y)\n\n        self.n_labels_ = np.shape(Y)[1]\n        self.n_features_ = np.shape(X)[1]\n\n        self.clfs_ = []\n        for i in range(self.n_labels_):\n            # TODO should we handle it here or we should handle it before calling\n            if len(np.unique(Y[:, i])) == 1:\n                clf = DummyClf()\n            else:\n                clf = copy.deepcopy(self.base_clf)\n            self.clfs_.append(clf)\n\n        Parallel(n_jobs=self.n_jobs, backend='threading')(\n            delayed(_fit_model)(self.clfs_[i], X, Y[:, i])\n            for i in range(self.n_labels_))\n        #clf.train(Dataset(X, Y[:, i]))\n\n        return self", "response": "Train model with given feature vector."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict_real(self, X):\n        X = np.asarray(X)\n        if self.clfs_ is None:\n            raise ValueError(\"Train before prediction\")\n        if X.shape[1] != self.n_features_:\n            raise ValueError('given feature size does not match')\n\n        pred = np.zeros((X.shape[0], self.n_labels_))\n        for i in range(self.n_labels_):\n            pred[:, i] = self.clfs_[i].predict_real(X)[:, 1]\n        return pred", "response": "r Predict the probability of being 1 for each label."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef score(self, testing_dataset, criterion='hamming'):\n        # TODO check if data in dataset are all correct\n        X, Y = testing_dataset.format_sklearn()\n        if criterion == 'hamming':\n            return np.mean(np.abs(self.predict(X) - Y).mean(axis=1))\n        elif criterion == 'f1':\n            Z = self.predict(X)\n            Z = Z.astype(int)\n            Y = Y.astype(int)\n            up = 2*np.sum(Z & Y, axis=1).astype(float)\n            down1 = np.sum(Z, axis=1)\n            down2 = np.sum(Y, axis=1)\n\n            down = (down1 + down2)\n            down[down==0] = 1.\n            up[down==0] = 1.\n            return np.mean(up / down)\n        else:\n            raise NotImplementedError(\n                \"criterion '%s' not implemented\" % criterion)", "response": "Return the mean accuracy on the test dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npredicts the probability of a given cluster being labeled.", "response": "def predict(self):\n        \"\"\"\n        Returns\n        -------\n        proba : ndarray, shape=(n_clusters, )\n            The probability of given cluster being label 1.\n\n        \"\"\"\n        if self.w_ is not None:\n            sigmoid = lambda t: 1. / (1. + np.exp(-t))\n            return sigmoid(np.dot(self.centers, self.w_[:-1]) + self.w_[-1])\n        else:\n            # TODO the model is not trained\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a multidimensional scaling of the given set of points.", "response": "def _smacof_single_p(similarities, n_uq, metric=True, n_components=2, init=None,\r\n                   max_iter=300, verbose=0, eps=1e-3, random_state=None):\r\n    \"\"\"\r\n    Computes multidimensional scaling using SMACOF algorithm\r\n\r\n    Parameters\r\n    ----------\r\n    n_uq\r\n\r\n    similarities: symmetric ndarray, shape [n * n]\r\n        similarities between the points\r\n\r\n    metric: boolean, optional, default: True\r\n        compute metric or nonmetric SMACOF algorithm\r\n\r\n    n_components: int, optional, default: 2\r\n        number of dimension in which to immerse the similarities\r\n        overwritten if initial array is provided.\r\n\r\n    init: {None or ndarray}, optional\r\n        if None, randomly chooses the initial configuration\r\n        if ndarray, initialize the SMACOF algorithm with this array\r\n\r\n    max_iter: int, optional, default: 300\r\n        Maximum number of iterations of the SMACOF algorithm for a single run\r\n\r\n    verbose: int, optional, default: 0\r\n        level of verbosity\r\n\r\n    eps: float, optional, default: 1e-6\r\n        relative tolerance w.r.t stress to declare converge\r\n\r\n    random_state: integer or numpy.RandomState, optional\r\n        The generator used to initialize the centers. If an integer is\r\n        given, it fixes the seed. Defaults to the global numpy random\r\n        number generator.\r\n\r\n    Returns\r\n    -------\r\n    X: ndarray (n_samples, n_components), float\r\n               coordinates of the n_samples points in a n_components-space\r\n\r\n    stress_: float\r\n        The final value of the stress (sum of squared distance of the\r\n        disparities and the distances for all constrained points)\r\n\r\n    n_iter : int\r\n        Number of iterations run.\r\n\r\n    \"\"\"\r\n    similarities = check_symmetric(similarities, raise_exception=True)\r\n\r\n    n_samples = similarities.shape[0]\r\n    random_state = check_random_state(random_state)\r\n\r\n    W = np.ones((n_samples, n_samples))\r\n    W[:n_uq, :n_uq] = 0.0\r\n    W[n_uq:, n_uq:] = 0.0\r\n    # W[np.arange(len(W)), np.arange(len(W))] = 0.0\r\n\r\n    V = -W\r\n    V[np.arange(len(V)), np.arange(len(V))] = W.sum(axis=1)\r\n    e = np.ones((n_samples, 1))\r\n\r\n    Vp = np.linalg.inv(V + np.dot(e, e.T)/n_samples) - np.dot(e, e.T)/n_samples\r\n    # Vp = np.linalg.pinv(V)\r\n\r\n    # sim_flat = ((1 - np.tri(n_samples)) * similarities).ravel()\r\n    sim_flat = similarities.ravel()\r\n    sim_flat_w = sim_flat[sim_flat != 0]\r\n    if init is None:\r\n        # Randomly choose initial configuration\r\n        X = random_state.rand(n_samples * n_components)\r\n        X = X.reshape((n_samples, n_components))\r\n    else:\r\n        # overrides the parameter p\r\n        n_components = init.shape[1]\r\n        if n_samples != init.shape[0]:\r\n            raise ValueError(\"init matrix should be of shape (%d, %d)\" %\r\n                             (n_samples, n_components))\r\n        X = init\r\n\r\n    old_stress = None\r\n    ir = IsotonicRegression()\r\n    for it in range(max_iter):\r\n        # Compute distance and monotonic regression\r\n        dis = euclidean_distances(X)\r\n\r\n        if metric:\r\n            disparities = similarities\r\n        else:\r\n            # dis_flat = dis.ravel()\r\n            # # similarities with 0 are considered as missing values\r\n            # dis_flat_w = dis_flat[sim_flat != 0]\r\n\r\n            # # Compute the disparities using a monotonic regression\r\n            # disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\r\n            # disparities = dis_flat.copy()\r\n            # disparities[sim_flat != 0] = disparities_flat\r\n            # disparities = disparities.reshape((n_samples, n_samples))\r\n            # disparities *= np.sqrt((n_samples * (n_samples - 1) / 2) /\r\n            #                        (disparities ** 2).sum())\r\n            \r\n            dis_flat = dis.ravel()\r\n            # similarities with 0 are considered as missing values\r\n            dis_flat_w = dis_flat[sim_flat != 0]\r\n\r\n            # Compute the disparities using a monotonic regression\r\n            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\r\n            disparities = dis_flat.copy()\r\n            disparities[sim_flat != 0] = disparities_flat\r\n            disparities = disparities.reshape((n_samples, n_samples))\r\n            disparities *= np.sqrt((n_samples * (n_samples - 1) / 2) / (disparities ** 2).sum())\r\n            disparities[similarities==0] = 0\r\n\r\n        # Compute stress\r\n        # stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2\r\n        _stress = (W.ravel()*((dis.ravel() - disparities.ravel()) ** 2)).sum() / 2\r\n\r\n        # Update X using the Guttman transform\r\n        # dis[dis == 0] = 1e-5\r\n        # ratio = disparities / dis\r\n        # B = - ratio\r\n        # B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)\r\n        # X = 1. / n_samples * np.dot(B, X)\r\n        # print (1. / n_samples * np.dot(B, X))[:5].T\r\n\r\n        dis[dis == 0] = 1e-5\r\n        ratio = disparities / dis\r\n        _B = - W*ratio\r\n        _B[np.arange(len(_B)), np.arange(len(_B))] += (W*ratio).sum(axis=1)\r\n\r\n        X = np.dot(Vp, np.dot(_B, X))\r\n        # print X[:5].T\r\n\r\n        dis = np.sqrt((X ** 2).sum(axis=1)).sum()\r\n        \r\n        if verbose >= 2:\r\n            print('it: %d, stress %s' % (it, _stress))\r\n        if old_stress is not None:\r\n            if(old_stress - _stress / dis) < eps:\r\n                if verbose:\r\n                    print('breaking at iteration %d with stress %s' % (it,\r\n                                                                       _stress))\r\n                break\r\n        old_stress = _stress / dis\r\n\r\n    return X, _stress, it + 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new instance of the multidimensional scaling algorithm for the given set of similarities.", "response": "def smacof_p(similarities, n_uq, metric=True, n_components=2, init=None, n_init=8,\r\n           n_jobs=1, max_iter=300, verbose=0, eps=1e-3, random_state=None,\r\n           return_n_iter=False):\r\n    \"\"\"\r\n    Computes multidimensional scaling using SMACOF (Scaling by Majorizing a\r\n    Complicated Function) algorithm\r\n\r\n    The SMACOF algorithm is a multidimensional scaling algorithm: it minimizes\r\n    a objective function, the *stress*, using a majorization technique. The\r\n    Stress Majorization, also known as the Guttman Transform, guarantees a\r\n    monotone convergence of Stress, and is more powerful than traditional\r\n    techniques such as gradient descent.\r\n\r\n    The SMACOF algorithm for metric MDS can summarized by the following steps:\r\n\r\n    1. Set an initial start configuration, randomly or not.\r\n    2. Compute the stress\r\n    3. Compute the Guttman Transform\r\n    4. Iterate 2 and 3 until convergence.\r\n\r\n    The nonmetric algorithm adds a monotonic regression steps before computing\r\n    the stress.\r\n\r\n    Parameters\r\n    ----------\r\n    similarities : symmetric ndarray, shape (n_samples, n_samples)\r\n        similarities between the points\r\n\r\n    metric : boolean, optional, default: True\r\n        compute metric or nonmetric SMACOF algorithm\r\n\r\n    n_components : int, optional, default: 2\r\n        number of dimension in which to immerse the similarities\r\n        overridden if initial array is provided.\r\n\r\n    init : {None or ndarray of shape (n_samples, n_components)}, optional\r\n        if None, randomly chooses the initial configuration\r\n        if ndarray, initialize the SMACOF algorithm with this array\r\n\r\n    n_init : int, optional, default: 8\r\n        Number of time the smacof_p algorithm will be run with different\r\n        initialisation. The final results will be the best output of the\r\n        n_init consecutive runs in terms of stress.\r\n\r\n    n_jobs : int, optional, default: 1\r\n\r\n        The number of jobs to use for the computation. This works by breaking\r\n        down the pairwise matrix into n_jobs even slices and computing them in\r\n        parallel.\r\n\r\n        If -1 all CPUs are used. If 1 is given, no parallel computing code is\r\n        used at all, which is useful for debugging. For n_jobs below -1,\r\n        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\r\n        are used.\r\n\r\n    max_iter : int, optional, default: 300\r\n        Maximum number of iterations of the SMACOF algorithm for a single run\r\n\r\n    verbose : int, optional, default: 0\r\n        level of verbosity\r\n\r\n    eps : float, optional, default: 1e-6\r\n        relative tolerance w.r.t stress to declare converge\r\n\r\n    random_state : integer or numpy.RandomState, optional\r\n        The generator used to initialize the centers. If an integer is\r\n        given, it fixes the seed. Defaults to the global numpy random\r\n        number generator.\r\n\r\n    return_n_iter : bool\r\n        Whether or not to return the number of iterations.\r\n\r\n    Returns\r\n    -------\r\n    X : ndarray (n_samples,n_components)\r\n        Coordinates of the n_samples points in a n_components-space\r\n\r\n    stress : float\r\n        The final value of the stress (sum of squared distance of the\r\n        disparities and the distances for all constrained points)\r\n\r\n    n_iter : int\r\n        The number of iterations corresponding to the best stress.\r\n        Returned only if `return_n_iter` is set to True.\r\n\r\n    Notes\r\n    -----\r\n    \"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.;\r\n    Groenen P. Springer Series in Statistics (1997)\r\n\r\n    \"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J.\r\n    Psychometrika, 29 (1964)\r\n\r\n    \"Multidimensional scaling by optimizing goodness of fit to a nonmetric\r\n    hypothesis\" Kruskal, J. Psychometrika, 29, (1964)\r\n    \"\"\"\r\n\r\n    similarities = check_array(similarities)\r\n    random_state = check_random_state(random_state)\r\n\r\n    if hasattr(init, '__array__'):\r\n        init = np.asarray(init).copy()\r\n        if not n_init == 1:\r\n            warnings.warn(\r\n                'Explicit initial positions passed: '\r\n                'performing only one init of the MDS instead of %d'\r\n                % n_init)\r\n            n_init = 1\r\n\r\n    best_pos, best_stress = None, None\r\n\r\n    if n_jobs == 1:\r\n        for it in range(n_init):\r\n            pos, stress, n_iter_ = _smacof_single_p(\r\n                similarities, n_uq, metric=metric,\r\n                n_components=n_components, init=init,\r\n                max_iter=max_iter, verbose=verbose,\r\n                eps=eps, random_state=random_state)\r\n            if best_stress is None or stress < best_stress:\r\n                best_stress = stress\r\n                best_pos = pos.copy()\r\n                best_iter = n_iter_\r\n    else:\r\n        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\r\n        results = Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))(\r\n            delayed(_smacof_single_p)(\r\n                similarities, n_uq, metric=metric, n_components=n_components,\r\n                init=init, max_iter=max_iter, verbose=verbose, eps=eps,\r\n                random_state=seed)\r\n            for seed in seeds)\r\n        positions, stress, n_iters = zip(*results)\r\n        best = np.argmin(stress)\r\n        best_stress = stress[best]\r\n        best_pos = positions[best]\r\n        best_iter = n_iters[best]\r\n\r\n    if return_n_iter:\r\n        return best_pos, best_stress, best_iter\r\n    else:\r\n        return best_pos, best_stress"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(self, X, y=None, init=None):\r\n        self.fit_transform(X, init=init)\r\n        return self", "response": "Fits the embedding space with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit_transform(self, X, y=None, init=None):\r\n        X = check_array(X)\r\n        if X.shape[0] == X.shape[1] and self.dissimilarity != \"precomputed\":\r\n            warnings.warn(\"The MDS API has changed. ``fit`` now constructs an\"\r\n                          \" dissimilarity matrix from data. To use a custom \"\r\n                          \"dissimilarity matrix, set \"\r\n                          \"``dissimilarity=precomputed``.\")\r\n\r\n        if self.dissimilarity == \"precomputed\":\r\n            self.dissimilarity_matrix_ = X\r\n        elif self.dissimilarity == \"euclidean\":\r\n            self.dissimilarity_matrix_ = euclidean_distances(X)\r\n        else:\r\n            raise ValueError(\"Proximity must be 'precomputed' or 'euclidean'.\"\r\n                             \" Got %s instead\" % str(self.dissimilarity))\r\n\r\n        self.embedding_, self.stress_, self.n_iter_ = smacof_p(\r\n            self.dissimilarity_matrix_, self.n_uq, metric=self.metric,\r\n            n_components=self.n_components, init=init, n_init=self.n_init,\r\n            n_jobs=self.n_jobs, max_iter=self.max_iter, verbose=self.verbose,\r\n            eps=self.eps, random_state=self.random_state,\r\n            return_n_iter=True)\r\n\r\n        return self.embedding_", "response": "Fit the data from X and returns the embedded coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef seed_random_state(seed):\n    if (seed is None) or (isinstance(seed, int)):\n        return np.random.RandomState(seed)\n    elif isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\"%r can not be used to generate numpy.random.RandomState\"\n                     \" instance\" % seed)", "response": "Turn seed into np. random. RandomState instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_cost(y, yhat, cost_matrix):\n    return np.mean(cost_matrix[list(y), list(yhat)])", "response": "Calculate the cost with given cost matrix y yhat"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a query of the unlabeled entries.", "response": "def make_query(self, return_score=False):\n        \"\"\"Return the index of the sample to be queried and labeled and\n        selection score of each sample. Read-only.\n\n        No modification to the internal states.\n\n        Returns\n        -------\n        ask_id : int\n            The index of the next unlabeled sample to be queried and labeled.\n\n        score : list of (index, score) tuple\n            Selection score of unlabled entries, the larger the better.\n\n        \"\"\"\n        dataset = self.dataset\n        self.model.train(dataset)\n\n        unlabeled_entry_ids, X_pool = zip(*dataset.get_unlabeled_entries())\n\n        if isinstance(self.model, ProbabilisticModel):\n            dvalue = self.model.predict_proba(X_pool)\n        elif isinstance(self.model, ContinuousModel):\n            dvalue = self.model.predict_real(X_pool)\n\n        if self.method == 'lc':  # least confident\n            score = -np.max(dvalue, axis=1)\n\n        elif self.method == 'sm':  # smallest margin\n            if np.shape(dvalue)[1] > 2:\n                # Find 2 largest decision values\n                dvalue = -(np.partition(-dvalue, 2, axis=1)[:, :2])\n            score = -np.abs(dvalue[:, 0] - dvalue[:, 1])\n\n        elif self.method == 'entropy':\n            score = np.sum(-dvalue * np.log(dvalue), axis=1)\n\n        ask_id = np.argmax(score)\n\n        if return_score:\n            return unlabeled_entry_ids[ask_id], \\\n                   list(zip(unlabeled_entry_ids, score))\n        else:\n            return unlabeled_entry_ids[ask_id]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _vote_disagreement(self, votes):\n        ret = []\n        for candidate in votes:\n            ret.append(0.0)\n            lab_count = {}\n            for lab in candidate:\n                lab_count[lab] = lab_count.setdefault(lab, 0) + 1\n\n            # Using vote entropy to measure disagreement\n            for lab in lab_count.keys():\n                ret[-1] -= lab_count[lab] / self.n_students * \\\n                    math.log(float(lab_count[lab]) / self.n_students)\n\n        return ret", "response": "Return the disagreement measurement of the given number of votes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the Kullback - Leibler divergence disagreement measure.", "response": "def _kl_divergence_disagreement(self, proba):\n        \"\"\"\n        Calculate the Kullback-Leibler (KL) divergence disaagreement measure.\n\n        Parameters\n        ----------\n        proba : array-like, shape=(n_samples, n_students, n_class)\n\n        Returns\n        -------\n        disagreement : list of float, shape=(n_samples)\n            The kl_divergence of the given probability.\n        \"\"\"\n        n_students = np.shape(proba)[1]\n        consensus = np.mean(proba, axis=1) # shape=(n_samples, n_class)\n        # average probability of each class across all students\n        consensus = np.tile(consensus, (n_students, 1, 1)).transpose(1, 0, 2)\n        kl = np.sum(proba * np.log(proba / consensus), axis=2)\n        return np.mean(kl, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsamples labeled entries uniformly", "response": "def _labeled_uniform_sample(self, sample_size):\n        \"\"\"sample labeled entries uniformly\"\"\"\n        labeled_entries = self.dataset.get_labeled_entries()\n        samples = [labeled_entries[\n            self.random_state_.randint(0, len(labeled_entries))\n        ]for _ in range(sample_size)]\n        return Dataset(*zip(*samples))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef teach_students(self):\n        dataset = self.dataset\n        for student in self.students:\n            bag = self._labeled_uniform_sample(int(dataset.len_labeled()))\n            while bag.get_num_of_labels() != dataset.get_num_of_labels():\n                bag = self._labeled_uniform_sample(int(dataset.len_labeled()))\n                LOGGER.warning('There is student receiving only one label,'\n                               're-sample the bag.')\n            student.train(bag)", "response": "Train each model with the labeled data using bootstrap\n        aggregating."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the reward value", "response": "def calc_reward_fn(self):\n        \"\"\"Calculate the reward value\"\"\"\n        model = copy.copy(self.model)\n        model.train(self.dataset)\n\n        # reward function: Importance-Weighted-Accuracy (IW-ACC) (tau, f)\n        reward = 0.\n        for i in range(len(self.queried_hist_)):\n            reward += self.W[i] * (\n                model.predict(\n                    self.dataset.data[\n                        self.queried_hist_[i]][0].reshape(1, -1)\n                    )[0] ==\n                self.dataset.data[self.queried_hist_[i]][1]\n            )\n        reward /= (self.dataset.len_labeled() + self.dataset.len_unlabeled())\n        reward /= self.T\n        return reward"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_query(self):\n        # initial query\n        if self.query_dist is None:\n            self.query_dist = self.exp4p_.next(-1, None, None)\n        else:\n            self.query_dist = self.exp4p_.next(\n                self.calc_reward_fn(),\n                self.queried_hist_[-1],\n                self.dataset.data[self.queried_hist_[-1]][1]\n            )\n        return", "response": "Calculate the sampling query distribution"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next(self, reward, ask_id, lbl):\n        # first run don't have reward, TODO exception on reward == -1 only once\n        if reward == -1:\n            return next(self.exp4p_gen)\n        else:\n            # TODO exception on reward in [0, 1]\n            return self.exp4p_gen.send((reward, ask_id, lbl))", "response": "Taking the label and the reward value of last question and returns\n        the next question to ask."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exp4p(self):\n        while True:\n            # TODO probabilistic active learning algorithm\n            # len(self.unlabeled_invert_id_idx) is the number of unlabeled data\n            query = np.zeros((self.N, len(self.unlabeled_invert_id_idx)))\n            if self.uniform_sampler:\n                query[-1, :] = 1. / len(self.unlabeled_invert_id_idx)\n            for i, model in enumerate(self.query_strategies_):\n                query[i][self.unlabeled_invert_id_idx[model.make_query()]] = 1\n\n            # choice vector, shape = (self.K, )\n            W = np.sum(self.w)\n            p = (1 - self.K * self.pmin) * self.w / W + self.pmin\n\n            # query vector, shape= = (self.n_unlabeled, )\n            query_vector = np.dot(p, query)\n\n            reward, ask_id, _ = yield query_vector\n            ask_idx = self.unlabeled_invert_id_idx[ask_id]\n\n            rhat = reward * query[:, ask_idx] / query_vector[ask_idx]\n\n            # The original advice vector in Exp4.P in ALBL is a identity matrix\n            yhat = rhat\n            vhat = 1 / p\n            self.w = self.w * np.exp(\n                self.pmin / 2 * (\n                    yhat + vhat * np.sqrt(\n                        np.log(self.N / self.delta) / self.K / self.T\n                    )\n                )\n            )\n\n        raise StopIteration", "response": "The main part of the main part of the Exp4. P generator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_libsvm_sparse(filename):\n    from sklearn.datasets import load_svmlight_file\n    X, y = load_svmlight_file(filename)\n    return Dataset(X.toarray().tolist(), y.tolist())", "response": "Imports dataset file in libsvm sparse format"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending a new entry into the dataset.", "response": "def append(self, feature, label=None):\n        \"\"\"\n        Add a (feature, label) entry into the dataset.\n        A None label indicates an unlabeled entry.\n\n        Parameters\n        ----------\n        feature : {array-like}, shape = (n_features)\n            Feature of the sample to append to dataset.\n\n        label : {int, None}\n            Label of the sample to append to dataset. None if unlabeled.\n\n        Returns\n        -------\n        entry_id : {int}\n            entry_id for the appened sample.\n        \"\"\"\n        self.data.append((feature, label))\n        self.modified = True\n        return len(self.data) - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, entry_id, new_label):\n        self.data[entry_id] = (self.data[entry_id][0], new_label)\n        self.modified = True\n        for callback in self._update_callback:\n            callback(entry_id, new_label)", "response": "Updates an entry with the given label and updates the entry with the given entry_id with the given label."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_sklearn(self):\n        X, y = zip(*self.get_labeled_entries())\n        return np.array(X), np.array(y)", "response": "Returns dataset in ( X y format for use in scikit - learn."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of unlabeled features along with their entry_ids", "response": "def get_unlabeled_entries(self):\n        \"\"\"\n        Returns list of unlabeled features, along with their entry_ids\n\n        Returns\n        -------\n        unlabeled_entries : list of (entry_id, feature) tuple\n            Labeled entries\n        \"\"\"\n        return [\n            (idx, entry[0]) for idx, entry in enumerate(self.data)\n            if entry[1] is None\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef labeled_uniform_sample(self, sample_size, replace=True):\n        if replace:\n            samples = [\n                random.choice(self.get_labeled_entries())\n                for _ in range(sample_size)\n            ]\n        else:\n            samples = random.sample(self.get_labeled_entries(), sample_size)\n        return Dataset(*zip(*samples))", "response": "Returns a Dataset object with labeled data only with given sample size."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the action is satisfied.", "response": "def action_is_satisfied(action):\n    ''' Returns False if the parse would raise an error if no more arguments are given to this action, True otherwise.\n    '''\n    num_consumed_args = _num_consumed_args.get(action, 0)\n\n    if action.nargs in [OPTIONAL, ZERO_OR_MORE, REMAINDER]:\n        return True\n    if action.nargs == ONE_OR_MORE:\n        return num_consumed_args >= 1\n    if action.nargs == PARSER:\n        # Not sure what this should be, but this previously always returned False\n        # so at least this won't break anything that wasn't already broken.\n        return False\n    if action.nargs is None:\n        return num_consumed_args == 1\n\n    assert isinstance(action.nargs, int), 'failed to handle a possible nargs value: %r' % action.nargs\n    return num_consumed_args == action.nargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef action_is_greedy(action, isoptional=False):\n    ''' Returns True if action will necessarily consume the next argument.\n    isoptional indicates whether the argument is an optional (starts with -).\n    '''\n    num_consumed_args = _num_consumed_args.get(action, 0)\n\n    if action.option_strings:\n        if not isoptional and not action_is_satisfied(action):\n            return True\n        return action.nargs == REMAINDER\n    else:\n        return action.nargs == REMAINDER and num_consumed_args >= 1", "response": "Returns True if action will necessarily consume the next argument."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push_token(self, tok):\n        \"Push a token onto the stack popped by the get_token method\"\n        if self.debug >= 1:\n            print(\"shlex: pushing token \" + repr(tok))\n        self.pushback.appendleft(tok)", "response": "Push a token onto the stack popped by the get_token method"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npop the input source stack.", "response": "def pop_source(self):\n        \"Pop the input source stack.\"\n        self.instream.close()\n        (self.infile, self.instream, self.lineno) = self.filestack.popleft()\n        if self.debug:\n            print('shlex: popping to %s, line %d' \\\n                  % (self.instream, self.lineno))\n        self.state = ' '"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmonkeys - patch argparse. ArgumentParser to monkey - patch all argparse. SubParsersAction and argparse. ArgumentParser to monkey - patch all argparse. ArgumentParser s active_parsers and argparse. SubParsersAction classes.", "response": "def _patch_argument_parser(self):\n        \"\"\"\n        Since argparse doesn't support much introspection, we monkey-patch it to replace the parse_known_args method and\n        all actions with hooks that tell us which action was last taken or about to be taken, and let us have the parser\n        figure out which subparsers need to be activated (then recursively monkey-patch those).\n        We save all active ArgumentParsers to extract all their possible option names later.\n        \"\"\"\n        self.active_parsers = []\n        self.visited_positionals = []\n\n        completer = self\n\n        def patch(parser):\n            completer.visited_positionals.append(parser)\n            completer.active_parsers.append(parser)\n\n            if isinstance(parser, IntrospectiveArgumentParser):\n                return\n\n            classname = \"MonkeyPatchedIntrospectiveArgumentParser\"\n            if USING_PYTHON2:\n                classname = bytes(classname)\n            parser.__class__ = type(classname, (IntrospectiveArgumentParser, parser.__class__), {})\n\n            for action in parser._actions:\n\n                if hasattr(action, \"_orig_class\"):\n                    continue\n\n                # TODO: accomplish this with super\n                class IntrospectAction(action.__class__):\n                    def __call__(self, parser, namespace, values, option_string=None):\n                        debug(\"Action stub called on\", self)\n                        debug(\"\\targs:\", parser, namespace, values, option_string)\n                        debug(\"\\torig class:\", self._orig_class)\n                        debug(\"\\torig callable:\", self._orig_callable)\n\n                        if not completer.completing:\n                            self._orig_callable(parser, namespace, values, option_string=option_string)\n                        elif issubclass(self._orig_class, argparse._SubParsersAction):\n                            debug(\"orig class is a subparsers action: patching and running it\")\n                            patch(self._name_parser_map[values[0]])\n                            self._orig_callable(parser, namespace, values, option_string=option_string)\n                        elif self._orig_class in safe_actions:\n                            if not self.option_strings:\n                                completer.visited_positionals.append(self)\n\n                            self._orig_callable(parser, namespace, values, option_string=option_string)\n\n                action._orig_class = action.__class__\n                action._orig_callable = action.__call__\n                action.__class__ = IntrospectAction\n\n        patch(self._parser)\n\n        debug(\"Active parsers:\", self.active_parsers)\n        debug(\"Visited positionals:\", self.visited_positionals)\n\n        return self.active_parsers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect_completions(self, active_parsers, parsed_args, cword_prefix, debug):\n        completions = []\n\n        debug(\"all active parsers:\", active_parsers)\n        active_parser = active_parsers[-1]\n        debug(\"active_parser:\", active_parser)\n        if self.always_complete_options or (len(cword_prefix) > 0 and cword_prefix[0] in active_parser.prefix_chars):\n            completions += self._get_option_completions(active_parser, cword_prefix)\n        debug(\"optional options:\", completions)\n\n        next_positional = self._get_next_positional()\n        debug(\"next_positional:\", next_positional)\n\n        if isinstance(next_positional, argparse._SubParsersAction):\n            completions += self._get_subparser_completions(next_positional, cword_prefix)\n\n        completions = self._complete_active_option(active_parser, next_positional, cword_prefix, parsed_args,\n                                                   completions)\n        debug(\"active options:\", completions)\n        debug(\"display completions:\", self._display_completions)\n\n        return completions", "response": "Returns a list of strings that are available for all the options in the active parsers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_next_positional(self):\n        active_parser = self.active_parsers[-1]\n        last_positional = self.visited_positionals[-1]\n\n        all_positionals = active_parser._get_positional_actions()\n        if not all_positionals:\n            return None\n\n        if active_parser == last_positional:\n            return all_positionals[0]\n\n        i = 0\n        for i in range(len(all_positionals)):\n            if all_positionals[i] == last_positional:\n                break\n\n        if i + 1 < len(all_positionals):\n            return all_positionals[i + 1]\n\n        return None", "response": "Get the next positional action if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering the completions to only contain those that are not already in the set of excluded completions.", "response": "def filter_completions(self, completions):\n        \"\"\"\n        Ensures collected completions are Unicode text, de-duplicates them, and excludes those specified by ``exclude``.\n        Returns the filtered completions as an iterable.\n\n        This method is exposed for overriding in subclasses; there is no need to use it directly.\n        \"\"\"\n        # On Python 2, we have to make sure all completions are unicode objects before we continue and output them.\n        # Otherwise, because python disobeys the system locale encoding and uses ascii as the default encoding, it will\n        # try to implicitly decode string objects using ascii, and fail.\n        completions = [ensure_str(c) for c in completions]\n\n        # De-duplicate completions and remove excluded ones\n        if self.exclude is None:\n            self.exclude = set()\n        seen = set(self.exclude)\n        return [c for c in completions if c not in seen and not seen.add(c)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquoting the completions in the current word.", "response": "def quote_completions(self, completions, cword_prequote, last_wordbreak_pos):\n        \"\"\"\n        If the word under the cursor started with a quote (as indicated by a nonempty ``cword_prequote``), escapes\n        occurrences of that quote character in the completions, and adds the quote to the beginning of each completion.\n        Otherwise, escapes all characters that bash splits words on (``COMP_WORDBREAKS``), and removes portions of\n        completions before the first colon if (``COMP_WORDBREAKS``) contains a colon.\n\n        If there is only one completion, and it doesn't end with a **continuation character** (``/``, ``:``, or ``=``),\n        adds a space after the completion.\n\n        This method is exposed for overriding in subclasses; there is no need to use it directly.\n        \"\"\"\n        special_chars = \"\\\\\"\n        # If the word under the cursor was quoted, escape the quote char.\n        # Otherwise, escape all special characters and specially handle all COMP_WORDBREAKS chars.\n        if cword_prequote == \"\":\n            # Bash mangles completions which contain characters in COMP_WORDBREAKS.\n            # This workaround has the same effect as __ltrim_colon_completions in bash_completion\n            # (extended to characters other than the colon).\n            if last_wordbreak_pos:\n                completions = [c[last_wordbreak_pos + 1:] for c in completions]\n            special_chars += \"();<>|&!`$* \\t\\n\\\"'\"\n        elif cword_prequote == '\"':\n            special_chars += '\"`$!'\n\n        if os.environ.get(\"_ARGCOMPLETE_SHELL\") == \"tcsh\":\n            # tcsh escapes special characters itself.\n            special_chars = \"\"\n        elif cword_prequote == \"'\":\n            # Nothing can be escaped in single quotes, so we need to close\n            # the string, escape the single quote, then open a new string.\n            special_chars = \"\"\n            completions = [c.replace(\"'\", r\"'\\''\") for c in completions]\n\n        for char in special_chars:\n            completions = [c.replace(char, \"\\\\\" + char) for c in completions]\n\n        if self.append_space:\n            # Similar functionality in bash was previously turned off by supplying the \"-o nospace\" option to complete.\n            # Now it is conditionally disabled using \"compopt -o nospace\" if the match ends in a continuation character.\n            # This code is retained for environments where this isn't done natively.\n            continuation_chars = \"=/:\"\n            if len(completions) == 1 and completions[0][-1] not in continuation_chars:\n                if cword_prequote == \"\":\n                    completions[0] += \" \"\n\n        return completions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the completion of the given text.", "response": "def rl_complete(self, text, state):\n        \"\"\"\n        Alternate entry point for using the argcomplete completer in a readline-based REPL. See also\n        `rlcompleter <https://docs.python.org/2/library/rlcompleter.html#completer-objects>`_.\n        Usage:\n\n        .. code-block:: python\n\n            import argcomplete, argparse, readline\n            parser = argparse.ArgumentParser()\n            ...\n            completer = argcomplete.CompletionFinder(parser)\n            readline.set_completer_delims(\"\")\n            readline.set_completer(completer.rl_complete)\n            readline.parse_and_bind(\"tab: complete\")\n            result = input(\"prompt> \")\n\n        (Use ``raw_input`` instead of ``input`` on Python 2, or use `eight <https://github.com/kislyuk/eight>`_).\n        \"\"\"\n        if state == 0:\n            cword_prequote, cword_prefix, cword_suffix, comp_words, first_colon_pos = split_line(text)\n            comp_words.insert(0, sys.argv[0])\n            matches = self._get_completions(comp_words, cword_prefix, cword_prequote, first_colon_pos)\n            self._rl_matches = [text + match[len(cword_prefix):] for match in matches]\n\n        if state < len(self._rl_matches):\n            return self._rl_matches[state]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shellcode(executables, use_defaults=True, shell='bash', complete_arguments=None):\n    '''\n    Provide the shell code required to register a python executable for use with the argcomplete module.\n\n    :param str executables: Executables to be completed (when invoked exactly with this name\n    :param bool use_defaults: Whether to fallback to readline's default completion when no matches are generated.\n    :param str shell: Name of the shell to output code for (bash or tcsh)\n    :param complete_arguments: Arguments to call complete with\n    :type complete_arguments: list(str) or None\n    '''\n\n    if complete_arguments is None:\n        complete_options = '-o nospace -o default' if use_defaults else '-o nospace'\n    else:\n        complete_options = \" \".join(complete_arguments)\n\n    if shell == 'bash':\n        quoted_executables = [quote(i) for i in executables]\n        executables_list = \" \".join(quoted_executables)\n        code = bashcode % dict(complete_opts=complete_options, executables=executables_list)\n    else:\n        code = \"\"\n        for executable in executables:\n            code += tcshcode % dict(executable=executable)\n\n    return code", "response": "Generates the code required to run a python executable with the argcomplete module."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a unique file name.", "response": "def _get_filename(self):\n        \"\"\"Return a unique file name.\"\"\"\n        if self._fname is None:\n            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            fname = \"%s-%s.log\" % (timestamp, abs(id(self)))\n            self._fname = os.path.join(self.file_path, fname)\n        return self._fname"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_balance(self):\n        if not SMSGLOBAL_CHECK_BALANCE_COUNTRY:\n            raise Exception('SMSGLOBAL_CHECK_BALANCE_COUNTRY setting must be set to check balance.')\n        \n        params = {\n          'user' : self.get_username(),\n          'password' : self.get_password(),\n          'country' : SMSGLOBAL_CHECK_BALANCE_COUNTRY,\n        }\n        \n        req = urllib2.Request(SMSGLOBAL_API_URL_CHECKBALANCE, urllib.urlencode(params))\n        response = urllib2.urlopen(req).read()\n\n        # CREDITS:8658.44;COUNTRY:AU;SMS:3764.54;\n        if response.startswith('ERROR'):\n            raise Exception('Error retrieving balance: %s' % response.replace('ERROR:', ''))\n        \n        return dict([(p.split(':')[0].lower(), p.split(':')[1]) for p in response.split(';') if len(p) > 0])", "response": "Get balance with provider."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends one or more SmsMessage objects and returns the number of messages sent.", "response": "def send_messages(self, sms_messages):\n        \"\"\"\n        Sends one or more SmsMessage objects and returns the number of sms\n        messages sent.\n        \"\"\"\n        if not sms_messages:\n            return\n        \n        num_sent = 0\n        for message in sms_messages:\n            if self._send(message):\n                num_sent += 1\n        return num_sent"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _send(self, message):\n        charset='UTF-8'\n        params = {\n          'action' : 'sendsms',\n          'user' : self.get_username(),\n          'password' : self.get_password(),\n          'from' : message.from_phone,\n          'to' : \",\".join(message.to),\n          'text' : message.body,\n          'clientcharset' : charset,\n          'detectcharset' : 1,\n          'maxsplit': int(math.ceil(len(message.body) / 160))\n        }\n        \n        req = urllib2.Request(SMSGLOBAL_API_URL_SENDSMS, urllib.urlencode(params))\n        result_page = urllib2.urlopen(req).read()\n        results = self._parse_response(result_page)\n        \n        if results is None:\n            if not self.fail_silently:\n                raise Exception(\"Error determining response: [\" + result_page + \"]\")\n            return False\n        \n        code, sendqmsgid, msgid = results\n        \n        if code != '0':\n            if not self.fail_silently:\n                raise Exception(\"Error sending sms: [%s], extracted results(code, sendqmsgid, msgid): [%s]\" % (result_page, results))\n            return False\n        else:\n            logger.info('SENT to: %s; sender: %s; code: %s; sendqmsgid: %s; msgid: %s; message: %s' % (\n                message.to,\n                message.from_phone,\n                code, \n                sendqmsgid, \n                msgid,\n                message.body\n            ))\n            return True", "response": "A helper method that does the actual sending."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_response(self, result_page):\n        # Sample result_page, single line -> \"OK: 0; Sent queued message ID: 2063619577732703 SMSGlobalMsgID:6171799108850954\"\n        resultline = result_page.splitlines()[0] # get result line\n        if resultline.startswith('ERROR:'):\n            raise Exception(resultline.replace('ERROR: ', ''))\n        patt = re.compile(r'^.+?:\\s*(.+?)\\s*;\\s*Sent queued message ID:\\s*(.+?)\\s*SMSGlobalMsgID:(.+?)$', re.IGNORECASE)\n        m = patt.match(resultline)\n        if m:\n            return (m.group(1), m.group(2), m.group(3)) \n        return None", "response": "Parses the response from the sms server into a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_sms(body, from_phone, to, flash=False, fail_silently=False,\n             auth_user=None, auth_password=None, connection=None):\n    \"\"\"\n    Easy wrapper for send a single SMS to a recipient list.\n\n    :returns: the number of SMSs sent.\n    \"\"\"\n    from sendsms.message import SmsMessage\n    connection = connection or get_connection(\n        username = auth_user,\n        password = auth_password,\n        fail_silently = fail_silently\n    )\n    return SmsMessage(body=body, from_phone=from_phone, to=to, \\\n        flash=flash, connection=connection).send()", "response": "Send a single SMS to a list of recipients."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_mass_sms(datatuple, fail_silently=False,\n             auth_user=None, auth_password=None, connection=None):\n    \"\"\"\n    Given a datatuple of (message, from_phone, to, flash), sends each message to each\n    recipient list.\n\n    :returns: the number of SMSs sent.\n    \"\"\"\n\n    from sendsms.message import SmsMessage\n    connection = connection or get_connection(\n        username = auth_user,\n        password = auth_password,\n        fail_silently = fail_silently\n    )\n    messages = [SmsMessage(message=message, from_phone=from_phone, to=to, flash=flash)\n                for message, from_phone, to, flash in datatuple]\n    connection.send_messages(messages)", "response": "Sends a mass SMS message to each node in the order they were sent."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_connection(path=None, fail_silently=False, **kwargs):\n\n    path = path or getattr(settings, 'SENDSMS_BACKEND', 'sendsms.backends.locmem.SmsBackend')\n    try:\n        mod_name, klass_name = path.rsplit('.', 1)\n        mod = import_module(mod_name)\n    except AttributeError as e:\n        raise ImproperlyConfigured(u'Error importing sms backend module %s: \"%s\"' % (mod_name, e))\n\n    try:\n        klass = getattr(mod, klass_name)\n    except AttributeError:\n        raise ImproperlyConfigured('Module \"%s\" does not define a \"%s\" class' % (mod_name, klass_name))\n\n    return klass(fail_silently=fail_silently, **kwargs)", "response": "Load an SMS backend and return an instance of it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize sms. sluzba. cz API library.", "response": "def open(self):\n        \"\"\"Initializes sms.sluzba.cz API library.\"\"\"\n        self.client = SmsGateApi(getattr(settings, 'SMS_SLUZBA_API_LOGIN', ''),\n                                 getattr(settings, 'SMS_SLUZBA_API_PASSWORD', ''),\n                                 getattr(settings, 'SMS_SLUZBA_API_TIMEOUT', 2),\n                                 getattr(settings, 'SMS_SLUZBA_API_USE_SSL', True))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending SMS messages via sms. sluzba. cz API.", "response": "def send_messages(self, messages):\n        \"\"\"Sending SMS messages via sms.sluzba.cz API.\n\n        Note:\n          This method returns number of actually sent sms messages\n          not number of SmsMessage instances processed.\n\n        :param messages: list of sms messages\n        :type messages: list of sendsms.message.SmsMessage instances\n        :returns: number of sent sms messages\n        :rtype: int\n\n        \"\"\"\n        count = 0\n        for message in messages:\n            message_body = unicodedata.normalize('NFKD', unicode(message.body)).encode('ascii', 'ignore')\n            for tel_number in message.to:\n                try:\n                    self.client.send(tel_number, message_body, getattr(settings, 'SMS_SLUZBA_API_USE_POST', True))\n                except Exception:\n                    if self.fail_silently:\n                        log.exception('Error while sending sms via sms.sluzba.cz backend API.')\n                    else:\n                        raise\n                else:\n                    count += 1\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_messages(self, messages):\n        if not messages:\n            return\n        self._lock.acquire()\n        try:\n            # The try-except is nested to allow for\n            # Python 2.4 support (Refs #12147)\n            try:\n                stream_created = self.open()\n                for message in messages:\n                    self.stream.write(render_message(message))\n                    self.stream.write('\\n')\n                    self.stream.write('-'*79)\n                    self.stream.write('\\n')\n                    self.stream.flush()  # flush after each message\n                if stream_created:\n                    self.close()\n            except:\n                if not self.fail_silently:\n                    raise\n        finally:\n            self._lock.release()\n        return len(messages)", "response": "Write all messages to the stream in a thread - safe way."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_response(self, response):\n\n        response_dict = {}\n        for line in response.splitlines():\n            key, value = response.split(\"=\", 1)\n            response_dict[key] = value\n        return response_dict", "response": "Parse the response into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _send(self, message):\n\n        params = {\n            'EsendexUsername': self.get_username(),\n            'EsendexPassword': self.get_password(),\n            'EsendexAccount': self.get_account(), \n            'EsendexOriginator': message.from_phone, \n            'EsendexRecipient': \",\".join(message.to),\n            'EsendexBody': message.body,\n            'EsendexPlainText':'1'\n        }\n        if ESENDEX_SANDBOX:\n            params['EsendexTest'] = '1'\n\n        response = requests.post(ESENDEX_API_URL, params)\n        if response.status_code != 200:\n            if not self.fail_silently:\n                raise Exception('Bad status code')\n            else:\n                return False\n        \n        if not response.content.startswith(b'Result'):\n            if not self.fail_silently:\n                raise Exception('Bad result')\n            else: \n                return False\n\n        response = self._parse_response(response.content.decode('utf8'))\n        \n        if ESENDEX_SANDBOX and response['Result'] == 'Test':\n            return True\n        else:\n            if response['Result'].startswith('OK'):\n                return True\n            else:\n                if not self.fail_silently:\n                    raise Exception('Bad result')\n        \n        return False", "response": "Private method to send one message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading an object from an import_path.", "response": "def load_object(import_path):\n    \"\"\"\n    Shamelessly stolen from https://github.com/ojii/django-load\n    \n    Loads an object from an 'import_path', like in MIDDLEWARE_CLASSES and the\n    likes.\n\n    Import paths should be: \"mypackage.mymodule.MyObject\". It then imports the\n    module up until the last dot and tries to get the attribute after that dot\n    from the imported module.\n\n    If the import path does not contain any dots, a TypeError is raised.\n\n    If the module cannot be imported, an ImportError is raised.\n\n    If the attribute does not exist in the module, a AttributeError is raised.\n    \"\"\"\n    if '.' not in import_path:\n        raise TypeError(\n            \"'import_path' argument to 'load_object' must \"\n            \"contain at least one dot.\"\n        )\n    module_name, object_name = import_path.rsplit('.', 1)\n    module = import_module(module_name)\n    return getattr(module, object_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends the SMS message to the specified to_phone and returns the number of bytes sent.", "response": "def send(self, fail_silently=False):\n        \"\"\"\n        Sends the sms message\n        \"\"\"\n        if not self.to:\n            # Don't bother creating the connection if there's nobody to send to\n            return 0\n        res = self.get_connection(fail_silently).send_messages([self])\n        sms_post_send.send(sender=self, to=self.to, from_phone=self.from_phone, body=self.body)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _send(self, message):\n\n        params = {\n            'from': message.from_phone, \n            'to': \",\".join(message.to),\n            'text': message.body,\n            'api_key': self.get_api_key(),\n            'api_secret': self.get_api_secret(),\n        }\n\n\n        print(params)\n\n        logger.debug(\"POST to %r with body: %r\", NEXMO_API_URL, params)\n\n        return self.parse(NEXMO_API_URL, requests.post(NEXMO_API_URL, data=params))", "response": "A helper method that does the actual sending of a message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a list of SmsMessage instances to the server.", "response": "def send_messages(self, messages):\n        \"\"\"\n        Send messages.\n\n        :param list messages: List of SmsMessage instances.\n        :returns: number of messages sended successful.\n        :rtype: int\n        \"\"\"\n        counter = 0\n        for message in messages:\n            res, _ = self._send(message)\n            if res:\n                counter += 1\n\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _api_query(self, protection=None, path_dict=None, options=None):\n\n        if not options:\n            options = {}\n\n        if self.api_version not in path_dict:\n            raise Exception('method call not available under API version {}'.format(self.api_version))\n\n        request_url = BASE_URL_V2_0 if self.api_version == API_V2_0 else BASE_URL_V1_1\n        request_url = request_url.format(path=path_dict[self.api_version])\n\n        nonce = str(int(time.time() * 1000))\n\n        if protection != PROTECTION_PUB:\n            request_url = \"{0}apikey={1}&nonce={2}&\".format(request_url, self.api_key, nonce)\n\n        request_url += urlencode(options)\n\n        try:\n            if sys.version_info >= (3, 0) and protection != PROTECTION_PUB:\n\n                apisign = hmac.new(bytearray(self.api_secret, 'ascii'),\n                                   bytearray(request_url, 'ascii'),\n                                   hashlib.sha512).hexdigest()\n\n            else:\n\n                apisign = hmac.new(self.api_secret.encode(),\n                                   request_url.encode(),\n                                   hashlib.sha512).hexdigest()\n\n            self.wait()\n\n            return self.dispatch(request_url, apisign)\n\n        except Exception:\n            return {\n                'success': False,\n                'message': 'NO_API_RESPONSE',\n                'result': None\n            }", "response": "Queries Bittrex to get information about the current bittrex entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_market_summary(self, market):\n        return self._api_query(path_dict={\n            API_V1_1: '/public/getmarketsummary',\n            API_V2_0: '/pub/Market/GetMarketSummary'\n        }, options={'market': market, 'marketname': market}, protection=PROTECTION_PUB)", "response": "This method returns the last 24 hour summary of all active exchanges in a specific market"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_orderbook(self, market, depth_type=BOTH_ORDERBOOK):\n        return self._api_query(path_dict={\n            API_V1_1: '/public/getorderbook',\n            API_V2_0: '/pub/Market/GetMarketOrderBook'\n        }, options={'market': market, 'marketname': market, 'type': depth_type}, protection=PROTECTION_PUB)", "response": "This method returns the orderbook for a given market."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_market_history(self, market):\n        return self._api_query(path_dict={\n            API_V1_1: '/public/getmarkethistory',\n        }, options={'market': market, 'marketname': market}, protection=PROTECTION_PUB)", "response": "This method returns the latest trades that have occurred for a specific market."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef buy_limit(self, market, quantity, rate):\n        return self._api_query(path_dict={\n            API_V1_1: '/market/buylimit',\n        }, options={'market': market,\n                    'quantity': quantity,\n                    'rate': rate}, protection=PROTECTION_PRV)", "response": "Place a buy order in a specific market"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_open_orders(self, market=None):\n        return self._api_query(path_dict={\n            API_V1_1: '/market/getopenorders',\n            API_V2_0: '/key/market/getopenorders'\n        }, options={'market': market, 'marketname': market} if market else None, protection=PROTECTION_PRV)", "response": "This method returns the open orders info in JSON format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_deposit_address(self, currency):\n        return self._api_query(path_dict={\n            API_V1_1: '/account/getdepositaddress',\n            API_V2_0: '/key/balance/getdepositaddress'\n        }, options={'currency': currency, 'currencyname': currency}, protection=PROTECTION_PRV)", "response": "This method returns the deposit address for a specific currency"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef withdraw(self, currency, quantity, address, paymentid=None):\n        options = {\n            'currency': currency,\n            'quantity': quantity,\n            'address': address\n        }\n        if paymentid:\n            options['paymentid'] = paymentid\n        return self._api_query(path_dict={\n            API_V1_1: '/account/withdraw',\n            API_V2_0: '/key/balance/withdrawcurrency'\n        }, options=options, protection=PROTECTION_PRV)", "response": "This method is used to withdraw funds from your account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_order_history(self, market=None):\n        if market:\n            return self._api_query(path_dict={\n                API_V1_1: '/account/getorderhistory',\n                API_V2_0: '/key/market/GetOrderHistory'\n            }, options={'market': market, 'marketname': market}, protection=PROTECTION_PRV)\n        else:\n            return self._api_query(path_dict={\n                API_V1_1: '/account/getorderhistory',\n                API_V2_0: '/key/orders/getorderhistory'\n            }, protection=PROTECTION_PRV)", "response": "This method returns the order trade history of the account"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse to get details of buy or sell order", "response": "def get_order(self, uuid):\n        \"\"\"\n        Used to get details of buy or sell order\n\n        Endpoint:\n        1.1 /account/getorder\n        2.0 /key/orders/getorder\n\n        :param uuid: uuid of buy or sell order\n        :type uuid: str\n        :return:\n        :rtype : dict\n        \"\"\"\n        return self._api_query(path_dict={\n            API_V1_1: '/account/getorder',\n            API_V2_0: '/key/orders/getorder'\n        }, options={'uuid': uuid, 'orderid': uuid}, protection=PROTECTION_PRV)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of markets that appear in a currency.", "response": "def list_markets_by_currency(self, currency):\n        \"\"\"\n        Helper function to see which markets exist for a currency.\n\n        Endpoint: /public/getmarkets\n\n        Example ::\n            >>> Bittrex(None, None).list_markets_by_currency('LTC')\n            ['BTC-LTC', 'ETH-LTC', 'USDT-LTC']\n\n        :param currency: String literal for the currency (ex: LTC)\n        :type currency: str\n        :return: List of markets that the currency appears in\n        :rtype: list\n        \"\"\"\n        return [market['MarketName'] for market in self.get_markets()['result']\n                if market['MarketName'].lower().endswith(currency.lower())]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses to view your pending withdrawals Endpoint: 1.1 NO EQUIVALENT 2.0 /key/balance/getpendingwithdrawals :param currency: String literal for the currency (ie. BTC) :type currency: str :return: pending withdrawals in JSON :rtype : list", "response": "def get_pending_withdrawals(self, currency=None):\n        \"\"\"\n        Used to view your pending withdrawals\n\n        Endpoint:\n        1.1 NO EQUIVALENT\n        2.0 /key/balance/getpendingwithdrawals\n\n        :param currency: String literal for the currency (ie. BTC)\n        :type currency: str\n        :return: pending withdrawals in JSON\n        :rtype : list\n        \"\"\"\n        return self._api_query(path_dict={\n            API_V2_0: '/key/balance/getpendingwithdrawals'\n        }, options={'currencyname': currency} if currency else None,\n            protection=PROTECTION_PRV)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenters a sell order into the book Endpoint 1.1 NO EQUIVALENT -- see sell_market or sell_limit 2.0 /key/market/tradesell :param market: String literal for the market (ex: BTC-LTC) :type market: str :param order_type: ORDERTYPE_LIMIT = 'LIMIT' or ORDERTYPE_MARKET = 'MARKET' :type order_type: str :param quantity: The amount to purchase :type quantity: float :param rate: The rate at which to place the order. This is not needed for market orders :type rate: float :param time_in_effect: TIMEINEFFECT_GOOD_TIL_CANCELLED = 'GOOD_TIL_CANCELLED', TIMEINEFFECT_IMMEDIATE_OR_CANCEL = 'IMMEDIATE_OR_CANCEL', or TIMEINEFFECT_FILL_OR_KILL = 'FILL_OR_KILL' :type time_in_effect: str :param condition_type: CONDITIONTYPE_NONE = 'NONE', CONDITIONTYPE_GREATER_THAN = 'GREATER_THAN', CONDITIONTYPE_LESS_THAN = 'LESS_THAN', CONDITIONTYPE_STOP_LOSS_FIXED = 'STOP_LOSS_FIXED', CONDITIONTYPE_STOP_LOSS_PERCENTAGE = 'STOP_LOSS_PERCENTAGE' :type condition_type: str :param target: used in conjunction with condition_type :type target: float :return:", "response": "def trade_sell(self, market=None, order_type=None, quantity=None, rate=None, time_in_effect=None,\n                   condition_type=None, target=0.0):\n        \"\"\"\n        Enter a sell order into the book\n        Endpoint\n        1.1 NO EQUIVALENT -- see sell_market or sell_limit\n        2.0 /key/market/tradesell\n\n        :param market: String literal for the market (ex: BTC-LTC)\n        :type market: str\n        :param order_type: ORDERTYPE_LIMIT = 'LIMIT' or ORDERTYPE_MARKET = 'MARKET'\n        :type order_type: str\n        :param quantity: The amount to purchase\n        :type quantity: float\n        :param rate: The rate at which to place the order.\n            This is not needed for market orders\n        :type rate: float\n        :param time_in_effect: TIMEINEFFECT_GOOD_TIL_CANCELLED = 'GOOD_TIL_CANCELLED',\n                TIMEINEFFECT_IMMEDIATE_OR_CANCEL = 'IMMEDIATE_OR_CANCEL', or TIMEINEFFECT_FILL_OR_KILL = 'FILL_OR_KILL'\n        :type time_in_effect: str\n        :param condition_type: CONDITIONTYPE_NONE = 'NONE', CONDITIONTYPE_GREATER_THAN = 'GREATER_THAN',\n                CONDITIONTYPE_LESS_THAN = 'LESS_THAN', CONDITIONTYPE_STOP_LOSS_FIXED = 'STOP_LOSS_FIXED',\n                CONDITIONTYPE_STOP_LOSS_PERCENTAGE = 'STOP_LOSS_PERCENTAGE'\n        :type condition_type: str\n        :param target: used in conjunction with condition_type\n        :type target: float\n        :return:\n        \"\"\"\n        return self._api_query(path_dict={\n            API_V2_0: '/key/market/tradesell'\n        }, options={\n            'marketname': market,\n            'ordertype': order_type,\n            'quantity': quantity,\n            'rate': rate,\n            'timeInEffect': time_in_effect,\n            'conditiontype': condition_type,\n            'target': target\n        }, protection=PROTECTION_PRV)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_candles(self, market, tick_interval):\n\n        return self._api_query(path_dict={\n            API_V2_0: '/pub/market/GetTicks'\n        }, options={\n            'marketName': market, 'tickInterval': tick_interval\n        }, protection=PROTECTION_PUB)", "response": "This method returns a list of available tick candles for a given market and tick interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd advanced_filters form to changelist context", "response": "def changelist_view(self, request, extra_context=None):\n        \"\"\"Add advanced_filters form to changelist context\"\"\"\n        if extra_context is None:\n            extra_context = {}\n        response = self.adv_filters_handle(request,\n                                           extra_context=extra_context)\n        if response:\n            return response\n        return super(AdminAdvancedFiltersMixin, self\n                     ).changelist_view(request, extra_context=extra_context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters items that should be displayed to a user", "response": "def filter_by_user(self, user):\n        \"\"\"All filters that should be displayed to a user (by users/group)\"\"\"\n\n        return self.filter(Q(users=user) | Q(groups__in=user.groups.all()))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(self):\n        if not self.b64_query:\n            return None\n        s = QSerializer(base64=True)\n        return s.loads(self.b64_query)", "response": "De - serialize decode and return an ORM query stored in b64_query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(self, value):\n        if not isinstance(value, Q):\n            raise Exception('Must only be passed a Django (Q)uery object')\n        s = QSerializer(base64=True)\n        self.b64_query = s.dumps(value)", "response": "Serialize an ORM query Base - 64 encode it and set it to the b64_query field"}
